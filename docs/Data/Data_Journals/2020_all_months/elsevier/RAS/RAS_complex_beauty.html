<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>RAS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ras---171">RAS - 171</h2>
<ul>
<li><details>
<summary>
(2020). Hybrid global positioning system-adaptive neuro-fuzzy
inference system based autonomous mobile robot navigation. <em>RAS</em>,
<em>134</em>, 103669. (<a
href="https://doi.org/10.1016/j.robot.2020.103669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The collision-free navigation of a mobile robot in clutter environments is challenging. Global Positioning System (GPS) and adaptive neuro-fuzzy inference system (ANFIS) are well-known techniques widely used for navigation and control, respectively. This paper proposes a hybrid GPS-ANFIS based method for collision-free navigation of autonomous mobile robots . The GPS-based controller keeps the navigation direction of the robot toward the static or dynamic target. It uses the coordinates received from the two GPS modules on the edges of the longitudinal axis of the robot all together with the coordinates of the target to divert it from the current path making a certain angle towards the target. The performance of the proposed method in navigating a mobile robot in clutter environments and its effectiveness in comparison with the other collision-free navigation methods has been evaluated through simulations. The evaluation criteria are on the basis of the obstacle avoidance behavior and the length of the discovered collision-free path by the robot. The results have shown that our hybrid GPS-ANFIS method navigates the robot toward the goal via a shorter path while avoiding the obstacles.},
  archive      = {J_RAS},
  author       = {Mohammad Samadi Gharajeh and Hossein B. Jond},
  doi          = {10.1016/j.robot.2020.103669},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103669},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Hybrid global positioning system-adaptive neuro-fuzzy inference system based autonomous mobile robot navigation},
  volume       = {134},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Assist-as-needed control of a hip exoskeleton based on a
novel strength index. <em>RAS</em>, <em>134</em>, 103667. (<a
href="https://doi.org/10.1016/j.robot.2020.103667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the challenging concept of Assist-As-Needed control of exoskeleton robots. The proposed controller boosts the voluntary participation of the patient by providing assistance according to the ability of the wearer in performing the assigned task. A novel strength index is presented that combines interaction force and position-tracking error into a single quantity to estimate the physical strength of the wearer during the therapy. The estimated strength is used to adjust the boundaries of a virtual tunnel around the desired trajectory , defined to assume a degree of freedom for the wearer’s motions and to compensate for asymmetric gait patterns . The required assistance is then defined by an adaptive impedance controller according to the distance of the tracking error from the tunnel boundaries. To ensure that the assistance is accurately supplied to the patient, an adaptive torque controller is integrated into the control loop. The adaptive torque controller uses a generalized fuzzy hyperbolic model to compensate for the inherent impedance of the exoskeleton. Simulation results on a hemiplegic model show that the proposed index can estimate the wearer’s strength properly and the proposed assist-as-needed controller can reduce the tracking error. The performance of the proposed method is also evaluated experimentally on a healthy subject wearing a hip exoskeleton. The results verify that the proposed method can be used in a variety of therapeutic applications where it is important to track the desired trajectory with minimum interventions.},
  archive      = {J_RAS},
  author       = {Naeim Naghavi and Alireza Akbarzadeh and S. Mohammad Tahamipour-Z. and Iman Kardan},
  doi          = {10.1016/j.robot.2020.103667},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103667},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Assist-as-needed control of a hip exoskeleton based on a novel strength index},
  volume       = {134},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Predicting human navigation goals based on bayesian
inference and activity regions. <em>RAS</em>, <em>134</em>, 103664. (<a
href="https://doi.org/10.1016/j.robot.2020.103664">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Anticipation of human movements is of great importance for service robots , as it is necessary to avoid interferences and predict areas where human–robot collaboration may be needed. In indoor scenarios , human movements often depend on objects with which they interacted before. For example, if a human interacts with a cup the probability that a table or coffee machine might be the next navigation goal is high. Typically, objects are grouped together in regions depending on the related activities so that environments consist of a set of activity regions. For example, a workspace region may contain a PC, a chair, and a table with many smaller objects on top of it. In this article, we present an approach to predict the navigation goal of a moving human in indoor environments. We hereby combine prior knowledge about typical human transitions between activity regions with robot observations about the human’s current pose and the last object interaction to predict the navigation goal using Bayesian inference. In the experimental evaluation in several simulated environments we demonstrate that our approach leads to a significantly more accurate prediction of the navigation goal in comparison to previous work. Furthermore, we show in a real-world experiment how such human motion anticipation can be used to realize foresighted navigation with an assistance robot, i.e. how predicted human movements can be used to increase the time efficiency of the robot’s navigation policy by early anticipating the user’s navigation goal and moving towards it.},
  archive      = {J_RAS},
  author       = {Lilli Bruckschen and Kira Bungert and Nils Dengler and Maren Bennewitz},
  doi          = {10.1016/j.robot.2020.103664},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103664},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Predicting human navigation goals based on bayesian inference and activity regions},
  volume       = {134},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive parallel reflex- and decoupled CPG-based control
for complex bipedal locomotion. <em>RAS</em>, <em>134</em>, 103663. (<a
href="https://doi.org/10.1016/j.robot.2020.103663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The achievement of adaptive, stable, and robust locomotion and dealing with asymmetrical conditions for bipedal robots remain a challenging problem. To address the problem, this paper introduces adaptive parallel reflex- and decoupled central pattern generator (CPG)-based control for a planar bipedal robot. The control has modular structure consisting of two parallel modules that work together. Firstly, as the main controller , the reflex-based control module inspired by an agonist–antagonist model, utilizes proprioceptive sensory feedback to adaptively generate various stable gaits. In parallel, as an auxiliary controller, the decoupled CPG-based control units individually governing the robot legs have the ability to learn the generated gaits in an online manner. Using the proposed framework, our study shows that this real-time control approach contributes to stable gait generation with robustness toward sensory feedback malfunction and adaptability to deal with environmental and morphological changes. Herein this study, we demonstrate the planar bipedal robot control functionality on a variable speed treadmill, dealing with asymmetric conditions such as weight imbalance and asymmetrical elastic resistance in the legs. However, the approach does not require robot kinematic and dynamic models as well as an environmental model and is therefore flexible. As such, it can be used as a basis for controlling other bipedal locomotion systems, like lower-limb exoskeletons.},
  archive      = {J_RAS},
  author       = {Chaicharn Akkawutvanich and Frederik Ibsgaard Knudsen and Anders Falk Riis and Jørgen Christian Larsen and Poramate Manoonpong},
  doi          = {10.1016/j.robot.2020.103663},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103663},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Adaptive parallel reflex- and decoupled CPG-based control for complex bipedal locomotion},
  volume       = {134},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An improved h-infinity unscented FastSLAM with adaptive
genetic resampling. <em>RAS</em>, <em>134</em>, 103661. (<a
href="https://doi.org/10.1016/j.robot.2020.103661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The FastSLAM is a typical tracking algorithm for SLAM , but it often suffers from the low tracking accuracy. To mitigate the problem, an improved H-Infinity unscented FastSLAM (IHUFastSLAM) with adaptive genetic resampling is proposed in this paper. Specifically, the H-Infinity unscented Kalman filter algorithm is improved using an adaptive factor and is employed as importance sampling in particle filter. Next, the process noise and the measurement noise are estimated by a time varying noise estimator. Moreover, an adaptive genetic algorithm is used to complete the resampling of particle filter. Finally, the improved H-Infinity UFastSLAM with adaptive genetic resampling is proposed to complete robot tracking. The proposed algorithm can track robot with good accuracy, and obtain reliable state estimation in SLAM . Simulation results reveal the validity of the proposed algorithm.},
  archive      = {J_RAS},
  author       = {Ming Tang and Zhe Chen and Fuliang Yin},
  doi          = {10.1016/j.robot.2020.103661},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103661},
  shortjournal = {Robot. Auton. Syst.},
  title        = {An improved H-infinity unscented FastSLAM with adaptive genetic resampling},
  volume       = {134},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The impact of adding perspective-taking to spatial
referencing during human–robot interaction. <em>RAS</em>, <em>134</em>,
103654. (<a href="https://doi.org/10.1016/j.robot.2020.103654">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For effective verbal communication in collaborative tasks, robots need to account for the different perspectives of their human partners when referring to objects in a shared space. For example, when a robot helps its partner find correct pieces while assembling furniture, it needs to understand how its collaborator perceives the world and refer to objects accordingly. In this work, we propose a method to endow robots with perspective-taking abilities while spatially referring to objects. To examine the impact of our proposed method, we report the results of a user study showing that when the objects are spatially described from the users’ perspectives, participants take less time to find the referred objects, find the correct objects more often and consider the task easier.},
  archive      = {J_RAS},
  author       = {Fethiye Irmak Doğan and Sarah Gillet and Elizabeth J. Carter and Iolanda Leite},
  doi          = {10.1016/j.robot.2020.103654},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103654},
  shortjournal = {Robot. Auton. Syst.},
  title        = {The impact of adding perspective-taking to spatial referencing during human–robot interaction},
  volume       = {134},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SkillMaN — a skill-based robotic manipulation framework
based on perception and reasoning. <em>RAS</em>, <em>134</em>, 103653.
(<a href="https://doi.org/10.1016/j.robot.2020.103653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the problems that service robotics deals with is to bring mobile manipulators to work in semi-structured human scenarios, which requires an efficient and flexible way to execute every-day tasks, like serve a cup in a cluttered environment. Usually, for those tasks, the combination of symbolic and geometric levels of planning is necessary, as well as the integration of perception models with knowledge to guide both planning levels, resulting in a sequence of actions or skills which, according to the current knowledge of the world, may be executed. This paper proposes a planning and execution framework, called SkillMaN, for robotic manipulation tasks, which is equipped with a module with experiential knowledge (learned from its experience or given by the user) on how to execute a set of skills, like pick-up, put-down or open a drawer, using workflows as well as robot trajectories. The framework also contains an execution assistant with geometric tools and reasoning capabilities to manage how to actually execute the sequence of motions to perform a manipulation task (which are forwarded to the executor module), as well as the capacity to store the relevant information to the experiential knowledge for further usage, and the capacity to interpret the actual perceived situation (in case the preconditions of an action do not hold) and to feed back the updated state to the planner to resume from there, allowing the robot to adapt to non-expected situations. To evaluate the viability of the proposed framework, an experiment has been proposed involving different skills performed with various types of objects in different scene contexts.},
  archive      = {J_RAS},
  author       = {Mohammed Diab and Mihai Pomarlan and Daniel Beßler and Aliakbar Akbari and Jan Rosell and John Bateman and Michael Beetz},
  doi          = {10.1016/j.robot.2020.103653},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103653},
  shortjournal = {Robot. Auton. Syst.},
  title        = {SkillMaN — a skill-based robotic manipulation framework based on perception and reasoning},
  volume       = {134},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-awareness in intelligent vehicles: Feature based
dynamic bayesian models for abnormality detection. <em>RAS</em>,
<em>134</em>, 103652. (<a
href="https://doi.org/10.1016/j.robot.2020.103652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolution of Intelligent Transportation Systems in recent times necessitates the development of self-awareness in agents. Before the intensive use of Machine Learning , the detection of abnormalities was manually programmed by checking every variable and creating huge nested conditions that are very difficult to track. This paper aims to introduce a novel method to develop self-awareness in autonomous vehicles that mainly focuses on detecting abnormal situations around the considered agents. Multi-sensory time-series data from the vehicles are used to develop the data-driven Dynamic Bayesian Network (DBN) models used for future state prediction and the detection of dynamic abnormalities. Moreover, an initial level collective awareness model that can perform joint anomaly detection in co-operative tasks is proposed. The GNG algorithm learns the DBN models’ discrete node variables; probabilistic transition links connect the node variables. A Markov Jump Particle Filter (MJPF) is applied to predict future states and detect when the vehicle is potentially misbehaving using learned DBNs as filter parameters. In this paper, datasets from real experiments of autonomous vehicles performing various tasks used to learn and test a set of switching DBN models.},
  archive      = {J_RAS},
  author       = {Divya Thekke Kanapram and Pablo Marin-Plaza and Lucio Marcenaro and David Martin and Arturo de la Escalera and Carlo Regazzoni},
  doi          = {10.1016/j.robot.2020.103652},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103652},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Self-awareness in intelligent vehicles: Feature based dynamic bayesian models for abnormality detection},
  volume       = {134},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling and experimental analysis of a multi-rod parallel
continuum robot using the cosserat theory. <em>RAS</em>, <em>134</em>,
103650. (<a href="https://doi.org/10.1016/j.robot.2020.103650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallel continuum robots get their compliance and compactness from continuum rods while also having the stability and strength of parallel robots . Their potential to provide multi-degree-of-freedom articulation gives them versatility and makes them very useful in various applications. The purpose of this paper is to model a six link parallel continuum robot using the Cosserat theory. The single rod model is initially derived and experimentally verified with an average error of 12\%. Then, the parallel continuum robot model is obtained by combining six elastic links, and eventually, some experiments are done on a real system to analyze the results of the obtained model.},
  archive      = {J_RAS},
  author       = {Morteza Ghafoori and Ali Keymasi Khalaji},
  doi          = {10.1016/j.robot.2020.103650},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103650},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Modeling and experimental analysis of a multi-rod parallel continuum robot using the cosserat theory},
  volume       = {134},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Map merging with terrain-adaptive density using mobile 3D
laser scanner. <em>RAS</em>, <em>134</em>, 103649. (<a
href="https://doi.org/10.1016/j.robot.2020.103649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building 3D maps of various terrains is a necessary approach to gain environmental information for mobile robots when they are exploring in unknown territories. In this paper, we propose a method to construct a point cloud map with laser-measured data as the robot moves around. A terrain-adaptive density mapping technique is used to balance the demands of small data size and high terrain accuracy by utilizing the local curvatures as the simplification criteria. The adaptive density mapping technique is further integrated within the map merging framework to improve the matching speed and accuracy. Indoor and outdoor experiments are proceeded, which verifies the effects of using terrain-adaptive density point cloud on controlling the map size and decreasing the map alignment error.},
  archive      = {J_RAS},
  author       = {Yangmin Xie and Yujie Tang and Rui Zhou and Yukun Guo and Hang Shi},
  doi          = {10.1016/j.robot.2020.103649},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103649},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Map merging with terrain-adaptive density using mobile 3D laser scanner},
  volume       = {134},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Shared mixed reality-bilateral telerobotic system.
<em>RAS</em>, <em>134</em>, 103648. (<a
href="https://doi.org/10.1016/j.robot.2020.103648">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study proposes a new shared mixed reality (MR)-bilateral telerobotic system. The main contribution of this study is to combine MR teleoperation and bilateral teleoperation , which takes advantage of the two types of teleoperation and compensates for each other’s drawbacks. With this combination, the proposed system can address the asymmetry issues in bilateral teleoperation, such as kinematic redundancy and workspace inequality, and provide force feedback , which is lacking in MR teleoperation. In addition, this system effectively supports long-distance movements and fine movements. In this system, a new MR interface is developed to provide the operator with an immersive visual feedback of the workspace, in which a useful virtual controller known as an interaction proxy—is designed. Compared with previous virtual reality-based teleoperation systems, this interaction proxy can freely decouple the operator from the control loop, such that the operational burden can be substantially alleviated. Additionally, the force feedback provided by the bilateral teleoperation gives the operator an advanced perception about the remote workspace and can improve task performance. Experiments on multiple pick-and-place tasks are provided to demonstrate the feasibility and effectiveness of the proposed system.},
  archive      = {J_RAS},
  author       = {Da Sun and Qianfang Liao and Andrey Kiselev and Todor Stoyanov and Amy Loutfi},
  doi          = {10.1016/j.robot.2020.103648},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103648},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Shared mixed reality-bilateral telerobotic system},
  volume       = {134},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simulation-based lidar super-resolution for ground vehicles.
<em>RAS</em>, <em>134</em>, 103647. (<a
href="https://doi.org/10.1016/j.robot.2020.103647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a methodology for lidar super-resolution with ground vehicles driving on roadways, which relies completely on a driving simulator to enhance, via deep learning , the apparent resolution of a physical lidar . To increase the resolution of the point cloud captured by a sparse 3D lidar, we convert this problem from 3D Euclidean space into an image super-resolution problem in 2D image space, which is solved using a deep convolutional neural network . By projecting a point cloud onto a range image, we are able to efficiently enhance the resolution of such an image using a deep neural network . Typically, the training of a deep neural network requires vast real-world data. Our approach does not require any real-world data, as we train the network purely using computer-generated data. Thus our method is applicable to the enhancement of any type of 3D lidar theoretically. By novelly applying Monte-Carlo dropout in the network and removing the predictions with high uncertainty, our method produces high accuracy point clouds comparable with the observations of a real high resolution lidar. We present experimental results applying our method to several simulated and real-world datasets. We argue for the method’s potential benefits in real-world robotics applications such as occupancy mapping and terrain modeling.},
  archive      = {J_RAS},
  author       = {Tixiao Shan and Jinkun Wang and Fanfei Chen and Paul Szenher and Brendan Englot},
  doi          = {10.1016/j.robot.2020.103647},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103647},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Simulation-based lidar super-resolution for ground vehicles},
  volume       = {134},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rapid path planning for dubins vehicles under environmental
currents. <em>RAS</em>, <em>134</em>, 103646. (<a
href="https://doi.org/10.1016/j.robot.2020.103646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a rapid (real time) solution to the minimum-time path planning problem for Dubins vehicles under environmental currents (wind or ocean currents). Real-time solutions are essential in time-critical situations (such as replanning under dynamically changing environments or tracking fast moving targets). Typically, Dubins problem requires to solve for six path types; however, due to the presence of currents, four of these path types require to solve the root-finding problem involving transcendental functions . Thus, the existing methods result in high computation times and their applicability for real-time applications is limited. In this regard, in order to obtain a real-time solution, this paper proposes a novel approach where only a subset of two Dubins path types ( L S L LSL and R S R RSR ) are used which have direct analytical solutions in the presence of currents. However, these two path types do not provide full reachability . We show that by extending the feasible range of circular arcs in the L S L LSL and R S R RSR path types from 2 π 2π to 4 π 4π : (1) full reachability of any goal pose is guaranteed, and (2) paths with lower time costs as compared to the corresponding 2 π 2π -arc paths can be produced. Theoretical properties are rigorously established, supported by several examples, and evaluated in comparison to the Dubins solutions by extensive Monte-Carlo simulations.},
  archive      = {J_RAS},
  author       = {Khushboo Mittal and Junnan Song and Shalabh Gupta and Thomas A. Wettergren},
  doi          = {10.1016/j.robot.2020.103646},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103646},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Rapid path planning for dubins vehicles under environmental currents},
  volume       = {134},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fuzzy controlled humanoid robots: A literature review.
<em>RAS</em>, <em>134</em>, 103643. (<a
href="https://doi.org/10.1016/j.robot.2020.103643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humanoid robots generated by inspiring by human appearances and abilities have became essential in human society to improve the quality of their life. All over the world, there have been many researchers who have focused on humanoid robots to develop the capabilities of humanoid robots. Generally, humanoid robot systems include mechanisms of decision making and information processing. Because of the uncertainty behind decision making and information processes, fuzzy sets are used most commonly. This study investigates a comprehensive literature review about humanoid robots that presents the recent technological developments and the theories associated with fuzzy set models. The basic principles and concepts of fuzzy sets for humanoid robots are presented.},
  archive      = {J_RAS},
  author       = {Cengiz Kahraman and Muhammet Deveci and Eda Boltürk and Seda Türk},
  doi          = {10.1016/j.robot.2020.103643},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103643},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Fuzzy controlled humanoid robots: A literature review},
  volume       = {134},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spatiotemporal chaotification of delta robot mixer for
homogeneous graphene nanocomposite dispersing. <em>RAS</em>,
<em>134</em>, 103633. (<a
href="https://doi.org/10.1016/j.robot.2020.103633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the design, implementation and polymer nanocomposite mixing application of a robust spatiotemporal chaotic delta robot. Blending fluids efficiently is a vital process for the preparation of graphene nanocomposite mixing. The most commonly used mixing materials are polymeric materials that need to be blended in non-Newtonian fluids. To achieve a superior blending performance over the conventional ones, it is used two different chaotification mechanisms for the realization of the spatiotemporal chaotic delta robot mixer system. One of them is for the chaotification of the mixer propeller while the second one is for the chaotification of the three-dimensional position of the endpoint of the delta robot. The model-based robust chaotification scheme based on sliding mode control is applied to chaotify the speed of the delta robot-mixer via dynamical state-feedback chaotification method. The chaotification of 3D position of the mixer is realized in a feedforward way by producing chaotic input signals. The implemented robust chaotic delta robot mixer exploits the efficacy of chaotic mixing in obtaining homogeneity in the mixture with less operation time, and hence reduced electrical energy consumption. In these performance evaluations, energy consumption and material characterization, which are measured by reliable material characterization methods such as X-ray diffraction, Fourier-transform-infrared spectroscopy, water contact angle, dynamical mechanical analysis, atomic force microscopy, Raman and field emission-scanning electron microscope analyses, are used as criteria. The obtained results show that, for the delta robot, the proposed chaotic-speed together with 3D chaotic-movement operation mode provides a better mixing performance than other mixing operation modes.},
  archive      = {J_RAS},
  author       = {Savas Sahin and Ali Emre Kavur and Sibel Demiroglu Mustafov and Ozgur Seydibeyoglu and Ozgun Baser and Yalcin Isler and Cuneyt Guzelis},
  doi          = {10.1016/j.robot.2020.103633},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103633},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Spatiotemporal chaotification of delta robot mixer for homogeneous graphene nanocomposite dispersing},
  volume       = {134},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An iterative optimization approach for multi-robot pattern
formation in obstacle environment. <em>RAS</em>, <em>133</em>, 103645.
(<a href="https://doi.org/10.1016/j.robot.2020.103645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pattern formation for multi-robot systems has received increasing attention in different scenarios. However, existing methods cannot efficiently optimize pattern formation in the obstacle environment. To address this limitation, this paper proposes a new planning method that assigns the optimal goals to the robots and iteratively computes collision-free paths to reach goal positions. Firstly, according to the random initial position of the group robot and the arbitrary shape, convex quadratic programming is used to minimize the distance to obtain the optimal pattern parameters under certain constraints. Secondly, the iterative controller plans the collision-free path of each robot to the goal considering a preferred velocity. Simulation results verified the effectiveness of the proposed methodology for scenarios of letter formation, in comparison to a commonly-used method.},
  archive      = {J_RAS},
  author       = {Fangfang Zhang and Tingting Wang and Qiyan Li and Jianbin Xin},
  doi          = {10.1016/j.robot.2020.103645},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103645},
  shortjournal = {Robot. Auton. Syst.},
  title        = {An iterative optimization approach for multi-robot pattern formation in obstacle environment},
  volume       = {133},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Surveillance planning with safe emergency landing guarantee
for fixed-wing aircraft. <em>RAS</em>, <em>133</em>, 103644. (<a
href="https://doi.org/10.1016/j.robot.2020.103644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study Emergency Landing Aware Surveillance Planning (ELASP) to determine a cost-efficient trajectory to visit a given set of target locations such that a safe emergency landing is possible at any point of the multi-goal trajectory. The problem is motivated to guarantee a safe mission plan in a case of loss of thrust for which it is desirable to have a safe gliding trajectory to a nearby airport . The problem combines computational challenges of the combinatorial multi-goal planning with demanding motion planning to determine safe landing trajectories for the curvature-constrained aerial vehicle. The crucial property of safe landing is a minimum safe altitude of the vehicle that can be found by trajectory planning to nearby airports using sampling-based motion planning such as RRT*. A trajectory is considered safe if the vehicle is at least at the minimum safe altitude at any point of the trajectory. Thus, a huge number of samples have to be evaluated to guarantee the safety of the trajectory, and an evaluation of all possible multi-goal trajectories is quickly computationally intractable. Therefore, we propose to utilize a roadmap of safe altitudes combined with the estimation of the trajectory lengths to evaluate only the most promising candidate trajectories. Based on the reported results, the proposed approach significantly reduces the computational burden and enables a solution of ELASP instances with tens of locations in units of minutes using standard single-core computational resources.},
  archive      = {J_RAS},
  author       = {Petr Váňa and Jakub Sláma and Jan Faigl},
  doi          = {10.1016/j.robot.2020.103644},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103644},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Surveillance planning with safe emergency landing guarantee for fixed-wing aircraft},
  volume       = {133},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fuzzy logic compliance adaptation for an assist-as-needed
controller on the gait rehabilitation exoskeleton (GAREX). <em>RAS</em>,
<em>133</em>, 103642. (<a
href="https://doi.org/10.1016/j.robot.2020.103642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Assist-as-needed control strategy is an emerging approach to improve the effectiveness of gait rehabilitation training. We have proposed a pneumatic muscle (PM) driven Gait Rehabilitation Exoskeleton (GAREX) implemented with a multi-input–multi-output (MIMO) sliding mode control system to actively adjust the assistance level provided during gait rehabilitation. To realize the assist-as-needed control strategy, a specific algorithm is imperative to assess the active participation or effort of wearers and adapt the amount of assistance accordingly. We sought to establish a fuzzy logic compliance adaptation (FLCA) controller to form a novel cascade control system. We evaluated the feasibility of implemented FLCA controller on the performance of adjusting the compliance of GAREX’s knee joint according to the online assessment of the wear’s active participation level once in every gait cycle . Using controlled, treadmill-based walking tests involved three healthy subjects, we demonstrate that FLCA controller could effectively distinguish the capability/effort levels of wearers and enable the exoskeleton to adapt the knee joint compliance accordingly. Obtained results reveal that FLCA controller can collaborate well with MIMO sliding mode controller in a system and indicate the novel method of realizing assist-as-needed concept with the pneumatic muscle powered mechanisms.},
  archive      = {J_RAS},
  author       = {Bin Zhong and Jinghui Cao and Kaiqi Guo and Andrew McDaid and Yuxin Peng and Qing Miao and ShengQ. Xie and Mingming Zhang},
  doi          = {10.1016/j.robot.2020.103642},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103642},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Fuzzy logic compliance adaptation for an assist-as-needed controller on the gait rehabilitation exoskeleton (GAREX)},
  volume       = {133},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Feasibility analysis of using the hp-adaptive radau
pseudospectral method for minimum-effort collision-free docking
operations of AUV. <em>RAS</em>, <em>133</em>, 103641. (<a
href="https://doi.org/10.1016/j.robot.2020.103641">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper continues the previous effort on the development of a trajectory generation platform that assures minimum-control expenditure and collision-free manoeuvre of a torpedo-shaped autonomous underwater vehicle (AUV) into a funnel-shaped stationary docking station (DS). The earlier-developed guidance system was based on the Inverse Dynamics in the Virtual Domain (IDVD) method accounting for AUV’s dynamics and producing a smooth trackable trajectory, thus guaranteeing safe arrival to DS. The optimality of the real-time generated solutions has been assessed via comparing them with the Legendre–Gauss–Lobatto pseudo-spectral (PS) method solutions that could only be obtained off-line. This paper explores a possibility of employing a more advanced hp-adaptive Radau (hp-AR) PS method for the same Hamiltonian two-point boundary-value problem. The considered approach explicitly encapsulates all realistic vehicular and environmental constraints such as the AUV’s dynamics, ocean current disturbances, no-fly zones, and DS pose while minimizing the vehicle’s controls expenditure and permitting precise manoeuvring into DS. The performance evaluation of the hp-AR PS based optimization routine is carried out through extensive software-in-the-loop simulations. For completeness, computational effectiveness and solution optimality of the trajectory generator engine based on the hp-AR method is compared with two other well-known PS methods based on Legendre and Chebyshev polynomial approximation . The results of this study show the superb performance of the hp-AR method-based trajectory generator among all other PS methods and a possibility of using it along with IDVD in the real-time implementation.},
  archive      = {J_RAS},
  author       = {A.M. Yazdani and K. Sammut and O.A. Yakimenko and A. Lammas},
  doi          = {10.1016/j.robot.2020.103641},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103641},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Feasibility analysis of using the hp-adaptive radau pseudospectral method for minimum-effort collision-free docking operations of AUV},
  volume       = {133},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Object-RPE: Dense 3D reconstruction and pose estimation with
convolutional neural networks. <em>RAS</em>, <em>133</em>, 103632. (<a
href="https://doi.org/10.1016/j.robot.2020.103632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present an approach for recognizing objects present in a scene and estimating their full pose by means of an accurate 3D instance-aware semantic reconstruction. Our framework couples convolutional neural networks (CNNs) and a state-of-the-art dense Simultaneous Localization and Mapping (SLAM) system, ElasticFusion (Whelan et al., 2016), to achieve both high-quality semantic reconstruction as well as robust 6D pose estimation for relevant objects. We leverage the pipeline of ElasticFusion as a backbone, and propose a joint geometric and photometric error function with per-pixel adaptive weights. While the main trend in CNN-based 6D pose estimation has been to infer object’s position and orientation from single views of the scene, our approach explores performing pose estimation from multiple viewpoints, under the conjecture that combining multiple predictions can improve the robustness of an object detection system. The resulting system is capable of producing high-quality instance-aware semantic reconstructions of room-sized environments, as well as accurately detecting objects and their 6D poses. The developed method has been verified through extensive experiments on different datasets. Experimental results confirmed that the proposed system achieves improvements over state-of-the-art methods in terms of surface reconstruction and object pose prediction. Our code and video are available at https://sites.google.com/view/object-rpe .},
  archive      = {J_RAS},
  author       = {Dinh-Cuong Hoang and Achim J. Lilienthal and Todor Stoyanov},
  doi          = {10.1016/j.robot.2020.103632},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103632},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Object-RPE: Dense 3D reconstruction and pose estimation with convolutional neural networks},
  volume       = {133},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). C-nav: Distributed coordination in crowded multi-agent
navigation. <em>RAS</em>, <em>133</em>, 103631. (<a
href="https://doi.org/10.1016/j.robot.2020.103631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In crowded multi-agent navigation, the motion of the agents is significantly constrained by the motion of the nearby agents. This makes planning paths very difficult and leads to inefficient global motion. To address this problem, we propose a distributed approach, which we call C-Nav , that introduces politeness into multi agent navigation. With our approach, agents take into account the velocities and goals of their neighbors and optimize their motion accordingly and in real-time. Further, we perform a theoretical analysis of the algorithm, and experimentally demonstrate its advantages in simulation, with hundreds of agents in a variety of scenarios, and in real world navigation tasks with several mobile robots.},
  archive      = {J_RAS},
  author       = {Julio Godoy and Stephen J. Guy and Maria Gini and Ioannis Karamouzas},
  doi          = {10.1016/j.robot.2020.103631},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103631},
  shortjournal = {Robot. Auton. Syst.},
  title        = {C-nav: Distributed coordination in crowded multi-agent navigation},
  volume       = {133},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving robot dual-system motor learning with
intrinsically motivated meta-control and latent-space experience
imagination. <em>RAS</em>, <em>133</em>, 103630. (<a
href="https://doi.org/10.1016/j.robot.2020.103630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Combining model-based and model-free learning systems has been shown to improve the sample efficiency of learning to perform complex robotic tasks. However, dual-system approaches fail to consider the reliability of the learned model when it is applied to make multiple-step predictions, resulting in a compounding of prediction errors and performance degradation . In this paper, we present a novel dual-system motor learning approach where a meta-controller arbitrates online between model-based and model-free decisions based on an estimate of the local reliability of the learned model. The reliability estimate is used in computing an intrinsic feedback signal, encouraging actions that lead to data that improves the model. Our approach also integrates arbitration with imagination where a learned latent-space model generates imagined experiences, based on its local reliability, to be used as additional training data. We evaluate our approach against baseline and state-of-the-art methods on learning vision-based robotic grasping in simulation and real world. The results show that our approach outperforms the compared methods and learns near-optimal grasping policies in dense- and sparse-reward environments.},
  archive      = {J_RAS},
  author       = {Muhammad Burhan Hafez and Cornelius Weber and Matthias Kerzel and Stefan Wermter},
  doi          = {10.1016/j.robot.2020.103630},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103630},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Improving robot dual-system motor learning with intrinsically motivated meta-control and latent-space experience imagination},
  volume       = {133},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Battery charge scheduling in long-life autonomous mobile
robots via multi-objective decision making under uncertainty.
<em>RAS</em>, <em>133</em>, 103629. (<a
href="https://doi.org/10.1016/j.robot.2020.103629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The daily working hours of mobile robots are limited primarily by battery life. Most systems use a combination of thresholds and fixed periods to decide when to charge. This produces charging behaviour that ignores high-value tasks that must be performed within time-windows or by deadlines. Instead the robot should schedule charging adaptively, taking into account the times of day when it is expected to be given more valuable tasks to perform. This paper proposes an approach that exploits the fact that, during long-term deployments, the robot can learn when it is most probable that valuable tasks are added to the system, enabling it to schedule charging at times that are expected to be less busy. We pose the problem of scheduling battery charging as a multi-objective sequential decision making problem over a time-dependent Markov decision process model of expected task rewards and battery dynamics. We evaluate the scalability and solution quality of our multi-objective scheduler, and compare it with a typical rule-based approach. Empirical results show that our approach enables more flexible and efficient robot behaviour, which takes into account both the value of current available tasks and the predicted value of future tasks to decide whether to charge at a given time.},
  archive      = {J_RAS},
  author       = {Milan Tomy and Bruno Lacerda and Nick Hawes and Jeremy L. Wyatt},
  doi          = {10.1016/j.robot.2020.103629},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103629},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Battery charge scheduling in long-life autonomous mobile robots via multi-objective decision making under uncertainty},
  volume       = {133},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A review of terrain detection systems for applications in
locomotion assistance. <em>RAS</em>, <em>133</em>, 103628. (<a
href="https://doi.org/10.1016/j.robot.2020.103628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Terrain detection systems have been developed for a large body of applications. For instance, a bionic leg prosthesis would have to adapt its behavior as a function of the terrain, in order to restore a sound lower-limb biomechanics to the amputee. Visually impaired people benefit from such systems in order to collect information about their locomotion environment and avoid obstacles. Finally, mobile robots use them for estimating terrain traversability, and adjusting control algorithms as a function of the surface type. This diversity of applications led to a large repertoire of systems, regarding both hardware (sensors, processing unit) and software used for classification. This paper provides an extended review of these systems, with a specific focus on the assistance of disabled walker. More precisely, it overviews the sensory systems and algorithms that were implemented to identify different locomotion terrains in indoor or urban environments (flat ground, stairs, slopes) in a way that they are or can be worn by a human user, and running in real-time. Contributions from mobile robotics are also included, pending that they could be adapted to a scenario of locomotion assistance. The systems are classified into two categories: these relying on proprioceptive sensors only and those further using exteroceptive sensors. Contributions from both categories are then compared according to their main specifications, such as accuracy and prediction time. The paper unambiguously shows that systems with exteroceptive sensors have higher prediction capability than systems with proprioceptive sensors only, and should thus be favored for assistive devices requiring predicting transitions between locomotion tasks.},
  archive      = {J_RAS},
  author       = {Ali H.A. Al-dabbagh and Renaud Ronsse},
  doi          = {10.1016/j.robot.2020.103628},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103628},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A review of terrain detection systems for applications in locomotion assistance},
  volume       = {133},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FUHAR: A transformable wheel-legged hybrid mobile robot.
<em>RAS</em>, <em>133</em>, 103627. (<a
href="https://doi.org/10.1016/j.robot.2020.103627">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a mobile robot with a new type of transformable wheel legs that can be used for flat and rough terrain. It integrates the stability and maneuverability of a wheeled robot and the legged robot’s obstacle climbing capacity using a transformable mechanism with wheel legs. With a transformation structure based on a four-bar mechanism, these two modes can be easily changed. This paper analyzes the movements for the proposed robot in wheeled and legged mode. Dynamic modeling and design of a control system were obtained. Then, the obstacle climbing strategies under legged modes were carried out. Finally, on the basis of the simulation, a prototype of the proposed robot was designed and produced. The results from the experiments validate the efficiency of the designed hybrid mobile robot.},
  archive      = {J_RAS},
  author       = {İrem Mertyüz and Alper K. Tanyıldızı and Beyda Taşar and Ahmet B. Tatar and Oğuz Yakut},
  doi          = {10.1016/j.robot.2020.103627},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103627},
  shortjournal = {Robot. Auton. Syst.},
  title        = {FUHAR: A transformable wheel-legged hybrid mobile robot},
  volume       = {133},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visual–spatial attention as a comfort measure in human–robot
collaborative tasks. <em>RAS</em>, <em>133</em>, 103626. (<a
href="https://doi.org/10.1016/j.robot.2020.103626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new formulation to consider visual–spatial attention in order to improve the comfort of a human standing or operating near a collaborative robot. This formulation is based on the principle of having the robot’s end-effector in the human visual–spatial attention as much as possible. The integration of this new constraint into the Inverse Kinematics (IK) problem is thoroughly studied and efficient solutions are proposed. Moreover, to allow the robot to react rapidly in the case of unforeseen events, adding the manipulability index to the IK problem is also studied and its impact is analyzed. The proposed method is then extensively tested in simulation and verified on the real Baxter research robot, these experiments pointed out the method efficiency to improve the task visibility while avoiding self-occlusion and singular configurations . Moreover, real experiments revealed the method robustness to uncertainties such as imprecision of detecting human gaze direction.},
  archive      = {J_RAS},
  author       = {Kévin Dufour and Jorge Ocampo-Jimenez and Wael Suleiman},
  doi          = {10.1016/j.robot.2020.103626},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103626},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Visual–spatial attention as a comfort measure in human–robot collaborative tasks},
  volume       = {133},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visual place recognition by spatial matching of high-level
CNN features. <em>RAS</em>, <em>133</em>, 103625. (<a
href="https://doi.org/10.1016/j.robot.2020.103625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a Visual Place Recognition (VPR) pipeline that achieves substantially improved precision as compared with approaches commonly appearing in the literature. It is based on a standard image retrieval configuration, with an initial stage that retrieves the closest candidates to a query from a database and a second stage where the list of candidates is re-ranked. The latter is realized by the introduction of a novel geometric verification procedure that uses the activations of a pre-trained convolutional neural network . It is both remarkably simple and robust to viewpoint and condition changes. As a stand-alone, general spatial matching methodology, it could be easily added and used to enhance existing VPR approaches whose output is a ranked list of candidates. The proposed two-stage pipeline is also improved through extensive optimization of hyperparameters and by the implementation of a frame-based temporal filter that takes into account past recognition results.},
  archive      = {J_RAS},
  author       = {Luis G. Camara and Libor Přeučil},
  doi          = {10.1016/j.robot.2020.103625},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103625},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Visual place recognition by spatial matching of high-level CNN features},
  volume       = {133},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Associated reality: A cognitive human–machine layer for
autonomous driving. <em>RAS</em>, <em>133</em>, 103624. (<a
href="https://doi.org/10.1016/j.robot.2020.103624">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advanced Driver Assistance Systems (ADAS) and Automated and Autonomous Vehicles (AV) are cooperative systems and processes that use: artificial intelligence , cognitive methods, cloud technologies, cooperative vehicle-to-everything-communications (V2X), software–hardwareplatforms, sensor platforms and incipient intelligent transport infrastructures, to get self-driving systems and smart connected mobility services. This paper, to support automated driving systems (assisted, semi-autonomous and fully autonomous vehicles), introduces a cognitive layer called Associated Reality to enhance the involved information, knowledge and communication processes. The architecture defined includes an augmented Local Dynamic Map, with complementary layers, and an augmented Graph Database , with complementary semantic–cognitive relations, for the considered purpose, in cooperative human–machine and machine–machine systems. Virtual augmented landmarks are defined to improve the connectivity and intelligence of the involved spatial-information systems. Different structure landmarks and sequence landmarks (which includes regular, repetitive and periodic landmarks) are defined, categorized and used in diverse visual localization and mapping scenarios, for autonomous driving . In this paper, it is also shown, as a proof-of-concept for vehicle localization and mapping in road tunnels , the visual detection of different sequences of periodic luminaires , using YOLO v3 for the corresponding LED lights detection, or a specific alternative procedure developed with very low computational cost.},
  archive      = {J_RAS},
  author       = {Felipe Fernandez and Angel Sanchez and Jose F. Velez and Belen Moreno},
  doi          = {10.1016/j.robot.2020.103624},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103624},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Associated reality: A cognitive Human–Machine layer for autonomous driving},
  volume       = {133},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint semantic segmentation of road objects and lanes using
convolutional neural networks. <em>RAS</em>, <em>133</em>, 103623. (<a
href="https://doi.org/10.1016/j.robot.2020.103623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a multi-task instance segmentation neural network able to provide both road lane and road participants detection. The multi-task approach, ERFNet-based, allows feature sharing and reduces the computational requirements of the overall detection architecture, allowing real time performance even in configurations with limited hardware. The proposed method includes an ad-hoc training procedure and automatic dataset creation mechanism that is also introduced in this paper. The proposed solution has been tested and validated through a newly generated public dataset derived from the BDD100K of 19K images, and in real scenarios. The results obtained prove the viability of the work for road application and its real time performance.},
  archive      = {J_RAS},
  author       = {Leonardo Cabrera Lo Bianco and Jorge Beltrán and Gerardo Fernández López and Fernando García and Abdulla Al-Kaff},
  doi          = {10.1016/j.robot.2020.103623},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103623},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Joint semantic segmentation of road objects and lanes using convolutional neural networks},
  volume       = {133},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robot exploration of indoor environments using incomplete
and inaccurate prior knowledge. <em>RAS</em>, <em>133</em>, 103622. (<a
href="https://doi.org/10.1016/j.robot.2020.103622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploration is a task in which autonomous mobile robots incrementally discover features of interest in initially unknown environments. We consider the problem of exploration for map building, in which a robot explores an indoor environment in order to build a metric map. Most of the current exploration strategies used to select the next best locations to visit ignore prior knowledge about the environments to explore that, in some practical cases, could be available. In this paper, we present an exploration strategy that evaluates the amount of new areas that can be perceived from a location according to a priori knowledge about the structure of the indoor environment being explored, like the floor plan or the contour of external walls. Although this knowledge can be incomplete and inaccurate (e.g., a floor plan typically does not represent furniture and objects and consequently may not fully mirror the structure of the real environment), we experimentally show, both in simulation and with real robots, that employing prior knowledge improves the exploration performance in a wide range of settings.},
  archive      = {J_RAS},
  author       = {Matteo Luperto and Michele Antonazzi and Francesco Amigoni and N. Alberto Borghese},
  doi          = {10.1016/j.robot.2020.103622},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103622},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Robot exploration of indoor environments using incomplete and inaccurate prior knowledge},
  volume       = {133},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Autonomous drone race: A computationally efficient
vision-based navigation and control strategy. <em>RAS</em>,
<em>133</em>, 103621. (<a
href="https://doi.org/10.1016/j.robot.2020.103621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drone racing is becoming a popular sport where human pilots have to control their drones to fly at high speed through complex environments and pass a number of gates in a pre-defined sequence. In this paper, we develop an autonomous system for drones to race fully autonomously using only onboard resources. Instead of commonly used visual navigation methods , such as simultaneous localization and mapping and visual inertial odometry , which are computationally expensive for micro aerial vehicles (MAVs), we developed the highly efficient snake gate detection algorithm for visual navigation, which can detect the gate at 20 HZ on a Parrot Bebop drone. Then, with the gate detection result, we developed a robust pose estimation algorithm which has better tolerance to detection noise than a state-of-the-art perspective-n-point method. During the race, sometimes the gates are not in the drone’s field of view. For this case, a state prediction-based feed-forward control strategy is developed to steer the drone to fly to the next gate. Experiments show that the drone can fly a half-circle with 1.5 m radius within 2 s with only 30 c m 30cm error at the end of the circle without any position feedback. Finally, the whole system is tested in a complex environment (a showroom in the faculty of Aerospace Engineering , TU Delft). The result shows that the drone can complete the track of 15 gates with a speed of 1 . 5 m ∕ s 1.5m∕s which is faster than the speeds exhibited at the 2016 and 2017 IROS autonomous drone races.},
  archive      = {J_RAS},
  author       = {Shuo Li and Michaël M.O.I. Ozo and Christophe De Wagter and Guido C.H.E. de Croon},
  doi          = {10.1016/j.robot.2020.103621},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103621},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Autonomous drone race: A computationally efficient vision-based navigation and control strategy},
  volume       = {133},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Three level sequence-based loop closure detection.
<em>RAS</em>, <em>133</em>, 103620. (<a
href="https://doi.org/10.1016/j.robot.2020.103620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recognition of previously visited places, known as Loop Closure Detection (LCD), composes one of the problems widely studied in robotics: simultaneous localization and mapping (SLAM). In this paper we propose a three level hierarchy based LCD method. In our serialized approach, in the First Level, a sequence of the most recently visited places is used as query to search for candidate sequences in our topological map composed by previously visited places. After that, at the Second Level, the method selects the most similar sequence to the query among all candidate sequences which is temporally consistent with the previous LCD method response. Then, at the Third Level, we match the image sequences belonging to the query sequence to the candidate sequence selected in the Second Level. The method is evaluated in different and challenging public datasets, and presents expressive results that overcome the LCD state-of-the-art methods.},
  archive      = {J_RAS},
  author       = {Fernanda Rodrigues and Renata Neuland and Mathias Mantelli and Diego Pittol and Renan Maffei and Edson Prestes and Mariana Kolberg},
  doi          = {10.1016/j.robot.2020.103620},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103620},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Three level sequence-based loop closure detection},
  volume       = {133},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cross-entropy based stochastic optimization of robot
trajectories using heteroscedastic continuous-time gaussian processes.
<em>RAS</em>, <em>133</em>, 103618. (<a
href="https://doi.org/10.1016/j.robot.2020.103618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High dimensional robot motion planning has recently been approached with trajectory optimization methods that efficiently minimize a suitable objective function in order to generate robot trajectories that are both optimal and feasible. However, finding a globally optimal solution is often an insurmountable problem in practice and state-of-the-art trajectory optimization methods are thus prone to local minima, mainly in cluttered environments. In this paper, we propose a novel trajectory planning algorithm that employs stochastic optimization in order to find a collision-free trajectory generated from a continuous-time Gaussian process (GP). The contributions of the proposed motion planning method stem from introducing the heteroscedasticity of the GP, together with exploited sparsity for efficient covariance estimation, and a cross-entropy based stochastic optimization for importance sampling based trajectory optimization. We evaluate the proposed method on three simulated scenarios: a maze benchmark, a 7 DOF robot arm planning benchmark and a 10 DOF mobile manipulator trajectory planning example and compare it to a state-of-the-art GP trajectory optimization method, namely the Gaussian process motion planner 2 algorithm (GPMP2). Our results demonstrate the following: (i) the proposed method yields a more thorough exploration of the solution space in complex environments than GPMP2, while having comparable execution time, (ii) the introduced heteroscedasticity generates GP priors better suited for collision avoidance and (iii) the proposed method has the ability to efficiently tackle high-dimensional trajectory planning problems.},
  archive      = {J_RAS},
  author       = {Luka Petrović and Juraj Peršić and Marija Seder and Ivan Marković},
  doi          = {10.1016/j.robot.2020.103618},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103618},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Cross-entropy based stochastic optimization of robot trajectories using heteroscedastic continuous-time gaussian processes},
  volume       = {133},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Geometric and constrained control for a string of tethered
drones. <em>RAS</em>, <em>133</em>, 103609. (<a
href="https://doi.org/10.1016/j.robot.2020.103609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we present a novel concept of a multi tethered drone system. The system includes an arbitrary number of drones connected serially to an active ground station. The considered drones are of quadrotor type. Utilizing a unique pulley–gimbal mechanism, each drone can freely move along the tether, and its state is measured with respect to the ground station without the use of standard onboard inertial sensors or GPS. The proposed system can be thought of as a robotic arm where each tether section acts as a variable-length link and each drone is a joint actuator. We model the coupled behavior of the ground station and the string, taking into account an arbitrary number of drones. Then, a controller that combines tools from geometric-control and Model Predictive Control is suggested. The developed model and control approach are also applicable for other swarm applications where the position of agents is to be controlled to a string-like form. Finally, the concept is demonstrated using numerical simulations and an initial experiment, which illustrate its potential effectiveness.},
  archive      = {J_RAS},
  author       = {Benny Kosarnovsky and Shai Arogeti},
  doi          = {10.1016/j.robot.2020.103609},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103609},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Geometric and constrained control for a string of tethered drones},
  volume       = {133},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Road detection based on simultaneous deep learning
approaches. <em>RAS</em>, <em>133</em>, 103605. (<a
href="https://doi.org/10.1016/j.robot.2020.103605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most important challenges for Autonomous Driving and Driving Assistance systems is the detection of the road to perform or monitor navigation. Many works can be found in the literature to perform road and lane detection, using both algorithmic processing and learning based techniques. However, no single solution is mentioned to be applicable in any circumstance of mixed scenarios of structured, unstructured, lane based, line based or curb based limits, and other sorts of boundaries. So, one way to embrace this challenge is to have multiple techniques, each specialized on a different approach, and combine them to obtain the best solution from individual contributions. That is the central concern of this paper. By improving a previously developed architecture to combine multiple data sources, a solution is proposed to merge the outputs of two Deep Learning based techniques for road detection . A new representation for the road is proposed along with a workflow of procedures for the combination of two simultaneous Deep Learning models, based on two adaptations of the ENet model. The results show that the overall solution copes with the alternate failures or under-performances of each model, producing a road detection result that is more reliable than the one given by each approach individually.},
  archive      = {J_RAS},
  author       = {Tiago Almeida and Bernardo Lourenço and Vitor Santos},
  doi          = {10.1016/j.robot.2020.103605},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103605},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Road detection based on simultaneous deep learning approaches},
  volume       = {133},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comparative study of self tuning, adaptive and multiplexing
FTC strategies for successive failures in an octorotor UAV.
<em>RAS</em>, <em>133</em>, 103602. (<a
href="https://doi.org/10.1016/j.robot.2020.103602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents three fault-tolerant control (FTC) strategies for a coaxial octorotor unmanned aerial vehicle (UAV) regarding motor failures. The first FTC is based on a control mixing strategy which consists of a set of control laws designed offline, each one dedicated to a specific fault situation. The second FTC, a robust adaptive sliding mode control allocation is presented, where the control gains of the controller are adjusted online in order to redistribute the control signals among the healthy motors in order to stabilize the overall system. The third FTC strategy is a new strategy proposed in this article, which is based on a self-tuning sliding mode control (STSMC) where the control gains are readjusted based on the detected error to maintain the stability of the system. Multiple indoor experiments on an octorotor UAV are conducted to show and compare the effectiveness and the behavior of each FTC scheme after successive faults are injected. More specifically, we inject complete actuator’s failures into the top four motors of our octorotor. Every strategies show good fault tolerance results, although the control mixing method performs slightly better overall while the adaptive method performs slightly worse. However, the control mixing method requires a huge design effort to take into account as much situations as possible, while the adaptive method and the STSMC only require to determine a few gains. The adaptive method do not need fault detection to operate, but it thus does not provide information on the system’s health without an additional fault identification and diagnosis mechanism, while both the control mixing method and the STSMC provide such information.},
  archive      = {J_RAS},
  author       = {Hussein Hamadi and Benjamin Lussier and Isabelle Fantoni and Clovis Francis and Hassan Shraim},
  doi          = {10.1016/j.robot.2020.103602},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103602},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Comparative study of self tuning, adaptive and multiplexing FTC strategies for successive failures in an octorotor UAV},
  volume       = {133},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel odor source localization system based on particle
filtering and information entropy. <em>RAS</em>, <em>132</em>, 103619.
(<a href="https://doi.org/10.1016/j.robot.2020.103619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {So far, gas leakage caused by natural or human factors has led to serious consequences in terms of social security. Previous strategies for locating the odor sources appear to be either defective or incomplete. For enhancing the success rate and rapidity, this paper aims to present a novel and complete strategy in search of lurking gas sources. Particle filtering and information entropy are both employed to track the plume information. To improve the tracking efficiency in this process, a novel objective function is designed by considering the entropy gains of the suspected targets as well as the repeated exploration scores. Considering the pseudo sourced caused by obstacles, a statistics-based source determine algorithm is proposed to confirm the source’s authenticity, while the artificial potential field method is subsequently applied to eliminate the distractions introduced by the pseudo sources. Simulations and on-site tests are both carried out while results showed that the proposed scheme is competent to complete sources localization task in the scene that contains randomly distributed obstacles and pseudo source.},
  archive      = {J_RAS},
  author       = {Hongbiao Zhu and Yibo Wang and Chengjin Du and Quan Zhang and Weidong Wang},
  doi          = {10.1016/j.robot.2020.103619},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103619},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A novel odor source localization system based on particle filtering and information entropy},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FinnForest dataset: A forest landscape for visual SLAM.
<em>RAS</em>, <em>132</em>, 103610. (<a
href="https://doi.org/10.1016/j.robot.2020.103610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel challenging dataset that offers a new landscape of testing material for mobile robotics, autonomous driving research, and forestry operation. In contrast to common urban structures, we explore an unregulated natural environment to exemplify sub-urban and forest environment. The sequences provide two-natured data where each place is visited in summer and winter conditions. The vehicle used for recording is equipped with a sensor rig that constitutes four RGB cameras, an Inertial Measurement Unit , and a Global Navigation Satellite System receiver. The sensors are synchronized based on non-drifting timestamps. The dataset provides trajectories of varying complexity both for the state of the art visual odometry approaches and visual simultaneous localization and mapping algorithms. The full dataset and toolkits are available for download at: http://urn.fi/urn:nbn:fi:att:9b8157a7-1e0f-47c2-bd4e-a19a7e952c0d . As an alternative, you can browse for the dataset using the article title at: http://etsin.fairdata.fi .},
  archive      = {J_RAS},
  author       = {Ihtisham Ali and Ahmed Durmush and Olli Suominen and Jari Yli-Hietanen and Sari Peltonen and Jussi Collin and Atanas Gotchev},
  doi          = {10.1016/j.robot.2020.103610},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103610},
  shortjournal = {Robot. Auton. Syst.},
  title        = {FinnForest dataset: A forest landscape for visual SLAM},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Range-based target localization and pursuit with autonomous
vehicles: An approach using posterior CRLB and model predictive control.
<em>RAS</em>, <em>132</em>, 103608. (<a
href="https://doi.org/10.1016/j.robot.2020.103608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the general problem of multiple target localization and pursuit using measurements of the ranges from the targets to a set of autonomous pursuing vehicles, referred to as trackers. We develop a general framework for targets with models exhibiting uncertainty in the initial state, process, and measurement noise. The main objective is to compute optimal motions for the trackers that maximize the range-based information available for target localization and at the same time yield good target pursuit performance. The solution proposed is rooted in an estimation-theoretical setting that involves the computation of an appropriately defined Bayesian Fisher Information Matrix (FIM). The inverse of the latter yields a posterior Cramér–Rao Lower Bound (CRLB) on the covariance of the targets’ state estimation errors that can be possibly achieved with any estimator. Using the FIM, sufficient conditions on the trackers’ motions are derived for the ideal relative geometry between the trackers and the targets for which the range information acquired is maximal. This allows for an intuitive understanding of the types of ideal tracker trajectories. To deal with realistic constraints on the trackers’ motions and the requirement that the trackers pursue the targets, we then propose a model predictive control (MPC) framework for optimal tracker motion generation with a view to maximizing the predicted range information for target localization while taking explicitly into account the trackers’ dynamics, strict constraints on the trackers’ states and inputs, and prior knowledge about the targets’ states. The efficacy of the MPC is assessed in simulation through the help of representative examples motivated by operational scenarios involving single and multiple targets and trackers.},
  archive      = {J_RAS},
  author       = {Nguyen T. Hung and N. Crasta and David Moreno-Salinas and António M. Pascoal and Tor A. Johansen},
  doi          = {10.1016/j.robot.2020.103608},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103608},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Range-based target localization and pursuit with autonomous vehicles: An approach using posterior CRLB and model predictive control},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the consensus of nonlinear agents in unknown cluttered
environments using random planning. <em>RAS</em>, <em>132</em>, 103607.
(<a href="https://doi.org/10.1016/j.robot.2020.103607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The consensus of multi-agent dynamic systems is a metaphor for many different tasks involving group agreement. However, ensuring consensus in real-world scenarios, with non-convex obstacles and kinodynamic motion constraints, proves to be a hard task, since it is quite difficult to model such a problem analytically. Therefore, this paper studies the problem of state agreement for Multi-Robot Systems (MRS) in unknown cluttered complex environments. Here, we propose and analyze a distributed consensus algorithm combined with a Rapidly-exploring Random Tree-based planner, which allows linear and nonlinear systems to reach a common target on their states inside bi- or three-dimensional spaces filled with static obstacles. We demonstrate that, with enough time, our planning strategy ensures probabilistic completeness convergence independently of the topological communication network employed, since some connectivity constraints are observed. Simulated results with linear and nonlinear models are provided, showing the effectiveness of our proposed method in comparison with the state-of-the-art literature for the specific case of position consensus (rendezvous) missions.},
  archive      = {J_RAS},
  author       = {Armando Alves Neto and Leonardo A. Mozelli and Douglas G. Macharet},
  doi          = {10.1016/j.robot.2020.103607},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103607},
  shortjournal = {Robot. Auton. Syst.},
  title        = {On the consensus of nonlinear agents in unknown cluttered environments using random planning},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel path planning methodology for automated valet
parking based on directional graph search and geometry curve.
<em>RAS</em>, <em>132</em>, 103606. (<a
href="https://doi.org/10.1016/j.robot.2020.103606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper presents a novel path planning methodology based on the directional graph search and the geometry curve for the Automated Valet Parking (AVP) system. The whole path planning methodology is divided into three parts including the global path planning, the path coordination strategy and the parking path planning. Firstly, the global path planning is triggered to find a path from the parking slot entrance to the rough location of the assigned parking spot. A novel directional Hybrid A* algorithm is proposed to generate the global path efficiently without redundant searches, such as the dead end. Afterwards, the path coordination strategy gives a transitional path to connect the end node of the global path to the parking planning start node. The transitional path is composed of geometry curves including arcs and line segments based on the optimal parking start node. Finally, the parking path planning generates a parking path to guide the vehicle from parking start node to the parking space. A modified C-type vertical parking path planning algorithm is utilized to generate the parking path, offering flexibility for choosing the parking start node. Simulation results based on Matlab and PreScan show that it takes less time for the proposed path planning algorithm to generate a feasible path for the AVP system compared with the general planning algorithm. The novel AVP path planning algorithm also has the potential for practical use.},
  archive      = {J_RAS},
  author       = {Zhaobo Qin and Xin Chen and Manjiang Hu and Liang Chen and Jingjing Fan},
  doi          = {10.1016/j.robot.2020.103606},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103606},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A novel path planning methodology for automated valet parking based on directional graph search and geometry curve},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Collective navigation of a multi-robot system in an unknown
environment. <em>RAS</em>, <em>132</em>, 103604. (<a
href="https://doi.org/10.1016/j.robot.2020.103604">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The navigation of autonomous, mobile multi-robot systems in changing environments is a challenging problem investigated over the past years. Cooperative, multiple robots are employed for many different tasks to increase the efficiency and success of a mission. However, many of the existing collective path planning approaches do not guarantee a reliable escape in environments with complex, non-convex obstacles without any prior knowledge. In this study, we developed a navigation framework for multi-robot systems in unknown areas that solely exploit the sensing information and shared data among the agents. The key contribution of this paper is the simultaneous, collision-free motion planning for fully autonomous robots in a collective manner. Furthermore, our communication architecture enables the robots to find an appropriate path to a desired, joint target position, despite the limited sensing and communication range.},
  archive      = {J_RAS},
  author       = {Ertug Olcay and Fabian Schuhmann and Boris Lohmann},
  doi          = {10.1016/j.robot.2020.103604},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103604},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Collective navigation of a multi-robot system in an unknown environment},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation of body direction based on gait for service robot
applications. <em>RAS</em>, <em>132</em>, 103603. (<a
href="https://doi.org/10.1016/j.robot.2020.103603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, there have been several studies on the research and development of service robots , such as reception or waiter robots for facilities and companion robots for support of baggage transportation or guidance in public spaces. Several experimental results in real environments have been reported. To realize socially acceptable human–robot interaction for service robots , human recognition, including not only position but also body direction, around the robot is important. Using an RGB-D camera, it is possible to detect the posture of a person. However, because the viewing angle of the camera is narrow, it is difficult to recognize the environment around the robot with a single device. This study proposes the estimation of the body direction based on the gait, that is, not only the position and velocity, but also the state of the legs (stance or swing phase), using laser range sensors installed at shin height. We verify the effectiveness of the proposed method for several patterns of movement, which are seen when a person interacts with the service robot and evaluate measurement accuracy.},
  archive      = {J_RAS},
  author       = {Ayanori Yorozu and Masaki Takahashi},
  doi          = {10.1016/j.robot.2020.103603},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103603},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Estimation of body direction based on gait for service robot applications},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On planar self-folding magnetic chains: Comparison of
newton–euler dynamics and internal energy optimisation. <em>RAS</em>,
<em>132</em>, 103601. (<a
href="https://doi.org/10.1016/j.robot.2020.103601">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Within the wide field of self-assembly, the self-folding chain has the unique capability to pass through narrow openings, too small for the assembled structure, yet consists in one connected body. This paper presents a novel analytical framework and corresponding experimental setup to quantify the results of a self-folding process using magnetic forces at the centimetre-scale, with the aim to put experimental results and prediction methods in the context of surgical anchoring and therapy. Two possibilities to predict the folding of a chain of magnetic components in 2D are compared and investigated in an experimental setup. Folding prediction by system Coulomb energy, neglecting folding dynamics, is compared with a simulation of the system dynamics using a novel approach for 2D folding chains, derived from the Newton–Euler equations. The presented algorithm is designed for the parallel computation architecture of modern computer systems to be easily applicable and to achieve an improved simulation speed. The experimental setup for the self-folding chain used to validate the simulation results consists of a chain of magnetic components where movement is limited to one plane and the chain is agitated by the magnetic forces between the chain components. The folding process of the experimental setup is validated for its stability and predictability under different deployment modes. Finally, the results are discussed in light of the folding prediction of longer chains. The implications of the presented findings for a 3D folding chain are discussed together with the challenges to apply the novel dynamics simulation algorithm to the 3D case. The work clearly demonstrates the potential for this novel approach for complex self-folding applications such as magnetic compression anastomosis and anchoring in minimally invasive surgery .},
  archive      = {J_RAS},
  author       = {T.H. Fass and Guangbo Hao and Pádraig Cantillon-Murphy},
  doi          = {10.1016/j.robot.2020.103601},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103601},
  shortjournal = {Robot. Auton. Syst.},
  title        = {On planar self-folding magnetic chains: Comparison of Newton–Euler dynamics and internal energy optimisation},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visual place recognition using directed acyclic graph
association measures and mutual information-based feature selection.
<em>RAS</em>, <em>132</em>, 103598. (<a
href="https://doi.org/10.1016/j.robot.2020.103598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual localization is a challenging problem, especially over the long run, since places can exhibit significant variation due to dynamic environmental and seasonal changes. To tackle this problem, we propose a visual place recognition method based on directed acyclic graph matching and feature maps extracted from deep convolutional neural networks (DCNN). Furthermore, in order to find the best subset of DCNN feature maps with minimal redundancy, we propose to form probability distributions on image representation features and leverage the Jensen–Shannon divergence to rank features. We evaluate the proposed approach on two challenging public datasets, namely the Bonn and the Freiburg datasets, and compare it to the state-of-the-art methods. For image representations, we evaluated the following DCNN architectures: AlexNet, OverFeat, ResNet18 and ResNet50 . Due to the proposed graph structure, we are able to account for any kind of correlations in image sequences, and therefore dub our approach NOSeqSLAM. Algorithms with and without feature selection were evaluated based on precision–recall curves, area under the curve score, best recall at 100\% precision score and running time, with NOSeqSLAM outperforming the counterpart approaches. Furthermore, by formulating the mutual information-based feature selection specifically for visual place recognition and by selecting the feature percentile with the best score, all the algorithms, and not just NOSeqSLAM, exhibited enhanced performance with the reduced feature set.},
  archive      = {J_RAS},
  author       = {Jurica Maltar and Ivan Marković and Ivan Petrović},
  doi          = {10.1016/j.robot.2020.103598},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103598},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Visual place recognition using directed acyclic graph association measures and mutual information-based feature selection},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A robust stereo feature-aided semi-direct SLAM system.
<em>RAS</em>, <em>132</em>, 103597. (<a
href="https://doi.org/10.1016/j.robot.2020.103597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In autonomous driving , many intelligent perception technologies have been put in use. However, visual SLAM still has problems with robustness, which limits its application, although it has been developed for a long time. We propose a feature-aided semi-direct approach to combine the direct and indirect methods in visual SLAM to allow robust localization under various situations, including large-baseline motion, textureless environment, and great illumination changes. In our approach, we first calculate inter-frame pose estimation by feature matching. Then we use the direct alignment and a multi-scale pyramid, which employs the previous coarse estimation as a priori, to obtain a more precise result. To get more accurate photometric parameters, we combine the online photometric calibration method with visual odometry . Furthermore, we replace the Shi–Tomasi corner with the ORB feature, which is more robust to illumination. For extreme brightness change, we employ the dark channel prior to weaken the halation and maintain the consistency of the image. To evaluate our approach, we build a full stereo visual SLAM system. Experiments on the publicly available dataset and our mobile robot dataset indicate that our approach improves the accuracy and robustness of the SLAM system.},
  archive      = {J_RAS},
  author       = {Xiangrui Zhao and Lina Liu and Renjie Zheng and Wenlong Ye and Yong Liu},
  doi          = {10.1016/j.robot.2020.103597},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103597},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A robust stereo feature-aided semi-direct SLAM system},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Human grasp position estimation for human–robot cooperative
object manipulation. <em>RAS</em>, <em>131</em>, 103600. (<a
href="https://doi.org/10.1016/j.robot.2020.103600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of human grasp position estimation in a physical human–robot object handling scenario. The problem is formulated as a linear regression by considering the human grasp position and their exerted torque as unknown parameters. We propose a modified least-squares algorithm to estimate the parameters by evaluating the quality of the estimates based on the assumption that the parameters should remain constant for a period of time. The solution is model-agnostic in terms of the human force/torque model – requiring only force/torque measurements on the robot side and proprioception – and is model-based in terms of the object model. The proposed grasp position estimation method is compared statistically with a conventional contact point estimation method using the collected experimental data. Moreover, the performance of the developed method is evaluated through various scenarios of physical human–robot interaction.},
  archive      = {J_RAS},
  author       = {Ramin Jaberzadeh Ansari and Giuseppe Giordano and Jonas Sjöberg and Yiannis Karayiannidis},
  doi          = {10.1016/j.robot.2020.103600},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103600},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Human grasp position estimation for human–robot cooperative object manipulation},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A lobster-inspired articulated shaft for minimally invasive
surgery. <em>RAS</em>, <em>131</em>, 103599. (<a
href="https://doi.org/10.1016/j.robot.2020.103599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Novel applications of soft pneumatic actuation in minimally invasive surgery (MIS) are proposed due to its relatively safe robot–environment interactions. Although the inherent compliance of soft robots makes them suitable for surgery, their low force output and complicated system response and behavior may limit their potential as practical MIS instruments. In this paper, three lobster-inspired antagonistic modules are proposed to realize bidirectional translational, bending and rotational motions and variable stiffness in centimeter scale. Their modular design enables flexible combinations of articulated shafts to satisfy end-effector workspace requirements in MIS. Theoretical models are proposed to relate the input pressure, deformation, output force/torque and stiffness, which provide quantitative solutions for independent adjustment on the deformation and stiffness of each module. A series of experimental results show that the proposed modules can deliver sufficient force and torque output for MIS applications, and they can be conveniently assembled into articulated shafts featuring safe actuation , high dexterity, stiffness tuning and reconfigurability .},
  archive      = {J_RAS},
  author       = {Yaohui Chen and Hoam Chung and Bernard Chen and Baoyinjiya},
  doi          = {10.1016/j.robot.2020.103599},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103599},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A lobster-inspired articulated shaft for minimally invasive surgery},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Vehicle tracking with kalman filter using online situation
assessment. <em>RAS</em>, <em>131</em>, 103596. (<a
href="https://doi.org/10.1016/j.robot.2020.103596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicle tracking is an attractive problem in the field of public transportation with several research projects conducted using Kalman filter (KF) to tackle this. While a driver may act on his own decision, there exist parameters affecting his behavior so called situation assessment such as neighboring drivers, possible obstacles, or alternative routes changing over time. In this paper, utilizing online situation assessment (SA) inside Kalman filter is studied. Motion History Graph is used as online modeling of the history of the vehicle motions and is used to augment the estimation. Experimental results on video sequences from different datasets show an average 25 percent performance improvement when using online SA inside KF.},
  archive      = {J_RAS},
  author       = {Maryam Baradaran Khalkhali and Abedin Vahedian and Hadi Sadoghi Yazdi},
  doi          = {10.1016/j.robot.2020.103596},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103596},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Vehicle tracking with kalman filter using online situation assessment},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient RRT cache method in dynamic environments for
path planning. <em>RAS</em>, <em>131</em>, 103595. (<a
href="https://doi.org/10.1016/j.robot.2020.103595">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concentrated on path planning for robots working in a dynamic environment to satisfy real-time needs. An efficient bias-goal factor RRT (EBG-RRT), which is multiple-query sampling-based replanning algorithm, is proposed with rapid response and high success rate. Specifically, a relay node method is proposed to get a position where the robot and dynamic obstacles will be no-collision and help robots to move without suspended. Based on the relay node method, Connection strategy performs minimal modifications to maintain the interrupted path. In order to overcome the short of Waypoint Cache method, an efficient and optimal Waypoint Cache (EOWC) method is proposed to make use of potential cache information and find an optimal path to repair. The EOWC method is combined with the BG-RRT algorithm according to the iterative characteristics. Finally, the EBG-RRT algorithm is verified on ROS with Aubo-i5 manipulator. Simulation results provide the EBG-RRT algorithm is outperformed both in static and dynamic environments.},
  archive      = {J_RAS},
  author       = {Chengren Yuan and Guifeng Liu and Wenqun Zhang and Xinglong Pan},
  doi          = {10.1016/j.robot.2020.103595},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103595},
  shortjournal = {Robot. Auton. Syst.},
  title        = {An efficient RRT cache method in dynamic environments for path planning},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fixed-wing UAVs flocking in continuous spaces: A deep
reinforcement learning approach. <em>RAS</em>, <em>131</em>, 103594. (<a
href="https://doi.org/10.1016/j.robot.2020.103594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fixed-Wing UAVs (Unmanned Aerial Vehicles) flocking is still a challenging problem due to the kinematics complexity and environmental dynamics. In this paper, we solve the leader–followers flocking problem using a novel deep reinforcement learning algorithm that can generate roll angle and velocity commands by training an end-to-end controller in continuous state and action spaces. Specifically, we choose CACLA (Continuous Actor–Critic Learning Automation) as the base algorithm and we use the multi-layer perceptron to represent both the actor and the critic. Besides, we further improve the learning efficiency by using the experience replay technique that stores the training data in the experience memory and samples from the memory as needed. We have compared the performance of the proposed CACER (Continuous Actor–Critic with Experience Replay) algorithm with benchmark algorithms such as DDPG and double DQN in numerical simulation, and we have demonstrated the performance of the learned optimal policy in semi-physical simulation without any parameter tuning.},
  archive      = {J_RAS},
  author       = {Chao Yan and Xiaojia Xiang and Chang Wang},
  doi          = {10.1016/j.robot.2020.103594},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103594},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Fixed-wing UAVs flocking in continuous spaces: A deep reinforcement learning approach},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A smart mobile robot commands predictor using recursive
neural network. <em>RAS</em>, <em>131</em>, 103593. (<a
href="https://doi.org/10.1016/j.robot.2020.103593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous navigation of mobile robot via classic neural network (NN) models are no more valid in terms of efficiency and accuracy due to the development of new advanced techniques. However, the necessity of finding an implementable Recursive Neural Network (RNN) model to predict the motor control of the robot with both speed and accuracy constraints still remains stagnant because of the nonlinearity and complexity of the trajectories. To provide new solutions for smart navigation problems , this paper proposes a new implementable recursive neural network controller (RNNC) predictor that calculates the Pulse Width Modulation (PMW) signals of the motors. Such proposed Multi-input Multi-output (MIMO) Controller succeeded to solve the problem of speed and accuracy of autonomous navigation. The Smart RNNC model design is illustrated with its architecture in details. Due to the complexity and the non-efficiency of the training process in real-world, a 3D Simulator was developed to create all possible scenarios. The machine learning and navigation predictions processes for designing the new RNNC model are presented together in details. In addition, the motor commands generation speed and accuracy as well as their efficiency are theoretically and practically proven. Moreover, numerical studies, 3D scenarios of trajectory tracking and obstacle avoidance prove the effectiveness and robustness of the proposed technique.},
  archive      = {J_RAS},
  author       = {Khaled Khnissi and Chiraz Ben Jabeur and Hassene Seddik},
  doi          = {10.1016/j.robot.2020.103593},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103593},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A smart mobile robot commands predictor using recursive neural network},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Vision-based posture-consistent teleoperation of robotic arm
using multi-stage deep neural network. <em>RAS</em>, <em>131</em>,
103592. (<a href="https://doi.org/10.1016/j.robot.2020.103592">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a visual teleoperation with human–robot posture-consistent based on deep neural network . A multi-stage structure of visual teleoperation network, in which the angles of robotic joints are obtained from human, is deduced. Furthermore, a novel human–robot posture-consistent mapping method is developed to generate dataset of the visual teleoperation network by solving constrained nonlinear matrix functions . Based on the designed framework, the data generator and a well trained multi-stage visual teleoperation network are presented. Finally teleoperation experiments are implemented to demonstrate that the proposed method is effectiveness and reliable.},
  archive      = {J_RAS},
  author       = {Bin Fang and Xiao Ma and Jiachun Wang and Fuchun Sun},
  doi          = {10.1016/j.robot.2020.103592},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103592},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Vision-based posture-consistent teleoperation of robotic arm using multi-stage deep neural network},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Planning the sequence of tasks for harvesting robots.
<em>RAS</em>, <em>131</em>, 103591. (<a
href="https://doi.org/10.1016/j.robot.2020.103591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A methodology for planning the sequence of tasks for a harvesting robot is presented. The fruit targets are situated at unknown locations and must be detected by the robot through a sequence of sensing tasks. Once the targets are detected, the robot must execute a harvest action at each target location. The traveling salesman paradigm (TSP) is used to plan the sequence of sensing and harvesting tasks taking into account the costs of the sensing and harvesting actions and the traveling times. Sensing is planned online. The methodology is validated and evaluated in both laboratory and greenhouse conditions for a case study of a sweet pepper harvesting robot. The results indicate that planning the sequence of tasks for a sweet pepper harvesting robot results in 12\% cost reduction. Incorporating the sensing operation in the planning sequence for fruit harvesting is a new approach in fruit harvesting robots and is important for cycle time reduction . Furthermore, the sequence is re-planned as sensory information becomes available and the costs of these new sensing operations are also considered in the planning.},
  archive      = {J_RAS},
  author       = {Polina Kurtser and Yael Edan},
  doi          = {10.1016/j.robot.2020.103591},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103591},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Planning the sequence of tasks for harvesting robots},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). TDOA based localization and its application to the
initialization of LiDAR based autonomous robots. <em>RAS</em>,
<em>131</em>, 103590. (<a
href="https://doi.org/10.1016/j.robot.2020.103590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work considers the problem of locating a single robot given a set of squared noisy range difference measurements to a set of points (anchors) whose positions are known. In the sequel, localization problem is solved in the Least-Squares (LS) sense by writing the robot position in polar/spherical coordinates. This representation transforms the original nonconvex/multimodal cost function into the quotient of two quadratic forms, whose constrained maximization is more tractable than the original problem. Simulation results indicate that the proposed method has similar accuracy to state-of-the-art optimization-based localization algorithms in its class, and the simple algorithmic structure and computational efficiency makes it appealing for applications with strong computational constraints. Additionally, location information is used to find the initial orientation of the robot with respect to the previously obtained map in scan matching. Thus, the crucial problem of the autonomous initialization and localization in robotics is solved.},
  archive      = {J_RAS},
  author       = {Pınar Oğuz-Ekim},
  doi          = {10.1016/j.robot.2020.103590},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103590},
  shortjournal = {Robot. Auton. Syst.},
  title        = {TDOA based localization and its application to the initialization of LiDAR based autonomous robots},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Monocular vision-based gripping of objects. <em>RAS</em>,
<em>131</em>, 103589. (<a
href="https://doi.org/10.1016/j.robot.2020.103589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optics-based systems may provide high spatial and temporal resolution for close range object detection in underwater environments. By using a monocular camera on a low cost underwater vehicle manipulator system , objects can be tracked by the vehicle and handled by the manipulator. In this paper, a monocular camera is used to detect an object of interest through object detection. Spatial features of the object are extracted, and a dynamic positioning system is designed for the underwater vehicle in order for it to maintain a desired position relative to the object. A manipulator mounted under the vehicle is used to retrieve the object through a developed kinematic control system. Experimental tests verify the proposed methodology. A stability analysis proves asymptotic stability properties for the chosen sliding mode controller and exponential stability for the task error.},
  archive      = {J_RAS},
  author       = {Bent Oddvar Arnesen Haugaløkken and Martin Breivik Skaldebø and Ingrid Schjølberg},
  doi          = {10.1016/j.robot.2020.103589},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103589},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Monocular vision-based gripping of objects},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Trajectory coordination for a cooperative multi-manipulator
system and dynamic simulation error analysis. <em>RAS</em>,
<em>131</em>, 103588. (<a
href="https://doi.org/10.1016/j.robot.2020.103588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To achieve temporal and spatial correspondence between multiple robotic manipulators , the system must correctly analyze the coordinated path required for a specific task. Based on manipulator kinematics analysis , we first studied the kinematic constraints between the end-effectors of cooperative manipulators, and deduced the multi-manipulator cooperative kinematics constraint equations in the Cartesian coordinate system space under different motion modes. Then, we created two MD-6 manipulator models to simulate the trajectory simulation of the synchronous and relative motion of the manipulator. This allowed us to verify the correctness of the proposed trajectory coordination method, and analyze the influence of external loads on the position and posture of the end-effector of the manipulator, to effectively predict the cooperative motion error of the manipulator system . Finally, in order to verify the effectiveness of the proposed trajectory coordination method, we established a robotic experimental platform and conducted experimental research. The results show that the multi-manipulator trajectory coordination method studied in this paper can make multi-manipulators effectively achieve the target requirements of tasks such as time and space cooperative handling and circular drawing operations.},
  archive      = {J_RAS},
  author       = {Chunjian Su and Shuai Zhang and Shumei Lou and Rui Wang and Gaohua Cao and Longyun Yang and Qing Wang},
  doi          = {10.1016/j.robot.2020.103588},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103588},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Trajectory coordination for a cooperative multi-manipulator system and dynamic simulation error analysis},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time topological localization using structured-view
ConvNet with expectation rules and training renewal. <em>RAS</em>,
<em>131</em>, 103578. (<a
href="https://doi.org/10.1016/j.robot.2020.103578">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile service robots possess high potential of providing numerous assistances in the working areas. In an attempt to develop a mobile service robot which is dynamically balanced for faster movement and taller manipulation capability, we designed and prototyped J4.alpha, which is intended for swift navigation and nimble manipulation. Previously, we devised a pure visual method based on a supervised deep learning model for real-time recognition of nodal locations. Four low-resolution RGB cameras are installed around J4.alpha to capture the surrounding visual features for training and detection. As the method is developed for ease of implementation, fast real-time application, accurate detection, and low cost, we further improve the accuracy and the practicality of the method in this study. Specifically, a set of expectation rules are introduced to reject outlier detections , and a scheme of training renewal is devised to effectively react to environmental modifications. In our previous tests, precision and recall rates of the location coordinate detection by the ConvNet models were generally between 0.78 and 0.91; by introducing the expectation rules, precision and recall are improved by approximately 10\%. A large scale field test is also carried out here for both corridor and factory scenarios; the performance of the proposed method was tested for detection accuracy and verified for 2 m and 0.5 m nodal intervals. The scheme of training renewal designed for capturing and reflecting environmental modifications was also proved to be effective.},
  archive      = {J_RAS},
  author       = {Chih-Hung G. Li and Yi-Feng Hong and Po-Kai Hsu and Thavida Maneewarn},
  doi          = {10.1016/j.robot.2020.103578},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103578},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Real-time topological localization using structured-view ConvNet with expectation rules and training renewal},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visual-inertial teach and repeat. <em>RAS</em>,
<em>131</em>, 103577. (<a
href="https://doi.org/10.1016/j.robot.2020.103577">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Teach and Repeat (T&amp;R) refers to the technology that allows a robot to autonomously follow a previously traversed route, in a natural scene and using only its onboard sensors. In this paper we present a Visual-Inertial Teach and Repeat (VI-T&amp;R) algorithm that uses stereo and inertial data and targets Unmanned Aerial Vehicles with limited on-board computational resources. We propose a tightly-coupled relative formulation of the visual-inertial constraints that is tailored to the T&amp;R application. In order to achieve real-time operation on limited hardware, we reduce the problem to motion-only visual-inertial Bundle Adjustment. In the repeat stage, we detail how to generate a trajectory and smoothly follow it with a constantly changing relative frame. The proposed method is validated in simulated environments, using real sensor data from the public EuRoC dataset, and using our own robotic setup and closed-loop control. Our experimental results demonstrate high accuracy and real-time performance both on a standard desktop system and on a low-cost Odroid X-U4 embedded computer.},
  archive      = {J_RAS},
  author       = {Matías Nitsche and Facundo Pessacg and Javier Civera},
  doi          = {10.1016/j.robot.2020.103577},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103577},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Visual-inertial teach and repeat},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Planning the trajectory of an autonomous wheel loader and
tracking its trajectory via adaptive model predictive control.
<em>RAS</em>, <em>131</em>, 103570. (<a
href="https://doi.org/10.1016/j.robot.2020.103570">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a typical operation mode, a wheel loader frequently accelerates and decelerates, and the curvature of the driving path is inconsistent. In the past, autonomous vehicle trajectory planning has not considered the related changes in the velocity of the vehicle. Therefore, the trajectory tracking control process has seldom considered the impact of curving paths on the trajectory tracking performance. To address these problems, this study evaluated an autonomous wheel loader based on the trajectory of its non-uniform driving motion and constructed an adaptive model predictive control (AMPC) trajectory tracking system that considers disturbances in the path curvature . The trajectory of the autonomous wheel loader was then tracked using the proposed AMPC system with a planned non-uniform motion trajectory as the target. Its performance was then compared with that of a conventional model predictive control (MPC) trajectory tracking system that does not consider any path curvature disturbances. The maximum displacement error and heading error obtained by the proposed AMPC system were found to be 65.7\% and 60\%, respectively, smaller than those obtained by the MPC system. The desired trajectory can also be tracked well under different curvature amplitudes using the AMPC trajectory tracking system, ensuring active safety performance of an autonomous wheel loader in the process of trajectory tracking.},
  archive      = {J_RAS},
  author       = {Junren Shi and Dongye Sun and Datong Qin and Minghui Hu and Yingzhe Kan and Ke Ma and Ruibo Chen},
  doi          = {10.1016/j.robot.2020.103570},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103570},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Planning the trajectory of an autonomous wheel loader and tracking its trajectory via adaptive model predictive control},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Obstacle avoidance in dynamic environments based on velocity
space optimization. <em>RAS</em>, <em>131</em>, 103569. (<a
href="https://doi.org/10.1016/j.robot.2020.103569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robotic obstacle avoidance is an important issue in robotic navigation for unknown or partially known, dynamic environments. A good number of techniques have already been proposed to navigate obstacles in this kind of environment. They include a series of velocity space methods that have been successful implemented in several applications. They formulate the problem as one of constrained optimization in the velocity space of the robot. The constraints include the obstacles in the environment assuming they are static. In this paper, we present an efficient, real-time method (BCM-DO) to include the restrictions imposed by dynamic objects. The optimization function has also been adapted to include these new restrictions. The new function is evaluated in two sets of points. A first set is obtained from a coarse sampling in the reachable window of velocities and a second set is selected in the limits of each curvature interval to avoid missing small openings between static objects. The whole system has first been extensively tested in several simulated robots and finally applied to a hotel assistant robot (BellBot) resulting in an efficient, real-time obstacle avoidance method that produces smooth and reliable routes.},
  archive      = {J_RAS},
  author       = {Joaquín López and Pablo Sanchez-Vilariño and Miguel Díaz Cacho and Elena López Guillén},
  doi          = {10.1016/j.robot.2020.103569},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103569},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Obstacle avoidance in dynamic environments based on velocity space optimization},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Combining reinforcement learning with rule-based controllers
for transparent and general decision-making in autonomous driving.
<em>RAS</em>, <em>131</em>, 103568. (<a
href="https://doi.org/10.1016/j.robot.2020.103568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design of high-level decision-making systems is a topical problem in the field of autonomous driving . In this paper, we combine traditional rule-based strategies and reinforcement learning (RL) with the goal of achieving transparency and robustness. On the one hand, the use of handcrafted rule-based controllers allows for transparency, i.e., it is always possible to determine why a given decision was made, but they struggle to scale to complex driving scenarios, in which several objectives need to be considered. On the other hand, black-box RL approaches enable us to deal with more complex scenarios, but they are usually hardly interpretable. In this paper, we combine the best properties of these two worlds by designing parametric rule-based controllers, in which interpretable rules can be provided by domain experts and their parameters are learned via RL. After illustrating how to apply parameter-based RL methods (PGPE) to this setting, we present extensive numerical simulations in the highway and in two urban scenarios: intersection and roundabout. For each scenario, we show the formalization as an RL problem and we discuss the results of our approach in comparison with handcrafted rule-based controllers and black-box RL techniques.},
  archive      = {J_RAS},
  author       = {Amarildo Likmeta and Alberto Maria Metelli and Andrea Tirinzoni and Riccardo Giol and Marcello Restelli and Danilo Romano},
  doi          = {10.1016/j.robot.2020.103568},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103568},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Combining reinforcement learning with rule-based controllers for transparent and general decision-making in autonomous driving},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised semantic clustering and localization for mobile
robotics tasks. <em>RAS</em>, <em>131</em>, 103567. (<a
href="https://doi.org/10.1016/j.robot.2020.103567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to its vast applicability, the semantic interpretation of regions or entities increasingly attracts the attention of scholars within the robotics community. The paper at hand introduces a novel unsupervised technique to semantically identify the position of an autonomous agent in unknown environments. When the robot explores a certain path for the first time, community detection is achieved through graph-based segmentation. This allows the agent to semantically define its surroundings in future traverses even if the environment’s lighting conditions are changed. The proposed semantic clustering technique exploits the Louvain community detection algorithm , which constitutes a novel and efficient method for identifying groups of measurements with consistent similarity. The produced communities are combined with metric information, as provided by the robot’s odometry through a hierarchical agglomerative clustering method. The suggested algorithm is evaluated in indoors and outdoors datasets creating topological maps capable of assisting semantic localization . We demonstrate that the system categorizes the places correctly when the robot revisits an environment despite the possible lighting variation.},
  archive      = {J_RAS},
  author       = {Vasiliki Balaska and Loukas Bampis and Moses Boudourides and Antonios Gasteratos},
  doi          = {10.1016/j.robot.2020.103567},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103567},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Unsupervised semantic clustering and localization for mobile robotics tasks},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A spiking network classifies human sEMG signals and triggers
finger reflexes on a robotic hand. <em>RAS</em>, <em>131</em>, 103566.
(<a href="https://doi.org/10.1016/j.robot.2020.103566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The interaction between robots and humans is of great relevance for the field of neurorobotics as it can provide insights on how humans perform motor control and sensor processing and on how it can be applied to robotics. We propose a spiking neural network (SNN) to trigger finger motion reflexes on a robotic hand based on human surface Electromyography (sEMG) data. The first part of the network takes sEMG signals to measure muscle activity, then classify the data to detect which finger is being flexed in the human hand. The second part triggers single finger reflexes on the robot using the classification output. The finger reflexes are modeled with motion primitives activated with an oscillator and mapped to the robot kinematic. We evaluated the SNN by having users wear a non-invasive sEMG sensor, record a training dataset, and then flex different fingers, one at a time. The muscle activity was recorded using a Myo sensor with eight different channels. The sEMG signals were successfully encoded into spikes as input for the SNN. The classification could detect the active finger and trigger the motion generation of finger reflexes. The SNN was able to control a real Schunk SVH 5-finger robotic hand online. Being able to map myo-electric activity to functions of motor control for a task, can provide an interesting interface for robotic applications , and a platform to study brain functioning. SNN provide a challenging but interesting framework to interact with human data. In future work the approach will be extended to control also a robot arm at the same time.},
  archive      = {J_RAS},
  author       = {J. Camilo Vasquez Tieck and Sandro Weber and Terrence C. Stewart and Jacques Kaiser and Arne Roennau and Rüdiger Dillmann},
  doi          = {10.1016/j.robot.2020.103566},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103566},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A spiking network classifies human sEMG signals and triggers finger reflexes on a robotic hand},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rapidly-exploring random trees multi-robot map exploration
under optimization framework. <em>RAS</em>, <em>131</em>, 103565. (<a
href="https://doi.org/10.1016/j.robot.2020.103565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rapidly-exploring Randomized Trees (RRT) is a kind of probabilistically complete exploration algorithm based on the tree structure. It has been widely used in the robotic navigation since it guarantees the complete discovery and the exploration of environment maps through robots. In the present study, the RRT algorithm is extended to propose an optimization-based map exploration strategy for multiple robots to actively explore and build environment maps. The present study adopts a market-based task allocation strategy, which to maximize the profit, for the coordination between robots. In the extension of the RRT, the cost function consists the unknown region and the passed unknown region. The unknown region is explored for a given frontier point, while the passed unknown region is the area, where the robot moves towards the target frontier point. When the robot moves from the start position to the target frontier point, the trajectory length is defined as a constraint for the optimization. The main contributions of the present study can be summarized in optimizing the frontier points, defining a new task allocation strategy and applying different evaluation rules, including the running time and the trajectory length . These rules are applied to explore the multi-robot map in simulated and practical environments. Then the Robot Operating System (ROS) is utilized to evaluate the application of the proposed exploration strategy on Turtlebots in a 270 m 2 m2 room. Obtained results from the simulation and the experiment demonstrate that the proposed method outperforms the Umari’s approach from both the running time and the trajectory length aspects.},
  archive      = {J_RAS},
  author       = {Liwei Zhang and Zhibin Lin and Jie Wang and Bingwei He},
  doi          = {10.1016/j.robot.2020.103565},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103565},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Rapidly-exploring random trees multi-robot map exploration under optimization framework},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Coordination of thrusters, reaction wheels, and arm in
orbital robots. <em>RAS</em>, <em>131</em>, 103564. (<a
href="https://doi.org/10.1016/j.robot.2020.103564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fuel-efficient control strategy for a manipulator-equipped spacecraft is presented. The strategy uses the thrusters , the reaction wheels, and the arm drives in a coordinated way to limit the use of the thrusters and achieve ideally zero fuel consumption in contact-free maneuvering. The thrusters are activated automatically only after contact, to stabilize the inertial motion of the system. The controller regulates the translation of the center-of-mass (CoM) of the whole space robot, the rotation of the spacecraft, and the pose of the end-effector (EE) in a decoupled way, utilizing the thrusters to control the CoM translation only and the remaining actuators to control the rotation and end-effector coordinately. The method is validated experimentally using a hardware-in-the-loop simulator composed of a seven degrees-of-freedom (DOF) arm mounted on a 6DOF simulated spacecraft. Numerical simulations with discrete thrusters assess the fuel efficiency of the proposed strategy.},
  archive      = {J_RAS},
  author       = {Alessandro M. Giordano and Alexander Dietrich and Christian Ott and Alin Albu-Schäffer},
  doi          = {10.1016/j.robot.2020.103564},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103564},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Coordination of thrusters, reaction wheels, and arm in orbital robots},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust image completion and masking with application to
robotic bin picking. <em>RAS</em>, <em>131</em>, 103563. (<a
href="https://doi.org/10.1016/j.robot.2020.103563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated image completion and masking have been emerged as a subject of keen interest due to their impact on image modification and interpretation. The current state-of-the-art approaches require a fixed format of missing parts and are ineffective for handling corrupted images. Besides, they focus exclusively on the image completion without taking into consideration the image masking as an inverse process of completion. This paper proposes a deep learning approach to an integrated framework of image completion and masking based on the cross-mapping generative adversarial network or CM-GAN, in short. CM-GAN offers the robustness in image completion under corruptions as well as the capability of synthesizing various masked images with arbitrary mask locations and shapes. In particular, the capability of CM-GAN in image masking is shown to be extended into the removal of unwanted backgrounds in images. We verify the superior performance of CM-GAN for image completion and masking based on extensive experiments. Furthermore, we implement a deep learning based robotic bin picking to demonstrate that the background removal capability of CM-GAN plays a key role for estimating the 3D pose of randomly filed multiple industrial parts in a bin.},
  archive      = {J_RAS},
  author       = {Sukhan Lee and Naeem Ul Islam and Soojin Lee},
  doi          = {10.1016/j.robot.2020.103563},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103563},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Robust image completion and masking with application to robotic bin picking},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Human-friendly control system design for two-wheeled service
robot with optimal control approach. <em>RAS</em>, <em>131</em>, 103562.
(<a href="https://doi.org/10.1016/j.robot.2020.103562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel control system design for a two-wheeled service robot that follows a person as an assistant without knowing the person’s destination. For this kind of service robot , the key skill is to realize human-friendly movement. However, appropriate motion always changed depending on the situation. For instance, when the robot is close and person turns toward it, it is important to suppress the robot’s acceleration. Likewise, if the person turns away from the robot, the robot should maintain its position within an appropriate area. Therefore, to deal with various required movements, our control system is able to change its properties automatically and suitably depending on the situation by using weights of the cost function in nonlinear model predictive control (NMPC) as a function of the relative distance between the person and the robot. Unlike previous methods, our design includes only one controller. Consequently, we are able to take into account system stability. Moreover, owing to proposing in NMPC framework, it is easy to extend our method by adopting other recognition or goal-setting methods. We conducted simulations using actual human walking data taken by the robot’s laser range sensors. The experiments demonstrate that the robot can follow a person who performs U-turn, confirming that our method can produce human-friendly robot movement in a practical scene.},
  archive      = {J_RAS},
  author       = {Shunichi Sekiguchi and Ayanori Yorozu and Kazuhiro Kuno and Masaki Okada and Yutaka Watanabe and Masaki Takahashi},
  doi          = {10.1016/j.robot.2020.103562},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103562},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Human-friendly control system design for two-wheeled service robot with optimal control approach},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Appearance-invariant place recognition by adversarially
learning disentangled representation. <em>RAS</em>, <em>131</em>,
103561. (<a href="https://doi.org/10.1016/j.robot.2020.103561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Place recognition is an essential component to address the problem of visual navigation and SLAM. The long-term place recognition is challenging as the environment exhibits significant variations across different times of the days, months, and seasons. In this paper, we view appearance changes as multiple domains and propose a Feature Disentanglement Network (FDNet) based on a convolutional auto-encoder and adversarial learning to extract two independent deep features — content and appearance. In our network, the content feature is learned which only retains the content information of images through the competition with the discriminators and content encoder. Besides, we utilize the triplets loss to make the appearance feature encode the appearance information. The generated content features are directly used to measure the similarity of images without dimensionality reduction operations. We use datasets that contain extreme appearance changes to carry out experiments, which show how meaningful recall at 100\% precision can be achieved by our proposed method where existing state-of-art approaches often get worse performance.},
  archive      = {J_RAS},
  author       = {Cao Qin and Yunzhou Zhang and Yan Liu and Sonya Coleman and Dermot Kerr and Guanghao Lv},
  doi          = {10.1016/j.robot.2020.103561},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103561},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Appearance-invariant place recognition by adversarially learning disentangled representation},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Coupled task scheduling for heterogeneous multi-robot system
of two robot types performing complex-schedule order fulfillment tasks.
<em>RAS</em>, <em>131</em>, 103560. (<a
href="https://doi.org/10.1016/j.robot.2020.103560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses multi-robot task scheduling for two robot types arising from heterogeneous robotic order fulfillment systems. The heterogeneous multi-robot system comprises two types of robots with specialized and complementary capabilities to achieve long-cycle and multi-station order fulfillment tasks on a logistic network. This problem is extremely challenging because of innate complex-schedule constraints of tasks and coupled temporal–spatial relations between all robots. After set-theoretic and mixed integer linear programming problem formulations, we use coupled approach, instead of decoupled approaches to explore the synergy between heterogeneous robots, which is different from most existing similar works. To model the structural (complex-schedule) and quantitative (temporal–spatial) coupledness of robots’ time-extended task schedules, an edge-weighted and vertex-weighted block sequence graph is introduced. Based on this model, time-extended task scheduling is achieved using rank-minimal heuristic and genetic algorithm metaheuristic . Theoretically, this model is complete and non-redundant. Empirically, compared with decoupled approach, optimality and efficiency of the proposed methods are evaluated on designed instances. The results demonstrate that coupled methods can achieve near-optimal solutions with higher performance ratio than decoupled methods in moderate time. At the same time, coupled methods can leverage spatial and temporal properties of miscellaneous tasks, and balance instantaneous and time-extended decisions to achieve tight collective synergy in the long run.},
  archive      = {J_RAS},
  author       = {Hanfu Wang and Weidong Chen and Jingchuan Wang},
  doi          = {10.1016/j.robot.2020.103560},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103560},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Coupled task scheduling for heterogeneous multi-robot system of two robot types performing complex-schedule order fulfillment tasks},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Advanced mapping robot and high-resolution dataset.
<em>RAS</em>, <em>131</em>, 103559. (<a
href="https://doi.org/10.1016/j.robot.2020.103559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a fully hardware synchronized mapping robot with support for a hardware synchronized external tracking system, for super-precise timing and localization . Nine high-resolution cameras and two 32-beam 3D Lidars were used along with a professional, static 3D scanner for ground truth map collection. With all the sensors calibrated on the mapping robot, three datasets are collected to evaluate the performance of mapping algorithms within a room and between rooms. Based on these datasets we generate maps and trajectory data , which is then fed into evaluation algorithms. We provide the datasets for download and the mapping and evaluation procedures are made in a very easily reproducible manner for maximum comparability . We have also conducted a survey on available robotics-related datasets and compiled a big table with those datasets and a number of properties of them.},
  archive      = {J_RAS},
  author       = {Hongyu Chen and Zhijie Yang and Xiting Zhao and Guangyuan Weng and Haochuan Wan and Jianwen Luo and Xiaoya Ye and Zehao Zhao and Zhenpeng He and Yongxia Shen and Sören Schwertfeger},
  doi          = {10.1016/j.robot.2020.103559},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103559},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Advanced mapping robot and high-resolution dataset},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A ROS framework for the extrinsic calibration of intelligent
vehicles: A multi-sensor, multi-modal approach. <em>RAS</em>,
<em>131</em>, 103558. (<a
href="https://doi.org/10.1016/j.robot.2020.103558">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a general approach to the problem of extrinsic calibration of multiple sensors of varied modalities. This is of particular relevance for intelligent vehicles , which are complex systems that often encompass several sensors of different modalities. Our approach is seamlessly integrated with the Robot Operating System (ROS) framework, and allows for the interactive positioning of sensors and labelling of data, facilitating the calibration procedure. The calibration is formulated as a simultaneous optimization for all sensors, in which the objective function accounts for the various sensor modalities. Results show that the proposed procedure produces accurate calibrations, on par with state of the art approaches which operate only for pairwise setups.},
  archive      = {J_RAS},
  author       = {Miguel Oliveira and Afonso Castro and Tiago Madeira and Eurico Pedrosa and Paulo Dias and Vítor Santos},
  doi          = {10.1016/j.robot.2020.103558},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103558},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A ROS framework for the extrinsic calibration of intelligent vehicles: A multi-sensor, multi-modal approach},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Safeguarding against prefix interception attacks via online
learning. <em>RAS</em>, <em>131</em>, 103556. (<a
href="https://doi.org/10.1016/j.robot.2020.103556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In human–robot cooperation, the information interaction plays a key role. Most of the information interaction rely on Border Gateway Protocol (BGP), which is a vital route protocol on networks. However, the BGP is susceptible to the prefix interception attacks because the rightful origin of each prefix cannot be verified in BGP. For this reason, we propose a novel and effective route selection method against prefix interception attacks , which combines the resilience of routers and the historical performance of routers to choose a secure route. Moreover, we estimate the performance of BGP by introducing the definition of resilience and the historical performance of routers via online learning against the prefix interception attack. Furthermore, we analyze the bound of regret and obtain O ( T ) O(T) regret, where T T denotes the time horizon. In addition, the proposed method is verified both on synthetic data and network simulations. The results show that the proposed method has more resilience against prefix interception attacks than Counter-Raptor.},
  archive      = {J_RAS},
  author       = {Meng Meng and Ruijuan Zheng and Ruxi Peng and Junlong Zhu and Mingchuan Zhang and Qingtao Wu},
  doi          = {10.1016/j.robot.2020.103556},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103556},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Safeguarding against prefix interception attacks via online learning},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time 3D object proposal generation and classification
using limited processing resources. <em>RAS</em>, <em>130</em>, 103557.
(<a href="https://doi.org/10.1016/j.robot.2020.103557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of detecting 3D objects is important in various robotic applications . The existing deep learning-based detection techniques have achieved impressive performances. However, these techniques are limited to being run on a graphics processing unit (GPU) in a real-time environment. To achieve real-time 3D object detection with limited computational resources, we propose an efficient detection method based on 3D proposal generation and classification. The proposal generation is based mainly on point segmentation, while proposal classification is performed by a lightweight convolution neural network (CNN). KITTI datasets are then used to validate our method. It takes only 0.082 s s for our method to process one point block with one core of the central processing unit (CPU). In addition to efficiency, the experimental results also demonstrate the capability of the proposed method of producing a competitive performance in object recall and classification.},
  archive      = {J_RAS},
  author       = {Xuesong Li and Jose Guivant and Subhan Khan},
  doi          = {10.1016/j.robot.2020.103557},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103557},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Real-time 3D object proposal generation and classification using limited processing resources},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). WAGNN: A weighted aggregation graph neural network for
robot skill learning. <em>RAS</em>, <em>130</em>, 103555. (<a
href="https://doi.org/10.1016/j.robot.2020.103555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robotic skill learning suffers from the diversity and complexity of robotic tasks in continuous domains, making the learning of transferable skills one of the most challenging issues in this area, especially for the case where robots differ in terms of structure. Aiming at making the policy easier to be generalized or transferred, the graph neural networks (GNN) was previously employed to incorporate explicitly the robot structure into the policy network. In this paper, with the help of graph neural networks , we further investigate the problem of efficient learning transferable policies for robots with serial structure, which commonly appears in various robot bodies, such as robotic arms and the leg of centipede. Based on a kinematics analysis on the serial robotic structure, the policy network is improved by proposing a weighted information aggregation strategy. It is experimentally shown on different robotics structures that in a few-shot policy learning setting, the new aggregation strategy significantly improves the performance not only on the learning speed, but also on the control accuracy.},
  archive      = {J_RAS},
  author       = {Fengyi Zhang and Zhiyong Liu and Fangzhou Xiong and Jianhua Su and Hong Qiao},
  doi          = {10.1016/j.robot.2020.103555},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103555},
  shortjournal = {Robot. Auton. Syst.},
  title        = {WAGNN: A weighted aggregation graph neural network for robot skill learning},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Earth-fixed trajectory and map online estimation: Building
on GES sensor-based SLAM filters. <em>RAS</em>, <em>130</em>, 103552.
(<a href="https://doi.org/10.1016/j.robot.2020.103552">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of obtaining an Earth-fixed trajectory and map (ETM), with the associated uncertainty, using the sensor-based map provided by a globally asymptotically/exponentially stable (GES) SLAM filter. The algorithm builds on an optimization problem with a closed-form solution, and its uncertainty description is derived resorting to perturbation theory. The combination of the algorithm proposed in this paper with sensor-based SLAM filtering results in a complete SLAM methodology, which is directly applied to the three main different formulations: range-and-bearing, range-only, and bearing-only. Simulation and experimental results for all these formulations are included in this work to illustrate the performance of the proposed algorithm under realistic conditions. The ETM algorithm proposed in this paper is truly sensor-agnostic, as it only requires a sensor-based map and imposes no constraints on how this map is acquired nor how egomotion is captured. However, in the experiments presented herein, all the sensor-based filters use a sensor to measure the angular velocity and, for the range-only and bearing-only formulations, a sensor to measure the linear velocity .},
  archive      = {J_RAS},
  author       = {Pedro Lourenço and Bruno J. Guerreiro and Pedro Batista and Paulo Oliveira and Carlos Silvestre},
  doi          = {10.1016/j.robot.2020.103552},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103552},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Earth-fixed trajectory and map online estimation: Building on GES sensor-based SLAM filters},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Kinematic and dynamic manipulability analysis for
free-floating space robots with closed chain constraints. <em>RAS</em>,
<em>130</em>, 103548. (<a
href="https://doi.org/10.1016/j.robot.2020.103548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the manipulability analysis of free-floating multi-arm space robots. Evaluation of manipulator capability is useful both in the design and in the operation phase. After capturing a target, closed kinematic chains are formed with multi-arm cooperative manipulating a common object. Owing to the dynamic coupling effect, the manipulability analysis of free-floating systems is more complex than that of ground-fixed closed chain systems. To analyze the cooperative manipulability, kinematic and dynamic formulations for the free-floating closed chain systems are firstly derived. The formulations describe the mapping of joint velocities and torques, respectively, to task velocities and forces, as well as joint torques to task accelerations and forces, by using the generalized Jacobian matrices . Next, the well-known concepts of manipulability ellipsoid, manipulability measure and task compatibility of the free-floating closed chain system are formally extended. Besides, a new approach called scaling factor method is used in the analysis of the task compatibility, which is more accurate compared with the manipulability ellipsoid method. Three applications of the performance indices are considered: (1) the feasibility analysis for a given task, (2) the trajectory planing giving a desired task path, and (3) configuration optimization with different task requirements. The proposed index is proved a very efficient tool that can be utilized in the cooperative manipulation tasks for free-floating space robotic systems .},
  archive      = {J_RAS},
  author       = {Ruonan Xu and Jianjun Luo and Mingming Wang},
  doi          = {10.1016/j.robot.2020.103548},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103548},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Kinematic and dynamic manipulability analysis for free-floating space robots with closed chain constraints},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel coordinated motion planner based on capability map
for autonomous mobile manipulator. <em>RAS</em>, <em>129</em>, 103554.
(<a href="https://doi.org/10.1016/j.robot.2020.103554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of the robotic technology, Autonomous Mobile Manipulator (AMM) is increasingly used in more applications. Reasonable motion planning for AMM to maintain high manipulation capability is the prerequisite for the success of the mobile manipulation task. In this paper, the Capability Map (CM) of AMM that gives the distribution of the manipulability in cartesian space is first built. Then given the path of the end effector , we design a novel path planner for the mobile robot by querying CM online so that AMM keeps high manipulability . Moreover, a task-priority coordinated motion controller is developed to control the mobile robot and the manipulator to track their trajectories. In this controller, the trajectory of the manipulator is used as the primary task, and the trajectory of the mobile robot is treated as the constrained task. Simulation results show that the path of the mobile robot can be found online, and AMM follows the trajectories well.},
  archive      = {J_RAS},
  author       = {Heng Zhang and Qi Sheng and Yuxin Sun and Xinjun Sheng and Zhenhua Xiong and Xiangyang Zhu},
  doi          = {10.1016/j.robot.2020.103554},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103554},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A novel coordinated motion planner based on capability map for autonomous mobile manipulator},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Grasp prediction and evaluation of multi-fingered dexterous
hands using deep learning. <em>RAS</em>, <em>129</em>, 103550. (<a
href="https://doi.org/10.1016/j.robot.2020.103550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from human skills has become one of the popular inspirations in grasp prediction and evaluation, but lack of effective methods on groups of grasp points for multi-fingered dexterous hands yields an open challenge. When facing an object, humans firstly predict a variety of options for grasps, which can be concerned as a complex multi-valued problem. After prediction, humans evaluate grasps and then choose the optimal one. Inspired by human skills, we propose Grasp Prediction Networks (GPNs) based on Convolutional Neural Networks (CNNs) and Mixture Density Networks (MDNs). The proposed GPNs map from a depth image to a set of parameters for Gaussian Mixture Model (GMM), from which candidate groups of grasp points can be sampled for prediction. Besides, we also propose Grasp Evaluation Networks (GENs) to evaluate candidate groups and then choose the optimal group of grasp points. The proposed GENs consider force-closure metric as grasp quality for evaluation. Different from other related work, our method (1) utilizes a probabilistic model to predict multiple groups of grasp points from a monocular depth image and (2) evaluates grasp quality with force-closure metric given a monocular depth image and a group of grasp points. Furthermore, we built a grasp dataset which consists of depth images, groups of grasp points and each group’s grasp quality. Herein, three different experiments were designed to validate our approach. The first one was a comparative experiment and revealed that GPNs show equivalent performance as GraspIt! in terms of high-quality grasp planning. The second one was also a comparative experiment , which validated that GENs can evaluate grasps as precisely as GraspIt!. Moreover, the last one was an actual experiment implemented on Shadow Hand Lite, and experimental results indicated that our approach achieved finely grasp of novel objects.},
  archive      = {J_RAS},
  author       = {Zengzhi Zhao and Weiwei Shang and Haoyuan He and Zhijun Li},
  doi          = {10.1016/j.robot.2020.103550},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103550},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Grasp prediction and evaluation of multi-fingered dexterous hands using deep learning},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A connectivity preserving node permutation local method in
limited range robotic networks. <em>RAS</em>, <em>129</em>, 103540. (<a
href="https://doi.org/10.1016/j.robot.2020.103540">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Limited communication range, together with mobility of robots, makes it crucial to design the control plans such that connectivity of a multi-robot network is maintained. Recently, many local and global connectivity maintenance schemes have been proposed to preserve connectivity of a robotic network . The traditional local connectivity maintenance method (LCM) is known to preserve every existing link, even though some of the existing connections might not be necessary for maintaining a path between each pair of robots, which is the aim of global connectivity maintenance (GCM) methods. However, the flexibility of movement provided by the global method costs restriction on speed and bandwidth. In this paper, a modified local connectivity maintenance method is provided to gain more flexibility of movement, while preserving the properties and simplicity of a local method. The proposed method is based on traditional local connectivity maintenance equipped with a basic operation to exchange the neighbors between two adjacent robots. Permutation of robots could be beneficial in many robotic applications such as exchanging the leader role in a V-formed robotic group or providing a path for a robot to reach its desired position while preserving the networks connectivity.},
  archive      = {J_RAS},
  author       = {Koresh Khateri and Mahdi Pourgholi and Mohsen Montazeri and Lorenzo Sabattini},
  doi          = {10.1016/j.robot.2020.103540},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103540},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A connectivity preserving node permutation local method in limited range robotic networks},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Development of a robotic finger with a branching tendon
mechanism and sensing based on the moment-equivalent point.
<em>RAS</em>, <em>129</em>, 103538. (<a
href="https://doi.org/10.1016/j.robot.2020.103538">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we developed an underactuated robotic finger with three joints having a branching tendon mechanism and a sensing system to estimate the wrench applied to the fingertip based on the moment-equivalent point (MEP). We proposed the design to combine the branching tendon mechanism and the principle of wrench sensing based on the MEP. The proposed system realized the measurement of the wrench applied to the fingertip using a simple force sensor and a wire-driven system. Furthermore, we experimentally confirmed their functioning.},
  archive      = {J_RAS},
  author       = {Shouhei Shirafuji and Jun Ota},
  doi          = {10.1016/j.robot.2020.103538},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103538},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Development of a robotic finger with a branching tendon mechanism and sensing based on the moment-equivalent point},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new approach for the estimation of non-cooperative
satellites based on circular feature extraction. <em>RAS</em>,
<em>129</em>, 103532. (<a
href="https://doi.org/10.1016/j.robot.2020.103532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pose estimation of non-cooperative satellites has been a hot topic in the study of astronautics as the visual feedback will highly enhance the safety of on-orbit services. A stereo vision system is proposed in this paper. It works as an eye-to-hand vision camera in the final approach phase Based on circular feature extraction, a closed-form solution is presented. The position and orientation of the adapter ring can be figured out in real-time as well as the unknown radius. Neither additional sensors nor prior knowledge is required, and the orientation-duality problem has been solved. It works well on the partial ellipses and is robust to outliers, noise and occlusions. Experimental results on both synthetic and real images have demonstrated the effectiveness and efficiency of the proposed method.},
  archive      = {J_RAS},
  author       = {Yang Liu and Zongwu Xie and Qi Zhang and Xiaoyu Zhao and Hong Liu},
  doi          = {10.1016/j.robot.2020.103532},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103532},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A new approach for the estimation of non-cooperative satellites based on circular feature extraction},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robots and autonomous systems, SI DARS 2018. <em>RAS</em>,
<em>129</em>, 103530. (<a
href="https://doi.org/10.1016/j.robot.2020.103530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_RAS},
  author       = {Nikolaus Correll ( Editors ) and Mac Schwager},
  doi          = {10.1016/j.robot.2020.103530},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103530},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Robots and autonomous systems, SI DARS 2018},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust shape estimation with false-positive contact
detection. <em>RAS</em>, <em>129</em>, 103527. (<a
href="https://doi.org/10.1016/j.robot.2020.103527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a means of omni-directional contact detection using accelerometers instead of tactile sensors for object shape estimation using touch. Unlike tactile sensors , our contact-based detection method tends to induce a degree of uncertainty with false-positive contact data because the sensors may react not only to actual contact but also to the unstable behavior of the robot. Therefore, it is crucial to consider a robust shape estimation method capable of handling such false-positive contact data. To realize this, we introduce the concept of heteroscedasticity into the contact data and propose a robust shape estimation algorithm based on Gaussian process implicit surfaces (GPIS). We confirmed that our algorithm not only reduces shape estimation errors caused by false-positive contact data but also distinguishes false-positive contact data more clearly than the GPIS through simulations and actual experiments using a quadcopter.},
  archive      = {J_RAS},
  author       = {Kazuki Shibata and Tatsuya Miyano and Tomohiko Jimbo and Takamitsu Matsubara},
  doi          = {10.1016/j.robot.2020.103527},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103527},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Robust shape estimation with false-positive contact detection},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LiDAR-based vehicle localization on the satellite image via
a neural network. <em>RAS</em>, <em>129</em>, 103519. (<a
href="https://doi.org/10.1016/j.robot.2020.103519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel method to localize the vehicle on an easily accessible geo-referenced satellite image based on LiDAR . We first design a neural network to extract and compare the spatial-discriminative feature maps of the satellite image patch and the LiDAR points, and obtain the probability of correspondence. Then based on the outputs of the network, a particle filter is used to obtain the probability distribution of the vehicle pose. This method can use LiDAR points and any type of odometry as input to localize the vehicle. The experimental results show that our model can generalize well on several datasets. Compared with other methods, ours is more robust in some challenging scenarios such as the occluded or shadowed area on the satellite image.},
  archive      = {J_RAS},
  author       = {Mengyin Fu and Minzhao Zhu and Yi Yang and Wenjie Song and Meiling Wang},
  doi          = {10.1016/j.robot.2020.103519},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103519},
  shortjournal = {Robot. Auton. Syst.},
  title        = {LiDAR-based vehicle localization on the satellite image via a neural network},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal communication relay positioning in mobile multi-node
networks. <em>RAS</em>, <em>129</em>, 103517. (<a
href="https://doi.org/10.1016/j.robot.2020.103517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an optimal communication relay positioning method to improve the communication performance of mobile multi-node networks in complex environments. The communication channel quality prediction between nodes is of primary concern to find the optimal relay node positions while considering the uncertain and dynamic nature of environments. To this end, the learning-based or the distance model-based method is used for the channel prediction depending on the mobility of the communication nodes of interest. The global message connectivity and the worst case connectivity are introduced as the communication performance metric of networked agents. The optimal relay positions are found by maximizing the performance with respect to the relay positions through a heuristic optimization technique. This algorithm outperforms a recently-developed relay positioning algorithm in the simulations. The indoor experiments are conducted to show that the proposed approach using mobile relays improves the communication performance of the complex network significantly with the accurate channel prediction.},
  archive      = {J_RAS},
  author       = {Jongyun Kim and Pawel Ladosz and Hyondong Oh},
  doi          = {10.1016/j.robot.2020.103517},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103517},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Optimal communication relay positioning in mobile multi-node networks},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fine-grained action plausibility rating. <em>RAS</em>,
<em>129</em>, 103511. (<a
href="https://doi.org/10.1016/j.robot.2020.103511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An essential capability of humans is the effortless identification of useful tasks based on visual cues in everyday situations. Objects and their surroundings are integrated and processed to differentiate plausible from implausible actions. In this work, we study how to teach this ability to robots. In contrast to many tasks in computer vision where the goal is an accurate description (object labels, caption, scene class) of the present situation here the challenge is to make reasonable guesses about which forms of plausible and implausible actions can be conducted. To this end, we collect a dataset that associates images with probabilities over a set of actions. A convolutional neural network is trained to match these ground truth plausibility scores using this dataset. We compare the performance of state-of-the-art encoder architectures and specifically analyze the role of contextual cues quantitatively. While the object recognition capabilities of the encoder have a strong impact on performance, using context did not lead to substantial improvements. We show qualitatively the utility of such a system for robotic action selection in a household setting.},
  archive      = {J_RAS},
  author       = {Timo Lüddecke and Florentin Wörgötter},
  doi          = {10.1016/j.robot.2020.103511},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103511},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Fine-grained action plausibility rating},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tuning and sensitivity analysis of a hexapod state
estimator. <em>RAS</em>, <em>129</em>, 103509. (<a
href="https://doi.org/10.1016/j.robot.2020.103509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important envisaged application of legged robots is the exploration and mapping of extreme environments with an unknown terrain. Corin is a hexapod designed at the University of Manchester, which is able to perform motions using footholds on surfaces perpendicular to the ground plane. This allows it to be able to navigate through confined and narrow spaces. The hexapod requires an accurate estimate of its pose in order to be able to perform these motions. Current state-of-the-art state estimators for legged robots that solely use proprioceptive sensors, fuse inertial and leg kinematic measurements through an extended Kalman filter (EKF). This paper describes the implementation and validation of a state estimator on the Corin hexapod whilst performing motions using surface perpendicular to the ground plane. Another novelty of the work is the analysis of the algorithm sensitivity to the filter parameters and motion variables, and the tuning of the EKF using particle swarm optimisation (PSO). The results show that the average error achieved was below 6\% for both position and orientation estimates.},
  archive      = {J_RAS},
  author       = {Hassan H. Khalili and Wei Cheah and Tomas B. Garcia-Nathan and Joaquin Carrasco and Simon Watson and Barry Lennox},
  doi          = {10.1016/j.robot.2020.103509},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103509},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Tuning and sensitivity analysis of a hexapod state estimator},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Skill transfer learning for autonomous robots and
human–robot cooperation: A survey. <em>RAS</em>, <em>128</em>, 103515.
(<a href="https://doi.org/10.1016/j.robot.2020.103515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing a robot system with reasoning and learning ability has gradually become a research focus in robotics research field. Recently, Skill Transfer Learning (STL), i.e., the ability of transferring human skills to robots, has become a research thrust for autonomous robots and human–robot cooperation. It provides the following benefits: (i) the skill transfer learning system with independent decision-making and learning ability enables the robot to learn and acquire manipulation skills in a complex and dynamic environment, which can overcome the shortages of conventional methods such as traditional programming, and greatly improve the adaptability of the robot to complex environments and (ii) human physiological signals allow us to extract motion control characteristics from physiological levels which create a rich sensory signal. In this survey, we provide an overview of the most important applications of STL by analyzing and categorizing existing works in autonomous robots and human–robot cooperation area. We close this survey by discussing remaining open challenges and promising research topics in future.},
  archive      = {J_RAS},
  author       = {Yueyue Liu and Zhijun Li and Huaping Liu and Zhen Kan},
  doi          = {10.1016/j.robot.2020.103515},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103515},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Skill transfer learning for autonomous robots and human–robot cooperation: A survey},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A robust phase oscillator design for wearable robotic
systems. <em>RAS</em>, <em>128</em>, 103514. (<a
href="https://doi.org/10.1016/j.robot.2020.103514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design of a phase-based robust oscillator for wearable robots , that could assist humans performing periodic or repetitive tasks, is presented in this paper. The bounds on perturbations, that guaranteed the stability of the output for the phase oscillator controller, were identified and the Lyapunov redesign method was applied to construct a robust controller using a bounding function . The robust controller produced a bounded control signal to modify the amplitude and frequency of the resulting second-order oscillator to modulate the stiffness and damping properties . In this paper, the focus is on the mathematical modeling of the controller, its dynamic stability and robustness for human–robot application. The proposed approach was verified through a simple pendulum experiment. The results provided evidence that a better limit cycle, with a controlled radial spread of the steady state, was obtained with Lyapunov redesigned phase oscillator. Finally, the potential of the proposed approach for hip assistance in a healthy subject wearing HeSa (Hip Exoskeleton for Superior Assistance) during periodic activities are discussed with preliminary results.},
  archive      = {J_RAS},
  author       = {Juan De La Fuente and Susheelkumar C. Subramanian and Thomas G. Sugar and Sangram Redkar},
  doi          = {10.1016/j.robot.2020.103514},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103514},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A robust phase oscillator design for wearable robotic systems},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design of transfer learning structure for slot wedge
tightness inspection robot. <em>RAS</em>, <em>128</em>, 103507. (<a
href="https://doi.org/10.1016/j.robot.2020.103507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tightness inspection for the slot wedges is significant for the safe operation of large generators. One of the traditional methods is analysis of the acoustic signals of knocking on the surface of the slot wedge by inspection experts . Nowadays the slot wedge inspecting robot is an effective way to measure the tightness of the slot wedges and classify the level of the slot wedges into different groups. However, there are many types of generators and the precision cannot be guaranteed if the model of one type of generators is applied to another. Although the machine learning methods such as CNN (Convolutional Neural Networks) and RNN (Recurrent Neural Networks) are widely used for classification, they are not suitable for model transfer between different generators. In this paper, a transfer learning based structure is introduced to solve the problem and also the mixture of RNN and CNN is designed to fulfill the system. The structure is tested to transfer models with the acoustic signal sampled by the inspecting robot between the 500 MW and 600 MW generators. Experiment results show that the transfer learning structure can transfer models from one type of generators to another. Compared with the state-of-the-art methods, the proposed structure can improve the inspection precision by at least 36.7\% and obtain the average precision over 79.0\%.},
  archive      = {J_RAS},
  author       = {Wenbin Yu and Yingjie Zhao and Lu Ding and Lei Song and Dan Huang},
  doi          = {10.1016/j.robot.2020.103507},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103507},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Design of transfer learning structure for slot wedge tightness inspection robot},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-camera visual SLAM for off-road navigation.
<em>RAS</em>, <em>128</em>, 103505. (<a
href="https://doi.org/10.1016/j.robot.2020.103505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of computer vision , vision-based simultaneous localization and mapping (vSLAM) plays an increasingly important role in the field of unmanned driving. However, traditional SLAM methods based on a monocular camera only perform well in simple indoor environments or urban environments with obvious structural features. In off-road environments, the situation that SLAM encounters could be complicated by problems such as direct sunlight, leaf occlusion, rough roads, sensor failure, sparsity of stably trackable texture. Traditional methods are highly susceptible to these factors, which lead to compromised stability and reliability. To counter such problems, we propose a panoramic vision SLAM method based on multi-camera collaboration, aiming at utilizing the characters of panoramic vision and stereo perception to improve the localization precision in off-road environments. At the same time, the independence and information sharing of each camera in multi-camera system can improve its ability to resist bumps, illumination, occlusion and sparse texture in an off-road environment, and enable our method to recover the metric scale. These characters ensure unmanned ground vehicles (UGVs) to locate and navigate safely and reliably in complex off-road environments.},
  archive      = {J_RAS},
  author       = {Yi Yang and Di Tang and Dongsheng Wang and Wenjie Song and Junbo Wang and Mengyin Fu},
  doi          = {10.1016/j.robot.2020.103505},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103505},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Multi-camera visual SLAM for off-road navigation},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A scene comprehensive safety evaluation method based on
binocular camera. <em>RAS</em>, <em>128</em>, 103503. (<a
href="https://doi.org/10.1016/j.robot.2020.103503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent vehicle is an inevitable trend in urban transportation development. Whether for driver assistance systems or advanced driverless systems, safety issues are one of the cores of these systems. This paper proposes a scene safety evaluation method based on binocular camera. By proposing a comprehensive safety evaluation model, the uncertainty of the scene can be represented with a single value, which can be expressed as a quantitative evaluation of the safety of intended functionality (SOTIF). It provides real-time safety monitoring and protection for the driving process of drivers and occupants by the human–computer interaction. Based on the calculation results, different monitoring methods can be adopted for different levels of autonomous driving to further improve the safety of autonomous driving .},
  archive      = {J_RAS},
  author       = {Xinyu Zhang and Wenbo Shao and Mo Zhou and Qifan Tan and Jun Li},
  doi          = {10.1016/j.robot.2020.103503},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103503},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A scene comprehensive safety evaluation method based on binocular camera},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Grappling claws for a robot to climb rough wall surfaces:
Mechanical design, grasping algorithm, and experiments. <em>RAS</em>,
<em>128</em>, 103501. (<a
href="https://doi.org/10.1016/j.robot.2020.103501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wall-climbing robots have been widely applied in the inspection of smooth walls. However, only a few adhesion methods have been developed for robots that will allow them to climb cliffs and dusty, high-altitude, rough walls (constructed using coarse concrete, bricks, and stones, etc.) that may be subjected to vibrations. This paper proposes a suitable adhesion method that employs grappling-hook-like claws arranged in a cross shape. First, we address the implementation mechanism required. Then, a method of extracting the characteristic parameter is revealed rough wall was devised, 3D profiles of rough walls were simulated, and the discriminant conditions necessary for the claws to stably grasp the wall were provided. A method of triangulation is proposed to judge which regions of a 3D wall can be gripped, and we subsequently present a grasping discrimination algorithm for the interaction between the miniature claws and 3D wall profile. Finally, a prototype of the grappling-hook-like claw system was fabricated. A test platform was built to test the robot which incorporates an electromagnetic vibration shaker to simulate a vibrating wall. Experiments were then carried out on the robot using the vibrating wall and a random outdoor wall. The results verified the feasibility of the proposed claws and the validity of the discriminant algorithm for gripping 3D walls. Compared with traditional adhesion approaches, the proposed method (based on hook-like claws) is more adaptable to suit various types of wall. It also has higher resistance to disturbances and so provides a more reliable method of adhesion for robots on rough walls.},
  archive      = {J_RAS},
  author       = {Fengyu Xu and Fanchang Meng and Quansheng Jiang and Gaoliang Peng},
  doi          = {10.1016/j.robot.2020.103501},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103501},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Grappling claws for a robot to climb rough wall surfaces: Mechanical design, grasping algorithm, and experiments},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A minimal biologically-inspired algorithm for robots
foraging energy in uncertain environments. <em>RAS</em>, <em>128</em>,
103499. (<a href="https://doi.org/10.1016/j.robot.2020.103499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work details the design and simulation results of a bioinspired minimalist algorithm based on C. elegans , using autonomous agents to forage for attractant energy sources. The robotic agents are energy-constrained and depend on the energy they forage to recharge their batteries , which is significant as the foraging task is one of the canonical testbeds for cooperative robotics. The algorithm consists of 6 input parameters which were simulated and optimised in 9 unbounded environments of varying difficulty levels, containing attractant sources which robots would then have to forage from to maintain energy levels and survive the entirety of the simulation. The robots running the algorithm were then optimised using Evolutionary Algorithms and the best solutions in all 9 environments were categorised with the use of clustering techniques . The clustering results highlighted the different strategies which emerged. Ultimately across the 9 environments, 6 different strategies have been identified. The results demonstrate the applicability of the proposed algorithm to localise attractant sources and harvest energy in different scenarios using the same core algorithm.},
  archive      = {J_RAS},
  author       = {Gabriela R. Andrade and Jordan H. Boyle},
  doi          = {10.1016/j.robot.2020.103499},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103499},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A minimal biologically-inspired algorithm for robots foraging energy in uncertain environments},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Coupling humanoid walking pattern generation and visual
constraint feedback for pose-regulation and visual path-following.
<em>RAS</em>, <em>128</em>, 103497. (<a
href="https://doi.org/10.1016/j.robot.2020.103497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we show how visual constraints such as homographies and fundamental matrices can be integrated tightly into the locomotion controller of a humanoid robot to drive it from one configuration to another (pose-regulation), only by means of images. The visual errors generated by these constraints are stacked as terms of the objective function of a Quadratic Program so as to specify the final pose of the robot with a reference image. By using homographies or fundamental matrices instead of specific points, we avoid the features occlusion problem . This image-based strategy is also extended to solve the problem of following a visual path by a humanoid robot , which allows the robot to execute much longer paths and plans than when using just one reference image. The effectiveness of our approach is validated with a humanoid dynamic simulator.},
  archive      = {J_RAS},
  author       = {Noé G. Aldana-Murillo and Luis Sandoval and Jean-Bernard Hayet and Claudia Esteves and Hector M. Becerra},
  doi          = {10.1016/j.robot.2020.103497},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103497},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Coupling humanoid walking pattern generation and visual constraint feedback for pose-regulation and visual path-following},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Wheeled motion kinematics and control of a hybrid mobility
CENTAURO robot. <em>RAS</em>, <em>128</em>, 103482. (<a
href="https://doi.org/10.1016/j.robot.2020.103482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Legged-wheeled robots combine the advantages of efficient wheeled mobility with the capability of adapting to real-world terrains through the legged locomotion. Thanks to their hybrid mobility skill, they can excel in many application scenarios where other mobile platforms are not suitable for. However, the improved versatility of their mobility increases the number of constraints in their motion control, where both the properties of legged and wheeled functionalities need to be considered. Relevant schemes for legged-wheeled motion control so far have attempted to address the problem by exploiting separate motion control of the wheeled and legged functionalities. The contribution of this paper is the introduction of derivation of the legged-wheeled motion kinematics without constraining the camber angles of the wheels. To this end, the wheel geometry is approximated by torus that more precisely represents a real wheel geometry than a standard sphere/cylinder. On the basis of the derived legged-wheeled motion kinematics, a first-order inverse kinematics (IK) scheme that resolves the legged-wheeled robot whole-body motion respecting the wheel rolling constraint is described. Furthermore, a higher-level method to resolve wheel steering to comply with a non-holonomic constraint is designed. A damping scheme is proposed to handle a structural singularity when a system non-holonomy deteriorates. Finally, the work adopts a floating base model that allows to easily incorporate the legged motion into the proposed scheme. The developed control scheme is tested in experiments on a legged-wheeled centaur-like robot — CENTAURO.},
  archive      = {J_RAS},
  author       = {Małgorzata Kameduła and Navvab Kashiri and Nikos G. Tsagarakis},
  doi          = {10.1016/j.robot.2020.103482},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103482},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Wheeled motion kinematics and control of a hybrid mobility CENTAURO robot},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Human-in-the-loop optimization of wearable robots to reduce
the human metabolic energy cost in physical movements. <em>RAS</em>,
<em>127</em>, 103495. (<a
href="https://doi.org/10.1016/j.robot.2020.103495">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most designs of wearable robots are based on human biomechanical statistics, engineering experience or individual experiments. Despite great successes, few of them consider the human–robot integration and individual differences between users. Additionally, the design periods, cost and safety also need to be further improved. Learning from the natural driving mechanism of human body, we propose a general human-in-the-loop (HIL) optimization designing approach for this kind of wearable robots . Firstly, the human–robot coupling model of the personalized wearable robot and the human musculoskeletal model are established. Then, the Computed Muscle Control (CMC) tool embedded in software OpenSim and the Bayesian optimization used in machine learning are combined to find the optimal design scheme for the personalized wearable robots to reduce the human metabolic energy cost in specific physical movement. The HIL approach could not only optimize the control parameters of wearable robots, but also optimize their geometry, material and any other design parameters flexibly and effectively. An application example for the HIL approach is also provided to help designers better understand and use the HIL method proposed in this paper.},
  archive      = {J_RAS},
  author       = {Jing Fang and Yuan Yuan},
  doi          = {10.1016/j.robot.2020.103495},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103495},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Human-in-the-loop optimization of wearable robots to reduce the human metabolic energy cost in physical movements},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Development of 3UPU-i parallel sensor with six
division-force limbs for measuring robotic wrist load. <em>RAS</em>,
<em>127</em>, 103486. (<a
href="https://doi.org/10.1016/j.robot.2020.103486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A 3UPU-I parallel sensor with six division-force limbs and six standard force sensors is developed for measuring robotic wrist load. Its measuring approach and performances are studied and evaluated. A prototype of the developed parallel sensor is built up and its merits are analyzed. A statics equation among the forces of the six standard force sensors and the wrist load is established, and a mapped matrix from the workload to the forces of the six standard force sensors is derived based on a 3UPU-I parallel mechanism of the developed parallel sensor. The performances of the developed parallel sensor are analyzed and evaluated by respectively varying key parameters for constructing the developed parallel sensor, and the reasonable values of the key parameters are determined. The forces of the six standard force sensors are measured by adding different workload components onto the loaded platform of the prototype. Finally, some theoretical solutions of the developed parallel sensor are solved and verified by the FE simulation solutions of the developed parallel sensor. The experimental calibration solutions of the prototype are coincident with the theoretical solutions.},
  archive      = {J_RAS},
  author       = {Yi Lu and Yongli Wang and Yang Lu},
  doi          = {10.1016/j.robot.2020.103486},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103486},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Development of 3UPU-I parallel sensor with six division-force limbs for measuring robotic wrist load},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robot-assisted intervention for children with special needs:
A comparative assessment for autism screening. <em>RAS</em>,
<em>127</em>, 103484. (<a
href="https://doi.org/10.1016/j.robot.2020.103484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the increment of researches related to Social Assistive Robotics (SAR), achieving a plausible Robot-Assisted Diagnosis (RAD) for Children with Autism Spectrum Disorders (CwASD) remains a considerable challenge to the clinical and robotics community. The work of specialists regarding ASD diagnosis is hard and labor-intensive due to the condition’s manifestations are inherently heterogeneous and makes the process more difficult. Besides, the aforementioned complexity may be the main reason for the slow progress in the development of SAR with diagnostic purposes . Thus, this work provides a comprehensive Robot-Assisted Intervention for CwASD showing the conditions in which a Robot-based approach can be useful to assess autism risk factors for an autism diagnosis purpose. The intervention scheme consists of an improved version of a multimodal environment for Robot-based intervention proposed in our previous work. More specifically, we compared the behavior of CwASD with that of children in a control group during a human/robot-mediated intervention while Joint Attention (JA) behaviors are elicited and analyzed. Through statistical data analysis, it was possible to identify that 17 out of 23 children of the CwASD group showed a different behavior pattern related to three characteristics of autism, which suggests that this pattern can be used to identify autism risk factors through Robot-based interventions.},
  archive      = {J_RAS},
  author       = {Andrés A. Ramírez-Duque and Teodiano Bastos and Marcela Munera and Carlos A. Cifuentes and Anselmo Frizera-Neto},
  doi          = {10.1016/j.robot.2020.103484},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103484},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Robot-assisted intervention for children with special needs: A comparative assessment for autism screening},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybrid strategy based model parameter estimation of
irregular-shaped underwater vehicles for predicting velocity.
<em>RAS</em>, <em>127</em>, 103480. (<a
href="https://doi.org/10.1016/j.robot.2020.103480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hydrodynamic model can be used to predict velocity of underwater vehicles in still water. However, there are few economical and effective methods for estimating the hydrodynamic parameters of irregular-shaped underwater vehicles. Thus, this paper proposes a hybrid estimation strategy, which contains a rough estimation using a least squares (LS) based algorithm and a precise estimation using an improved particle swarm optimization (IPSO) algorithm. The numerical simulation and field data based tests suggest that the accuracy of the predicted velocity using the hydrodynamic parameters estimated by the IPSO-based hybrid strategy is better than two state-of-the-art algorithms. Finally, a pool experiment is conducted to verify the accuracy of the predicted horizontal velocity of the underwater vehicle.},
  archive      = {J_RAS},
  author       = {Mingwei Lin and Canjun Yang and Dejun Li},
  doi          = {10.1016/j.robot.2020.103480},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103480},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Hybrid strategy based model parameter estimation of irregular-shaped underwater vehicles for predicting velocity},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DeepGoal: Learning to drive with driving intention from
human control demonstration. <em>RAS</em>, <em>127</em>, 103477. (<a
href="https://doi.org/10.1016/j.robot.2020.103477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent research on automotive driving has developed an efficient end-to-end learning mode that directly maps visual input to control commands. However, it models distinct driving variations in a single network, which increases learning complexity and is less adaptive for modular integration. In this paper, we re-investigate human’s driving style and propose to learn an intermediate driving intention region to relax the difficulties in end-to-end approach. The intention region follows both road structure in image and direction towards goal in public route planner, which addresses visual variations only and figures out where to go without conventional precise localization . Then the learned visual intention is projected on vehicle local coordinate and fused with reliable obstacle perception to render a navigation score map that is widely used for motion planning. The core of the proposed system is a weakly-supervised cGAN-LSTM model trained to learn driving intention from human demonstration. The adversarial loss learns from limited demonstration data with one local planned route and enables reasoning of multi-modal behaviors with diverse routes while testing. Comprehensive experiments are conducted with real-world datasets. Results indicate the proposed paradigm can produce more consistent motion commands with human demonstration and shows better reliability and robustness to environment change. Our code is available at https://github.com/HuifangZJU/visual-navigation .},
  archive      = {J_RAS},
  author       = {Huifang Ma and Yue Wang and Rong Xiong and Sarath Kodagoda and Li Tang},
  doi          = {10.1016/j.robot.2020.103477},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103477},
  shortjournal = {Robot. Auton. Syst.},
  title        = {DeepGoal: Learning to drive with driving intention from human control demonstration},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning robots to grasp by demonstration. <em>RAS</em>,
<em>127</em>, 103474. (<a
href="https://doi.org/10.1016/j.robot.2020.103474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, we have witnessed the proliferation of so-called collaborative robots or cobots, that are designed to work safely along with human operators. These cobots typically use the “program from demonstration” paradigm to record and replay trajectories, rather than the traditional source-code based programming approach. While this requires less knowledge from the operator, the basic functionality of a cobot is limited to simply replay the sequence of actions as they were recorded. In this paper, we present a system that mitigates this restriction and learns to grasp an arbitrary object from visual input using demonstrated examples. While other learning-based approaches for robotic grasping require collecting a large amount of examples, either manually or automatically harvested in a real or simulated world, our approach learns to grasp from a single demonstration with the ability to improve on accuracy using additional input samples. We demonstrate grasping of various objects with the Franka Panda collaborative robot. We show that the system is able to grasp various objects from demonstration, regardless their position and rotation in less than 5 min of training time on a NVIDIA Titan X GPU , achieving over 90\% average success rate.},
  archive      = {J_RAS},
  author       = {Elias De Coninck and Tim Verbelen and Pieter Van Molle and Pieter Simoens and Bart Dhoedt},
  doi          = {10.1016/j.robot.2020.103474},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103474},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Learning robots to grasp by demonstration},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model predictive control without terminal constraints or
costs for holonomic mobile robots. <em>RAS</em>, <em>127</em>, 103468.
(<a href="https://doi.org/10.1016/j.robot.2020.103468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate Model Predictive Control (MPC) schemes without stabilizing constraints or costs for the set-point stabilization of holonomic mobile robots. Herein, we ensure closed-loop asymptotic stability using the concept of cost controllability. To this end, we derive a growth bound on the finite-horizon value function in terms of the running costs evaluated at the current state, which is then used to determine a stabilizing prediction horizon. In the discrete-time setting, we additionally show that asymptotic stability holds for the shortest possible prediction horizon. Moreover, we deduce a lower bound on the MPC performance on the infinite horizon. Theoretical results are verified by numerical simulations as well as laboratory experiments of stabilizing a holonomic mobile robot to a reference set point.},
  archive      = {J_RAS},
  author       = {Mohamed W. Mehrez and Karl Worthmann and Joseph P.V. Cenerini and Mostafa Osman and William W. Melek and Soo Jeon},
  doi          = {10.1016/j.robot.2020.103468},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103468},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Model predictive control without terminal constraints or costs for holonomic mobile robots},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Roombots extended: Challenges in the next generation of
self-reconfigurable modular robots and their application in adaptive and
assistive furniture. <em>RAS</em>, <em>127</em>, 103467. (<a
href="https://doi.org/10.1016/j.robot.2020.103467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents a series of demonstrations of our self-reconfigurable modular robots (SRMR) “Roombots” in the context of adaptive and assistive furniture. In literature, simulations are often ahead of what currently can be demonstrated in hardware with such systems due to significant challenges in transferring them to the real world. Here, we describe how Roombots tackled these difficulties in real hardware and focus qualitatively on selected hardware experiments rather than on quantitative measurements (in hardware and simulation) to showcase the many possibilities of an SRMR. We envision Roombots to be used in our living space and define five key tasks that such a system must possess. Consequently, we demonstrate these tasks, including self-reconfiguration with 12 modules (36 Degrees of Freedom), autonomously moving furniture, object manipulation and gripping capabilities, human-module-interaction and the development of an easy-to-use user interface. We conclude with the remaining challenges and point out possible directions of research for the future of adaptive and assistive furniture with Roombots.},
  archive      = {J_RAS},
  author       = {S. Hauser and M. Mutlu and P.-A. Léziart and H. Khodr and A. Bernardino and A.J. Ijspeert},
  doi          = {10.1016/j.robot.2020.103467},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103467},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Roombots extended: Challenges in the next generation of self-reconfigurable modular robots and their application in adaptive and assistive furniture},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Selective grasp in occluded space by all-around proximity
perceptible finger. <em>RAS</em>, <em>127</em>, 103464. (<a
href="https://doi.org/10.1016/j.robot.2020.103464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of this research is to develop a “Selective Grasp” system with which robots can grasp and identify the target object even in occluded environments. In pursuit of this goal, we first develop a robot hand on which proximity sensors are mounted all around. In addition to the development, we propose a sensor model of the robot hand. By using the sensor model, robots can estimate the distance to the object and calibrate the sensors. With our robot hand, robots can accurately recognize their surroundings without touch. Secondly, we propose an approach in which robots can memorize spatial information of surroundings by building an environment map. The building map motion is generated by a combination of manipulation primitives based on proximity sensors . Thirdly, we propose a grasp planning method and an object shape classification method based on the environment map. By these methods, robots can grasp objects and classify shapes of the objects in occluded spaces. Lastly, we conduct real robot experiments , through which we validate the effectiveness of our proposed Selective Grasp system.},
  archive      = {J_RAS},
  author       = {Naoya Yamaguchi and Shun Hasegawa and Masaki Murooka and Kei Okada and Masayuki Inaba},
  doi          = {10.1016/j.robot.2020.103464},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103464},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Selective grasp in occluded space by all-around proximity perceptible finger},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stochastic triangular mesh mapping: A terrain mapping
technique for autonomous mobile robots. <em>RAS</em>, <em>127</em>,
103449. (<a href="https://doi.org/10.1016/j.robot.2020.103449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For mobile robots to operate autonomously in general environments, perception is required in the form of a dense metric map. For this purpose, we present the stochastic triangular mesh (STM) mapping technique: a 2.5-D representation of the surface of the environment using a continuous mesh of triangular surface elements, where each surface element models the mean plane and roughness of the underlying surface. In contrast to existing mapping techniques, an STM map models the structure of the environment by ensuring a continuous model, while also being able to be incrementally updated with linear computational cost in the number of measurements. We reduce the effect of uncertainty in the robot pose (position and orientation) by using landmark-relative submaps. The uncertainty in the measurements and robot pose are accounted for by the use of Bayesian inference techniques during the map update. We demonstrate that an STM map can be used with sensors that generate point measurements, such as stochastic triangular mesh (LiDAR) sensors and stereo cameras . We show that an STM map is a more accurate model than the only comparable online surface mapping technique – a standard elevation map – and we also provide qualitative results on practical datasets.},
  archive      = {J_RAS},
  author       = {Clint D. Lombard and Corné E. van Daalen},
  doi          = {10.1016/j.robot.2020.103449},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103449},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Stochastic triangular mesh mapping: A terrain mapping technique for autonomous mobile robots},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Subject-specific compliance control of an upper-limb
bilateral robotic system. <em>RAS</em>, <em>126</em>, 103478. (<a
href="https://doi.org/10.1016/j.robot.2020.103478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new compliance control strategy on a robot-assisted bilateral upper limb rehabilitation system. The robotic compliance regulation was achieved by modifying predefined training trajectories in real time, based on measured human–robot interaction force and human users’ position within subject-specific workspace. Experimental data were collected from healthy subjects, and indicate that human users can follow predefined training trajectories well under real-time compliance variation, with the maximum normalized root mean square error (NRMSE) value no greater than 1.44\%. Preliminary findings are encouraging, which demonstrates the availability of the proposed subject-specific compliance adaptation strategy, and the potential with enhanced training safety and efficacy. Future work will consider conducting a direct comparison between a bilateral upper limb rehabilitation device (BULReD)-assisted compliance training with or without subject-specific adaptation on a large sample of participants with upper limb disabilities.},
  archive      = {J_RAS},
  author       = {Qing Miao and Yuxin Peng and Li Liu and Andrew McDaid and Mingming Zhang},
  doi          = {10.1016/j.robot.2020.103478},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103478},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Subject-specific compliance control of an upper-limb bilateral robotic system},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deploying MAVs for autonomous navigation in dark underground
mine environments. <em>RAS</em>, <em>126</em>, 103472. (<a
href="https://doi.org/10.1016/j.robot.2020.103472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Operating Micro Aerial Vehicles (MAVs) in subterranean environments is becoming more and more relevant in the field of aerial robotics. Despite the large spectrum of technological advances in the field, flying in such challenging environments is still an ongoing quest that requires the combination of multiple sensor modalities like visual/thermal cameras as well as 3D and 2D lidars. Nevertheless, there exist cases in subterranean environments where the aim is to deploy fast and lightweight aerial robots for area reckoning purposes after an event (e.g. blasting in production areas). This work proposes a novel baseline approach for the navigation of resource constrained robots, introducing the aerial underground scout, with the main goal to rapidly explore unknown areas and provide a feedback to the operator. The main proposed framework focuses on the navigation, control and vision capabilities of the aerial platforms with low-cost sensor suites, contributing significantly towards real-life applications. The merit of the proposed control architecture is that it considers the flying platform as a floating object, composing a velocity controller on the x x , y y axes and altitude control to navigate along the tunnel . Two novel approaches make up the cornerstone of the proposed contributions for the task of navigation: (1) a vector geometry method based on 2D lidar , and (2) a Deep Learning (DL) method through a classification process based on an on-board image stream, where both methods correct the heading towards the center of the mine tunnel . Finally, the framework has been evaluated in multiple field trials in an underground mine in Sweden.},
  archive      = {J_RAS},
  author       = {Sina Sharif Mansouri and Christoforos Kanellakis and Dariusz Kominiak and George Nikolakopoulos},
  doi          = {10.1016/j.robot.2020.103472},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103472},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Deploying MAVs for autonomous navigation in dark underground mine environments},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Loop closure detection using supervised and unsupervised
deep neural networks for monocular SLAM systems. <em>RAS</em>,
<em>126</em>, 103470. (<a
href="https://doi.org/10.1016/j.robot.2020.103470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of true loop closure in Visual Simultaneous Localization And Mapping (vSLAM) can help in many ways, it helps in re-localization, improves the accuracy of the map, and helps in registration algorithms to obtain more accurate and consistent results. The loop closure detection is affected by many parameters, including illumination conditions , seasons, different viewpoints and mobile objects. This paper proposes a novel approach based on super dictionary different from traditional BoW dictionary that uses more advanced and more abstract features of deep learning . The proposed approach does not need to generate vocabulary, which makes it memory efficient and instead it stores exact features, which are small in number and hold very less amount of memory as compared to traditional BoW approach in which each frame holds the same amount of memory as the number of words in the vocabulary. Two deep neural networks are used together to speed up the loop closure detection and to ignore the effect of mobile objects on loop closure detection. We have compared the results with most popular Bag of Words methods DBoW2 and DBoW3, and state-of-the-art iBoW-LCD using five publicly available datasets, and the results show that the proposed method robustly performs loop closure detection and is eight times faster than the state-of-the-art approaches of a similar kind.},
  archive      = {J_RAS},
  author       = {Azam Rafique Memon and Hesheng Wang and Abid Hussain},
  doi          = {10.1016/j.robot.2020.103470},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103470},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Loop closure detection using supervised and unsupervised deep neural networks for monocular SLAM systems},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Autonomous navigation for UAVs managing motion and sensing
uncertainty. <em>RAS</em>, <em>126</em>, 103455. (<a
href="https://doi.org/10.1016/j.robot.2020.103455">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a motion planner for the autonomous navigation of UAVs that manages motion and sensing uncertainty at planning time. By doing so, optimal paths in terms of probability of collision, traversal time and uncertainty are obtained. Moreover, our approach takes into account the real dimensions of the UAV in order to reliably estimate the probability of collision from the predicted uncertainty. The motion planner relies on a graduated fidelity state lattice and a novel multi-resolution heuristic which adapt to the obstacles in the map. This allows managing the uncertainty at planning time and yet obtaining solutions fast enough to control the UAV in real time. Experimental results show the reliability and the efficiency of our approach in different real environments and with different motion models. Finally, we also report planning results for the reconstruction of 3D scenarios, showing that with our approach the UAV can obtain a precise 3D model autonomously.},
  archive      = {J_RAS},
  author       = {Adrián González-Sieira and Daniel Cores and Manuel Mucientes and Alberto Bugarín},
  doi          = {10.1016/j.robot.2020.103455},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103455},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Autonomous navigation for UAVs managing motion and sensing uncertainty},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Research on optimized time-synchronous online trajectory
generation method for a robot arm. <em>RAS</em>, <em>126</em>, 103453.
(<a href="https://doi.org/10.1016/j.robot.2020.103453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on sensor input, a robot arm should dynamically adjust its trajectory while maintaining stability to react to a sudden change in the target point in an unknown environment. To solve this problem, in this study, a time-optimized online trajectory generation (OTG) method is proposed using an S-curve velocity profile , which can generate trajectories in response to external sensor signals. The generated trajectory has characteristics that can guarantee the synchronization of multijoints according to an arbitrary initial state and a desired target state under velocity, acceleration, and jerk constraints. For multijoints time synchronization, two different characteristics are considered according to different application scenarios: minimum velocity or peak acceleration , which correspond to two sub-methods. The first one can be used to calculate with a minimum velocity peak, which can quickly adjust the trajectory according to the signal feedback. The second can be used to calculate the minimization of the acceleration peak, which can reduce the vibration of the robot arm due to a change in the motion. Compared with other OTG methods, the second proposed sub-method can effectively reduce the acceleration peak of the planned motion with the same synchronization time and parameters. Additionally, both sub-methods have the advantage of a rapid calculation and can generate time synchronization motion trajectories for 6 axes in 0.21 ms on a personal computer, fully satisfying the requirements of online motion planning. Finally, the effectiveness of the algorithm is verified by simulations and experiments with a lab-developed robot arm.},
  archive      = {J_RAS},
  author       = {Mingli Wang and Juliang Xiao and Fan Zeng and Guodong Wang},
  doi          = {10.1016/j.robot.2020.103453},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103453},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Research on optimized time-synchronous online trajectory generation method for a robot arm},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). I-support: A robotic platform of an assistive bathing robot
for the elderly population. <em>RAS</em>, <em>126</em>, 103451. (<a
href="https://doi.org/10.1016/j.robot.2020.103451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we present a prototype integrated robotic system , the I-Support bathing robot, that aims at supporting new aspects of assisted daily-living activities on a real-life scenario. The paper focuses on describing and evaluating key novel technological features of the system, with the emphasis on cognitive human–robot interaction modules and their evaluation through a series of clinical validation studies. The I-Support project on its whole has envisioned the development of an innovative, modular, ICT-supported service robotic system that assists frail seniors to safely and independently complete an entire sequence of physically and cognitively demanding bathing tasks, such as properly washing their back and their lower limbs. A variety of innovative technologies have been researched and a set of advanced modules of sensing, cognition, actuation and control have been developed and seamlessly integrated to enable the system to adapt to the target population abilities. These technologies include: human activity monitoring and recognition, adaptation of a motorized chair for safe transfer of the elderly in and out the bathing cabin, a context awareness system that provides full environmental awareness, as well as a prototype soft robotic arm and a set of user-adaptive robot motion planning and control algorithms. This paper focuses in particular on the multimodal action recognition system, developed to monitor, analyze and predict user actions with a high level of accuracy and detail in real-time, which are then interpreted as robotic tasks. In the same framework, the analysis of human actions that have become available through the project’s multimodal audio–gestural dataset, has led to the successful modeling of Human–Robot Communication, achieving an effective and natural interaction between users and the assistive robotic platform . In order to evaluate the I-Support system, two multinational validation studies were conducted under realistic operating conditions in two clinical pilot sites. Some of the findings of these studies are presented and analyzed in the paper, showing good results in terms of: (i) high acceptability regarding the system usability by this particularly challenging target group, the elderly end-users, and (ii) overall task effectiveness of the system in different operating modes.},
  archive      = {J_RAS},
  author       = {A. Zlatintsi and A.C. Dometios and N. Kardaris and I. Rodomagoulakis and P. Koutras and X. Papageorgiou and P. Maragos and C.S. Tzafestas and P. Vartholomeos and K. Hauer and C. Werner and R. Annicchiarico and M.G. Lombardi and F. Adriano and T. Asfour and A.M. Sabatini and C. Laschi and M. Cianchetti and A. Güler and I. Kokkinos and B. Klein and R. López},
  doi          = {10.1016/j.robot.2020.103451},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103451},
  shortjournal = {Robot. Auton. Syst.},
  title        = {I-support: A robotic platform of an assistive bathing robot for the elderly population},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Development of a virtual reality simulator for a strategy
for coordinating cooperative manipulator robots using cloud computing.
<em>RAS</em>, <em>126</em>, 103447. (<a
href="https://doi.org/10.1016/j.robot.2020.103447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article deals with the development of a simulator that recreates, in virtual reality, a team of Selectively Compliance Assembly Robot Arms (SCARA). This team works cooperatively to fulfill the task of stacking rectangular objects coordinated through a strategy that includes a cloud server responsible for communication between the robots. The execution of the task is based on a leader/follower configuration. In this configuration, the leader performs a computed trajectory constantly reporting its position to the remote server. The remote server, in turn, sends this information back to the follower so this can follow the leader. The application combines MatLab® and Java. The latter is specifically used for communication routines, since its versatility makes it easy to be incorporated into any type of machine. This paper seeks to demonstrate the advantages of incorporating cloud resources into a multi-robot system and how its performance can be tested by means of the application developed.},
  archive      = {J_RAS},
  author       = {Claudio Urrea and Rodrigo Matteoda},
  doi          = {10.1016/j.robot.2020.103447},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103447},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Development of a virtual reality simulator for a strategy for coordinating cooperative manipulator robots using cloud computing},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cable driven exoskeleton for upper-limb rehabilitation: A
design review. <em>RAS</em>, <em>126</em>, 103445. (<a
href="https://doi.org/10.1016/j.robot.2020.103445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the primary reasons for long-term disabilities in the world is strokes. The causes of these cerebrovascular diseases are various, i.e., high blood pressure, heart disease, etc. For those who survive strokes, this affectation causes loss in the mobility of extremities, requiring the intervention of long sessions with a therapeutic professional to recover the movement of the impair limb. Hence, the investment to threaten this condition is usually high, motivating researchers to implements exoskeletons in the rehabilitation process. Those devices permit the user means to conduct the therapies without the constant supervision of a professional. Furthermore, exoskeletons are capable of maintaining a detailed recording of the forces and movements developed for the patients throughout the session. However, the construction of an exoskeleton is not cheap principally for the actuation systems , especially if the exoskeleton requires the actuator to be placed at the joints of the user; in which, the actuator at a joint would have to withstand the load of the actuator of the following joint and so on. Researchers have addressed this drawback by applying cable transmission systems that allow the exoskeleton to place their actuator at a fixed base, reducing the weight of their design, and decreasing their cost. Thus, this paper reviews the principal models of cable-driven exoskeleton for stroke rehabilitation focusing on the upper-limb. The analysis departs from the study of the anatomy of the arm, including the shoulder, elbow, wrist, fingers, and thumb. Besides, it also includes the mechanical consideration to design a proper exoskeleton. Then, the article presents a compendium of the different transmission systems found in the literature, addressing their advantages, disadvantages and their requirements for the design. Lastly, the paper reviews the cable-driven exoskeleton for stroke rehabilitation of the upper limb. Again, for this analysis, it is included the design consideration of each prototype, focusing on their advantages in terms of anatomical mechanics.},
  archive      = {J_RAS},
  author       = {J.D. Sanjuan and A.D. Castillo and M.A. Padilla and M.C. Quintero and E.E. Gutierrez and I.P. Sampayo and J.R. Hernandez and M.H. Rahman},
  doi          = {10.1016/j.robot.2020.103445},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103445},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Cable driven exoskeleton for upper-limb rehabilitation: A design review},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Understanding a public environment via continuous robot
observations. <em>RAS</em>, <em>126</em>, 103443. (<a
href="https://doi.org/10.1016/j.robot.2020.103443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a study on a point cloud analysis captured by a robot navigating in a shopping mall environment. It investigates the type and how much information the robot could extract from the environment. For this purpose, information regarding environmental changes and the number of people in shops was extracted and analyzed. First, the robot was manually controlled to collect data in a typical shopping mall having different types of shops and a food court. As the robot navigated thoroughly around the environment, seven data recordings of data obtained from various onboard sensors were recorded during afternoon hours over three consecutive days. We built a composite map by overlaying 3D point clouds for each recording sharing the same coordinate frame, which reveals the changes in the environment’s static objects. The number of humans at each shop in each recording was computed using a human tracker. Then, we computed a fourteen-dimensional vector for each shop: seven dimensions for environmental changes and seven for human density. Experimental results show that the environmental changes and the human density at each shop are consistent with the visual changes that occurred in the shops and the number of people who visited the shops. Correlation analysis was done among shop changes, shop open space, and human density where results suggest that change in shop configurations are often done in smaller shops and shops with larger open space tend to attract larger number of customers. Finally, information extracted from shops was used to categorize the shops according to similarity.},
  archive      = {J_RAS},
  author       = {Deneth Karunarathne and Yoichi Morales and Takayuki Kanda and Hiroshi Ishiguro},
  doi          = {10.1016/j.robot.2020.103443},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103443},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Understanding a public environment via continuous robot observations},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DWnet: Deep-wide network for 3D action recognition.
<em>RAS</em>, <em>126</em>, 103441. (<a
href="https://doi.org/10.1016/j.robot.2020.103441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Action recognition plays an important role in human–robot cooperation and interaction. By recognizing human actions, robots can imitate or reproduce human actions and obtain skills. Recently, convolutional neural networks (CNNs) have been widely used to recognize actions based on 3D skeleton. Good performance has been achieved due to the approximation capability gained from the depth of the model. Unfortunately, in the mainstream deep structures, dropout and fully connected layers are usually used to classify actions. That is to say, ensemble is used to guarantee the recognition performance, which decreases the computational efficiency. In order to improve the computational efficiency, we propose in this paper a deep-wide network (DWnet) to recognize human actions based on 3D skeleton. Specifically, we modify the decision-making mechanism of the deep CNN with a shallow structure, which improves the computational efficiency. The state-of-the-art deep CNN is used to extract spatial–temporal features from the skeletal sequence. Then features are transformed into a higher dimensional feature space to obtain global information and classified by the modified decision making mechanism. Experiments on two skeletal datasets demonstrate the advantage of the proposed model on testing efficiency and the effectiveness of the novel model to recognize the action. The code has been publicly available at https://github.com/YHDang/DWnet .},
  archive      = {J_RAS},
  author       = {Yonghao Dang and Fuxing Yang and Jianqin Yin},
  doi          = {10.1016/j.robot.2020.103441},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103441},
  shortjournal = {Robot. Auton. Syst.},
  title        = {DWnet: Deep-wide network for 3D action recognition},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semantic approach to RIoT autonomous robots mission
coordination. <em>RAS</em>, <em>126</em>, 103438. (<a
href="https://doi.org/10.1016/j.robot.2020.103438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Internet of Things (IoT) has recently become the key for innovation and progress in many industrial sectors and scientific areas. However, it brings many challenges and issues, such as growing number of connected devices, heterogeneity, amount of generated data, security and privacy issues, interoperability and many others. Since devices not only collect data, but also take actions that affect the environment, device coordination in the context of IoT systems is becoming more and more important, especially if the IoT convergence with robotics, known as “Internet of Robotic Things” (RIoT), is taken into consideration. In novel cyber–physical systems coordination is very important for situations when many devices working parallel have higher potential to achieve the given task more effectively, than a single device operating independently. RIoT experimentation testbeds facilitate development of such cyber–physical systems where devices need to be aware of the environment while interacting with other devices in order to achieve a common goal. In this paper, we propose a semantic-driven framework for automated autonomous robots coordination in the context of RIoT-based experimentation testbeds . Framework for automatic coordinated mission generation within the robotics experimentation platform testbed is evaluated. Results of evaluation are presented and discussed.},
  archive      = {J_RAS},
  author       = {Valentina Nejkovic and Nenad Petrovic and Milorad Tosic and Nenad Milosevic},
  doi          = {10.1016/j.robot.2020.103438},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103438},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Semantic approach to RIoT autonomous robots mission coordination},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Solving the area coverage problem with UAVs: A vehicle
routing with time windows variation. <em>RAS</em>, <em>126</em>, 103435.
(<a href="https://doi.org/10.1016/j.robot.2020.103435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real life, providing security for a set of large areas by covering the areas with Unmanned Aerial Vehicles (UAVs) is a difficult problem that consists of multiple objectives. These difficulties are even greater if the area coverage has to be sustained through a specific time window. We address this by considering a Vehicle Routing Problem with a Time Windows (VRPTW) variation in which the capacity of agents is counted as one and each customer (target area) is to be supplied with more than one vehicle simultaneously and without violating time windows. In this problem, our aim is to find a way to cover all areas with the necessary number of UAVs during the time windows, while minimizing the total distance traveled, and providing a fast solution by satisfying the additional constraint that each agent has limited fuel. We present a novel algorithm that relies on clustering the target areas according to their time windows, and then incrementally generating transportation problems with each cluster and the ready UAVs. We then solve the transportation problems with a simplex algorithm . The performance of the proposed algorithm and other algorithms implemented in order to compare the solution quality is evaluated through example scenarios with practical problem sizes.},
  archive      = {J_RAS},
  author       = {Fatih Semiz and Faruk Polat},
  doi          = {10.1016/j.robot.2020.103435},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103435},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Solving the area coverage problem with UAVs: A vehicle routing with time windows variation},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Object shape estimation and modeling, based on sparse
gaussian process implicit surfaces, combining visual data and tactile
exploration. <em>RAS</em>, <em>126</em>, 103433. (<a
href="https://doi.org/10.1016/j.robot.2020.103433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inferring and representing three-dimensional shapes is an important part of robotic perception. However, it is challenging to build accurate models of novel objects based on real sensory data, because observed data is typically incomplete and noisy. Furthermore, imperfect sensory data suggests that uncertainty about shapes should be explicitly modeled during shape estimation. Such uncertainty models can usefully enable exploratory action planning for maximum information gain and efficient use of data. This paper presents a probabilistic approach for acquiring object models, based on visual and tactile data . We study Gaussian Process Implicit Surface (GPIS) representation. GPIS enables a non-parametric probabilistic reconstruction of object surfaces from 3D data points, while also providing a principled approach to encode the uncertainty associated with each region of the reconstruction. We investigate different configurations for GPIS, and interpret an object surface as the level-set of an underlying sparse GP. Experiments are performed on both synthetic data, and also real data sets obtained from two different robots physically interacting with objects. We evaluate performance by assessing how close the reconstructed surfaces are to ground-truth object models. We also evaluate how well objects from different categories are clustered, based on the reconstructed surface shapes. Results show that sparse GPs enable a reliable approximation to the full GP solution, and the proposed method yields adequate surface representations to distinguish objects. Additionally the presented approach is shown to provide computational efficiency, and also efficient use of the robot’s exploratory actions.},
  archive      = {J_RAS},
  author       = {Gabriela Zarzar Gandler and Carl Henrik Ek and Mårten Björkman and Rustam Stolkin and Yasemin Bekiroglu},
  doi          = {10.1016/j.robot.2020.103433},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103433},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Object shape estimation and modeling, based on sparse gaussian process implicit surfaces, combining visual data and tactile exploration},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Magnetic crawler climbing detection robot basing on metal
magnetic memory testing technology. <em>RAS</em>, <em>125</em>, 103439.
(<a href="https://doi.org/10.1016/j.robot.2020.103439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Failure detection of high facilities always presents a tremendous challenge. Climbing-wall robot with detection capacity has become a main approach. But owing to their limitations in overcoming obstacles and complicated wall situation, reliable wall-climbing property and precise detection abilities are the most basic demand for achieving this function. Hence, further research is required to enhance robot capabilities in overcoming obstacles and accurate detection signal. The paper presents a new climbing-wall detection robot mechanism. The wall-climbing robot consists of two climbing modules. The two climbing modules are connected by anti-overturning mechanism to provide a capacity of anti-overturning during overcoming obstacle. The detection mechanism is installed at the bottom of the robot. Detailed design issues are presented with analyses of the design parameters. Transition displacement of anti-overturning mechanism and force transfer equation are derived, and stable operating conditions are verified. The abilities of flat surface locomotion, anti-overturning, preload and detection capacity are validated by using experiments. Experiment results show that the prototype achieves 10kg payload capacity on vertical surfaces and can overcome 10mm obstacle. 1mm × × 1mm circular groove can be found.},
  archive      = {J_RAS},
  author       = {Fumin Gao and JianChun Fan and Laibin Zhang and Jiankang Jiang and Shoujie He},
  doi          = {10.1016/j.robot.2020.103439},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103439},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Magnetic crawler climbing detection robot basing on metal magnetic memory testing technology},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Operational space analysis of human muscular effort in robot
assisted reaching tasks. <em>RAS</em>, <em>125</em>, 103429. (<a
href="https://doi.org/10.1016/j.robot.2020.103429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human motor performance is a key area of investigation in both biomechanics and robotics. In robotics, understanding human muscular control is important to synthesize prosthetic motions and ensure safe human–robot interaction. Building controllable biomechanical models can help in quantifying the characteristics of a subject’s motion and in designing effective treatments, like motion training. This paper presents the task-based motion analysis of muscular effort using an upper-body musculoskeletal model, validated through motion capture experiments and dynamic simulations. To study the contribution of robotic assistance in improving human motor skills, the muscular effort of the task of reaching with and without robotic assistance was investigated for 10 subjects. Reduction of 21.4\% in the arm muscular effort was observed for the tasks with robotic assistance.},
  archive      = {J_RAS},
  author       = {Emel Demircan and Stephanie Yung and Mathew Choi and Jon Baschshi and Brian Nguyen and Javier Rodriguez},
  doi          = {10.1016/j.robot.2020.103429},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103429},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Operational space analysis of human muscular effort in robot assisted reaching tasks},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A dual-mode soft gripper for food packaging. <em>RAS</em>,
<em>125</em>, 103427. (<a
href="https://doi.org/10.1016/j.robot.2020.103427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robotics and automation in the food industry is not as widely applied as in other industries, such as automotive and electrical industries, due to the large variations in the shape and properties of food materials and the frequent alterations of food products. Robotic end effectors that can adapt to these variations and handle multiple types of food materials are in high demand. Therefore, we propose a dual-mode soft gripper made of rubber material that can grasp and suck different types of objects. The gripper consists of four soft fingers, fabricated with rubber material using casting process , each of which is designed as a combination of a PenuNet bending actuator and a suction pad located at the fingertip. We introduce a new design for the air paths, which play an important role in the proper function of the soft finger. Finite element (FE) simulations were performed to confirm the finger design. Experimental tests were conducted to evaluate single finger bending, gripper lifting force, and grasping and sucking actions for various types of food materials. Results show that the soft gripper can lift a 273.97-g hot dog in the grasping mode, as well as a 512.62-g bagged Kernel corn and a 1072.65-g Macbook Air in the suction mode. It can adapt to approximately circular and square targets, such as a piece of fried chicken and an orange, when the soft fingers are in a perpendicular configuration. While in a parallel configuration, the gripper can successfully handle elongated targets, such as a hot dog. An experiment is also presented to demonstrate the automatic packaging of a Japanese boxed lunch, which requires both grasping and suction modes to be employed.},
  archive      = {J_RAS},
  author       = {Zhongkui Wang and Keung Or and Shinichi Hirai},
  doi          = {10.1016/j.robot.2020.103427},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103427},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A dual-mode soft gripper for food packaging},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Genetically optimized parameter estimation of mathematical
model for multi-joints hip–knee exoskeleton. <em>RAS</em>, <em>125</em>,
103425. (<a href="https://doi.org/10.1016/j.robot.2020.103425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Achieving precise parameters of multi-joints actuators for Hip–Knee Exoskeleton (HKE) is a crucial process due to its non-linear characteristics. In this paper, a Genetic Algorithm (GA) based optimization is used for parameter estimation of the mathematical model for a four-Degree of Freedom (DoF) multi-joint HKE, which is a type of Lower Limb Exoskeleton (LLE). Mathematical model for electro-mechanical, mechanical, and electrical components of the HKE has been formulated, and its parameters are estimated using GA and experimental method. An objective function is determined based on the difference between the simulated and actual angular trajectory for each joint. The performance of the mathematical model is examined with different voltages under the range of 4 V to 8 V for hip and knee, respectively. Furthermore, the performance of the estimated model is compared with Particle Swarm Optimization (PSO). The results and numerical analysis demonstrated that the estimated model by GA and PSO with varying voltages predicted the actual angular trajectory with acceptable error, while GA provides the more accurate model. It can be ascertained that the proposed method of estimation for mathematical model of the HKE is applicable to identify its parameters, and useful for designing a control system.},
  archive      = {J_RAS},
  author       = {Mohammad Soleimani Amiri and Rizauddin Ramli and Mohd Faisal Ibrahim},
  doi          = {10.1016/j.robot.2020.103425},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103425},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Genetically optimized parameter estimation of mathematical model for multi-joints hip–knee exoskeleton},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A programmable central pattern generator with bounded
output. <em>RAS</em>, <em>125</em>, 103423. (<a
href="https://doi.org/10.1016/j.robot.2020.103423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite extensive studies on cyclic tasks in robotics, definitive solutions for the problem of trajectory generation for periodic motions have not been achieved so far. In this paper, we present an approach for online trajectory generation from a library of desired periodic trajectories. The proposed approach consists of a Central Pattern Generator (CPG) architecture ensuring entrainment of any periodic trajectory, smooth motion modulation and observing position and velocity limits of the robot. The proposed CPG is composed of a synchronized network of novel bounded output oscillatory systems. Every oscillatory system is a three-dimensional dynamical system encoding a one-dimensional periodic function as a stable limit cycle . We also use the state transformation method to bound the oscillator’s output and its first time derivative. Finally, we present a synchronization technique to construct a synchronized network of the proposed oscillators for generating multi dimensional periodic functions. Using Lyapunov based arguments, we prove that the proposed CPG ensures stability, convergence, and synchronization of the desired trajectory . The soundness of the proposed oscillator and the resulting CPG are validated both in simulations and experiments on the humanoid robot iCub.},
  archive      = {J_RAS},
  author       = {Venus Pasandi and Aiko Dinale and Mehdi Keshmiri and Daniele Pucci},
  doi          = {10.1016/j.robot.2020.103423},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103423},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A programmable central pattern generator with bounded output},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Collaborative infotaxis: Searching for a signal-emitting
source based on particle filter and gaussian fitting. <em>RAS</em>,
<em>125</em>, 103414. (<a
href="https://doi.org/10.1016/j.robot.2019.103414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To effectively leverage the spatio-temporal sensing capabilities of the team searching for a signal-emitting source, this paper presents a collaborative search method, in which each robot employs the weighted social Bayesian estimation and executes the distributed infotaxis search for the source. Cognition difference between robots, measuring the dissimilarity of probability maps, is specially introduced to obtain the heterogeneous weights of Bayesian estimation. However, the requirement of exchanging the whole probability map presents additional challenges in computation and communication for real-time applications. In this work, a solution for fast low-cost collaborative infotaxis method based on a combination of particle filter and Gaussian fitting is proposed. A particle filter is first employed for the representation of the source probability distribution, which makes the infotaxis strategy computationally tractable for large complex spaces using the limited and tractable amount of randomly drawn particles. By fitting a Gaussian density to the particles, each robot obtains the likelihood weight for social Bayesian estimation by only reporting the mean and the covariance matrix of Gaussian distribution rather than exchanging the whole probability maps. The simulation shows the proposed collaborative infotaxis can achieve an efficient search behavior in complex environments using a small number of particles and a lower communication bandwidth .},
  archive      = {J_RAS},
  author       = {Cheng Song and Yuyao He and Branko Ristic and Xiaokang Lei},
  doi          = {10.1016/j.robot.2019.103414},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103414},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Collaborative infotaxis: Searching for a signal-emitting source based on particle filter and gaussian fitting},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Virtual-joint based motion similarity criteria for
human–robot kinematics mapping. <em>RAS</em>, <em>125</em>, 103412. (<a
href="https://doi.org/10.1016/j.robot.2019.103412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motion mapping is an important part in human–robot cooperation. In this paper, a novel concept of virtual-joint based similarity criteria is proposed for flexible and efficient kinematics mapping between dissimilar embodiments, including different degrees of freedom (DOFs), different body morphology, and so on. Virtual joints are defined respectively in both the demonstrator and the imitator, with the same number. In virtual joints, the neglecting, re-ordering and repetitive usage of DOFs could be realized through the virtual decomposing matrices. Each virtual joint of the demonstrator and the corresponding one of the imitator formed a virtual joint pair. The Total Metric of Motion Similarity is the weighted sum of the metrics defined for each virtual joint pairs. Unlike traditional joint-space or Cartesian-space based metrics describing motion similarity solely at the DOF kinematic mode level, virtual-joint-based metrics can be adopted to describe different aspects of motion similarity between dissimilar agents, both in joint space and in Cartesian space. Two experiments are conducted to illustrate the effectiveness of the proposed approach.},
  archive      = {J_RAS},
  author       = {Zhang Chen and Ziwei Wang and Rongjian Liang and Bin Liang and Tao Zhang},
  doi          = {10.1016/j.robot.2019.103412},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103412},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Virtual-joint based motion similarity criteria for human–robot kinematics mapping},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-agent sensitivity enhanced iterative best response: A
real-time game theoretic planner for drone racing in 3D environments.
<em>RAS</em>, <em>125</em>, 103410. (<a
href="https://doi.org/10.1016/j.robot.2019.103410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a real-time game theoretic motion planning approach that enables an autonomous drone to race competitively against an arbitrary number of opponent drones along a 2D or 3D racecourse. Our method computes an approximate Nash equilibrium in the space of robot trajectories to maximally advance the ego robot while taking into account the opponents’ intentions and responses. The core of our solution is a “sensitivity enhanced” iterative best response algorithm that the ego robot uses to repeatedly plan its own trajectory and infer opponents’ trajectories, ultimately seeking a Nash equilibrium in the joint space of trajectories for all the drones. The algorithm includes a term that allows the ego vehicle to gain advantage by exploiting the influence of the ego drone’s trajectory on the adversaries’ objectives through the shared collision avoidance constraints among the vehicles. We also propose two methods for accelerating this computationally intensive iterative algorithm using (i) parallel computing with multiple CPU cores, and (ii) a neural network model that learns to predict trajectories close to the Nash equilibrium through offline training examples. Extensive simulation studies are conducted to benchmark the performance of our game theoretic planner and the statistical results show that our approach largely outperforms a baseline model predictive control algorithm that does not account for the opponents’ reactions. Hardware experiments with 4 quadrotor robots on a 3D racecourse are performed to show the applicability of our method in real-time robotic systems .},
  archive      = {J_RAS},
  author       = {Zijian Wang and Tim Taubner and Mac Schwager},
  doi          = {10.1016/j.robot.2019.103410},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103410},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Multi-agent sensitivity enhanced iterative best response: A real-time game theoretic planner for drone racing in 3D environments},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Assistive devices of human knee joint: A review.
<em>RAS</em>, <em>125</em>, 103394. (<a
href="https://doi.org/10.1016/j.robot.2019.103394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knee dysfunction, such as knee osteoarthritis, meniscus injury, ligament injury, spinal cord injury and stroke, considerably impacts the normal living ability and mental health of these patients. Developing more effective knee assistive devices is in urgent need for effectively recovering their motion capabilities and improving their self-living activities. In this paper, we review and discuss the mechanical system design, sensing and control systems design, and performance evaluation of the main research advances in knee assistive devices . Firstly, in order to clearly illustrate and compare the mechanical system design, the mechanical system design is classified into four components to discuss: human attachment design, joint alignment design, actuation design and power transmission design. Then, the sensing and control systems design, which includes human biological signals based control systems, human–device interaction signals based control systems and device signals only based control systems, is compared and discussed. Furthermore, the performance evaluation methods and effectiveness of most of the knee assistive devices are reviewed. Finally, a discussion of the existing problems in the current studies and some recommendations for future research are presented.},
  archive      = {J_RAS},
  author       = {Li Zhang and Prof. Geng Liu and Bing Han and Zhe Wang and Han Li and Yan Jiao},
  doi          = {10.1016/j.robot.2019.103394},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103394},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Assistive devices of human knee joint: A review},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design and performance analysis of a parallel wrist
rehabilitation robot (PWRR). <em>RAS</em>, <em>125</em>, 103390. (<a
href="https://doi.org/10.1016/j.robot.2019.103390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wrist rehabilitation robots are essential for assisting patients with stoke or wrist injuries. Such devices compensate for deficiencies in manual rehabilitation training, and reduce the workload of rehabilitation physicians. A parallel wrist rehabilitation robot (PWRR) driven by two pneumatic actuators is developed in this paper, consisting of two rotational degrees of freedom for the movements of flexion/extension (F/E) and radial/ulnar deviation (R/U). All components connected to the forearm or the wrist adopt an open structure to improve the wearable convenience, and the PWRR is suitable for most patients, especially those with hypertonia. To determine the PWRR range of motion, the physiological motion space (PMS) of the wrist joint in autonomous and boundary elliptical movements is measured with the help of a VICON motion capture system. The PMS in boundary motions processes an elliptical shape , and the ulnar deviations occupy the most range of motion. The theoretical workspace (TWS) of PWRR is then calculated and designed based on the kinematic model and the distribution characteristics of PMS. In addition, two indices are introduced to evaluate the kinematic performance of PWRR. A PWRR prototype is developed based on the optimal geometrical parameters and detailed structures. Its effective workspace (EWS), which has more clinical significance, is acquired by measuring the F/E and R/U movements during autonomic movements. The EWS, is smaller than TWS due to the physical structure, volume, and interference of mechanical elements. Besides, EWS can nearly encircle PMS, and satisfies all single-axis rehabilitations and compound motions of the wrist complex. The two indices, motion isotropy d a da and condition number κ κ , within TWS change smoothly with no mutation, suggesting that PWRR is sufficiently kinematically isotropic, and has no singularity configuration. The analysis shows that the developed PWRR can be applied widely in the wrist rehabilitation.},
  archive      = {J_RAS},
  author       = {Leiyu Zhang and Jianfeng Li and Ying Cui and Mingjie Dong and Bin Fang and Pengfei Zhang},
  doi          = {10.1016/j.robot.2019.103390},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103390},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Design and performance analysis of a parallel wrist rehabilitation robot (PWRR)},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Min–max time efficient inspection of ground vehicles by a
UAV team. <em>RAS</em>, <em>125</em>, 103370. (<a
href="https://doi.org/10.1016/j.robot.2019.103370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a control design for N N unmanned aerial vehicles (UAVs) tasked with an inspection of M M ground moving vehicles. The location of each ground vehicle is known to each UAV, but the navigation and intent of each ground vehicle are unknown, therefore, this uncertainty has to be anticipated in each UAV’s navigation. We use the minimum time stochastic optimal control to navigate each UAV towards the inspection of each ground vehicle. Based on this control, we formulate assignments of ground vehicles to be inspected by UAVs as an optimization problem to inspect all ground vehicles in the minimum expected time. Accounting for ground vehicle uncertain trajectories, we update the optimal assignment by a Markov inequality rule. The rule prevents the possibility of indefinite updating of assignments without finishing the inspection of all vehicles. On the other hand, it updates an assignment if it leads to a statistically significant improvement of the inspection expected time. The presented approach is illustrated with numerical examples.},
  archive      = {J_RAS},
  author       = {Alexey A. Munishkin and Dejan Milutinović and David W. Casbeer},
  doi          = {10.1016/j.robot.2019.103370},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103370},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Min–max time efficient inspection of ground vehicles by a UAV team},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Human-robot cooperative control based on sEMG for the upper
limb exoskeleton robot. <em>RAS</em>, <em>125</em>, 103350. (<a
href="https://doi.org/10.1016/j.robot.2019.103350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An exoskeleton robot is a mechanical structure that integrates with the exterior of the human body to improve the wearer’s muscular power. The key to ensure performances and comfort of the system is human–robot cooperation. This paper proposes a human–robot cooperative control method based on sEMG (surface Electromyography) signals to drive a pneumatic upper limb exoskeleton to act in accordance with the wearer’s motion intentions. The intended movement information of the human is estimated by combining the regression method with the classification method. Based on the joint torque estimation model which is originated from the Hill-type musculoskeletal model, the regression method is used to estimate the joint’s desired torque by merging the sEMG signal with the joint angle. To avoid shaking and keep the robot’s limbs in the static condition, a classification method with the support vector machine is developed to find out the joint state that the human intends to keep. It was then applied to the exoskeleton’s elbow joint flexion and extension movement experiments to verify the controller’s effectiveness. The experimental results demonstrate that the controller can estimate human’s motion intention accurately and is appropriate for the human–robot collaboration.},
  archive      = {J_RAS},
  author       = {Hao Liu and Jun Tao and Pan Lyu and Fang Tian},
  doi          = {10.1016/j.robot.2019.103350},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103350},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Human-robot cooperative control based on sEMG for the upper limb exoskeleton robot},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lagrange modeling and navigation based on quaternion for
controlling a micro AUV under perturbations. <em>RAS</em>, <em>124</em>,
103408. (<a href="https://doi.org/10.1016/j.robot.2019.103408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the modeling of an autonomous underwater vehicle using quaternion formulation for angular position description and Lagrange method to compute the equations of motion. As the four parameters are dependent and generate a constraint, Lagrange multipliers are used with Baumgarte method to solve and stabilize the system. The dynamic model includes underwater effects like added mass and inertia, hydrodynamic damping, buoyancy and propeller forces. Moreover, a quaternion-based line of sight guidance algorithm is derived to avoid any use of trigonometric function and compute directly the orientation error of the underwater vehicle and the desired attitude in terms of quaternions. Motion control is achieved with a quaternion-based adaptive sliding mode controller rejecting model uncertainties and water current. The simulation results, where the vehicle follows a sequence of way-points including vertical diving motion demonstrate that the proposed guidance algorithm and motion control are deeply relevant both in terms of effectiveness and robustness for this particular type of vehicle and orientation formulation.},
  archive      = {J_RAS},
  author       = {Jonathan Rodriguez and Herman Castañeda and J.L. Gordillo},
  doi          = {10.1016/j.robot.2019.103408},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103408},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Lagrange modeling and navigation based on quaternion for controlling a micro AUV under perturbations},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Shape-centric modeling for control of traveling wave
rectilinear locomotion on snake-like robots. <em>RAS</em>, <em>124</em>,
103406. (<a href="https://doi.org/10.1016/j.robot.2019.103406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A traveling wave rectilinear gait for elongated, continuous bodies is modeled as a cyclically-varying backbone curve . The gait shapes are represented as planar deviations relative to an average body curve and an associated, rigidly-attached body frame. Body-ground contact patterns and other geometric properties integral to computation of external forcing are conveniently defined with respect to this average body curve. Introducing a body-ground rolling friction model permits the controlled equations of motion to be derived in closed form. Incorporating a constant curvature into the average body realizes turning movements, and hence turning control. Repeated numerical integration of the system dynamics facilitates construction of a control-to-action mapping, characterizing steady system behavior with respect to the gait’s parameter space. The control-to-action map reduces this complex dynamical system to a kinematic unicycle model for which feedback tracking strategies are well understood. To illustrate its utility, it is applied in a trajectory planning and tracking framework for locomotion around obstacles. Using the framework, a robotic snake exercising the traveling wave rectilinear gait successfully plans feasible trajectories and traverses non-trivial obstacle arrangements to reach specified goal positions.},
  archive      = {J_RAS},
  author       = {Alexander H. Chang and Patricio A. Vela},
  doi          = {10.1016/j.robot.2019.103406},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103406},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Shape-centric modeling for control of traveling wave rectilinear locomotion on snake-like robots},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust mission planning for autonomous marine vehicle
fleets. <em>RAS</em>, <em>124</em>, 103404. (<a
href="https://doi.org/10.1016/j.robot.2019.103404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mission planning for Autonomous Marine Vehicles (AMVs) is non-trivial because significant uncertainty is present when profiling the operating environment, especially for underwater missions. Mission complexity is compounded for each vehicle added to the mission. In practice, fleet operations are formulated as separate temporal problems by the operator and solved using a temporal planner. This paper proposes a planning method that uses energy as the base planning resource instead of time. Unlike temporal planners, energy planners account for physical loads endured by the vehicles. The extent of uncertainty in the vehicle loads is clarified by using the vehicle dynamics model and Monte Carlo simulation on the model parameters. The planning method is a multistage procedure to decompose operator specified task, obstacle, and vehicle data into an energy formulation of the Team Orienteering Problem (TOP) which is then solved using Discrete Strengthened PSO (DStPSO). The DStPSO algorithm has been modified to include a selective swarm size decay method that allows for larger initial swarm sizes to promote early exploration and preserves a percentage of the best performing particles on each iteration to save computational resources. The planner produces near-optimal routes containing feasible trajectories for individual vehicles that maximise tasks completed according to individual vehicle energy constraints. A case-study mission for long-term, large-scale, underwater inspection of a wind turbine array was converted into input data to evaluate the planner. Energy planning presents the opportunity for vehicles to actively monitor the feasibility of their individual plan against their current energy consumption, allowing for advanced reasoning and fault handling to occur in situ without operator assistance.},
  archive      = {J_RAS},
  author       = {Fletcher Thompson and Roberto Galeazzi},
  doi          = {10.1016/j.robot.2019.103404},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103404},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Robust mission planning for autonomous marine vehicle fleets},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Disturbance compensation based controller for an indoor
blimp robot. <em>RAS</em>, <em>124</em>, 103402. (<a
href="https://doi.org/10.1016/j.robot.2019.103402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the robust controller design for an indoor blimp robot to achieve application such as the surveillance. The commonly used 6 degrees of freedom dynamic model is simplified under reasonable assumptions and decoupled into two independent parts. The blimp simplified horizontal plane movement model is complemented with disturbance terms to ensure the modeling accuracy, then it is transformed to a simpler form for the ease of controller design . Next, the disturbance terms are evaluated by the designed real-time estimator, and the perturbation estimates are compensated in the conceived motion controller for cancellation of the influence of disturbances. The performance and robustness of the disturbance compensation-based controller are verified by both simulations and experiments on the developed blimp robot. Finally, the results prove the feasibility of the blimp robot in indoor surveillance application by stabilizing itself at a fixed position or patrolling along a predefined path.},
  archive      = {J_RAS},
  author       = {Yue Wang and Gang Zheng and Denis Efimov and Wilfrid Perruquetti},
  doi          = {10.1016/j.robot.2019.103402},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103402},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Disturbance compensation based controller for an indoor blimp robot},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enhancing adaptability with local reactive behaviors for
hexapod walking robot via sensory feedback integrated central pattern
generator. <em>RAS</em>, <em>124</em>, 103401. (<a
href="https://doi.org/10.1016/j.robot.2019.103401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local reactive behaviors endow animals the ability to exhibit agile and dexterous performance when traversing challenging terrains. This paper presents a novel locomotion control method based on the central pattern generator (CPG) concept for hexapod walking robot with local reactive behavior to cope with terrain irregularities. Firstly, a two-layered CPG-based single-leg controller is developed to generate the rhythmical movement for each leg executing tripod walking. The Van der Pol oscillator is employed on the high-layer to construct a coupled CPG network which serves as a phase regulator (PR) to produce rhythmic signals with prescribed phase relations amongst neurons. On the low-layer, an auxiliary linear converter (LC) transforms these signals into the desired joint trajectories. Subsequently, by embodying the proprioceptive sensing and external tactile information as the sensory feedback, two typical local reactive mechanisms including the elevator reflex and searching reflex are achieved by virtue of on-line adjusting the coupling scheme of the PR and the coefficients of the LC. A locomotion control framework for hexapod walking robot is further established by combining the single-leg controller with a finite state machine to allocate swing/stance commands for individual joints in dealing with terrain perturbations. The effectiveness of the proposed method has been verified through both virtual model simulation and experiments on a physical hexapod platform.},
  archive      = {J_RAS},
  author       = {Haitao Yu and Haibo Gao and Zongquan Deng},
  doi          = {10.1016/j.robot.2019.103401},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103401},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Enhancing adaptability with local reactive behaviors for hexapod walking robot via sensory feedback integrated central pattern generator},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bootstrapped neuro-simulation as a method of concurrent
neuro-evolution and damage recovery. <em>RAS</em>, <em>124</em>, 103398.
(<a href="https://doi.org/10.1016/j.robot.2019.103398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bootstrapped Neuro-Simulation (BNS) is a method of concurrent simulator and robot controller evolution. The algorithm requires little domain knowledge and no pre-investigation data gathering. Additionally, it bridges the reality gap effectively, rapidly evolves functional controllers, and recovers from damage automatically. In this paper, the first evidence of the ability of BNS to evolve closed-loop controllers is shown; in this case to solve a light-following problem. The algorithm is then evaluated for its damage recovery ability for these closed-loop controllers and shown to be very effective, with only minor adaptations.},
  archive      = {J_RAS},
  author       = {Brydon A. Leonard and Mathys C. du Plessis and Grant W. Woodford},
  doi          = {10.1016/j.robot.2019.103398},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103398},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Bootstrapped neuro-simulation as a method of concurrent neuro-evolution and damage recovery},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Practical formulation of obstacle avoidance in the
task-priority framework for use in robotic inspection and intervention
scenarios. <em>RAS</em>, <em>124</em>, 103396. (<a
href="https://doi.org/10.1016/j.robot.2019.103396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new formulation of a reactive obstacle avoidance algorithm, in the Task-Priority framework, delivering a practical solution for obstacle avoidance between vehicle-manipulator systems and complex environments. The presented concepts were implemented on an intervention autonomous underwater vehicle (I-AUV) and tested in an underwater pipe structure inspection and valve turning scenario, in a test tank, using GIRONA500 with an ECA 5E Micro manipulator. The obstacle avoidance is treated as an inequality (set-based) task, which takes into account all obstacles that are interacting with the robot links. Both the robot and the obstacles are represented by spheres to allow for analytical formulation. However, the environment is wrapped with spheres based on its actual geometry stored as an Octomap, hence it can be represented at different resolutions. Depending on the type of the mission performed by the robot we defined two modes of operation: (1) Navigation and Inspection, and (2) Intervention. For each mode, the algorithm takes into account different number of key points at the I-AUV and a different resolution of the environment representation. Typically, for the Intervention mode the resolution is higher, to allow for more precise motion. We also present an escape point strategy in case of the robot getting stuck between obstacles.},
  archive      = {J_RAS},
  author       = {Patryk Cieślak and Roberto Simoni and Pere Ridao Rodríguez and Dina Youakim},
  doi          = {10.1016/j.robot.2019.103396},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103396},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Practical formulation of obstacle avoidance in the task-priority framework for use in robotic inspection and intervention scenarios},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LPV-MP planning for autonomous racing vehicles considering
obstacles. <em>RAS</em>, <em>124</em>, 103392. (<a
href="https://doi.org/10.1016/j.robot.2019.103392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present an effective online planning solution for autonomous vehicles that aims at improving the computational load while preserving high levels of performance in racing scenarios. The method follows the structure of the model predictive (MP) optimal strategy where the main objective is to maximize the velocity while smoothing the dynamic behavior and fulfilling varying constraints. We focus on reformulating the non-linear original problem into a pseudo-linear problem by convexifying the objective function and reformulating the non-linear vehicle equations to be expressed in a Linear Parameter Varying (LPV) form. In addition, the ability of avoiding obstacles is introduced in a simple way and with reduced computational cost. We test and compare the performance of the proposed strategy against its non-linear approach through simulations. We focus on testing the performance of the trajectory planning approach in a racing scenario. First, the case of free obstacles track and afterwards a scenario including static obstacles. Simulation results show the effectiveness of the proposed strategy by reducing the algorithm elapsed time while finding appropriate trajectories under several input/state constraints.},
  archive      = {J_RAS},
  author       = {Eugenio Alcalá and Vicenç Puig and Joseba Quevedo},
  doi          = {10.1016/j.robot.2019.103392},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103392},
  shortjournal = {Robot. Auton. Syst.},
  title        = {LPV-MP planning for autonomous racing vehicles considering obstacles},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Safety barrier functions and multi-camera tracking for
human–robot shared environment. <em>RAS</em>, <em>124</em>, 103388. (<a
href="https://doi.org/10.1016/j.robot.2019.103388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new vision in human–robot collaboration has allowed to place robots nearby human operators, working close to each other in industrial environments. As a consequence, human safety has become a dominant issue, together with production efficiency. In this paper we propose an optimization-based control algorithm that allows robots to avoid obstacles (like human operators) while minimizing the difference between the nominal acceleration input and the commanded one. Control Barrier Functions are exploited to build safety barriers around each robot link, to guarantee collision-free trajectories along the whole robot body. Human accelerations and velocities are computed by means of a bank of Kalman filters . To solve obstruction problems, two RGB-D cameras are used and the measured skeleton data are processed and merged using the mentioned bank of Kalman filters . The algorithm is implemented on an Universal Robots UR5 in order to validate the proposed approach.},
  archive      = {J_RAS},
  author       = {Federica Ferraguti and Chiara Talignani Landi and Silvia Costi and Marcello Bonfè and Saverio Farsoni and Cristian Secchi and Cesare Fantuzzi},
  doi          = {10.1016/j.robot.2019.103388},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103388},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Safety barrier functions and multi-camera tracking for human–robot shared environment},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning inverse kinematics and dynamics of a robotic
manipulator using generative adversarial networks. <em>RAS</em>,
<em>124</em>, 103386. (<a
href="https://doi.org/10.1016/j.robot.2019.103386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Obtaining inverse kinematics and dynamics of a robotic manipulator is often crucial for robot control. Analytical models are typically used to approximate real robot systems, and various controllers have been designed on top of the analytical model to compensate for the approximation error. Recently, machine learning techniques have been developed for error compensation, resulting in better performance. Unfortunately, combining a learned compensator with an analytical model makes the designed controller redundant and computationally expensive. Also, general machine learning techniques require a lot of data to perform the training process and approximation , especially in solving high dimensional problems. As a result, state-of-the-art machine learning applications are either expensive in terms of computation and data collection, or limited to a local approximation for a specific task or routine. In order to address the high dimensionality problem in learning inverse kinematics and dynamics, as well as to make the training process more data efficient, this paper presents a novel approach using a series of modified Generative Adversarial Networks (GANs). Namely, we use Conditional GANs (CGANs), Least Squares GANs (LSGANs), Bidirectional GANs (BiGANs) and Dual GANs(DualGANs). We trained and tested the proposed methods using real-world data collected from two types of robotic manipulators, a MICO robotic manipulator and a Fetch robotic manipulator. The data input to the GANs was obtained using a sampling method applied to the real data. The proposed approach enables approximating the real model using limited data without compromising the performance and accuracy. The proposed methods were tested in real-world experiments using unseen trajectories to validate the “learned” approximate inverse kinematics and inverse dynamics as well as to demonstrate the capability and effectiveness of the proposed algorithm over existing analytical models.},
  archive      = {J_RAS},
  author       = {Hailin Ren and Pinhas Ben-Tzvi},
  doi          = {10.1016/j.robot.2019.103386},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103386},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Learning inverse kinematics and dynamics of a robotic manipulator using generative adversarial networks},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-optimization of resilient topologies for fallible
multi-robots. <em>RAS</em>, <em>124</em>, 103384. (<a
href="https://doi.org/10.1016/j.robot.2019.103384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective exchange of information in multi-robot systems is one of the grand challenges of today’s robotics. Here, we address the problem of simultaneously maximizing the (i) resilience to faults and (ii) area coverage of dynamic multi-robot topologies. We want to avoid the onset of single points of failure , i.e., situations in which the failure of a single robot causes the loss of connectivity in the overall network. Our methodology is based on (i) a three-fold control law and (ii) a distributed online optimization strategy that computes the optimal choice of control parameters for each robot. By doing so, connectivity is not only preserved, but also made resilient to failures as the network topology evolves. To assess the effectiveness of our approach, we ran experiments with a team of eight two-wheeled robots and we evaluated it against the injection of two separate classes of faults: communication and hardware failures. Results show that the proposed approach continues to perform as intended, even in the presence of these hazards.},
  archive      = {J_RAS},
  author       = {Marco Minelli and Jacopo Panerati and Marcel Kaufmann and Cinara Ghedini and Giovanni Beltrame and Lorenzo Sabattini},
  doi          = {10.1016/j.robot.2019.103384},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103384},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Self-optimization of resilient topologies for fallible multi-robots},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). A survey of underwater docking guidance systems.
<em>RAS</em>, <em>124</em>, 103382. (<a
href="https://doi.org/10.1016/j.robot.2019.103382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous underwater vehicles (AUVs) are increasingly being used for underwater survey and exploration missions. The expanding mission scope for AUVs highlights the need for a long-endurance operational capability, which mainly depends on propulsion system efficiency and battery capacity. The use of submerged docking stations permitting battery recharge and data download/upload offers a means of enabling persistence without compromising propulsion and payload power budgets, while also reducing associated deployment/recovery costs and risks. Autonomous docking with an underwater station is, however, complicated by the presence of currents and obstacles in the water, and by the relative dynamic differences in pose between the dock and the vehicle. A robust docking guidance system is identified as a core and crucial component for ensuring successful AUV docking. This paper presents a detailed literature review summarizing the current state-of-the-art in AUV docking guidance methodologies, identifying their relative merits and shortcomings, and revealing the docking guidance methodologies that seems to be the most prominent.},
  archive      = {J_RAS},
  author       = {A.M. Yazdani and K. Sammut and O. Yakimenko and A. Lammas},
  doi          = {10.1016/j.robot.2019.103382},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103382},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A survey of underwater docking guidance systems},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evolution of robust high speed optical-flow-based landing
for autonomous MAVs. <em>RAS</em>, <em>124</em>, 103380. (<a
href="https://doi.org/10.1016/j.robot.2019.103380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic optimization of robotic behavior has been the long-standing goal of Evolutionary Robotics. Allowing the problem at hand to be solved by automation often leads to novel approaches and new insights. A common problem encountered with this approach is that when this optimization occurs in a simulated environment, the optimized policies are subject to the reality gap when implemented in the real world. This often results in sub-optimal behavior, if it works at all. This paper investigates the automatic optimization of neurocontrollers to perform quick but safe landing maneuvers for a quadrotor micro air vehicle using the divergence of the optical flow field of a downward looking camera. The optimized policies showed that a piece-wise linear control scheme is more effective than the simple linear scheme commonly used, something not yet considered by human designers. Additionally, we show the utility in using abstraction on the input and output of the controller as a tool to improve the robustness of the optimized policies to the reality gap by testing our policies optimized in simulation on real world vehicles. We tested the neurocontrollers using two different methods to generate and process the visual input, one using a conventional CMOS camera and one a dynamic vision sensor, both of which perform significantly differently than the simulated sensor. The use of the abstracted input resulted in near seamless transfer to the real world with the controllers showing high robustness to a clear reality gap.},
  archive      = {J_RAS},
  author       = {Kirk Y.W. Scheper and Guido C.H.E. de Croon},
  doi          = {10.1016/j.robot.2019.103380},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103380},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Evolution of robust high speed optical-flow-based landing for autonomous MAVs},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Representation and classification of whole-body motion
integrated with finger motion. <em>RAS</em>, <em>124</em>, 103378. (<a
href="https://doi.org/10.1016/j.robot.2019.103378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel approach toward representing human whole-body motions with fingers and classification of human motions while performing tasks that require delicate finger movements, such as holding or grasping. Human whole-body motions are recorded using an optical motion capture system that measures positions of markers attached to a performer. Additionally, the performer wears data gloves with strain gauges fixed at the finger joints to measure flexions and extensions. Combining whole-body motion with finger motions forms a representation of integrated motion, which is subsequently encoded into a probabilistic model whose parameters are optimized such that the model most likely generates the training data for the integrated motion. Observations of integrated motion are classified into the relevant probabilistic model with the largest probability of generating the observation. Synchronous measurements of human whole-body and finger motions created a dataset of integrated human motions. We tested our proposed approach on this dataset, thereby demonstrating that representations of whole-body motion integrated with finger motions improved classification of human motions while manipulating objects.},
  archive      = {J_RAS},
  author       = {Wataru Takano and Yusuke Murakami and Yoshihiko Nakamura},
  doi          = {10.1016/j.robot.2019.103378},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103378},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Representation and classification of whole-body motion integrated with finger motion},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A chaotic path planning generator based on logistic map and
modulo tactics. <em>RAS</em>, <em>124</em>, 103377. (<a
href="https://doi.org/10.1016/j.robot.2019.103377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A simple, short and efficient chaotic path planning algorithm is proposed for autonomous mobile robots , with the aim of covering a given terrain using chaotic, unpredictable motion. The proposed technique utilizes the logistic map with a chaotic tactic that utilizes a modulo function to produce a sequence of directions for a robot that can move in eight different directions on a grid. Extensive simulations are performed, and the results show a fast and efficient scanning of the given area. In addition, the proposed algorithm is further enhanced with a pheromone inspired memory technique, with good improvements in efficiency.},
  archive      = {J_RAS},
  author       = {Lazaros Moysis and Eleftherios Petavratzis and Christos Volos and Hector Nistazakis and Ioannis Stouboulos},
  doi          = {10.1016/j.robot.2019.103377},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103377},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A chaotic path planning generator based on logistic map and modulo tactics},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation and exploitation of objects ’ inertial parameters
in robotic grasping and manipulation: A survey. <em>RAS</em>,
<em>124</em>, 103374. (<a
href="https://doi.org/10.1016/j.robot.2019.103374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inertial parameters characterise an object’s motion under applied forces, and can provide strong priors for planning and control of robotic actions to manipulate the object. However, these parameters are not available a-priori in situations where a robot encounters new objects. In this paper, we describe and categorise the ways that a robot can identify an object’s inertial parameters. We also discuss grasping and manipulation methods in which knowledge of inertial parameters is exploited in various ways. We begin with a discussion of literature which investigates how humans estimate the inertial parameters of objects, to provide background and motivation for this area of robotics research. We frame our discussion of the robotics literature in terms of three categories of estimation methods, according to the amount of interaction with the object: purely visual , exploratory , and fixed-object . Each category is analysed and discussed. To demonstrate the usefulness of inertial estimation research, we describe a number of grasping and manipulation applications that make use of the inertial parameters of objects. The aim of the paper is to thoroughly review and categorise existing work in an important, but under-explored, area of robotics research, present its background and applications, and suggest future directions. Note that this paper does not examine methods of identification of the robot’s inertial parameters, but rather the identification of inertial parameters of other objects which the robot is tasked with manipulating.},
  archive      = {J_RAS},
  author       = {Nikos Mavrakis and Rustam Stolkin},
  doi          = {10.1016/j.robot.2019.103374},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103374},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Estimation and exploitation of objects ’ inertial parameters in robotic grasping and manipulation: A survey},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust aerial scene-matching algorithm based on relative
velocity model. <em>RAS</em>, <em>124</em>, 103372. (<a
href="https://doi.org/10.1016/j.robot.2019.103372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a robust scene-matching (SM) algorithm using time-invariant features that are propagated and bounded by a model propagator and pixel boundary. The SM based absolute navigation has the advantage that the position of the vehicle can be independently calculated without external information, making it possible to calculate a stable navigation solution without cumulative errors. However, SM-based absolute localization has a mismatching problem, this is due to the difference between the reference for the matching and the input image, and the more the change, the higher the probability of mismatching. In this paper we propose an algorithm that can mitigate the mismatching problem with a model-based propagator and time-invariant features. The propagator is based on a relative velocity of the inertial navigation system (INS) model, which is very accurate for a short time. Also the propagated feature points have pixel boundaries, which considers not only INS model uncertainty but also distortion of the aerial images caused by various terrain characteristics. The proposed algorithm is verified by simulation using real experimental data. Consequently we can found the proposed algorithm is very effective in mitigating the mismatching problem in urban areas.},
  archive      = {J_RAS},
  author       = {Sung Hyuk Choi and Chan Gook Park},
  doi          = {10.1016/j.robot.2019.103372},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103372},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Robust aerial scene-matching algorithm based on relative velocity model},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distributed tunneling reconfiguration of cubic modular
robots without meta-module’s disassembling in severe space requirement.
<em>RAS</em>, <em>124</em>, 103369. (<a
href="https://doi.org/10.1016/j.robot.2019.103369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies a tunneling-based reconfiguration algorithm for cubic modular robots. Tunneling-based reconfiguration is a promising approach for cubic modular robot reconfiguration in severe space requirements. This is because a tunneling modular robot only uses spaces occupied by the start and goal configurations. However, previously proposed methods have a limitation on the arrangement of the start and goal configurations, in which the overlapped part between them must be connected. We propose a tunneling reconfiguration algorithm that removes the limitation and is available for cases with multi-overlapped parts between the start and goal configurations. It is often the case that a tunneling-based reconfiguration assumes the use of a meta-module-based structure to maintain the connectivity and mobility of the robot structure. However, in previous methods, the meta-modules often come apart during the tunneling process, and each module belongs to a different meta-module before and after the reconfiguration. The proposed algorithm also solves this problem. We implement the algorithm in a distributed form and prove its completeness for assumed robot structures. We examine the proposed tunneling algorithm by simulation.},
  archive      = {J_RAS},
  author       = {Hiroshi Kawano},
  doi          = {10.1016/j.robot.2019.103369},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103369},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Distributed tunneling reconfiguration of cubic modular robots without meta-module’s disassembling in severe space requirement},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Vision-based magnetic actuator positioning for wireless
control of microrobots. <em>RAS</em>, <em>124</em>, 103366. (<a
href="https://doi.org/10.1016/j.robot.2019.103366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work is concerned with targeted drug delivery inside the human body using magnetic microrobots . It proposes a vision-based magnetic platform for guiding microrobots in both open-loop/closed-loop schemes. The open-loop scheme can be used for example in the case of the inner ear, where the microrobots cannot be localized in real time. On the other hand, for more accuracy, closed-loop scheme can be used for organs as the human eye since microrobots can be localized using a vision sensor. For both schemes, the platform is designed to compensate for human body movements. It is composed of a new magnetic actuator mounted on a robot end-effector and a hybrid vision system. The latter consists of a camera and two microscopes, while the newly proposed magnetic actuator is built using four permanent magnets. The proposed actuator has been designed to create a local maximum of the magnetic field magnitude in a planar workspace. This results in a convergence point for magnetic microrobots that are in its influence zone, making possible open-loop control with a satisfactory accuracy. The procedures for calibrating each component of the proposed platform are described and validated. Finally, several experiments have been carried out to validate the modeling part and to show the feasibility of the concept. The obtained experimental results show that using such platform, the microrobots guiding can be achieved in open-loop under reasonable perturbations and in closed-loop with an accuracy of 200 μ m μm .},
  archive      = {J_RAS},
  author       = {Azaddien Zarrouk and Karim Belharet and Omar Tahri},
  doi          = {10.1016/j.robot.2019.103366},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103366},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Vision-based magnetic actuator positioning for wireless control of microrobots},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A dual-stage parking system for differential-drive robots.
<em>RAS</em>, <em>124</em>, 103365. (<a
href="https://doi.org/10.1016/j.robot.2019.103365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the nonholonomic constraints, it is challenging to asymptotically stabilize a differential-drive robot at an arbitrary pose with desirable transient response . In this paper, an advanced parking system is introduced to tackle the problem of nonholonomic stabilization from an arbitrary starting position. The overall parking process is composed of two stages: a reference tracking stage and an asymptotically stabilization one. In the tracking stage, a reference trajectory is carefully generated by taking the robot kinematic constraints into consideration. An existing reference tracking controller is adopted to drive the robot to follow the prescribed route. In the second stage, a parking controller is switched on and is able to asymptotically stabilize the robot by taking advantage of straight and smooth motions in a “singularity line”. The overall performance of the parking system has been validated through simulation and real experiments.},
  archive      = {J_RAS},
  author       = {Zhengguo Li and Wenchao Gao and Jiawei Ong},
  doi          = {10.1016/j.robot.2019.103365},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103365},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A dual-stage parking system for differential-drive robots},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Biologically inspired jumping robots: A comprehensive
review. <em>RAS</em>, <em>124</em>, 103362. (<a
href="https://doi.org/10.1016/j.robot.2019.103362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Applying concepts and methods of bionics to endow autonomous robots with elegant and agile mobility just like natural living beings is gradually becoming a hot research topic in intelligent robot field. Compared with walking, crawling, rolling and other motion modes, jumping performs considerable advantages that can leap across obstacles and move to different heights in agility and flexibility. In this paper, we specifically review the developments of biologically inspired jumping robots in the past decades, and give comprehensive analysis on some key technologies for implementing a practical jumping robot effectively. First, the jumping mechanism of frog (amphibian, quadruped), locust (arthropod, hexapod), kangaroo (mammality, bipedalism) as examples of typical animals good at jumping is introduced and analyzed, from which it is concluded that power sources, limbs coordination and control are key elements for excellent jumping performances, which should be synthetically improved by combination with structure design and model establishment. Then, spring loaded inverted pendulum (SLIP), bio-inspired open-chain and closed-chain multi-linkage as representative jumping mechanical structures, their characteristics are explored accompanied with dynamic analysis. After a detailed analysis to actuators and energy storage devices and a comprehensive summarization to functional and soft materials commonly applied in jumping robots, different control methods and strategies adopted to achieve better jumping performance are reviewed and analyzed, from self-righting, driving control to path planning . Especially, how to analyze the stability of a jumping control system and how to stabilize it are explained theoretically by taking a vertical monopedal jumping robot as an example and via limit cycle analysis. Finally, some feasible and potential future developments in bio-inspired jumping robots are also presented after detailed discussions on current status and existing deficiencies.},
  archive      = {J_RAS},
  author       = {Chi Zhang and Wei Zou and Liping Ma and Zhiqing Wang},
  doi          = {10.1016/j.robot.2019.103362},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103362},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Biologically inspired jumping robots: A comprehensive review},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RISE-based adaptive control for EICoSI exoskeleton to assist
knee joint mobility. <em>RAS</em>, <em>124</em>, 103354. (<a
href="https://doi.org/10.1016/j.robot.2019.103354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exoskeleton devices are used to assist joint motion of subjects suffering from mobility deficiencies. Controlling an exoskeleton subjects to high nonlinearities, which are mainly due to the mechanical coupling, external disturbances , parameter uncertainties, and modeling errors. Keeping in view the requirement of a relatively accurate movement tracking while reducing the disturbances effects, there is a need for a robust controller. In this paper, an adaptive RISE (Robust Integral of Sign Error) controller is developed and implemented on the EICoSI (Exoskeleton Intelligently Communicating and Sensitive to Intention) knee exoskeleton. RISE has an advantage over standard controllers that it achieves semi-global asymptotic tracking even in the presence of unstructured disturbances. But to achieve this tracking, high control gains are required. Thus, to limit such high gains, RISE control strategy is combined with an adaptive controller , which has the advantage of improving the tracking performance while reducing the eventual overshoots. The stability of the coupled human/ exoskeleton system is analyzed based on Lyapunov theory and the system has shown semi-global asymptotic stability . The adaptive RISE controller gives better SNR (signal to noise ratio) by 11\% and 2\% as compared with adaptive and RISE controller respectively. In terms of tracking error, the adaptive RISE controller shows 9\% more RMSE than adaptive controller but 41\% less when compared with RISE controller Three experimental scenarios are analyzed to validate the proposed controller, namely (i) external disturbances , that could come from the ground during walking; (ii) induced payload, that could come from the resistive/assistive torque from the muscles and (iii) payload with external disturbances. The system is found to be robust and efficient in tracking the reference trajectories while maintaining limited error and high signal to noise ratio .},
  archive      = {J_RAS},
  author       = {Kashif I.K. Sherwani and Neelesh Kumar and Ahmed Chemori and Munna Khan and Samer Mohammed},
  doi          = {10.1016/j.robot.2019.103354},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103354},
  shortjournal = {Robot. Auton. Syst.},
  title        = {RISE-based adaptive control for EICoSI exoskeleton to assist knee joint mobility},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A data-driven approach to probabilistic impedance control
for humanoid robots. <em>RAS</em>, <em>124</em>, 103353. (<a
href="https://doi.org/10.1016/j.robot.2019.103353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel approach toward synthesizing whole-body motions from visual perception and reaction force for a humanoid robot that maintains a suitable physical interaction with an environment. A behavior containing a whole-body motion, reaction force, and visual perception is encoded into a probabilistic model referred to as a “motion symbol”. The humanoid robot selects a motion symbol appropriate to the current situation and computes references for joint angles and reaction forces according to the selected symbol. The robot subsequently modifies these references to satisfy a desired impedance relating the robot whole-body positions and forces. This computation builds visual and physical feedback loops with knowledge about the behaviors, making it possible for a humanoid robot to not only perform human-like motion behaviors similar to training behaviors, but to also physically adapt to the immediate environment. We applies this proposed framework only to controlling the upper-body motion for a humanoid robot. Experiments demonstrate that the proposed method allows a humanoid robot to control its upper-body motion in response to visual perception and reaction forces acting on its hands to achieve five tasks while controlling its lower-body motion for its balance.},
  archive      = {J_RAS},
  author       = {Wataru Takano and Hiroki Kanayama and Taro Takahashi and Tomohisa Moridaira and Yoshihiko Nakamura},
  doi          = {10.1016/j.robot.2019.103353},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103353},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A data-driven approach to probabilistic impedance control for humanoid robots},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Monocular person tracking and identification with on-line
deep feature selection for person following robots. <em>RAS</em>,
<em>124</em>, 103348. (<a
href="https://doi.org/10.1016/j.robot.2019.103348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new person tracking and identification framework based on solely a monocular camera. In this framework, we first track persons in the robot coordinate space using Unscented Kalman filter with the ground plane information and human height estimation. Then, we identify the target person to be followed with the combination of Convolutional Channel Features (CCF) and online boosting. It allows us to take advantage of deep neural network-based feature representation while adapting the person classifier to a specific target person depending on the circumstances. The entire system can be run on a recent embedded computation board with a GPU (NVIDIA Jetson TX2), and it can easily be reproduced and reused on a new mobile robot platform. Through evaluations, we validated that the proposed method outperforms existing person identification methods for mobile robots. We applied the proposed method to a real person following robot, and it has been shown that CCF-based person identification realizes robust person following in both indoor and outdoor environments.},
  archive      = {J_RAS},
  author       = {Kenji Koide and Jun Miura and Emanuele Menegatti},
  doi          = {10.1016/j.robot.2019.103348},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103348},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Monocular person tracking and identification with on-line deep feature selection for person following robots},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Waterline and obstacle detection in images from low-cost
autonomous boats for environmental monitoring. <em>RAS</em>,
<em>124</em>, 103346. (<a
href="https://doi.org/10.1016/j.robot.2019.103346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Waterline detection from images taken by cameras mounted on low-cost autonomous surface vehicles (ASVs) is a key process for obtaining a fast obstacle detection . Achieving an accurate waterline prediction is difficult due to the instability of the ASV on which the camera is mounted and the presence of reflections, illumination changes, and waves. In this work, we present a method for waterline and obstacle detection designed for low-cost ASVs employed in environmental monitoring. The proposed approach is made of two steps: (1) a pixel-wise segmentation of the current image is used to generate a binary mask separating water and non-water regions, (2) the mask is analyzed to infer the position of the waterline, which in turn is used for detecting obstacles. Experiments were carried out on two publicly available datasets containing floating obstacles such as buoys, sailing and motor boats , and swans moving near the ASV. Quantitative results show the effectiveness of the proposed approach with 98.8\% pixel-wise segmentation accuracy running at 10 frames per second on an embedded GPU board.},
  archive      = {J_RAS},
  author       = {L. Steccanella and D.D. Bloisi and A. Castellini and A. Farinelli},
  doi          = {10.1016/j.robot.2019.103346},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103346},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Waterline and obstacle detection in images from low-cost autonomous boats for environmental monitoring},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Active robot-assisted feeding with a general-purpose mobile
manipulator: Design, evaluation, and lessons learned. <em>RAS</em>,
<em>124</em>, 103344. (<a
href="https://doi.org/10.1016/j.robot.2019.103344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Eating is an essential activity of daily living (ADL) for staying healthy and living at home independently. Although numerous assistive devices have been introduced, many people with disabilities are still restricted from independent eating due to the devices’ physical or perceptual limitations. In this work, we present a new meal-assistance system and evaluations of this system with people with motor impairments . We also discuss learned lessons and design insights based on the evaluations. The meal-assistance system uses a general-purpose mobile manipulator , a Willow Garage PR2, which has the potential to serve as a versatile form of assistive technology . Our active feeding framework enables the robot to autonomously deliver food to the user’s mouth, reducing the need for head movement by the user. The user interface, visually-guided behaviors, and safety tools allow people with severe motor impairments to successfully use the system. We evaluated our system with a total of 10 able-bodied participants and 9 participants with motor impairments. Both groups of participants successfully ate various foods using the system and reported high rates of success for the system’s autonomous behaviors. In general, participants who operated the system reported that it was comfortable, safe, and easy-to-use.},
  archive      = {J_RAS},
  author       = {Daehyung Park and Yuuna Hoshi and Harshal P. Mahajan and Ho Keun Kim and Zackory Erickson and Wendy A. Rogers and Charles C. Kemp},
  doi          = {10.1016/j.robot.2019.103344},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103344},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Active robot-assisted feeding with a general-purpose mobile manipulator: Design, evaluation, and lessons learned},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A synthetic dataset for visual SLAM evaluation.
<em>RAS</em>, <em>124</em>, 103336. (<a
href="https://doi.org/10.1016/j.robot.2019.103336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-based self-localization methods are key functionalities for various research topics. Recent research results on related fields have catalyzed several accurate, versatile and reliable real-time Visual SLAM systems suitable for self-localization under a wide variety of environmental preconditions. These methods extend their functionalities from being only a good camera tracker to being able to recursively build up camera’s surroundings. The fast development of Visual SLAM research has proposed demands on innovating evaluation methods for Visual SLAM systems. However, retrieving images and ground truth from various kinds of environments, estimating calibration parameters between several sensors and annotating useful labels all require cumbersome human labor and will introduce inevitable errors. In this paper, we propose a method that uses virtually established models to automatically generate photorealistic images with accurate ground truth and several kinds of pixel-level annotations useful for Visual SLAM development and evaluation. We build and render a challenging dataset in low-texture environments with large scale camera movement, multiple moving objects and varying luminance status. We also propose several new evaluation criteria that can fully take advantage of ground truth and annotations from synthetic datasets . Experiments are conducted using the proposed datasets and criteria with several state-of-the-art Visual SLAM methods to demonstrate the functionality of our datasets.},
  archive      = {J_RAS},
  author       = {Senbo Wang and Jiguang Yue and Yanchao Dong and Shibo He and Haotian Wang and Shaochun Ning},
  doi          = {10.1016/j.robot.2019.103336},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103336},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A synthetic dataset for visual SLAM evaluation},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A robot-assisted bilateral upper limb training strategy with
subject-specific workspace: A pilot study. <em>RAS</em>, <em>124</em>,
103334. (<a href="https://doi.org/10.1016/j.robot.2019.103334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new robot-assisted bilateral upper limb training strategy, focusing on the bilateral coordination of users’ upper limbs. The strategy is implemented and evaluated on a bilateral upper limb rehabilitation device (BULReD) that is an H-bot mechanism actuated by two Maxon DC motors. The control system consists of a position controller, an admittance controller and an adaptive algorithm, where the BULReD stiffness is modified session by session based on training performance. This strategy is also integrated with subject-specific workspace for enhanced training safety. Experiments were carried out with five subjects through active reaching tasks. Results indicate that the proposed training strategy requires significant coordination of bilateral upper limbs for task completion, and is able to tune control parameters to an appropriate difficulty level based on participants’ training performance. Future work will focus on its clinical evaluation on patients with upper limb disabilities.},
  archive      = {J_RAS},
  author       = {Qing Miao and Mingming Zhang and Andrew McDaid and Yuxin Peng and Sheng Q. Xie},
  doi          = {10.1016/j.robot.2019.103334},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103334},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A robot-assisted bilateral upper limb training strategy with subject-specific workspace: A pilot study},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Flocking and topology manipulation based on space
partitioning. <em>RAS</em>, <em>124</em>, 103328. (<a
href="https://doi.org/10.1016/j.robot.2019.103328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network topology plays a critical role in enabling a multi-agent system to adapt to environment changes and achieve desired objectives. This paper presents distributed topology manipulation schemes for a group of mobile agents . The agents have limited heterogeneous communication ranges, and connections among them are directional. The topology is established from the overlapping communication ranges. The admissible space is partitioned into enclosed areas by connectivity among the agents based on their communication ranges. Each agent occupies an enclosed area, and its decision-making manipulates the topology by guiding itself to an adjacent enclosed area. Both independent and coordinated decision-making approaches are provided. A guidance algorithm is designed to drive the vehicles to a flexible formation, in which the robustness of the network topology is enhanced.},
  archive      = {J_RAS},
  author       = {Hongjun Yu and Cheng-Chew Lim and Robert Hunjet and Peng Shi},
  doi          = {10.1016/j.robot.2019.103328},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103328},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Flocking and topology manipulation based on space partitioning},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Touch driven controller and tactile features for physical
interactions. <em>RAS</em>, <em>123</em>, 103332. (<a
href="https://doi.org/10.1016/j.robot.2019.103332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an approach that considers controlling contact between a robot and the environment during physical interactions. Current physical interaction control approaches are limited in terms of the range of tasks that can be performed. To allow robots to perform more tasks, we derive tactile features representing deformations of the mechanically compliant sensing surface of a tactile sensor and incorporate these features to a robot controller , akin to a visual servo, via touch- and task-dependent tactile feature mapping matrices . As a first contribution, we derive tactile features to localize a contact coordinate frame between an object and an array of pressure sensing elements, with a mechanically compliant surface, attached onto a robot arm end-effector interacting with the object. As a second contribution, we propose tactile projection matrices to design a tactile servoing controller that combines these tactile features with a Cartesian impedance controller of the robot arm. These matrices convert the proposed tactile features to balance not only normal forces but also torques about the sensor’s axes. It allows the end-effector to steer the contact frame in a desired manner by regulating errors in the tactile features to address several common issues in robotics: exploration and co-manipulation.},
  archive      = {J_RAS},
  author       = {Zhanat Kappassov and Juan-Antonio Corrales and Véronique Perdereau},
  doi          = {10.1016/j.robot.2019.103332},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103332},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Touch driven controller and tactile features for physical interactions},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enabling garment-agnostic laundry tasks for a robot
household companion. <em>RAS</em>, <em>123</em>, 103330. (<a
href="https://doi.org/10.1016/j.robot.2019.103330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domestic chores, such as laundry tasks, are dull and repetitive. These tasks consume a significant amount of daily time, and are however unavoidable. Additionally, a great portion of elder and disabled people require help to perform them due to lack of mobility. In this work we present advances towards a Robot Household Companion (RHC), focusing on the performance of two particular laundry tasks: unfolding and ironing garments. Unfolding is required to recognize the garment prior to any later folding operation . For unfolding, we apply an interactive algorithm based on the analysis of a colored 3D reconstruction of the garment. Regions are clustered based on height, and a bumpiness value is computed to determine the most suitable pick and place points to unfold the overlapping region. For ironing, a custom Wrinkleness Local Descriptor (WiLD) descriptor is applied to a 3D reconstruction to find the most significant wrinkles in the garment. These wrinkles are then ironed using an iterative path-following control algorithm that regulates the amount of pressure exerted on the garment. Both algorithms focus on the feasibility of a physical implementation in real unmodified environments. A set of experiments to validate the algorithms have been performed using a full-sized humanoid robot .},
  archive      = {J_RAS},
  author       = {David Estevez and Juan G. Victores and Raul Fernandez-Fernandez and Carlos Balaguer},
  doi          = {10.1016/j.robot.2019.103330},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103330},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Enabling garment-agnostic laundry tasks for a robot household companion},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Admittance control based robotic clinical gait training with
physiological cost evaluation. <em>RAS</em>, <em>123</em>, 103326. (<a
href="https://doi.org/10.1016/j.robot.2019.103326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate an effective control strategy for a cane-type assistive mobile robot toward clinical gait training. Assistive robots are expected to provide aid in order to reduce the burden of caregivers and physical therapists, e.g., in gait rehabilitation of elderly people. Our group has been developing a series of cane-type walking assistive robots named Intelligent Cane as a mobile hand-holding device based on admittance control to provide safe and efficient gait training. This paper explores a systematic design methodology of the admittance control model in order to provide suitable walking load during gait training. We first conduct a pilot experiment to investigate the relationship between the physiological cost of user’s walking and the coefficients in the admittance control model of the cane robot. Then, we present a clinical gait training study conducted in a hospital to evaluate the feasibility in practical use of the proposed control strategy of our cane robot in gait rehabilitation. These experimental results suggest the effectiveness of the proposed gait rehabilitation strategy with our robot.},
  archive      = {J_RAS},
  author       = {Shunki Itadera and Jun Nakanishi and Yasuhisa Hasegawa and Toshio Fukuda and Masanori Tanimoto and Izumi Kondo},
  doi          = {10.1016/j.robot.2019.103326},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103326},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Admittance control based robotic clinical gait training with physiological cost evaluation},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Global matching of point clouds for scan registration and
loop detection. <em>RAS</em>, <em>123</em>, 103324. (<a
href="https://doi.org/10.1016/j.robot.2019.103324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a robust Global Matching technique focused on 3D mapping applications using laser range-finders. Our approach works under the assumption that places can be recognized by analyzing the projection of the observed points along the gravity direction. Relative poses between pairs of 3D point clouds are estimated by aligning their 2D projective representations and benefiting from the corresponding dimensional reduction . We present the complete processing pipeline for two different applications that use the global matcher as a core component: First, the global matcher is used for the registration of static scan sets where no a-priori information of the relative poses is available. It is combined with an effective procedure for validating the matches that exploits the implicit empty space information associated to single acquisitions. In the second use case, the global matcher is used for the loop detection required for 3D SLAM applications. We use an Extended Kalman Filter to obtain a belief of the map poses, which allows to validate matches and to execute hierarchical overlap tests, which reduce the number of potential matches to be evaluated. Additionally, the global matcher is combined with a fast local technique. In both use cases, the global reconstruction problem is modeled as a sparse graph, where scan poses (nodes) are connected through matches (edges). The graph structure allows formulating a sparse global optimization problem that optimizes scan poses, considering simultaneously all accepted matches. Our approach is being used in production systems and has been successfully evaluated on several real and publicly available datasets.},
  archive      = {J_RAS},
  author       = {Carlos Sánchez-Belenguer and Simone Ceriani and Pierluigi Taddei and Erik Wolfart and Vítor Sequeira},
  doi          = {10.1016/j.robot.2019.103324},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103324},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Global matching of point clouds for scan registration and loop detection},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Crop edge detection based on stereo vision. <em>RAS</em>,
<em>123</em>, 103323. (<a
href="https://doi.org/10.1016/j.robot.2019.103323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the development of a crop edge detection algorithm based on the point cloud produced by a stereo camera system using the GPU for fast matching of the camera images. The approach utilizes the 3D characteristics of the transition between the crop and the stubbles or the ground. Therefore, the point cloud is sorted into a grid of cells to create an elevation map. A segmentation in crop and ground is obtained using the Expectation–Maximization algorithm with a Gaussian Mixture Model to represent the distribution of the cell’s heights. This segmentation is Bayesian filtered over a short time frame to create a more robust segmentation result. Afterward, the resulting potential crop edge locations are processed using robust linear regression to come up with an overall linear crop edge model. The implemented system has been tested in a series of experiments with detailed results stated at the end of this work.},
  archive      = {J_RAS},
  author       = {Johannes Kneip and Patrick Fleischmann and Karsten Berns},
  doi          = {10.1016/j.robot.2019.103323},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103323},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Crop edge detection based on stereo vision},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A boundary node method for path planning of mobile robots.
<em>RAS</em>, <em>123</em>, 103320. (<a
href="https://doi.org/10.1016/j.robot.2019.103320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose a new method for solving the path planning problem in a static environment to find an optimal collision-free path between starting and goal points. First, the grid model of the robot’s working environment is constructed, and then the potential value of the grid cells is calculated based on the new proposed potential function. This function is used to guide the robot to move toward the desired goal, it has the lowest value at the goal position and the value is increased as the robot moves further away. Second, we developed an efficient method, called the Boundary Node Method, to find the initial feasible path. In this method, the robot is simulated by a nine-node quadrilateral element, where the centroid node represents the robot’s position. The robot moves in the working environment toward the goal with eight-boundary nodes based on the potential value of the boundary nodes. The initial feasible path is generated from a sequence of waypoints that the robot has to traverse as it moves toward the goal point without colliding with any obstacles. However, the proposed method can generate the path safely and efficiently, but the path is not optimal in terms of the total path length. Therefore, in order to construct an optimal or near-optimal collision-free path, an additional method, called the Path Enhancement Method, is developed. Finally, the cubic spline interpolation is adopted to generate a continuous smooth path that connects the starting point to the goal point. The proposed method has been tested in several working environments with different degrees of complexities. The results demonstrated that the proposed method is able to generate near-optimal collision-free path efficiently. Moreover, we compared the performance of the proposed methods with the other path planning methods in terms of path length and computational time. The results revealed that the proposed method can solve the robot path planning problem more efficiently. Finally, in order to verify the performance of the developed method for generating a collision-free path, experimental studies were carried out on the real robot.},
  archive      = {J_RAS},
  author       = {R.A. Saeed and Diego Reforgiato Recupero and Paolo Remagnino},
  doi          = {10.1016/j.robot.2019.103320},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103320},
  shortjournal = {Robot. Auton. Syst.},
  title        = {A boundary node method for path planning of mobile robots},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Indoor place classification by building cardinal-direction
prototyping blocks on point clouds. <em>RAS</em>, <em>123</em>, 103318.
(<a href="https://doi.org/10.1016/j.robot.2019.103318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Making robots know people’s place concepts has attracted researchers for decades. People believe that this capability will firmly benefit not only robot–human interaction but also reasonable and social robot behaviors, or even traditional problems in robot research such as object detection. Focusing on place classification, this paper builds a kind of native pure 3D geometric description to capture place layouts based on common point clouds. This perspective enables our method to naturally accommodate various illuminations, including extremely bad lighting for which traditional image methods cannot work properly. The space of a place is first divided into 3D voxels. The cardinal orientations of this space are then extracted, and the geometric attributes of the voxels are subsequently represented based on the cardinal orientations. The voxels with geometric attributes are defined as the cardinal-direction prototyping blocks (CDPBs). Next, the CDPB distribution for a scene is calculated by qualitative spatial description technology, thereby obtaining the complete place description. Given the sparse description, the sparse random forest (SRF) is used for learning. The experiments indicate that the CDPB-based method outperforms the current 3D geometric method and its mixed method, and it has good time performance. The main advantages of our method are that it does not require any strict hypotheses on surfaces, such as planar surfaces, it requires smaller fusion windows to attain satisfactory classification rates, it can be used in extreme lighting environments, and its parameter selection is easy.},
  archive      = {J_RAS},
  author       = {Bo Zhu and Xiang Gao and Guozheng Xu and Yi Wang and Youqi Zheng},
  doi          = {10.1016/j.robot.2019.103318},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103318},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Indoor place classification by building cardinal-direction prototyping blocks on point clouds},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Trajectory planning of a spatial flexible manipulator for
vibration suppression. <em>RAS</em>, <em>123</em>, 103316. (<a
href="https://doi.org/10.1016/j.robot.2019.103316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vibration decreases operational accuracy and productivity of flexible manipulators, and trajectory planning is an effective way to suppress vibration. However, the existing trajectory planning methods can only suppress vibration of planar flexible manipulators. In this paper, a trajectory planning method is proposed to suppress vibration of spatial flexible manipulators. Firstly, the dynamic models of each link of the flexible manipulator are established separately. Then with the constraint equations, the dynamic model of the flexible manipulator is established as a Differential Algebraic Equation (DAE). Secondly, the trajectory functions are designed as quintic polynomials, and the conditions are deduced to satisfy acceleration limits of each joint . Finally, the trajectory planning problem is transferred to an optimal problem. Particle Swam Optimization (PSO) is adopted to solve the optimal problem. Numerical simulation is conducted to demonstrate the good performance of the proposed method.},
  archive      = {J_RAS},
  author       = {Leilei Cui and Hesheng Wang and Weidong Chen},
  doi          = {10.1016/j.robot.2019.103316},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103316},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Trajectory planning of a spatial flexible manipulator for vibration suppression},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating and reacting to forces and torques resulting from
common aerodynamic disturbances acting on quadrotors. <em>RAS</em>,
<em>123</em>, 103314. (<a
href="https://doi.org/10.1016/j.robot.2019.103314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quadrotors are increasingly expected to perform a wide variety of tasks that put them in close proximity to other objects and surfaces in the environment (including other quadrotors), where they are often subject to significant external forces and torques resulting from aerodynamic effects . We present an algorithm – based on an Unscented Kalman Filter – that estimates such forces and torques without making assumptions about their source, allowing us to bypass much of the complexity involved in modeling how wind currents interact with quadrotor dynamics. Furthermore, our algorithm does not rely on special sensors, making it suitable for commercial systems where payload and add-on capabilities are limited. Via experiment we show that the estimation algorithm can be used in conjunction with controls and machine learning for detecting and avoiding downwash and walls, and for tracking wind from a fan. We also show that the algorithm is sensitive enough to measure even small changes in force and torque.},
  archive      = {J_RAS},
  author       = {Christopher D. McKinnon and Angela P. Schoellig},
  doi          = {10.1016/j.robot.2019.103314},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103314},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Estimating and reacting to forces and torques resulting from common aerodynamic disturbances acting on quadrotors},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multimodal representation models for prediction and control
from partial information. <em>RAS</em>, <em>123</em>, 103312. (<a
href="https://doi.org/10.1016/j.robot.2019.103312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Similar to humans, robots benefit from interacting with their environment through a number of different sensor modalities, such as vision, touch, sound. However, learning from different sensor modalities is difficult, because the learning model must be able to handle diverse types of signals, and learn a coherent representation even when parts of the sensor inputs are missing. In this paper, a multimodal variational autoencoder is proposed to enable an iCub humanoid robot to learn representations of its sensorimotor capabilities from different sensor modalities. The proposed model is able to (1) reconstruct missing sensory modalities , (2) predict the sensorimotor state of self and the visual trajectories of other agents actions, and (3) control the agent to imitate an observed visual trajectory. Also, the proposed multimodal variational autoencoder can capture the kinematic redundancy of the robot motion through the learned probability distribution. Training multimodal models is not trivial due to the combinatorial complexity given by the possibility of missing modalities. We propose a strategy to train multimodal models, which successfully achieves improved performance of different reconstruction models. Finally, extensive experiments have been carried out using an iCub humanoid robot , showing high performance in multiple reconstruction, prediction and imitation tasks.},
  archive      = {J_RAS},
  author       = {Martina Zambelli and Antoine Cully and Yiannis Demiris},
  doi          = {10.1016/j.robot.2019.103312},
  journal      = {Robotics and Autonomous Systems},
  pages        = {103312},
  shortjournal = {Robot. Auton. Syst.},
  title        = {Multimodal representation models for prediction and control from partial information},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
