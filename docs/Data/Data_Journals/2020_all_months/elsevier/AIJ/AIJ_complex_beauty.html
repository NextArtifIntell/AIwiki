<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>AIJ_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="aij---101">AIJ - 101</h2>
<ul>
<li><details>
<summary>
(2020). On composition of bounded-recall plans. <em>AIJ</em>,
<em>289</em>, 103399. (<a
href="https://doi.org/10.1016/j.artint.2020.103399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article studies the ability of agents with bounded memory to execute consecutive composition of plans. It gives an upper limit on the amount of memory required to execute the composed plans and shows that the limit cannot be improved. Furthermore, the article shows that there are, essentially, no other universal properties of plans for bounded-recall agents expressible through the relation “there is a plan for an agent with a given memory size to navigate from one given set of states to another”.},
  archive      = {J_AIJ},
  author       = {Kaya Deuser and Pavel Naumov},
  doi          = {10.1016/j.artint.2020.103399},
  journal      = {Artificial Intelligence},
  pages        = {103399},
  shortjournal = {Artif. Intell.},
  title        = {On composition of bounded-recall plans},
  volume       = {289},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Understanding the power of max-SAT resolution through
UP-resilience. <em>AIJ</em>, <em>289</em>, 103397. (<a
href="https://doi.org/10.1016/j.artint.2020.103397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A typical Branch and Bound algorithm for Max-SAT computes the lower bound by estimating the number of disjoint Inconsistent Subsets (IS) of the formula. The IS detection is ensured by Simulated Unit Propagation (SUP). Then, the inference rule for Max-SAT, Max-SAT resolution, is applied to ensure that the detected IS is counted only once. Learning Max-SAT resolution transformations can be detrimental to the algorithm performance, so they are usually selectively learned if they match certain patterns. In this paper, we study the impact of the transformations by Max-SAT resolution on the SUP mechanism, indispensable for IS detection. We introduce the notion of UP-resilience of a transformation which quantifies this impact and provides, from a theoretical point of view, an explanation to the empirical efficiency of the learning schemes developed in the last ten years. We also focus on recently introduced patterns called Unit Clause Subsets (UCSs). We characterize the transformations of certain UCSs using UP-resilience and we explain how our result can help extend the current patterns. Finally, we present empirical observations that support the relevance of the UP-resilience property and further consolidate our theoretical results.},
  archive      = {J_AIJ},
  author       = {Mohamed Sami Cherif and Djamal Habet and André Abramé},
  doi          = {10.1016/j.artint.2020.103397},
  journal      = {Artificial Intelligence},
  pages        = {103397},
  shortjournal = {Artif. Intell.},
  title        = {Understanding the power of max-SAT resolution through UP-resilience},
  volume       = {289},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time reasoning in OWL2 for GDPR compliance.
<em>AIJ</em>, <em>289</em>, 103389. (<a
href="https://doi.org/10.1016/j.artint.2020.103389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper shows how knowledge representation and reasoning techniques can be used to support organizations in complying with the GDPR , that is, the new European data protection regulation. This work is carried out in a European H2020 project called SPECIAL. Data usage policies, the consent of data subjects, and selected fragments of the GDPR are encoded in a fragment of OWL2 called PL PL (policy language); compliance checking and policy validation are reduced to subsumption checking and concept consistency checking . This work proposes a satisfactory tradeoff between the expressiveness requirements on PL PL posed by the modeling of the GDPR, and the scalability requirements that arise from the use cases provided by SPECIAL&#39;s industrial partners. Real-time compliance checking is achieved by means of a specialized reasoner , called PLR PLR , that leverages knowledge compilation and structural subsumption techniques. The performance of a prototype implementation of PLR PLR is analyzed through systematic experiments, and compared with the performance of other important reasoners. Moreover, we show how PL PL and PLR PLR can be extended to support richer ontologies, by means of import-by-query techniques. We prove novel tractability and intractability results related to PL PL , and some negative results about the restrictions posed on ontology import.},
  archive      = {J_AIJ},
  author       = {Piero A. Bonatti and Luca Ioffredo and Iliana M. Petrova and Luigi Sauro and Ida R. Siahaan},
  doi          = {10.1016/j.artint.2020.103389},
  journal      = {Artificial Intelligence},
  pages        = {103389},
  shortjournal = {Artif. Intell.},
  title        = {Real-time reasoning in OWL2 for GDPR compliance},
  volume       = {289},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Explanation in AI and law: Past, present and future.
<em>AIJ</em>, <em>289</em>, 103387. (<a
href="https://doi.org/10.1016/j.artint.2020.103387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explanation has been a central feature of AI systems for legal reasoning since their inception. Recently, the topic of explanation of decisions has taken on a new urgency, throughout AI in general, with the increasing deployment of AI tools and the need for lay users to be able to place trust in the decisions that the support tools are recommending. This paper provides a comprehensive review of the variety of techniques for explanation that have been developed in AI and Law. We summarise the early contributions and how these have since developed. We describe a number of notable current methods for automated explanation of legal reasoning and we also highlight gaps that must be addressed by future systems to ensure that accurate, trustworthy, unbiased decision support can be provided to legal professionals. We believe that insights from AI and Law, where explanation has long been a concern, may provide useful pointers for future development of explainable AI .},
  archive      = {J_AIJ},
  author       = {Katie Atkinson and Trevor Bench-Capon and Danushka Bollegala},
  doi          = {10.1016/j.artint.2020.103387},
  journal      = {Artificial Intelligence},
  pages        = {103387},
  shortjournal = {Artif. Intell.},
  title        = {Explanation in AI and law: Past, present and future},
  volume       = {289},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Artificial intelligence requires more than deep learning —
but what, exactly? <em>AIJ</em>, <em>289</em>, 103386. (<a
href="https://doi.org/10.1016/j.artint.2020.103386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AIJ},
  author       = {Michael Wooldridge},
  doi          = {10.1016/j.artint.2020.103386},
  journal      = {Artificial Intelligence},
  pages        = {103386},
  shortjournal = {Artif. Intell.},
  title        = {Artificial intelligence requires more than deep learning — but what, exactly?},
  volume       = {289},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). So, what exactly is a qualitative calculus? <em>AIJ</em>,
<em>289</em>, 103385. (<a
href="https://doi.org/10.1016/j.artint.2020.103385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paradigm of algebraic constraint-based reasoning, embodied in the notion of a qualitative calculus, is studied within two alternative frameworks. One framework defines a qualitative calculus as “a non-associative relation algebra (NA) with a qualitative representation”, the other as “an algebra generated by jointly exhaustive and pairwise disjoint (JEPD) relations”. These frameworks provide complementary perspectives: the first is intensional (axiom-based), whereas the second one is extensional (based on semantic structures). However, each definition admits calculi that lie beyond the scope of the other. Thus, a qualitatively representable NA may be incomplete or non-atomic, whereas an algebra generated by JEPD relations may have non-involutive converse and no identity element. The divergence of definitions creates a confusion around the notion of a qualitative calculus and makes the “what” question posed by Ligozat and Renz actual once again. Here we define the relation-type qualitative calculus unifying the intensional and extensional approaches. By introducing the notions of weak identity, inference completeness and Q-homomorphism, we give equivalent definitions of qualitative calculi both intensionally and extensionally. We show that “algebras generated by JEPD relations” and “qualitatively representable NAs” are embedded into the class of relation-type qualitative algebras.},
  archive      = {J_AIJ},
  author       = {Armen Inants and Jérôme Euzenat},
  doi          = {10.1016/j.artint.2020.103385},
  journal      = {Artificial Intelligence},
  pages        = {103385},
  shortjournal = {Artif. Intell.},
  title        = {So, what exactly is a qualitative calculus?},
  volume       = {289},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quantifying controllability in temporal networks with
uncertainty. <em>AIJ</em>, <em>289</em>, 103384. (<a
href="https://doi.org/10.1016/j.artint.2020.103384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Controllability for Simple Temporal Networks with Uncertainty (STNUs) has thus far been limited to three levels: strong, dynamic, and weak. Because of this, there is currently no systematic way for an agent to assess just how far from being controllable an uncontrollable STNU is. We provide new insights inspired by a geometric interpretation of STNUs to introduce the degrees of strong and dynamic controllability — continuous metrics that measure how far a network is from being controllable. We utilize these metrics to approximate the probabilities that an STNU can be dispatched successfully offline and online respectively. We introduce new methods for predicting the degrees of strong and dynamic controllability for uncontrollable networks. We further generalize these metrics by defining likelihood of controllability, a controllability measure that applies to Probabilistic Simple Temporal Networks (PSTNs). Finally, we empirically demonstrate that these metrics are good predictors of actual dispatch success rate for STNUs and PSTNs.},
  archive      = {J_AIJ},
  author       = {Shyan Akmal and Savana Ammons and Hemeng Li and Michael Gao and Lindsay Popowski and James C. Boerkoel Jr.},
  doi          = {10.1016/j.artint.2020.103384},
  journal      = {Artificial Intelligence},
  pages        = {103384},
  shortjournal = {Artif. Intell.},
  title        = {Quantifying controllability in temporal networks with uncertainty},
  volume       = {289},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Intrinsic approaches to prioritizing diagnoses in
multi-context systems. <em>AIJ</em>, <em>289</em>, 103383. (<a
href="https://doi.org/10.1016/j.artint.2020.103383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-context systems introduced by Brewka and Eiter provide a promising framework for interlinking heterogeneous and autonomous knowledge sources. The notion of diagnosis has been proposed for analyzing inconsistency in multi-context systems, which captures a pair of subsets of bridge rules of a multi-context system needed to be deactivated and activated unconditionally, respectively, in order to restore the consistency for that system. Generally, diagnoses need to be prioritized from some specific perspectives in order to select the most desirable ones to resolve inconsistency. In this paper, we propose a series of intrinsic approaches to prioritizing diagnoses based on the structure of information exchange over contexts in a multi-context system, which allow us to rank diagnoses in cases where no external knowledge is available. We use a heterogeneous graph, termed information exchange network, to formulate the information exchange over contexts via bridge rules in a multi-context system. Then we propose three kinds of approaches to prioritizing diagnoses based on the information exchange network for a multi-context system. Approaches of the first kind focus on ranking diagnoses by evaluating the impact of potential revision according to each diagnosis on direct information exchange over contexts from different perspectives, whilst approaches of the second kind rank diagnoses by evaluating the impact of potential revision according to each diagnosis on betweenness centralities of contexts with regard to the whole information exchange network. The last one uses the betweenness centralities of bridge rules in the information exchange network to prioritize diagnoses directly.},
  archive      = {J_AIJ},
  author       = {Kedian Mu},
  doi          = {10.1016/j.artint.2020.103383},
  journal      = {Artificial Intelligence},
  pages        = {103383},
  shortjournal = {Artif. Intell.},
  title        = {Intrinsic approaches to prioritizing diagnoses in multi-context systems},
  volume       = {289},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Autoepistemic answer set programming. <em>AIJ</em>,
<em>289</em>, 103382. (<a
href="https://doi.org/10.1016/j.artint.2020.103382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Defined by Gelfond in 1991, epistemic specifications constitute an extension of Answer Set Programming (ASP) that introduces subjective literals . A subjective literal allows checking whether some regular literal is true in all (or in some of) the answer sets of the program, that are further collected in a set called world view . One epistemic program may yield several world views but, under the original semantics, some of them resulted from self-supported derivations. During the last eight years, several alternative approaches have been proposed to get rid of these self-supported world views. Unfortunately, their success could only be measured by studying their behaviour on a set of common examples in the literature, since no formal property of “self-supportedness” had been defined. To fill this gap, we extend in this paper the idea of unfounded set from standard logic programming to the epistemic case. We define when a world view is founded with respect to some program. Accordingly, we define the foundedness property for an arbitrary semantics, so it holds when its world views are always founded. Using counterexamples , we explain that the previous approaches violate foundedness, and proceed to propose a new semantics based on a combination of Moore&#39;s Autoepistemic Logic and Pearce&#39;s Equilibrium Logic. This combination paves the way for the development of an autoepistemic extension of ASP. The main result proves that this new semantics precisely captures the set of world views of the original semantics that are founded.},
  archive      = {J_AIJ},
  author       = {Pedro Cabalar and Jorge Fandinno and Luis Fariñas del Cerro},
  doi          = {10.1016/j.artint.2020.103382},
  journal      = {Artificial Intelligence},
  pages        = {103382},
  shortjournal = {Artif. Intell.},
  title        = {Autoepistemic answer set programming},
  volume       = {289},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). When security games hit traffic: A deployed optimal traffic
enforcement system. <em>AIJ</em>, <em>289</em>, 103381. (<a
href="https://doi.org/10.1016/j.artint.2020.103381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Road accidents are the leading causes of death among youths and young adults worldwide. Efficient traffic enforcement is an essential, yet complex, component in preventing road accidents. In this article, we present a novel model, an optimizing algorithm and a deployed system which together mitigate many of the computational and real-world challenges of traffic enforcement allocation in large road networks . Our approach allows for scalable, coupled and non-Markovian optimization of multiple police units and guarantees optimality . Our deployed system, which utilizes the proposed approach, is used by the Israeli traffic police and is shown to provide meaningful benefits compared to existing standard traffic police enforcement practices.},
  archive      = {J_AIJ},
  author       = {Ariel Rosenfeld and Oleg Maksimov and Sarit Kraus},
  doi          = {10.1016/j.artint.2020.103381},
  journal      = {Artificial Intelligence},
  pages        = {103381},
  shortjournal = {Artif. Intell.},
  title        = {When security games hit traffic: A deployed optimal traffic enforcement system},
  volume       = {289},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SAT-based explicit LTLf satisfiability checking.
<em>AIJ</em>, <em>289</em>, 103369. (<a
href="https://doi.org/10.1016/j.artint.2020.103369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear Temporal Logic over finite traces ( LTL f LTLf ) was proposed in 2013 and has attracted increasing interest around the AI community. Though the theoretic basis for LTL f LTLf has been thoroughly explored since that time, there are still few algorithmic tools that are able to provide an efficient reasoning strategy for LTL f LTLf . In this paper, we present a SAT-based framework for LTL f LTLf satisfiability checking, which is the foundation of LTL f LTLf reasoning. We use propositional SAT-solving techniques to construct a transition system, which is an automata-style structure, for an input LTL f LTLf formula; satisfiability checking is then reduced to a path-search problem over this transition system. Based on this framework, we further present CDLSC (Conflict-Driven LTL f LTLf Satisfiability Checking), a novel algorithm (heuristic) that leverages information produced by propositional SAT solvers, utilizing both satisfiability and unsatisfiability results. More specifically, the satisfiable results of the SAT solver are used to create new states of the transition system and the unsatisfiable results to accelerate the path search over the system. We evaluate all 5 off-the-shelf LTL f LTLf satisfiability algorithms against our new approach CDLSC . Based on a comprehensive evaluation over 4 different LTL f LTLf benchmark suits with a total amount of 9317 formulas, our time-cost analysis shows that 1) CDLSC performs best on checking unsatisfiable formulas by achieving approximately a 4X time speedup, compared to the second-best solution (K-LIVE [1] ); 2) Although no approaches dominate checking satisfiable formulas, CDLSC performs best on 2 of the total 4 tested satisfiable benchmark suits; and 3) CDLSC gains the best overall performance when considering both satisfiable and unsatisfiable instances.},
  archive      = {J_AIJ},
  author       = {Jianwen Li and Geguang Pu and Yueling Zhang and Moshe Y. Vardi and Kristin Y. Rozier},
  doi          = {10.1016/j.artint.2020.103369},
  journal      = {Artificial Intelligence},
  pages        = {103369},
  shortjournal = {Artif. Intell.},
  title        = {SAT-based explicit LTLf satisfiability checking},
  volume       = {289},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dashed strings for string constraint solving. <em>AIJ</em>,
<em>289</em>, 103368. (<a
href="https://doi.org/10.1016/j.artint.2020.103368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {String processing is ubiquitous across computer science, and arguably more so in web programming — where it is also a critical part of security issues such as injection attacks. In recent years, a number of string solvers have been developed to solve combinatorial problems involving string variables and constraints. We examine the dashed string approach to string constraint solving, which represents an unknown string as a sequence of blocks of characters with bounds on their cardinalities. The solving approach relies on propagation of information about the blocks of characters that arise from reasoning about the constraints in which they occur. This approach shows promising performance on many benchmarks involving constraints like string length, equality, concatenation, and regular expression membership. In this paper, we formally review the definition, the properties and the use of dashed strings for string constraint solving, and we provide an empirical validation that confirms the effectiveness of this approach.},
  archive      = {J_AIJ},
  author       = {Roberto Amadini and Graeme Gange and Peter J. Stuckey},
  doi          = {10.1016/j.artint.2020.103368},
  journal      = {Artificial Intelligence},
  pages        = {103368},
  shortjournal = {Artif. Intell.},
  title        = {Dashed strings for string constraint solving},
  volume       = {289},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Probability pooling for dependent agents in collective
learning. <em>AIJ</em>, <em>288</em>, 103371. (<a
href="https://doi.org/10.1016/j.artint.2020.103371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of copulas is proposed as a way of modelling dependencies between different agents&#39; probability judgements when carrying out probability pooling. This is combined with an established Bayesian model in which pooling is viewed as a form of updating on the basis of probability values provided by different individuals. Adopting the Frank family of copulas we investigate the effect of different assumed levels of comonotonic dependence between individuals, in the context of a collective learning problem in which a population of agents must reach consensus on which of two mutually exclusive and exhaustive hypotheses is true. In this scenario agents receive evidence from two sources; directly from the environment and also from other agents in the form of probability judgements. They then apply Bayesian updating to the former and probability pooling to the latter. We carry out multi-agent simulation experiments and show that optimal population level performance is obtained under the assumption of some degree of comonotonicity between agents, and consequently show that the standard assumption of agent independence is suboptimal. This is found to be particularly true of scenarios where there is a large amount of noise and very low amounts of direct evidence. Finally, we investigate dynamic environments in which the true state of the world changes and show that identifying the optimal level of agent dependency has an even greater effect on performance than for static environments in which the true state remains constant.},
  archive      = {J_AIJ},
  author       = {Jonathan Lawry and Chanelle Lee},
  doi          = {10.1016/j.artint.2020.103371},
  journal      = {Artificial Intelligence},
  pages        = {103371},
  shortjournal = {Artif. Intell.},
  title        = {Probability pooling for dependent agents in collective learning},
  volume       = {288},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fixed point semantics for stream reasoning. <em>AIJ</em>,
<em>288</em>, 103370. (<a
href="https://doi.org/10.1016/j.artint.2020.103370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reasoning over streams of input data is an essential part of human intelligence. During the last decade stream reasoning has emerged as a research area within the AI-community with many potential applications. In fact, the increased availability of streaming data via services like Google and Facebook has raised the need for reasoning engines coping with data that changes at high rate. Recently, the rule-based formalism LARS for non-monotonic stream reasoning under the answer set semantics has been introduced. Syntactically, LARS programs are logic programs with negation incorporating operators for temporal reasoning, most notably window operators for selecting relevant time points. Unfortunately, by preselecting fixed intervals for the semantic evaluation of programs, the rigid semantics of LARS programs is not flexible enough to constructively cope with rapidly changing data dependencies . Moreover, we show that defining the answer set semantics of LARS in terms of FLP reducts leads to undesirable circular justifications similar to other ASP extensions. This paper fixes all of the aforementioned shortcomings of LARS. More precisely, we contribute to the foundations of stream reasoning by providing an operational fixed point semantics for a fully flexible variant of LARS and we show that our semantics is sound and constructive in the sense that answer sets are derivable bottom-up and free of circular justifications.},
  archive      = {J_AIJ},
  author       = {Christian Antić},
  doi          = {10.1016/j.artint.2020.103370},
  journal      = {Artificial Intelligence},
  pages        = {103370},
  shortjournal = {Artif. Intell.},
  title        = {Fixed point semantics for stream reasoning},
  volume       = {288},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interestingness elements for explainable reinforcement
learning: Understanding agents’ capabilities and limitations.
<em>AIJ</em>, <em>288</em>, 103367. (<a
href="https://doi.org/10.1016/j.artint.2020.103367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an explainable reinforcement learning (XRL) framework that analyzes an agent&#39;s history of interaction with the environment to extract interestingness elements that help explain its behavior. The framework relies on data readily available from standard RL algorithms, augmented with data that can easily be collected by the agent while learning. We describe how to create visual summaries of an agent&#39;s behavior in the form of short video-clips highlighting key interaction moments, based on the proposed elements. We also report on a user study where we evaluated the ability of humans to correctly perceive the aptitude of agents with different characteristics, including their capabilities and limitations, given visual summaries automatically generated by our framework. The results show that the diversity of aspects captured by the different interestingness elements is crucial to help humans correctly understand an agent&#39;s strengths and limitations in performing a task, and determine when it might need adjustments to improve its performance.},
  archive      = {J_AIJ},
  author       = {Pedro Sequeira and Melinda Gervasio},
  doi          = {10.1016/j.artint.2020.103367},
  journal      = {Artificial Intelligence},
  pages        = {103367},
  shortjournal = {Artif. Intell.},
  title        = {Interestingness elements for explainable reinforcement learning: Understanding agents&#39; capabilities and limitations},
  volume       = {288},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Utilitarian welfare and representation guarantees of
approval-based multiwinner rules. <em>AIJ</em>, <em>288</em>, 103366.
(<a href="https://doi.org/10.1016/j.artint.2020.103366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To choose a suitable multiwinner voting rule is a hard and ambiguous task. Depending on the context, it varies widely what constitutes the choice of an “optimal” subset of alternatives. In this paper, we provide a quantitative analysis of multiwinner voting rules using methods from the theory of approximation algorithms—we estimate how well multiwinner rules approximate two extreme objectives: a representation criterion defined via the Approval Chamberlin–Courant rule and a utilitarian criterion defined via Multiwinner Approval Voting. With both theoretical and experimental methods, we classify multiwinner rules in terms of their quantitative alignment with these two opposing objectives. Our results provide fundamental information about the nature of multiwinner rules and, in particular, about the necessary tradeoffs when choosing such a rule.},
  archive      = {J_AIJ},
  author       = {Martin Lackner and Piotr Skowron},
  doi          = {10.1016/j.artint.2020.103366},
  journal      = {Artificial Intelligence},
  pages        = {103366},
  shortjournal = {Artif. Intell.},
  title        = {Utilitarian welfare and representation guarantees of approval-based multiwinner rules},
  volume       = {288},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Knowledge-based programs as succinct policies for partially
observable domains. <em>AIJ</em>, <em>288</em>, 103365. (<a
href="https://doi.org/10.1016/j.artint.2020.103365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We suggest to express policies for contingent planning by knowledge-based programs (KBPs). KBPs, introduced by Fagin et al. (1995) [32] , are high-level protocols describing the actions that the agent should perform as a function of their current knowledge: branching conditions are epistemic formulas that are interpretable by the agent. The main aim of our paper is to show that KBPs can be seen as a succinct language for expressing policies in single-agent contingent planning. KBP are conceptually very close to languages used for expressing policies in the partially observable planning literature: like them, they have conditional and looping structures, with actions as atomic programs and Boolean formulas on beliefs for choosing the execution path. Now, the specificity of KBPs is that branching conditions refer to the belief state and not to the observations. Because of their structural proximity, KBPs and standard languages for representing policies have the same power of expressivity: every standard policy can be expressed as a KBP, and every KBP can be “unfolded” into a standard policy. However, KBPs are more succinct, more readable, and more explainable than standard policies. On the other hand, they require more online computation time, but we show that this is an unavoidable tradeoff. We study knowledge-based programs along four criteria: expressivity, succinctness, complexity of online execution, and complexity of verification.},
  archive      = {J_AIJ},
  author       = {Bruno Zanuttini and Jérôme Lang and Abdallah Saffidine and François Schwarzentruber},
  doi          = {10.1016/j.artint.2020.103365},
  journal      = {Artificial Intelligence},
  pages        = {103365},
  shortjournal = {Artif. Intell.},
  title        = {Knowledge-based programs as succinct policies for partially observable domains},
  volume       = {288},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Price of pareto optimality in hedonic games. <em>AIJ</em>,
<em>288</em>, 103357. (<a
href="https://doi.org/10.1016/j.artint.2020.103357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Price of Anarchy measures the welfare loss caused by selfish behavior: it is defined as the ratio of the social welfare in a socially optimal outcome and in a worst Nash equilibrium . Similar measures can be derived for other classes of stable outcomes. We observe that Pareto optimality can be seen as a notion of stability: an outcome is Pareto optimal if and only if it does not admit a deviation by the grand coalition that makes all players weakly better off and some players strictly better off. Motivated by this observation, we introduce the concept of Price of Pareto Optimality: this is an analogue of the Price of Anarchy, with the worst Nash equilibrium replaced with the worst Pareto optimal outcome. We then study this concept in the context of hedonic games, and provide lower and upper bounds on the Price of Pareto Optimality in three classes of hedonic games: additively separable hedonic games, fractional hedonic games, and modified fractional hedonic games.},
  archive      = {J_AIJ},
  author       = {Edith Elkind and Angelo Fanelli and Michele Flammini},
  doi          = {10.1016/j.artint.2020.103357},
  journal      = {Artificial Intelligence},
  pages        = {103357},
  shortjournal = {Artif. Intell.},
  title        = {Price of pareto optimality in hedonic games},
  volume       = {288},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Negotiating team formation using deep reinforcement
learning. <em>AIJ</em>, <em>288</em>, 103356. (<a
href="https://doi.org/10.1016/j.artint.2020.103356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When autonomous agents interact in the same environment, they must often cooperate to achieve their goals. One way for agents to cooperate effectively is to form a team, make a binding agreement on a joint plan, and execute it. However, when agents are self-interested, the gains from team formation must be allocated appropriately to incentivize agreement. Various approaches for multi-agent negotiation have been proposed, but typically only work for particular negotiation protocols. More general methods usually require human input or domain-specific data, and so do not scale. To address this, we propose a framework for training agents to negotiate and form teams using deep reinforcement learning . Importantly, our method makes no assumptions about the specific negotiation protocol, and is instead completely experience driven. We evaluate our approach on both non-spatial and spatially extended team-formation negotiation environments, demonstrating that our agents beat hand-crafted bots and reach negotiation outcomes consistent with fair solutions predicted by cooperative game theory . Additionally, we investigate how the physical location of agents influences negotiation outcomes.},
  archive      = {J_AIJ},
  author       = {Yoram Bachrach and Richard Everett and Edward Hughes and Angeliki Lazaridou and Joel Z. Leibo and Marc Lanctot and Michael Johanson and Wojciech M. Czarnecki and Thore Graepel},
  doi          = {10.1016/j.artint.2020.103356},
  journal      = {Artificial Intelligence},
  pages        = {103356},
  shortjournal = {Artif. Intell.},
  title        = {Negotiating team formation using deep reinforcement learning},
  volume       = {288},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Combining experts’ causal judgments. <em>AIJ</em>,
<em>288</em>, 103355. (<a
href="https://doi.org/10.1016/j.artint.2020.103355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider a policymaker who wants to decide which intervention to perform in order to change a currently undesirable situation. The policymaker has at her disposal a team of experts, each with their own understanding of the causal dependencies between different factors contributing to the outcome. The policymaker has varying degrees of confidence in the experts&#39; opinions. She wants to combine their opinions in order to decide on the most effective intervention. We formally define the notion of an effective intervention, and then consider how experts&#39; causal judgments can be combined in order to determine the most effective intervention. We define a notion of two causal models being compatible , and show how compatible causal models can be merged. We then use it as the basis for combining experts&#39; causal judgments. We also provide a definition of decomposition for causal models to cater for cases when models are incompatible. We illustrate our approach on a number of real-life examples.},
  archive      = {J_AIJ},
  author       = {Dalal Alrajeh and Hana Chockler and Joseph Y. Halpern},
  doi          = {10.1016/j.artint.2020.103355},
  journal      = {Artificial Intelligence},
  pages        = {103355},
  shortjournal = {Artif. Intell.},
  title        = {Combining experts&#39; causal judgments},
  volume       = {288},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Old techniques in new ways: Clause weighting, unit
propagation and hybridization for maximum satisfiability. <em>AIJ</em>,
<em>287</em>, 103354. (<a
href="https://doi.org/10.1016/j.artint.2020.103354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maximum Satisfiability (MaxSAT) is a basic and important constraint optimization problem . When dealing with both hard and soft constraints, the MaxSAT problem is referred to as Partial MaxSAT, which has been used to effectively solve many combinatorial optimization problems in real world. The local search method and the SAT-based method are two popular methods for Partial MaxSAT. Nevertheless, local search algorithms have been dominated by SAT-based algorithms on industrial benchmarks. This work develops an effective local search algorithm for industrial benchmarks, which for the first time is competitive with SAT-based algorithms on industrial MaxSAT benchmarks. We propose a local search algorithm, called SATLike, which exploits the structure of Partial MaxSAT by a novel clause weighting scheme. Then we improve SATLike by integrating a unit propagation based decimation algorithm to generate initial solutions, leading to an improved algorithm, called SATLike 2.0. We also refine SATLike 2.0 and obtain SATLike 3.0. Furthermore, we apply SATLike 3.0 to improve two SAT-based solvers, leading to two hybrid MaxSAT solvers that push the state of the art in MaxSAT solving. Experiments on benchmarks from the MaxSAT Evaluations 2017 and 2018 show that SATLike significantly outperforms previous local search solvers on all the benchmarks. More importantly, SATLike 3.0 has better performance than state-of-the-art SAT-based solvers on unweighted industrial benchmarks. The two hybridized solvers obtain significant improvements on both unweighted and weighted benchmarks.},
  archive      = {J_AIJ},
  author       = {Shaowei Cai and Zhendong Lei},
  doi          = {10.1016/j.artint.2020.103354},
  journal      = {Artificial Intelligence},
  pages        = {103354},
  shortjournal = {Artif. Intell.},
  title        = {Old techniques in new ways: Clause weighting, unit propagation and hybridization for maximum satisfiability},
  volume       = {287},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automated temporal equilibrium analysis: Verification and
synthesis of multi-player games. <em>AIJ</em>, <em>287</em>, 103353. (<a
href="https://doi.org/10.1016/j.artint.2020.103353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of multi-agent systems, the rational verification problem is concerned with checking which temporal logic properties will hold in a system when its constituent agents are assumed to behave rationally and strategically in pursuit of individual objectives. Typically, those objectives are expressed as temporal logic formulae which the relevant agent desires to see satisfied. Unfortunately, rational verification is computationally complex, and requires specialised techniques in order to obtain practically useable implementations. In this paper, we present such a technique. This technique relies on a reduction of the rational verification problem to the solution of a collection of parity games. Our approach has been implemented in the E quilibrium V erification E nvironment ( EVE ) system. The EVE system takes as input a model of a concurrent/multi-agent system represented using the Simple Reactive Modules Language (SRML), where agent goals are represented as Linear Temporal Logic ( ) formulae, together with a claim about the equilibrium behaviour of the system, also expressed as an formula. EVE can then check whether the claim holds on some (or every) computation of the system that could arise through agents choosing Nash equilibrium strategies; it can also check whether a system has a Nash equilibrium , and synthesise individual strategies for players in the multi-player game. After presenting our basic framework, we describe our new technique and prove its correctness. We then describe our implementation in the EVE system, and present experimental results which show that EVE performs favourably in comparison to other existing tools that support rational verification.},
  archive      = {J_AIJ},
  author       = {Julian Gutierrez and Muhammad Najib and Giuseppe Perelli and Michael Wooldridge},
  doi          = {10.1016/j.artint.2020.103353},
  journal      = {Artificial Intelligence},
  pages        = {103353},
  shortjournal = {Artif. Intell.},
  title        = {Automated temporal equilibrium analysis: Verification and synthesis of multi-player games},
  volume       = {287},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Probabilistic reasoning about epistemic action narratives.
<em>AIJ</em>, <em>287</em>, 103352. (<a
href="https://doi.org/10.1016/j.artint.2020.103352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose the action language EPEC – Epistemic Probabilistic Event Calculus – that supports probabilistic, epistemic reasoning about narratives of action occurrences and environmentally triggered events, and in particular facilitates reasoning about future belief-conditioned actions and their consequences in domains that include both perfect and imperfect sensing actions. To provide a declarative semantics for sensing and belief conditioned actions in a probabilistic, narrative setting we introduce the novel concept of an epistemic reduct . We then formally compare our language with two established frameworks for probabilistic reasoning about action – the action language PAL by Baral et al., and the extension of the situation calculus to reason about noisy sensors and effectors by Bacchus et al. In both cases we prove a correspondence with EPEC for a class of domains representable in both frameworks.},
  archive      = {J_AIJ},
  author       = {Fabio Aurelio D&#39;Asaro and Antonis Bikakis and Luke Dickens and Rob Miller},
  doi          = {10.1016/j.artint.2020.103352},
  journal      = {Artificial Intelligence},
  pages        = {103352},
  shortjournal = {Artif. Intell.},
  title        = {Probabilistic reasoning about epistemic action narratives},
  volume       = {287},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A technical survey on statistical modelling and design
methods for crowdsourcing quality control. <em>AIJ</em>, <em>287</em>,
103351. (<a href="https://doi.org/10.1016/j.artint.2020.103351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online crowdsourcing provides a scalable and inexpensive means to collect knowledge (e.g. labels) about various types of data items (e.g. text, audio, video). However, it is also known to result in large variance in the quality of recorded responses which often cannot be directly used for training machine learning systems . To resolve this issue, a lot of work has been conducted to control the response quality such that low-quality responses cannot adversely affect the performance of the machine learning systems . Such work is referred to as the quality control for crowdsourcing. Past quality control research can be divided into two major branches: quality control mechanism design and statistical models . The first branch focuses on designing measures, thresholds, interfaces and workflows for payment, gamification , question assignment and other mechanisms that influence workers&#39; behaviour. The second branch focuses on developing statistical models to perform effective aggregation of responses to infer correct responses. The two branches are connected as statistical models (i) provide parameter estimates to support the measure and threshold calculation, and (ii) encode modelling assumptions used to derive (theoretical) performance guarantees for the mechanisms. There are surveys regarding each branch but they lack technical details about the other branch. Our survey is the first to bridge the two branches by providing technical details on how they work together under frameworks that systematically unify crowdsourcing aspects modelled by both of them to determine the response quality. We are also the first to provide taxonomies of quality control papers based on the proposed frameworks. Finally, we specify the current limitations and the corresponding future directions for the quality control research.},
  archive      = {J_AIJ},
  author       = {Yuan Jin and Mark Carman and Ye Zhu and Yong Xiang},
  doi          = {10.1016/j.artint.2020.103351},
  journal      = {Artificial Intelligence},
  pages        = {103351},
  shortjournal = {Artif. Intell.},
  title        = {A technical survey on statistical modelling and design methods for crowdsourcing quality control},
  volume       = {287},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evaluation of the moral permissibility of action plans.
<em>AIJ</em>, <em>287</em>, 103350. (<a
href="https://doi.org/10.1016/j.artint.2020.103350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research in classical planning so far has been mainly concerned with generating a satisficing or an optimal plan. However, if such systems are used to make decisions that are relevant to humans, one should also consider the ethical consequences generated plans can have. Traditionally, ethical principles are formulated in an action-based manner, allowing to judge the execution of one action. We show how such a judgment can be generalized to plans. Further, we study the computational complexity of making ethical judgment about plans.},
  archive      = {J_AIJ},
  author       = {Felix Lindner and Robert Mattmüller and Bernhard Nebel},
  doi          = {10.1016/j.artint.2020.103350},
  journal      = {Artificial Intelligence},
  pages        = {103350},
  shortjournal = {Artif. Intell.},
  title        = {Evaluation of the moral permissibility of action plans},
  volume       = {287},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An approach for combining ethical principles with public
opinion to guide public policy. <em>AIJ</em>, <em>287</em>, 103349. (<a
href="https://doi.org/10.1016/j.artint.2020.103349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a framework for incorporating public opinion into policy making in situations where values are in conflict. This framework advocates creating vignettes representing value choices, eliciting the public&#39;s opinion on these choices, and using machine learning to extract principles that can serve as succinct statements of the policies implied by these choices and rules to guide the behavior of autonomous systems .},
  archive      = {J_AIJ},
  author       = {Edmond Awad and Michael Anderson and Susan Leigh Anderson and Beishui Liao},
  doi          = {10.1016/j.artint.2020.103349},
  journal      = {Artificial Intelligence},
  pages        = {103349},
  shortjournal = {Artif. Intell.},
  title        = {An approach for combining ethical principles with public opinion to guide public policy},
  volume       = {287},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Designing normative theories for ethical and legal
reasoning: LogiKEy framework, methodology, and tool support.
<em>AIJ</em>, <em>287</em>, 103348. (<a
href="https://doi.org/10.1016/j.artint.2020.103348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A framework and methodology—termed LogiKEy —for the design and engineering of ethical reasoners , normative theories and deontic logics is presented. The overall motivation is the development of suitable means for the control and governance of intelligent autonomous systems . LogiKEy &#39;s unifying formal framework is based on semantical embeddings of deontic logics, logic combinations and ethico-legal domain theories in expressive classic higher-order logic (HOL). This meta-logical approach enables the provision of powerful tool support in LogiKEy : off-the-shelf theorem provers and model finders for HOL are assisting the LogiKEy designer of ethical intelligent agents to flexibly experiment with underlying logics and their combinations, with ethico-legal domain theories, and with concrete examples—all at the same time. Continuous improvements of these off-the-shelf provers, without further ado, leverage the reasoning performance in LogiKEy . Case studies, in which the LogiKEy framework and methodology has been applied and tested, give evidence that HOL&#39;s undecidability often does not hinder efficient experimentation.},
  archive      = {J_AIJ},
  author       = {Christoph Benzmüller and Xavier Parent and Leendert van der Torre},
  doi          = {10.1016/j.artint.2020.103348},
  journal      = {Artificial Intelligence},
  pages        = {103348},
  shortjournal = {Artif. Intell.},
  title        = {Designing normative theories for ethical and legal reasoning: LogiKEy framework, methodology, and tool support},
  volume       = {287},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Memetic algorithms outperform evolutionary algorithms in
multimodal optimisation. <em>AIJ</em>, <em>287</em>, 103345. (<a
href="https://doi.org/10.1016/j.artint.2020.103345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memetic algorithms integrate local search into an evolutionary algorithm to combine the advantages of rapid exploitation and global optimisation. We provide a rigorous runtime analysis of memetic algorithms on the Hurdle problem, a landscape class of tunable difficulty with a “big valley structure”, a characteristic feature of many hard combinatorial optimisation problems . A parameter called hurdle width describes the length of fitness valleys that need to be overcome. We show that the expected runtime of plain evolutionary algorithms like the (1+1) EA increases steeply with the hurdle width, yielding superpolynomial times to find the optimum, whereas a simple memetic algorithm, (1+1) MA, only needs polynomial expected time. Surprisingly, while increasing the hurdle width makes the problem harder for evolutionary algorithms, it becomes easier for memetic algorithms. We further give the first rigorous proof that crossover can decrease the expected runtime in memetic algorithms. A (2+1) MA using mutation, crossover and local search outperforms any other combination of these operators. Our results demonstrate the power of memetic algorithms for problems with big valley structures and the benefits of hybridising multiple search operators.},
  archive      = {J_AIJ},
  author       = {Phan Trung Hai Nguyen and Dirk Sudholt},
  doi          = {10.1016/j.artint.2020.103345},
  journal      = {Artificial Intelligence},
  pages        = {103345},
  shortjournal = {Artif. Intell.},
  title        = {Memetic algorithms outperform evolutionary algorithms in multimodal optimisation},
  volume       = {287},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DEL-based epistemic planning: Decidability and complexity.
<em>AIJ</em>, <em>287</em>, 103304. (<a
href="https://doi.org/10.1016/j.artint.2020.103304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epistemic planning can be used for decision making in multi-agent systems with distributed knowledge and capabilities. Dynamic Epistemic Logic (DEL) has been shown to provide a very natural and expressive framework for epistemic planning. In this paper, we present a systematic overview of known complexity and decidability results for epistemic planning based on DEL, as well as provide some new results and improved proofs of existing results based on reductions between the problems.},
  archive      = {J_AIJ},
  author       = {Thomas Bolander and Tristan Charrier and Sophie Pinchinat and François Schwarzentruber},
  doi          = {10.1016/j.artint.2020.103304},
  journal      = {Artificial Intelligence},
  pages        = {103304},
  shortjournal = {Artif. Intell.},
  title        = {DEL-based epistemic planning: Decidability and complexity},
  volume       = {287},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Boolean algebras of conditionals, probability and logic.
<em>AIJ</em>, <em>286</em>, 103347. (<a
href="https://doi.org/10.1016/j.artint.2020.103347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents an investigation on the structure of conditional events and on the probability measures which arise naturally in that context. In particular we introduce a construction which defines a (finite) Boolean algebra of conditionals from any (finite) Boolean algebra of events. By doing so we distinguish the properties of conditional events which depend on probability and those which are intrinsic to the logico-algebraic structure of conditionals. Our main result provides a way to regard standard two-place conditional probabilities as one-place probability functions on conditional events. We also consider a logical counterpart of our Boolean algebras of conditionals with links to preferential consequence relations for non-monotonic reasoning. The overall framework of this paper provides a novel perspective on the rich interplay between logic and probability in the representation of conditional knowledge.},
  archive      = {J_AIJ},
  author       = {Tommaso Flaminio and Lluis Godo and Hykel Hosni},
  doi          = {10.1016/j.artint.2020.103347},
  journal      = {Artificial Intelligence},
  pages        = {103347},
  shortjournal = {Artif. Intell.},
  title        = {Boolean algebras of conditionals, probability and logic},
  volume       = {286},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Effective footstep planning using homotopy-class guidance.
<em>AIJ</em>, <em>286</em>, 103346. (<a
href="https://doi.org/10.1016/j.artint.2020.103346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Planning the motion for humanoid robots is a computationally-complex task due to the high dimensionality of the system. Thus, a common approach is to first plan in the low-dimensional space induced by the robot&#39;s feet—a task referred to as footstep planning . This low-dimensional plan is then used to guide the full motion of the robot. One approach that has proven successful in footstep planning is using search-based planners such as A* and its many variants. To do so, these search-based planners have to be endowed with effective heuristics to efficiently guide them through the search space. However, designing effective heuristics is a time-consuming task that requires the user to have good domain knowledge. Thus, our goal is to be able to effectively plan the footstep motions taken by a humanoid robot while obviating the burden on the user to carefully design local-minima free heuristics. To this end, we propose to use user-defined homotopy classes in the workspace that are intuitive to define. These homotopy classes are used to automatically generate heuristic functions that efficiently guide the footstep planner. Additionally, we present an extension to homotopy classes such that they are applicable to complex multi-level environments. We compare our approach for footstep planning with a standard approach that uses a heuristic common to footstep planning. In simple scenarios, the performance of both algorithms is comparable. However, in more complex scenarios our approach allows for a speedup in planning of several orders of magnitude when compared to the standard approach.},
  archive      = {J_AIJ},
  author       = {Vinitha Ranganeni and Sahit Chintalapudi and Oren Salzman and Maxim Likhachev},
  doi          = {10.1016/j.artint.2020.103346},
  journal      = {Artificial Intelligence},
  pages        = {103346},
  shortjournal = {Artif. Intell.},
  title        = {Effective footstep planning using homotopy-class guidance},
  volume       = {286},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Handling and measuring inconsistency in non-monotonic
logics. <em>AIJ</em>, <em>286</em>, 103344. (<a
href="https://doi.org/10.1016/j.artint.2020.103344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the issue of quantitatively assessing the severity of inconsistencies in non-monotonic frameworks. While measuring inconsistency in classical logics has been investigated for some time now, taking the non-monotonicity into account poses new challenges. In order to tackle them, we focus on the structure of minimal strongly K K -inconsistent subsets of a knowledge base K K —a sound generalization of minimal inconsistent subsets to arbitrary, possibly non-monotonic, frameworks which induces a generalization of Reiter&#39;s famous hitting set duality between minimal inconsistent and maximal consistent subsets of a knowledge base. We propose measures based on this notion and investigate their behavior in a non-monotonic setting by revisiting existing rationality postulates, analyzing the compliance of the proposed measures with these postulates, and by investigating their computational complexity . Motivated by the observation that a knowledge base of a non-monotonic logic can also be repaired by adding formulas – whereas Reiter&#39;s duality is only concerned about removing –, we also investigate situations where we are given potential additional assumptions to repair a knowledge base. For this, we characterize the minimal modifications to a knowledge base in terms of a hitting set duality},
  archive      = {J_AIJ},
  author       = {Markus Ulbricht and Matthias Thimm and Gerhard Brewka},
  doi          = {10.1016/j.artint.2020.103344},
  journal      = {Artificial Intelligence},
  pages        = {103344},
  shortjournal = {Artif. Intell.},
  title        = {Handling and measuring inconsistency in non-monotonic logics},
  volume       = {286},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The distortion of distributed voting. <em>AIJ</em>,
<em>286</em>, 103343. (<a
href="https://doi.org/10.1016/j.artint.2020.103343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Voting can abstractly model any decision-making scenario and as such it has been extensively studied over the decades. Recently, the related literature has focused on quantifying the impact of utilizing only limited information in the voting process on the societal welfare for the outcome, by bounding the distortion of voting rules. Even though there has been significant progress towards this goal, almost all previous works have so far neglected the fact that in many scenarios (like presidential elections) voting is actually a distributed procedure. In this paper, we consider a setting in which the voters are partitioned into disjoint districts and vote locally therein to elect local winning alternatives using a voting rule; the final outcome is then chosen from the set of these alternatives. We prove tight bounds on the distortion of well-known voting rules for such distributed elections both from a worst-case perspective as well as from a best-case one. Our results indicate that the partition of voters into districts leads to considerably higher distortion, a phenomenon which we also experimentally showcase using real-world data.},
  archive      = {J_AIJ},
  author       = {Aris Filos-Ratsikas and Evi Micha and Alexandros A. Voudouris},
  doi          = {10.1016/j.artint.2020.103343},
  journal      = {Artificial Intelligence},
  pages        = {103343},
  shortjournal = {Artif. Intell.},
  title        = {The distortion of distributed voting},
  volume       = {286},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interpretable time series kernel analytics by pre-image
estimation. <em>AIJ</em>, <em>286</em>, 103342. (<a
href="https://doi.org/10.1016/j.artint.2020.103342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel methods are known to be effective to analyse complex objects by implicitly embedding them into some feature space. To interpret and analyse the obtained results, it is often required to restore in the input space the results obtained in the feature space, by using pre-image estimation methods. This work proposes a new closed-form pre-image estimation method for time series kernel analytics that consists of two steps. In the first step, a time warp function, driven by distance constraints in the feature space, is defined to embed time series in a metric space where analytics can be performed conveniently. In the second step, the time series pre-image estimation is cast as learning a linear (or a nonlinear) transformation that ensures a local isometry between the time series embedding space and the feature space. The proposed method is compared to the state of the art through three major tasks that require pre-image estimation: 1) time series averaging, 2) time series reconstruction and denoising and 3) time series representation learning . The extensive experiments conducted on 33 publicly-available datasets show the benefits of the pre-image estimation for time series kernel analytics.},
  archive      = {J_AIJ},
  author       = {Thi Phuong Thao Tran and Ahlame Douzal-Chouakria and Saeed Varasteh Yazdi and Paul Honeine and Patrick Gallinari},
  doi          = {10.1016/j.artint.2020.103342},
  journal      = {Artificial Intelligence},
  pages        = {103342},
  shortjournal = {Artif. Intell.},
  title        = {Interpretable time series kernel analytics by pre-image estimation},
  volume       = {286},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the equivalence of optimal recommendation sets and
myopically optimal query sets. <em>AIJ</em>, <em>286</em>, 103328. (<a
href="https://doi.org/10.1016/j.artint.2020.103328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Preference elicitation is an important component in many AI applications , including decision support and recommender systems . Such systems must assess user preferences, based on interactions with their users, and make recommendations using (possibly incomplete and imprecise) beliefs about those preferences. Mechanisms for explicit preference elicitation —asking users to answer direct queries about their preferences—can be of great value; but due to the cognitive and time cost imposed on users, it is important to minimize the number of queries by asking those that have high (expected) value of information. An alternative approach is to simply make recommendations and have users provide feedback (e.g., accept a recommendation or critique it in some way) and use this more indirect feedback to gradually improve the quality of the recommendations. Due to inherent uncertainty about a user&#39;s true preferences, often a set of recommendations is presented to the user at each stage. Conceptually, a set of recommendations can also be viewed as choice query , in which the user indicates which option is most preferred from that set. Because of the potential tension between making a good set recommendation and asking an informative choice query, we explore the connection between the two. We consider two different models of preference uncertainty and optimization: (a) a Bayesian framework in which a posterior over user utility functions is maintained, optimal recommendations are assessed using expected utility, and queries are assessed using expected value of information; and (b) a minimax-regret framework in which user utility uncertainty is strict (represented by a polytope), recommendations are made using the minimax-regret robustness criterion, and queries are assessed using worst-case regret reduction. We show that, somewhat surprisingly, in both cases, there is no tradeoff to be made between good recommendations and good queries: we prove that the optimal recommendation set of size k is also an optimal choice query of size k . We also examine the case where user responses to choice queries are error prone (using both constant and mixed multinomial logit noise models) showing the results are robust to this form of noise. In both frameworks, our theoretical results have practical consequences for the design of interactive recommenders. Our results also allow us to design efficient algorithms to compute optimal query/recommendation sets. We develop several such algorithms (both exact and approximate) for both settings and provide empirical validation of their performance.},
  archive      = {J_AIJ},
  author       = {Paolo Viappiani and Craig Boutilier},
  doi          = {10.1016/j.artint.2020.103328},
  journal      = {Artificial Intelligence},
  pages        = {103328},
  shortjournal = {Artif. Intell.},
  title        = {On the equivalence of optimal recommendation sets and myopically optimal query sets},
  volume       = {286},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the limits of forgetting in answer set programming.
<em>AIJ</em>, <em>286</em>, 103307. (<a
href="https://doi.org/10.1016/j.artint.2020.103307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selectively forgetting information while preserving what matters the most is becoming an increasingly important issue in many areas, including in knowledge representation and reasoning . Depending on the application at hand, forgetting operators are defined to obey different sets of desirable properties . It turns out that, of the myriad of desirable properties discussed in the context of forgetting in Answer Set Programming , strong persistence , which imposes certain conditions on the correspondence between the answer sets of the program pre- and post-forgetting, and a certain independence from non-forgotten atoms, seems to best capture its essence, and be desirable in general. However, it has remained an open problem whether it is always possible to forget a set of atoms from a program while obeying strong persistence. In this paper, we investigate the limits of forgetting in Answer Set Programming . After showing that it is not always possible to forget a set of atoms from a program while obeying this property, we move forward and precisely characterize what can and cannot be forgotten from a program, by presenting a necessary and sufficient criterion. This characterization allows us to draw some important conclusions regarding the existence of forgetting operators for specific classes of logic programs, to characterize the class of forgetting operators that achieve the correct result whenever forgetting is possible, and investigate the related question of determining what we can forget from some specific logic program. Subsequently, we address the issue of what to do when we must forget a set of atoms, but cannot without violating this property. To this end, we investigate three natural alternatives to forget when forgetting without violating strong persistence is not possible, which turn out to correspond to the different natural possible relaxations of the characterization of strong persistence. Additionally, before concluding, we address computational complexity issues – namely of checking whether the novel criterion holds and whether a certain program is a result according to the different classes of forgetting operators we introduce – and discuss the related literature.},
  archive      = {J_AIJ},
  author       = {Ricardo Gonçalves and Matthias Knorr and João Leite and Stefan Woltran},
  doi          = {10.1016/j.artint.2020.103307},
  journal      = {Artificial Intelligence},
  pages        = {103307},
  shortjournal = {Artif. Intell.},
  title        = {On the limits of forgetting in answer set programming},
  volume       = {286},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The logic of gossiping. <em>AIJ</em>, <em>286</em>, 103306.
(<a href="https://doi.org/10.1016/j.artint.2020.103306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The so-called gossip problem is a formal model of peer-to-peer communication. In order to perform such communication efficiently, it is important to keep track of what agents know about who holds what information at a given point in time. The knowledge that the agents possess depends strongly on the particular type of communication that is used. Here, we formally define a large number of different variants of the gossip problem, that differ in the extent to which communication is private (observable, synchronous or asynchronous), the direction of the flow of information (caller to callee, callee to caller or both) and whether the agents become aware of the exact set of information possessed by their communication partner. We consider a number of formulas that represent interesting properties that a gossip situation may or may not enjoy, and show for which variants they are valid. Additionally, we show that the model checking and validity checking problems for each variant are decidable, and we introduce sound and complete proof systems for them.},
  archive      = {J_AIJ},
  author       = {Hans van Ditmarsch and Wiebe van der Hoek and Louwe B. Kuijer},
  doi          = {10.1016/j.artint.2020.103306},
  journal      = {Artificial Intelligence},
  pages        = {103306},
  shortjournal = {Artif. Intell.},
  title        = {The logic of gossiping},
  volume       = {286},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic term-modal logics for first-order epistemic
planning. <em>AIJ</em>, <em>286</em>, 103305. (<a
href="https://doi.org/10.1016/j.artint.2020.103305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many classical planning frameworks are built on first-order languages. The first-order expressive power is desirable for compactly representing actions via schemas, and for specifying quantified conditions such as ¬ ∃ x blocks _ door ( x ) ¬∃xblocks_door(x) . In contrast, several recent epistemic planning frameworks are built on propositional epistemic logic. The epistemic language is useful to describe planning problems involving higher-order reasoning or epistemic goals such as K a ¬ problem Ka¬problem . This paper develops a first-order version of Dynamic Epistemic Logic (DEL). In this framework, for example, ∃ x K x ∃ y blocks _ door ( y ) ∃xKx∃yblocks_door(y) is a formula. The formalism combines the strengths of DEL (higher-order reasoning) with those of first-order logic (lifted representation) to model multi-agent epistemic planning. The paper introduces an epistemic language with a possible-worlds semantics, followed by novel dynamics given by first-order action models and their execution via product updates. Taking advantage of the first-order machinery, epistemic action schemas are defined to provide compact, problem-independent domain descriptions, in the spirit of PDDL. Concerning metatheory, the paper defines axiomatic normal term-modal logics, shows a Canonical Model Theorem-like result which allows establishing completeness through frame characterization formulas, shows decidability for the finite agent case, and shows a general completeness result for the dynamic extension by reduction axioms.},
  archive      = {J_AIJ},
  author       = {Andrés Occhipinti Liberman and Andreas Achen and Rasmus Kræmmer Rendsvig},
  doi          = {10.1016/j.artint.2020.103305},
  journal      = {Artificial Intelligence},
  pages        = {103305},
  shortjournal = {Artif. Intell.},
  title        = {Dynamic term-modal logics for first-order epistemic planning},
  volume       = {286},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PopMNet: Generating structured pop music melodies using
neural networks. <em>AIJ</em>, <em>286</em>, 103303. (<a
href="https://doi.org/10.1016/j.artint.2020.103303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, many deep learning models have been proposed to generate symbolic melodies. However, generating pop music melodies with well organized structures remains to be challenging. In this paper, we present a melody structure-based model called PopMNet to generate structured pop music melodies. The melody structure is defined by pairwise relations, specifically, repetition and sequence , between all bars in a melody. PopMNet consists of a Convolutional Neural Network (CNN)-based Structure Generation Net (SGN) and a Recurrent Neural Network (RNN)-based Melody Generation Net (MGN). The former generates melody structures and the latter generates melodies conditioned on the structures and chord progressions. The proposed model is compared with four existing models AttentionRNN, LookbackRNN, MidiNet and Music Transformer. The results indicate that the melodies generated by our model contain much clearer structures compared to those generated by other models, as confirmed by human behavior experiments.},
  archive      = {J_AIJ},
  author       = {Jian Wu and Xiaoguang Liu and Xiaolin Hu and Jun Zhu},
  doi          = {10.1016/j.artint.2020.103303},
  journal      = {Artificial Intelligence},
  pages        = {103303},
  shortjournal = {Artif. Intell.},
  title        = {PopMNet: Generating structured pop music melodies using neural networks},
  volume       = {286},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Verification of multi-agent systems with public actions
against strategy logic. <em>AIJ</em>, <em>285</em>, 103302. (<a
href="https://doi.org/10.1016/j.artint.2020.103302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model checking multi-agent systems, in which agents are distributed and thus may have different observations of the world, against strategic behaviours is known to be a complex problem in a number of settings. There are traditionally two ways of ameliorating this complexity: imposing a hierarchy on the observations of the agents, or restricting agent actions so that they are observable by all agents. We study systems of the latter kind, since they are more suitable for modelling rational agents. In particular, we define multi-agent systems in which all actions are public and study the model checking problem of such systems against Strategy Logic with equality, a very rich strategic logic that can express relevant concepts such as Nash equilibria , Pareto optimality , and due to the novel addition of equality, also evolutionary stable strategies. The main result is that the corresponding model checking problem is decidable.},
  archive      = {J_AIJ},
  author       = {Francesco Belardinelli and Alessio Lomuscio and Aniello Murano and Sasha Rubin},
  doi          = {10.1016/j.artint.2020.103302},
  journal      = {Artificial Intelligence},
  pages        = {103302},
  shortjournal = {Artif. Intell.},
  title        = {Verification of multi-agent systems with public actions against strategy logic},
  volume       = {285},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Special issue on autonomous agents modelling other agents:
Guest editorial. <em>AIJ</em>, <em>285</em>, 103292. (<a
href="https://doi.org/10.1016/j.artint.2020.103292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Much research in artificial intelligence is concerned with enabling autonomous agents to reason about various aspects of other agents (such as their beliefs, goals, plans, or decisions) and to utilise such reasoning for effective interaction. This special issue contains new technical contributions addressing open problems in autonomous agents modelling other agents, as well as research perspectives about current developments, challenges, and future directions.},
  archive      = {J_AIJ},
  author       = {Stefano V. Albrecht and Peter Stone and Michael P. Wellman},
  doi          = {10.1016/j.artint.2020.103292},
  journal      = {Artificial Intelligence},
  pages        = {103292},
  shortjournal = {Artif. Intell.},
  title        = {Special issue on autonomous agents modelling other agents: Guest editorial},
  volume       = {285},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Compact and efficient encodings for planning in factored
state and action spaces with learned binarized neural network transition
models. <em>AIJ</em>, <em>285</em>, 103291. (<a
href="https://doi.org/10.1016/j.artint.2020.103291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we leverage the efficiency of Binarized Neural Networks (BNNs) to learn complex state transition models of planning domains with discretized factored state and action spaces. In order to directly exploit this transition structure for planning, we present two novel compilations of the learned factored planning problem with BNNs based on reductions to Weighted Partial Maximum Boolean Satisfiability (FD-SAT-Plan+) as well as Binary Linear Programming (FD-BLP-Plan+). Theoretically, we show that our SAT-based Bi-Directional Neuron Activation Encoding is asymptotically the most compact encoding relative to the current literature and supports Unit Propagation (UP) – an important property that facilitates efficiency in SAT solvers. Experimentally, we validate the computational efficiency of our Bi-Directional Neuron Activation Encoding in comparison to an existing neuron activation encoding and demonstrate the ability to learn complex transition models with BNNs. We test the runtime efficiency of both FD-SAT-Plan+ and FD-BLP-Plan+ on the learned factored planning problem showing that FD-SAT-Plan+ scales better with increasing BNN size and complexity. Finally, we present a finite-time incremental constraint generation algorithm based on generalized landmark constraints to improve the planning accuracy of our encodings through simulated or real-world interaction.},
  archive      = {J_AIJ},
  author       = {Buser Say and Scott Sanner},
  doi          = {10.1016/j.artint.2020.103291},
  journal      = {Artificial Intelligence},
  pages        = {103291},
  shortjournal = {Artif. Intell.},
  title        = {Compact and efficient encodings for planning in factored state and action spaces with learned binarized neural network transition models},
  volume       = {285},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Complexity of abstract argumentation under a claim-centric
view. <em>AIJ</em>, <em>285</em>, 103290. (<a
href="https://doi.org/10.1016/j.artint.2020.103290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract argumentation frameworks have been introduced by Dung as part of an argumentation process, where arguments and conflicts are derived from a given knowledge base. It is solely this relation between arguments that is then used in order to identify acceptable sets of arguments. A final step concerns the acceptance status of particular statements by reviewing the actual contents of the acceptable arguments. Complexity analysis of abstract argumentation so far has neglected this final step and is concerned with argument names instead of their contents, i.e. their claims. As we outline in this paper, this is not only a slight deviation but can lead to different complexity results. We, therefore, give a comprehensive complexity analysis of abstract argumentation under a claim-centric view and analyse the four main decision problems under seven popular semantics. In addition, we also address the complexity of common sub-classes and introduce novel parameterisations – which exploit the nature of claims explicitly – along with fixed-parameter tractability results.},
  archive      = {J_AIJ},
  author       = {Wolfgang Dvořák and Stefan Woltran},
  doi          = {10.1016/j.artint.2020.103290},
  journal      = {Artificial Intelligence},
  pages        = {103290},
  shortjournal = {Artif. Intell.},
  title        = {Complexity of abstract argumentation under a claim-centric view},
  volume       = {285},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On strengthening the logic of iterated belief revision:
Proper ordinal interval operators. <em>AIJ</em>, <em>285</em>, 103289.
(<a href="https://doi.org/10.1016/j.artint.2020.103289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Darwiche and Pearl&#39;s seminal 1997 article outlined a number of baseline principles for a logic of iterated belief revision. These principles, the DP postulates, have been supplemented in a number of alternative ways. However, most of the suggestions for doing so have been radical enough to result in a dubious ‘reductionist’ principle that identifies belief states with orderings of worlds. The present paper offers a more modest strengthening of Darwiche and Pearl&#39;s proposal. While the DP postulates constrain the relation between a prior and a posterior conditional belief set, our new principles govern the relation between two posterior conditional belief sets obtained from a common prior by different revisions. We show that operators from the family that these principles characterise, which subsumes both lexicographic and restrained revision, can be represented as relating belief states that are associated with a ‘proper ordinal interval’ assignment, a structure more fine-grained than a simple ordering of worlds. We close the paper by noting that these operators satisfy iterated versions of a large number of AGM era postulates.},
  archive      = {J_AIJ},
  author       = {Richard Booth and Jake Chandler},
  doi          = {10.1016/j.artint.2020.103289},
  journal      = {Artificial Intelligence},
  pages        = {103289},
  shortjournal = {Artif. Intell.},
  title        = {On strengthening the logic of iterated belief revision: Proper ordinal interval operators},
  volume       = {285},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the complexity of reasoning about opinion diffusion under
majority dynamics. <em>AIJ</em>, <em>284</em>, 103288. (<a
href="https://doi.org/10.1016/j.artint.2020.103288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study opinion diffusion on social graphs where agents hold binary opinions and where social pressure leads them to conform to the opinion manifested by the majority of their neighbors. We provide bounds relating the number of agents that suffice to spread an opinion to all other agents with the number of required propagation steps . Bounds are established constructively, via polynomial time algorithms that identify the agents that must act as seeds . In particular, we show that, on any given social graph G = ( N , E ) G=(N, E) , it is possible to efficiently identify a set formed by half of the agents that can lead to consensus in min ⁡ { ⌊ | N | / 2 ⌋ , even ( G ) + 1 } min⁡{⌊|N|/2⌋, even(G)+1} propagation steps , where even ( G ) even(G) is the number of agents with an even number of neighbors in G . The result marks the boundary of tractability, since we show that the existence of sets of seeds consisting of less than half of the agents depends on certain features of the underlying graphs, which are NP-hard to identify. In fact, other NP-hardness results emerge from our analysis. In particular, by closing a problem left open in the literature, we show that it is intractable to decide whether further stable configurations exist in addition to the “consensus” ones (where all agents hold the same opinion). Eventually, for all these problems related to reasoning about opinion diffusion, we show that islands of tractability can be identified by focusing on classes of tree-like social graphs.},
  archive      = {J_AIJ},
  author       = {Vincenzo Auletta and Diodato Ferraioli and Gianluigi Greco},
  doi          = {10.1016/j.artint.2020.103288},
  journal      = {Artificial Intelligence},
  pages        = {103288},
  shortjournal = {Artif. Intell.},
  title        = {On the complexity of reasoning about opinion diffusion under majority dynamics},
  volume       = {284},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Knowing the price of success. <em>AIJ</em>, <em>284</em>,
103287. (<a href="https://doi.org/10.1016/j.artint.2020.103287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {If an agent, or a coalition of agents, knows that it has a strategy to achieve a certain outcome, it does not mean that the agent knows what the strategy is. Even if the agent knows what the strategy is, she might not know the price of executing this strategy. The article considers modality “the coalition not only knows the strategy, but also knows an upper bound on the price of executing the strategy”. The main technical result is a sound and complete bimodal logical system that describes the interplay between this modality and the distributed knowledge modality.},
  archive      = {J_AIJ},
  author       = {Rui Cao and Pavel Naumov},
  doi          = {10.1016/j.artint.2020.103287},
  journal      = {Artificial Intelligence},
  pages        = {103287},
  shortjournal = {Artif. Intell.},
  title        = {Knowing the price of success},
  volume       = {284},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The book of why: The new science of cause and effect, judea
pearl, dana mackenzie, basic books (2018). <em>AIJ</em>, <em>284</em>,
103286. (<a href="https://doi.org/10.1016/j.artint.2020.103286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AIJ},
  author       = {Norman Fenton and Martin Neil and Anthony Constantinou},
  doi          = {10.1016/j.artint.2020.103286},
  journal      = {Artificial Intelligence},
  pages        = {103286},
  shortjournal = {Artif. Intell.},
  title        = {The book of why: The new science of cause and effect, judea pearl, dana mackenzie, basic books (2018)},
  volume       = {284},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On quasi-inconsistency and its complexity. <em>AIJ</em>,
<em>284</em>, 103276. (<a
href="https://doi.org/10.1016/j.artint.2020.103276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the issue of analyzing potential inconsistencies in knowledge bases. This refers to knowledge bases that contain rules which will always be activated together, and the knowledge base will become inconsistent, should these rules be activated. We investigate this problem in the context of the industrial use-case of business rule management, where it is often required that sets of (only) rules are analyzed for potential inconsistencies, e.g., during business rule modelling. To this aim, we introduce the notion of quasi-inconsistency, which is a formalization of the above-mentioned problem of potential inconsistencies. We put a specific focus on the analysis of computational complexity of some involved problems and show that many of them are intractable.},
  archive      = {J_AIJ},
  author       = {Carl Corea and Matthias Thimm},
  doi          = {10.1016/j.artint.2020.103276},
  journal      = {Artificial Intelligence},
  pages        = {103276},
  shortjournal = {Artif. Intell.},
  title        = {On quasi-inconsistency and its complexity},
  volume       = {284},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Combining gaze and AI planning for online human intention
recognition. <em>AIJ</em>, <em>284</em>, 103275. (<a
href="https://doi.org/10.1016/j.artint.2020.103275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intention recognition is the process of using behavioural cues, such as deliberative actions, eye gaze, and gestures, to infer an agent&#39;s goals or future behaviour. In artificial intelligence , one approach for intention recognition is to use a model of possible behaviour to rate intentions as more likely if they are a better ‘fit’ to actions observed so far. In this paper, we draw from literature linking gaze and visual attention, and we propose a novel model of online human intention recognition that combines gaze and model-based AI planning to build probability distributions over a set of possible intentions. In human-behavioural experiments ( n = 40 n=40 ) involving a multi-player board game, we demonstrate that adding gaze-based priors to model-based intention recognition improved the accuracy of intention recognition by 22\% ( p p&amp;lt; 0.05 ), determined those intentions ≈90 seconds earlier ( p p&amp;lt; 0.05 ), and at no additional computational cost. We also demonstrate that, when evaluated in the presence of semi-rational or deceptive gaze behaviours, the proposed model is significantly more accurate (9\% improvement) ( p p&amp;lt; 0.05 ) compared to a model-based or gaze only approaches. Our results indicate that the proposed model could be used to design novel human-agent interactions in cases when we are unsure whether a person is honest, deceitful, or semi-rational.},
  archive      = {J_AIJ},
  author       = {Ronal Singh and Tim Miller and Joshua Newn and Eduardo Velloso and Frank Vetere and Liz Sonenberg},
  doi          = {10.1016/j.artint.2020.103275},
  journal      = {Artificial Intelligence},
  pages        = {103275},
  shortjournal = {Artif. Intell.},
  title        = {Combining gaze and AI planning for online human intention recognition},
  volume       = {284},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CPCES: A planning framework to solve conformant planning
problems through a counterexample guided refinement. <em>AIJ</em>,
<em>284</em>, 103271. (<a
href="https://doi.org/10.1016/j.artint.2020.103271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce cpces , a novel planner for the problem of deterministic conformant planning. cpces solves the problem by producing candidate plans based on a sample of the initial belief state, searching for counter-examples to these plans, and assigning these counter-examples to the sample, until a valid plan has been produced or the problem has been proved unfeasible. On top of providing a means to compute a conformant plan, the sample can also be understood as a justification for the plan being found, or relevant reasons why a plan cannot be found. We study the theoretical properties that cpces enjoys—correctness, completeness, and optimality—and how the several variants of cpces we describe differ in behaviour. Moreover, we establish a theoretical connection between the cpces framework and well-known concepts from the literature such as tags and width. With this connection we prove the worst case complexity for some variants of cpces . Finally, we show how cpces can be used in a more incremental fashion by learning sequencing of actions from the previous plan being found. Such a technique mimics the use of macro-operators, widely used in automated planning to speedup resolution. Our theoretical analysis is accompanied with a thorough experimental evaluation of the (many) possible incarnations of cpces . This not only proves our theoretical findings from an empirical perspective, but also shows that cpces is able to handle problems that have been traditionally hard to solve by the existing conformant planners, whilst remaining competitive over “easier” conformant planning problems. Importantly, cpces is able to prove many unsolvable conformant planning problems as such, extending substantially the reach of conformant planners.},
  archive      = {J_AIJ},
  author       = {Alban Grastien and Enrico Scala},
  doi          = {10.1016/j.artint.2020.103271},
  journal      = {Artificial Intelligence},
  pages        = {103271},
  shortjournal = {Artif. Intell.},
  title        = {CPCES: A planning framework to solve conformant planning problems through a counterexample guided refinement},
  volume       = {284},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Compatibility, desirability, and the running intersection
property. <em>AIJ</em>, <em>283</em>, 103274. (<a
href="https://doi.org/10.1016/j.artint.2020.103274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compatibility is the problem of checking whether some given probabilistic assessments have a common joint probabilistic model. When the assessments are unconditional, the problem is well established in the literature and finds a solution through the running intersection property ( RIP ). This is not the case of conditional assessments. In this paper, we study the compatibility problem in a very general setting: any possibility space, unrestricted domains, imprecise (and possibly degenerate) probabilities. We extend the unconditional case to our setting, thus generalising most of previous results in the literature. The conditional case turns out to be fundamentally different from the unconditional one. For such a case, we prove that the problem can still be solved in general by RIP but in a more involved way: by constructing a junction tree and propagating information over it. Still, RIP does not allow us to optimally take advantage of sparsity : in fact, conditional compatibility can be simplified further by joining junction trees with coherence graphs .},
  archive      = {J_AIJ},
  author       = {Enrique Miranda and Marco Zaffalon},
  doi          = {10.1016/j.artint.2020.103274},
  journal      = {Artificial Intelligence},
  pages        = {103274},
  shortjournal = {Artif. Intell.},
  title        = {Compatibility, desirability, and the running intersection property},
  volume       = {283},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Intention as commitment toward time. <em>AIJ</em>,
<em>283</em>, 103270. (<a
href="https://doi.org/10.1016/j.artint.2020.103270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we address the interplay among intention, time, and belief in dynamic environments. The first contribution is a logic for reasoning about intention, time and belief, in which assumptions of intentions are represented by preconditions of intended actions. Intentions and beliefs are coherent as long as these assumptions are not violated, i.e. as long as intended actions can be performed such that their preconditions hold as well. The second contribution is the formalization of what-if scenarios: what happens with intentions and beliefs if a new (possibly conflicting) intention is adopted, or a new fact is learned? An agent is committed to its intended actions as long as its belief-intention database is coherent. We conceptualize intention as commitment toward time and we develop AGM-based postulates for the iterated revision of belief-intention databases, and we prove a Katsuno-Mendelzon-style representation theorem.},
  archive      = {J_AIJ},
  author       = {Marc van Zee and Dragan Doder and Leendert van der Torre and Mehdi Dastani and Thomas Icard and Eric Pacuit},
  doi          = {10.1016/j.artint.2020.103270},
  journal      = {Artificial Intelligence},
  pages        = {103270},
  shortjournal = {Artif. Intell.},
  title        = {Intention as commitment toward time},
  volume       = {283},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An epistemic logic of blameworthiness. <em>AIJ</em>,
<em>283</em>, 103269. (<a
href="https://doi.org/10.1016/j.artint.2020.103269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blameworthiness of an agent or a coalition of agents can be defined in terms of the principle of alternative possibilities: for the coalition to be responsible for an outcome, the outcome must take place and the coalition should be a minimal one that had a strategy to prevent the outcome. In this article we argue that in the settings with imperfect information , not only should the coalition have had a strategy, but it also should be the minimal one that knew that it had a strategy and what the strategy was. The main technical result of the article is a sound and complete bimodal logic that describes the interplay between knowledge and blameworthiness in strategic games with imperfect information .},
  archive      = {J_AIJ},
  author       = {Pavel Naumov and Jia Tao},
  doi          = {10.1016/j.artint.2020.103269},
  journal      = {Artificial Intelligence},
  pages        = {103269},
  shortjournal = {Artif. Intell.},
  title        = {An epistemic logic of blameworthiness},
  volume       = {283},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On pruning search trees of impartial games. <em>AIJ</em>,
<em>283</em>, 103262. (<a
href="https://doi.org/10.1016/j.artint.2020.103262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we study computing Sprague-Grundy values for short impartial games under the normal play convention. We put forward new game-agnostic methods for effective pruning search trees of impartial games. These algorithms are inspired by the α - β , a well-known pruning method for minimax trees. However, our algorithms prune trees whose node values are assigned by the mex function instead of min/max. We have verified the effectiveness of our algorithms experimentally on instances of some standard impartial games (that is Nim, Chomp, and Cram). Based on the results of our experiments we have concluded that: (1) our methods generally perform well, especially when transposition table can store only a small fraction of all game positions (which is typical when larger games are concerned); (2) one of our algorithms constitutes a more universal alternative to the state-of-the-art algorithm proposed by Lemoine and Viennot.},
  archive      = {J_AIJ},
  author       = {Piotr Beling and Marek Rogalski},
  doi          = {10.1016/j.artint.2020.103262},
  journal      = {Artificial Intelligence},
  pages        = {103262},
  shortjournal = {Artif. Intell.},
  title        = {On pruning search trees of impartial games},
  volume       = {283},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adapting a kidney exchange algorithm to align with human
values. <em>AIJ</em>, <em>283</em>, 103261. (<a
href="https://doi.org/10.1016/j.artint.2020.103261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The efficient and fair allocation of limited resources is a classical problem in economics and computer science. In kidney exchanges, a central market maker allocates living kidney donors to patients in need of an organ. Patients and donors in kidney exchanges are prioritized using ad-hoc weights decided on by committee and then fed into an allocation algorithm that determines who gets what—and who does not. In this paper, we provide an end-to-end methodology for estimating weights of individual participant profiles in a kidney exchange. We first elicit from human subjects a list of patient attributes they consider acceptable for the purpose of prioritizing patients (e.g., medical characteristics, lifestyle choices, and so on). Then, we ask subjects comparison queries between patient profiles and estimate weights in a principled way from their responses. We show how to use these weights in kidney exchange market clearing algorithms. We then evaluate the impact of the weights in simulations and find that the precise numerical values of the weights we computed matter little, other than the ordering of profiles that they imply. However, compared to not prioritizing patients at all, there is a significant effect, with certain classes of patients being (de)prioritized based on the human-elicited value judgments.},
  archive      = {J_AIJ},
  author       = {Rachel Freedman and Jana Schaich Borg and Walter Sinnott-Armstrong and John P. Dickerson and Vincent Conitzer},
  doi          = {10.1016/j.artint.2020.103261},
  journal      = {Artificial Intelligence},
  pages        = {103261},
  shortjournal = {Artif. Intell.},
  title        = {Adapting a kidney exchange algorithm to align with human values},
  volume       = {283},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Batch repair actions for automated troubleshooting.
<em>AIJ</em>, <em>283</em>, 103260. (<a
href="https://doi.org/10.1016/j.artint.2020.103260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Repairing a set of components as a batch is often cheaper than repairing each of them separately. A primary reason for this is that initiating a repair action and testing the system after performing a repair action often incurs non-negligible overhead. However, most troubleshooting algorithms proposed to date do not consider the option of performing batch repair actions . In this work we close this gap, and address the combinatorial problem of choosing which batch repair action to perform so as to minimize the overall repair costs. We call this problem the Batch Repair Problem (BRP) and formalize it. Then, we propose several approaches for solving it. The first seeks to choose to repair the set of components that are most likely to be faulty. The second estimates the cost wasted by repairing a given set of components, and tried to find the set of components that minimizes these costs. The third approach models BRP as a Stochastic Shortest Path Problem (SSP-MDP) [1] , and solves the resulting problem with a dedicated solver. Experimentally, we compare the pros and cons of the proposed BRP algorithms on a standard Boolean circuit benchmark and a novel benchmark from the Physiotherapy domain. Results show the clear benefit of performing batch repair actions with our BRP algorithms compared to repairing components one at a time.},
  archive      = {J_AIJ},
  author       = {Hilla Shinitzky and Roni Stern},
  doi          = {10.1016/j.artint.2020.103260},
  journal      = {Artificial Intelligence},
  pages        = {103260},
  shortjournal = {Artif. Intell.},
  title        = {Batch repair actions for automated troubleshooting},
  volume       = {283},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Qualitative case-based reasoning and learning. <em>AIJ</em>,
<em>283</em>, 103258. (<a
href="https://doi.org/10.1016/j.artint.2020.103258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of autonomous agents that perform tasks with the same dexterity as performed by humans is one of the challenges of artificial intelligence and robotics. This motivates the research on intelligent agents, since the agent must choose the best action in a dynamic environment in order to maximise the final score. In this context, the present paper introduces a novel algorithm for Qualitative Case-Based Reasoning and Learning (QCBRL), which is a case-based reasoning system that uses qualitative spatial representations to retrieve and reuse cases by means of relations between objects in the environment. Combined with reinforcement learning , QCBRL allows the agent to learn new qualitative cases at runtime, without assuming a pre-processing step. In order to avoid cases that do not lead to the maximum performance, QCBRL executes case-base maintenance, excluding these cases and obtaining new (more suitable) ones. Experimental evaluation of QCBRL was conducted in a simulated robot-soccer environment, in a real humanoid-robot environment and on simple tasks in two distinct gridworld domains. Results show that QCBRL outperforms traditional RL methods. As a result of running QCBRL in autonomous soccer matches, the robots performed a higher average number of goals than those obtained when using pure numerical models. In the gridworlds considered, the agent was able to learn optimal and safety policies.},
  archive      = {J_AIJ},
  author       = {Thiago Pedro Donadon Homem and Paulo Eduardo Santos and Anna Helena Reali Costa and Reinaldo Augusto da Costa Bianchi and Ramon Lopez de Mantaras},
  doi          = {10.1016/j.artint.2020.103258},
  journal      = {Artificial Intelligence},
  pages        = {103258},
  shortjournal = {Artif. Intell.},
  title        = {Qualitative case-based reasoning and learning},
  volume       = {283},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). How do fairness definitions fare? Testing public attitudes
towards three algorithmic definitions of fairness in loan allocations.
<em>AIJ</em>, <em>283</em>, 103238. (<a
href="https://doi.org/10.1016/j.artint.2020.103238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {What is the best way to define algorithmic fairness? While many definitions of fairness have been proposed in the computer science literature, there is no clear agreement over a particular definition. In this work, we investigate ordinary people&#39;s perceptions of three of these fairness definitions. Across three online experiments, we test which definitions people perceive to be the fairest in the context of loan decisions, and whether fairness perceptions change with the addition of sensitive information (i.e., race or gender of the loan applicants). Overall, one definition (calibrated fairness) tends to be more preferred than the others, and the results also provide support for the principle of affirmative action.},
  archive      = {J_AIJ},
  author       = {Nripsuta Ani Saxena and Karen Huang and Evan DeFilippis and Goran Radanovic and David C. Parkes and Yang Liu},
  doi          = {10.1016/j.artint.2020.103238},
  journal      = {Artificial Intelligence},
  pages        = {103238},
  shortjournal = {Artif. Intell.},
  title        = {How do fairness definitions fare? testing public attitudes towards three algorithmic definitions of fairness in loan allocations},
  volume       = {283},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Limited lookahead in imperfect-information games.
<em>AIJ</em>, <em>283</em>, 103218. (<a
href="https://doi.org/10.1016/j.artint.2019.103218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Limited lookahead has been studied for decades in perfect-information games. We initiate a new direction via two simultaneous deviation points: generalization to imperfect-information games and a game-theoretic approach. We study how one should act when facing an opponent whose lookahead is limited. We study this for opponents that differ based on their lookahead depth, based on whether they, too, have imperfect information , and based on how they break ties. We characterize the hardness of finding a Nash equilibrium or an optimal commitment strategy for either player, showing that in some of these variations the problem can be solved in polynomial time while in others it is PPAD-hard, NP-hard, or inapproximable. We proceed to design algorithms for computing optimal commitment strategies—for when the opponent breaks ties favorably, according to a fixed rule, or adversarially. We then experimentally investigate the impact of limited lookahead. The limited-lookahead player often obtains the value of the game if she knows the expected values of nodes in the game tree for some equilibrium—but we prove this is not sufficient in general. Finally, we study the impact of noise in those estimates and different lookahead depths.},
  archive      = {J_AIJ},
  author       = {Christian Kroer and Tuomas Sandholm},
  doi          = {10.1016/j.artint.2019.103218},
  journal      = {Artificial Intelligence},
  pages        = {103218},
  shortjournal = {Artif. Intell.},
  title        = {Limited lookahead in imperfect-information games},
  volume       = {283},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fair navigation planning: A resource for characterizing and
designing fairness in mobile robots. <em>AIJ</em>, <em>282</em>, 103259.
(<a href="https://doi.org/10.1016/j.artint.2020.103259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the development and deployment of autonomous systems such as mobile robots have been increasingly common. Investigating and implementing ethical considerations such as fairness in autonomous systems is an important problem that is receiving increased attention, both because of recent findings of their potential undesired impacts and a related surge in ethical principles and guidelines. In this paper we take a new approach to considering fairness in the design of autonomous systems: we examine fairness by obtaining formal definitions, applying them to a system, and simulating system deployment in order to anticipate challenges. We undertake this analysis in the context of the particular technical problem of robot navigation . We start by showing that there is a fairness dimension to robot navigation , and we then collect and translate several formal definitions of distributive justice into the navigation planning domain. We use a walkthrough example of a rescue robot to bring out design choices and issues that arise during the development of a fair system. We discuss indirect discrimination, fairness-efficiency trade-offs, the existence of counter-productive fairness definitions, privacy and other issues. Finally, we elaborate on important aspects of a research agenda and reflect on the adequacy of our methodology in this paper as a general approach to responsible innovation in autonomous systems.},
  archive      = {J_AIJ},
  author       = {Martim Brandão and Marina Jirotka and Helena Webb and Paul Luff},
  doi          = {10.1016/j.artint.2020.103259},
  journal      = {Artificial Intelligence},
  pages        = {103259},
  shortjournal = {Artif. Intell.},
  title        = {Fair navigation planning: A resource for characterizing and designing fairness in mobile robots},
  volume       = {282},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Autoepistemic equilibrium logic and epistemic
specifications. <em>AIJ</em>, <em>282</em>, 103249. (<a
href="https://doi.org/10.1016/j.artint.2020.103249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epistemic specifications extend disjunctive answer-set programs by an epistemic modal operator that may occur in the body of rules. Their semantics is in terms of world views, which are sets of answer sets, and the idea is that the epistemic modal operator quantifies over these answer sets. Several such semantics were proposed in the literature. We here propose a new semantics that is based on the logic of here-and-there: we add epistemic modal operators to its language and define epistemic here-and-there models. We then successively define epistemic equilibrium models and autoepistemic equilibrium models. The former are obtained from epistemic here-and-there models in exactly the same way as Pearce&#39;s equilibrium models are obtained from here-and-there models, viz. by minimising truth; they provide an epistemic extension of equilibrium logic. The latter are obtained from the former by maximising the set of epistemic possibilities, and they provide a new semantics for Gelfond&#39;s epistemic specifications. For both semantics we establish a strong equivalence result: we characterise strong equivalence of two epistemic programs by means of logical equivalence in epistemic here-and-there logic. We finally compare our approach to the existing semantics of epistemic specifications and discuss which formalisms provide more intuitive results by pointing out some formal properties a semantics proposal should satisfy.},
  archive      = {J_AIJ},
  author       = {Ezgi Iraz Su and Luis Fariñas del Cerro and Andreas Herzig},
  doi          = {10.1016/j.artint.2020.103249},
  journal      = {Artificial Intelligence},
  pages        = {103249},
  shortjournal = {Artif. Intell.},
  title        = {Autoepistemic equilibrium logic and epistemic specifications},
  volume       = {282},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automated construction of bounded-loss imperfect-recall
abstractions in extensive-form games. <em>AIJ</em>, <em>282</em>,
103248. (<a href="https://doi.org/10.1016/j.artint.2020.103248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extensive-form games (EFGs) model finite sequential interactions between players. The amount of memory required to represent these games is the main bottleneck of algorithms for computing optimal strategies and the size of these strategies is often impractical for real-world applications. A common approach to tackle the memory bottleneck is to use information abstraction that removes parts of information available to players thus reducing the number of decision points in the game. However, existing information-abstraction techniques are either specific for a particular domain, they do not provide any quality guarantees, or they are applicable to very small subclasses of EFGs. We present domain-independent abstraction methods for creating imperfect recall abstractions in extensive-form games that allow computing strategies that are (near) optimal in the original game. To this end, we introduce two novel algorithms, FPIRA and CFR+IRA, based on fictitious play and counterfactual regret minimization. These algorithms can start with an arbitrary domain specific, or the coarsest possible, abstraction of the original game. The algorithms iteratively detect the missing information they require for computing a strategy for the abstract game that is (near) optimal in the original game. This information is then included back into the abstract game. Moreover, our algorithms are able to exploit imperfect-recall abstractions that allow players to forget even history of their own actions. However, the algorithms require traversing the complete unabstracted game tree. We experimentally show that our algorithms can closely approximate Nash equilibrium of large games using abstraction with as little as 0.9\% of information sets of the original game. Moreover, the results suggest that memory savings increase with the increasing size of the original games.},
  archive      = {J_AIJ},
  author       = {Jiří Čermák and Viliam Lisý and Branislav Bošanský},
  doi          = {10.1016/j.artint.2020.103248},
  journal      = {Artificial Intelligence},
  pages        = {103248},
  shortjournal = {Artif. Intell.},
  title        = {Automated construction of bounded-loss imperfect-recall abstractions in extensive-form games},
  volume       = {282},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust learning with imperfect privileged information.
<em>AIJ</em>, <em>282</em>, 103246. (<a
href="https://doi.org/10.1016/j.artint.2020.103246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the learning using privileged information (LUPI) paradigm, example data cannot always be clean, while the gathered privileged information can be imperfect in practice. Here, imperfect privileged information can refer to auxiliary information that is not always accurate or perturbed by noise, or alternatively to incomplete privileged information, where privileged information is only available for part of the training data. Because of the lack of clear strategies for handling noise in example data and imperfect privileged information, existing learning using privileged information (LUPI) methods may encounter serious issues. Accordingly, in this paper, we propose a Robust SVM+ method to tackle imperfect data in LUPI. In order to make the SVM+ model robust to noise in example data and privileged information, Robust SVM+ maximizes the lower bound of the perturbations that may influence the judgement based on a rigorous theoretical analysis. Moreover, in order to deal with the incomplete privileged information, we use the available privileged information to help us in approximating the missing privileged information of training data. The optimization problem of the proposed method can be efficiently solved by employing a two-step alternating optimization strategy , based on iteratively deploying off-the-shelf quadratic programming solvers and the alternating direction method of multipliers (ADMM) technique. Comprehensive experiments on real-world datasets demonstrate the effectiveness of the proposed Robust SVM+ method in handling imperfect privileged information.},
  archive      = {J_AIJ},
  author       = {Xue Li and Bo Du and Chang Xu and Yipeng Zhang and Lefei Zhang and Dacheng Tao},
  doi          = {10.1016/j.artint.2020.103246},
  journal      = {Artificial Intelligence},
  pages        = {103246},
  shortjournal = {Artif. Intell.},
  title        = {Robust learning with imperfect privileged information},
  volume       = {282},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rethinking epistemic logic with belief bases. <em>AIJ</em>,
<em>282</em>, 103233. (<a
href="https://doi.org/10.1016/j.artint.2020.103233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new semantics for a family of logics of explicit and implicit belief based on the concept of multi-agent belief base. Differently from standard semantics for epistemic logic in which the notions of possible world and doxastic/epistemic alternative are primitive, in our semantics they are non-primitive but are computed from the concept of belief base. We provide complete axiomatizations and prove decidability for our logics via finite model arguments. Furthermore, we provide polynomial embeddings of our logics into Fagin &amp; Halpern&#39;s logic of general awareness and establish complexity results via the embeddings. We also present variants of the logics incorporating different forms of epistemic introspection for explicit and/or implicit belief and provide complexity results for some of these variants. Finally, we present a number of dynamic extensions of the static framework by informative actions of both public and private type, including public announcement, belief base expansion and forgetting. We illustrate the application potential of the logical framework with the aid of a concrete example taken from the domain of conversational agents .},
  archive      = {J_AIJ},
  author       = {Emiliano Lorini},
  doi          = {10.1016/j.artint.2020.103233},
  journal      = {Artificial Intelligence},
  pages        = {103233},
  shortjournal = {Artif. Intell.},
  title        = {Rethinking epistemic logic with belief bases},
  volume       = {282},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Regression and progression in stochastic domains.
<em>AIJ</em>, <em>281</em>, 103247. (<a
href="https://doi.org/10.1016/j.artint.2020.103247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reasoning about degrees of belief in uncertain dynamic worlds is fundamental to many applications, such as robotics and planning, where actions modify state properties and sensors provide measurements, both of which are prone to noise. With the exception of limited cases such as Gaussian processes over linear phenomena, belief state evolution can be complex and hard to reason with in a general way, especially when the agent has to deal with categorical assertions, incomplete information such as disjunctive knowledge, as well as probabilistic knowledge. Among the many approaches for reasoning about degrees of belief in the presence of noisy sensing and acting, the logical account proposed by Bacchus, Halpern, and Levesque is perhaps the most expressive, allowing for such belief states to be expressed naturally as constraints. While that proposal is powerful, the task of how to plan effectively is not addressed. In fact, at a more fundamental level, the task of projection, that of reasoning about beliefs effectively after acting and sensing, is left entirely open. To aid planning algorithms , we study the projection problem in this work. In the reasoning about actions literature, there are two main solutions to projection: regression and progression. Both of these have proven enormously useful for the design of logical agents, essentially paving the way for cognitive robotics . Roughly, regression reduces a query about the future to a query about the initial state. Progression, on the other hand, changes the initial state according to the effects of each action and then checks whether the formula holds in the updated state. In this work, we show how both of these generalize in the presence of degrees of belief, noisy acting and sensing. Our results allow for both discrete and continuous probability distributions to be used in the specification of beliefs and dynamics.},
  archive      = {J_AIJ},
  author       = {Vaishak Belle and Hector J. Levesque},
  doi          = {10.1016/j.artint.2020.103247},
  journal      = {Artificial Intelligence},
  pages        = {103247},
  shortjournal = {Artif. Intell.},
  title        = {Regression and progression in stochastic domains},
  volume       = {281},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ethical approaches and autonomous systems. <em>AIJ</em>,
<em>281</em>, 103239. (<a
href="https://doi.org/10.1016/j.artint.2020.103239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we consider how the three main approaches to ethics – deontology, consequentialism and virtue ethics – relate to the implementation of ethical agents. We provide a description of each approach and how agents might be implemented by designers following the different approaches. Although there are numerous examples of agents implemented within the consequentialist and deontological approaches, this is not so for virtue ethics. We therefore propose a novel means of implementing agents within the virtue ethics approach. It is seen that each approach has its own particular strengths and weaknesses when considered as the basis for implementing ethical agents, and that the different approaches are appropriate to different kinds of system.},
  archive      = {J_AIJ},
  author       = {T.J.M. Bench-Capon},
  doi          = {10.1016/j.artint.2020.103239},
  journal      = {Artificial Intelligence},
  pages        = {103239},
  shortjournal = {Artif. Intell.},
  title        = {Ethical approaches and autonomous systems},
  volume       = {281},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Epistemic graphs for representing and reasoning with
positive and negative influences of arguments. <em>AIJ</em>,
<em>281</em>, 103236. (<a
href="https://doi.org/10.1016/j.artint.2020.103236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces epistemic graphs as a generalization of the epistemic approach to probabilistic argumentation. In these graphs, an argument can be believed or disbelieved up to a given degree, thus providing a more fine–grained alternative to the standard Dung&#39;s approaches when it comes to determining the status of a given argument. Furthermore, the flexibility of the epistemic approach allows us to both model the rationale behind the existing semantics as well as completely deviate from them when required. Epistemic graphs can model both attack and support as well as relations that are neither support nor attack. The way other arguments influence a given argument is expressed by the epistemic constraints that can restrict the belief we have in an argument with a varying degree of specificity. The fact that we can specify the rules under which arguments should be evaluated and we can include constraints between unrelated arguments permits the framework to be more context–sensitive. It also allows for better modelling of imperfect agents, which can be important in multi–agent applications.},
  archive      = {J_AIJ},
  author       = {Anthony Hunter and Sylwia Polberg and Matthias Thimm},
  doi          = {10.1016/j.artint.2020.103236},
  journal      = {Artificial Intelligence},
  pages        = {103236},
  shortjournal = {Artif. Intell.},
  title        = {Epistemic graphs for representing and reasoning with positive and negative influences of arguments},
  volume       = {281},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Story embedding: Learning distributed representations of
stories based on character networks. <em>AIJ</em>, <em>281</em>, 103235.
(<a href="https://doi.org/10.1016/j.artint.2020.103235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study aims to learn representations of stories in narrative works (i.e., creative works that contain stories) using fixed-length vectors. Vector representations of stories enable us to compare narrative works regardless of their media or formats. To computationally represent stories, we focus on social networks among characters (character networks). We assume that the structural features of the character networks reflect the characteristics of stories. By extending substructure-based graph embedding models, we propose models to learn distributed representations of character networks in stories. The proposed models consist of three parts: (i) discovering substructures of character networks, (ii) embedding each substructure (Char2Vec), and (iii) learning vector representations of each character network (Story2Vec). We find substructures around each character in multiple scales based on proximity between characters. We suppose that a character&#39;s substructures signify its ‘social roles’. Subsequently, a Char2Vec model is designed to embed a social role based on co-occurred social roles. Since character networks are dynamic social networks that temporally evolve, we use temporal changes and adjacency of social roles to determine their co-occurrence. Finally, Story2Vec models predict occurrences of social roles in each story for embedding the story. To predict the occurrences, we apply two approaches: (i) considering temporal changes in social roles as with the Char2Vec model and (ii) focusing on the final social roles of each character. We call the embedding model with the first approach ‘flow-oriented Story2Vec.’ This approach can reflect the context and flow of stories if the dynamics of character networks is well understood. Second, based on the final states of social roles, we can emphasize the denouement of stories, which is an overview of the static structure of the character networks. We name this model as ‘denouement-oriented Story2Vec.’ In addition, we suggest ‘unified Story2Vec’ as a combination of these two models. We evaluated the quality of vector representations generated by the proposed embedding models using movies in the real world.},
  archive      = {J_AIJ},
  author       = {O-Joun Lee and Jason J. Jung},
  doi          = {10.1016/j.artint.2020.103235},
  journal      = {Artificial Intelligence},
  pages        = {103235},
  shortjournal = {Artif. Intell.},
  title        = {Story embedding: Learning distributed representations of stories based on character networks},
  volume       = {281},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Synchronous bidirectional inference for neural sequence
generation. <em>AIJ</em>, <em>281</em>, 103234. (<a
href="https://doi.org/10.1016/j.artint.2020.103234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In sequence to sequence generation tasks (e.g. machine translation and abstractive summarization), inference is generally performed in a left-to-right manner to produce the result token by token. The neural approaches, such as LSTM and self-attention networks, are now able to make full use of all the predicted history hypotheses from left side during inference, but cannot meanwhile access any future (right side) information and usually generate unbalanced outputs (e.g. left parts are much more accurate than right ones in Chinese-English translation). In this work, we propose a synchronous bidirectional inference model to generate outputs using both left-to-right and right-to-left decoding simultaneously and interactively. First, we introduce a novel beam search algorithm that facilitates synchronous bidirectional decoding. Then, we present the core approach which enables left-to-right and right-to-left decoding to interact with each other, so as to utilize both the history and future predictions simultaneously during inference. We apply the proposed model to both LSTM and self-attention networks. Furthermore, we propose a novel fine-tuning based parameter optimization algorithm in addition to the simple two-pass strategy. The extensive experiments on machine translation and abstractive summarization demonstrate that our synchronous bidirectional inference model can achieve remarkable improvements over the strong baselines.},
  archive      = {J_AIJ},
  author       = {Jiajun Zhang and Long Zhou and Yang Zhao and Chengqing Zong},
  doi          = {10.1016/j.artint.2020.103234},
  journal      = {Artificial Intelligence},
  pages        = {103234},
  shortjournal = {Artif. Intell.},
  title        = {Synchronous bidirectional inference for neural sequence generation},
  volume       = {281},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Definability for model counting. <em>AIJ</em>, <em>281</em>,
103229. (<a href="https://doi.org/10.1016/j.artint.2019.103229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We define and evaluate a new preprocessing technique for propositional model counting. This technique leverages definability, i.e., the ability to determine that some gates are implied by the input formula Σ. Such gates can be exploited to simplify Σ without modifying its number of models. Unlike previous techniques based on gate detection and replacement, gates do not need to be made explicit in our approach. Our preprocessing technique thus consists of two phases: computing a bipartition 〈 I , O 〉 〈I, O〉 of the variables of Σ where the variables from O are defined in Σ in terms of I , then eliminating some variables of O in Σ. Our experiments show the computational benefits which can be achieved by taking advantage of our preprocessing technique for model counting.},
  archive      = {J_AIJ},
  author       = {Jean-Marie Lagniez and Emmanuel Lonca and Pierre Marquis},
  doi          = {10.1016/j.artint.2019.103229},
  journal      = {Artificial Intelligence},
  pages        = {103229},
  shortjournal = {Artif. Intell.},
  title        = {Definability for model counting},
  volume       = {281},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The computational complexity of angry birds. <em>AIJ</em>,
<em>280</em>, 103232. (<a
href="https://doi.org/10.1016/j.artint.2019.103232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The physics-based simulation game Angry Birds has been heavily researched by the AI community over the past five years, and has been the subject of a popular AI competition that is currently held annually as part of a leading AI conference. Developing intelligent agents that can play this game effectively has been an incredibly complex and challenging problem for traditional AI techniques to solve, even though the game is simple enough that any human player could learn and master it within a short time. In this paper we analyse how hard the problem really is, presenting several proofs for the computational complexity of Angry Birds. By using a combination of several gadgets within this game&#39;s environment, we are able to demonstrate that the decision problem of solving general levels for different versions of Angry Birds is either NP-hard, PSPACE-hard, PSPACE-complete or EXPTIME-hard. Proof of NP-hardness is by reduction from 3-SAT, whilst proof of PSPACE-hardness is by reduction from True Quantified Boolean Formula (TQBF). Proof of EXPTIME-hardness is by reduction from G2, a known EXPTIME-complete problem similar to that used for many previous games such as Chess, Go and Checkers. To the best of our knowledge, this is the first time that a single-player game has been proven EXPTIME-hard. This is achieved by using stochastic game engine dynamics to effectively model the real world, or in our case the physics simulator, as the opponent against which we are playing. These proofs can also be extended to other physics-based games with similar mechanics.},
  archive      = {J_AIJ},
  author       = {Matthew Stephenson and Jochen Renz and Xiaoyu Ge},
  doi          = {10.1016/j.artint.2019.103232},
  journal      = {Artificial Intelligence},
  pages        = {103232},
  shortjournal = {Artif. Intell.},
  title        = {The computational complexity of angry birds},
  volume       = {280},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Relative inconsistency measures. <em>AIJ</em>, <em>280</em>,
103231. (<a href="https://doi.org/10.1016/j.artint.2019.103231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The literature on inconsistency measures has ignored a distinction, that is, differentiating absolute measures and relative measures. An absolute measure gives the total amount of inconsistency in the knowledge base but a relative measure computes, by some criteria, the proportion of the base that is inconsistent. To compare the inconsistency measures, researchers have proposed postulates for such measures. We split these postulates into three groups: ones (including two new postulates) that relative measures should satisfy, ones inappropriate for relative measures, and ones that relative measures may satisfy. We obtain some new results upon the relationships between these groups of postulates. On these grounds, we introduce a formal definition for relative inconsistency measures. We consider some relative measures previously proposed and define several new ones that serve as examples. We show that all of these measures satisfy the new formal definition.},
  archive      = {J_AIJ},
  author       = {Philippe Besnard and John Grant},
  doi          = {10.1016/j.artint.2019.103231},
  journal      = {Artificial Intelligence},
  pages        = {103231},
  shortjournal = {Artif. Intell.},
  title        = {Relative inconsistency measures},
  volume       = {280},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SCCWalk: An efficient local search algorithm and its
improvements for maximum weight clique problem. <em>AIJ</em>,
<em>280</em>, 103230. (<a
href="https://doi.org/10.1016/j.artint.2019.103230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The maximum weight clique problem (MWCP) is an important generalization of the maximum clique problem with wide applications. In this study, we develop two efficient local search algorithms for MWCP, namely SCCWalk and SCCWalk4L, where SCCWalk4L is improved from SCCWalk for large graphs. There are two main ideas in SCCWalk, including strong configuration checking (SCC) and walk perturbation. SCC is a new variant of a powerful strategy called configuration checking for local search. The walk perturbation procedure is used to lead the algorithm to leave the current area and come into a new area of feasible solution space. Moreover, to improve the performance on massive graphs, we apply a low-complexity heuristic called best from multiple selection to select the swapping vertex pair quickly and effectively, resulting in the SCCWalk4L algorithm. In addition, SCCWalk4L uses two recent reduction rules to decrease the scale of massive graphs. We carry out experiments to evaluate our algorithms on several popular benchmarks, which are divided into two groups, including classical benchmarks of small graphs namely DIMACS, BHOSLIB, winner determination problem, and graphs derived from clustering aggregation, as well as massive graphs, including a suite of massive real-world graphs and large-scale FRB graphs. Experiments show that, compared to state-of-the-art heuristic algorithms and exact algorithm, the proposed algorithms perform better on classical benchmarks, and obtain the best solutions for most massive graphs.},
  archive      = {J_AIJ},
  author       = {Yiyuan Wang and Shaowei Cai and Jiejiang Chen and Minghao Yin},
  doi          = {10.1016/j.artint.2019.103230},
  journal      = {Artificial Intelligence},
  pages        = {103230},
  shortjournal = {Artif. Intell.},
  title        = {SCCWalk: An efficient local search algorithm and its improvements for maximum weight clique problem},
  volume       = {280},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reasoning about uncertain parameters and agent behaviors
through encoded experiences and belief planning. <em>AIJ</em>,
<em>280</em>, 103228. (<a
href="https://doi.org/10.1016/j.artint.2019.103228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Robots are expected to handle increasingly complex tasks. Such tasks often include interaction with objects or collaboration with other agents. One of the key challenges for reasoning in such situations is the lack of accurate models that hinders the effectiveness of planners. We present a system for online model adaptation that continuously validates and improves models while solving tasks with a belief space planner. We employ the well known online belief planner POMCP. Particles are used to represent hypotheses about the current state and about models of the world. They are sufficient to configure a simulator to provide transition and observation models. We propose an enhanced particle reinvigoration process that leverages prior experiences encoded in a recurrent neural network (RNN). The network is trained through interaction with a large variety of object and agent parametrizations . The RNN is combined with a mixture density network (MDN) to process the current history of observations in order to propose suitable particles and models parametrizations . The proposed method also ensures that newly generated particles are consistent with the current history. These enhancements to the particle reinvigoration process help alleviate problems arising from poor sampling quality in large state spaces and enable handling of dynamics with discontinuities. The proposed approach can be applied to a variety of domains depending on what uncertainty the decision maker needs to reason about. We evaluate the approach with experiments in several domains and compare against other state-of-the-art methods. Experiments are done in a collaborative multi-agent and a single agent object manipulation domain. The experiments are performed both in simulation and on a real robot. The framework handles reasoning with uncertain agent behaviors and with unknown object and environment parametrizations well. The results show good performance and indicate that the proposed approach can improve existing state-of-the-art methods.},
  archive      = {J_AIJ},
  author       = {Akinobu Hayashi and Dirk Ruiken and Tadaaki Hasegawa and Christian Goerick},
  doi          = {10.1016/j.artint.2019.103228},
  journal      = {Artificial Intelligence},
  pages        = {103228},
  shortjournal = {Artif. Intell.},
  title        = {Reasoning about uncertain parameters and agent behaviors through encoded experiences and belief planning},
  volume       = {280},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An introduction to the planning domain definition language
(PDDL): Book review. <em>AIJ</em>, <em>280</em>, 103221. (<a
href="https://doi.org/10.1016/j.artint.2019.103221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AIJ},
  author       = {Alfonso Emilio Gerevini},
  doi          = {10.1016/j.artint.2019.103221},
  journal      = {Artificial Intelligence},
  pages        = {103221},
  shortjournal = {Artif. Intell.},
  title        = {An introduction to the planning domain definition language (PDDL): Book review},
  volume       = {280},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Polynomial rewritings from expressive description logics
with closed predicates to variants of datalog. <em>AIJ</em>,
<em>280</em>, 103220. (<a
href="https://doi.org/10.1016/j.artint.2019.103220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many scenarios, complete and incomplete information coexist. For this reason, the knowledge representation and database communities have long shown interest in simultaneously supporting the closed- and the open-world views when reasoning about logic theories. Here we consider the setting of querying possibly incomplete data using logic theories, formalized as the evaluation of an ontology-mediated query (OMQ) that pairs a query with a theory, sometimes called an ontology , expressing background knowledge. This can be further enriched by specifying a set of closed predicates from the theory that are to be interpreted under the closed-world assumption, while the rest are interpreted with the open-world view. In this way we can retrieve more precise answers to queries by leveraging the partial completeness of the data. The central goal of this paper is to understand the relative expressiveness of ontology-mediated query languages in which the ontology part is written in the expressive Description Logic (DL) ALCHOI ALCHOI and includes a set of closed predicates. We consider a restricted class of conjunctive queries . Our main result is to show that every query in this non-monotonic query language can be translated in polynomial time into Datalog with negation as failure under the stable model semantics . To overcome the challenge that Datalog has no direct means to express the existential quantification present in ALCHOI ALCHOI , we define a two-player game that characterizes the satisfaction of the ontology, and design a Datalog query that can decide the existence of a winning strategy for the game. If there are no closed predicates—in the case of querying an ALCHOI ALCHOI knowledge base—our translation yields a positive disjunctive Datalog program of polynomial size. To the best of our knowledge, unlike previous translations for related fragments with expressive (non-Horn) DLs, these are the first polynomial time translations.},
  archive      = {J_AIJ},
  author       = {Shqiponja Ahmetaj and Magdalena Ortiz and Mantas Šimkus},
  doi          = {10.1016/j.artint.2019.103220},
  journal      = {Artificial Intelligence},
  pages        = {103220},
  shortjournal = {Artif. Intell.},
  title        = {Polynomial rewritings from expressive description logics with closed predicates to variants of datalog},
  volume       = {280},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). When autonomous agents model other agents: An appeal for
altered judgment coupled with mouths, ears, and a little more tape.
<em>AIJ</em>, <em>280</em>, 103219. (<a
href="https://doi.org/10.1016/j.artint.2019.103219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Agent modeling has rightfully garnered much attention in the design and study of autonomous agents that interact with other agents. However, despite substantial progress to date, existing agent-modeling methods too often (a) have unrealistic computational requirements and data needs; (b) fail to properly generalize across environments, tasks, and associates; and (c) guide behavior toward inefficient (myopic) solutions. Can these challenges be overcome? Or are they just inherent to a very complex problem? In this reflection, I argue that some of these challenges may be reduced by, first, modeling alternative processes than what is often modeled by existing algorithms and, second, considering more deeply the role of non-binding communication signals. Additionally, I believe that progress in developing autonomous agents that effectively interact with other agents will be enhanced as we develop and utilize a more comprehensive set of measurement tools and benchmarks. I believe that further development of these areas is critical to creating autonomous agents that effectively model and interact with other agents.},
  archive      = {J_AIJ},
  author       = {Jacob W. Crandall},
  doi          = {10.1016/j.artint.2019.103219},
  journal      = {Artificial Intelligence},
  pages        = {103219},
  shortjournal = {Artif. Intell.},
  title        = {When autonomous agents model other agents: An appeal for altered judgment coupled with mouths, ears, and a little more tape},
  volume       = {280},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The hanabi challenge: A new frontier for AI research.
<em>AIJ</em>, <em>280</em>, 103216. (<a
href="https://doi.org/10.1016/j.artint.2019.103216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {From the early days of computing, games have been important testbeds for studying how well machines can do sophisticated decision making. In recent years, machine learning has made dramatic advances with artificial agents reaching superhuman performance in challenge domains like Go, Atari, and some variants of poker. As with their predecessors of chess, checkers, and backgammon, these game domains have driven research by providing sophisticated yet well-defined challenges for artificial intelligence practitioners. We continue this tradition by proposing the game of Hanabi as a new challenge domain with novel problems that arise from its combination of purely cooperative gameplay with two to five players and imperfect information . In particular, we argue that Hanabi elevates reasoning about the beliefs and intentions of other agents to the foreground. We believe developing novel techniques for such theory of mind reasoning will not only be crucial for success in Hanabi, but also in broader collaborative efforts, especially those with human partners. To facilitate future research, we introduce the open-source Hanabi Learning Environment, propose an experimental framework for the research community to evaluate algorithmic advances, and assess the performance of current state-of-the-art techniques.},
  archive      = {J_AIJ},
  author       = {Nolan Bard and Jakob N. Foerster and Sarath Chandar and Neil Burch and Marc Lanctot and H. Francis Song and Emilio Parisotto and Vincent Dumoulin and Subhodeep Moitra and Edward Hughes and Iain Dunning and Shibl Mourad and Hugo Larochelle and Marc G. Bellemare and Michael Bowling},
  doi          = {10.1016/j.artint.2019.103216},
  journal      = {Artificial Intelligence},
  pages        = {103216},
  shortjournal = {Artif. Intell.},
  title        = {The hanabi challenge: A new frontier for AI research},
  volume       = {280},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Landmark-based approaches for goal recognition as planning.
<em>AIJ</em>, <em>279</em>, 103217. (<a
href="https://doi.org/10.1016/j.artint.2019.103217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognizing goals and plans from complete or partial observations can be efficiently achieved through automated planning techniques. In many applications, it is important to recognize goals and plans not only accurately, but also quickly. To address this challenge, we develop novel goal recognition approaches based on planning techniques that rely on planning landmarks. In automated planning, landmarks are properties (or actions) that cannot be avoided to achieve a goal. We show the applicability of a number of planning techniques with an emphasis on landmarks for goal recognition tasks in two settings: (1) we use the concept of landmarks to develop goal recognition heuristics; and (2) we develop a landmark-based filtering method to refine existing planning-based goal and plan recognition approaches. These recognition approaches are empirically evaluated in experiments over several classical planning domains. We show that our goal recognition approaches yield not only accuracy comparable to (and often higher than) other state-of-the-art techniques, but also result in substantially faster recognition time over existing techniques.},
  archive      = {J_AIJ},
  author       = {Ramon Fraga Pereira and Nir Oren and Felipe Meneguzzi},
  doi          = {10.1016/j.artint.2019.103217},
  journal      = {Artificial Intelligence},
  pages        = {103217},
  shortjournal = {Artif. Intell.},
  title        = {Landmark-based approaches for goal recognition as planning},
  volume       = {279},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Train-o-matic: Supervised word sense disambiguation with no
(manual) effort. <em>AIJ</em>, <em>279</em>, 103215. (<a
href="https://doi.org/10.1016/j.artint.2019.103215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Word Sense Disambiguation (WSD) is the task of associating the correct meaning with a word in a given context. WSD provides explicit semantic information that is beneficial to several downstream applications, such as question answering, semantic parsing and hypernym extraction. Unfortunately, WSD suffers from the well-known knowledge acquisition bottleneck problem: it is very expensive, in terms of both time and money, to acquire semantic annotations for a large number of sentences. To address this blocking issue we present Train-O-Matic, a knowledge-based and language-independent approach that is able to provide millions of training instances annotated automatically with word meanings. The approach is fully automatic, i.e., no human intervention is required, and the only type of human knowledge used is a task-independent WordNet-like resource. Moreover, as the sense distribution in the training set is pivotal to boosting the performance of WSD systems, we also present two unsupervised and language-independent methods that automatically induce a sense distribution when given a simple corpus of sentences. We show that, when the learned distributions are taken into account for generating the training sets, the performance of supervised methods is further enhanced. Experiments have proven that Train-O-Matic on its own, and also coupled with word sense distribution learning methods, lead a supervised system to achieve state-of-the-art performance consistently across gold standard datasets and languages. Importantly, we show how our sense distribution learning techniques aid Train-O-Matic to scale well over domains, without any extra human effort. To encourage future research, we release all the training sets in 5 different languages and the sense distributions for each domain of SemEval-13 and SemEval-15 at http://trainomatic.org .},
  archive      = {J_AIJ},
  author       = {Tommaso Pasini and Roberto Navigli},
  doi          = {10.1016/j.artint.2019.103215},
  journal      = {Artificial Intelligence},
  pages        = {103215},
  shortjournal = {Artif. Intell.},
  title        = {Train-O-matic: Supervised word sense disambiguation with no (manual) effort},
  volume       = {279},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Governing convergence of max-sum on DCOPs through damping
and splitting. <em>AIJ</em>, <em>279</em>, 103212. (<a
href="https://doi.org/10.1016/j.artint.2019.103212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Max-sum is a version of Belief Propagation, used for solving DCOPs. In tree-structured problems, Max-sum converges to the optimal solution in linear time. Unfortunately, when the constraint graph representing the problem includes multiple cycles (as in many standard DCOP benchmarks), Max-sum does not converge and explores low quality solutions. Recent attempts to address this limitation proposed versions of Max-sum that guarantee convergence, while ignoring some of the problem&#39;s constraints. Damping is a method that is often used for increasing the chances that Belief Propagation will converge. That being said, it has not been suggested for inclusion in the algorithms that propose Max-sum for solving DCOPs. In this paper we advance the research on incomplete-inference DCOP algorithms by: 1) investigating the effect of damping on Max-sum. We prove that, while damping slows down the propagation of information among agents, on tree-structured graphs, Max-sum with damping is guaranteed to converge to the optimal solution in weakly polynomial time ; and 2) proposing a novel method for adjusting the level of asymmetry in the factor graph , in order to achieve a balance between exploitation and exploration, when using Max-sum for solving DCOPs. By converting a standard factor graph to an equivalent split constraint factor graph (SCFG), in which each function-node is split into two function-nodes, we can control the level of asymmetry for each constraint. Our empirical results demonstrate a drastic improvement in the performance of Max-sum when using damping (referred to herein as Damped Max-sum, DMS). However, in contrast to the common assumption that Max-sum performs best when converging, we demonstrate that non converging versions perform efficient exploration, and produce high quality results, when implemented within an anytime framework. On most standard benchmarks, the best results were achieved using versions with a high damping factor, which outperformed existing incomplete DCOP algorithms. In addition, our results imply that by applying DMS to SCFGs with a minor level of asymmetry, we can find high quality solutions within a small number of iterations, even without using an anytime framework. We prove that for a factor graph with a single constraint, if this constraint is split symmetrically, Max-sum applied to the resulting cycle is guaranteed to converge to the optimal solution. We further demonstrate that for an asymmetric split, convergence is not guaranteed.},
  archive      = {J_AIJ},
  author       = {Liel Cohen and Rotem Galiki and Roie Zivan},
  doi          = {10.1016/j.artint.2019.103212},
  journal      = {Artificial Intelligence},
  pages        = {103212},
  shortjournal = {Artif. Intell.},
  title        = {Governing convergence of max-sum on DCOPs through damping and splitting},
  volume       = {279},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Preference elicitation and robust winner determination for
single- and multi-winner social choice. <em>AIJ</em>, <em>279</em>,
103203. (<a href="https://doi.org/10.1016/j.artint.2019.103203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of voting schemes based on rankings of alternatives to solve social choice problems can often impose significant burden on voters, both in terms of communication and cognitive requirements. In this paper, we develop techniques for preference elicitation in voting settings (i.e., vote elicitation ) that can alleviate this burden by minimizing the amount of preference information needed to find (approximately or exactly) optimal outcomes. We first describe robust optimization techniques for determining winning alternatives given partial preference information (i.e., partial rankings) using the notion of minimax regret . We show that the corresponding computational problem is tractable for some important voting rules, and intractable for others. We then use the solution to the minimax-regret optimization as the basis for vote elicitation schemes that determine appropriate preference queries for voters to quickly reduce potential regret. We apply these techniques to multi-winner social choice problems as well, in which a slate of alternatives must be selected, developing both exact and greedy robust optimization procedures. Empirical results on several data sets validate the effectiveness of our techniques.},
  archive      = {J_AIJ},
  author       = {Tyler Lu and Craig Boutilier},
  doi          = {10.1016/j.artint.2019.103203},
  journal      = {Artificial Intelligence},
  pages        = {103203},
  shortjournal = {Artif. Intell.},
  title        = {Preference elicitation and robust winner determination for single- and multi-winner social choice},
  volume       = {279},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Recursively modeling other agents for decision making: A
research perspective. <em>AIJ</em>, <em>279</em>, 103202. (<a
href="https://doi.org/10.1016/j.artint.2019.103202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individuals exhibit theory of mind, attributing beliefs, intent, and mental states to others as explanations of observed actions. Dennett&#39;s intentional stance offers an analogous abstraction for computational agents seeking to understand, explain, or predict others&#39; behaviors. These recognized theories provide a formal basis to ongoing investigations of recursive modeling. We review and situate various frameworks for recursive modeling that have been studied in game- and decision- theories, and have yielded methods useful to AI researchers. Sustained attention given to these frameworks has produced new analyses and methods with an aim toward making recursive modeling practicable. Indeed, we also review some emerging uses and the insights these yielded, which are indicative of pragmatic progress in this area. The significance of these frameworks is that higher-order reasoning is critical to correctly recognizing others&#39; intent or outthinking opponents. Such reasoning has been utilized in academic, business, military, security, and other contexts both to train and inform decision-making agents in organizational and strategic contexts, and also to more realistically predict and best respond to other agents&#39; intent.},
  archive      = {J_AIJ},
  author       = {Prashant Doshi and Piotr Gmytrasiewicz and Edmund Durfee},
  doi          = {10.1016/j.artint.2019.103202},
  journal      = {Artificial Intelligence},
  pages        = {103202},
  shortjournal = {Artif. Intell.},
  title        = {Recursively modeling other agents for decision making: A research perspective},
  volume       = {279},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mind the gaps: Assuring the safety of autonomous systems
from an engineering, ethical, and legal perspective. <em>AIJ</em>,
<em>279</em>, 103201. (<a
href="https://doi.org/10.1016/j.artint.2019.103201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper brings together a multi-disciplinary perspective from systems engineering, ethics, and law to articulate a common language in which to reason about the multi-faceted problem of assuring the safety of autonomous systems . The paper&#39;s focus is on the “gaps” that arise across the development process: the semantic gap , where normal conditions for a complete specification of intended functionality are not present; the responsibility gap, where normal conditions for holding human actors morally responsible for harm are not present; and the liability gap, where normal conditions for securing compensation to victims of harm are not present. By categorising these “gaps” we can expose with greater precision key sources of uncertainty and risk with autonomous systems . This can inform the development of more detailed models of safety assurance and contribute to more effective risk control.},
  archive      = {J_AIJ},
  author       = {Simon Burton and Ibrahim Habli and Tom Lawton and John McDermid and Phillip Morgan and Zoe Porter},
  doi          = {10.1016/j.artint.2019.103201},
  journal      = {Artificial Intelligence},
  pages        = {103201},
  shortjournal = {Artif. Intell.},
  title        = {Mind the gaps: Assuring the safety of autonomous systems from an engineering, ethical, and legal perspective},
  volume       = {279},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Clause vivification by unit propagation in CDCL SAT solvers.
<em>AIJ</em>, <em>279</em>, 103197. (<a
href="https://doi.org/10.1016/j.artint.2019.103197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Original and learnt clauses in Conflict-Driven Clause Learning (CDCL) SAT solvers often contain redundant literals. This may have a negative impact on solver performance, because redundant literals may deteriorate both the effectiveness of Boolean constraint propagation and the quality of subsequent learnt clauses. To overcome this drawback, we propose a clause vivification approach that eliminates redundant literals by applying unit propagation . The proposed clause vivification is activated before the SAT solver triggers some selected restarts, and only affects a subset of original and learnt clauses, which are considered to be more relevant according to metrics like the literal block distance (LBD). Moreover, we conducted an empirical investigation with instances coming from the hard combinatorial and application categories of recent SAT competitions. The results show that a significant number of additional instances are solved when the proposed approach is incorporated into five of the best performing CDCL SAT solvers (Glucose, TC_Glucose, COMiniSatPS, MapleCOMSPS and MapleCOMSPS_LRB). More importantly, the empirical investigation includes an in-depth analysis of the effectiveness of clause vivification. It is worth mentioning that one of the SAT solvers described here was ranked first in the main track of SAT Competition 2017 thanks to the incorporation of the proposed clause vivification. That solver was further improved in this paper and won the bronze medal in the main track of SAT Competition 2018.},
  archive      = {J_AIJ},
  author       = {Chu-Min Li and Fan Xiao and Mao Luo and Felip Manyà and Zhipeng Lü and Yu Li},
  doi          = {10.1016/j.artint.2019.103197},
  journal      = {Artificial Intelligence},
  pages        = {103197},
  shortjournal = {Artif. Intell.},
  title        = {Clause vivification by unit propagation in CDCL SAT solvers},
  volume       = {279},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design and results of the second international competition
on computational models of argumentation. <em>AIJ</em>, <em>279</em>,
103193. (<a href="https://doi.org/10.1016/j.artint.2019.103193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Argumentation is a major topic in the study of Artificial Intelligence . Since the first edition in 2015, advancements in solving (abstract) argumentation frameworks are assessed in competition events, similar to other closely related problem solving technologies. In this paper, we report about the design and results of the Second International Competition on Computational Models of Argumentation, which has been jointly organized by TU Dresden (Germany), TU Wien (Austria), and the University of Genova (Italy), in affiliation with the 2017 International Workshop on Theory and Applications of Formal Argumentation. This second edition maintains some of the design choices made in the first event, e.g. the I/O formats, the basic reasoning problems, and the organization into tasks and tracks. At the same time, it introduces significant novelties, e.g. three additional prominent semantics, and an instance selection stage for classifying instances according to their empirical hardness.},
  archive      = {J_AIJ},
  author       = {Sarah A. Gaggl and Thomas Linsbichler and Marco Maratea and Stefan Woltran},
  doi          = {10.1016/j.artint.2019.103193},
  journal      = {Artificial Intelligence},
  pages        = {103193},
  shortjournal = {Artif. Intell.},
  title        = {Design and results of the second international competition on computational models of argumentation},
  volume       = {279},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New models for generating hard random boolean formulas and
disjunctive logic programs. <em>AIJ</em>, <em>279</em>, 103185. (<a
href="https://doi.org/10.1016/j.artint.2019.103185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose two models of random quantified boolean formulas and their natural random disjunctive logic program counterparts. The models extend the standard models of random k -CNF formulas and the Chen-Interian model of random 2QBFs. The first model controls the generation of programs and QSAT formulas by imposing a specific structure on rules and clauses, respectively. The second model is based on a family of QSAT formulas in a non-clausal form. We provide theoretical bounds for the phase transition region in our models, and show experimentally the presence of the easy-hard-easy pattern and its alignment with the location of the phase transition. We show that boolean formulas and logic programs from our models are significantly harder than those obtained from the standard k -CNF and Chen-Interian models, and that their combination yields formulas and programs that are “super-hard” to evaluate. We also provide evidence suggesting that formulas from one of our models are well suited for assessing solvers tuned to real-world instances. Finally, it is noteworthy that, to the best of our knowledge, our models and results on random disjunctive logic programs are the first of their kind.},
  archive      = {J_AIJ},
  author       = {Giovanni Amendola and Francesco Ricca and Miroslaw Truszczynski},
  doi          = {10.1016/j.artint.2019.103185},
  journal      = {Artificial Intelligence},
  pages        = {103185},
  shortjournal = {Artif. Intell.},
  title        = {New models for generating hard random boolean formulas and disjunctive logic programs},
  volume       = {279},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Introducing article numbering to artificial intelligence.
<em>AIJ</em>, <em>278</em>, 103210. (<a
href="https://doi.org/10.1016/S0004-3702(19)30204-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_AIJ},
  author       = {Sweitze Roffel},
  doi          = {10.1016/S0004-3702(19)30204-8},
  journal      = {Artificial Intelligence},
  pages        = {103210},
  shortjournal = {Artif. Intell.},
  title        = {Introducing article numbering to artificial intelligence},
  volume       = {278},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Representing and planning with interacting actions and
privacy. <em>AIJ</em>, <em>278</em>, 103200. (<a
href="https://doi.org/10.1016/j.artint.2019.103200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interacting actions – actions whose joint effect differs from the union of their individual effects – are challenging both to represent and to plan with due to their combinatorial nature. So far, there have been few attempts to provide a succinct language for representing them that can also support efficient centralized planning and distributed privacy preserving planning. In this paper we suggest an approach for representing interacting actions succinctly and show how such a domain model can be compiled into a standard single-agent planning problem as well as to privacy preserving multi-agent planning. We test the performance of our method on a number of novel domains involving interacting actions and privacy.},
  archive      = {J_AIJ},
  author       = {Shashank Shekhar and Ronen I. Brafman},
  doi          = {10.1016/j.artint.2019.103200},
  journal      = {Artificial Intelligence},
  pages        = {103200},
  shortjournal = {Artif. Intell.},
  title        = {Representing and planning with interacting actions and privacy},
  volume       = {278},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Probabilistic sentence satisfiability: An approach to PSAT.
<em>AIJ</em>, <em>278</em>, 103199. (<a
href="https://doi.org/10.1016/j.artint.2019.103199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information analysis often involves heterogeneous sources expressed as logical sentences, numerical models, sensor data, etc. Each of these has its own way to describe uncertainty or error; e.g., frequency analysis, algorithmic truncation, floating point roundoff, Gaussian distributions, etc. A unifying framework is proposed here to represent this information as logical sentences with associated probabilities in order to allow the inference of the probability of a query sentence. Given such a knowledge base in Conjunctive Normal Form (CNF) for use by an intelligent agent, with probabilities assigned to the conjuncts, the probability of any new query sentence can be determined by solving the Probabilistic Satisfiability Problem (PSAT). This involves finding a consistent probability distribution over the atoms (if they are independent) or complete conjunction set of the atoms. For each sentence in the knowledge base, we propose to produce an equation in terms of atoms and conditional probabilities . This system of equations is then solved numerically to get a solution consistent with the sentence probabilities. Finding such a solution is called the Probabilistic Sentence Satisfiability (PS-SAT) problem. In particular, findings include:},
  archive      = {J_AIJ},
  author       = {T.C. Henderson and R. Simmons and B. Serbinowski and M. Cline and D. Sacharny and X. Fan and A. Mitiche},
  doi          = {10.1016/j.artint.2019.103199},
  journal      = {Artificial Intelligence},
  pages        = {103199},
  shortjournal = {Artif. Intell.},
  title        = {Probabilistic sentence satisfiability: An approach to PSAT},
  volume       = {278},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Realizability of three-valued semantics for abstract
dialectical frameworks. <em>AIJ</em>, <em>278</em>, 103198. (<a
href="https://doi.org/10.1016/j.artint.2019.103198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate fundamental properties of three-valued semantics for abstract dialectical frameworks (ADFs). In particular, we deal with realizability , i.e., the question whether there exists an ADF that has a given set of interpretations as its semantics. We provide necessary and sufficient conditions that hold for a set of three-valued interpretations whenever there is an ADF realizing it under admissible, complete, grounded, or preferred semantics. Moreover, we show how to construct such an ADF in case of realizability. Our results lay the ground for studying the expressiveness of ADFs under three-valued semantics. Since ADFs generalize many existing approaches, our results have immediate impact on many argumentation formalisms including abstract argumentation frameworks. As first applications, we study implications of our results on the existence of certain join operators on ADFs. Furthermore, we exploit our (exact) characterizations to obtain realizability results also for a more relaxed setting, where realizing ADFs may contain hidden statements.},
  archive      = {J_AIJ},
  author       = {Jörg Pührer},
  doi          = {10.1016/j.artint.2019.103198},
  journal      = {Artificial Intelligence},
  pages        = {103198},
  shortjournal = {Artif. Intell.},
  title        = {Realizability of three-valued semantics for abstract dialectical frameworks},
  volume       = {278},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scalable constraint-based virtual data center allocation.
<em>AIJ</em>, <em>278</em>, 103196. (<a
href="https://doi.org/10.1016/j.artint.2019.103196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Constraint-based techniques can solve challenging problems arising in highly diverse applications. This paper considers the problem of virtual data center (VDC) allocation, an important, emerging challenge for modern data center operators. To address this problem, we introduce Netsolver , a system for VDC allocation that is based on constraint solving. Netsolver represents a major improvement over existing approaches: it is sound, complete, and scalable, providing support for end-to-end, multi-path bandwidth guarantees across all the layers of hosting infrastructure, from servers to top-of-rack switches to aggregation switches to access routers. Netsolver scales to realistic data center sizes and VDC topologies, typically requiring just seconds to allocate VDCs of 5–15 virtual machines to physical data centers with 1000+ servers, maintaining this efficiency even when the data center is nearly saturated. In many cases, Netsolver can allocate 150\% − 300\% 150\%−300\% as many total VDCs to the same physical data center as previous methods. Finally, we show how Netsolver can be extended with additional optimization constraints , such as VM affinity and hotspot minimization, demonstrating the flexibility of our approach. The performance and flexibility of Netsolver are made possible by our formalization of the VDC allocation problem in terms of multi-commodity flows, and the corresponding efficient handling of network flow problems in the underlying constraint solvers. This shows the importance of supporting flow-based constraints, which are more mature in ILP- vs. SMT-based constraint solving.},
  archive      = {J_AIJ},
  author       = {Sam Bayless and Nodir Kodirov and Syed M. Iqbal and Ivan Beschastnikh and Holger H. Hoos and Alan J. Hu},
  doi          = {10.1016/j.artint.2019.103196},
  journal      = {Artificial Intelligence},
  pages        = {103196},
  shortjournal = {Artif. Intell.},
  title        = {Scalable constraint-based virtual data center allocation},
  volume       = {278},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Algorithms for estimating the partition function of
restricted boltzmann machines. <em>AIJ</em>, <em>278</em>, 103195. (<a
href="https://doi.org/10.1016/j.artint.2019.103195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate estimates of the normalization constants (partition functions) of energy-based probabilistic models (Markov random fields) are highly important, for example, for assessing the performance of models, monitoring training progress, and conducting likelihood ratio tests . Several algorithms for estimating the partition function (in relation to a reference distribution) have been introduced, including Annealed Importance Sampling (AIS) and Bennett&#39;s Acceptance Ratio method (BAR). However, their conceptual similarities and differences have not been worked out so far and systematic comparisons of their behavior in practice have been missing. We devise a unifying theoretical framework for these algorithms, which comprises existing variants and suggests new approaches. It is based on a generalized form of Crooks&#39; equality linking the expectation over a distribution of samples generated by a transition operator to the expectation over the distribution induced by the reversed operator. The framework covers different ways of generating samples, such as parallel tempering and path sampling. An empirical comparison revealed the differences between the methods when estimating the partition function of restricted Boltzmann machines and Ising models. In our experiments, BAR using parallel tempering worked well with a small number of bridging distributions, while path sampling based AIS performed best when many bridging distributions were available. Because BAR gave the overall best results, we favor it over AIS. Furthermore, the experiments showed the importance of choosing a proper reference distribution.},
  archive      = {J_AIJ},
  author       = {Oswin Krause and Asja Fischer and Christian Igel},
  doi          = {10.1016/j.artint.2019.103195},
  journal      = {Artificial Intelligence},
  pages        = {103195},
  shortjournal = {Artif. Intell.},
  title        = {Algorithms for estimating the partition function of restricted boltzmann machines},
  volume       = {278},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variable neighborhood search for graphical model energy
minimization. <em>AIJ</em>, <em>278</em>, 103194. (<a
href="https://doi.org/10.1016/j.artint.2019.103194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphical models factorize a global probability distribution/energy function as the product/ sum of local functions. A major inference task, known as MAP in Markov Random Fields and MPE in Bayesian Networks , is to find a global assignment of all the variables with maximum a posteriori probability/minimum energy. A usual distinction on MAP solving methods is complete/incomplete, i.e. the ability to prove optimality or not. Most complete methods rely on tree search, while incomplete methods rely on local search. Among them, we study Variable Neighborhood Search ( VNS ) for graphical models. In this paper, we propose an iterative approach above VNS that uses (partial) tree search inside its local neighborhood exploration. The proposed approach performs several neighborhood explorations of increasing search complexity, by controlling two parameters, the discrepancy limit and the neighborhood size. Thus, optimality of the obtained solutions can be proven when the neighborhood size is maximal and with unbounded tree search. We further propose a parallel version of our method improving its anytime behavior on difficult instances coming from a large graphical model benchmark. Last we experiment on the challenging minimum energy problem found in Computational Protein Design, showing the practical benefit of our parallel version. A solver is available at https://github.com/toulbar2/toulbar2 .},
  archive      = {J_AIJ},
  author       = {Abdelkader Ouali and David Allouche and Simon de Givry and Samir Loudni and Yahia Lebbah and Lakhdar Loukil and Patrice Boizumault},
  doi          = {10.1016/j.artint.2019.103194},
  journal      = {Artificial Intelligence},
  pages        = {103194},
  shortjournal = {Artif. Intell.},
  title        = {Variable neighborhood search for graphical model energy minimization},
  volume       = {278},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Gradient conjugate priors and multi-layer neural networks.
<em>AIJ</em>, <em>278</em>, 103184. (<a
href="https://doi.org/10.1016/j.artint.2019.103184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper deals with learning probability distributions of observed data by artificial neural networks . We suggest a so-called gradient conjugate prior (GCP) update appropriate for neural networks , which is a modification of the classical Bayesian update for conjugate priors. We establish a connection between the gradient conjugate prior update and the maximization of the log-likelihood of the predictive distribution . Unlike for the Bayesian neural networks , we use deterministic weights of neural networks, but rather assume that the ground truth distribution is normal with unknown mean and variance and learn by the neural networks the parameters of a prior (normal-gamma distribution) for these unknown mean and variance. The update of the parameters is done, using the gradient that, at each step, directs towards minimizing the Kullback–Leibler divergence from the prior to the posterior distribution (both being normal-gamma). We obtain a corresponding dynamical system for the prior&#39;s parameters and analyze its properties. In particular, we study the limiting behavior of all the prior&#39;s parameters and show how it differs from the case of the classical full Bayesian update. The results are validated on synthetic and real world data sets.},
  archive      = {J_AIJ},
  author       = {Pavel Gurevich and Hannes Stuke},
  doi          = {10.1016/j.artint.2019.103184},
  journal      = {Artificial Intelligence},
  pages        = {103184},
  shortjournal = {Artif. Intell.},
  title        = {Gradient conjugate priors and multi-layer neural networks},
  volume       = {278},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computing AES related-key differential characteristics with
constraint programming. <em>AIJ</em>, <em>278</em>, 103183. (<a
href="https://doi.org/10.1016/j.artint.2019.103183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cryptanalysis aims at testing the properties of encryption processes , and this usually implies solving hard optimization problems . In this paper, we focus on related-key differential attacks for the Advanced Encryption Standard (AES), which is the encryption standard for block ciphers . To mount these attacks, cryptanalysts need to solve the optimal related-key differential characteristic problem. Dedicated approaches do not scale well for this problem, and need weeks to solve its hardest instances. In this paper, we improve existing Constraint Programming (CP) approaches for computing optimal related-key differential characteristics: we add new constraints that detect inconsistencies sooner, and we introduce a new decomposition of the problem in two steps. These improvements allow us to compute all optimal related-key differential characteristics for AES-128, AES-192 and AES-256 in a few hours.},
  archive      = {J_AIJ},
  author       = {David Gerault and Pascal Lafourcade and Marine Minier and Christine Solnon},
  doi          = {10.1016/j.artint.2019.103183},
  journal      = {Artificial Intelligence},
  pages        = {103183},
  shortjournal = {Artif. Intell.},
  title        = {Computing AES related-key differential characteristics with constraint programming},
  volume       = {278},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The complexity of exact learning of acyclic conditional
preference networks from swap examples. <em>AIJ</em>, <em>278</em>,
103182. (<a href="https://doi.org/10.1016/j.artint.2019.103182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning of user preferences, as represented by, for example, Conditional Preference Networks (CP-nets), has become a core issue in AI research. Recent studies investigate learning of CP-nets from randomly chosen examples or from membership and equivalence queries. To assess the optimality of learning algorithms as well as to better understand the combinatorial structure of classes of CP-nets, it is helpful to calculate certain learning-theoretic information complexity parameters. This article focuses on the frequently studied case of exact learning from so-called swap examples, which express preferences among objects that differ in only one attribute. It presents bounds on or exact values of some well-studied information complexity parameters, namely the VC dimension, the teaching dimension, and the recursive teaching dimension, for classes of acyclic CP-nets. We further provide algorithms that exactly learn tree-structured and general acyclic CP-nets from membership queries. Using our results on complexity parameters, we prove that our algorithms, as well as another query learning algorithm for acyclic CP-nets presented in the literature, are near-optimal.},
  archive      = {J_AIJ},
  author       = {Eisa Alanazi and Malek Mouhoub and Sandra Zilles},
  doi          = {10.1016/j.artint.2019.103182},
  journal      = {Artificial Intelligence},
  pages        = {103182},
  shortjournal = {Artif. Intell.},
  title        = {The complexity of exact learning of acyclic conditional preference networks from swap examples},
  volume       = {278},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Grounded language interpretation of robotic commands through
structured learning. <em>AIJ</em>, <em>278</em>, 103181. (<a
href="https://doi.org/10.1016/j.artint.2019.103181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The presence of robots in everyday life is increasing day by day at a growing pace. Industrial and working environments, health-care assistance in public or domestic areas can benefit from robots&#39; services to accomplish manifold tasks that are difficult and annoying for humans. In such scenarios, Natural Language interactions, enabling collaboration and robot control, are meant to be situated, in the sense that both the user and the robot access and make reference to the environment. Contextual knowledge may thus play a key role in solving inherent ambiguities of grounded language as, for example, the prepositional phrase attachment. In this work, we present a linguistic pipeline for semantic processing of robotic commands, that combines discriminative structured learning, distributional semantics and contextual evidence extracted from the working environment. The final goal is to make the interpretation process of linguistic exchanges depending on physical, cognitive and language-dependent aspects. We present, formalize and discuss an adaptive Spoken Language Understanding chain for robotic commands, that explicitly depends on the operational context during both the learning and processing stages. The resulting framework allows to model heterogeneous information concerning the environment (e.g., positional information about the objects and their properties) and to inject it in the learning process. Empirical results demonstrate a significant contribution of such additional dimensions, achieving up to a 25\% of relative error reduction with respect to a pipeline that only exploits linguistic evidence.},
  archive      = {J_AIJ},
  author       = {Andrea Vanzo and Danilo Croce and Emanuele Bastianelli and Roberto Basili and Daniele Nardi},
  doi          = {10.1016/j.artint.2019.103181},
  journal      = {Artificial Intelligence},
  pages        = {103181},
  shortjournal = {Artif. Intell.},
  title        = {Grounded language interpretation of robotic commands through structured learning},
  volume       = {278},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Coalitional games induced by matching problems: Complexity
and islands of tractability for the shapley value. <em>AIJ</em>,
<em>278</em>, 103180. (<a
href="https://doi.org/10.1016/j.artint.2019.103180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper focuses on cooperative games where the worth of any coalition of agents is determined by the optimal value of a matching problem on (possibly weighted) graphs. These games come in different forms that can be grouped in two broad classes, namely of matching and allocation games, and they have a wide spectrum of applications, ranging from two-sided markets where buyers and sellers are encoded as vertices in a graph, to allocation problems where indivisible goods have to be assigned (matched) to agents in a fair way, possibly using monetary compensations. The Shapley value and the related notion of Banzhaf value have often been identified as appropriate solution concepts for many applications of matching/allocation games, but their computation is intractable in general. It is known that these concepts can be computed in polynomial time for matching games on unweighted trees and on graphs having degree at most two. However, it was open whether or not such positive results could be extended to the more general case of graphs having bounded treewidth, and to the case of allocation problems on weighted graphs . The paper provides a positive answer to these questions, by showing that computing the Shapley value and the Banzhaf value is feasible in polynomial time for the following classes of games: matching games over unweighted graphs having bounded treewidth, allocation games over weighted graphs having bounded treewidth, and allocation games over weighted graphs and such that each good is of interest for two agents at most. Without such structural restrictions, computing these solution concepts on allocation games is instead shown to be #P-hard, even in the case of unweighted graphs.},
  archive      = {J_AIJ},
  author       = {Gianluigi Greco and Francesco Lupia and Francesco Scarcello},
  doi          = {10.1016/j.artint.2019.103180},
  journal      = {Artificial Intelligence},
  pages        = {103180},
  shortjournal = {Artif. Intell.},
  title        = {Coalitional games induced by matching problems: Complexity and islands of tractability for the shapley value},
  volume       = {278},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Artificial systems with moral capacities? A research design
and its implementation in a geriatric care system. <em>AIJ</em>,
<em>278</em>, 103179. (<a
href="https://doi.org/10.1016/j.artint.2019.103179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of increasingly intelligent and autonomous technologies will eventually lead to these systems having to face morally problematic situations. This gave rise to the development of artificial morality, an emerging field in artificial intelligence which explores whether and how artificial systems can be furnished with moral capacities. This will have a deep impact on our lives. Yet, the methodological foundations of artificial morality are still sketchy and often far off from possible applications. One important area of application of artificial systems with moral capacities is geriatric care. The goal of this article is to afford the methodological foundations for artificial morality, i.e., for implementing moral capacities in artificial systems in general, and to discuss them with respect to an assistive system in geriatric care which is capable of moral learning.},
  archive      = {J_AIJ},
  author       = {Catrin Misselhorn},
  doi          = {10.1016/j.artint.2019.103179},
  journal      = {Artificial Intelligence},
  pages        = {103179},
  shortjournal = {Artif. Intell.},
  title        = {Artificial systems with moral capacities? a research design and its implementation in a geriatric care system},
  volume       = {278},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reordering all agents in asynchronous backtracking for
distributed constraint satisfaction problems. <em>AIJ</em>,
<em>278</em>, 103169. (<a
href="https://doi.org/10.1016/j.artint.2019.103169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed constraint satisfaction problems (DisCSPs) can express decision problems where physically distributed agents control different decision variables, but must coordinate with each other to agree on a global solution. Asynchronous Backtracking (ABT) is a pivotal search procedure for DisCSPs. ABT requires a static total ordering on the agents. However, reordering agents during search is an essential component for efficiently solving a DisCSP. All polynomial space algorithms proposed so far to improve ABT by reordering agents during search only allow a limited amount of reordering. In this paper, we propose AgileABT, a general framework for reordering agents asynchronously that is able to change the ordering of all agents. This is done via the original notion of termination value , a label attached to the orders exchanged by agents during search. We prove that AgileABT is sound and complete. We show that, thanks to termination values, our framework allows us to implement the main variable ordering heuristics from centralized CSPs, which until now could not be applied to the distributed setting. We prove that AgileABT terminates and has a polynomial space complexity in all these cases. Our empirical study shows the significance of our framework compared to state-of-the-art asynchronous dynamic ordering algorithms for solving distributed CSP.},
  archive      = {J_AIJ},
  author       = {Younes Mechqrane and Mohamed Wahbi and Christian Bessiere and Kenneth N. Brown},
  doi          = {10.1016/j.artint.2019.103169},
  journal      = {Artificial Intelligence},
  pages        = {103169},
  shortjournal = {Artif. Intell.},
  title        = {Reordering all agents in asynchronous backtracking for distributed constraint satisfaction problems},
  volume       = {278},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
