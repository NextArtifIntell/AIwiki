<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>CVIU_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="cviu---93">CVIU - 93</h2>
<ul>
<li><details>
<summary>
(2020). Photometric stereo with central panoramic cameras.
<em>CVIU</em>, <em>201</em>, 103080. (<a
href="https://doi.org/10.1016/j.cviu.2020.103080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce and rigorously solve for the first time the photometric stereo problem for central panoramic systems . In particular, by leveraging the unified camera model, we present a new spherical image irradiance equation and analyze its mathematical properties in detail. The discrete gradient field issued from this equation is fed into a drift-free normal integration algorithm tailored to the spherical image geometry, and the light direction is estimated using the specular highlights observed on mirror balls inside the scene. Extensive experiments conducted with analytical surfaces, and synthetic and real-world images captured by central panoramic cameras, show that the proposed 3D reconstruction pipeline is effective and tolerant to noise. The image database and the code developed are publicly available on the Internet at the address: https://home.mis.u-picardie.fr/ fabio/ustereo.html},
  archive      = {J_CVIU},
  author       = {Jordan Caracotte and Fabio Morbidi and El Mustapha Mouaddib},
  doi          = {10.1016/j.cviu.2020.103080},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103080},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Photometric stereo with central panoramic cameras},
  volume       = {201},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ghost removal via channel attention in exposure fusion.
<em>CVIU</em>, <em>201</em>, 103079. (<a
href="https://doi.org/10.1016/j.cviu.2020.103079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High dynamic range (HDR) imaging is to reconstruct high-quality images with a broad range of illuminations from a set of differently exposed images. Some existing algorithms align the input images before merging them into an HDR image , but artifacts of the registration appear due to misalignment. Recent works try to remove the ghosts by detecting motion region or skipping the registered process, however, the result still suffers from ghost artifacts for scenes with significant motions. In this paper, we propose a novel Multi-scale Channel Attention guided Network ( MCANet ) to address the ghosting problem. We use multi-scale blocks consisting of dilated convolution layers to extract informative features. The channel attention blocks suppress undesired components and guide the network to refine features to make full use of feature maps. The proposed MCANet recovers the occluded or saturated details and reduces artifacts due to misalignment. Experiments show that the proposed MCANet can achieve state-of-the-art quantitative and qualitative results.},
  archive      = {J_CVIU},
  author       = {Qingsen Yan and Bo Wang and Peipei Li and Xianjun Li and Ao Zhang and Qinfeng Shi and Zheng You and Yu Zhu and Jinqiu Sun and Yanning Zhang},
  doi          = {10.1016/j.cviu.2020.103079},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103079},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Ghost removal via channel attention in exposure fusion},
  volume       = {201},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Video scene parsing: An overview of deep learning methods
and datasets. <em>CVIU</em>, <em>201</em>, 103077. (<a
href="https://doi.org/10.1016/j.cviu.2020.103077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video scene parsing (VSP) has become a key problem in the field of computer vision in recent years due to its wide range of applications in numerous domains (e.g., autonomous driving). With the renaissance of deep learning (DL) techniques, various of VSP methods under this framework have demonstrated promising performance. However, no thorough review has been provided to comprehensively summarize the advantages and disadvantages of these methods, their datasets, or the directions for development. To remedy this, we provide an overview of the different DL methods applied to VSP in various scientific and engineering areas. Firstly, we describe several indispensable preliminaries of this field, defining essential background concepts as well as fundamental terminologies and differentiating between VSP and other similar problems. Then, according to their principles, contributions and importance, recent advanced DL methods for VSP are meticulously classified and thoroughly analyzed. Thirdly, we elaborate on the most frequently-used datasets and describe common evaluation metrics for VSP. Besides, extensive of experimental results for the aforementioned methods are presented to demonstrate their advantages and disadvantages. This is followed by further comparisons and discussions on the main challenges faced by researchers. Finally, we sum up the paper by drawing conclusions on the state-of-the-art methods for VSP and highlights potential research orientations as well as promising future work for DL techniques applied to VSP.},
  archive      = {J_CVIU},
  author       = {Xiyu Yan and Huihui Gong and Yong Jiang and Shu-Tao Xia and Feng Zheng and Xinge You and Ling Shao},
  doi          = {10.1016/j.cviu.2020.103077},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103077},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Video scene parsing: An overview of deep learning methods and datasets},
  volume       = {201},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accurate MR image super-resolution via lightweight lateral
inhibition network. <em>CVIU</em>, <em>201</em>, 103075. (<a
href="https://doi.org/10.1016/j.cviu.2020.103075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, convolutional neural networks (CNNs) have shown their advantages on MR image super-resolution (SR) tasks. Many current SR models, however, have heavy demands on computation and memory, which are not friendly to magnetic resonance imaging (MRI) where computing resource is usually constrained. On the other hand, a basic consideration in most MRI experiments is how to reduce scanning time to improve patient comfort and reduce motion artifacts. In this work, we ease the problem by presenting an effective and lightweight model that supports fast training and accurate SR inference. The proposed network is inspired by the lateral inhibition mechanism, which assumes that there exist inhibitory effects between adjacent neurons . The backbone of our network consists of several lateral inhibition blocks, where the inhibitory effect is explicitly implemented by a battery of cascaded local inhibition units. When model scale is small, explicitly inhibiting feature activations is expected to further explore model representational capacity . For more effective feature extraction, several parallel dilated convolutions are also used to extract shallow features directly from the input image. Extensive experiments on typical MR images demonstrate that our lateral inhibition network (LIN) achieves better SR performance than other lightweight models with similar model scale.},
  archive      = {J_CVIU},
  author       = {Xiaole Zhao and Xiafei Hu and Ying Liao and Tian He and Tao Zhang and Xueming Zou and Jinsha Tian},
  doi          = {10.1016/j.cviu.2020.103075},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103075},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Accurate MR image super-resolution via lightweight lateral inhibition network},
  volume       = {201},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A comparison of methods for 3D scene shape retrieval.
<em>CVIU</em>, <em>201</em>, 103070. (<a
href="https://doi.org/10.1016/j.cviu.2020.103070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D scene shape retrieval is a brand new but important research direction in content-based 3D shape retrieval. To promote this research area, two Shape Retrieval Contest (SHREC) tracks on 2D scene sketch-based and image-based 3D scene model retrieval have been organized by us in 2018 and 2019, respectively. In 2018, we built the first benchmark for each track which contains 2D and 3D scene data for ten (10) categories, while they share the same 3D scene target dataset. Four and five distinct 3D scene shape retrieval methods have competed with each other in these two contests, respectively. In 2019, to measure and compare the scalability performance of the participating and other promising Query-by-Sketch or Query-by-Image 3D scene shape retrieval methods, we built a much larger extended benchmark for each type of retrieval which has thirty (30) classes and organized two extended tracks. Again, two and three different 3D scene shape retrieval methods have contended in these two tracks, separately. To solicit state-of-the-art approaches, we perform a comprehensive comparison of all the above methods and an additional new retrieval methods by evaluating them on the two benchmarks. The benchmarks, evaluation results and tools are publicly available at our track websites (Yuan et al., 2019 [1]; Abdul-Rashid et al., 2019 [2]; Yuan et al., 2019 [3]; Abdul-Rashid et al., 2019 [4]), while code for the evaluated methods are also available: http://github.com/3DSceneRetrieval .},
  archive      = {J_CVIU},
  author       = {Juefei Yuan and Hameed Abdul-Rashid and Bo Li and Yijuan Lu and Tobias Schreck and Song Bai and Xiang Bai and Ngoc-Minh Bui and Minh N. Do and Trong-Le Do and Anh-Duc Duong and Kai He and Xinwei He and Mike Holenderski and Dmitri Jarnikov and Tu-Khiem Le and Wenhui Li and Anan Liu and Xiaolong Liu and Vlado Menkovski and Khac-Tuan Nguyen and Thanh-An Nguyen and Vinh-Tiep Nguyen and Weizhi Nie and Van-Tu Ninh and Perez Rey and Yuting Su and Vinh Ton-That and Minh-Triet Tran and Tianyang Wang and Shu Xiang and Shandian Zhe and Heyu Zhou and Yang Zhou and Zhichao Zhou},
  doi          = {10.1016/j.cviu.2020.103070},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103070},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A comparison of methods for 3D scene shape retrieval},
  volume       = {201},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The synergy of double attention: Combine sentence-level and
word-level attention for image captioning. <em>CVIU</em>, <em>201</em>,
103068. (<a href="https://doi.org/10.1016/j.cviu.2020.103068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The existing attention models of image captioning typically extract only word-level attention information, i.e., the attention mechanism extracts local attention information from the image to generate the current word, and lacks accurate image global information guidance. In this paper, we first propose an image captioning approach based on self-attention. Sentence-level attention information is extracted from the image through self-attention mechanism to represent the global image information needed to generate sentences. Furthermore, we propose a double attention model which combines sentence-level attention model with word-level attention model to generate more accurate captions. We implement supervision and optimization in the intermediate stage of the model to solve information interference problems. In addition, we perform two-stage training with reinforcement learning to optimize the evaluation metric of the model. Finally, we evaluated our model on three standard datasets, i.e., Flickr8k, Flickr30k and MSCOCO. Experimental results show that our double attention model can generate more accurate and richer captions, and outperforms many state-of-the-art image captioning approaches in various evaluation metrics.},
  archive      = {J_CVIU},
  author       = {Haiyang Wei and Zhixin Li and Canlong Zhang and Huifang Ma},
  doi          = {10.1016/j.cviu.2020.103068},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103068},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {The synergy of double attention: Combine sentence-level and word-level attention for image captioning},
  volume       = {201},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MTRNet++: One-stage mask-based scene text eraser.
<em>CVIU</em>, <em>201</em>, 103066. (<a
href="https://doi.org/10.1016/j.cviu.2020.103066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A precise, controllable, interpretable and easily trainable text removal approach is necessary for both user-specific and large-scale text removal applications. To achieve this, we propose a one-stage mask-based text inpainting network, MTRNet++. It has a novel architecture that includes mask-refine, coarse-inpainting and fine-inpainting branches, and attention blocks. With this architecture, MTRNet++ can remove text either with or without an external mask. It achieves state-of-the-art results on both the Oxford and SCUT datasets without using external ground-truth masks. The results of ablation studies demonstrate that the proposed multi-branch architecture with attention blocks is effective and essential. It also demonstrates controllability and interpretability .},
  archive      = {J_CVIU},
  author       = {Osman Tursun and Simon Denman and Rui Zeng and Sabesan Sivapalan and Sridha Sridharan and Clinton Fookes},
  doi          = {10.1016/j.cviu.2020.103066},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103066},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {MTRNet++: One-stage mask-based scene text eraser},
  volume       = {201},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Effective crowd counting using multi-resolution context and
image quality assessment-guided training. <em>CVIU</em>, <em>201</em>,
103065. (<a href="https://doi.org/10.1016/j.cviu.2020.103065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd counting is the challenging task in the crowd scene analysis . To tackle the scale variant issue and to calculate the more accurate result in this target task, this paper designs an effective crowd counting method based on multi-resolution context and image quality assessment-guided training. Specially, a multi-resolution context module is designed to extract the multi-scale context adaptively to enhance the final counting performance through learning the imbalance between different scale paths. An image quality assessment-guided training approach is developed to facilitate the crowd counting network to generate high-quality density map and more accurate counting result. Extensive experiments on benchmarks demonstrate the effectiveness of the proposed method on crowd counting task, the generalization of the proposed method, and the generalization of the developed image quality assessment-guided training approach.},
  archive      = {J_CVIU},
  author       = {He Li and Weihang Kong and Shihui Zhang},
  doi          = {10.1016/j.cviu.2020.103065},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103065},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Effective crowd counting using multi-resolution context and image quality assessment-guided training},
  volume       = {201},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PS-DeVCEM: Pathology-sensitive deep learning model for video
capsule endoscopy based on weakly labeled data. <em>CVIU</em>,
<em>201</em>, 103062. (<a
href="https://doi.org/10.1016/j.cviu.2020.103062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel pathology-sensitive deep learning model (PS-DeVCEM) for frame-level anomaly detection and multi-label classification of different colon diseases in video capsule endoscopy (VCE) data. Our proposed model is capable of coping with the key challenge of colon apparent heterogeneity caused by several types of diseases. Our model is driven by attention-based deep multiple instance learning and is trained end-to-end on weakly labeled data using video labels instead of detailed frame-by-frame annotation. This makes it a cost-effective approach for the analysis of large capsule video endoscopy repositories. Other advantages of our proposed model include its capability to localize gastrointestinal anomalies in the temporal domain within the video frames, and its generality, in the sense that abnormal frame detection is based on automatically derived image features . The spatial and temporal features are obtained through ResNet50 and residual Long short-term memory (residual LSTM) blocks, respectively. Additionally, the learned temporal attention module provides the importance of each frame to the final label prediction. Moreover, we developed a self-supervision method to maximize the distance between classes of pathologies. We demonstrate through qualitative and quantitative experiments that our proposed weakly supervised learning model gives a superior precision and F1-score reaching, 61.6\% and 55.1\%, as compared to three state-of-the-art video analysis methods respectively. We also show our model’s ability to temporally localize frames with pathologies, without frame annotation information during training. Furthermore, we collected and annotated the first and largest VCE dataset with only video labels. The dataset contains 455 short video segments with 28,304 frames and 14 classes of colorectal diseases and artifacts. Dataset and code supporting this publication will be made available on our home page.},
  archive      = {J_CVIU},
  author       = {Ahmed Mohammed and Ivar Farup and Marius Pedersen and Sule Yildirim and Øistein Hovde},
  doi          = {10.1016/j.cviu.2020.103062},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103062},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {PS-DeVCEM: Pathology-sensitive deep learning model for video capsule endoscopy based on weakly labeled data},
  volume       = {201},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning instance-aware object detection using determinantal
point processes. <em>CVIU</em>, <em>201</em>, 103061. (<a
href="https://doi.org/10.1016/j.cviu.2020.103061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent object detectors localize instances and classify candidate regions simultaneously. The number of candidate regions is typically larger than the number of objects and each region is evaluated independently. To assign a single detection bounding box for each object, heuristic algorithms , such as non-maximum suppression (NMS), have been used widely. While simple heuristic algorithms are effective for stand-alone objects, they often fail to detect overlapped objects. In this paper, we address this issue by training a network to distinguish different objects using the relationship between candidate boxes. We propose an instance-aware detection network (IDNet), which can learn to extract features from candidate regions and measure their similarities. Based on pairwise similarities and detection qualities, the IDNet selects a subset of candidate bounding boxes using instance-aware determinantal point process inference (IDPP). Extensive experiments demonstrate that the proposed algorithm achieves significant improvements for detecting overlapped objects compared to existing state-of-the-art detection methods on CrowdHuman, Pascal VOC, and MS COCO datasets.},
  archive      = {J_CVIU},
  author       = {Nuri Kim and Donghoon Lee and Songhwai Oh},
  doi          = {10.1016/j.cviu.2020.103061},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103061},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Learning instance-aware object detection using determinantal point processes},
  volume       = {201},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep online classification using pseudo-generative models.
<em>CVIU</em>, <em>201</em>, 103048. (<a
href="https://doi.org/10.1016/j.cviu.2020.103048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we propose a new deep learning based approach for online classification on streams of high-dimensional data. While requiring very little historical data storage, our approach is able to alleviate catastrophic forgetting in the scenario of continual learning with no assumption on the stationarity of the data in the stream. To make up for the absence of historical data, we propose a new generative autoencoder endowed with an auxiliary loss function that ensures fast task-sensitive convergence. To evaluate our approach we perform experiments on two well-known image datasets, MNIST and LSUN, in a continuous streaming mode. We extend the experiments to a large multi-class synthetic dataset that allows to check the performance of our method in more challenging settings with up to 1000 distinct classes. Our approach is able to perform classification on dynamic data streams with an accuracy close to the results obtained in the offline classification setup where all the data are available for the full duration of training. In addition, we demonstrate the ability of our method to adapt to unseen data classes and new instances of already known data categories, while avoiding catastrophic forgetting of previously acquired knowledge.},
  archive      = {J_CVIU},
  author       = {Andrey Besedin and Pierre Blanchart and Michel Crucianu and Marin Ferecatu},
  doi          = {10.1016/j.cviu.2020.103048},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103048},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Deep online classification using pseudo-generative models},
  volume       = {201},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). NCMS: Towards accurate anchor free object detection through
ℓ2 norm calibration and multi-feature selection. <em>CVIU</em>,
<em>200</em>, 103050. (<a
href="https://doi.org/10.1016/j.cviu.2020.103050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present simple and flexible drop-in modules in feature pyramids for general object detection, which can be easily generalized to other anchor-free detectors without introducing extra parameters, and only involves negligible computational cost on training and testing. The proposed detector, called NCMS, inserts a simple norm calibration (NC) operation between the feature pyramids and detection head to alleviate and balance the norm bias caused by feature pyramid network (FPN). Furthermore, the NCMS leverages an enhanced multi-feature selective strategy (MS) during training to assign the ground-truth to particular feature pyramid levels as supervisions, in order to obtain more discriminative representation for objects. By generalizing to the state-of-the-art FSAF module (Zhu et al., 2019), our NCMS improves it by 1.6\% on COCO val set without bells and whistles. The resulting best model achieves 44.0\% mAP with single-model and single-scale testing, which is a fairly competitive result.},
  archive      = {J_CVIU},
  author       = {Fangyi Chen and Chenchen Zhu and Zhiqiang Shen and Han Zhang and Marios Savvides},
  doi          = {10.1016/j.cviu.2020.103050},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103050},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {NCMS: Towards accurate anchor free object detection through ℓ2 norm calibration and multi-feature selection},
  volume       = {200},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cross-spectral stereo matching for facial disparity
estimation in the dark. <em>CVIU</em>, <em>200</em>, 103046. (<a
href="https://doi.org/10.1016/j.cviu.2020.103046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous applications on human faces hinge on depth information. Often, facial stereo matching provides an opportunity to estimate disparity without active projectors. However, existing algorithms are less effective at night due to unclear texture and severe noises in RGB images . In this paper, we address this problem by estimating facial disparity maps from NIR-RGB pairs. We develop a neural network composed of a multi-spectral transfer network (MSTN) and a disparity estimation network (DEN). MSTN is used to produce a pseudo-NIR image aligned with the RGB view using a spatially weighted sum on the NIR one by a kernel prediction network (KPN). As the pseudo-NIR and the NIR images share the same appearance, the facial disparity map is predicted by the proposed DEN with the same-spectral stereo pair . The whole network can be trained in an end-to-end manner and the experimental results demonstrate that it performs favorably against state-of-the-art algorithms on both synthetic and real data.},
  archive      = {J_CVIU},
  author       = {Songnan Lin and Jiawei Zhang and Jing Chen and Yongtian Wang and Yicun Liu and Jimmy Ren},
  doi          = {10.1016/j.cviu.2020.103046},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103046},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Cross-spectral stereo matching for facial disparity estimation in the dark},
  volume       = {200},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Open cross-domain visual search. <em>CVIU</em>,
<em>200</em>, 103045. (<a
href="https://doi.org/10.1016/j.cviu.2020.103045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses cross-domain visual search, where visual queries retrieve category samples from a different domain. For example, we may want to sketch an airplane and retrieve photographs of airplanes. Despite considerable progress, the search occurs in a closed setting between two pre-defined domains. In this paper, we make the step towards an open setting where multiple visual domains are available. This notably translates into a search between any pair of domains, from a combination of domains or within multiple domains. We introduce a simple – yet effective – approach. We formulate the search as a mapping from every visual domain to a common semantic space, where categories are represented by hyperspherical prototypes. Open cross-domain visual search is then performed by searching in the common semantic space, regardless of which domains are used as source or target. Domains are combined in the common space to search from or within multiple domains simultaneously. A separate training of every domain-specific mapping function enables an efficient scaling to any number of domains without affecting the search performance. We empirically illustrate our capability to perform open cross-domain visual search in three different scenarios. Our approach is competitive with respect to existing closed settings, where we obtain state-of-the-art results on several benchmarks for three sketch-based search tasks.},
  archive      = {J_CVIU},
  author       = {William Thong and Pascal Mettes and Cees G.M. Snoek},
  doi          = {10.1016/j.cviu.2020.103045},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103045},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Open cross-domain visual search},
  volume       = {200},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning deep edge prior for image denoising. <em>CVIU</em>,
<em>200</em>, 103044. (<a
href="https://doi.org/10.1016/j.cviu.2020.103044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image restoration is an important technique to deal with the degradation of the image. This paper presents an efficient and trusty denoising scheme, which combines the convolutional neural network (CNN) technique with the traditional variational model, to offer interpretable and high quality reconstructions. In this scheme, CNN, which has proven effectiveness in feature extraction tasks, is adopted to obtain the designed edge features from the noisy images , to be the prior of the reconstruction through an edge regularization . In the proposed denoising model, the total variation (TV) regularization is also adopted for its superior performance in allowing the sharp edges. The solution of the proposed model is obtained by using the Bregman splitting method, with the existence and the uniqueness of the solution also analyzed in this paper. Extensive experiments show that the two regularizations combined in the proposed model are able to fix the staircasing defects effectively and retrieve the fine textures in the recovered images as well, which outperforms the state-of-the-art interpretable denoising methods. Moreover, the proposed edge regularization can be easily extended into other kinds of noise or other restoration tasks, which implies the strong adaptivity of the proposed scheme.},
  archive      = {J_CVIU},
  author       = {Yingying Fang and Tieyong Zeng},
  doi          = {10.1016/j.cviu.2020.103044},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103044},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Learning deep edge prior for image denoising},
  volume       = {200},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pointly-supervised scene parsing with uncertainty mixture.
<em>CVIU</em>, <em>200</em>, 103040. (<a
href="https://doi.org/10.1016/j.cviu.2020.103040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pointly-supervised learning is an important topic for scene parsing , as dense annotation is extremely expensive and hard to scale. The state-of-the-art method harvests pseudo labels by applying thresholds upon softmax outputs (logits). There are two issues with this practice: (1) Softmax output does not necessarily reflect the confidence of the network output. (2) There is no principled way to decide on the optimal threshold. Tuning thresholds can be time-consuming for deep neural networks . Our method, by contrast, builds upon uncertainty measures instead of logits and is free of threshold tuning. We motivate the method with a large-scale analysis of the distribution of uncertainty measures, using strong models and challenging databases. This analysis leads to the discovery of a statistical phenomenon called uncertainty mixture. Specifically speaking, for each independent category, the distribution of uncertainty measures for unlabeled points is a mixture of two components (certain v.s. uncertain samples). The phenomenon of uncertainty mixture is surprisingly ubiquitous in real-world datasets like PascalContext and ADE20k. Inspired by this discovery, we propose to decompose the distribution of uncertainty measures with a Gamma mixture model, leading to a principled method to harvest reliable pseudo labels. Beyond that, we assume the uncertainty measures for labeled points are always drawn from the certain component. This amounts to a regularized Gamma mixture model. We provide a thorough theoretical analysis of this model, showing that it can be solved with an EM-style algorithm with convergence guarantee. Our method is also empirically successful. On PascalContext and ADE20k, we achieve clear margins over the baseline, notably with no threshold tuning in the pseudo label generation procedure. On the absolute scale, since our method collaborates well with strong baselines, we reach new state-of-the-art performance on both datasets.},
  archive      = {J_CVIU},
  author       = {Hao Zhao and Ming Lu and Anbang Yao and Yiwen Guo and Yurong Chen and Li Zhang},
  doi          = {10.1016/j.cviu.2020.103040},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103040},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Pointly-supervised scene parsing with uncertainty mixture},
  volume       = {200},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). E-ProSRNet: An enhanced progressive single image
super-resolution approach. <em>CVIU</em>, <em>200</em>, 103038. (<a
href="https://doi.org/10.1016/j.cviu.2020.103038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the convolutional neural networks (CNNs) have been successfully applied to single image super-resolution (SISR) task. However, most of the CNN based SISR methods obtain better performance with a huge amount of training parameters which increases the computational complexity of their SISR models. Such SR networks suffer from a heavy burden on computational assets and as a result, they are no longer appropriate for many real-world applications. Hence, in the computer vision community, it is an interest to endorse an SR approach which makes use of less number of training parameters with better SR performance. In this paper, we propose a computationally efficient SR approach called enhanced progressive super-resolution network i.e., E-ProSRNet. This approach is the enhanced version of our base proposed model called ProSRNet. In E-ProSRNet model, we propose a novel enhanced parallel densely connected residual network (E-PDRN) which helps to extract rich features in the low-resolution (LR) observation. The SR performance of proposed E-ProSRNet model is better than that of ProSRNet and it uses a less number of training parameters when compared to that of ProSRNet model. The experimental analysis on common testing benchmark datasets shows that the proposed E-ProSRNet sets new state-of-the-art performance on SISR task for upscaling factor × 4 ×4 . The E-ProSRNet method obtains better SR performance when compared to that obtained using proposed ProSRNet as well as the other state-of-the-art methods with significant reduction in the computational complexity.},
  archive      = {J_CVIU},
  author       = {Vishal Chudasama and Kishor Upla},
  doi          = {10.1016/j.cviu.2020.103038},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103038},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {E-ProSRNet: An enhanced progressive single image super-resolution approach},
  volume       = {200},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fine-grained facial landmark detection exploiting
intermediate feature representations. <em>CVIU</em>, <em>200</em>,
103036. (<a href="https://doi.org/10.1016/j.cviu.2020.103036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial landmark detection has been an active research subject over the last decade. In this paper, we present a new approach for Fine-grained Facial Landmark Detection (FFLD) improving on the precision of the detected points. A high spatial precision of facial landmarks is crucial for many applications related to aesthetic rendering , such as face modeling , face animation, virtual make-up, etc. In this paper, we present an approach that improves the detection precision. Since most facial landmarks are positioned on visible boundary lines, we train a model that encourages the detected landmarks to stay on these boundaries. Our proposed Convolutional Neural Networks (CNN) effectively exploits lower-level feature maps containing abundant boundary information. To this end, beside the main CNN predicting facial landmark positions, we use several additional components, called CropNets. CropNet receives patches cropped from feature maps at different stages of this CNN, and estimate fine corrections of its predicted positions. We also introduce a novel robust spatial loss function based on pixel-wise differences between patches cropped from predicted and ground-truth positions. To further improve the landmark localization , our framework uses several loss functions optimizing the precision at several stages in different ways. Extensive experiments show that our framework significantly increases the local precision of state-of-the-art deep coordinate regression models.},
  archive      = {J_CVIU},
  author       = {Yongzhe Yan and Stefan Duffner and Priyanka Phutane and Anthony Berthelier and Xavier Naturel and Christophe Blanc and Christophe Garcia and Thierry Chateau},
  doi          = {10.1016/j.cviu.2020.103036},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103036},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Fine-grained facial landmark detection exploiting intermediate feature representations},
  volume       = {200},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Texture collinearity foreground segmentation for night
videos. <em>CVIU</em>, <em>200</em>, 103032. (<a
href="https://doi.org/10.1016/j.cviu.2020.103032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the most difficult scenarios for unsupervised segmentation of moving objects is found in nighttime videos where the main challenges are the poor illumination conditions resulting in low-visibility of objects, very strong lights, surface-reflected light, a great variance of light intensity, sudden illumination changes, hard shadows, camouflaged objects, and noise. This paper proposes a novel method, coined COLBMOG (COLlinearity Boosted MOG), devised specifically for the foreground segmentation in nighttime videos, that shows the ability to overcome some of the limitations of state-of-the-art methods and still perform well in daytime scenarios. It is a texture-based classification method, using local texture modeling, complemented by a color-based classification method. The local texture at the pixel neighborhood is modeled as an N N -dimensional vector. For a given pixel, the classification is based on the collinearity between this feature in the input frame and the reference background frame. For this purpose, a multimodal temporal model of the collinearity between texture vectors of background pixels is maintained. COLBMOG was objectively evaluated using the ChangeDetection.net (CDnet) 2014, Night Videos category, benchmark. COLBMOG ranks first among all the unsupervised methods . A detailed analysis of the results revealed the superior performance of the proposed method compared to the best performing state-of-the-art methods in this category, particularly evident in the presence of the most complex situations where all the algorithms tend to fail.},
  archive      = {J_CVIU},
  author       = {Isabel Martins and Pedro Carvalho and Luís Corte-Real and José Luis Alba-Castro},
  doi          = {10.1016/j.cviu.2020.103032},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103032},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Texture collinearity foreground segmentation for night videos},
  volume       = {200},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two-stream deep sparse network for accurate and efficient
image restoration. <em>CVIU</em>, <em>200</em>, 103029. (<a
href="https://doi.org/10.1016/j.cviu.2020.103029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural network (CNN) has achieved great success in image restoration. However, previous methods ignore the complementarity between low-level and high-level features, thereby leading to limited image reconstruction quality. In this paper, we propose a two-stream sparse network (TSSN) to explicitly learn shallow and deep features to enforce their respective contribution to image restoration. The shallow stream learns shallow features ( e.g. , texture edges), and the deep stream learns deep features ( e.g. , salient semantics). In each stream, sparse residual block (SRB) is proposed to efficiently aggregate hierarchical features by constructing sparse connections among layers in the local block. Spatial-wise and channel-wise attention are used to fuse the shallow and deep stream which recalibrates features by weight assignment in both spatial and channel dimensions. A novel loss function called Softmax- L 1 L1 loss is proposed to increase penalties of pixels that have large L 1 L1 loss ( i.e. , hard pixels). TSSN is evaluated with three representative IR applications, i.e. , single image super-resolution, image denoising and JPEG deblocking. Extensive experiments demonstrate that TSSN outperforms most of state-of-the-art methods on benchmark datasets on both quantitative metric and visual quality.},
  archive      = {J_CVIU},
  author       = {Shuhui Wang and Ling Hu and Liang Li and Weigang Zhang and Qingming Huang},
  doi          = {10.1016/j.cviu.2020.103029},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103029},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Two-stream deep sparse network for accurate and efficient image restoration},
  volume       = {200},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Refining high-frequencies for sharper super-resolution and
deblurring. <em>CVIU</em>, <em>199</em>, 103034. (<a
href="https://doi.org/10.1016/j.cviu.2020.103034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A sub-problem of paramount importance in super-resolution is the generation of an upsampled image (or frame) that is ‘sharp’. In deblurring, the core problem itself is of removing the blur, and it is equivalent to the problem of generating a ‘sharper’ version of the given image. This sharpness in the generated image comes by accurately predicting the high-frequency details (commonly referred to as fine-details) such as object edges. Thus high-frequency prediction is a vital sub-problem in super-resolution and a core problem in deblurring. To generate a sharp upsampled or deblurred image, this paper proposes a multi-stage neural network architecture ‘HFR-Net’ that works on the principle of ‘explicit refinement and fusion of high-frequency details’. To implement this principle, HFR-Net is trained with a novel 2-phase progressive–retrogressive training method. In addition to the training method, this paper also introduces dual motion warping with attention . It is a technique that is specifically designed to handle videos that have different rates of motion. Results obtained from extensive experiments on multiple super-resolution and deblurring datasets reveal that the proposed approach gives better results than the current state-of-the-art techniques.},
  archive      = {J_CVIU},
  author       = {Vikram Singh and Keerthan Ramnath and Anurag Mittal},
  doi          = {10.1016/j.cviu.2020.103034},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103034},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Refining high-frequencies for sharper super-resolution and deblurring},
  volume       = {199},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Task differentiation: Constructing robust branches for
precise object detection. <em>CVIU</em>, <em>199</em>, 103030. (<a
href="https://doi.org/10.1016/j.cviu.2020.103030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most prevailing object detection methods share networks and features between localization and classification components, which easily leads to sub-optimal learning for the two separate tasks. In this paper, we propose a conception of task differentiation and design specialized sub-networks for both localization and classification components based on SSD framework. A novel probability based localization method is introduced into the one-stage framework and combined with bounding box regression for precise object localization. Furthermore, a new feature fusion strategy, together with a global attention mechanism , is proposed to learn more robust features. Experimental results on PASCAL VOC and MS COCO data sets indicate that our method has impressive performance compared with other state-of-the-art object detection approaches.},
  archive      = {J_CVIU},
  author       = {Bisheng Wang and Guo Cao and Licun Zhou and Youqiang Zhang and Yanfeng Shang},
  doi          = {10.1016/j.cviu.2020.103030},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103030},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Task differentiation: Constructing robust branches for precise object detection},
  volume       = {199},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Low-level multiscale image segmentation and a benchmark for
its evaluation. <em>CVIU</em>, <em>199</em>, 103026. (<a
href="https://doi.org/10.1016/j.cviu.2020.103026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a segmentation algorithm to detect low-level structure present in images. The algorithm is designed to partition a given image into regions, corresponding to image structures, regardless of their shapes, sizes, and levels of interior homogeneity. We model a region as a connected set of pixels that is surrounded by ramp edge discontinuities where the magnitude of these discontinuities is large compared to the variation inside the region. Each region is associated with a scale that depends upon the fraction of the strong and weak parts of its boundary. Traversing through the range of all possible scales, we obtain all regions present in the image. Regions strictly merge as the scale increases; hence a tree is formed where the root node corresponds to the whole image, nodes closer to the root along a path correspond to larger regions and those further from the root capture smaller regions representing embedded details. To evaluate the accuracy and precision of our algorithm, as well as to compare it to the existing algorithms, we present a new benchmark dataset for low-level image segmentation . We provide evaluation methods for both boundary-based and region-based performance of algorithms. We also annotate different parts of the images that are difficult to segment with the types of segmentation challenges they pose. This enables our benchmark to give an account of which algorithm fails where. We show that our proposed algorithm performs better than the widely used low-level segmentation algorithms on this benchmark.},
  archive      = {J_CVIU},
  author       = {Emre Akbas and Narendra Ahuja},
  doi          = {10.1016/j.cviu.2020.103026},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103026},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Low-level multiscale image segmentation and a benchmark for its evaluation},
  volume       = {199},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Representation learning of image composition for aesthetic
prediction. <em>CVIU</em>, <em>199</em>, 103024. (<a
href="https://doi.org/10.1016/j.cviu.2020.103024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Photo quality assessment (PQA) aims at computationally and precisely evaluating the quality of images from the aspect of aesthetic. Image aesthetic is strongly correlated with composition. However, few existing works have taken composition into consideration. Besides, existing composition features are typically hand-crafted. In this paper, we propose a novel end-to-end framework for representation learning of image composition. Specially, we build a fully connected graph based on deep features in Convolutional Neural Networks (CNNs). In the graph, edge attributes i.e. similarities between deep features at different positions are used for representing image composition. Besides, we use global attributes of the graph to represent miscellaneous aesthetic aspects. Finally, we use a gate unit to combine both composition features and miscellaneous aesthetic features for aesthetic prediction. The whole network can be trained in an end-to-end manner. Experimental results show that the proposed techniques significantly improves the prediction precision of aesthetic and composition over various datasets. We have released our codes at: .},
  archive      = {J_CVIU},
  author       = {Lin Zhao and Meimei Shang and Fei Gao and Rongsheng Li and Fei Huang and Jun Yu},
  doi          = {10.1016/j.cviu.2020.103024},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103024},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Representation learning of image composition for aesthetic prediction},
  volume       = {199},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). End-to-end deep learning-based fringe projection framework
for 3D profiling of objects. <em>CVIU</em>, <em>199</em>, 103023. (<a
href="https://doi.org/10.1016/j.cviu.2020.103023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fringe projection techniques are widely used for precise three-dimensional depth profiling of objects. Existing signal processing-based fringe projection techniques measure the phase deformation of the projected fringe patterns with a sequence of operations, such as fringe denoising , fringe analysis for wrapped phase extraction, followed by phase unwrapping. However, the error induced in any of the stages leads to erroneous depth estimation. Furthermore, any aliasing in frequency domain fringe analysis and ill-posed nature of phase unwrapping limit the overall accuracy of the Fringe Projection Profilometry (FPP). To this end, unlike the traditional approaches, we propose a paradigm shift by introducing a novel end-to-end deep learning-based framework for FPP that does not need any frequency domain filtering and phase unwrapping. The proposed framework directly reconstructs the object’s depth profile from the deformed fringe itself through a multi-resolution similarity assessment convolutional neural network . We compare the performance of the proposed framework with two widely used conventional approaches. The evaluations are performed for various challenging and important scenarios such as low fringe-frequency, depth profiles with high dynamic range and noisy fringes. The results demonstrate that the proposed framework achieves promising results in all these scenarios.},
  archive      = {J_CVIU},
  author       = {Rakesh Chowdary Machineni and G.E. Spoorthi and Krishna Sumanth Vengala and Subrahmanyam Gorthi and Rama Krishna Sai S. Gorthi},
  doi          = {10.1016/j.cviu.2020.103023},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103023},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {End-to-end deep learning-based fringe projection framework for 3D profiling of objects},
  volume       = {199},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic mode decomposition via dictionary learning for
foreground modeling in videos. <em>CVIU</em>, <em>199</em>, 103022. (<a
href="https://doi.org/10.1016/j.cviu.2020.103022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate extraction of foregrounds in videos is one of the challenging problems in computer vision . In this study, we propose dynamic mode decomposition via dictionary learning (dl-DMD), which is applied to extract moving objects by separating the sequence of video frames into foreground and background information with a dictionary learned using block patches on the video frames. Dynamic mode decomposition (DMD) decomposes spatiotemporal data into spatial modes, each of whose temporal behavior is characterized by a single frequency and growth/decay rate and is applicable to split a video into foregrounds and the background when applying it to a video. And, in dl-DMD, DMD is applied on coefficient matrices estimated over a learned dictionary, which enables accurate estimation of dynamical information in videos. Due to this scheme, dl-DMD can analyze the dynamics of respective regions in a video based on estimated amplitudes and temporal evolution over patches. The results on synthetic data exhibit that dl-DMD outperforms the standard DMD and compressed DMD (cDMD) based methods. Also, the results of an empirical performance evaluation in the case of foreground extraction from videos using publicly available dataset demonstrates the effectiveness of the proposed dl-DMD algorithm and achieves a performance that is comparable to that of the state-of-the-art techniques in foreground extraction tasks.},
  archive      = {J_CVIU},
  author       = {Israr Ul Haq and Keisuke Fujii and Yoshinobu Kawahara},
  doi          = {10.1016/j.cviu.2020.103022},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103022},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Dynamic mode decomposition via dictionary learning for foreground modeling in videos},
  volume       = {199},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Partial domain adaptation based on shared class oriented
adversarial network. <em>CVIU</em>, <em>199</em>, 103018. (<a
href="https://doi.org/10.1016/j.cviu.2020.103018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing domain adaptation methods assume that the label space of the source domain is the same as the label space of the target domain. However, this assumption is generally untenable due to the differences between the two domains. Therefore, a novel domain adaptation paradigm called Partial Domain Adaptation (PDA), which only assumes that the source label space is large enough to subsume the target label space has been proposed recently to relax such strict assumption. Previous partial domain adaptation methods mainly utilize weighting mechanisms to alleviate negative transfer caused by outlier classes samples. Though these methods have achieved high performance in PDA tasks, all the heterogeneous data is retained during the whole training process, which still contributes to negative transfer. In this work, we propose a shared class oriented adversarial network (SCOAN) for partial domain adaptation. Outlier samples are excluded from training process via weighting strategy to entirely circumvent negative transfer and positive transfer is performed by combining adversarial network and Maximum Mean Discrepancy (MMD) to bridge domain gap. Multi-classifier module is proposed to further improve the generalization ability of the network. Extensive experiments show that SCOAN achieves state-of-the-art results on several benchmark partial domain adaptation datasets.},
  archive      = {J_CVIU},
  author       = {Wenjie Qiu and Wendong Chen and Haifeng Hu},
  doi          = {10.1016/j.cviu.2020.103018},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103018},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Partial domain adaptation based on shared class oriented adversarial network},
  volume       = {199},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Residual network with detail perception loss for single
image super-resolution. <em>CVIU</em>, <em>199</em>, 103007. (<a
href="https://doi.org/10.1016/j.cviu.2020.103007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep convolutional neural networks have demonstrated high-quality reconstruction for single image super-resolution. In this study, we present a network by using residual blocks with cascading simple blocks to improve the image resolution. Cascading simple blocks with a multi-layer perceptron are conducive to extract features and approximate a complex mapping with fewer parameters. Skip connections can help to alleviate the vanishing-gradient problem of deep networks. In addition, our network contains two pathways. One is to predict the high frequency information of the high resolution image and the other is to predict the low frequency information of the high resolution image. Then the information of two pathways is fused, and pixel-shuffle is used for upsampling. Moreover, to capture texture details of images, we introduce a novel loss function called detail perception loss, which is used to measure the difference of the wavelet coefficients from the reconstructed image and ground truth. By reducing detail perception loss, texture details of the reconstructed image are becoming more similar with texture details of ground truth. Extensive quantitative and qualitative experiments on four benchmark datasets show that our method achieves superior performance over typical single image super-resolution methods.},
  archive      = {J_CVIU},
  author       = {Zhijie Wen and Jiawei Guan and Tieyong Zeng and Ying Li},
  doi          = {10.1016/j.cviu.2020.103007},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103007},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Residual network with detail perception loss for single image super-resolution},
  volume       = {199},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Infrared and visible image fusion via gradientlet filter.
<em>CVIU</em>, <em>197-198</em>, 103016. (<a
href="https://doi.org/10.1016/j.cviu.2020.103016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an image filter based on fuzzy gradient threshold function and global optimization, termed as gradientlet filter, from the perspective of luminance and gradient separation. It can remove small gradient textures and noise while maintaining the overall brightness and edge gradients of an image. Based on gradientlet filter and image saliency, we further put forward a new method for infrared and visible image fusion, which can overcome the challenges of low contrast, edge blurring and noise existing in traditional fused images. First, the gradientlet filter is used to decompose source images into approximate layers and residual layers, where the former reflects the overall brightness of source images without edge blurring and noise, and the latter reflects the small gradient texture and noise of source images. Second, according to the characteristics of the approximate and residual layers, we propose contrast and gradient saliency maps and construct corresponding weight matrices. Finally, the fused image is obtained by fusion and reconstruction based on previously obtained sub-images and weight matrices. Extensive experiments on publicly available databases demonstrate the advantages of our method over state-of-the-art methods in terms of maintaining image contrast, improving target saliency, preventing edge blurring, and reducing noise.},
  archive      = {J_CVIU},
  author       = {Jiayi Ma and Yi Zhou},
  doi          = {10.1016/j.cviu.2020.103016},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103016},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Infrared and visible image fusion via gradientlet filter},
  volume       = {197-198},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). JSNet: A simulation network of JPEG lossy compression and
restoration for robust image watermarking against JPEG attack.
<em>CVIU</em>, <em>197-198</em>, 103015. (<a
href="https://doi.org/10.1016/j.cviu.2020.103015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based watermarking methods have achieved a better performance in capacity and invisibility than some traditional methods. However, their robustness against JPEG lossy compression attack is still to be improved. To enhance the robustness and construct an end-to-end method, it is urgent to simulate the JPEG lossy compression by a neural network and then introduce it into the deep learning-based watermarking methods. In this paper, a JPEG simulation network JSNet is proposed to reappear the whole procedure of the JPEG lossy compression and restoration except entropy encoding as realistically as possible. The steps of sampling, DCT , and quantization are modeled by the max-pooling layer, convolution layer , and 3D noise-mask, respectively. The proposed JSNet can simulate JPEG lossy compression with any quality factors . To verify the proposed JSNet in improving the robustness against JPEG compression attack, a CNN-based robust watermarking network (CRWNet) is proposed as an application example. The end-to-end CRWNet contains three subnetworks , i.e., embedding subnetwork, JSNet, and extraction subnetwork. Here. the JSNet is regarded as an attack module in the pipeline. Experimental results on two publicly available datasets (ImageNet, BossBase) demonstrate that: (a) the proposed JSNet can well simulate JPEG lossy compression under any quality factors with small Root mean square error (RMSE) values; (b) the proposed CRWNet considering JSNet has achieved an average 30.6 percent advantage over the basic model without consideration of JSNet.},
  archive      = {J_CVIU},
  author       = {Beijing Chen and Yunqing Wu and Gouenou Coatrieux and Xiao Chen and Yuhui Zheng},
  doi          = {10.1016/j.cviu.2020.103015},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103015},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {JSNet: A simulation network of JPEG lossy compression and restoration for robust image watermarking against JPEG attack},
  volume       = {197-198},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-speed multi-person pose estimation with deep feature
transfer. <em>CVIU</em>, <em>197-198</em>, 103010. (<a
href="https://doi.org/10.1016/j.cviu.2020.103010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in deep learning have significantly improved the accuracy of multi-person pose estimation from RGB images. However, these deep learning methods typically rely on a large number of deep refinement modules to refine the features of body joints and limbs, which hugely reduce the run-time speed and therefore limit the application domain. In this paper, we propose a feature transfer framework to capture the concurrent correlations between body joint and limb features. The concurrent correlations of these features form a complementary structural relationship, which mutually strengthens the network’s inferences and reduces the needs of refinement modules. The transfer sub-network is implemented with multiple convolutional layers, and is merged with the body part detection network to form an end-to-end system. The transfer relationship is automatically learned from ground-truth data instead of being manually encoded, resulting in a more general and efficient design. The proposed framework is validated on the multiple popular multi-person pose estimation benchmarks - MPII, COCO 2018 and PoseTrack 2017 and 2018. Experimental results show that our method not only significantly increases the inference speed to 73.8 frame per second (FPS), but also attains comparable state-of-the-art performance.},
  archive      = {J_CVIU},
  author       = {Ying Huang and Hubert P.H. Shum and Edmond S.L. Ho and Nauman Aslam},
  doi          = {10.1016/j.cviu.2020.103010},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103010},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {High-speed multi-person pose estimation with deep feature transfer},
  volume       = {197-198},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scalable learning for bridging the species gap in
image-based plant phenotyping. <em>CVIU</em>, <em>197-198</em>, 103009.
(<a href="https://doi.org/10.1016/j.cviu.2020.103009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional paradigm of applying deep learning – collect, annotate and train on data – is not applicable to image-based plant phenotyping. Data collection involves the growth of many physical samples, imaging them at multiple growth stages and finally manually annotating each image. This process is error-prone, expensive, time consuming and often requires specialised equipment. Almost 400,000 different plant species exist across the world. Each varying greatly in appearance, geometry and structure, a species gap exists between the domain of each plant species. The performance of a model is not generalisable and may not transfer to images of an unseen plant species. With the costs of data collection and number of plant species, it is not tractable to apply deep learning to the automation of plant phenotyping measurements. Hence, training using synthetic data is effective as the cost of data collection and annotation is free. We investigate the use of synthetic data for image-based plant phenotyping. Our conclusions and released data are applicable to the measurement of phenotypic traits including plant area, leaf count, leaf area and shape. In this paper, we validate our proposed approach on leaf instance segmentation for the measurement of leaf area. We study multiple synthetic data training regimes using Mask-RCNN when few or no annotated real data is available. We also present UPGen : a Universal Plant Generator for bridging the species gap . UPGen leverages domain randomisation to produce widely distributed data samples and models stochastic biological variation. A model trained on our synthetic dataset traverses the domain and species gaps . In validating UPGen , the relationship between different data parameters and their effects on leaf segmentation performance is investigated. Imitating a plant phenotyping facility processing a new plant species, our methods outperform standard practices, such as transfer learning from publicly available plant data, by 26.6\% and 51.46\% on two unseen plant species respectively. We benchmark UPGen by using it to compete in the CVPPP Leaf Segmentation Challenge. Generalising across multiple plant species, our method achieves state-of-the-art performance scoring a mean of 88\% across A1-4 test datasets . Our synthetic dataset and pretrained model are available at https://csiro-robotics.github.io/UPGen-Webpage/ .},
  archive      = {J_CVIU},
  author       = {Daniel Ward and Peyman Moghadam},
  doi          = {10.1016/j.cviu.2020.103009},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103009},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Scalable learning for bridging the species gap in image-based plant phenotyping},
  volume       = {197-198},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning lightweight multi-scale feedback residual network
for single image super-resolution. <em>CVIU</em>, <em>197-198</em>,
103005. (<a href="https://doi.org/10.1016/j.cviu.2020.103005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past years, convolutional neural networks (CNNs) have demonstrated great success for single image super-resolution (SISR). However, existing CNNs for SISR generally have two limitations: (1) the network depth is very deep, which not only weakens the information flow from bottom to top but also has a heavy model capacity; (2) the network architectures are often feed-forward, which prevent the previous layers capturing the useful information from the following layers, limiting the feature learning capability. To address these issues, this paper presents a lightweight Multi-scale Feedback Residual network for SISR. Specifically, we design a lightweight feedback-based recurrent neural network (FRNN) tailored to SISR. The FRNN is consists of a series of recursive Densely-Connected Blocks (DCBs) with the Low-Resolution (LR) image features and the output of the former DCB as inputs. Each DCB adaptively fuses multi-level features from the side-output intermediate feature maps to generate a powerful feature representation. Meanwhile, the DCB cascades a set of Multi-scale Residual Blocks (MRBs), each of which has an enlarged field of view to fully capture multi-scale context information. Moreover, the MRB has a novel Multi-Kernel Fusion Block (MKFB) design, which can dynamically adjust the receptive field size of the output feature representation based on the multi-scale inputs. The whole network of our MFRSR is lightweight with only ∼ 4 . 5 ∼4.5 M parameters, but achieves favorable performance on five benchmark datasets compared to the state-of-the-art methods in terms of PSNR and SSIM.},
  archive      = {J_CVIU},
  author       = {Wenjie Xu and Huihui Song and Kaihua Zhang and Qingshan Liu and Jia Liu},
  doi          = {10.1016/j.cviu.2020.103005},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103005},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Learning lightweight multi-scale feedback residual network for single image super-resolution},
  volume       = {197-198},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hyperspectral image restoration via CNN denoiser prior
regularized low-rank tensor recovery. <em>CVIU</em>, <em>197-198</em>,
103004. (<a href="https://doi.org/10.1016/j.cviu.2020.103004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral images (HSIs) are widely used in various tasks such as mineral detection and food safety. However, during the imaging process , they are often contaminated by various noises. In this paper for HSIs restoration tasks, we firstly investigate the advantages of traditional physical restoration models and the denoising convolutional neural networks (CNN). For the physical prior of HSIs, a Tucker decomposition based low-rank tensor approximation can fully explore the global correlations in both the spatial and spectral domains . And for the implicit prior, a CNN based method can represent the prior which cannot be designed by mathematical theory tools. Then, we combine the advantages of the two methods to introduce the HSI restoration CNN with the low-rank tensor approximation based regularization in the flexible and extensible plug-and-play framework. The proposed model can be quickly solved using the alternating direction method of multipliers method. Experiments with simulated data and real data show that, compared with competitive methods, the proposed method achieves better HSI restoration results in various quantitative evaluation indicators.},
  archive      = {J_CVIU},
  author       = {Haijin Zeng and Xiaozhen Xie and Haojie Cui and Yuan Zhao and Jifeng Ning},
  doi          = {10.1016/j.cviu.2020.103004},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103004},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Hyperspectral image restoration via CNN denoiser prior regularized low-rank tensor recovery},
  volume       = {197-198},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pyramid channel-based feature attention network for image
dehazing. <em>CVIU</em>, <em>197-198</em>, 103003. (<a
href="https://doi.org/10.1016/j.cviu.2020.103003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional deep learning-based image dehazing methods usually use the high-level features (which contain more semantic information) to remove haze in the input image, while ignoring the low-level features (which contain more detail information). In this paper, a Pyramid Channel-based Feature Attention Network (PCFAN) is proposed for single image dehazing, which leverages complementarity among different level features in a pyramid manner with channel attention mechanism. PCFAN consists of three modules: a three-scale feature extraction module, a pyramid channel-based feature attention module (PCFA), and an image reconstruction module. The three-scale feature extraction module simultaneously captures the low-level spatial structural features and the high-level contextual features in different scales. The PCFA module utilizes the feature pyramid and the channel attention mechanism, which effectively extracts interdependent channel maps and selectively aggregates the more important features in a pyramid manner for image dehazing. The image reconstruction module is used to reconstruct features to recover a clear image. Meanwhile, a loss function that combines a mean square error loss part and an edge loss part is employed in PCFAN, which can better preserve image details. Experimental results demonstrate that the proposed PCFAN outperforms existing state-of-the-art algorithms on standard benchmark datasets in terms of accuracy, efficiency, and visual effect. The code will be made publicly available.},
  archive      = {J_CVIU},
  author       = {Xiaoqin Zhang and Tao Wang and Jinxin Wang and Guiying Tang and Li Zhao},
  doi          = {10.1016/j.cviu.2020.103003},
  journal      = {Computer Vision and Image Understanding},
  pages        = {103003},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Pyramid channel-based feature attention network for image dehazing},
  volume       = {197-198},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A minimum barrier distance for multivariate images with
applications. <em>CVIU</em>, <em>197-198</em>, 102993. (<a
href="https://doi.org/10.1016/j.cviu.2020.102993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distance transforms and the saliency maps they induce are widely used in image processing , computer vision , and pattern recognition. The minimum barrier distance (MBD) has proved to provide accurate results in this context. Recently, Géraud et al. have presented a fast-to-compute alternative definition of this distance, called the Dahu pseudo-distance. This distance is efficient, powerful, and have many important applications. However, it is restricted to grayscale images . In this article we revisit this pseudo-distance. First, we offer an extension to multivariate image. We call this extension the vectorial Dahu pseudo-distance . We provide an efficient way to compute it. This new version is not only able to deal with color images but also multi-spectral and multi-modal ones. Besides, through our benchmarks, we demonstrate how robust and competitive the vectorial Dahu pseudo-distance is, compared to other MB-based distances. This shows that this distance is promising for salient object detection, shortest path finding, and object segmentation. Secondly, we combine the Dahu pseudo-distance with the geodesic distance to take into account spatial information from the image. This combination of distances provides efficient results in many applications such as segmentation of thin elements or path finding in images.},
  archive      = {J_CVIU},
  author       = {Minh Ôn Vũ Ngọc and Nicolas Boutry and Jonathan Fabrizio and Thierry Géraud},
  doi          = {10.1016/j.cviu.2020.102993},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102993},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A minimum barrier distance for multivariate images with applications},
  volume       = {197-198},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An attention recurrent model for human cooperation
detection. <em>CVIU</em>, <em>197-198</em>, 102991. (<a
href="https://doi.org/10.1016/j.cviu.2020.102991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User cooperative behaviour is mandatory and valuable to warranty data acquisition quality in forensic biometrics . In the present paper, we consider human cooperative behaviour in front of wearable security cameras. Moreover, we propose a human cooperation detection pipeline based on deep learning . Recently, recurrent neural networks (RNN) have shown remarkable performance on several tasks such as image captioning, video analysis, or natural language processing . Our proposal describes an RNN architecture with the aim at detecting whether a human is exhibiting an adversarial behaviour by trying to avoid the camera. This data is obtained by analysing the noise patterns of human movement. More specifically, we are not only providing an extensive analysis on the proposed pipeline considering different configurations and a wide variety of RNN types, but also an ensemble of the generated models to outperform each single model. The experiment has been carried out using videos captured from a mobile device camera (GOTCHA Dataset) and the obtained results have demonstrated the robustness of the proposed method.},
  archive      = {J_CVIU},
  author       = {David Freire-Obregón and Modesto Castrillón-Santana and Paola Barra and Carmen Bisogni and Michele Nappi},
  doi          = {10.1016/j.cviu.2020.102991},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102991},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {An attention recurrent model for human cooperation detection},
  volume       = {197-198},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint identification–verification for person
re-identification: A four stream deep learning approach with improved
quartet loss function. <em>CVIU</em>, <em>197-198</em>, 102989. (<a
href="https://doi.org/10.1016/j.cviu.2020.102989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A deep four-stream convolutional neural network (CNN) is proposed for person re-identification (re-ID) to overcome the poor generalisation of the traditional triplet loss function. Specifically, the proposed method is a four-stream network, taking four input images where two images are from the same identity and the other two are from different identities. The network uses dual identification and verification losses in a single framework to minimise the intra-class distance while maximising the inter-class distance. Extensive experiments illustrate the state-of-the-art performance of the proposed approach on seven challenging person re-ID datasets: VIPeR, CUHK03, CUHK01, PRID2011, i-LIDS, Market-1501, and DukeMTMC-reID. In addition, we build a five-stream network and a four-stream network with an alternate formulation of positive and negative pairs to further explore the performance of the proposed four-stream network. We also demonstrate promising performance when training and testing sets are from different domains, highlighting the real-world applicability of the approach.},
  archive      = {J_CVIU},
  author       = {Amena Khatun and Simon Denman and Sridha Sridharan and Clinton Fookes},
  doi          = {10.1016/j.cviu.2020.102989},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102989},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Joint identification–verification for person re-identification: A four stream deep learning approach with improved quartet loss function},
  volume       = {197-198},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adversarial examples for replay attacks against CNN-based
face recognition with anti-spoofing capability. <em>CVIU</em>,
<em>197-198</em>, 102988. (<a
href="https://doi.org/10.1016/j.cviu.2020.102988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the race of arms between attackers, trying to build more and more realistic face replay attacks, and defenders, deploying spoof detection modules with ever-increasing capabilities, CNN-based methods have shown outstanding detection performance thus raising the bar for the construction of realistic replay attacks against face-based authentication systems. Rather than trying to rebroadcast even more realistic faces, we show that attackers can successfully fool a face authentication system equipped with a deep learning spoof detection module, by exploiting the vulnerabilities of CNNs to adversarial perturbations. We first show that mounting such an attack is not a trivial task due to the unique features of spoofing detection modules. Then, we propose a method to craft adversarial images that can be successfully exploited to build an effective replay attack. Experiments conducted on the REPLAY-MOBILE database demonstrate that our attacked images achieve good performance against a face recognition system equipped with CNN-based anti-spoofing, in that they are able to pass the face detection, spoof detection and face recognition modules of the authentication chain.},
  archive      = {J_CVIU},
  author       = {Bowen Zhang and Benedetta Tondi and Mauro Barni},
  doi          = {10.1016/j.cviu.2020.102988},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102988},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Adversarial examples for replay attacks against CNN-based face recognition with anti-spoofing capability},
  volume       = {197-198},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visual BMI estimation from face images using a label
distribution based method. <em>CVIU</em>, <em>197-198</em>, 102985. (<a
href="https://doi.org/10.1016/j.cviu.2020.102985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Body mass index (BMI) analysis from face images is an interesting and challenging topic in machine learning and computer vision . Recent research shows that facial adiposity is associated with BMI prediction. In this work, we investigate the problem of visual BMI estimation from face images by a two-stage learning framework. BMI-related facial features are learned from the first stage. Then a label distribution based BMI estimator is learned by an optimization procedure that is implemented by projecting the features and assigned labels to a new domain which maximizing the correlation between them. Two label assignment strategies are analyzed for modeling the single BMI value as a discrete probability distribution over a range of BMIs. Extensive experiments are conducted on FIW-BMI, Morph II and VIP_attribute datasets. The experimental results show that the two-stage learning framework improves the performance step by step. More importantly, the proposed BMI estimator efficiently reduces the error. It outperforms regression based methods, two label distribution methods and two deep learning methods in most cases.},
  archive      = {J_CVIU},
  author       = {Min Jiang and Guodong Guo and Guowang Mu},
  doi          = {10.1016/j.cviu.2020.102985},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102985},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Visual BMI estimation from face images using a label distribution based method},
  volume       = {197-198},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-supervised on-line cumulative learning from video
streams. <em>CVIU</em>, <em>197-198</em>, 102983. (<a
href="https://doi.org/10.1016/j.cviu.2020.102983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel online self-supervised method for face identity learning from video streams. The method exploits deep face feature descriptors together with a memory based learning mechanism that takes advantage of the temporal coherence of visual data. Specifically, we introduce a discriminative descriptor matching solution based on Reverse Nearest Neighbor and a memory based cumulative learning strategy that discards redundant descriptors while time progresses. This allows building a comprehensive and cumulative representation of all the past visual information observed so far. It is shown that the proposed learning procedure is asymptotically stable and can be effectively used in relevant applications like multiple face identification and tracking from unconstrained video streams. Experimental results show that the proposed method achieves comparable results in the task of multiple face tracking and better performance in face identification with offline approaches exploiting future information.},
  archive      = {J_CVIU},
  author       = {Federico Pernici and Matteo Bruni and Alberto Del Bimbo},
  doi          = {10.1016/j.cviu.2020.102983},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102983},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Self-supervised on-line cumulative learning from video streams},
  volume       = {197-198},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Color edge preserving image colorization with a coupled
natural vectorial total variation. <em>CVIU</em>, <em>196</em>, 102981.
(<a href="https://doi.org/10.1016/j.cviu.2020.102981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel colorization model based on the natural vectorial total variation T V J TVJ is proposed. T V J TVJ supports a common edge direction for all channels, it leads to a better preservation of color edges. We give the existence of the minimizer of the proposed model and use the primal–dual algorithm to numerically solve this model. Experiment results both on structure images and texture images show that the proposed model can preserve the contours well and reduce color bleeding effects at edges compared with the known methods.},
  archive      = {J_CVIU},
  author       = {Lihua Min and Zhenhua Li and Zhengmeng Jin and Qiang Cui},
  doi          = {10.1016/j.cviu.2020.102981},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102981},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Color edge preserving image colorization with a coupled natural vectorial total variation},
  volume       = {196},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Study of naturalness in tone-mapped images. <em>CVIU</em>,
<em>196</em>, 102971. (<a
href="https://doi.org/10.1016/j.cviu.2020.102971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, images can be obtained in various ways such as capturing photos in single-exposure mode, applying Multiple Exposure Fusion algorithms to generate an image from multiple shoots of the same scene, mapping High Dynamic Range (HDR) images to Standard Dynamic Range (SDR) images, converting raw formats to displayable formats, or applying post-processing techniques to enhance image quality, aesthetic quality ,…When looking at some photos, one might have a feeling of unnaturalness. This paper deals with the problem of developing a model firstly to estimate if an image looks natural or not to humans and the second purpose is to try to understand how the unnaturalness feeling is induced by a photo: Are there specific unnaturalness clues or is unnaturalness a general feeling when looking at a photo? The study focuses on SDR images, especially on tone-mapped images. The first contribution of the paper is the setting of an experiment gathering human naturalness opinions on 1900 SDR images mainly obtained from tone mapping operators. Based on the collected data, the second contribution is to study the efficiency of different feature types including handcrafted features and learned features for image naturalness analysis. A binary classification model is then developed based on the determined features to classify if an image looks natural or unnatural.},
  archive      = {J_CVIU},
  author       = {Quyet-Tien Le and Patricia Ladret and Huu-Tuan Nguyen and Alice Caplier},
  doi          = {10.1016/j.cviu.2020.102971},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102971},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Study of naturalness in tone-mapped images},
  volume       = {196},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Classifier-agnostic saliency map extraction. <em>CVIU</em>,
<em>196</em>, 102969. (<a
href="https://doi.org/10.1016/j.cviu.2020.102969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently available methods for extracting saliency maps identify parts of the input which are the most important to a specific fixed classifier. We show that this strong dependence on a given classifier hinders their performance. To address this problem, we propose classifier-agnostic saliency map extraction, which finds all parts of the image that any classifier could use, not just one given in advance. We observe that the proposed approach extracts higher quality saliency maps than prior work while being conceptually simple and easy to implement. The method sets the new state of the art result for localization task on the ImageNet data, outperforming all existing weakly-supervised localization techniques, despite not using the ground truth labels at the inference time. The code reproducing the results is available at https://github.com/kondiz/casme .},
  archive      = {J_CVIU},
  author       = {Konrad Zolna and Krzysztof J. Geras and Kyunghyun Cho},
  doi          = {10.1016/j.cviu.2020.102969},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102969},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Classifier-agnostic saliency map extraction},
  volume       = {196},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Intelligent video analysis: A pedestrian trajectory
extraction method for the whole indoor space without blind areas.
<em>CVIU</em>, <em>196</em>, 102968. (<a
href="https://doi.org/10.1016/j.cviu.2020.102968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian trajectory extraction is an important part of intelligent monitoring, which is of great significance to many fields such as statistics on pedestrian flow and density, population behavior analysis, abnormal behavior detection, etc. However, it is quite challenging to extract pedestrian trajectory without blind areas in the whole space due to the limited view angle of ordinary cameras. So far, no efficient method has been proposed to deal with this problem. In this paper, we propose a pedestrian trajectory extraction method based on a single fisheye camera, which can realize no blind areas pedestrian trajectory extraction in the whole interior space. First, the fisheye camera with a perspective of 18 0 ∘ 180∘ is adopted in our work which can realize the entire space monitoring without blind areas and avoid object matching among multiple cameras. Then, the deep convolutional neural network , the Kalman Filter algorithm, and the Hungarian algorithm are combined for pedestrian head detection and tracking. In order to calculate the coordinates of the trajectory points according to the obtained head position, we propose a novel pedestrian height estimation method for fisheye cameras. Finally, the pedestrian trajectory points are calculated based on the detected head position and the estimated height. The performance of the proposed pedestrian trajectory extraction method has been evaluated by a variety of experiments. The experimental results show that the trajectories of multiple pedestrians can be extracted simultaneously through the method proposed in this paper, and the average error of the trajectory points is less than 5.07 pixels in the 512 × × 512 images.},
  archive      = {J_CVIU},
  author       = {Lie Yang and Guanghua Hu and Yonghao Song and Guofeng Li and Longhan Xie},
  doi          = {10.1016/j.cviu.2020.102968},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102968},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Intelligent video analysis: A pedestrian trajectory extraction method for the whole indoor space without blind areas},
  volume       = {196},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Product image recognition with guidance learning and noisy
supervision. <em>CVIU</em>, <em>196</em>, 102963. (<a
href="https://doi.org/10.1016/j.cviu.2020.102963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers to recognize products from daily photos, which is an important problem in real-world applications but also challenging due to background clutters, category diversities, noisy labels, etc. We address this problem by two contributions. First, we introduce a novel large-scale product image dataset, termed as Product-90. Instead of collecting product images by laborious and time-intensive image capturing, we take advantage of the web and download images from the reviews of several e-commerce websites where the images are casually captured by consumers. Labels are assigned automatically by the categories of e-commerce websites. Totally the Product-90 consists of more than 140K images with 90 categories. Due to the fact that consumers may upload unrelated images, it is inevitable that our Product-90 introduces noisy labels. As the second contribution, we develop a simple yet efficient guidance learning (GL) method for training convolutional neural networks (CNNs) with noisy supervision. The GL method first trains an initial teacher network with the full noisy dataset, and then trains a target/student network with both large-scale noisy set and small manually-verified clean set in a multi-task manner. Specifically, in the stage of student network training, the large-scale noisy data is supervised by its guidance knowledge which is the combination of its given noisy label and the soften label from the teacher network. We conduct extensive experiments on our Products-90 and four public datasets, namely Food101, Food-101N, Clothing1M and synthetic noisy CIFAR-10. Our guidance learning method achieves performance superior to state-of-the-art methods on these datasets.},
  archive      = {J_CVIU},
  author       = {Qing Li and Xiaojiang Peng and Liangliang Cao and Wenbin Du and Hao Xing and Yu Qiao and Qiang Peng},
  doi          = {10.1016/j.cviu.2020.102963},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102963},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Product image recognition with guidance learning and noisy supervision},
  volume       = {196},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Age estimation from faces using deep learning: A comparative
analysis. <em>CVIU</em>, <em>196</em>, 102961. (<a
href="https://doi.org/10.1016/j.cviu.2020.102961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic Age Estimation (AAE) has attracted attention due to the wide variety of possible applications. However, it is a challenging task because of the large variation of facial appearance and several other extrinsic and intrinsic factors. Most of the proposed approaches in the literature use hand-crafted features to encode ageing patterns. Deeply learned features extracted by Convolutional Neural Networks (CNNs) algorithms usually perform better than hand-crafted features. The main contribution of this paper is an extensive comparative analysis of several frameworks for real AAE based on deep learning architectures. Different well-known CNN architectures are considered and their performances are compared. MORPH, FG-NET, FACES, PubFig and CASIA-web Face datasets are used in our experiments. The robustness of the best deep estimator is evaluated under noise, expression changes, “crossing” ethnicity and “crossing” gender. The experimental results demonstrate the high performances of the popular CNNs frameworks against the state-of-art methods of automatic age estimation. A Layer-wise transfer learning evaluation is done to study the optimal number of layers to fine-tune on AAE task. An evaluation framework of Knowledge transfer from face recognition task across AAE is performed. We have made our best-performing CNNs models publicly available that would allow one to duplicate the results and for further research on the use of CNNs for AAE from face images.},
  archive      = {J_CVIU},
  author       = {Alice Othmani and Abdul Rahman Taleb and Hazem Abdelkawy and Abdenour Hadid},
  doi          = {10.1016/j.cviu.2020.102961},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102961},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Age estimation from faces using deep learning: A comparative analysis},
  volume       = {196},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Probabilistic object detection and shape extraction in
remote sensing data. <em>CVIU</em>, <em>195</em>, 102953. (<a
href="https://doi.org/10.1016/j.cviu.2020.102953">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote sensing mainly focuses on information extraction from data acquired by sensors on satellite and aerial platforms. Here, one such area of interest is ground object detection and shape extraction. Recently launched satellites and conventional aerial platforms (such as commercial UAV and professional drones) have sensors leading to more detailed and rich data source for this purpose. From these, data most of the times come in the form of optical images and LiDAR measurements. Resolution of this acquired data has increased significantly such that most ground objects (as buildings, trees, ships, cars, airplanes) can be detected and analyzed in detail. Therefore, computer vision methods have become extremely useful in remote sensing applications such as building detection and shape extraction for urban planning; tree crown measurement for crop yield forecasting; ship detection for monitoring unlawful fishery; car detection for traffic flow monitoring and intelligent transportation; and airplane detection for military and commercial operations. Researchers proposed several methods to automate the mentioned applications since manually handling them is extremely hard and prohibitively time consuming. Unfortunately, the proposed methods focus on one object type most of the times. Therefore, there is no general method to handle all the mentioned applications using computer vision tools. To overcome this problem, we propose a general framework for object detection and shape extraction in remote sensing data . Our method is based on probabilistic representation inspired by our previous work and perceptual organization principles. Due to space limitations, we only focus on buildings, trees, ships, airplanes, and cars as objects of interest in this study. We test the proposed method on several optical images acquired by different satellites and LiDAR data obtained from an aerial platform. For all objects of interest, we provide test results on both object detection and shape extraction steps. We analyze the proposed method based on these tests and discuss its strengths and weaknesses. We also comment on possible future extensions of the proposed method.},
  archive      = {J_CVIU},
  author       = {Abdullah H. Özcan and Cem Ünsalan},
  doi          = {10.1016/j.cviu.2020.102953},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102953},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Probabilistic object detection and shape extraction in remote sensing data},
  volume       = {195},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two motion models for improving video object tracking
performance. <em>CVIU</em>, <em>195</em>, 102951. (<a
href="https://doi.org/10.1016/j.cviu.2020.102951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two motion models are proposed to enhance the performance of video object tracking (VOT) algorithms. The first one is a random walk model that captures the randomness of motion patterns. The second one is a data-adaptive vector auto-regressive (VAR) model that exploits more regular motion patterns. The performance of these models is evaluated empirically using real-world datasets. Three real-time publicly available visual object trackers: the normalized cross-correlation (NCC) tracker, the New Scale Adaptive with Multiple Features (NSAMF) tracker, and the correlation filter neural network (CFNet) are modified using each of these two models. The tracking performances are then compared against the original formulation. It is observed that both models of the prior information lead to performance enhancement of all three trackers. This validates the hypothesis that when training videos are available, prior information embodied in the motion models can improve the tracking performance.},
  archive      = {J_CVIU},
  author       = {Ji Qiu and Lide Wang and Yu Hen Hu and Yin Wang},
  doi          = {10.1016/j.cviu.2020.102951},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102951},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Two motion models for improving video object tracking performance},
  volume       = {195},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visual complexity analysis using deep intermediate-layer
features. <em>CVIU</em>, <em>195</em>, 102949. (<a
href="https://doi.org/10.1016/j.cviu.2020.102949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we focus on visual complexity , an image attribute that humans can subjectively evaluate based on the level of details in the image. We explore unsupervised information extraction from intermediate convolutional layers of deep neural networks to measure visual complexity. We derive an activation energy metric that combines convolutional layer activations to quantify visual complexity. To show the effectiveness of our proposed metric for various applications, we introduce Savoias , a visual complexity dataset that compromises of more than 1,400 images from seven diverse image categories (e.g., advertisement and interior design). We demonstrate high correlations of our deep neural network-based measure of visual complexity with human-curated ground-truth (GT) scores on various widely used network architectures , e.g., VGG16, ResNet-v2-152, and EfficientNet, and in networks trained on two classification tasks (object and scene classification). This result reveals that intermediate convolutional layers of deep neural networks carry information about the complexity of images that is meaningful to people. Furthermore, we show that our method of measuring visual complexity outperforms traditional methods on Savoias and two other state-of-the-art benchmark datasets. Moreover, we perform extensive analysis on the performance difference between our unsupervised method and a supervised method trained on the feature map, and show that by supervision, we can improve the prediction. Finally, we demonstrate that, within the context of a category, visually more complex images are also more memorable to human observers.},
  archive      = {J_CVIU},
  author       = {Elham Saraee and Mona Jalal and Margrit Betke},
  doi          = {10.1016/j.cviu.2020.102949},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102949},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Visual complexity analysis using deep intermediate-layer features},
  volume       = {195},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LSTM guided ensemble correlation filter tracking with
appearance model pool. <em>CVIU</em>, <em>195</em>, 102935. (<a
href="https://doi.org/10.1016/j.cviu.2020.102935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning based visual trackers have the potential to provide good performance for object tracking. Most of them use hierarchical features learned from multiple layers of a deep network. However, issues related to deterministic aggregation of these features from various layers, difficulties in estimating variations in scale or rotation of the object being tracked, as well as challenges in effectively modeling the object’s appearance over long time periods leaves substantial scope to improve performance. In this paper, we propose a tracker that learns correlation filters over features from multiple layers of a VGG network. A correlation filter for an individual layer is used to predict the target location. We adaptively learn the contribution of an ensemble of correlation filters for the final location estimation using an LSTM . An adaptive approach is advantageous as different layers encode diverse feature representations and a uniform contribution would not fully exploit this contrastive information. To this end, we use an LSTM as it encodes the interactions for past appearances which is useful for tracking. Further, the scale and rotation parameters are estimated using respective correlation filters. Additionally, an appearance model pool is used that prevents the correlation filter from drifting. Experimental results achieved on five public datasets — Object Tracking Benchmark (OTB100), Visual Object Tracking (VOT) Benchmark 2016, VOT Benchmark 2017, Tracking Dataset and UAV123 Dataset, reveal that our approach outperforms state of the art approaches for object tracking.},
  archive      = {J_CVIU},
  author       = {Monika Jain and Subramanyam A.V. and Simon Denman and Sridha Sridharan and Clinton Fookes},
  doi          = {10.1016/j.cviu.2020.102935},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102935},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {LSTM guided ensemble correlation filter tracking with appearance model pool},
  volume       = {195},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Video anomaly detection and localization via gaussian
mixture fully convolutional variational autoencoder. <em>CVIU</em>,
<em>195</em>, 102920. (<a
href="https://doi.org/10.1016/j.cviu.2020.102920">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel end-to-end partially supervised deep learning approach for video anomaly detection and localization using only normal samples. The insight that motivates this study is that the normal samples can be associated with at least one Gaussian component of a Gaussian Mixture Model (GMM), while anomalies either do not belong to any Gaussian component. The method is based on Gaussian Mixture Variational Autoencoder , which can learn feature representations of the normal samples as a Gaussian Mixture Model trained using deep learning. A Fully Convolutional Network (FCN) that does not contain a fully-connected layer is employed for the encoder–decoder structure to preserve relative spatial coordinates between the input image and the output feature map . Based on the joint probabilities of each of the Gaussian mixture components, we introduce a sample energy based method to score the anomaly of image test patches . A two-stream network framework is employed to combine the appearance and motion anomalies, using RGB frames for the former and dynamic flow images, for the latter. We test our approach on two popular benchmarks (UCSD Dataset and Avenue Dataset). The experimental results verify the superiority of our method compared to the state of the art.},
  archive      = {J_CVIU},
  author       = {Yaxiang Fan and Gongjian Wen and Deren Li and Shaohua Qiu and Martin D. Levine and Fei Xiao},
  doi          = {10.1016/j.cviu.2020.102920},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102920},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Video anomaly detection and localization via gaussian mixture fully convolutional variational autoencoder},
  volume       = {195},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ALCN: Adaptive local contrast normalization. <em>CVIU</em>,
<em>194</em>, 102947. (<a
href="https://doi.org/10.1016/j.cviu.2020.102947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To make Robotics and Augmented Reality applications robust to illumination changes, the current trend is to train a Deep Network with training images captured under many different lighting conditions. Unfortunately, creating such a training set is a very unwieldy and complex task. We therefore propose a novel illumination normalization method that can easily be used for different problems with challenging illumination conditions . Our preliminary experiments show that among current normalization methods, the Difference-of-Gaussians method remains a very good baseline, and we introduce a novel illumination normalization model that generalizes it. Our key insight is then that the normalization parameters should depend on the input image, and we aim to train a Convolutional Neural Network to predict these parameters from the input image. This, however, cannot be done in a supervised manner, as the optimal parameters are not known a priori . We thus designed a method to train this network jointly with another network that aims to recognize objects under different illuminations: The latter network performs well when the former network predicts good values for the normalization parameters. We show that our method significantly outperforms standard normalization methods and would also be appear to be universal since it does not have to be re-trained for each new application. Our method improves the robustness to light changes of state-of-the-art 3D object detection and face recognition methods .},
  archive      = {J_CVIU},
  author       = {Mahdi Rad and Peter M. Roth and Vincent Lepetit},
  doi          = {10.1016/j.cviu.2020.102947},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102947},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {ALCN: Adaptive local contrast normalization},
  volume       = {194},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A spatial–spectral semisupervised deep learning framework
using siamese networks and angular loss. <em>CVIU</em>, <em>194</em>,
102943. (<a href="https://doi.org/10.1016/j.cviu.2020.102943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has gained popularity in recent times in the field of feature-extraction, object-identification, object-tracking, change-detection, image-classification, spatio-temporal-data analysis, and hyperspectral imaging . Most of the supervised tasks using deep learning require a large number of labeled samples, barring which the model tends to overfit and do not generalize well to the test data. Semi-supervised learning is very beneficial for hyperspectral images which contain abundant unlabeled data samples in comparison to labeled data. Furthermore, it is known that for datasets in which samples are related to each other in all three dimensions such as videos, three-dimensional biological images and hyperspectral images, the use of spatial–spectral/spatial–temporal based deep learning strategies, which can exploit the relationship between pixels in all three-dimensions, has also seen a rise in the past few years. Moreover, to date, deep feature extraction and classification has been done using euclidean distance based metrics. Foray into the field of angular feature extraction and classification, which is known to work better when samples are impacted by resolution or illumination differences, has not yet been made. We propose a novel spatial–spectral semisupervised deep learning approach based on angular distances by projecting the deep features onto the surface of an l 2 l2 -normalized unit hypersphere.},
  archive      = {J_CVIU},
  author       = {Souvick Mukherjee and Saurabh Prasad},
  doi          = {10.1016/j.cviu.2020.102943},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102943},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A spatial–spectral semisupervised deep learning framework using siamese networks and angular loss},
  volume       = {194},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image dehazing based on a transmission fusion strategy by
automatic image matting. <em>CVIU</em>, <em>194</em>, 102933. (<a
href="https://doi.org/10.1016/j.cviu.2020.102933">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most dehazing methods fail to estimate satisfactory transmission simultaneously in both normal and bright regions. To estimate more accurate transmission for these two kinds of regions, we propose a transmission fusion strategy based on automatic image matting for image dehazing. We first extract the mean and variance of a local patch around each pixel, and propose a binary classification method with the mean and variance of each patch to coarsely segment an input image into a binary map of normal and bright regions. Then we smooth and quantize the binary map to automatically generate a trimap of ternary values. Thus we can avoid the difficulty in manually labeling trimaps. Both the image and the trimap are input into a Bayesian matting method for soft segmentation of normal and bright regions to produce an alpha map. The dark channel prior (DCP) is adopted to extract a transmission map for normal regions, while an improved atmospheric veil correction (AVC) method is proposed to generate another transmission map for bright regions. Finally, we propose to use the alpha map to fuzzily fuse the two transmission maps for final image dehazing. Experimental results show that our method significantly outperforms existing methods.},
  archive      = {J_CVIU},
  author       = {Feiniu Yuan and Yu Zhou and Xue Xia and Jinting Shi and Yuming Fang and Xueming Qian},
  doi          = {10.1016/j.cviu.2020.102933},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102933},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Image dehazing based on a transmission fusion strategy by automatic image matting},
  volume       = {194},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rotation invariant features based on three dimensional
gaussian markov random fields for volumetric texture classification.
<em>CVIU</em>, <em>194</em>, 102931. (<a
href="https://doi.org/10.1016/j.cviu.2020.102931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a set of rotation invariant features based on three dimensional Gaussian Markov Random Fields (3D-GMRF) for volumetric texture image classification . In the method proposed here, the mathematical notion of spherical harmonics is employed to produce a set of features which are used to construct the rotation invariant descriptor. Our proposed method is evaluated and compared with other method in the literature for datasets containing synthetic textures as well as medical images. The results of our experiments demonstrate excellent classification performance for our proposed method compared with state-of-the-art methods. Furthermore, our method is evaluated using a clinical dataset and show good performance in discriminating between healthy individuals and COPD patients. Our method also performs well in classifying lung nodules in the LIDC-IDRI dataset. Our results indicate that our 3D-GMRF-based method enjoys more superior performance compared with other methods in the literature.},
  archive      = {J_CVIU},
  author       = {Yasseen Almakady and Sasan Mahmoodi and Joy Conway and Michael Bennett},
  doi          = {10.1016/j.cviu.2020.102931},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102931},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Rotation invariant features based on three dimensional gaussian markov random fields for volumetric texture classification},
  volume       = {194},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Protuberance of depth: Detecting interest points from a
depth image. <em>CVIU</em>, <em>194</em>, 102927. (<a
href="https://doi.org/10.1016/j.cviu.2020.102927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting distinctive interest points in a scene or an object allows estimating which details a human finds interesting in advance to understand the scene or the object. This also forms the important basis of a variety of latter tasks related to visual detection and tracking. In this paper, we propose a simple but effective approach to extract the feature from a depth image, namely Protuberance of Depth (PoD). The proposed approach semantically explores the inherent feature representing three-dimensional protuberance by using depth which only contains two-dimensional distance information. Our approach directly allows detecting consistent interest points in a depth image. The experimental results show that our method is effective against the isometric deformation and rotation of a depth region and is applicable for real-time applications.},
  archive      = {J_CVIU},
  author       = {Yuseok Ban and Sangyoun Lee},
  doi          = {10.1016/j.cviu.2020.102927},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102927},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Protuberance of depth: Detecting interest points from a depth image},
  volume       = {194},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient distance transformation for path-based metrics.
<em>CVIU</em>, <em>194</em>, 102925. (<a
href="https://doi.org/10.1016/j.cviu.2020.102925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many applications, separable algorithms have demonstrated their efficiency to perform high performance volumetric processing of shape, such as distance transformation or medial axis extraction. In the literature, several authors have discussed about conditions on the metric to be considered in a separable approach. In this article, we present generic separable algorithms to efficiently compute Voronoi maps and distance transformations for a large class of metrics. Focusing on path-based norms (chamfer masks, neighborhood sequences), we propose efficient algorithms to compute such volumetric transformation in dimension n n . We describe a new O ( n ⋅ N n ⋅ log N ⋅ ( n + log f ) ) O(n⋅Nn⋅logN⋅(n+logf)) algorithm for shapes in a N n Nn domain for chamfer norms with a rational ball of f f facets (compared to O ( f ⌊ n 2 ⌋ ⋅ N n ) O(f⌊n2⌋⋅Nn) with previous approaches). Last we further investigate a more elaborate algorithm with the same worst-case complexity, but reaching a complexity of O ( n ⋅ N n ⋅ log f ⋅ ( n + log f ) ) O(n⋅Nn⋅logf⋅(n+logf)) experimentally, under assumption of regularity distribution of the mask vectors.},
  archive      = {J_CVIU},
  author       = {David Coeurjolly and Isabelle Sivignon},
  doi          = {10.1016/j.cviu.2020.102925},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102925},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Efficient distance transformation for path-based metrics},
  volume       = {194},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Momental directional patterns for dynamic texture
recognition. <em>CVIU</em>, <em>194</em>, 102882. (<a
href="https://doi.org/10.1016/j.cviu.2019.102882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the chaotic motions of dynamic textures (DTs) is a challenging problem of video representation for different tasks in computer vision . This paper presents a new approach for an efficient DT representation by addressing the following novel concepts. First, a model of moment volumes is introduced as an effective pre-processing technique for enriching the robust and discriminative information of dynamic voxels with low computational cost. Second, two important extensions of Local Derivative Pattern operator are proposed to improve its performance in capturing directional features. Third, we present a new framework, called Momental Directional Patterns, taking into account the advantages of filtering and local-feature-based approaches to form effective DT descriptors . Furthermore, motivated by convolutional neural networks , the proposed framework is boosted by utilizing more global features extracted from max-pooling videos to improve the discrimination power of the descriptors. Our proposal is verified on benchmark datasets, i.e., UCLA, DynTex, and DynTex++, for DT classification issue. The experimental results substantiate the interest of our method.},
  archive      = {J_CVIU},
  author       = {Thanh Tuan Nguyen and Thanh Phuong Nguyen and Frédéric Bouchara and Xuan Son Nguyen},
  doi          = {10.1016/j.cviu.2019.102882},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102882},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Momental directional patterns for dynamic texture recognition},
  volume       = {194},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graph convolutional neural network for multi-scale feature
learning. <em>CVIU</em>, <em>194</em>, 102881. (<a
href="https://doi.org/10.1016/j.cviu.2019.102881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic deformable 3D modeling is computationally expensive, especially when considering complex position, orientation and scale variations. We present a volume segmentation framework to utilize local and global regularizations in a data-driven approach. We introduce automated correspondence search to avoid manually labeling landmarks and improve scalability. We propose a novel marginal space learning technique, utilizing multi-resolution pooling to obtain local and contextual features without training numerous detectors or excessively dense patches. Unlike conventional convolutional neural network operators, graph-based operators allow spatially related features to be learned on the irregular domain of the multi-resolution space, and a graph-based convolutional neural network is proposed to learn representations for position and orientation classification. The graph-CNN classifiers are used within a marginal space learning framework to provide efficient and accurate shape pose parameter hypothesis prediction. During segmentation, a global constraint is initially non-iteratively applied, with local and geometric constraints applied iteratively for refinement. Comparison is provided against both classical deformable models and state-of-the-art techniques in the complex problem domain of segmenting aortic root structure from computerized tomography scans. The proposed method shows improvement in both pose parameter estimation and segmentation performance .},
  archive      = {J_CVIU},
  author       = {Michael Edwards and Xianghua Xie and Robert I. Palmer and Gary K.L. Tam and Rob Alcock and Carl Roobottom},
  doi          = {10.1016/j.cviu.2019.102881},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102881},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Graph convolutional neural network for multi-scale feature learning},
  volume       = {194},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-exposure photomontage with hand-held cameras.
<em>CVIU</em>, <em>193</em>, 102929. (<a
href="https://doi.org/10.1016/j.cviu.2020.102929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper studies the image fusion from multiple images taken by hand-held cameras with different exposures. Existing methods often generate unsatisfactory results, such as blurring/ghosting artifacts due to the problematic handling of camera motions, dynamic contents, and inappropriately fusion of local regions (e.g., over or under exposed). In addition, they often require a high-quality image registration, which is hard to achieve in scenarios with large depth variations and dynamic textures, and is also time-consuming. In this paper, we propose to enable a rough registration by a single homography and combine the inputs seamlessly to hide any possible misalignment. Specifically, the method first uses a Markov Random Field (MRF) energy for the labeling of all pixels, which assigns different labels to different aligned input images. During the labeling, it chooses well-exposed regions and skips moving objects at the same time. Then, the proposed method combines a Laplacian image according to the labels and constructs the fusion result by solving the Poisson equation. Furthermore, it adds some internal constraints when solving the Poisson equation for balancing and improving fusion results. We present various challenging examples, including static/dynamic, indoor/outdoor and daytime/nighttime scenes, to demonstrate the effectiveness and practicability of the proposed method.},
  archive      = {J_CVIU},
  author       = {Ru Li and Shuaicheng Liu and Guanghui Liu and Tiecheng Sun and Jishun Guo},
  doi          = {10.1016/j.cviu.2020.102929},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102929},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Multi-exposure photomontage with hand-held cameras},
  volume       = {193},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adversarial autoencoders for compact representations of 3D
point clouds. <em>CVIU</em>, <em>193</em>, 102921. (<a
href="https://doi.org/10.1016/j.cviu.2020.102921">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep generative architectures provide a way to model not only images but also complex, 3-dimensional objects, such as point clouds. In this work, we present a novel method to obtain meaningful representations of 3D shapes that can be used for challenging tasks, including 3D points generation, reconstruction, compression, and clustering. Contrary to existing methods for 3D point cloud generation that train separate decoupled models for representation learning and generation, our approach is the first end-to-end solution that allows to simultaneously learn a latent space of representation and generate 3D shape out of it. Moreover, our model is capable of learning meaningful compact binary descriptors with adversarial training conducted on a latent space. To achieve this goal, we extend a deep Adversarial Autoencoder model (AAE) to accept 3D input and create 3D output. Thanks to our end-to-end training regime, the resulting method called 3D Adversarial Autoencoder (3dAAE) obtains either binary or continuous latent space that covers a much broader portion of training data distribution. Finally, our quantitative evaluation shows that 3dAAE provides state-of-the-art results for 3D points clustering and 3D object retrieval.},
  archive      = {J_CVIU},
  author       = {Maciej Zamorski and Maciej Zięba and Piotr Klukowski and Rafał Nowak and Karol Kurach and Wojciech Stokowiec and Tomasz Trzciński},
  doi          = {10.1016/j.cviu.2020.102921},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102921},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Adversarial autoencoders for compact representations of 3D point clouds},
  volume       = {193},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep code operation network for multi-label image retrieval.
<em>CVIU</em>, <em>193</em>, 102916. (<a
href="https://doi.org/10.1016/j.cviu.2020.102916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep hashing methods have been extensively studied for large-scale image search and achieved promising results in recent years. However, there are two major limitations of previous deep hashing methods for multi-label image retrieval : the first one concerns the flexibility for users to express their query intention (so-called the intention gap), and the second one concerns the exploitation of rich similarity structures of the semantic space (so-called the semantic gap). To address these issues, we propose a novel Deep Code Operation Network (CoNet), in which a user is allowed to simultaneously present multiple images instead of a single one as his/her query, and then the system triggers a series of code operators to extract the hidden relations among them. In this way, a set of new queries are automatically constructed to cover users’ real complex query intention, without the need of explicitly stating them. The CoNet is trained with a newly proposed margin-adaptive triplet loss function, which effectively encourages the system to incorporate the hierarchical similarity structures of the semantic space into the learning procedure of the code operations. The whole system has an end-to-end differentiable architecture, equipped with an adversarial mechanism to further improve the quality of the final intention representation. Experimental results on four multi-label image datasets demonstrate that our method significantly improves the state-of-the-art in performing complex multi-label retrieval tasks with multiple query images.},
  archive      = {J_CVIU},
  author       = {Ge Song and Xiaoyang Tan},
  doi          = {10.1016/j.cviu.2020.102916},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102916},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Deep code operation network for multi-label image retrieval},
  volume       = {193},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). UA-DETRAC: A new benchmark and protocol for multi-object
detection and tracking. <em>CVIU</em>, <em>193</em>, 102907. (<a
href="https://doi.org/10.1016/j.cviu.2020.102907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective multi-object tracking (MOT) methods have been developed in recent years for a wide range of applications including visual surveillance and behavior understanding. Existing performance evaluations of MOT methods usually separate the tracking step from the detection step by using one single predefined setting of object detection for comparisons. In this work, we propose a new University at Albany DEtection and TRACking (UA-DETRAC) dataset for comprehensive performance evaluation of MOT systems especially on detectors. The UA-DETRAC benchmark dataset consists of 100 challenging videos captured from real-world traffic scenes (over 140,000 frames with rich annotations, including illumination, vehicle type, occlusion, truncation ratio, and vehicle bounding boxes) for multi-object detection and tracking. We evaluate complete MOT systems constructed from combinations of state-of-the-art object detection and tracking methods. Our analysis shows the complex effects of detection accuracy on MOT system performance. Based on these observations, we propose effective and informative evaluation metrics for MOT systems that consider the effect of object detection for comprehensive performance analysis.},
  archive      = {J_CVIU},
  author       = {Longyin Wen and Dawei Du and Zhaowei Cai and Zhen Lei and Ming-Ching Chang and Honggang Qi and Jongwoo Lim and Ming-Hsuan Yang and Siwei Lyu},
  doi          = {10.1016/j.cviu.2020.102907},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102907},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {UA-DETRAC: A new benchmark and protocol for multi-object detection and tracking},
  volume       = {193},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning a confidence measure in the disparity domain from
o(1) features. <em>CVIU</em>, <em>193</em>, 102905. (<a
href="https://doi.org/10.1016/j.cviu.2020.102905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth sensing is of paramount importance for countless applications and stereo represents a popular, effective and cheap solution for this purpose. As highlighted by recent works concerned with stereo, uncertainty estimation can be a powerful cue to improve accuracy in stereo. Most confidence measures rely on features, mainly extracted from the cost volume, fed to a random forest or a convolutional neural network trained to estimate match uncertainty. In contrast, we propose a novel strategy for confidence estimation based on features computed in the disparity domain, making our proposal suited for any stereo system including COTS devices, and in constant time. We exhaustively assess the performance of our proposals, referred to as O1 and O2, on KITTI and Middlebury datasets with three popular and different stereo algorithms (CENSUS, MC-CNN and SGM), as well as a deep stereo network (PSM-Net). We also evaluate how well confidence measures generalize to different environments/datasets.},
  archive      = {J_CVIU},
  author       = {Matteo Poggi and Fabio Tosi and Stefano Mattoccia},
  doi          = {10.1016/j.cviu.2020.102905},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102905},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Learning a confidence measure in the disparity domain from o(1) features},
  volume       = {193},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A progressive learning framework based on single-instance
annotation for weakly supervised object detection. <em>CVIU</em>,
<em>193</em>, 102903. (<a
href="https://doi.org/10.1016/j.cviu.2020.102903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully-supervised object detection (FSOD) and weakly-supervised object detection (WSOD) are two extremes in the field of object detection. The former relies entirely on detailed bounding-box annotations while the later discards them completely. To balance these two extremes, we propose to make use of the so-called single-instance annotations, i.e., all images that contain only a single object are labeled with the corresponding bounding-boxes. By using such instance annotations of the simplest images, we propose a progressive learning framework that integrates image-level learning, single-instance learning, and multi-instance learning into an end-to-end network. Specifically, our framework is composed of three parallel streams that share a proposal feature extractor. The first stream is supervised by image-level annotations, which provides global information of all training data for the shared feature extractor. The second stream is supervised by single-instance annotations to bridge the features learning gap between the image level and instance level. To further learn from complex images, we propose an overlap-based instance mining algorithm to mine pseudo multi-instance annotations from the detection results of the second stream, and use them to supervise the third stream. Our method achieves a trade-off between the detection accuracy and annotation cost. Extensive experiments demonstrate the effectiveness of our proposed method on the PASCAL VOC and MS-COCO dataset, implying that a few single-instance annotations can improve the detection performance of WSOD significantly (more than 10\%) and reduce the average annotation cost of FSOD greatly (more than 5 times).},
  archive      = {J_CVIU},
  author       = {Ming Zhang and Bing Zeng},
  doi          = {10.1016/j.cviu.2020.102903},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102903},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A progressive learning framework based on single-instance annotation for weakly supervised object detection},
  volume       = {193},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Triplanar convolution with shared 2D kernels for 3D
classification and shape retrieval. <em>CVIU</em>, <em>193</em>, 102901.
(<a href="https://doi.org/10.1016/j.cviu.2019.102901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasing the depth of Convolutional Neural Networks (CNNs) has been recognized to provide better generalization performance . However, in the case of 3D CNNs, stacking layers increases the number of learnable parameters linearly, making it more prone to learn redundant features. In this paper, we propose a novel 3D CNN structure that learns shared 2D triplanar features viewed from the three orthogonal planes, which we term S3PNet . Due to the reduced dimension of the convolutions, the proposed S3PNet is able to learn 3D representations with substantially fewer learnable parameters. Experimental evaluations show that the combination of 2D representations on the different orthogonal views learned through the S3PNet is sufficient and effective for 3D representation, with the results outperforming current methods based on fully 3D CNNs. We support this with extensive evaluations on widely used 3D data sources in computer vision : CAD models, LiDAR point clouds, RGB-D images, and 3D Computed Tomography scans. Experiments further demonstrate that S3PNet has better generalization capability for smaller training sets, and learns more of kernels with less redundancy compared to kernels learned from 3D CNNs.},
  archive      = {J_CVIU},
  author       = {Eu Young Kim and Seung Yeon Shin and Soochahn Lee and Kyong Joon Lee and Kyoung Ho Lee and Kyoung Mu Lee},
  doi          = {10.1016/j.cviu.2019.102901},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102901},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Triplanar convolution with shared 2D kernels for 3D classification and shape retrieval},
  volume       = {193},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graph-matching-based correspondence search for nonrigid
point cloud registration. <em>CVIU</em>, <em>192</em>, 102899. (<a
href="https://doi.org/10.1016/j.cviu.2019.102899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonrigid registration finds transformations to fit a source point cloud/mesh to a target point cloud/mesh. Most nonrigid registration algorithms consist of two steps; finding correspondence and optimization. Among these, finding correspondence plays an important role in registration performance. However, when two point clouds have large displacement, it is hard to know correct correspondences and an algorithm often fails to find correct transformations. In this paper, we propose a novel graph-matching-based correspondence search for nonrigid registration and a corresponding optimization method for finding transformation to complete nonrigid registration. Considering global connectivity as well as local similarity for the correspondence search, the proposed method finds good correspondences according to semantics and consequently finds correct transformations even when the motion is large. Our algorithm is experimentally validated on human body and animal datasets, which verifies that it is capable of finding correct transformations to fit a source to a target.},
  archive      = {J_CVIU},
  author       = {Seunggyu Chang and Chanho Ahn and Minsik Lee and Songhwai Oh},
  doi          = {10.1016/j.cviu.2019.102899},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102899},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Graph-matching-based correspondence search for nonrigid point cloud registration},
  volume       = {192},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cascade multi-head attention networks for action
recognition. <em>CVIU</em>, <em>192</em>, 102898. (<a
href="https://doi.org/10.1016/j.cviu.2019.102898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-term temporal information yields crucial cues for video action understanding. Previous researches always rely on sequential models such as recurrent networks , memory units, segmental models, self-attention mechanism to integrate the local temporal features for long-term temporal modeling . Recurrent or memory networks record temporal patterns (or relations) by memory units, which are proved to be difficult to capture long-term information in machine translation. Self-attention mechanisms directly aggregate all local information with attention weights which is more straightforward and efficient than the former. However, the attention weights from self-attention ignore the relations between local information and global information which may lead to unreliable attention. To this end, we propose a new attention network architecture, termed as Cascade multi-head ATtention Network (CATNet), which constructs video representations with two-level attentions, namely multi-head local self-attentions and relation based global attentions. Starting from the segment features generated by backbone networks , CATNet first learns multiple attention weights for each segment to capture the importance of local features in a self-attention manner. With the local attention weights, CATNet integrates local features into several global representations, and then learns the second level attention for the global information by a relation manner. Extensive experiments on Kinetics, HMDB51, and UCF101 show that our CATNet boosts the baseline network with a large margin. With only RGB information, we respectively achieve 75.8\%, 75.2\%, and 96.0\% on these three datasets, which are comparable or superior to the state of the arts.},
  archive      = {J_CVIU},
  author       = {Jiaze Wang and Xiaojiang Peng and Yu Qiao},
  doi          = {10.1016/j.cviu.2019.102898},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102898},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Cascade multi-head attention networks for action recognition},
  volume       = {192},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Monocular human pose estimation: A survey of deep
learning-based methods. <em>CVIU</em>, <em>192</em>, 102897. (<a
href="https://doi.org/10.1016/j.cviu.2019.102897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-based monocular human pose estimation, as one of the most fundamental and challenging problems in computer vision , aims to obtain posture of the human body from input images or video sequences. The recent developments of deep learning techniques have been brought significant progress and remarkable breakthroughs in the field of human pose estimation. This survey extensively reviews the recent deep learning-based 2D and 3D human pose estimation methods published since 2014. This paper summarizes the challenges, main frameworks, benchmark datasets, evaluation metrics , performance comparison, and discusses some promising future research directions.},
  archive      = {J_CVIU},
  author       = {Yucheng Chen and Yingli Tian and Mingyi He},
  doi          = {10.1016/j.cviu.2019.102897},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102897},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Monocular human pose estimation: A survey of deep learning-based methods},
  volume       = {192},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Photometric camera characterization from a single image with
invariance to light intensity and vignetting. <em>CVIU</em>,
<em>192</em>, 102887. (<a
href="https://doi.org/10.1016/j.cviu.2019.102887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Photometric characterization of a camera entails describing how the camera transforms the light reaching its sensors into an image and how this image can be defined in a standard color space. Although the research in this area has been extensive, the current literature lacks practical methods designed for cameras operating under near light. There are two major application scenarios considered in this paper that would benefit from this type of approaches. Camera rigs for minimally-invasive-procedures cannot be calibrated in the operating room with the current methods. This comes from the fact that existing approaches need multiple images, assume uniform lighting, and/or use over-simplistic camera models, which does not allow for the calibration of near light setups in a fast and reliable way. The second scenario refers to the calibration of cellphone cameras, which currently cannot be calibrated at close range with a single image, specially if the flash is used, as there would be non-uniform lighting on the scene. In this work, we describe a method to characterize cameras from a single image of a known target. This enables both geometric and photometric calibrations to be performed on-the-fly without making assumptions on the vignetting nor on the spatial properties of the light. The presented method showed good repeatability and color accuracy even when compared to multiple-image approaches. Applications to laparoscopic cameras, generic cameras (such as cellphone cameras), and cameras other than trichromatic are shown to be viable.},
  archive      = {J_CVIU},
  author       = {Pedro M.C. Rodrigues and João P. Barreto and Michel Antunes},
  doi          = {10.1016/j.cviu.2019.102887},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102887},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Photometric camera characterization from a single image with invariance to light intensity and vignetting},
  volume       = {192},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Guess where? Actor-supervision for spatiotemporal action
localization. <em>CVIU</em>, <em>192</em>, 102886. (<a
href="https://doi.org/10.1016/j.cviu.2019.102886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of spatiotemporal localization of actions in videos. Compared to leading approaches, which all learn to localize based on carefully annotated boxes on training video frames, we adhere to a solution only requiring video class labels. We introduce an actor-supervised architecture that exploits the inherent compositionality of actions in terms of actor transformations, to localize actions. We make two contributions. First, we propose actor proposals derived from a detector for human and non-human actors intended for images, which are linked over time by Siamese similarity matching to account for actor deformations. Second, we propose an actor-based attention mechanism enabling localization from action class labels and actor proposals. It exploits a new actor pooling operation and is end-to-end trainable. Experiments on four action datasets show actor supervision is state-of-the-art for action localization from video class labels and is even competitive to some box-supervised alternatives.},
  archive      = {J_CVIU},
  author       = {Victor Escorcia and Cuong D. Dao and Mihir Jain and Bernard Ghanem and Cees Snoek},
  doi          = {10.1016/j.cviu.2019.102886},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102886},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Guess where? actor-supervision for spatiotemporal action localization},
  volume       = {192},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning feature aggregation in temporal domain for
re-identification. <em>CVIU</em>, <em>192</em>, 102883. (<a
href="https://doi.org/10.1016/j.cviu.2019.102883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification is a standard and established problem in the computer vision community. In recent years, vehicle re-identification is also getting more attention. In this paper, we focus on both these tasks and propose a method for aggregation of features in temporal domain as it is common to have multiple observations of the same object. The aggregation is based on weighting different elements of the feature vectors by different weights and it is trained in an end-to-end manner by a Siamese network . The experimental results show that our method outperforms other existing methods for feature aggregation in temporal domain on both vehicle and person re-identification tasks. Furthermore, to push research in vehicle re-identification further, we introduce a novel dataset CarsReId74k. The dataset is not limited to frontal/rear viewpoints. It contains 17,681 unique vehicles, 73,976 observed tracks, and 277,236 positive pairs. The dataset was captured by 66 cameras from various angles.},
  archive      = {J_CVIU},
  author       = {Jakub Špaňhel and Jakub Sochor and Roman Juránek and Petr Dobeš and Vojtěch Bartl and Adam Herout},
  doi          = {10.1016/j.cviu.2019.102883},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102883},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Learning feature aggregation in temporal domain for re-identification},
  volume       = {192},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automating RTI: Automatic light direction detection and
correcting non-uniform lighting for more accurate surface normals.
<em>CVIU</em>, <em>192</em>, 102880. (<a
href="https://doi.org/10.1016/j.cviu.2019.102880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reflectance Transformation Imaging (RTI) (Malzbender et al., 2001) is a photometric stereo technique that enables the interactive relighting of the object of interest from novel lighting directions, and an estimation of surface topography through the calculation of surface normal vectors. We propose a novel, fully automated technique for correcting common lighting errors in RTI and markedly improve the accuracy of surface normal estimation, as well as increasing the legibility of low relief surface variations. This moves RTI from the qualitative domain (e.g. enabling the reading of weathered inscriptions) into the quantitative domain of computer vision . RTI assumes only light direction, and not received intensity, changes as the object is imaged. Like other authors we show that this assumption is false and propose a novel method to correct for it. However, we estimate the lighting directions automatically, unlike other proposed correction techniques. Our method also requires no calibration equipment, meaning it can be easily retrofitted to any existing stack of RTI photographs. We increase the simplicity of the standard highlight RTI method by automatically detecting lighting directions and maintain its appeal to non-imaging professionals.},
  archive      = {J_CVIU},
  author       = {Matthew McGuigan and Jacqueline Christmas},
  doi          = {10.1016/j.cviu.2019.102880},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102880},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Automating RTI: Automatic light direction detection and correcting non-uniform lighting for more accurate surface normals},
  volume       = {192},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Horizon line detection using supervised learning and edge
cues. <em>CVIU</em>, <em>191</em>, 102879. (<a
href="https://doi.org/10.1016/j.cviu.2019.102879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditionally, edge detection has been extensively employed as the basic step for the horizon line detection problem. However, generally such methods do not discriminate between edges belonging to horizon boundary and others due to clouds or other natural phenomenon. Additionally, most edge based methods suffer more in the presence of edge gaps. To address these issues, we propose an edge-less horizon line detection approach based on pixel classification , hence not relying on edge information. The key idea is formulating a multi-stage graph using classification maps, instead of edge maps, where each node cost reflects the likelihood of pixel belonging to the horizon boundary. The shortest path is found in the formulated multi-stage graph using dynamic programming which conforms to the detected horizon line. We demonstrate the performance of the proposed approach on two challenging data sets and provide comparisons with two edge-based methods: one relying on edge detection while the other based on edge classification. Overall, the proposed approach achieves comparable performance against carefully crafted edge based formulations. A by-product of the edge-less approach is its capability of associating a confidence level with the found solution, which can be used to confirm the presence or absence of a horizon line in a given image. The method is also capable of dealing with partial horizon line in an image. To further improve the detection performance, we propose a fusion strategy which combines both edge-based and edge-less information. Extensive evaluations, including a publicly available data set, illustrate the superiority of the proposed fusion approach.},
  archive      = {J_CVIU},
  author       = {Touqeer Ahmad and George Bebis and Monica Nicolescu and Ara Nefian and Terry Fong},
  doi          = {10.1016/j.cviu.2019.102879},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102879},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Horizon line detection using supervised learning and edge cues},
  volume       = {191},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Human visual system vs convolution neural networks in food
recognition task: An empirical comparison. <em>CVIU</em>, <em>191</em>,
102878. (<a href="https://doi.org/10.1016/j.cviu.2019.102878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated food recognition from food plate is useful for smartphone-based applications promoting healthy lifestyles and for automated carbohydrate counting, e.g. targeted at type I diabetic patients, but the variation of appearance of food items makes it a difficult task. Convolution Neural Networks (CNNs) raised to prominence in recent years, and they will enable those applications if they are able to match HVS accuracy at least in meal classification. In this work we run an experimental comparison of accuracy between CNNs and HVS based on a simple meal recognition task. We set up a survey for humans with two phases, training and testing, and also give the food dataset to state-of-the-art CNNs. The results, considering some relevant variations in the setup, allow us to reach conclusions regarding the comparison, characteristics and limitations of CNNs, which are relevant for future improvements.},
  archive      = {J_CVIU},
  author       = {Pedro Furtado and Manuel Caldeira and Pedro Martins},
  doi          = {10.1016/j.cviu.2019.102878},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102878},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Human visual system vs convolution neural networks in food recognition task: An empirical comparison},
  volume       = {191},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comparison of monocular depth estimation methods using
geometrically relevant metrics on the IBims-1 dataset. <em>CVIU</em>,
<em>191</em>, 102877. (<a
href="https://doi.org/10.1016/j.cviu.2019.102877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of predicting a dense depth map from a monocular RGB image , commonly known as single-image depth estimation (SIDE) or monocular depth estimation (MDE), is an active research topic in computer vision for decades. With the significant progress of deep models in recent years, new standards were set yielding remarkable results in capturing the 3D structure from a single image. However, established evaluation schemes of predicted depth maps are still limited, as they only consider global statistics of the depth residuals. In order to allow for a geometry-aware analysis, we propose a set of novel quality criteria addressing the preservation of depth discontinuities and planar regions, the depth consistency across the image, and a distance-related assessment. As current datasets do not fulfill the requirements of all proposed error metrics, we provide a new high-quality indoor RGB-D test dataset , acquired by a digital single-lens reflex (DSLR) camera together with a laser scanner . New insights into the performance of current state-of-the-art SIDE approaches, as well as subtle differences among them, could be unveiled by employing the proposed error metrics on our reference dataset. Additionally, investigations on the real-world applicability of SIDE methods by a series of experiments regarding different image augmentations, illumination changes and textured planar regions have shown current limitations in this research field.},
  archive      = {J_CVIU},
  author       = {Tobias Koch and Lukas Liebel and Marco Körner and Friedrich Fraundorfer},
  doi          = {10.1016/j.cviu.2019.102877},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102877},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Comparison of monocular depth estimation methods using geometrically relevant metrics on the IBims-1 dataset},
  volume       = {191},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Registration and matching method for directed point set with
orientation attributes and local information. <em>CVIU</em>,
<em>191</em>, 102866. (<a
href="https://doi.org/10.1016/j.cviu.2019.102866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point set registration and matching are the basic problems of pattern recognition and computer vision . The key to solve these problems is to determine the correspondences between the two point sets and to describe the deformation between them. In this paper, we propose an optimization model for the registration and matching of 2D directed point sets with coordinate and orientation attributes ( x , y , θ ) (x,y,θ) . First, the thin-plate spline (TPS) function with coordinate and orientation attributes is derived by variational method to describe the directed point set deformation. Second, an optimization objective function with coordinates and angles is constructed for point set registration and matching. Finally, the objective function is solved by alternately obtaining the correspondences and describing the deformation between the point sets. In the algorithm, the initial solution of correspondences is obtained by utilizing the neighborhood information of the point set to make the algorithm more robust. Several registration and matching experiments were performed on the artificial point sets and FVC fingerprint image databases to verify the robustness, effectiveness, and accuracy of the algorithm. Compared with the current popular algorithm, the proposed algorithm shows higher precision and robustness.},
  archive      = {J_CVIU},
  author       = {Min Wu and Congying Han and Tiande Guo and Tong Zhao},
  doi          = {10.1016/j.cviu.2019.102866},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102866},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Registration and matching method for directed point set with orientation attributes and local information},
  volume       = {191},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CRF with deep class embedding for large scale
classification. <em>CVIU</em>, <em>191</em>, 102865. (<a
href="https://doi.org/10.1016/j.cviu.2019.102865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel deep learning architecture for classifying structured objects in ultrafine-grained datasets, where classes may not be clearly distinguishable by their appearance but rather by their context. We model sequences of images as linear-chain CRFs, and jointly learn the parameters from both local-visual features and neighboring class information. The visual features are learned by convolutional layers , whereas class-structure information is reparametrized by factorizing the CRF pairwise potential matrix. This forms a context-based semantic similarity space, learned alongside the visual similarities, and dramatically increases the learning capacity of contextual information. This new parametrization , however, forms a highly nonlinear objective function which is challenging to optimize. To overcome this, we develop a novel surrogate likelihood which allows for a local likelihood approximation of the original CRF with integrated batch-normalization. This model overcomes the difficulties of existing CRF methods to learn the contextual relationships thoroughly when there is a large number of classes and the data is sparse. The performance of the proposed method is illustrated on a huge dataset that contains images of retail-store product displays, and shows significantly improved results compared to linear CRF parametrization, unnormalized likelihood optimization, and RNN modeling. We also show improved results on a standard OCR dataset.},
  archive      = {J_CVIU},
  author       = {Eran Goldman and Jacob Goldberger},
  doi          = {10.1016/j.cviu.2019.102865},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102865},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {CRF with deep class embedding for large scale classification},
  volume       = {191},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An entropic optimal transport loss for learning deep neural
networks under label noise in remote sensing images. <em>CVIU</em>,
<em>191</em>, 102863. (<a
href="https://doi.org/10.1016/j.cviu.2019.102863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have established as a powerful tool for large scale supervised classification tasks . The state-of-the-art performances of deep neural networks are conditioned to the availability of large number of accurately labeled samples. In practice, collecting large scale accurately labeled datasets is a challenging and tedious task in most scenarios of remote sensing image analysis, thus cheap surrogate procedures are employed to label the dataset. Training deep neural networks on such datasets with inaccurate labels easily overfits to the noisy training labels and degrades the performance of the classification tasks drastically. To mitigate this effect, we propose an original solution with entropic optimal transportation . It allows to learn in an end-to-end fashion deep neural networks that are, to some extent, robust to inaccurately labeled samples. We empirically demonstrate on several remote sensing datasets, where both scene and pixel-based hyperspectral images are considered for classification. Our method proves to be highly tolerant to significant amounts of label noise and achieves favorable results against state-of-the-art methods.},
  archive      = {J_CVIU},
  author       = {Bharath Bhushan Damodaran and Rémi Flamary and Vivien Seguy and Nicolas Courty},
  doi          = {10.1016/j.cviu.2019.102863},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102863},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {An entropic optimal transport loss for learning deep neural networks under label noise in remote sensing images},
  volume       = {191},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient EM-ICP algorithm for non-linear registration of
large 3D point sets. <em>CVIU</em>, <em>191</em>, 102854. (<a
href="https://doi.org/10.1016/j.cviu.2019.102854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a new method for non-linear pairwise registration of 3D point sets. In this method, we consider the points of the first set as the draws of a Gaussian mixture model whose centres are the displaced points of the second set. Next we perform a maximum a posteriori estimation of the parameters (which include the unknown transformation) of this model using the expectation–maximisation (EM) algorithm. Compared to other methods using the same “EM-ICP” framework, we propose four key modifications leading to an efficient algorithm allowing for fast registration of large 3D point sets: (1) truncation of the cost function; (2) symmetrisation of the point-to-point correspondences; (3) specification of priors on these correspondences using differential geometry; (4) efficient encoding of deformations using the RKHS theory and the Fourier analysis. We evaluate the added value of these modifications and compare our method to the state-of-the-art CPD algorithm on real and simulated data.},
  archive      = {J_CVIU},
  author       = {Benoit Combès and Sylvain Prima},
  doi          = {10.1016/j.cviu.2019.102854},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102854},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {An efficient EM-ICP algorithm for non-linear registration of large 3D point sets},
  volume       = {191},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simultaneous compression and quantization: A joint approach
for efficient unsupervised hashing. <em>CVIU</em>, <em>191</em>, 102852.
(<a href="https://doi.org/10.1016/j.cviu.2019.102852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For unsupervised data-dependent hashing, the two most important requirements are to preserve similarity in the low-dimensional feature space and to minimize the binary quantization loss. A well-established hashing approach is Iterative Quantization (ITQ), which addresses these two requirements in separate steps. In this paper, we revisit the ITQ approach and propose novel formulations and algorithms to the problem. Specifically, we propose a novel approach, named Simultaneous Compression and Quantization (SCQ) , to jointly learn to compress (reduce dimensionality) and binarize input data in a single formulation under strict orthogonal constraint. With this approach, we introduce a loss function and its relaxed version, termed Orthonormal Encoder (OnE) and Orthogonal Encoder (OgE) respectively, which involve challenging binary and orthogonal constraints. We propose to attack the optimization using novel algorithms based on recent advance in cyclic coordinate descent approach. Comprehensive experiments on unsupervised image retrieval demonstrate that our proposed methods consistently outperform other state-of-the-art hashing methods . Notably, our proposed methods outperform recent deep neural networks and GAN based hashing in accuracy, while being very computationally-efficient.},
  archive      = {J_CVIU},
  author       = {Tuan Hoang and Thanh-Toan Do and Huu Le and Dang-Khoa Le-Tan and Ngai-Man Cheung},
  doi          = {10.1016/j.cviu.2019.102852},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102852},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Simultaneous compression and quantization: A joint approach for efficient unsupervised hashing},
  volume       = {191},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CompactNets: Compact hierarchical compositional networks for
visual recognition. <em>CVIU</em>, <em>191</em>, 102841. (<a
href="https://doi.org/10.1016/j.cviu.2019.102841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {CNN-based models currently provide state-of-the-art performance in image categorization tasks. While these methods are powerful in terms of representational capacity , they are generally not conceived with explicit means to control complexity. This might lead to scenarios where resources are used in a non-optimal manner, increasing the number of unspecialized or repeated neurons, and overfitting to data. In this work we propose CompactNets, a new approach to visual recognition that learns a hierarchy of shared, discriminative, specialized, and compact representations . CompactNets naturally capture the notion of compositional compactness , a characterization of complexity in compositional models, consisting on using the smallest number of patterns to build a suitable visual representation. We employ a structural regularizer with group-sparse terms in the objective function, that induces on each layer, an efficient and effective use of elements from the layer below. In particular, this allows groups of top-level features to be specialized based on category information. We evaluate CompactNets on the ILSVRC12 dataset, obtaining compact representations and competitive performance, using an order of magnitude less parameters than common CNN-based approaches. We show that CompactNets are able to outperform other group-sparse-based approaches, in terms of performance and compactness. Finally, transfer-learning experiments on small-scale datasets demonstrate high generalization power, providing remarkable categorization performance with respect to alternative approaches.},
  archive      = {J_CVIU},
  author       = {Hans Lobel and René Vidal and Alvaro Soto},
  doi          = {10.1016/j.cviu.2019.102841},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102841},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {CompactNets: Compact hierarchical compositional networks for visual recognition},
  volume       = {191},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Weakly supervised semantic segmentation using distinct class
specific saliency maps. <em>CVIU</em>, <em>191</em>, 102712. (<a
href="https://doi.org/10.1016/j.cviu.2018.08.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised segmentation has drawn considerable attention, because of the high costs associated with the creation of pixel-wise annotated image datasets that are used for training fully supervised segmentation models . We propose a weakly supervised semantic segmentation method using CNN-based class-specific saliency maps and fully connected CRF . To obtain distinct class-specific saliency maps (DCSM) that can be used as unary potentials of CRF, we propose a novel method of estimating class saliency maps, which significantly improves the method proposed by Simonyan et al. (2014) through the following improvements: (1) using CNN derivatives with respect to feature maps of the intermediate convolutional layers with up-sampling instead of an input image; (2) subtracting the saliency maps of other classes from the saliency maps of the target class to differentiate target objects among other objects; (3) aggregating multiple-scale class saliency maps to compensate for the low resolution in feature maps. In addition, we propose the use of a novel algorithm for estimating segmentation “Easiness” combined with the proposed saliency-based method. Wei et al. (2016) recently demonstrated that a fully supervised segmentation model enhanced the performance of weakly supervised segmentation by training the model using the estimated initial masks in a weakly supervised setting. However, the initial estimated masks tend to include some noise, which sometimes produces erroneous results. Therefore, we focus on improving the quality of the initial estimated masks for training a fully supervised segmentation model. We propose a method for retrieving “good seeds” by predicting the segmentation “Easiness” of images based on the consistency of the outputs under different conditions. We illustrate that our proposed method can retrieve “good seeds”. Despite of the trade-off between training data quality and the number of training images, retrieved images can improve the accuracy of weakly supervised segmentation by combining data augmentation .},
  archive      = {J_CVIU},
  author       = {Wataru Shimoda and Keiji Yanai},
  doi          = {10.1016/j.cviu.2018.08.006},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102712},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Weakly supervised semantic segmentation using distinct class specific saliency maps},
  volume       = {191},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel algebraic solution to the perspective-three-line
pose problem. <em>CVIU</em>, <em>191</em>, 102711. (<a
href="https://doi.org/10.1016/j.cviu.2018.08.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we present a novel algebraic method to the perspective-three-line (P3L) problem for determining the position and attitude of a calibrated camera from features of three known reference lines. Unlike other methods, the proposed method uses an intermediate camera frame F and an intermediate world frame E, with sparse known line coordinates, facilitating formulations of the P3L problem. Additionally, the rotation matrix between the frame E and the frame F is parameterized by using its orthogonality , and then a closed-form solution for the P3L pose problem is obtained from subsequent substitutions. This algebraic method makes the processes more easily followed and significantly improves the performance. The experimental results show that the proposed method offers numerical stability, accuracy and efficiency comparable or better than that of state-of-the-art method.},
  archive      = {J_CVIU},
  author       = {Ping Wang and Guili Xu and Yuehua Cheng},
  doi          = {10.1016/j.cviu.2018.08.005},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102711},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A novel algebraic solution to the perspective-three-line pose problem},
  volume       = {191},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Identifying motion pathways in highly crowded scenes: A
non-parametric tracklet clustering approach. <em>CVIU</em>,
<em>191</em>, 102710. (<a
href="https://doi.org/10.1016/j.cviu.2018.08.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many approaches that address the analysis of crowded scenes rely on using short trajectory fragments, also known as tracklets , of moving objects to identify motion pathways. Typically, such approaches aim at defining meaningful relationships among tracklets. However, defining these relationships and incorporating them in a crowded scene analysis framework is a challenge. In this article, we introduce a robust approach to identifying motion pathways based on tracklet clustering. We formulate a novel measure, inspired by line geometry, to capture the pairwise similarities between tracklets. For tracklet clustering, the recent distance dependent Chinese restaurant process (DD-CRP) model is adapted to use the estimated pairwise tracklet similarities. The motion pathways are identified based on two hierarchical levels of DD-CRP clustering such that the output clusters correspond to the pathways of moving objects in the crowded scene. Moreover, we extend our DD-CRP clustering adaptation to incorporate the source and sink gate probabilities for each tracklet as a high-level semantic prior for improving clustering performance. For qualitative evaluation, we propose a robust pathway matching metric, based on the chi-square distance, that accounts for both spatial coverage and motion orientation in the matched pathways. Our experimental evaluation on multiple crowded scene datasets, principally, the challenging Grand Central Station dataset, demonstrates the state-of-the-art performance of our approach. Finally, we demonstrate the task of motion abnormality detection, both at the tracklet and frame levels, against the normal motion patterns encountered in the motion pathways identified by our method, with competent quantitative performance on multiple datasets.},
  archive      = {J_CVIU},
  author       = {Allam S. Hassanein and Mohamed E. Hussein and Walid Gomaa and Yasushi Makihara and Yasushi Yagi},
  doi          = {10.1016/j.cviu.2018.08.004},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102710},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Identifying motion pathways in highly crowded scenes: A non-parametric tracklet clustering approach},
  volume       = {191},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Descriptor extraction based on a multilayer dictionary
architecture for classification of natural images. <em>CVIU</em>,
<em>191</em>, 102708. (<a
href="https://doi.org/10.1016/j.cviu.2018.08.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a descriptor extraction method in the context of image classification , based on a multilayer structure of dictionaries. We propose to learn an architecture of discriminative dictionaries for classification in a supervised framework using a patch-level approach. This method combines many layers of sparse coding and pooling in order to reduce the dimension of the problem. The supervised learning of dictionary atoms allows them to be specialized for a classification task . The method has been tested on known datasets of natural images such as MNIST, CIFAR-10 and STL, in various conditions, especially when the size of the training set is limited, and in a transfer learning application. The results are also compared with those obtained with Convolutional Neural Network (CNN) of similar complexity in terms of number of layers and processing pipeline.},
  archive      = {J_CVIU},
  author       = {Stefen Chan Wai Tim and Michele Rombaut and Denis Pellerin and Anuvabh Dutt},
  doi          = {10.1016/j.cviu.2018.08.002},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102708},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Descriptor extraction based on a multilayer dictionary architecture for classification of natural images},
  volume       = {191},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient multi-output scene coordinate prediction for fast
and accurate camera relocalization from a single RGB image.
<em>CVIU</em>, <em>190</em>, 102850. (<a
href="https://doi.org/10.1016/j.cviu.2019.102850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Camera relocalization refers to the problematic of the camera pose estimation including 3D translation and 3D rotation expressed in the world coordinate system with no temporal constraint . Camera relocalization is necessary in localization systems. However, it is still challenging to have both a real-time and accurate method. In this paper, we introduce our data-oriented hybrid method merging both machine learning and geometric approaches for fast and accurate camera relocalization from a single RGB image . We propose an efficient multi-output deep-forest regression based on a sparse feature detection, that uses a whole learned feature vector at each split function to improve the accuracy of 2D–3D point correspondences. Especially, multiple coordinate regression of our deep-forest allows to deal with ambiguous repetitive structure. The learned feature extraction is able to be pre-trained and reused for different scenes. The use of sparse feature detection reduces processing time and increases accuracy of predictions. Finally, we show favorable results in terms of accuracy and computational time compared to the state-of-the-art methods.},
  archive      = {J_CVIU},
  author       = {Nam-Duong Duong and Catherine Soladié and Amine Kacete and Pierre-Yves Richard and Jérôme Royan},
  doi          = {10.1016/j.cviu.2019.102850},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102850},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Efficient multi-output scene coordinate prediction for fast and accurate camera relocalization from a single RGB image},
  volume       = {190},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the benefit of adversarial training for monocular depth
estimation. <em>CVIU</em>, <em>190</em>, 102848. (<a
href="https://doi.org/10.1016/j.cviu.2019.102848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we address the benefit of adding adversarial training to the task of monocular depth estimation. A model can be trained in a self-supervised setting on stereo pairs of images, where depth (disparities) are an intermediate result in a right-to-left image reconstruction pipeline. For the quality of the image reconstruction and disparity prediction, a combination of different losses is used, including L1 image reconstruction losses and left–right disparity smoothness. These are local pixel-wise losses, while depth prediction requires global consistency. Therefore, we extend the self-supervised network to become a Generative Adversarial Network (GAN), by including a discriminator which should tell apart reconstructed (fake) images from real images. We evaluate Vanilla GANs, LSGANs and Wasserstein GANs in combination with different pixel-wise reconstruction losses. Based on extensive experimental evaluation, we conclude that adversarial training is beneficial if and only if the reconstruction loss is not too constrained. Even though adversarial training seems promising because it promotes global consistency, non-adversarial training outperforms (or is on par with) any method trained with a GAN when a constrained reconstruction loss is used in combination with batch normalisation . Based on the insights of our experimental evaluation we obtain state-of-the art monocular depth estimation results by using batch normalisation and different output scales.},
  archive      = {J_CVIU},
  author       = {Rick Groenendijk and Sezer Karaoglu and Theo Gevers and Thomas Mensink},
  doi          = {10.1016/j.cviu.2019.102848},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102848},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {On the benefit of adversarial training for monocular depth estimation},
  volume       = {190},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep sensorimotor learning for RGB-d object recognition.
<em>CVIU</em>, <em>190</em>, 102844. (<a
href="https://doi.org/10.1016/j.cviu.2019.102844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research findings in cognitive neuroscience establish that humans, early on, develop their understanding of real-world objects by observing others interact with them or by performing active exploration and physical interactions with them. This fact has motivated the so-called “sensorimotor” learning approach, where the object appearance information (sensory) is combined with the object affordances (motor), i.e. the types of actions a human can perform with the object. In this work, the aforementioned paradigm is adopted, and a neuro-biologically inspired two-stream model for RGB-D object recognition is investigated. Both streams are realized as state-of-the-art deep neural networks that process and fuse appearance and affordance information in multiple ways. In particular, three model variants are developed to efficiently encode the spatio-temporal nature of the hand–object interaction, while an attention mechanism that relies on the appearance stream confidence is also investigated. Additionally, a suitable auxiliary loss is proposed for model training, utilized to further optimize both information streams. Experiments on the challenging SOR3D dataset, which consists of 14 object types and 13 object affordances, demonstrate the efficacy of the proposed model in RGB-D object recognition. Overall, the best performing developed model achieves 90.70\% classification accuracy , which is further increased to 91.98\% when trained using the auxiliary loss. The latter corresponds to 46\% relative error reduction compared to the appearance-only classifier performance. Finally, a cross-view analysis on the SOR3D dataset provides valuable feedback for the viewpoint impact on the affordance information.},
  archive      = {J_CVIU},
  author       = {Spyridon Thermos and Georgios Th. Papadopoulos and Petros Daras and Gerasimos Potamianos},
  doi          = {10.1016/j.cviu.2019.102844},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102844},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Deep sensorimotor learning for RGB-D object recognition},
  volume       = {190},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A quantitative evaluation of comprehensive 3D local
descriptors generated with spatial and geometrical features.
<em>CVIU</em>, <em>190</em>, 102842. (<a
href="https://doi.org/10.1016/j.cviu.2019.102842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Designing a feature descriptor with high descriptiveness and strong robustness is an important task in 3D computer vision . Both encoding spatial and geometrical information are critical for a 3D local descriptor . In this paper, we comprehensively explore the performance of different methods for encoding spatial information with both the polar coordinate system and the Cartesian coordinate system , and also investigates the performance of combining these spatial methods with one effective geometrical attribute (i.e., normal deviation angle). Eventually, 26 descriptors are generated. Among these descriptors, eleven are generated with the polar coordinate system; fourteen are generated with the Cartesian coordinate system; one is constructed of only encoding the geometrical information. Extensive experiments are conducted for evaluating the 26 descriptors on four benchmark datasets in terms of descriptiveness, robustness, compactness, efficiency, and application performance. Based on the experimental results, the traits, merits and demerits of all the 26 descriptors are summarized. The results present that the descriptors generated by different combination of spatial attributes and geometrical attribute have significant influence to their performance. The results also show that some descriptors designed in this paper have superior overall performance compared with the state-of-the-art ones.},
  archive      = {J_CVIU},
  author       = {Bao Zhao and Xiaobo Chen and Xinyi Le and Juntong Xi},
  doi          = {10.1016/j.cviu.2019.102842},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102842},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {A quantitative evaluation of comprehensive 3D local descriptors generated with spatial and geometrical features},
  volume       = {190},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Video captioning using boosted and parallel long short-term
memory networks. <em>CVIU</em>, <em>190</em>, 102840. (<a
href="https://doi.org/10.1016/j.cviu.2019.102840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video captioning and its integration with deep learning is one of the most challenging issues in the field of machine vision and artificial intelligence . In this paper, a new boosted and parallel architecture is proposed for video captioning using Long Short-Term Memory (LSTM) networks. The proposed architecture comprises two LSTM layers and a word selection module. The first LSTM layer has the responsibility of encoding frame features extracted by a pre-trained deep Convolutional Neural Network (CNN). In the second LSTM layer, a novel architecture is used for video captioning by leveraging several decoding LSTMs in a parallel and boosting architecture. This layer, which is called Boosted and Parallel LSTM (BP-LSTM) layer, is constructed by iteratively training LSTM networks using a special kind of AdaBoost algorithm during the training phase. During the testing phase, the outputs of BP-LSTMs are concurrently combined using the maximum probability criterion and word selection module. We tested the proposed algorithm with two well-known video captioning datasets and compared the results with state-of-the-art algorithms. The results show that the proposed architecture considerably improves the accuracy of the generated sentence.},
  archive      = {J_CVIU},
  author       = {Masoomeh Nabati and Alireza Behrad},
  doi          = {10.1016/j.cviu.2019.102840},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102840},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Video captioning using boosted and parallel long short-term memory networks},
  volume       = {190},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Registration with probabilistic correspondences — accurate
and robust registration for pathological and inhomogeneous medical data.
<em>CVIU</em>, <em>190</em>, 102839. (<a
href="https://doi.org/10.1016/j.cviu.2019.102839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The registration of two medical images is usually based on the assumption that corresponding regions exist in both images. If this assumption is violated by e. g. pathologies most approaches encounter problems. The registration approach proposed is based on probabilistic correspondences of sparse image representations and enables a robust handling of potentially missing correspondences. A maximum a-posteriori framework is used to derive an optimization criterion with respect to deformation parameters that aim to reduce the shape and appearance differences between the registered images. A multi-resolution approach speeds up the optimization and increases the robustness of the registration. The computed probabilistic correspondences enable the approach to deal with missing correspondences in the images. Furthermore, they provide additional information about the quality of fit and potentially non-corresponding/pathological image regions. The approach is compared to two state-of-the-art registration methods using MR brain and cardiac images: a variational intensity-based registration algorithm and a feature-based registration approach using a discrete optimization scheme. The comprehensive quantitative evaluation using additional simulated stroke lesions shows a significantly higher accuracy and robustness of the proposed approach. Furthermore, the correspondence probability maps were used to characterize pathological regions in the MRI brain data.},
  archive      = {J_CVIU},
  author       = {Julia Krüger and Sandra Schultz and Heinz Handels and Jan Ehrhardt},
  doi          = {10.1016/j.cviu.2019.102839},
  journal      = {Computer Vision and Image Understanding},
  pages        = {102839},
  shortjournal = {Comput. Vis. Image Understanding},
  title        = {Registration with probabilistic correspondences — accurate and robust registration for pathological and inhomogeneous medical data},
  volume       = {190},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
