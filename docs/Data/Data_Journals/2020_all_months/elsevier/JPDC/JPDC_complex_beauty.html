<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JPDC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jpdc---172">JPDC - 172</h2>
<ul>
<li><details>
<summary>
(2020). Self-stabilizing token distribution on trees with constant
space. <em>JPDC</em>, <em>146</em>, 201–211. (<a
href="https://doi.org/10.1016/j.jpdc.2020.07.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-stabilizing and silent distributed algorithms for token distribution in rooted tree networks are given. Initially, each process of a graph holds at most ℓ ℓ tokens. Our goal is to distribute the tokens uniformly in the whole network so that every process holds exactly k k tokens. In the initial configuration , the total number of tokens in the network may not be n k nk where n n is the number of processes in the network. The root process is given the ability to create a new token or remove a token from the network. We aim to minimize the convergence time, the number of token moves, and the space complexity. First, a self-stabilizing token distribution algorithm that converges within O ( n ℓ ) O(nℓ) asynchronous rounds and needs Θ ( n h ϵ ) Θ(nhϵ) redundant (or unnecessary) token moves is given, where ϵ = min ( k , ℓ − k ) ϵ=min(k,ℓ−k) and h h is the height of the tree network. Next, two novel mechanisms to reduce the number of redundant token moves are presented. One reduces the number of redundant token moves to O ( n h ) O(nh) without any additional costs while the other reduces the number of redundant token moves to O ( n ) O(n) , but increases the convergence time to O ( n h ℓ ) O(nhℓ) . All given algorithms have constant memory at each process and each link register.},
  archive      = {J_JPDC},
  author       = {Yuichi Sudo and Ajoy K. Datta and Lawrence L. Larmore and Toshimitsu Masuzawa},
  doi          = {10.1016/j.jpdc.2020.07.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {201-211},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Self-stabilizing token distribution on trees with constant space},
  volume       = {146},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data placement in distributed data centers for improved SLA
and network cost. <em>JPDC</em>, <em>146</em>, 189–200. (<a
href="https://doi.org/10.1016/j.jpdc.2020.07.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale data-intensive applications provide services to users by routing service requests to geographically distributed data centers interconnected by Internet links. In order to achieve good reliability and data access latency performance, cloud service providers often simultaneously place multiple copies of the data in different data centers . The network communication required for updating the multiple data copies incurs an operational cost. At the same time, the penalty incurred by the Service Level Agreement (SLA) violation for data access from the data centers also imposes an operational cost on the service providers. In this paper, we tackle the problem of data placement in distributed data centers with the aim to minimize the operational cost incurred by delay SLA violation penalty and inter-data center network communication, assuming each data has K K data replicas. We propose a K-level Cluster-based Data Placement algorithm (K-CDP) for the problem. The algorithm solves the linear programming relaxation and dual programming problems corresponding to the problem of minimizing SLA violation penalty cost caused by placing a replica of each data in a data center. Based on the obtained solutions, the algorithm clusters the data so that the data with similar placeable data centers form a data cluster. For the data in each cluster, the algorithm selects K K data centers to minimize the operational cost. We prove that algorithm K-CDP is 2-approximation to the data placement problem. Our simulation results demonstrate that the proposed algorithm can effectively reduce the penalty cost incurred by delay SLA violation, the network communication cost, and the operational cost of data centers.},
  archive      = {J_JPDC},
  author       = {Yuqi Fan and Chen Wang and Bei Zhang and Shuyang Gu and Weili Wu and Dingzhu Du},
  doi          = {10.1016/j.jpdc.2020.07.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {189-200},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Data placement in distributed data centers for improved SLA and network cost},
  volume       = {146},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards cost-effective service migration in mobile edge: A
q-learning approach. <em>JPDC</em>, <em>146</em>, 175–188. (<a
href="https://doi.org/10.1016/j.jpdc.2020.08.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Service migration in mobile edge computing is a promising approach to improving the quality of service (QoS) for mobile users and reducing the network operational cost for service providers as well. However, these benefits are not free, coming at costs of bulk-data transfer, and likely service disruption , which could consequently increase the overall service costs. To gain the benefits of service migration while minimizing its cost across the edge nodes, in this paper, we leverage reinforcement learning (RL) method to design a cost-effective framework, called Mig-RL , for the service migration with a reduction of total service costs as a goal in a mobile edge environment. The Mig-RL leverages the infrastructure of edge network and deploys a migration agent through Q-learning to learn the optimal policy with respect to the service migration status. We distinguish the Mig-RL from other existing works in several major aspects. First, we fully exploit the nature of this problem in a modest migration space, which allows us to constrain the number of service replicas whereby a defined state–action space could be effectively handled, as opposed to those methods that need to always approximate a huge state–action space for policy optimality . Second, we advocate a migration policy-base as a cache to save the learning process by retrieving the most effective policy whenever a similar migration pattern is encountered as time goes on. Finally, by exploiting the idea of software defined network , we also investigate the efficient implementation of Mig-RL in mobile edge network. Experimental results based on some real and synthesized access sequences show that Mig-RL , compared with the selected existing algorithms, can substantially minimize the service costs, and in the meantime, efficiently improve the QoS by adapting to the changes of mobile access patterns.},
  archive      = {J_JPDC},
  author       = {Yang Wang and Shan Cao and Hongshuai Ren and Jianjun Li and Kejiang Ye and Chengzhong Xu and Xi Chen},
  doi          = {10.1016/j.jpdc.2020.08.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {175-188},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Towards cost-effective service migration in mobile edge: A Q-learning approach},
  volume       = {146},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal task scheduling benefits from a duplicate-free
state-space. <em>JPDC</em>, <em>146</em>, 158–174. (<a
href="https://doi.org/10.1016/j.jpdc.2020.07.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The NP-hard problem of task scheduling with communication delays ( P | p r e c , c i j | C max P|prec,cij|Cmax ) is often tackled using approximate methods, but guarantees on the quality of these heuristic solutions are hard to come by. Optimal schedules are therefore invaluable for properly evaluating these heuristics, as well as being very useful for applications in time critical systems. Optimal solving using branch-and-bound algorithms like A* has been shown to be promising in the past, with a state-space model we refer to as exhaustive list scheduling (ELS). The obvious weakness of this model is that it leads to the production of large numbers of duplicate states during a search, requiring special techniques to mitigate this which cost additional time and memory. In this paper we define a new state-space model (AO) in which we divide the problem into two distinct sub-problems: first we decide the allocations of all tasks to processors, and then we order the tasks on their allocated processors in order to produce a complete schedule. This two-phase state-space model offers no potential for the production of duplicates. We also describe how the pruning techniques and optimisations developed for the ELS model were adapted or made obsolete by the AO model. An experimental evaluation shows that the use of this new state-space model leads to a significant increase in the number of task graphs able to be scheduled within a feasible time-frame, particularly for task graphs with a high communication-to-computation ratio. Finally, some advanced lower bound heuristics are proposed for the AO model, and evaluation demonstrates that significant gains can be achieved from the consideration of necessary idle time.},
  archive      = {J_JPDC},
  author       = {Michael Orr and Oliver Sinnen},
  doi          = {10.1016/j.jpdc.2020.07.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {158-174},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Optimal task scheduling benefits from a duplicate-free state-space},
  volume       = {146},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An energy efficient adaptive scheduling scheme (EASS) for
mesh grid wireless sensor networks. <em>JPDC</em>, <em>146</em>,
139–157. (<a href="https://doi.org/10.1016/j.jpdc.2020.08.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The technology-driven shift in distributed network systems, Internet technology and energy grid digitization have changed the traditional spoke-hub distribution models to the more flexible and giant network-to-networks paradigm known as Energy Cloud. This energy cloud paradigm has the ability to adapt to the large-scale distributed energy resources with dynamic demand–response associated with smart grid and smart homes applications , which allows customers/users to have connectivity of electrical devices with supply grid into one giant energy cloud network. If efficient policies are not adapted in the design and operations of such networks, sensor devices may sense and forward redundant data packets. Hence, valuable energy of the node will be depleted quickly and as a result; the network will be down before accomplishing its intended task. To address the issue of energy-efficiency in energy-cloud at embedded networks, this paper proposes a novel scheme, “Energy Efficient Adaptive Scheduling Scheme (EASS) for Mesh Grid Wireless Sensor Networks”. In EASS, a sensor node configures and schedules its functions/roles according to the contents of sensed data packets and frequency of generated traffic. Like energy cloud, tasks are uniformly distributed on all nodes in mesh-grid in four states – each state of a node is responsible for configuring its components on a specific energy level – which aims to avoid link disconnection and vanish redundant data packets generation. Simulation results show that EASS increases energy-efficiency by 50\% due to the four-states model, 62.5\% due to alive nodes, 92.60\% due to minimized Cluster-Heads overhead and 38.11\% due to the reduction in dead nodes ratio.},
  archive      = {J_JPDC},
  author       = {Muhammad Nawaz Khan and Haseeb Ur Rahman and Muhamad Zahid Khan},
  doi          = {10.1016/j.jpdc.2020.08.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {139-157},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {An energy efficient adaptive scheduling scheme (EASS) for mesh grid wireless sensor networks},
  volume       = {146},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A popularity-aware reconstruction technique in erasure-coded
storage systems. <em>JPDC</em>, <em>146</em>, 122–138. (<a
href="https://doi.org/10.1016/j.jpdc.2020.08.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we develop a novel data reconstruction technique for parallel storage systems housed in modern data centers . We advocate for erasure-coded data storage systems to archive warm data (a.k.a., unpopular data), which attract a limited number of accesses or updates. Different from hot or cold data, warm data have to be treated in a distinctive way to optimize system performance and storage-space utilization. We pay particular attention to efficient data reconstruction in which faulty data nodes are rebuilt while responding to I/O requests. To achieve this goal, we employ two machine-learning algorithms to offer online data reconstruction in erasure coded storage systems. Our data reconstruction technique is conducive to recovering faulty nodes while boosting read performance for requests accessing data residing on the faulty nodes. Our system is reliant on a clustering mechanism to group files into multiple clusters, in each of which files share similar features. Furthermore, we implement a prediction module where a list of future popular data is projected by keeping track of historical I/O accesses. This popular-data list, in turn, provides predictions on files that are likely to be accessed in the not-too-distant future. The prediction module is responsible for computing similarities among users, thereby setting up priority levels of data blocks to be reconstructed. We implement our data reconstruction scheme in an erasure-coded parallel storage system to recover files with a guidance from the popular-data list. Our experimental results confirm that our system speeds up the data recovery of parallel storage systems while maintaining a high data access performance for on-line users.},
  archive      = {J_JPDC},
  author       = {Ting Cao and Xiaopu Peng and Chaowei Zhang and Taha Khalid Al Tekreeti and Jianzhou Mao and Xiao Qin and Jianzhong Huang},
  doi          = {10.1016/j.jpdc.2020.08.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {122-138},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A popularity-aware reconstruction technique in erasure-coded storage systems},
  volume       = {146},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Expelliarmus: Semantic-centric virtual machine image
management in IaaS clouds. <em>JPDC</em>, <em>146</em>, 107–121. (<a
href="https://doi.org/10.1016/j.jpdc.2020.08.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Infrastructure-as-a-service (IaaS) Clouds concurrently accommodate diverse sets of user requests, requiring an efficient strategy for storing and retrieving virtual machine images (VMIs) at a large scale. The VMI storage management requires dealing with multiple VMIs, typically in the magnitude of gigabytes, which entails VMI sprawl issues hindering the elastic resource management and provisioning. Unfortunately, existing techniques to facilitate VMI management overlook VMI semantics (i.e at the level of base image and software packages), with either restricted possibility to identify and extract reusable functionalities or with higher VMI publishing and retrieval overheads. In this paper, we propose Expelliarmus, a novel VMI management system that helps to minimize VMI storage, publishing and retrieval overheads. To achieve this goal, Expelliarmus incorporates three complementary features. First, it models VMIs as semantic graphs to facilitate their similarity computation. Second, it provides a semantically-aware VMI decomposition and base image selection to extract and store non-redundant base image and software packages. Third, it assembles VMIs based on the required software packages upon user request. We evaluate Expelliarmus through a representative set of synthetic Cloud VMIs on a real test-bed. Experimental results show that our semantic-centric approach is able to optimize the repository size by 2 . 3 − 22 2.3−22 times compared to state-of-the-art systems (e.g. IBM’s Mirage and Hemera) with significant VMI publishing and slight retrieval performance improvement.},
  archive      = {J_JPDC},
  author       = {Nishant Saurabh and Shajulin Benedict and Jorge G. Barbosa and Radu Prodan},
  doi          = {10.1016/j.jpdc.2020.08.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {107-121},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Expelliarmus: Semantic-centric virtual machine image management in IaaS clouds},
  volume       = {146},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards cost-efficient resource provisioning with multiple
mobile users in fog computing. <em>JPDC</em>, <em>146</em>, 96–106. (<a
href="https://doi.org/10.1016/j.jpdc.2020.08.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fog computing is an emerging paradigm that brings computing capabilities closer to distributed IoT devices, which provides networking services between end devices and traditional cloud data centers . One important mission is to further reduce the monetary cost of fog resources while meeting the ever-growing demands of multiple users. In this paper, we focus on minimizing the total cost for multiple mobile users to provide an efficient resource provisioning scheme in fog computing. The total cost includes two aspects: the replication cost and the transmission cost. We consider three cases for the resource provision problem by focusing on different cost models. First, one simple case where users can only upload one replication is discussed, and an optimal solution is proposed by converting the original problem into a bipartite graph matching. Then we consider a more complicated case in which each user can upload multiple replications on fog nodes in the resource provisioning . Specifically, two models are discussed: the 0–1 transmission cost model and the different transmission cost model. For the 0–1 transmission cost model, each user can upload multiple replications with a constant transmission cost, and one optimal greedy solution is proposed. For the different transmission cost model, the transmission cost is related to the distance of each pair of fog nodes. This problem is proven to be NP-hard. We first propose a non-adaptive algorithm which is proved to be bounded by 2 3 W + 1 3 O P T 23W+13OPT . Another 3 + ϵ 3+ϵ -approximation algorithm is proposed based on local search, which has better performance with higher complexity. Extensive simulations also prove the efficiency of our schemes.},
  archive      = {J_JPDC},
  author       = {Shuaibing Lu and Jie Wu and Yubin Duan and Ning Wang and Juan Fang},
  doi          = {10.1016/j.jpdc.2020.08.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {96-106},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Towards cost-efficient resource provisioning with multiple mobile users in fog computing},
  volume       = {146},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Election in unidirectional rings with homonyms.
<em>JPDC</em>, <em>146</em>, 79–95. (<a
href="https://doi.org/10.1016/j.jpdc.2020.08.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study leader election in unidirectional rings of homonyms that have no a priori knowledge of the number of processes. In this context, we show that there exists no algorithm that solves the process-terminating leader election problem for the class of asymmetrically labeled unidirectional rings. More precisely, we prove that there is no process-terminating leader election algorithm even for the subclass of unidirectional rings where at least one label is unique. Message-terminating leader election is also impossible for the class of unidirectional rings where only a bound on multiplicity is known. However, we show that the process-terminating leader election is possible for two particular subclasses of asymmetrically labeled unidirectional rings where the multiplicity is bounded. We propose three efficient algorithms and analyze their complexities. We also give some non-trivial lower bounds.},
  archive      = {J_JPDC},
  author       = {Karine Altisen and Ajoy K. Datta and Stéphane Devismes and Anaïs Durand and Lawrence L. Larmore},
  doi          = {10.1016/j.jpdc.2020.08.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {79-95},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Election in unidirectional rings with homonyms},
  volume       = {146},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Intelligently modeling, detecting, and scheduling elephant
flows in software defined energy cloud: A survey. <em>JPDC</em>,
<em>146</em>, 64–78. (<a
href="https://doi.org/10.1016/j.jpdc.2020.07.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Elephant flows (elephants) refer to the sequences of packets that contribute only 10\% of the total volume but consume over 90\% of the network bandwidth . They often cause network congestion and should be efficiently managed. Present cloud data centers often involve host- and switch-based approaches to detect and schedule elephants, but suffer (1) each host and switch in the network needs to be customized, and (2) dynamic models and advanced policies are difficult to be applied. Software Defined Cloud (SDC) addresses these issues by enabling controller-based approaches. With the aid of Machine Learning (ML) technologies, SDC can achieve learning-based models, flexible deployment, and early detection and schedule of elephants for the optimization of network performance and energy usage in a dynamic and intelligent manner. On this purpose, this article emphases the significance of models describing elephants, surveys the mechanisms that may apply to model, detect, and schedule elephants for SDC to optimize the network performance and energy usage. To the best of our knowledge, this work is the first effort that reviews the techniques in all these related subtopics simultaneously in the context of energy cloud.},
  archive      = {J_JPDC},
  author       = {Ling Xia Liao and Han-Chieh Chao and Mu-Yen Chen},
  doi          = {10.1016/j.jpdc.2020.07.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {64-78},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Intelligently modeling, detecting, and scheduling elephant flows in software defined energy cloud: A survey},
  volume       = {146},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Arbitrarily large tomography with iterative algorithms on
multiple GPUs using the TIGRE toolbox. <em>JPDC</em>, <em>146</em>,
52–63. (<a href="https://doi.org/10.1016/j.jpdc.2020.07.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D tomographic imaging requires the computation of solutions to very large inverse problems . In many applications, iterative algorithms provide superior results, however, memory limits in available computing hardware restrict the size of problems that can be solved. For this reason, iterative methods are not normally used to reconstruct typical data sets acquired with lab based CT systems . We thus use state of the art techniques such as dual buffering to develop an efficient strategy to compute the required operations for iterative reconstruction. This allows the iterative reconstruction of volumetric images of arbitrary size using any number of GPUs , each with arbitrarily small memory. Strategies for both the forward and backprojection operators are presented, along with two regularization approaches that are easily generalized to other projection types or regularizers. The proposed improvement also accelerates reconstruction of smaller images on single or multiple GPU systems, providing faster code for time-critical applications. The resulting algorithm has been added to the TIGRE toolbox, a repository for iterative reconstruction algorithms for general CT, but this memory-saving and problem-splitting strategy can be easily adapted for use with other GPU-based tomographic reconstruction code.},
  archive      = {J_JPDC},
  author       = {Ander Biguri and Reuben Lindroos and Robert Bryll and Hossein Towsyfyan and Hans Deyhle and Ibrahim El khalil Harrane and Richard Boardman and Mark Mavrogordato and Manjit Dosanjh and Steven Hancock and Thomas Blumensath},
  doi          = {10.1016/j.jpdc.2020.07.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {52-63},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Arbitrarily large tomography with iterative algorithms on multiple GPUs using the TIGRE toolbox},
  volume       = {146},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards autonomic data management for staging-based coupled
scientific workflows. <em>JPDC</em>, <em>146</em>, 35–51. (<a
href="https://doi.org/10.1016/j.jpdc.2020.07.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging scientific workflows running at extreme scale are composed of multiple applications that interact and exchange data at runtime. While staging-based approaches, e.g. in-situ/in-transit processing, are promising, dynamic behaviors (e.g. data volumes and distributions) in coupled applications and varying resource constraints at runtime make the efficient use of these techniques challenging. Addressing these challenges requires fundamental changes in the way that workflows are executed at runtime. Specifically, it is required to monitor the operating environment and running applications, and then adapt and tune the application behaviors and resource allocations at runtime while meeting the data management requirements and constraints. In this paper, we propose a policy-based autonomic data management (ADM) approach that can adaptively respond at runtime to dynamic data management requirements. We first formulate the schematic abstraction of this ADM approach including its conceptual model and system elements. Then, we explore the realization of ADM runtime and demonstrate how to achieve adaptations in a cross-layer manner with pre-defined autonomic policies. We also prototype our ADM approach and evaluate its performance on the Intrepid IBM-BlueGene and Titan Cray-XK7 systems using Chombo-based AMR applications and a visualization application. The experimental results demonstrate its effectiveness in meeting user defined objectives and accelerating overall scientific discovery.},
  archive      = {J_JPDC},
  author       = {Tong Jin and Fan Zhang and Qian Sun and Melissa Romanus and Hoang Bui and Manish Parashar},
  doi          = {10.1016/j.jpdc.2020.07.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {35-51},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Towards autonomic data management for staging-based coupled scientific workflows},
  volume       = {146},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Knowledge-driven machine learning based framework for
early-stage disease risk prediction in edge environment. <em>JPDC</em>,
<em>146</em>, 25–34. (<a
href="https://doi.org/10.1016/j.jpdc.2020.07.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early-stage disease risk prediction can be beneficial to improve the health of the mass and can reduce the economic burden of late treatment. Machine learning has played a pivotal role in predictive systems, which requires achieving a specific degree of accuracy for healthcare systems. Most recently researchers have found the necessity of bridging between epidemiology and machine learning classifications toward health risk prediction. This work proposes an epidemiology knowledge-driven unique model that follows the principle of association rule-based ontology to select features and classification techniques . The goal of this approach is to generalize a framework for future robust systems to predict the likelihood of diseases, which can be executed in the edge computing environment. The framework introduces epidemiological library and structured attribute set along with the library of precaution to derive the disease risk-prediction process. To investigate the adoption of the epidemiology knowledge-driven model, we considered a real dataset of early-stage likelihood prediction of diabetes and carried out a set of experiments for highlighting the significance of several epidemiological factors. The classification aspect of the framework is further compared with widely accepted approaches for machine learning based healthcare, which shows the novelty of the proposed model.},
  archive      = {J_JPDC},
  author       = {M. Anwar Hossain and Rahatara Ferdousi and Mohammed F. Alhamid},
  doi          = {10.1016/j.jpdc.2020.07.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {25-34},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Knowledge-driven machine learning based framework for early-stage disease risk prediction in edge environment},
  volume       = {146},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time monitoring and operation of microgrid using
distributed cloud–fog architecture. <em>JPDC</em>, <em>146</em>, 15–24.
(<a href="https://doi.org/10.1016/j.jpdc.2020.06.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new distributed multi-agent framework based on the three layers’ fog computing architecture is developed for real-time microgrid economic dispatch and monitoring. To this end, the changes of load at any time will be tracked by the proposed technique, considering unit sudden exits and entries. Moreover, to make the system more realistic, different renewable energies, including photovoltaics (PVs), wind turbines (WTs), fuel cells (FCs), and microturbines (MT) are considered in the proposed technique. To overcome the complexity of the problem, by using advantages of fog computing , a new fast consensus-based optimization algorithm is used, which is modified based on the fuzzy adaptive leader technique. Finally, the proposed technique is simulated and tested on microgrids with 6 and 14 buses, respectively. Simulation results demonstrate and validate the effectiveness of the proposed technique, as well as the capability to track the changes of load with the interactions in real-time and the fast convergence rate.},
  archive      = {J_JPDC},
  author       = {Morteza Dabbaghjamanesh and Amirhossein Moeini and Abdollah Kavousi-Fard and Alireza Jolfaei},
  doi          = {10.1016/j.jpdc.2020.06.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {15-24},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Real-time monitoring and operation of microgrid using distributed cloud–fog architecture},
  volume       = {146},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Blockchain-based verification framework for data integrity
in edge-cloud storage. <em>JPDC</em>, <em>146</em>, 1–14. (<a
href="https://doi.org/10.1016/j.jpdc.2020.06.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the popularity of the Internet of Things (IoT), data integrity verification in the edge cloud storage attracts attentions from many researchers. Due to the over dependence of the Third Party Auditor (TPA) and the dynamical nature of the IoT data, the traditional data integrity verification framework for cloud storage can hardly work. To satisfy the characteristics of the IoT and avoid the over dependence of the TPA, we propose a blockchain-based framework without TPA for data integrity verification in a decentralized edge-cloud storage (ECS) scenario in this paper. In our framework, we employ the Merkle tree with random challenging numbers for data integrity verification and analyze different Merkle tree structures to optimize the system performance. To solve the problem of limited resources and high real-time requirements, we further propose sampling verification and develop rational sampling strategies to make sampling verification more effective. The overhead and precision of the verification in ECS are studied by an optimal sample size strategy. Finally, a prototype system is implemented based on our framework. We conduct a series of experiments to evaluate the effectiveness of the proposed schemes. The experimental results show that our schemes can effectively improve the performance of data integrity verification.},
  archive      = {J_JPDC},
  author       = {Dongdong Yue and Ruixuan Li and Yan Zhang and Wenlong Tian and Yongfeng Huang},
  doi          = {10.1016/j.jpdc.2020.06.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-14},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Blockchain-based verification framework for data integrity in edge-cloud storage},
  volume       = {146},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). WP-SGD: Weighted parallel SGD for distributed
unbalanced-workload training system. <em>JPDC</em>, <em>145</em>,
202–216. (<a href="https://doi.org/10.1016/j.jpdc.2020.06.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic gradient descent (SGD) is a popular stochastic optimization method in machine learning . Traditional parallel SGD algorithms, e.g., SimuParallel SGD (Zinkevich, 2010), often require all nodes to have the same performance or to consume equal quantities of data. However, these requirements are difficult to satisfy when the parallel SGD algorithms run in a heterogeneous computing environment; low-performance nodes will exert a negative influence on the final result. In this paper, we propose an algorithm called weighted parallel SGD (WP-SGD). WP-SGD combines weighted model parameters from different nodes in the system to produce the final output. WP-SGD makes use of the reduction in standard deviation to compensate for the loss from the inconsistency in performance of nodes in the cluster, which means that WP-SGD does not require that all nodes consume equal quantities of data. We also propose the methods of running two other parallel SGD algorithms combined with WP-SGD in a heterogeneous environment. The experimental results show that WP-SGD significantly outperforms the traditional parallel SGD algorithms on distributed training systems with an unbalanced workload.},
  archive      = {J_JPDC},
  author       = {Daning Cheng and Shigang Li and Yunquan Zhang},
  doi          = {10.1016/j.jpdc.2020.06.011},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {202-216},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {WP-SGD: Weighted parallel SGD for distributed unbalanced-workload training system},
  volume       = {145},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Matrix multiplication on batches of small matrices in half
and half-complex precisions. <em>JPDC</em>, <em>145</em>, 188–201. (<a
href="https://doi.org/10.1016/j.jpdc.2020.07.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning and artificial intelligence (AI) applications often rely on performing many small matrix operations—in particular general matrix–matrix multiplication (GEMM). These operations are usually performed in a reduced precision, such as the 16-bit floating-point format (i.e., half precision or FP16). The GEMM operation is also very important for dense linear algebra algorithms, and half-precision GEMM operations can be used in mixed-precision linear solvers. Therefore, high-performance batched GEMM operations in reduced precision are significantly important, not only for deep learning frameworks, but also for scientific applications that rely on batched linear algebra , such as tensor contractions and sparse direct solvers. This paper presents optimized batched GEMM kernels for graphics processing units (GPUs) in FP16 arithmetic. The paper addresses both real and complex half-precision computations on the GPU. The proposed design takes advantage of the Tensor Core technology that was recently introduced in CUDA-enabled GPUs. With eight tuning parameters introduced in the design, the developed kernels have a high degree of flexibility that overcomes the limitations imposed by the hardware and software (in the form of discrete configurations for the Tensor Core APIs). For real FP16 arithmetic, performance speedups are observed against cuBLAS for sizes up to 128, and range between 1 . 5 × 1.5× and 2 . 5 × 2.5× . For the complex FP16 GEMM kernel, the speedups are between 1 . 7 × 1.7× and 7 × 7× thanks to a design that uses the standard interleaved matrix layout, in contrast with the planar layout required by the vendor’s solution. The paper also discusses special optimizations for extremely small matrices, where even higher performance gains are achievable.},
  archive      = {J_JPDC},
  author       = {Ahmad Abdelfattah and Stanimire Tomov and Jack Dongarra},
  doi          = {10.1016/j.jpdc.2020.07.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {188-201},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Matrix multiplication on batches of small matrices in half and half-complex precisions},
  volume       = {145},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two elementary instructions make compare-and-swap.
<em>JPDC</em>, <em>145</em>, 176–187. (<a
href="https://doi.org/10.1016/j.jpdc.2020.06.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Herlihy showed that multiprocessors must support advanced atomic objects, such as compare-and-swap, to be able to solve any arbitrary synchronization task among any number of processes (Herlihy, 1991). Elementary objects such as read-write registers and fetch-and-add are fundamentally limited to at most two processes with respect to solving an arbitrary synchronization task. Later, it was also shown that simulating an advanced atomic object using elementary objects is impossible. However, Ellen et al. observed that the above impossibility assumes computation by synchronization objects instead of synchronization instructions applied on memory locations, which is how the actual multiprocessors compute (Ellen et al., 2016). Building on that observation, we show that two elementary instructions, such as max-write and half-max, can be much better than the advanced compare-and-swap instruction. Concretely, we show the following.},
  archive      = {J_JPDC},
  author       = {Pankaj Khanchandani and Roger Wattenhofer},
  doi          = {10.1016/j.jpdc.2020.06.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {176-187},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Two elementary instructions make compare-and-swap},
  volume       = {145},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Proof of witness presence: Blockchain consensus for
augmented democracy in smart cities. <em>JPDC</em>, <em>145</em>,
160–175. (<a href="https://doi.org/10.1016/j.jpdc.2020.06.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart Cities evolve into complex and pervasive urban environments with a citizens’ mandate to meet sustainable development goals. Repositioning democratic values of citizens’ choices in these complex ecosystems has turned out to be imperative in an era of social media filter bubbles, fake news and opportunities for manipulating electoral results with such means. This paper introduces a new paradigm of augmented democracy that promises actively engaging citizens in a more informed decision-making augmented into public urban space. The proposed concept is inspired by a digital revive of the Ancient Agora of Athens, an arena of public discourse, a Polis where citizens assemble to actively deliberate and collectively decide about public matters. The core contribution of the proposed paradigm is the concept of proving witness presence: making decision-making subject of providing secure evidence and testifying for choices made in the physical space. This paper shows how the challenge of proving witness presence can be tackled with blockchain consensus to empower citizens’ trust and overcome security vulnerabilities of GPS localization . Moreover, a novel platform for collective decision-making and crowd-sensing in urban space is introduced: Smart Agora. It is shown how real-time collective measurements over citizens’ choices can be made in a fully decentralized and privacy-preserving way. Witness presence is tested by deploying a decentralized system for crowd-sensing the sustainable use of transport means. Furthermore, witness presence of cycling risk is validated using official accident data from public authorities, which are compared against wisdom of the crowd. The paramount role of dynamic consensus, self-governance and ethically aligned artificial intelligence in the augmented democracy paradigm is outlined.},
  archive      = {J_JPDC},
  author       = {Evangelos Pournaras},
  doi          = {10.1016/j.jpdc.2020.06.015},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {160-175},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Proof of witness presence: Blockchain consensus for augmented democracy in smart cities},
  volume       = {145},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Vulnerability assessment of fault-tolerant optical
network-on-chips. <em>JPDC</em>, <em>145</em>, 140–159. (<a
href="https://doi.org/10.1016/j.jpdc.2020.06.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi/Many-core systems based on the traditional electrical network-on-chip are confronted with the limited bandwidth , high latency, and reliability challenges. The emerging optical solution of silicon photonics has promised to improve the design parameters of electrical interconnections for future multiprocessor system on chips . Although the optical network-on-chip has advantages in terms of reliability compared to the electrical ones, important phenomena such as crosstalk noise, process variation, and temperature fluctuations must be carefully considered. These physical challenges have substantial adverse effects on the correct functionality of on-chip optical devices such as microring resonator and the optical waveguide . Malfunction of these elements may cause the injection of faults that should be tolerated by the reliable system. In this paper, we have assessed the effect of the fault-tolerant design of optical routers on the reliability parameters through application mapping. We propose a fault-tolerant algorithm to modify optical routers to improve the reliability parameter. The vulnerability analysis of the proposed algorithm shows that besides obtaining the fault-tolerant capability in optical routers, only about 9.63\% and 19.29\% of SNR decrease for real-world applications and seven traditional optical routers are achieved in the case of a single fault and two faults injections , respectively.},
  archive      = {J_JPDC},
  author       = {Meisam Abdollahi and Siamak Mohammadi},
  doi          = {10.1016/j.jpdc.2020.06.016},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {140-159},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Vulnerability assessment of fault-tolerant optical network-on-chips},
  volume       = {145},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hybrid anomaly-based intrusion detection system to improve
time complexity in the internet of energy environment. <em>JPDC</em>,
<em>145</em>, 124–139. (<a
href="https://doi.org/10.1016/j.jpdc.2020.06.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The technological evolution of the smart grids is going to take its shape in the form of a new paradigm called the Internet of Energy (IoE); which is considered to be the convergence of internet, communication, and energy. Like other evolved technologies, the IoE inherits security vulnerabilities from its constituents that need to be addressed. Intrusion Detection Systems (IDS) have been used to counteract malicious attacks . Among the types of IDS, anomaly-based IDS that employ mostly machine learning algorithms are considered to be the promising one, owing to their capability of detecting zero-day attacks. However, using complex algorithms to detect attacks, the existing anomaly-based IDS designed for IoE require considerable amount of time. It is tempting to reduce the training and testing time in order to make the IDS feasible for the IoE architecture. In this paper, we propose a hybrid anomaly-based IDS that can be installed at any networked site of the IoE architecture, such as Advanced Metering Infrastructure (AMI), to counteract security attacks. Our proposed system reduces the overall classification time of detection compared to the existing hybrid methods . The proposed solution uses a combination of K-means and Support Vector Machine , where the K-means centroids are used in a unique training method that reduces the training and testing times of the Support Vector Machine without compromising classification performance. We choose the best value of “k” and fine-tuned the SVM for best anomaly detection . Our approach achieves the highest accuracy of 99.9\% in comparison with the existing approaches.},
  archive      = {J_JPDC},
  author       = {Thomas Rose and Kashif Kifayat and Sohail Abbas and Muhammad Asim},
  doi          = {10.1016/j.jpdc.2020.06.012},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {124-139},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A hybrid anomaly-based intrusion detection system to improve time complexity in the internet of energy environment},
  volume       = {145},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PfTouch: Concurrent page-fault handling for intel restricted
transactional memory. <em>JPDC</em>, <em>145</em>, 111–123. (<a
href="https://doi.org/10.1016/j.jpdc.2020.06.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Page faults occurring within transactions jeopardize concurrency in Intel Restricted Transactional Memory (RTM). To make progress in spite of page-fault-induced aborts, the program must resort to the non-speculative fallback path and re-execute the affected transaction. Since the atomicity of a non-speculative transaction is guaranteed by impeding the execution of any other speculative transactions until the former completes, taking the fallback path is particularly harmful for performance. Therefore, such page-fault-induced aborts currently lead to thread serialization during the potentially long period of time taken to resolve them. In this work we propose PfTouch, a simple extension to RTM that allows page-fault handling to be moved out of non-speculative transactional execution in mutual exclusion . Our proposal sidesteps taking the fallback path in these cases and thus avoids its associated performance loss, by triggering page faults in the abort handler while other speculative transactions can run concurrently. PfTouch requires minimal modifications in the Intel RTM specification and keeps the OS unaltered. Through full-system simulation, we show that PfTouch achieves average reductions in execution time of 7.7\% (up to 24.4\%) for the STAMP benchmarks, closely matching the performance of the more complex suspended transactional mode in the IBM Power ISA.},
  archive      = {J_JPDC},
  author       = {Rubén Titos-Gil and Ricardo Fernández-Pascual and Alberto Ros and Manuel E. Acacio},
  doi          = {10.1016/j.jpdc.2020.06.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {111-123},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {PfTouch: Concurrent page-fault handling for intel restricted transactional memory},
  volume       = {145},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning-based dynamic cache management in a cloud.
<em>JPDC</em>, <em>145</em>, 98–110. (<a
href="https://doi.org/10.1016/j.jpdc.2020.06.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Caches are an important component of modern computing systems given their significant impact on performance. In particular, caches play a key role in the cloud due to the nature of large-scale, data-intensive processing. One of the key challenges for the cloud providers is how to share the caching capacity among tenants, under the circumstance that each often requires a different degree of quality of service (QoS) with respect to data access performance. The invariant is that the individual tenants’ QoS requirements should be satisfied while the cache usage is optimized in a system-wide manner. In this paper, we introduce a learning-based approach for dynamic cache management in a cloud, which is based on the estimation of data access pattern of a tenant and the prediction of cache performance for the access pattern in question. We consider a variety of probability distributions to estimate the data access pattern, and examine a set of learning-based regression techniques to predict the cache hit rate for the access pattern. The predicted cache hit rate is then used to make a decision whether reallocating cache space is needed to meet the QoS requirement for the tenant. Our experimental results with an extensive set of synthetic traces and the YCSB benchmark show that the proposed method consistently optimizes the cache space while satisfying the QoS requirement.},
  archive      = {J_JPDC},
  author       = {Jinhwan Choi and Yu Gu and Jinoh Kim},
  doi          = {10.1016/j.jpdc.2020.06.013},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {98-110},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Learning-based dynamic cache management in a cloud},
  volume       = {145},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lightweight collaborative anomaly detection for the IoT
using blockchain. <em>JPDC</em>, <em>145</em>, 75–97. (<a
href="https://doi.org/10.1016/j.jpdc.2020.06.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to their rapid growth and deployment, the Internet of things (IoT) have become a central aspect of our daily lives. Unfortunately, IoT devices tend to have many vulnerabilities which can be exploited by an attacker. Unsupervised techniques , such as anomaly detection , can be used to secure these devices in a plug-and-protect manner. However, anomaly detection models must be trained for a long time in order to capture all benign behaviors. Furthermore, the anomaly detection model is vulnerable to adversarial attacks since, during the training phase, all observations are assumed to be benign. In this paper, we propose (1) a novel approach for anomaly detection and (2) a lightweight framework that utilizes the blockchain to ensemble an anomaly detection model in a distributed environment. Blockchain framework incrementally updates a trusted anomaly detection model via self-attestation and consensus among the IoT devices. We evaluate our method on a distributed IoT simulation platform, which consists of 48 Raspberry Pis. The simulation demonstrates how the approach can enhance the security of each device and the security of the network as a whole.},
  archive      = {J_JPDC},
  author       = {Yisroel Mirsky and Tomer Golomb and Yuval Elovici},
  doi          = {10.1016/j.jpdc.2020.06.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {75-97},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Lightweight collaborative anomaly detection for the IoT using blockchain},
  volume       = {145},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Differential privacy in blockchain technology: A futuristic
approach. <em>JPDC</em>, <em>145</em>, 50–74. (<a
href="https://doi.org/10.1016/j.jpdc.2020.06.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain has received a widespread attention because of its decentralized, tamper-proof, and transparent nature. Blockchain works over the principle of distributed, secured, and shared ledger, which is used to record, and track data within a decentralized network. This technology has successfully replaced certain systems of economic transactions in organizations and has the potential to overtake various industrial business models in future. Blockchain works over peer-to-peer (P2P) phenomenon for its operation and does not require any trusted-third party authorization for data tracking and storage. The information stored in blockchain is distributed throughout the decentralized network and is usually protected using cryptographic hash functions . Since the beginning of blockchain technology, its use in different applications is increasing exponentially, but this increased use has also raised some questions regarding privacy and security of data being stored in it. Protecting privacy of blockchain data using data perturbation strategy such as differential privacy could be a novel approach to overcome privacy issues in blockchain. In this article, we cover the topic of integration of differential privacy in each layer of blockchain and in certain blockchain based scenarios. Moreover, we highlight some future challenges and application scenarios in which integration of differential privacy in blockchain can produce fruitful results.},
  archive      = {J_JPDC},
  author       = {Muneeb Ul Hassan and Mubashir Husain Rehmani and Jinjun Chen},
  doi          = {10.1016/j.jpdc.2020.06.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {50-74},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Differential privacy in blockchain technology: A futuristic approach},
  volume       = {145},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enabling efficient and secure energy cloud using edge
computing and 5G. <em>JPDC</em>, <em>145</em>, 42–49. (<a
href="https://doi.org/10.1016/j.jpdc.2020.06.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Energy cloud systems continue to shape the future of the energy sector. The complexity of energy cloud systems stems from their widespread and distributed aspects such as renewable energy sources , energy storage, customers engagement, social media and the advancements in communication and computing technologies. The unprecedented large-scale growth of energy cloud systems requires a crucial and dramatic paradigm shift in managing and optimizing the available energy assets in order to satisfy the increasing customers’ requirements. This paper proposes and evaluates an edge computing based framework that aims to efficiently manage and optimize energy cloud systems while increasing their reliability, safety, and security. The proposed framework exploits the current expansion in computing capabilities of the edge computing and the Fifth Communication Generation (5G) technology. The evaluation of the proposed framework shows that an edge computing infrastructure improves the service efficiency by 22.6\% compared with a cloud infrastructure. In addition, the latency is reduced by 69.1\%. The proposed framework provides threat detection capability by using the edge layer as an extra layer for defense against energy cloud system attacks. However, this defense mechanism incurs 10.9\% overhead and 9.6\% extra delay per service request on average.},
  archive      = {J_JPDC},
  author       = {Yaser Jararweh},
  doi          = {10.1016/j.jpdc.2020.06.014},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {42-49},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Enabling efficient and secure energy cloud using edge computing and 5G},
  volume       = {145},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Performance enhancement of a dynamic k-means algorithm
through a parallel adaptive strategy on multicore CPUs. <em>JPDC</em>,
<em>145</em>, 34–41. (<a
href="https://doi.org/10.1016/j.jpdc.2020.06.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The K-means algorithm is one of the most popular algorithms in Data Science, and it is aimed to discover similarities among the elements belonging to large datasets, partitioning them in K K distinct groups called clusters. The main weakness of this technique is that, in real problems, it is often impossible to define the value of K K as input data. Furthermore, the large amount of data used for useful simulations makes impracticable the execution of the algorithm on traditional architectures. In this paper, we address the previous two issues. On the one hand, we propose a method to dynamically define the value of K K by optimizing a suitable quality index with special care to the computational cost. On the other hand, to improve the performance and the effectiveness of the algorithm, we propose a strategy for parallel implementation on modern multicore CPUs.},
  archive      = {J_JPDC},
  author       = {Giuliano Laccetti and Marco Lapegna and Valeria Mele and Diego Romano and Lukasz Szustak},
  doi          = {10.1016/j.jpdc.2020.06.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {34-41},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Performance enhancement of a dynamic K-means algorithm through a parallel adaptive strategy on multicore CPUs},
  volume       = {145},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient parallel direction-based clustering algorithm.
<em>JPDC</em>, <em>145</em>, 24–33. (<a
href="https://doi.org/10.1016/j.jpdc.2020.06.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering, which explores the visualization and distribution of data, has recently been studied widely. Although the existing clustering algorithms can well detect arbitrary shape clusters, most of them face the limitation that they cluster points on the basis of two physical metrics, distance and density, but ignore the orientation relationship of data distribution. Beside, they have a difficulty of selecting suitable parameters, which are important inputs of the clustering algorithms . In this paper, we firstly introduce a new physical metric, namely direction. Then, based on this new metric, we propose an adaptive direction-based clustering algorithm, namely ADC, which can automatically calculate appropriate parameters. Finally, we develop a parallel ADC algorithm based on multi-processors to improve the performance of the ADC algorithm. Compared with other clustering algorithms, experimental results demonstrate that the proposed algorithms are more general and can get much better clustering results . In addition, the parallel ADC algorithm has the best scalability over large data sets.},
  archive      = {J_JPDC},
  author       = {Kai Zhong and Xu Zhou and Liqian Zhou and Zhibang Yang and Chubo Liu and Na Xiao},
  doi          = {10.1016/j.jpdc.2020.06.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {24-33},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {An efficient parallel direction-based clustering algorithm},
  volume       = {145},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SocialBlock: An architecture for decentralized user-centric
data management applications for communications in smart cities.
<em>JPDC</em>, <em>145</em>, 13–23. (<a
href="https://doi.org/10.1016/j.jpdc.2020.06.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart city projects stand out for including a wide array of communication systems and heterogeneous technologies operated by multiple entities that collect data from citizens in very different ways. Basically, these projects involve three main actors that may have conflicting interests regarding the ownership and exploitation of the generated data: public administrations, service providers and citizens. Nevertheless, this is not a new problem. Actually, the most popular communication systems on the Internet, such as social networks, often have issues of this kind that can become highly visible in the press, especially when service providers, who commonly store data generated by their users via centralized models, have a security issue leading to a massive data leak, or when providers take advantage of their dominant position to exploit data improperly or sell it to third parties. This paper proposes an architecture to create user-centric data management applications for communications in smart cities where data is stored and managed in a decentralized way to reduce dependency on service providers and return control of the data to the different actors involved in the communications. In this paper, a proof of concept of the proposed architecture is implemented, using Ethereum and the InterPlanetary File System (IPFS) as the main tools, to demonstrate its feasibility, show concrete protocols, and discuss the proposal on an empirical basis.},
  archive      = {J_JPDC},
  author       = {Victor Garcia-Font},
  doi          = {10.1016/j.jpdc.2020.06.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {13-23},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {SocialBlock: An architecture for decentralized user-centric data management applications for communications in smart cities},
  volume       = {145},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic blockchain whitepapers analysis via heterogeneous
graph neural network. <em>JPDC</em>, <em>145</em>, 1–12. (<a
href="https://doi.org/10.1016/j.jpdc.2020.05.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The blockchain whitepaper contains detailed technical and business information, so its analysis is important for blockchain text mining. Previous works focus on analyze homogeneous objects and relations. The main problem, however, is these works do not take into account the heterogeneity of information. This paper presents a new methodology for whitepapers analysis by designing heterogeneous graph neural network , named S-HGNN. In detail, this paper first builds a Heterogeneous Information Network (HIN) using heterogeneous objects and relationships extracted from the whitepaper to obtain similarity measures, then uses Graph Convolutional Network (GCN) and Graph Attention Network (GAT) to integrate both structural information and internal semantic into the whitepaper embedding. Compared with the previous models, this model improves 0.96\% ∼ ∼ 33.34\% in terms of F1-score for classification task , and 4.94\% ∼ ∼ 14.14\% in terms of purity for clustering task , and gets stable results on different tasks. The results show the effectiveness and robustness of this model for whitepapers analysis.},
  archive      = {J_JPDC},
  author       = {Lin Liu and Wei-Tek Tsai and Md Zakirul Alam Bhuiyan and Dong Yang},
  doi          = {10.1016/j.jpdc.2020.05.014},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-12},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Automatic blockchain whitepapers analysis via heterogeneous graph neural network},
  volume       = {145},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Front matter 1 - full title page (regular issues)/special
issue title page (special issues). <em>JPDC</em>, <em>144</em>, i. (<a
href="https://doi.org/10.1016/S0743-7315(20)30324-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JPDC},
  doi          = {10.1016/S0743-7315(20)30324-5},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {i},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Front matter 1 - full title page (regular issues)/Special issue title page (special issues)},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Compact self-stabilizing leader election for general
networks. <em>JPDC</em>, <em>144</em>, 278–294. (<a
href="https://doi.org/10.1016/j.jpdc.2020.05.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a self-stabilizing leader election algorithm for general networks, with space-complexity O ( log Δ + log log n ) O(logΔ+loglogn) bits per node in n n -node networks with maximum degree Δ Δ . This space complexity is sub-logarithmic in n n as long as Δ = n o ( 1 ) Δ=no(1) . The best space-complexity known so far for general networks was O ( log n ) O(logn) bits per node, and algorithms with sub-logarithmic space-complexities were known for the ring only. To our knowledge, our algorithm is the first algorithm for self-stabilizing leader election to break the Ω ( log n ) Ω(logn) bound for silent algorithms in general networks. Breaking this bound was obtained via the design of a (non-silent) self-stabilizing algorithm using sophisticated tools such as solving the distance-2 coloring problem in a silent self-stabilizing manner, with space-complexity O ( log Δ + log log n ) O(logΔ+loglogn) bits per node. Solving this latter coloring problem allows us to implement a sub-logarithmic encoding of spanning trees — storing the IDs of the neighbors requires Ω ( log n ) Ω(logn) bits per node, while we encode spanning trees using O ( log Δ + log log n ) O(logΔ+loglogn) bits per node. Moreover, we show how to construct such compactly encoded spanning trees without relying on variables encoding distances or number of nodes, as these two types of variables would also require Ω ( log n ) Ω(logn) bits per node.},
  archive      = {J_JPDC},
  author       = {Lélia Blin and Sébastien Tixeuil},
  doi          = {10.1016/j.jpdc.2020.05.019},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {278-294},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Compact self-stabilizing leader election for general networks},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards blockchain-enabled single character frequency-based
exclusive signature matching in IoT-assisted smart cities.
<em>JPDC</em>, <em>144</em>, 268–277. (<a
href="https://doi.org/10.1016/j.jpdc.2020.05.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increasing viability of Internet of Things (IoT), more devices are expected to be connected in a smart city environment. It can provide many benefits for people’s daily life, but is also susceptible to many security threats in practice. Intrusion detection systems (IDSs), especially signature-based IDSs, are one of the most commonly adopted security mechanisms to safeguard various network environments like IoT-assisted smart city against cyber attacks . The process of signature matching is a key limiting factor for a signature-based IDS, and the exclusive signature matching (ESM) was designed based on the observation that most network packets would not match any IDS signatures. However, exclusive signature matching like the single character frequency-based ESM may be vulnerable to some attacks in a hostile environment. To mitigate this issue, in this work, we propose a blockchain-enabled single character frequency-based ESM, which can build a verifiable database of malicious payloads via blockchains. In the evaluation, we investigate the performance of our approach under flooding and character padding attacks in both a simulated and a real IoT network environment. The results demonstrate the effectiveness of our approach in enhancing the robustness of single character frequency-based ESM against malicious traffic.},
  archive      = {J_JPDC},
  author       = {Weizhi Meng and Wenjuan Li and Steven Tug and Jiao Tan},
  doi          = {10.1016/j.jpdc.2020.05.013},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {268-277},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Towards blockchain-enabled single character frequency-based exclusive signature matching in IoT-assisted smart cities},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Securing transmissions by friendly jamming scheme in
wireless networks. <em>JPDC</em>, <em>144</em>, 260–267. (<a
href="https://doi.org/10.1016/j.jpdc.2020.04.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we focus on the design of optimal relay and jammer selection strategy in relay-aided wireless networks. Different from previous works, assuming that the channel state information (CSI) of illegitimate nodes was available and only an eavesdropper existed, we first analyze disadvantages of joint relay and jammer selection (JRJS), average optimal relay selection (AORS), traditional maximum relay selection (TMRS) schemes. Then, we design an optimal relay and jammer selection strategy where the ratio of received SNRs at the destination generated by any two relays is maximized. By applying proposed strategy, computation complexity can be reduced. Moreover, we derive the lower and upper bounds of the secrecy outage probability based on the assumptions of existence of only illegitimate node and symmetric case for mathematical convenience. Finally, simulation shows that the proposed strategy operating with no CSI of illegitimate nodes can work efficiently compared with JRJS, TMRS and AORS strategies.},
  archive      = {J_JPDC},
  author       = {Guangshun Li and Xiaofei Sheng and Junhua Wu and Haili Yu},
  doi          = {10.1016/j.jpdc.2020.04.013},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {260-267},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Securing transmissions by friendly jamming scheme in wireless networks},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Coordinated management of DVFS and cache partitioning under
QoS constraints to save energy in multi-core systems. <em>JPDC</em>,
<em>144</em>, 246–259. (<a
href="https://doi.org/10.1016/j.jpdc.2020.05.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reducing the energy expended to carry out a computational task is important. In this work, we explore the prospects of meeting Quality-of-Service requirements of tasks on a multi-core system while adjusting resources to expend a minimum of energy. This paper considers, for the first time, a QoS-driven coordinated resource management algorithm (RMA) that dynamically adjusts the size of the per-core last-level cache partitions and the per-core voltage–frequency settings to save energy while respecting QoS requirements of every application in multi-programmed workloads run on multi-core systems. It does so by doing configuration-space exploration across the spectrum of LLC partition sizes and Dynamic Voltage–Frequency Scaling (DVFS) settings at runtime at negligible overhead. We show that the energy of 4-core and 8-core systems can be reduced by up to 18\% and 14\%, respectively, compared to a baseline with even distribution of cache resources and a fixed mid-range core voltage–frequency setting. The energy savings can potentially reach 29\% if the QoS targets are relaxed to 40\% longer execution time.},
  archive      = {J_JPDC},
  author       = {Mehrzad Nejat and Madhavan Manivannan and Miquel Pericàs and Per Stenström},
  doi          = {10.1016/j.jpdc.2020.05.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {246-259},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Coordinated management of DVFS and cache partitioning under QoS constraints to save energy in multi-core systems},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Thermal-aware detour routing in 3D NoCs. <em>JPDC</em>,
<em>144</em>, 230–245. (<a
href="https://doi.org/10.1016/j.jpdc.2020.04.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional Network-on-Chips (3D NoCs) is a popular design choice due to its low packet latency, low network power consumption and high packing density . However, 3D NoCs suffer from high temperature issues. The 3D stacking of Si-layers elongates heat transfer path from different Si-layers to the heat sink resulting in increase in peak temperature of the chip. Since routers of NoCs have high power densities, a higher router activity may result in signification increase in temperature of the chip. Therefore, a judicious selection of the routing path is necessary to reduce the chip temperature. As the routers placed at the lower Si-layers have higher thermal conductance to the heat sink, a routing path consisting of more number of routers in the lower Si-layers may improve the temperature profile of the chip. In this paper, we have proposed two different thermal-aware routing approaches, which use downward detoured routing for some optimally selected communication paths to reduce the chip temperature. The first technique is a thermal-aware application-specific Mixed Integer Linear Programming based method (named TMD), while the second one is an application-agnostic heuristic approach (named TSD). To predict the effect of detour decisions on the temperature profile of the chip, TMD technique has applied two different thermal models with a constraint on the average packet delay (APD) of the network. Experimental results show that a significant temperature reduction (up to 22 ∘ ∘ ) can be achieved within minimal performance loss (10\% increase in APD) using either of the proposed techniques, compared to the minimal path routing algorithms – XYZ, ZXY and EDGE and the greedy detour approaches – All Detour and Random Detour.},
  archive      = {J_JPDC},
  author       = {Priyajit Mukherjee and Navonil Chatterjee and Santanu Chattopadhyay},
  doi          = {10.1016/j.jpdc.2020.04.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {230-245},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Thermal-aware detour routing in 3D NoCs},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Achieving high data reliability at low scrubbing cost via
failure-aware scrubbing. <em>JPDC</em>, <em>144</em>, 220–229. (<a
href="https://doi.org/10.1016/j.jpdc.2020.05.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latent Sector Errors (LSEs) happen at a significant frequency in the field and can impose a huge risk to data reliability. Disk scrubbing is a background process that reads disks periodically to detect LSEs timely, thus shortening the window of vulnerability to data loss. Nowadays, proactive error prediction, using machine learning techniques , has been proposed to improve storage system reliability by increasing the scrubbing rate for disks with higher error rates. Unfortunately, the majority of works incur non-trivial scrubbing costs and overlook the relationship between complete disk failures and LSEs. In this paper, we attempt to maintain or improve data reliability at reduced scrubbing costs. In particular, we design a novel adaptive approach that enforces a lower scrubbing rate for healthy disks and a higher scrubbing rate for disks which are subject to LSEs. Besides LSEs that are specific to partial disk failures, we also adjust scrubbing rates according to complete disk failure rates , because disks typically develop LSEs before they finally fail. Moreover, a voting-based method that exploits the periodic characteristic of scrubbing is proposed to ensure prediction accuracy. Experimental results on a real-world field dataset have demonstrated the effectiveness of our proposed approach. Specifically, the results show that we can achieve the same level of reliability, in terms of Mean-Time-To-Detection (MTTD), as the traditional fixed-rate scrubbing scheme with almost 49\% less scrubbing costs or we can improve the reliability by a factor of 2.4X without extra scrubbing costs. Compared with the state-of-the-art approaches, our method can achieve the same level of reliability with nearly 32\% less scrubbing costs.},
  archive      = {J_JPDC},
  author       = {Tianming Jiang and Ping Huang and Ke Zhou},
  doi          = {10.1016/j.jpdc.2020.05.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {220-229},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Achieving high data reliability at low scrubbing cost via failure-aware scrubbing},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Time–energy trade-offs in processing divisible loads on
heterogeneous hierarchical memory systems. <em>JPDC</em>, <em>144</em>,
206–219. (<a href="https://doi.org/10.1016/j.jpdc.2020.05.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze time and energy performance of distributed computations in heterogeneous systems with hierarchical memory. Different levels of memory hierarchy have different time and energy efficiency. Core memory may be too small to hold whole load to be processed, while computations using external storage are expensive in time and energy. In order to avoid the costs of processing the load in the external memory, it is allowed that the load is distributed to the worker processors in multiple installments. A minimum energy solution is found by use of mixed integer linear programming under a limit on schedule length. Two types of fast heuristics with several variants are also examined. The trade-off between the criteria of processing time and energy is studied. Key features of optimum solutions are analyzed. It is shown that holding machines in a diverse set of energy modes and limited use of the out-of-core memory can be beneficial for the time and energy performance. The proposed scheduling algorithms are evaluated in the terms of solution quality and runtimes.},
  archive      = {J_JPDC},
  author       = {Jędrzej Marszałkowski and Maciej Drozdowski and Gaurav Singh},
  doi          = {10.1016/j.jpdc.2020.05.015},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {206-219},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Time–energy trade-offs in processing divisible loads on heterogeneous hierarchical memory systems},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cache simulation for irregular memory traffic on multi-core
CPUs: Case study on performance models for sparse matrix–vector
multiplication. <em>JPDC</em>, <em>144</em>, 189–205. (<a
href="https://doi.org/10.1016/j.jpdc.2020.05.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parallel computations with irregular memory access patterns are often limited by the memory subsystems of multi-core CPUs, though it can be difficult to pinpoint and quantify performance bottlenecks precisely. We present a method for estimating volumes of data traffic caused by irregular, parallel computations on multi-core CPUs with memory hierarchies containing both private and shared caches. Further, we describe a performance model based on these estimates that applies to bandwidth-limited computations. As a case study, we consider two standard algorithms for sparse matrix–vector multiplication, a widely used, irregular kernel. Using three different multi-core CPU systems and a set of matrices that induce a range of irregular memory access patterns , we demonstrate that our cache simulation combined with the proposed performance model accurately quantifies performance bottlenecks that would not be detected using standard best- or worst-case estimates of the data traffic volume.},
  archive      = {J_JPDC},
  author       = {James D. Trotter and Johannes Langguth and Xing Cai},
  doi          = {10.1016/j.jpdc.2020.05.020},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {189-205},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Cache simulation for irregular memory traffic on multi-core CPUs: Case study on performance models for sparse matrix–vector multiplication},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evaluation of memory performance in NUMA architectures using
stochastic reward nets. <em>JPDC</em>, <em>144</em>, 172–188. (<a
href="https://doi.org/10.1016/j.jpdc.2020.05.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding memory performance in multi-core platforms is a prerequisite to perform optimizations. To this end, this paper presents analytical models based on Stochastic Reward Nets (SRNs) to model and evaluate the memory performance of Non-Uniform Memory Access (NUMA) multi-core architectures. The approach considers the details of the architecture and first proposes a monolithic SRN model that evaluates the memory performance in terms of the mean memory response time. Since the monolithic model incurs a state space explosion with an increasing number of cores and memory controllers , two approximate models are presented that are able to evaluate large-scale NUMA architectures. The SRNs are validated through measurements on two NUMA multi-core platforms, a 64-core AMD Opteron server and a 72-core Intel system. The results demonstrate the ability of the proposed models to accurately compute the mean memory response time on NUMA architectures. The results also provide valuable information that runtime systems and application designers can use to optimize execution of parallel applications on such architectures.},
  archive      = {J_JPDC},
  author       = {Reza Entezari-Maleki and Younghyun Cho and Bernhard Egger},
  doi          = {10.1016/j.jpdc.2020.05.022},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {172-188},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Evaluation of memory performance in NUMA architectures using stochastic reward nets},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Schedulability analysis of time-sensitive networks with
scheduled traffic and preemption support. <em>JPDC</em>, <em>144</em>,
153–171. (<a href="https://doi.org/10.1016/j.jpdc.2020.06.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Time-Sensitive Networking (TSN) set of standards introduces in IEEE 802.1 switches and end stations novel features to meet the requirements of a broad spectrum of applications that are characterized by time-sensitive and mission-critical traffic flows. In particular, the IEEE802.1Qbv-2015 amendment introduces enhancements that provide temporal isolation for scheduled traffic, i.e., a traffic class that requires transmission based on a known timescale, while the IEEE802.1Qbu-2016 introduces preemption as a mechanism to allow time-critical messages to interrupt ongoing non time-critical transmissions. Both amendments, that are now enrolled in the IEEE802.1Q-2018 standard, are very important for industrial networks, where scheduled traffic and low-latency real-time flows have to coexist, on the same network, with best-effort transmissions. In this context, this work presents a response time analysis of TSN networks that encompasses the enhancements for scheduled traffic and preemption, in various combinations. The paper presents the proposed analysis and a performance comparison between the response times calculated by the analysis and the response times obtained through OMNeT++ simulations in three different scenarios.},
  archive      = {J_JPDC},
  author       = {Lucia Lo Bello and Mohammad Ashjaei and Gaetano Patti and Moris Behnam},
  doi          = {10.1016/j.jpdc.2020.06.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {153-171},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Schedulability analysis of time-sensitive networks with scheduled traffic and preemption support},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distributed load balancing frequent colossal closed itemset
mining algorithm for high dimensional dataset. <em>JPDC</em>,
<em>144</em>, 136–152. (<a
href="https://doi.org/10.1016/j.jpdc.2020.05.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The focus of extracting colossal closed itemsets from high dimensional biological datasets has been great in recent times. A massive set of short and average sized mined itemsets do not confine complete and valuable information for decision making. But, the traditional itemset mining algorithms expend a gigantic measure of time in mining a massive set of short and average sized itemsets. The greater interest of research in the field of bioinformatics and the abundant data across the variety of domains paved the way for the generation of the high dimensional dataset. These datasets are depicted by an extensive number of features and a smaller number of rows. Colossal closed itemsets are very significant for numerous applications including the field of bioinformatics and are influential during the decision making. Extracting a huge amount of information and knowledge from the high dimensional dataset is a nontrivial task. The existing colossal closed itemsets mining algorithms for the high dimensional dataset are sequential and computationally expensive. Distributed and parallel computing is a good strategy to overcome the inefficiency of the existing sequential algorithm. Balanced Distributed Parallel Frequent Colossal Closed Itemset Mining (BDPFCCIM) algorithm is designed for high dimensional datasets. An efficient closeness checking method to check the closeness of the rowset and an efficient pruning strategy to snip the row enumeration mining search space is enclosed with the proposed BDPFCCIM algorithm. The proposed BDPFCCIM algorithm is the first distributed load balancing algorithm to mine frequent colossal closed itemsets from high dimensional biological datasets. The experimental results demonstrate the efficient performance of the proposed BDPFCCIM algorithm in comparison with the state-of-the-art algorithms.},
  archive      = {J_JPDC},
  author       = {Manjunath K Vanahalli and Nagamma Patil},
  doi          = {10.1016/j.jpdc.2020.05.017},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {136-152},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Distributed load balancing frequent colossal closed itemset mining algorithm for high dimensional dataset},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Blockchain-assisted access for federated smart grid domains:
Coupling and features. <em>JPDC</em>, <em>144</em>, 124–135. (<a
href="https://doi.org/10.1016/j.jpdc.2020.05.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Industry 4.0 technological expansion and the multiple accesses to the diverse Smart Grid domains (power networks, control systems, market, customer premises) entail the need to provide efficient interconnection mechanisms with connection from anywhere, at any time and in anyhow. However, this type of requirement should not only consist in imposing interoperability solutions between entities and domains, but also in searching the way to justify and trace connections (how, when, where, who) for future governance or auditing actions. This paper, therefore, presents a three layer-based interconnection architecture and several interconnection strategies, all of them adapting the traditional policy decision and enforcement approaches together with the blockchain technology to manage reliable and secure connections among entities, processes and critical resources. With this architecture in mind, the paper also analyzes the coupling level of the blockchain technology , and explores which interconnection strategy is more suitable for Smart Grid domains and their control systems.},
  archive      = {J_JPDC},
  author       = {Cristina Alcaraz and Juan E. Rubio and Javier Lopez},
  doi          = {10.1016/j.jpdc.2020.05.012},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {124-135},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Blockchain-assisted access for federated smart grid domains: Coupling and features},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CScript: A distributed programming language for building
mixed-consistency applications. <em>JPDC</em>, <em>144</em>, 109–123.
(<a href="https://doi.org/10.1016/j.jpdc.2020.05.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current programming models only provide abstractions for sharing data under a homogeneous consistency model. It is, however, not uncommon for a distributed application to provide strong consistency for one part of the shared data and eventual consistency for another part. Because mixing consistency models is not supported by current programming models, writing such applications is extremely difficult. In this paper we propose CScript, a distributed object-oriented programming language with built-in support for data replication . At its core are consistent and available replicated objects. CScript regulates the interactions between these objects to avoid subtle inconsistencies that arise when mixing consistency models. Our evaluation compares a collaborative text editor built atop CScript with a state-of-the-art implementation. The results show that our approach is flexible and more memory efficient.},
  archive      = {J_JPDC},
  author       = {Kevin De Porre and Florian Myter and Christophe Scholliers and Elisa Gonzalez Boix},
  doi          = {10.1016/j.jpdc.2020.05.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {109-123},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {CScript: A distributed programming language for building mixed-consistency applications},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Short and long term optimization for micro-object conveying
with air-jet modular distributed system. <em>JPDC</em>, <em>144</em>,
98–108. (<a href="https://doi.org/10.1016/j.jpdc.2020.05.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Smart surface is a new conveying technology composed of a 2D planar surface presenting a matrix of distributed autonomous blocks. Every block contains a micro-electro-mechanical system (MEMS) actuator that controls the transfer of a possible object located above the block to the neighboring blocks, using air-jet forces. The spatial characteristics of the blocks impose some limits on the memory, energy and computation capabilities of the MEMS blocks. On the other hand, the system can reach several thousands of blocks making necessary to propose scalable algorithmic solutions. This paper studies different distributed algorithms to convey an object from an initial to a target position in the smart surface. The conveying policy emphasizes the long term use of the smart surface and the objects conveying efficiency measured by the time of the transfer. The problem stands as an original case of multi-objective Shortest Path problem (MOSP). Original because the quality of a given path is not evaluated by the sum of the weights of its segments, and because the segment weights change according to the used paths as provided by the algorithm itself. Therefore, the efficiency of a given algorithm is assessed on the basis of its performance during a long period of time. We describe here the best way to combine these two objectives and we propose a scalable incremental distributed protocol for objects conveying. The path optimality is adjusted according to the required calculation complexity. The performances of the different algorithmic and modeling variations are analyzed in terms of memory, time, computation and exchanged messages complexity. The obtained results prove the scalability of the algorithm, with linear computational, memory and convergence time complexity, and confirm the improvement of smart surface usage compared to a naive approach . The system lifespan increases of up to 130\% on 40 × 40 smart surface, while the transfer cost (time and energy) is reduced. We show also that the computation time of the path with the incremental algorithm can be significantly reduced without significant degradation of the conveying system performance. For example, in a 40 × 40 smart surface, the number of messages is divided by 4 while the number of conveyed objects is only reduced by a ratio of 4\%.},
  archive      = {J_JPDC},
  author       = {Hakim Mabed and Eugen Dedu},
  doi          = {10.1016/j.jpdc.2020.05.016},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {98-108},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Short and long term optimization for micro-object conveying with air-jet modular distributed system},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Flow mapping on mesh-based deep learning accelerator.
<em>JPDC</em>, <em>144</em>, 80–97. (<a
href="https://doi.org/10.1016/j.jpdc.2020.04.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks have been proposed as an approach for classifying data corresponding to a variety of datasets. Indeed, developments in data diversity and information technology have increased the complexity of deep learning algorithms. Numerous trained models have been proposed for supporting complex algorithms and data detachment with high accuracy. Convolutional operations increase when the convolution depth of neural networks increases. Thus, employing deep convolutional networks is challenging regarding energy consumption, bandwidth, memory requirements, and memory access. Different types of on-chip communication platforms and traffic distribution methods are effective in the improvement of memory access and energy consumption induced by data transfer. Also, dataflow mapping methods have an impressive effect on reducing or increasing delay and energy consumption caused by exchanging data between cores of a communication network. Different methods have been proposed to dataflow mapping on various networks for reducing total hop counts that led to improve performance and cost. Dataflow mapping approach can affect performance improvement of the inference phase in neural networks . This paper proposes various traffic patterns by considering different memory access mechanisms for traffic distribution of a trained AlexNet model on mesh topology. We propose a flow mapping method (FMM) on the mesh to determine the data flow efficiency of different traffic patterns on energy consumption. The FMM reduced energy and total flow by approximately 17.86\% and 34.16\%, respectively, using different traffic patterns. Thus, the FMM improved the performance of AlexNet traffic distribution while the impact on data flow reduced energy consumption.},
  archive      = {J_JPDC},
  author       = {Seyedeh Yasaman Hosseini Mirmahaleh and Midia Reshadi and Nader Bagherzadeh},
  doi          = {10.1016/j.jpdc.2020.04.011},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {80-97},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Flow mapping on mesh-based deep learning accelerator},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sequential and parallel algorithms for all-pair k-mismatch
maximal common substrings. <em>JPDC</em>, <em>144</em>, 68–79. (<a
href="https://doi.org/10.1016/j.jpdc.2020.05.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying long pairwise maximal common substrings among a large set of sequences is a frequently used construct in computational biology, with applications in DNA sequence clustering and assembly. Due to errors made by sequencers, algorithms that can accommodate a small number of differences are of particular interest. Formally, let D D be a collection of n n sequences of total length N N , ϕ ϕ be a length threshold, and k k be a mismatch threshold. The goal is to identify and report all k k -mismatch maximal common substrings of length at least ϕ ϕ over all pairs of strings in D D . Heuristics based on seed-and-extend style filtering techniques are often employed in such applications. However, such methods cannot provide any provably efficient run time guarantees. To this end, we present a sequential algorithm with an expected run time of O ( N log k N + occ ) O(NlogkN+occ) , where occ occ is the output size. We then present a distributed memory parallel algorithm with an expected run time of O N p log N + occ log k N ONplogN+occlogkN using O log k + 1 N Ologk+1N expected rounds of global communications, under some realistic assumptions, where p p is the number of processors. Finally, we demonstrate the performance and scalability of our algorithms using experiments on large high throughput sequencing data.},
  archive      = {J_JPDC},
  author       = {Sriram P. Chockalingam and Sharma V. Thankachan and Srinivas Aluru},
  doi          = {10.1016/j.jpdc.2020.05.018},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {68-79},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Sequential and parallel algorithms for all-pair k-mismatch maximal common substrings},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GPU acceleration of ADMM for large-scale quadratic
programming. <em>JPDC</em>, <em>144</em>, 55–67. (<a
href="https://doi.org/10.1016/j.jpdc.2020.05.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The alternating direction method of multipliers (ADMM) is a powerful operator splitting technique for solving structured convex optimization problems . Due to its relatively low per-iteration computational cost and ability to exploit sparsity in the problem data, it is particularly suitable for large-scale optimization. However, the method may still take prohibitively long to compute solutions to very large problem instances. Although ADMM is known to be parallelizable, this feature is rarely exploited in real implementations. In this paper we exploit the parallel computing architecture of a graphics processing unit (GPU) to accelerate ADMM. We build our solver on top of OSQP, a state-of-the-art implementation of ADMM for quadratic programming . Our open-source CUDA C implementation has been tested on many large-scale problems and was shown to be up to two orders of magnitude faster than the CPU implementation.},
  archive      = {J_JPDC},
  author       = {Michel Schubiger and Goran Banjac and John Lygeros},
  doi          = {10.1016/j.jpdc.2020.05.021},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {55-67},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {GPU acceleration of ADMM for large-scale quadratic programming},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Blockchain-based access control management for decentralized
online social networks. <em>JPDC</em>, <em>144</em>, 41–54. (<a
href="https://doi.org/10.1016/j.jpdc.2020.05.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online Social Networks (OSNs) represent today a big communication channel where users spend a lot of time to share personal data. Unfortunately, the big popularity of OSNs can be compared with their big privacy issues. Indeed, several recent scandals have demonstrated their vulnerability. Decentralized Online Social Networks (DOSNs) have been proposed as an alternative solution to the current centralized OSNs. DOSNs do not have a service provider that acts as central authority and users have more control over their information. Several DOSNs have been proposed during the last years. However, the decentralization of the social services requires efficient distributed solutions for protecting the privacy of users. During the last years the blockchain technology has been applied to Social Networks in order to overcome the privacy issues and to offer a real solution to the privacy issues in a decentralized system. However, in these platforms the blockchain is usually used as a storage, and content is public. In this paper, we propose a manageable and auditable access control framework for DOSNs using blockchain technology for the definition of privacy policies. The resource owner uses the public key of the subject to define auditable access control policies using Access Control List (ACL), while the private key associated with the subject’s Ethereum account is used to decrypt the private data once access permission is validated on the blockchain. We provide an evaluation of our approach by exploiting the Rinkeby Ethereum testnet to deploy the smart contracts . Experimental results clearly show that our proposed ACL-based access control outperforms the Attribute-based access control (ABAC) in terms of gas cost. Indeed, a simple ABAC evaluation function requires 280,000 gas, instead our scheme requires 61,648 gas to evaluate ACL rules .},
  archive      = {J_JPDC},
  author       = {Mohsin Ur Rahman and Barbara Guidi and Fabrizio Baiardi},
  doi          = {10.1016/j.jpdc.2020.05.011},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {41-54},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Blockchain-based access control management for decentralized online social networks},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). EdgeKV: Decentralized, scalable, and consistent storage for
the edge. <em>JPDC</em>, <em>144</em>, 28–40. (<a
href="https://doi.org/10.1016/j.jpdc.2020.05.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing moves the computation closer to the data and the data closer to the user to overcome the high latency communication of cloud computing . Storage at the edge allows data access with high speeds that enable latency-sensitive applications in areas such as autonomous driving and smart grid. However, several distributed services are typically designed for the cloud and building an efficient edge-enabled storage system is challenging because of the distributed and heterogeneous nature of the edge and its limited resources. In this paper, we propose EdgeKV, a decentralized storage system designed for the network edge. EdgeKV offers fast and reliable storage, utilizing data replication with strong consistency guarantees. With a location-transparent and interface-based design, EdgeKV can scale with a heterogeneous system of edge nodes. We implement a prototype of the EdgeKV modules in Golang and evaluate it in both the edge and cloud settings on the Grid’5000 testbed . We utilize the Yahoo! Cloud Serving Benchmark (YCSB) to analyze the system’s performance under realistic workloads. Our evaluation results show that EdgeKV outperforms the cloud storage setting with both local and global data access with an average write response time and throughput improvements of 26\% and 19\% respectively under the same settings. Our evaluations also show that EdgeKV can scale with the number of clients, without sacrificing performance. Finally, we discuss the energy efficiency improvement when utilizing edge resources with EdgeKV instead of a centralized cloud.},
  archive      = {J_JPDC},
  author       = {Karim Sonbol and Öznur Özkasap and Ibrahim Al-Oqily and Moayad Aloqaily},
  doi          = {10.1016/j.jpdc.2020.05.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {28-40},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {EdgeKV: Decentralized, scalable, and consistent storage for the edge},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parallel algorithms for finding connected components using
linear algebra. <em>JPDC</em>, <em>144</em>, 14–27. (<a
href="https://doi.org/10.1016/j.jpdc.2020.04.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding connected components is one of the most widely used operations on a graph. Optimal serial algorithms for the problem have been known for half a century, and many competing parallel algorithms have been proposed over the last several decades under various different models of parallel computation. This paper presents a class of parallel connected-component algorithms designed using linear-algebraic primitives. These algorithms are based on a PRAM algorithm by Shiloach and Vishkin and can be designed using standard GraphBLAS operations. We demonstrate two algorithms of this class, one named LACC for Linear Algebraic Connected Components, and the other named FastSV which can be regarded as LACC’s simplification. With the support of the highly-scalable Combinatorial BLAS library, LACC and FastSV outperform the previous state-of-the-art algorithm by a factor of up to 12x for small to medium scale graphs. For large graphs with more than 50B edges, LACC and FastSV scale to 4K nodes (262K cores) of a Cray XC40 supercomputer and outperform previous algorithms by a significant margin. This remarkable performance is accomplished by (1) exploiting sparsity that was not present in the original PRAM algorithm formulation, (2) using high-performance primitives of Combinatorial BLAS, and (3) identifying hot spots and optimizing them away by exploiting algorithmic insights.},
  archive      = {J_JPDC},
  author       = {Yongzhe Zhang and Ariful Azad and Aydın Buluç},
  doi          = {10.1016/j.jpdc.2020.04.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {14-27},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Parallel algorithms for finding connected components using linear algebra},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FALCON-x: Zero-copy MPI derived datatype processing on
modern CPU and GPU architectures. <em>JPDC</em>, <em>144</em>, 1–13. (<a
href="https://doi.org/10.1016/j.jpdc.2020.05.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the challenges of MPI derived datatype processing and proposes FALCON-X — A Fast and Low-overhead Communication framework for optimized zero-copy intra-node derived datatype communication on emerging CPU/GPU architectures. We quantify various performance bottlenecks such as memory layout translation and copy overheads for highly fragmented MPI datatypes and propose novel pipelining and memoization-based designs to achieve efficient derived datatype communication. In addition, we also propose enhancements to the MPI standard to address the semantic limitations. The experimental evaluations show that our proposed designs significantly improve the intra-node communication latency and bandwidth over state-of-the-art MPI libraries on modern CPU and GPU systems. By using representative application kernels such as MILC, WRF, NAS_MG, Specfem3D, and Stencils on three different CPU architectures and two different GPU systems including DGX-2, we demonstrate up to 5.5x improvement on multi-core CPUs and 120x benefits on DXG-2 GPU system over state-of-the-art designs in other MPI libraries.},
  archive      = {J_JPDC},
  author       = {Jahanzeb Maqbool Hashmi and Ching-Hsiang Chu and Sourav Chakraborty and Mohammadreza Bayatpour and Hari Subramoni and Dhabaleswar K. Panda},
  doi          = {10.1016/j.jpdc.2020.05.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-13},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {FALCON-X: Zero-copy MPI derived datatype processing on modern CPU and GPU architectures},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). BPS: A reliable and efficient pub/sub communication model
with blockchain-enhanced paradigm in multi-tenant edge cloud.
<em>JPDC</em>, <em>143</em>, 167–178. (<a
href="https://doi.org/10.1016/j.jpdc.2020.05.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, with the rapid development of smart city, prevalent pub/sub (publish/subscribe) streaming systems have been increasingly employed as upstream middleware layer in multi-tenant edge clouds, and feed large volume of data gathered from IoT devices of different tenants into downstream systems (e.g., data analytics and warehouse). A shared tenancy model where multiple untrusted applications or tenants utilize the same pub/sub system is generally exploited in edge cloud, which poses crucial challenges including privacy-sensitive data/metadata access threat and critical metadata modification by unauthorized tenants. A centralized monitoring node is invariably adopted in existing security strategies (such as ACL , TLS), which causes the pub/sub streaming model vulnerable to external malicious attacks and single point failure. In this paper, inspired by outstanding features of blockchain including tamper-resistance, decentralization, strong consistency, and traceability, we propose BPS, a general and decentralized Blockchain-enhanced Pub/Sub communication model for multi-tenant edge cloud, to redesign pub/sub system internal security mechanisms. Specifically, by exploiting blockchain technology, BPS can detect the illegal operations and behaviors from both malicious tenants and untrusted publishers or subscribers. BPS directly leverages Merkel Hash Tree (MHT) of blockchain to verify the integrity of critical and confidential metadata. Regarding authorization, BPS introduces smart-contract-enabled fine-grained control over partition topic-classified messages by storing access control list (ACL) into an append-only blockchain ledger. Additionally, an incentive mechanism is employed in BPS to reward honest publishers and subscribers. We implement BPS prototype based on Kafka and EoS blockchain. Our security analysis and extensive experiments demonstrate that BPS outperforms the state-of-the-art pub/sub streaming system Kafka in security with minimal performance overhead.},
  archive      = {J_JPDC},
  author       = {Bobo Huang and Rui Zhang and Zhihui Lu and Yiming Zhang and Jie Wu and Lu Zhan and Patrick C.K. Hung},
  doi          = {10.1016/j.jpdc.2020.05.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {167-178},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {BPS: A reliable and efficient pub/sub communication model with blockchain-enhanced paradigm in multi-tenant edge cloud},
  volume       = {143},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Blockchain and AI amalgamation for energy cloud management:
Challenges, solutions, and future directions. <em>JPDC</em>,
<em>143</em>, 148–166. (<a
href="https://doi.org/10.1016/j.jpdc.2020.05.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the recent years, the Smart Grid (SG) system faces various challenges like the ever-increasing energy demand, the enormous growth of renewable energy sources (RES) with distributed energy generation (EG), the extensive Internet of Things (IoT) devices adaptation, the emerging security threats, and the foremost goal of sustaining the SG stability, efficiency and reliability. To cope up these issues there exists, the energy cloud management (ECM) system, which combines the infrastructure for energy, with intelligent energy usage and value-added services as per consumers demand. To achieve these, efficient demand-side forecasting and secure data transmission are the key factors. The energy management issues pose extreme gravity in finding sustainable solutions by using the blockchain (BC) and Artificial Intelligence (AI). AI-based techniques support various services such as energy load prediction, classification of the consumer, load management, and analysis where the BC provides data immutability and trust mechanism for secure energy management . Therefore, this paper reviews several existing AI-based approaches along with the advantages and challenges of integrating the BC technology and AI in the ECM system. We presented a decentralized AI-based ECM framework for energy management using BC and validate it using a case study. It is shown that how BC and AI can be used to mitigate ECM with security and privacy issues. Finally, we highlighted the open research issues and challenges of the BC-AI-based ECM system.},
  archive      = {J_JPDC},
  author       = {Aparna Kumari and Rajesh Gupta and Sudeep Tanwar and Neeraj Kumar},
  doi          = {10.1016/j.jpdc.2020.05.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {148-166},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Blockchain and AI amalgamation for energy cloud management: Challenges, solutions, and future directions},
  volume       = {143},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Transparent speculation in geo-replicated transactional data
stores. <em>JPDC</em>, <em>143</em>, 129–147. (<a
href="https://doi.org/10.1016/j.jpdc.2020.04.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents Speculative Transaction Replication (STR), a protocol that exploits transparent speculation techniques to enhance performance of geo-distributed, partially replicated transactional data stores. In addition, we define a new consistency model, Speculative Snapshot Isolation (SPSI), a variant of Snapshot Isolation (SI), which shelters applications from the subtle anomalies that arise when using speculative transaction processing techniques. STR provides a form of speculation that is fully transparent for programmers (it does not expose the effects of misspeculations to clients). Since the speculation techniques employed by STR satisfy SPSI, they can be leveraged by application programs in a transparent way, without requiring any source-code modification to applications designed to operate using SI. STR combines two key techniques: speculative reads, which allow transactions to observe pre-committed versions, which can reduce the ‘effective duration’ of pre-commit locks and enhance throughput; Precise Clocks, a novel timestamping mechanism that uses per-item timestamps with physical clocks, which together greatly enhance the probability of successful speculation. We assess STR’s performance on up to nine geo-distributed Amazon EC2 data centers , using both synthetic benchmarks as well as realistic benchmarks (TPC-C and RUBiS). Our evaluation shows that STR achieves throughput gains up to 11 × × and latency reduction up to 10 × × (with respect to non-speculative systems that ensure SI) in workloads characterized by low inter-data center contention. Furthermore, thanks to a self-tuning mechanism that dynamically and transparently enables and disables speculation, STR offers robust performance even when faced with unfavorable workloads that suffer from high misspeculation rates.},
  archive      = {J_JPDC},
  author       = {Zhongmiao Li and Paolo Romano and Peter Van Roy},
  doi          = {10.1016/j.jpdc.2020.04.014},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {129-147},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Transparent speculation in geo-replicated transactional data stores},
  volume       = {143},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Effective replica management for improving reliability and
availability in edge-cloud computing environment. <em>JPDC</em>,
<em>143</em>, 107–128. (<a
href="https://doi.org/10.1016/j.jpdc.2020.04.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multi-replica strategy can create multiple data replicas for the edge cloud system and store them in different DataNodes, which improves data availability and data service quality. However, the storage resources of DataNodes are limited and the user demand for data is time-varying, the unreasonable number of data replicas will cause a high storage burden on the file system or low data service quality. Therefore, the number of data replicas needs to be dynamically adjusted according to the actual situation. Based on this, a dynamic replica creation strategy based on the gray Markov chain is proposed. If the number of replicas needs to be increased, the newly added replicas need to be placed on the DataNodes. Considering the problem of load balancing of the DataNode during replica placement, this paper proposes a replica placement strategy based on the Fast Non-dominated Sorting Genetic algorithm . In addition, considering the problem of data replica synchronization and the data recovery of failed DataNodes in the edge cloud system , this paper proposes a delay-adaptive replica synchronization strategy and a load-balancing based replica recovery strategy. Finally, the experiments prove the effectiveness of the proposed strategies.},
  archive      = {J_JPDC},
  author       = {Chunlin Li and Mingyang Song and Min Zhang and Youlong Luo},
  doi          = {10.1016/j.jpdc.2020.04.012},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {107-128},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Effective replica management for improving reliability and availability in edge-cloud computing environment},
  volume       = {143},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating record linkage costs in distributed environments.
<em>JPDC</em>, <em>143</em>, 97–106. (<a
href="https://doi.org/10.1016/j.jpdc.2020.05.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Record Linkage (RL) is the task of identifying duplicate entities in a dataset or multiple datasets. In the era of Big Data, this task has gained notorious attention due to the intrinsic quadratic complexity of the problem in relation to the size of the dataset. In practice, this task can be outsourced to a cloud service, and thus, a service customer may be interested in estimating the costs of a record linkage solution before executing it. Since the execution time of a record linkage solution depends on a combination of various algorithms, their respective parameter values and the employed cloud infrastructure, in practice it is hard to perform an a priori estimation of infrastructure costs for executing a record linkage task. Besides estimating customer costs, the estimation of record linkage costs is also important to evaluate whether (or not) the application of a set of RL parameter values will satisfy predefined time and budget restrictions. Aiming to tackle these challenges, we propose a theoretical model for estimating RL costs taking into account the main steps that may influence the execution time of the RL task. We also propose an algorithm, denoted as T B F TBF , for evaluating the feasibility of RL parameter values, given a set of predefined customer restrictions. We evaluate the efficacy of the proposed model combined with regression techniques using record linkage results processed in real distributed environments. Based on the experimental results, we show that the employed regression technique has significant influence over the estimated record linkage costs. Moreover, we conclude that specific regression techniques are more suitable for estimating record linkage costs, depending on the evaluated scenario.},
  archive      = {J_JPDC},
  author       = {Dimas Cassimiro Nascimento and Carlos Eduardo Santos Pires and Tiago Brasileiro Araujo and Demetrio Gomes Mestre},
  doi          = {10.1016/j.jpdc.2020.05.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {97-106},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Estimating record linkage costs in distributed environments},
  volume       = {143},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new energy-aware tasks scheduling approach in fog
computing using hybrid meta-heuristic algorithm. <em>JPDC</em>,
<em>143</em>, 88–96. (<a
href="https://doi.org/10.1016/j.jpdc.2020.04.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, large computational problems have beensolved by the distributed environment in which applications are executed in parallel. Also, lately, fog computing or edge computing as a new environment is applied to collect data from the devices and preprocessing is done before sending for main processing in cloud computing . Since one of the crucial issues in such systems is task scheduling , this issue is addressed by considering reducing energy consumption . In this study, an energy-aware method is introduced by using the Dynamic Voltage and Frequency Scaling (DVFS) technique to reduce energy consumption. In addition, in order to construct valid task sequences, a hybrid Invasive Weed Optimization and Culture (IWO-CA) evolutionary algorithm is applied. The experimental results revealed that the proposed algorithm improves some current algorithms in terms of energy consumption.},
  archive      = {J_JPDC},
  author       = {Pejman Hosseinioun and Maryam Kheirabadi and Seyed Reza Kamel Tabbakh and Reza Ghaemi},
  doi          = {10.1016/j.jpdc.2020.04.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {88-96},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A new energy-aware tasks scheduling approach in fog computing using hybrid meta-heuristic algorithm},
  volume       = {143},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An energy efficient service composition mechanism using a
hybrid meta-heuristic algorithm in a mobile cloud environment.
<em>JPDC</em>, <em>143</em>, 77–87. (<a
href="https://doi.org/10.1016/j.jpdc.2020.05.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By increasing mobile devices in technology and human life, using a runtime and mobile services has gotten more complex along with the composition of a large number of atomic services . Different services are provided by mobile cloud components to represent the non-functional properties as Quality of Service (QoS), which is applied by a set of standards. On the other hand, the growth of the energy-source heterogeneity in mobile clouds is an emerging challenge according to the energy saving problem in mobile nodes. In order to mobile cloud service composition as an NP-Hard problem, an efficient selection method should be taken by problem using optimal energy-aware methods that can extend the deployment and interoperability of mobile cloud components. Also, an energy-aware service composition mechanism is required to preserve high energy saving scenarios for mobile cloud components. In this paper, an energy-aware mechanism is applied to optimize mobile cloud service composition using a hybrid Shuffled Frog Leaping Algorithm and Genetic Algorithm (SFGA). Experimental results capture that the proposed mechanism improves the feasibility of the service composition with minimum energy consumption, response time, and cost for mobile cloud components against some current algorithms.},
  archive      = {J_JPDC},
  author       = {Godar J. Ibrahim and Tarik A. Rashid and Mobayode O. Akinsolu},
  doi          = {10.1016/j.jpdc.2020.05.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {77-87},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {An energy efficient service composition mechanism using a hybrid meta-heuristic algorithm in a mobile cloud environment},
  volume       = {143},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MiniChain: A lightweight protocol to combat the UTXO growth
in public blockchain. <em>JPDC</em>, <em>143</em>, 67–76. (<a
href="https://doi.org/10.1016/j.jpdc.2020.05.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current UTXO-based blockchains require the validators to keep the entire ever-growing UTXO set to verify transactions, which is unsuitable for ordinary machines since they occupy a large size of RAM in the long run, resulting in network centralizations. Recently, stateless blockchain technology has been proposed which uses the accumulator to combine the large UTXO set into one short, constant-sized commitment. However, the UTXO commitments in these methods are inefficient since the UXTO set is required dynamic addition and removal of elements as transactions are processed. In this work, we propose MiniChain, which replaces the UTXO set with two append-only data structures : STXO (Spent Transaction Outputs) set and TXO (Transaction Outputs) set. Thus, a valid UTXO must belong to the TXO set, but not in STXO set. Then, we construct a novel STXO commitment and TXO commitment by using a trapdoor-less RSA accumulator and a Merkle Mountain Range (MMR) respectively, greatly increasing the efficiency of accumulator. Besides, we introduce a cache mechanism, by storing the STXOs of latest N N blocks, the transaction proof can be kept alive for a period of time, avoiding constantly recomputing proofs for unaccepted transactions. Our evaluation shows that (i) MiniChain only needs a fixed-size RAM and the disk usage grows very slow since only the block headers are stored; (ii) comparing to the state-of-the-art work, the performance of the accumulator update has been improved from O ( n 2 ) O(n2) to O ( n ) O(n) , enabling MiniChain to support a higher TPS.},
  archive      = {J_JPDC},
  author       = {Huan Chen and Yijie Wang},
  doi          = {10.1016/j.jpdc.2020.05.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {67-76},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {MiniChain: A lightweight protocol to combat the UTXO growth in public blockchain},
  volume       = {143},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybrid-DCA: A double asynchronous approach for stochastic
dual coordinate ascent. <em>JPDC</em>, <em>143</em>, 47–66. (<a
href="https://doi.org/10.1016/j.jpdc.2020.04.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In prior works, stochastic dual coordinate ascent (SDCA) has been parallelized in a multi-core environment where the cores communicate through shared memory, or in a multi-processor distributed memory environment where the processors communicate through message passing. In this paper, we propose a hybrid SDCA framework for multi-core clusters, the most common high performance computing environment that consists of multiple nodes each having multiple cores and its own shared memory. We distribute data across nodes where each node solves a local problem in an asynchronous parallel fashion on its cores, and then the local updates are aggregated via an asynchronous across-node update scheme. The proposed double asynchronous method converges to a global solution for L L -Lipschitz continuous loss functions, and at a linear convergence rate if a smooth convex loss function is used. Extensive empirical comparison has shown that our algorithm scales better than the best known shared-memory methods and runs faster than previous distributed-memory methods. Big datasets, such as one of 280 GB from the LIBSVM repository, cannot be accommodated on a single node and hence cannot be solved by a parallel algorithm . For such a dataset, our hybrid algorithm takes less than 30 s to achieve a duality gap of 1 0 − 5 10−5 on 16 nodes each using 12 cores, which is significantly faster than the best known distributed algorithms, such as CoCoA+, that take more than 160 s on 16 nodes.},
  archive      = {J_JPDC},
  author       = {Soumitra Pal and Tingyang Xu and Tianbao Yang and Sanguthevar Rajasekaran and Jinbo Bi},
  doi          = {10.1016/j.jpdc.2020.04.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {47-66},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Hybrid-DCA: A double asynchronous approach for stochastic dual coordinate ascent},
  volume       = {143},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cost optimization of secure routing with untrusted devices
in software defined networking. <em>JPDC</em>, <em>143</em>, 36–46. (<a
href="https://doi.org/10.1016/j.jpdc.2020.03.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the years, switches and network routers have been compromised frequently, and a lot of vulnerabilities have occurred in network infrastructure. Secure routing ( SR ) is one of the challenges that currently exists in computer networks. Software-defined networks ( SDN ) are designed by assuming that routers or switches are trustworthy. In SDN , untrusted devices have resulted in security issues such as traffic analysis, failure to receive information, packet change and removal, inaccurate routing, and network downtime. Using encryption methods is a possible solution to deal with some of these problems, but it requires additional infrastructure as well as significant overhead at runtime. One of the most trusted routing methods is through replicated devices (switches or routers). Recently we have seen less attention given to the number of replicated devices in SR . In this paper, the problem of SR is converted into a multi-objective optimization problem, considering the reliability of different manufacturers and SR with untrusted devices is performed. To this end, a mathematical model is provided to study the objectives of maximum reliability and cost minimization . The NSGA-II algorithm is applied to determine the optimal number of replicated devices in order to minimize the cost of implementing SR in spite of the presence of untrusted devices in SDN . Our simulation results show that our proposed method compared to the base method (without considering optimization) decreases implementation cost by 27\% and increases the reliability from 70\% to 93.2\%.},
  archive      = {J_JPDC},
  author       = {Abbas Yazdinejad and Reza M. Parizi and Ali Dehghantanha and Gautam Srivastava and Senthilkumar Mohan and Abedallah M. Rababah},
  doi          = {10.1016/j.jpdc.2020.03.021},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {36-46},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Cost optimization of secure routing with untrusted devices in software defined networking},
  volume       = {143},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). IoTSim-SDWAN: A simulation framework for interconnecting
distributed datacenters over software-defined wide area network
(SD-WAN). <em>JPDC</em>, <em>143</em>, 17–35. (<a
href="https://doi.org/10.1016/j.jpdc.2020.04.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software-defined networking (SDN) has evolved as an approach that allows network administrators to program and initialize, control, change and manage networking components (mostly at L2-L3 layers) of the OSI model. SDN is designed to address the programmability shortcomings of traditional networking architectures commonly used in cloud datacenters (CDC). Deployment of SDN solutions have demonstrated significant improvements in areas such as flow optimization and bandwidth allocation in a CDC. However, the benefits are significantly less explored when considering Software-Defined Wide Area Networks (SD-WAN) architectures in the context of delivering solutions by networking multiple CDCs. To support the testing and bench-marking of data-driven applications that rely on data ingestion and processing (e.g., Smart Energy Cloud, Content Delivery Networks) across multiple cloud datacenters, this paper presents the simulator, IoTSim-SDWAN. To the best of our knowledge, IoTSim-SDWAN is the first simulator that facilitates the modeling, simulating, and evaluating of new algorithms, policies, and designs in the context of SD-WAN ecosystems and SDN-enabled multiple cloud datacenters. Finally, IoTSim-SDWAN simulator is evaluated for network performance and energy to illustrate the difference between classical WAN and SD-WAN environments. The obtained results show that SD-WAN surpasses the classical WAN in terms of accelerating traffic flows and reducing power consumption .},
  archive      = {J_JPDC},
  author       = {Khaled Alwasel and Devki Nandan Jha and Eduardo Hernandez and Deepak Puthal and Mutaz Barika and Blesson Varghese and Saurabh Kumar Garg and Philip James and Albert Zomaya and Graham Morgan and Rajiv Ranjan},
  doi          = {10.1016/j.jpdc.2020.04.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {17-35},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {IoTSim-SDWAN: A simulation framework for interconnecting distributed datacenters over software-defined wide area network (SD-WAN)},
  volume       = {143},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). BTNC: A blockchain based trusted network connection protocol
in IoT. <em>JPDC</em>, <em>143</em>, 1–16. (<a
href="https://doi.org/10.1016/j.jpdc.2020.04.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Along with the rapid growth of the size and complexity of Internet of Things (IoT), the security of terminal devices has increasingly become a focus. In order to ensure the security of terminals, the trusted network connect (TNC) could realize not only the user authentication but also the platform attestation during the network access process. However, the existing TNC infrastructure is based on a centralized architecture, which is not suitable for distributed services. To address this problem, we present a blockchain-based TNC protocol named BTNC to ensure the reliability of terminals in IoT. Due to the decentralization, trustlessness, trackability, and immutability features of blockchain , BTNC is able to verify the security of terminal devices in IoT networks. First, we come up with some threats, including unauthorized user, illegal platform and platform replacement attack, then correspondingly define the security goals of our scheme. Second, combining key exchange protocol based on blockchain and D–H PN protocol included in TNC specification, we propose a blockchain-based trusted network connection protocol, which realizes mutual user authentication , platform attestation and trust network access by cryptography among terminals in IoT. Third, we make a security analysis in the PCL mode and conclude that our protocol can resist the attacks above. Finally, the performance overheads caused by our scheme are evaluated and the experiments show that it is efficient and feasible for different kinds of terminals in IoT.},
  archive      = {J_JPDC},
  author       = {Junwei Zhang and Zhuzhu Wang and Lei Shang and Di Lu and Jianfeng Ma},
  doi          = {10.1016/j.jpdc.2020.04.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-16},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {BTNC: A blockchain based trusted network connection protocol in IoT},
  volume       = {143},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Selective bypassing and mapping for heterogeneous
applications on GPGPUs. <em>JPDC</em>, <em>142</em>, 106–118. (<a
href="https://doi.org/10.1016/j.jpdc.2020.04.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern GPGPU supports executing multiple tasks with different run time characteristics and resource utilization. Having an efficient execution and resource management policy has been shown to be a critical performance factor when handling the concurrent execution of tasks with different run time behavior. Previous policies either assign equal resources to disparate tasks or allocate resources based on static or standalone behavior profiling. Treating tasks equally cannot efficiently utilize the system resources, while the standalone profiling ignores the correlated impact when running tasks concurrently and could hint incorrect task behavior. This paper addresses the above drawbacks and proposes a heterogeneity aware Selective Bypassing and Mapping (SBM) to manage both computing and cache resources for multiple tasks in a fine-grain manner. The light-weight run time profiling of SBM properly characterizes the disparate behavior of the concurrently executed multiple tasks, and selectively applies suited cache management and workgroup mapping policies to each task. When compared with the previous coarse-grained policies, SBM can achieve an average of 138\% and up to 895\% performance enhancement. When compared with the state-of-art fine-grained policy, SBM can achieve an average of 58\% and up to 378\% performance enhancement.},
  archive      = {J_JPDC},
  author       = {Moustafa Emara and Bo-Cheng Lai},
  doi          = {10.1016/j.jpdc.2020.04.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {106-118},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Selective bypassing and mapping for heterogeneous applications on GPGPUs},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Artificial intelligence inspired energy and spectrum aware
cluster based routing protocol for cognitive radio sensor networks.
<em>JPDC</em>, <em>142</em>, 90–105. (<a
href="https://doi.org/10.1016/j.jpdc.2020.04.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A Cognitive Radio Sensor Network (CRSN) is a distributed network of sensor nodes , which senses event signals and collaboratively communicates over dynamically available spectrum bands in a multi-hop mode. All nodes participating in CRSN have to be cognitive of the network environment and autonomous in decision making for resolving issues related to throughput maximization, delay, and energy minimization . Clustering in CRSN is proven to tackle such issues and enlarges the network’s lifetime. However, the existing clustering algorithms designed for WSNs do not consider the CR functionalities and challenges, and CR based networks work on the assumption of unlimited energy. This paper proposes an energy and spectrum aware unequal cluster based routing (ESUCR) protocol intending to resolve the issues of clustering and routing in CRSN. In ESUCR, cluster formation is mainly performed considering the residual energy of the secondary users (SUs) and relative spectrum awareness such that the common data channels for clusters are selected based on the appearance probability of PUs. ESUCR performs energy-efficient channel sensing by deciding the channel state with the statistic previous channel states. The premature death of cluster heads (CHs) is minimized by selecting and rotating the CHs based on intra-cluster channel stability, energy, distance, and neighbor connectivity. During event detection, ESUCR performs energy-efficient data routing towards the sink node by employing hop by hop forwarding through the CHs and primary/secondary gateways. The performance of the proposed ESUCR protocol is proved through extensive simulations and compared to those of the state-of-the-art protocols under a dynamic spectrum-aware data transmission environment.},
  archive      = {J_JPDC},
  author       = {Thompson Stephan and Fadi Al-Turjman and K. Suresh Joseph and Balamurugan Balusamy and Sweta Srivastava},
  doi          = {10.1016/j.jpdc.2020.04.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {90-105},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Artificial intelligence inspired energy and spectrum aware cluster based routing protocol for cognitive radio sensor networks},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel routing verification approach based on blockchain
for inter-domain routing in smart metropolitan area networks.
<em>JPDC</em>, <em>142</em>, 77–89. (<a
href="https://doi.org/10.1016/j.jpdc.2020.04.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, with the continuous expansion of metropolitan area networks , the routing security problem has become more and more serious. In particular, promise-violating attack to inter-domain routing protocol is one of the most difficult attacks to defend, which always leads to serious consequences, such as maliciously attracting traffic and disrupting the network. To deal with such attack, current research generally adopts routing verification. However, it can only detect attacks violating a specific routing policy triggered by one malicious node, and no research has yet solved the problem caused by multiple collusion nodes. In this paper, we propose BRVM, a blockchain-based routing verification model, to address the issue that violating the shortest AS Path policy. The main idea of BRVM is to construct a route proof chain to verify whether a route violates the policy with the help of the blockchain technology. The precondition that avoiding the collusion attack is that the proportion of the malicious verification nodes is lower than the fault tolerance rate of the consensus algorithm . Then, we prove the correctness of BRVM in theory, and implement a prototype based on Quagga and Hyperledger Fabric. Some experiments on this prototype show that BRVM can indeed solve the promise-violating problem caused by multiple collusion nodes, and about 15.5\% faster in performance compared with SPIDeR.},
  archive      = {J_JPDC},
  author       = {Yaping Liu and Shuo Zhang and Haojin Zhu and Peng-Jun Wan and Lixin Gao and Yaoxue Zhang and Zhihong Tian},
  doi          = {10.1016/j.jpdc.2020.04.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {77-89},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A novel routing verification approach based on blockchain for inter-domain routing in smart metropolitan area networks},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A distributed algorithm for a maximal 2-packing set in halin
graphs. <em>JPDC</em>, <em>142</em>, 62–76. (<a
href="https://doi.org/10.1016/j.jpdc.2020.03.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose Maximal-2-Packing-Halin , a distributed algorithm that finds a maximal 2-packing set in undirected non-geometric Halin graphs of order n n in linear time. First, this algorithm finds an external face of the input graph through the application of graph-reduction rules. Second, each vertex determines if it belongs to a maximal 2-packing set by applying a set of vertex-coloring rules.},
  archive      = {J_JPDC},
  author       = {Alejandro Flores-Lamas and José Alberto Fernández-Zepeda and Joel Antonio Trejo-Sánchez},
  doi          = {10.1016/j.jpdc.2020.03.016},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {62-76},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A distributed algorithm for a maximal 2-packing set in halin graphs},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient task pruning mechanism to improve robustness of
heterogeneous computing systems. <em>JPDC</em>, <em>142</em>, 46–61. (<a
href="https://doi.org/10.1016/j.jpdc.2020.03.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In heterogeneous distributed computing (HC) systems, diversity can exist in both computational resources and arriving tasks. In an inconsistently heterogeneous computing system , task types have different execution times on heterogeneous machines. A method is required to map arriving tasks to machines based on machine availability and performance, maximizing the number of tasks meeting deadlines (defined as robustness ). For tasks with hard deadlines ( e.g., those in live video streaming), tasks that miss their deadlines are dropped. The problem investigated in this research is maximizing the robustness of an oversubscribed HC system. A way to maximize this robustness is to prune ( i.e., defer or drop) tasks with low probability of meeting their deadlines to increase the probability of other tasks meeting their deadlines. In this paper, we first provide a mathematical model to estimate a task’s probability of meeting its deadline in the presence of task dropping. We then investigate methods for engaging probabilistic dropping. We propose methods to dynamically determine task dropping and deferring threshold probabilities. Next, we develop a pruning system and a pruning-aware mapping heuristic, which we extend to engender fairness across various task types. We present the pruning mechanism as an independent component that can be applied to any mapping heuristic to improve the system robustness. To reduce overhead of the pruning mechanism , we propose approximation methods that remarkably reduce the number of mathematical calculations and improve the practicality of deploying the mechanism in heterogeneous or even homogeneous computing systems. We show the cost and energy gains of the pruning mechanism. Simulation results, harnessing a selection of mapping heuristics, show efficacy of the pruning mechanism in improving robustness (on average by ≃ ≃ 22\%) and cost in an oversubscribed HC system by up to ≃ ≃ 33\%.},
  archive      = {J_JPDC},
  author       = {Chavit Denninnart and James Gentry and Ali Mokhtari and Mohsen Amini Salehi},
  doi          = {10.1016/j.jpdc.2020.03.018},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {46-61},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Efficient task pruning mechanism to improve robustness of heterogeneous computing systems},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybridization of firefly and improved multi-objective
particle swarm optimization algorithm for energy efficient load
balancing in cloud computing environments. <em>JPDC</em>, <em>142</em>,
36–45. (<a href="https://doi.org/10.1016/j.jpdc.2020.03.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Load balancing, in Cloud Computing (CC) environment, is defined as the method of splitting workloads and computing properties. It enables the enterprises to manage workload demands or application demands by distributing the resources among computers, networks or servers. In this research article, a new load balancing algorithm is proposed as a hybrid of firefly and Improved Multi-Objective Particle Swarm Optimization (IMPSO) technique, abbreviated as FIMPSO. This technique deploys Firefly (FF) algorithm to minimize the search space where as the IMPSO technique is implemented to identify the enhanced response. The IMPSO algorithm works by selecting the global best ( gbest ) particle with a small distance of point to a line. With the application of minimum distance from a point to a line, the gbest particle candidates could be elected. The proposed FIMPSO algorithm achieved effective average load for making and enhanced the essential measures like proper resource usage and response time of the tasks. The simulation outcome showed that the proposed FIMPSO model exhibited an effective performance when compared with other methods. From the simulation outcome , it is understood that the FIMPSO algorithm yielded an effective result with the least average response time of 13.58ms, maximum CPU utilization of 98\%, memory utilization of 93\%, reliability of 67\% and throughput of 72\% along with a make span of 148, which was superior to all the other compared methods.},
  archive      = {J_JPDC},
  author       = {A. Francis Saviour Devaraj and Mohamed Elhoseny and S. Dhanasekaran and E. Laxmi Lydia and K. Shankar},
  doi          = {10.1016/j.jpdc.2020.03.022},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {36-45},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Hybridization of firefly and improved multi-objective particle swarm optimization algorithm for energy efficient load balancing in cloud computing environments},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient and secure flash-based gaming CAPTCHA.
<em>JPDC</em>, <em>142</em>, 27–35. (<a
href="https://doi.org/10.1016/j.jpdc.2020.03.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growth of connectivity to smart grids, new applications, and the changing interaction between customer and energy clouds, clouds are more vulnerable to denial-of-service attacks. Efficient detection methods are required to authenticate, detect and control attackers. Completely Automated Public Turing test to tell Computers and Humans Apart , CAPTCHA, is one efficient tool to thwart denial of service attacks . The server presents the user with a client puzzle to solve in order to gain access to the service or website. The puzzle should be hard enough for computers, but easy for humans to solve. Several methods have been suggested including the popular image-based, as well as video-based, and text-based CAPTCHAs. In this paper, we present a new Flash-based gaming CAPTCHA to differentiate bots from humans. We propose a drag and drop client puzzle where the user will play a simple game to answer a visual question. Our method turns out to be convenient, easy for users and challenging for bots. Additionally, it has gaming aspect, which makes it interesting to users of all age groups.},
  archive      = {J_JPDC},
  author       = {Monther Aldwairi and Suaad Mohammed and Megana Lakshmi Padmanabhan},
  doi          = {10.1016/j.jpdc.2020.03.020},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {27-35},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Efficient and secure flash-based gaming CAPTCHA},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Load balancing of energy cloud using wind driven and firefly
algorithms in internet of everything. <em>JPDC</em>, <em>142</em>,
16–26. (<a href="https://doi.org/10.1016/j.jpdc.2020.02.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The smart applications dominating the planet in the present day and age, have innovatively progressed to deploy Internet of Things (IoT) based systems and related infrastructures in all spectrums of life. Since, variety of applications are being developed using this IoT paradigm, there is an immense necessity for storing data, processing them to get meaningful information and render suitable services to the end-users. The “thing” in this decade is not only a smart sensor or a device; it can be any physical or household object, a smart device or a mobile. With the ever increasing rise in population and smart device usage in every sphere of life, when all of such “thing”s generates data, there is a chance of huge data traffic in the internet. This could be handled only by integrating “Internet of Everything (IoE)” paradigm with a completely diversified technology — Cloud Computing . In order to handle this heavy flow of data traffic and process the same to generate meaningful information, various services in the global environment are utilized. Hence the primary focus revolves in integrating these two diversified paradigm shifts to develop intelligent information processing systems . Energy Efficient Cloud Based Internet of Everything (EECloudIoE) architecture is proposed in this study, which acts as an initial step in integrating these two wide areas thereby providing valuable services to the end users. The utilization of energy is optimized by clustering the various IoT network using Wind Driven Optimization Algorithm . Next, an optimized Cluster Head (CH) is chosen for each cluster, using Firefly Algorithm resulting in reduced data traffic in comparison to other non-clustering schemes. The proposed clustering of IoE is further compared with the widely used state of the art techniques like Artificial Bee Colony (ABC) algorithm, Genetic Algorithm (GA) and Adaptive Gravitational Search algorithm (AGSA). The results justify the superiority of the proposed methodology outperforming the existing approaches with an increased life-time and reduction in traffic.},
  archive      = {J_JPDC},
  author       = {Swarna Priya R.M. and Sweta Bhattacharya and Praveen Kumar Reddy Maddikunta and Siva Rama Krishnan Somayaji and Kuruva Lakshmanna and Rajesh Kaluri and Aseel Hussien and Thippa Reddy Gadekallu},
  doi          = {10.1016/j.jpdc.2020.02.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {16-26},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Load balancing of energy cloud using wind driven and firefly algorithms in internet of everything},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the monotonic lagrangian grid as antecedent to the
neighborhood grid. <em>JPDC</em>, <em>142</em>, 13–15. (<a
href="https://doi.org/10.1016/j.jpdc.2020.04.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We note that a recently described data structure , the Neighborhood Grid, is equivalent to a data structure developed in the mid-1980s called the Monotonic Lagrangian Grid (MLG). The MLG was originally developed to support high-performance molecular and fluid dynamics simulations on both supercomputer and vector processing architectures and still finds use in those and other areas. In this paper we emphasize that the rediscovery of the MLG offers benefits to users of the Neighborhood Grid in the form of an existing literature with results relevant to its efficient implementation in various contexts while users of the MLG similarly benefit from new theoretical results obtained for the Neighborhood grid.},
  archive      = {J_JPDC},
  author       = {Jeffrey Uhlmann},
  doi          = {10.1016/j.jpdc.2020.04.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {13-15},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {On the monotonic lagrangian grid as antecedent to the neighborhood grid},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GuardHealth: Blockchain empowered secure data management and
graph convolutional network enabled anomaly detection in smart
healthcare. <em>JPDC</em>, <em>142</em>, 1–12. (<a
href="https://doi.org/10.1016/j.jpdc.2020.03.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paradox between the dramatic development of medical data privacy demand and years of bureaucratic regulation has slowed innovation for electronic medical records (EMRs). We are at a historical point for such innovation to prompt patients data autonomy. In this paper, we propose GuardHealth: an efficient, secure and decentralized Blockchain system for data privacy preserving and sharing. GuardHealth manages confidentiality, authentication, data preserving and data sharing when handling sensitive information . We exploit consortium Blockchain and smart contract to achieve secure data storage and sharing, which prevents data sharing without permission. A trust model is utilized for precisely managing trust of users with the implementation of the state-of-art Graph Neural Network (GNN) for malicious node detection. Security analysis and experiment results show that the proposed scheme is applicable for smart healthcare system.},
  archive      = {J_JPDC},
  author       = {Ziyu Wang and Nanqing Luo and Pan Zhou},
  doi          = {10.1016/j.jpdc.2020.03.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-12},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {GuardHealth: Blockchain empowered secure data management and graph convolutional network enabled anomaly detection in smart healthcare},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CloudFNF: An ontology structure for functional and
non-functional features of cloud services. <em>JPDC</em>, <em>141</em>,
143–173. (<a href="https://doi.org/10.1016/j.jpdc.2020.03.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, cloud computing becomes one of the main orientations of many researchers and companies in the IT area. Therefore, a huge number of cloud services have been developed. Because of the diversity and heterogeneity in providing these services, it is urgently needed to develop a unified cloud ontology. Such ontology can classify these services appropriately and participate as a mapping layer to present such services in a unified description format. Although many studies have been conducted to build cloud ontologies, they have adopted the cloud service layers-based structure. On the other hand, the existing cloud services may involve functionalities from different layers due to the continual increase in the complexity of customer demands. Unfortunately, there are no clear relations to organize the interventions among these layers. Therefore, such services may be difficult to be classified into a specific cloud service layer. Additionally, the layers-based structure of ontologies represents an obstacle to address important issues, such as cloud service recommendation . Despite there are few cloud service functionality-based cloud ontologies, they suffer from many overlaps, lack of semantic relations , or poor granularity of concepts. Also, all the existing cloud ontologies (i.e., layers-based and functionality-based) lack important criteria, such as completeness, consistency, conciseness , clarity, preciseness, and granularity . In this paper, a comprehensive cloud ontology called CloudFNF has been proposed to overcome such drawbacks. According to the structure of the proposed ontology, cloud services are classified as functionality-based instead of layers-based. Also, non-functional features of cloud services (i.e., configuration and QoS features) are considered to enable services of the same functionalities to be ranked efficiently. Based on our previously suggested cloud ontology evaluation framework, our proposed cloud ontology has been evaluated compared to the most related cloud ontologies. The evaluation results show that the proposed CloudFNF ontology outperforms the other ontologies.},
  archive      = {J_JPDC},
  author       = {Mustafa M. Al-Sayed and Hesham A. Hassan and Fatma A. Omara},
  doi          = {10.1016/j.jpdc.2020.03.019},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {143-173},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {CloudFNF: An ontology structure for functional and non-functional features of cloud services},
  volume       = {141},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distributed on-demand clustering algorithm for lifetime
optimization in wireless sensor networks. <em>JPDC</em>, <em>141</em>,
129–142. (<a href="https://doi.org/10.1016/j.jpdc.2020.03.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless Sensor Networks (WSNs) play a significant role in Internet of Things (IoT) to provide cost effective solutions for various IoT applications, e.g., wildlife habitat monitoring, but are often highly resource constrained. Hence, preserving energy (or, battery power) of sensor nodes and maximizing the lifetime of WSNs is extremely important. To maximize the lifetime of WSNs, clustering is commonly considered as one of the efficient technique. In a cluster, the role of individual sensor nodes changes to minimize energy consumption, thereby prolonging network lifetime. This paper addresses the problem of lifetime maximization in WSNs by devising a novel clustering algorithm where clusters are formed dynamically. Specifically, we first analyze the network lifetime maximization problem by balancing the energy consumption among cluster heads . Based on the analysis, we provide an optimal clustering technique , in which the cluster radius is computed using alternating direction method of multiplier . Next, we propose a novel On-demand, oPTImal Clustering (OPTIC) algorithm for WSNs. Our cluster head election procedure is not periodic, but adaptive based on the dynamism of the occurrence of events. This on-demand execution of OPTIC aims to significantly reduce computation and message overheads. Experimental results demonstrate that OPTIC improves the energy balance by more than 18\% and network lifetime by more than 19\% compared to a non-clustering and two clustering solutions in the state-of-the-art.},
  archive      = {J_JPDC},
  author       = {Amrita Ghosal and Subir Halder and Sajal K. Das},
  doi          = {10.1016/j.jpdc.2020.03.014},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {129-142},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Distributed on-demand clustering algorithm for lifetime optimization in wireless sensor networks},
  volume       = {141},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-efficiency parallelism solution for a multiview
high-efficiency video coding decoder. <em>JPDC</em>, <em>141</em>,
115–128. (<a href="https://doi.org/10.1016/j.jpdc.2020.03.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Thus far, Multiview High-Efficiency Video Coding (MV-HEVC) can only use a central processing unit (CPU) to perform decompression on a personal computer (PC) or workstation. Because MV-HEVC is much more complex than High-Efficiency Video Coding (HEVC), decompressors need higher parallelism to decompress in real time. Therefore, this study presents a parallel method based on MV-HEVC. Inter-view complete parallelism is realized according to the dependent relationship between other MV-HEVC views, and a search range is not required to avoid the data dependence between frames. Based on the dependencies of each task in MV-HEVC, an advanced wavefront parallel processing method is proposed to achieve higher intra-frame parallelism. The parallel structure of the proposed method is compatible with that of the single-instruction multiple-data acceleration method . The results showed that the proposed method can decompress MV-HEVC with 20 threads in real time for 1088p video with three views.},
  archive      = {J_JPDC},
  author       = {Wei Liu and Wei Li and Yong Beom Cho},
  doi          = {10.1016/j.jpdc.2020.03.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {115-128},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {High-efficiency parallelism solution for a multiview high-efficiency video coding decoder},
  volume       = {141},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Predicting and reining in application-level slowdown on
spatial multitasking GPUs. <em>JPDC</em>, <em>141</em>, 99–114. (<a
href="https://doi.org/10.1016/j.jpdc.2020.03.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting performance degradation of a GPU application at co-location on a spatial multitasking GPU without prior application knowledge is essential in public Clouds. Prior work mainly targets CPU co-location, and is inaccurate and/or inefficient for predicting performance of applications at co-location on spatial multitasking GPUs. Our investigation shows that hardware event statistics caused by co-located applications strongly correlate with their slowdowns. Based on this observation, we present Themis with a kernel slowdown model (Themis-KSM), which performs precise and efficient online application slowdown prediction without prior application knowledge. The kernel slowdown model is trained offline. When new applications co-run, Themis-KSM collects event statistics and predicts their slowdowns simultaneously. In addition, we also propose a two-stage slowdown prediction mechanism (Themis-TSP) for real-system GPUs without any hardware modification. Our evaluation shows that Themis has negligible runtime overhead, and both Themis-KSM and Themis-TSP can precisely predict application-level slowdown with prediction error smaller than 9.5\% and 12.8\%, respectively. Based on Themis, we also implement an SM allocation engine to rein in application slowdown at co-location. Case studies show that the engine successfully enforces fair sharing and QoS.},
  archive      = {J_JPDC},
  author       = {Mengze Wei and Wenyi Zhao and Quan Chen and Hao Dai and Jingwen Leng and Chao Li and Wenli Zheng and Minyi Guo},
  doi          = {10.1016/j.jpdc.2020.03.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {99-114},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Predicting and reining in application-level slowdown on spatial multitasking GPUs},
  volume       = {141},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cost effective dynamic data placement for efficient access
of social networks. <em>JPDC</em>, <em>141</em>, 82–98. (<a
href="https://doi.org/10.1016/j.jpdc.2020.03.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Social networks boast a huge number of worldwide users who join, connect, and publish various content, often very large, e.g. videos, images etc. For such very large-scale data storage, data replication using geo-distributed cloud services with virtually unlimited capabilities are suitable to fulfill the users’ expectations, such as low latency when accessing their and their friends’ data. However, service providers ideally want to spend as little as possible on replicating users’ data. Moreover, social networks have a dynamic nature and thus replicas need to be adaptable according to the environment, users’ behaviors, social network topology , and workload at runtime. Hence, it is not only crucial to have an optimized data placement and request distribution – meeting individual users’ acceptable latency requirements while incurring minimum cost for service providers – but the data placement must be adapted based on changes in the social network to keep it efficient and effective over time. In this paper, we model data placement as a dynamic set cover problem and propose a novel approach to solve this problem. We have run several experiments using two large-scale, open Facebook and Gowala datasets and real latencies derived from Amazon cloud datacenters to demonstrate our novel strategy’s efficiency and effectiveness.},
  archive      = {J_JPDC},
  author       = {Hourieh Khalajzadeh and Dong Yuan and Bing Bing Zhou and John Grundy and Yun Yang},
  doi          = {10.1016/j.jpdc.2020.03.013},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {82-98},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Cost effective dynamic data placement for efficient access of social networks},
  volume       = {141},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Decomposition of augmented cubes into regular connected
pancyclic subgraphs. <em>JPDC</em>, <em>141</em>, 74–81. (<a
href="https://doi.org/10.1016/j.jpdc.2020.03.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the problem of decomposing the augmented cube A Q n AQn into two spanning, regular, connected and pancyclic subgraphs. We prove that for n ≥ 4 n≥4 and 2 n − 1 = n 1 + n 2 2n−1=n1+n2 with n 1 , n 2 ≥ 2 , n1,n2≥2, A Q n AQn can be decomposed into two spanning subgraphs H 1 H1 and H 2 H2 such that H i Hi is n i ni -regular and n i ni -connected for i = 1 , 2 i=1,2 . Moreover, H i Hi is 4-pancyclic if n i ≥ 3 . ni≥3.},
  archive      = {J_JPDC},
  author       = {S.A. Kandekar and Y.M. Borse and B.N. Waphare},
  doi          = {10.1016/j.jpdc.2020.03.017},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {74-81},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Decomposition of augmented cubes into regular connected pancyclic subgraphs},
  volume       = {141},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parallel quicksort algorithm on OTIS hyper hexa-cell
optoelectronic architecture. <em>JPDC</em>, <em>141</em>, 61–73. (<a
href="https://doi.org/10.1016/j.jpdc.2020.03.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last two decades, widespread attention has been paid in parallelizing algorithms, such as sorting and searching, for computationally intensive applications. Several interconnection networks were demonstrated for that purpose; such as hypercube and Hyper Hexa-Cell (HHC). However, the leverage of optical links in Optical Transpose Interconnection Systems (OTIS) stimulates the researchers toward developing optoelectronic architectures that utilize the optical links in addition to the electronic links; such as, OTIS-hypercube and OTIS-HHC. In this paper, we introduced a parallel quicksort algorithm for the OTIS-HHC optoelectronic architecture. This algorithm has been evaluated analytically and by simulation in terms of run time, speedup, and efficiency, where a set of simulation runs were carried out on different input data distributions types with various sizes. Thus, simulation results supported the analytical evaluation and met the expectations in which they showed good performance in terms of speedup and efficiency.},
  archive      = {J_JPDC},
  author       = {Aryaf Al-Adwan and Rawan Zaghloul and Basel A. Mahafzah and Ahmad Sharieh},
  doi          = {10.1016/j.jpdc.2020.03.015},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {61-73},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Parallel quicksort algorithm on OTIS hyper hexa-cell optoelectronic architecture},
  volume       = {141},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Preface: Security &amp; privacy in social big data.
<em>JPDC</em>, <em>141</em>, 59–60. (<a
href="https://doi.org/10.1016/j.jpdc.2020.03.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This special issue assembles a set of 11 papers, which provide deep research results to report the advance of security and privacy technology in social big data. This preface provides overview of all articles in the viewpoint set.},
  archive      = {J_JPDC},
  author       = {Qin Liu and Md Zakirul Alam Bhuiyan and Jiankun Hu and Jie Wu},
  doi          = {10.1016/j.jpdc.2020.03.011},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {59-60},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Preface: Security &amp; privacy in social big data},
  volume       = {141},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MalFCS: An effective malware classification framework with
automated feature extraction based on deep convolutional neural
networks. <em>JPDC</em>, <em>141</em>, 49–58. (<a
href="https://doi.org/10.1016/j.jpdc.2020.03.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying the family of malware can determine their malicious intent and attack patterns, which helps to efficiently analyze large numbers of malware variants. Methods based on traditional machine learning often require a lot of time and resources in feature engineering. Virtually all existing static analysis methods based on malware visualization are derived from grayscale images , while a single low-order feature representation may be detrimental to discovering hidden features in a malware family. Based on these problems, this paper proposes an effective malware classification framework (MalFCS) based on malware visualization and automated feature extraction. MalFCS includes mainly three modules: malware visualization, feature extraction, and classification. First, we visualize malware binaries as entropy graphs based on structural entropy. Second, we present a feature extractor based on deep convolutional neural networks to extract patterns shared by a family from entropy graphs automatically. Finally, we propose an SVM classifier to classify malware based on the extracted features. We evaluate the proposed MalFCS over two widely studied benchmark datasets, i.e., Malimg and Microsoft. Experimental results show that compared with the state-of-the-art methods, MalFCS can obtain excellent classification performance with accuracy of 0.997 and 1, respectively, achieving the state-of-the-art performance.},
  archive      = {J_JPDC},
  author       = {Guoqing Xiao and Jingning Li and Yuedan Chen and Kenli Li},
  doi          = {10.1016/j.jpdc.2020.03.012},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {49-58},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {MalFCS: An effective malware classification framework with automated feature extraction based on deep convolutional neural networks},
  volume       = {141},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accelerating fingerprint identification using FPGA for
large-scale applications. <em>JPDC</em>, <em>141</em>, 35–48. (<a
href="https://doi.org/10.1016/j.jpdc.2020.03.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fingerprint-based human authentication has shown great potential for civil, forensic and corporate security applications in recent years. For large-scale databases, the complexity of the identification system increases and implementing these systems on general-purpose sequential computing devices becomes challenging. Field Programmable Gate Array (FPGA) has demonstrated to be an efficient tool for the acceleration of computationally challenging applications by utilizing parallelism in the computations. In this study, an FPGA-based hardware accelerator is exploited to propose a fast and robust fingerprint identification solution that is based on a generalized minutiae neighbor based encoding and matching algorithm . The proposed FPGA implementation employs the Distributed RAM resources efficiently by using them as look-up tables for matching the encoded minutiae features. The proposed FPGA-based fingerprint matching system has the potential to match 2.75 million fingerprints per second while maintaining a low error rate. The proposed system can be deemed as an effective solution for Automated Fingerprint Identification Systems (AFIS) for large-scale applications.},
  archive      = {J_JPDC},
  author       = {Mohsin Shafiq and Imtiaz A. Taj and Mubeen Ghafoor and Syed Ali Tariq and Assad Abbas and Albert Y. Zomaya},
  doi          = {10.1016/j.jpdc.2020.03.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {35-48},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Accelerating fingerprint identification using FPGA for large-scale applications},
  volume       = {141},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ARVMEC: Adaptive recommendation of virtual machines for IoT
in edge–cloud environment. <em>JPDC</em>, <em>141</em>, 23–34. (<a
href="https://doi.org/10.1016/j.jpdc.2020.03.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge–cloud services provide heterogeneous virtual machine types to run various IoT workloads. Choosing the appropriate VM configuration for each workload can effectively improve performance and reduce costs. This article proposes ARVMEC, A daptive R ecommendation of V irtual M achines for IoT in E dge- C loud Environment, which can always provide users with the best VM recommendation according to their own budget or deadline constraints. ARVMEC uses a tree-based ensemble learning algorithm to make accurate predictions on workload performance for all VM types. It can abstract user purposes in a more flexible and general mode, thus offer reasonable recommendations accordingly. Compared to state-of-art methods, ARVMEC can make better predictions with a 15\% improvement in accuracy.},
  archive      = {J_JPDC},
  author       = {Yajing Xu and Junnan Li and Zhihui Lu and Jie Wu and Patrick C.K. Hung and Abdulhameed Alelaiwi},
  doi          = {10.1016/j.jpdc.2020.03.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {23-34},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {ARVMEC: Adaptive recommendation of virtual machines for IoT in Edge–Cloud environment},
  volume       = {141},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic memory-aware scheduling in spark computing
environment. <em>JPDC</em>, <em>141</em>, 10–22. (<a
href="https://doi.org/10.1016/j.jpdc.2020.03.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scheduling plays an important role in improving the performance of big data-parallel processing. Spark is an in-memory parallel computing framework that uses a multi-threaded model in task scheduling . Most Spark task scheduling processes do not take the memory into account, but the number of concurrent task threads determined by the user. It emerges as a potential limitation for the performance. To overcome the limitations in the Spark-core source code , this paper proposes a dynamic Spark memory-aware task scheduler (DMATS), which not only treats memory and network I/O as a computational resource but also dynamically adjusts concurrency when scheduling tasks. Specifically, we first analyze the RDD based Spark execution engine to obtain the amount of task processing data and propose an algorithm for estimating the initial adaptive task concurrency, which is integrated with the known task input information and the executor memory. Then, a dynamic adjustment algorithm is proposed to change the concurrency dynamically through feedback information to optimally utilize the limited memory resources. We implement a dynamic memory-aware task scheduling (DMATS) in Spark 2.3.4 and evaluate performance with two typical benchmarks, shuffle-light and shuffle-heavy. The results show that the algorithm not only reduces the execution time by 43.64\%, but also significantly improves resource utilization. Experiments also show that our proposed method has advantages compared with other similar works such as WASP.},
  archive      = {J_JPDC},
  author       = {Zhuo Tang and Ailing Zeng and Xuedong Zhang and Li Yang and Kenli Li},
  doi          = {10.1016/j.jpdc.2020.03.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {10-22},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Dynamic memory-aware scheduling in spark computing environment},
  volume       = {141},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A blockchain based decentralized data security mechanism for
the internet of things. <em>JPDC</em>, <em>141</em>, 1–9. (<a
href="https://doi.org/10.1016/j.jpdc.2020.03.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Things (IoT) is the core infrastructure of the smart city information system. With the explosive growth of IoT devices, how to securely maintain the important data generated by IoT devices has become an important issue. In the conventional IoT-cloud based infrastructure, the sensitive IoT data was stored in a third cloud service provider . However, in such a manner, the private IoT data may be disclosed by the cloud server since the cloud server knows all the data stored in it. This paper, for the first time, proposes a decentralized secure mechanism based on the blockchain technique to store the important data generated in the IoT system. This mechanism effectively solves the data reliability, security and privacy issues that may be encountered in the conventional IoT-cloud system. Considering the defects of simplified payment verification used by light nodes in blockchain networks, this paper proposes an Unspent Transaction Output (UTXO) verification mechanism based on the RSA accumulator, which makes the computational complexity of light nodes to generate and verify the UTXO proof to be constant. The proposed mechanism not only provides proof of inclusion but also supports efficient proof of exclusion for a lightweight node. Our experiment results indicate that the proposed scheme is practical and efficient.},
  archive      = {J_JPDC},
  author       = {Chunpeng Ge and Zhe Liu and Liming Fang},
  doi          = {10.1016/j.jpdc.2020.03.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-9},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A blockchain based decentralized data security mechanism for the internet of things},
  volume       = {141},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reordering sparse matrices into block-diagonal
column-overlapped form. <em>JPDC</em>, <em>140</em>, 99–109. (<a
href="https://doi.org/10.1016/j.jpdc.2020.03.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many scientific and engineering applications necessitate computing the minimum norm solution of a sparse underdetermined linear system of equations. The minimum 2-norm solution of such systems can be obtained by a recent parallel algorithm , whose numerical effectiveness and parallel scalability are validated in both shared- and distributed-memory architectures. This parallel algorithm assumes the coefficient matrix in a block-diagonal column-overlapped (BDCO) form, which is a variant of the block-diagonal form where the successive diagonal blocks may overlap along their columns. The total overlap size of the BDCO form is an important metric in the performance of the subject parallel algorithm since it determines the size of the reduced system, solution of which is a bottleneck operation in the parallel algorithm. In this work, we propose a hypergraph partitioning model for reordering sparse matrices into BDCO form with the objective of minimizing the total overlap size and the constraint of maintaining balance on the number of nonzeros of the diagonal blocks. Our model makes use of existing partitioning tools that support fixed vertices in the recursive bipartitioning paradigm. Experimental results validate the use of our model as it achieves small overlap size and balanced diagonal blocks.},
  archive      = {J_JPDC},
  author       = {Seher Acer and Cevdet Aykanat},
  doi          = {10.1016/j.jpdc.2020.03.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {99-109},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Reordering sparse matrices into block-diagonal column-overlapped form},
  volume       = {140},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). How fast can one resize a distributed file system?
<em>JPDC</em>, <em>140</em>, 80–98. (<a
href="https://doi.org/10.1016/j.jpdc.2020.02.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient resource utilization becomes a major concern as large-scale distributed computing infrastructures keep growing in size. Malleability, the possibility for resource managers to dynamically increase or decrease the amount of resources allocated to a job, is a promising way to save energy and costs. However, state-of-the-art parallel and distributed storage systems have not been designed with malleability in mind. The reason is mainly the supposedly high cost of data transfers required by resizing operations. Nevertheless, as network and storage technologies evolve, old assumptions about potential bottlenecks can be revisited. In this study, we evaluate the viability of malleability as a design principle for a distributed storage system. We specifically model the minimal duration of the commission and decommission operations. To show how our models can be used in practice, we evaluate the performance of these operations in HDFS, a relevant state-of-the-art distributed file system . We show that the existing decommission mechanism of HDFS is good when the network is the bottleneck, but can be accelerated by up to a factor 3 when storage is the limiting factor. We also show that the commission in HDFS can be substantially accelerated. With the highlights provided by our model, we suggest improvements to speed both operations in HDFS. We discuss how the proposed models can be generalized for distributed file systems with different assumptions and what perspectives are open for the design of efficient malleable distributed file systems.},
  archive      = {J_JPDC},
  author       = {Nathanaël Cheriere and Matthieu Dorier and Gabriel Antoniu},
  doi          = {10.1016/j.jpdc.2020.02.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {80-98},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {How fast can one resize a distributed file system?},
  volume       = {140},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). TSpoon: Transactions on a stream processor. <em>JPDC</em>,
<em>140</em>, 65–79. (<a
href="https://doi.org/10.1016/j.jpdc.2020.03.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stream processing systems are increasingly becoming a core element in the data processing stack of many large companies, where they complement data management frameworks to build comprehensive solutions for processing, storage, and query. The adoption of separate tools leads to complex architectures that leave developers with the difficult task of writing application-specific code that ensures integration correctness. This hinders design, implementation, maintenance, and evolution. We address this problem with a new model that seamlessly integrates data management capabilities within a distributed stream processor. The model makes the state of stream processing operators externally visible and queryable, providing transactional guarantees for state accesses and updates. It enables developers to configure transactions obtaining strong guarantees when needed and relaxing them for higher performance when possible. We introduce the new model and formalize the transactional guarantees it offers. We discuss the implementation of the model into the TSpoon tool and experiment different algorithms to enforce transactional behavior. We evaluate the performance of TSpoon with real world case studies and synthetic workloads, compare it with state-of-the-art tools for distributed in-memory stream processing and data management, and analyze in detail the cost to ensure various transactional semantics.},
  archive      = {J_JPDC},
  author       = {Lorenzo Affetti and Alessandro Margara and Gianpaolo Cugola},
  doi          = {10.1016/j.jpdc.2020.03.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {65-79},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {TSpoon: Transactions on a stream processor},
  volume       = {140},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Editorial on the special issue on advances in parallel
programming: Languages, models and algorithms. <em>JPDC</em>,
<em>140</em>, 63–64. (<a
href="https://doi.org/10.1016/j.jpdc.2020.03.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JPDC},
  author       = {Ami Marowka and Przemysław Stpiczyński},
  doi          = {10.1016/j.jpdc.2020.03.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {63-64},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Editorial on the special issue on advances in parallel programming: Languages, models and algorithms},
  volume       = {140},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CFSec: Password based secure communication protocol in
cloud-fog environment. <em>JPDC</em>, <em>140</em>, 52–62. (<a
href="https://doi.org/10.1016/j.jpdc.2020.02.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing needs of data across the world, it is almost hard to live without data a day. The fog computing concept is aiming to change the scenarios created by Cloud Computing environments and also to make the data-centric clouds decentralized and localized. Fog devices which aim to be at a shorter distance with the user, access data from the cloud itself. Hence, exchanging data between the cloud and fog device(s) is required and in this context security and privacy come into the picture to provide data confidentiality . This paper first shows an architecture for the data flow model between the cloud and fog computing and then designs an authentication protocol with proper key establishment between the cloud, fog, and user. We have simulated the proposed protocol using a popular simulator i.e., Scyther and proved that the parameters used in our protocol are strongly protected during protocol execution. Besides, our informal security analysis also confirms the robustness of the protocol. Our protocol achieves quick responses in comparison with state-of-the-art because of less computation overhead.},
  archive      = {J_JPDC},
  author       = {Ruhul Amin and Sourav Kunal and Arijit Saha and Debasis Das and Atif Alamri},
  doi          = {10.1016/j.jpdc.2020.02.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {52-62},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {CFSec: Password based secure communication protocol in cloud-fog environment},
  volume       = {140},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Prime clock: Encoded vector clock to characterize causality
in distributed systems. <em>JPDC</em>, <em>140</em>, 37–51. (<a
href="https://doi.org/10.1016/j.jpdc.2020.02.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vector clock is a fundamental tool for tracking causality in distributed applications. Unfortunately, it does not scale well to large systems because each process needs to maintain a vector of size n n , where n n is the total number of processes in the system. To address this problem, we propose the prime clock, which is based on the encoding of the vector clock using prime numbers and uses a single number to represent vector time. We propose the operations on the encoded vector clock (EVC). We then show how to timestamp global states and how to perform operations on the global states using the EVC. Using a theoretical analysis and a simulation model, we evaluate the growth rate of the size of the EVC. The EVC is seen to grow very fast and hence it does not appear to offer a general purpose practical replacement of vector clocks. To address this drawback, we propose several scalability techniques for the EVC that can allow the use of the EVC in practical applications. We then present two case studies of detecting memory consistency errors in MPI one-sided applications and of dynamic race detection in multi-threaded environments, that use a combination of two of these scalability techniques. The results show that the EVC is not just a theoretical concept, but it is applicable to practical problems and can compete in terms of both space and time requirements with other known protocols.},
  archive      = {J_JPDC},
  author       = {Ajay D. Kshemkalyani and Min Shen and Bhargav Voleti},
  doi          = {10.1016/j.jpdc.2020.02.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {37-51},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Prime clock: Encoded vector clock to characterize causality in distributed systems},
  volume       = {140},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design and implementation of multiple-precision BLAS level 1
functions for graphics processing units. <em>JPDC</em>, <em>140</em>,
25–36. (<a href="https://doi.org/10.1016/j.jpdc.2020.02.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Basic Linear Algebra Subprograms (BLAS) are the building blocks for various numerical algorithms and are widely used in scientific computations. However, some linear algebra applications need more precision than the standard double precision available in most existing BLAS libraries. In this paper, we implement and evaluate multiple-precision scalar and vector BLAS functions on graphics processing units (GPUs). We use the residue number system (RNS) to represent arbitrary length floating-point numbers. The non-positional nature of RNS enables parallelism in multiple-precision arithmetic and makes RNS a good tool for high-performance computing applications. We first present new data-parallel algorithms for multiplying and adding RNS-based floating-point representations. Next, we suggest algorithms for multiple-precision vectors specially designed for parallel computations on GPUs. Using these algorithms, we develop and evaluate four GPU-accelerated multiple-precision BLAS functions, ASUM, DOT, SCAL, and AXPY. It is shown through experiments that in many cases, the implemented functions achieve significantly better performance compared to existing multiple-precision software for CPU and GPU.},
  archive      = {J_JPDC},
  author       = {Konstantin Isupov and Vladimir Knyazkov and Alexander Kuvaev},
  doi          = {10.1016/j.jpdc.2020.02.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {25-36},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Design and implementation of multiple-precision BLAS level 1 functions for graphics processing units},
  volume       = {140},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multicore and manycore parallelization of cheap
synchronizing sequence heuristics. <em>JPDC</em>, <em>140</em>, 13–24.
(<a href="https://doi.org/10.1016/j.jpdc.2020.02.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important concept in finite state machine based testing is synchronization which is used to initialize an implementation to a particular state. Usually, synchronizing sequences are used for this purpose and the length of the sequence used is important since it determines the cost of the initialization process . Unfortunately, the shortest synchronization sequence problem is NP-Hard. Instead, heuristics are used to generate short sequences. However, the cubic complexity of even the fastest heuristic algorithms can be a problem in practice. In order to scale the performance of the heuristics for generating short synchronizing sequences, we propose algorithmic improvements together with a parallel implementation of the cheapest heuristics existing in the literature. To identify the bottlenecks of these heuristics, we experimented on random and slowly synchronizing automata . The identified bottlenecks in the algorithms are improved by using algorithmic modifications. We also implement the techniques on multicore CPUs and Graphics Processing Units (GPUs) to take benefit of the modern parallel computation architectures. The sequential implementation of the heuristic algorithms are compared to our parallel implementations by using a test suite consisting of 1200 automata . The speedup values obtained depend on the size and the nature of the automaton. In our experiments, we observe speedup values as high as 340x by using a 16-core CPU parallelization , and 496x by using a GPU. Furthermore, the proposed methods scale well and the speedup values increase as the size of the automata increases.},
  archive      = {J_JPDC},
  author       = {Sertaç Karahoda and Osman Tufan Erenay and Kamer Kaya and Uraz Cengiz Türker and Hüsnü Yenigün},
  doi          = {10.1016/j.jpdc.2020.02.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {13-24},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Multicore and manycore parallelization of cheap synchronizing sequence heuristics},
  volume       = {140},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A protection routing with secure mechanism in möbius cubes.
<em>JPDC</em>, <em>140</em>, 1–12. (<a
href="https://doi.org/10.1016/j.jpdc.2020.02.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The protection routing uses the multi-paths technique for integrating route discovery and route maintenance mechanisms in a network, and thus it can tolerate the failure of one component (including a node or a link). Tapolcai (2013) proposed a method showing that a network possessing two completely independent spanning trees (CISTs for short) suffices to configure a protection routing. However, it is well-known that the problem of determining whether there exist two CISTs in a graph (or network) is NP-complete. In this paper, we extend Tapolcai’s method such that the protection routing is configured by a combination of multiple CISTs. The first application of such an extension is that it can be used to deal with the problem of security in transmission, which we call the secure-protection routing scheme (SPR-scheme for short). Thus, a network transmission using the SPR-scheme ensures that no node other than the destination can receive the complete message. Moreover, we show that if a message M M is transmitted in a network G G using the SPR-scheme configured by a combination of n n CISTs, then each intermediate node of G G can receive a maximum of 2 ∕ n 2∕n ratio of M M . From a similar idea of the extension, another application is the so-called multiple-protection routing scheme (MPR-scheme for short) which can be used to increase the capability of fault-tolerance. For assessing the performance of routing using MPR-scheme, we first propose a construction of three CISTs in the two types of Möbius cubes, which are hypercube-variant networks and are superior to hypercubes due to having a smaller diameter. For the n n -dimensional Möbius cube, the diameters of CISTs we constructed are at most 14 when n = 6 n=6 and at most 2 n + 1 2n+1 when n ⩾ 7 n⩾7 . So, we configure the desired protection routing in the Möbius cubes via the three CISTs. Then, we provide simulation results to experimentally evaluate the performance of the newly proposed MPR-scheme in the n n -dimensional Möbius cubes for 6 ⩽ n ⩽ 10 6⩽n⩽10 . As an important point, our results show that the adoption of routing using MPR-scheme will result in a significant slowdown in the transmission failure rate.},
  archive      = {J_JPDC},
  author       = {Kung-Jui Pai and Ruay-Shiung Chang and Jou-Ming Chang},
  doi          = {10.1016/j.jpdc.2020.02.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-12},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A protection routing with secure mechanism in möbius cubes},
  volume       = {140},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scalable and energy efficient wireless inter chip
interconnection fabrics using THz-band antennas. <em>JPDC</em>,
<em>139</em>, 148–160. (<a
href="https://doi.org/10.1016/j.jpdc.2020.02.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computing platforms ranging from embedded systems to server blades comprise of multiple Systems-on-Chips (SoCs). Conventionally, communication between chips in these multichip platforms are realized using high-speed I/O modules over metal traces on a substrate. Due to the high-power consumption of I/O modules and non-scalable pitch of pins or solder bumps their bandwidth density and power consumption becomes bottleneck for multichip systems. Wireless chip-to-chip communication is emerging as an alternative solution to the traditional interconnection challenges of multichip systems. Novel devices based on graphene structures capable of establishing wireless links are explored in recent literature to provide high bandwidth THz links. In this work, we propose to utilize graphene-based wireless links to enable energy-efficient, multi-modal chip-to-chip communication protocol to create toroidal folding based interconnection architectures for multichip systems. With cycle-accurate simulations we demonstrate that such designs can outperform state-of-the-art wireline multichip systems.},
  archive      = {J_JPDC},
  author       = {Sagar Saxena and Deekshith Shenoy Manur and Naseef Mansoor and Amlan Ganguly},
  doi          = {10.1016/j.jpdc.2020.02.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {148-160},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Scalable and energy efficient wireless inter chip interconnection fabrics using THz-band antennas},
  volume       = {139},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PMSMC: Priority-based multi-requestor scheduler for embedded
system memory controller. <em>JPDC</em>, <em>139</em>, 135–147. (<a
href="https://doi.org/10.1016/j.jpdc.2020.01.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern Multi-Processor System-On-Chips (MPSOC) are widely used especially in real-time embedded systems due to their high throughput and low per unit cost. However, bounded latency is vital to guarantee fast response as well as fairness for applications running on multicore processors . In this paper, a new Priority-base Memory Controller for Embedded Systems ( P M S M C PMSMC ) that prioritizes concurrently running applications by assigning uneven quota for each requestor is proposed. Each requestor quota is accompanied by a timer to control the dispatch rate to prevent starvation. Moreover, P M S M C PMSMC can monitor the real-time application memory activity to assist the request scheduling to achieve efficient utilization of the shared DRAM resource while keeping the timing bounded. Hence, P M S M C PMSMC can serve both multimedia real-time applications and hard real-time applications concurrently. For 8-core processors, P M S M C PMSMC is able to achieve an overall performance speedup of 24\% and 16\% compared to the recently proposed WCAD and TRB-SP memory controllers, respectively. For the Energy-Delay Product (EDP) metric which combines both performance and energy consumption, P M S M C PMSMC achieves lower EDPs of 25\% and 60\% compared to the recently proposed WCAD and TRB-SP memory controllers, respectively.},
  archive      = {J_JPDC},
  author       = {Ali A. El-Moursy and Fadi N. Sibai and Magdy A. El-Moursy and Ahmed S.S. Mohamed},
  doi          = {10.1016/j.jpdc.2020.01.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {135-147},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {PMSMC: Priority-based multi-requestor scheduler for embedded system memory controller},
  volume       = {139},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Practical concurrent unrolled linked lists using lazy
synchronization. <em>JPDC</em>, <em>139</em>, 110–134. (<a
href="https://doi.org/10.1016/j.jpdc.2019.11.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linked lists and other list-based sets are some of the most ubiquitous data structures in computer science. They are useful in their own right and are frequently used as building blocks in other data structures . A linked list can be “unrolled” to combine multiple keys in each node; this improves storage density and overall performance. This organization also allows an operation to skip over nodes which cannot contain a key of interest. This work introduces a new high-performance concurrent unrolled linked list with a lazy synchronization strategy. Most write operations under this strategy can complete by locking a single node. Experiments show up to 300\% improvement over other concurrent list-based sets.},
  archive      = {J_JPDC},
  author       = {Kenneth Platz and Neeraj Mittal and S. Venkatesan},
  doi          = {10.1016/j.jpdc.2019.11.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {110-134},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Practical concurrent unrolled linked lists using lazy synchronization},
  volume       = {139},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Utilization-prediction-aware virtual machine consolidation
approach for energy-efficient cloud data centers. <em>JPDC</em>,
<em>139</em>, 99–109. (<a
href="https://doi.org/10.1016/j.jpdc.2019.12.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the age of the information explosion, the energy demand for cloud data centers has increased markedly; hence, reducing the energy consumption of cloud data centers is essential. Dynamic virtual machine VM consolidation, as one of the effective methods for reducing energy energy consumption is extensively employed in large cloud data centers. It achieves the energy reductions by concentrating the workload of active hosts and switching idle hosts into low-power state; moreover, it improves the resource utilization of cloud data centers. However, the quality of service (QoS) guarantee is fundamental for maintaining dependable services between cloud providers and their customers in the cloud environment. Therefore, reducing the power costs while preserving the QoS guarantee are considered as the two main goals of this study. To efficiently address this problem, the proposed VM consolidation approach considers the current and future utilization of resources through the host overload detection (UP-POD) and host underload detection (UP-PUD). The future utilization of resources is accurately predicted using a Gray-Markov-based model. In the experiment, the proposed approach is applied for real-world workload traces in CloudSim and were compared with the existing benchmark algorithms. Simulation results show that the proposed approaches significantly reduce the number of VM migrations and energy consumption while maintaining the QoS guarantee.},
  archive      = {J_JPDC},
  author       = {Sun-Yuan Hsieh and Cheng-Sheng Liu and Rajkumar Buyya and Albert Y. Zomaya},
  doi          = {10.1016/j.jpdc.2019.12.014},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {99-109},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Utilization-prediction-aware virtual machine consolidation approach for energy-efficient cloud data centers},
  volume       = {139},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling i/o performance variability in high-performance
computing systems using mixture distributions. <em>JPDC</em>,
<em>139</em>, 87–98. (<a
href="https://doi.org/10.1016/j.jpdc.2020.01.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Performance variability is an important factor of high-performance computing (HPC) systems. HPC performance variability is often complex because its sources interact and are distributed throughout the system stack. For example, the performance variability of I/O throughput can be affected by factors such as CPU frequency, the number of I/O threads, file size, and record size. In this paper, we focus on the I/O throughput variability across multiple executions of a benchmark program . For a given system configuration, the distribution of throughputs from run to run is of interest. We conduct large-scale experiments and collect a massive amount of data to study the distribution of I/O throughput under tens of thousands of system configurations. Despite normality often being assumed in the literature, our statistical analysis reveals that the performance variability is not normally distributed under most system configurations. Instead, multimodal distributions are common for many system configurations. We propose the use of mixture distributions to describe the multimodal behavior. Various underlying parametric distributions such as normal, gamma, and the Weibull are considered. We apply an expectation–maximization (EM) algorithm for parameter estimation and use the Bayesian information criterion (BIC) for parametric model selections. We also illustrate how to use the estimated mixture distribution to calculate the number of runs needed for future experiments on variability analysis. The paper provides a useful tool set in studying the behavior of performance variability.},
  archive      = {J_JPDC},
  author       = {Li Xu and Yueyao Wang and Thomas Lux and Tyler Chang and Jon Bernard and Bo Li and Yili Hong and Kirk Cameron and Layne Watson},
  doi          = {10.1016/j.jpdc.2020.01.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {87-98},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Modeling I/O performance variability in high-performance computing systems using mixture distributions},
  volume       = {139},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On-GPU thread-data remapping for nested branch divergence.
<em>JPDC</em>, <em>139</em>, 75–86. (<a
href="https://doi.org/10.1016/j.jpdc.2020.02.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nested branches are common in applications with decision trees . The more layers in the branch nest, the larger slowdown is caused by nested branch divergence on GPU . Since inner branches are impractical to evaluate on host end, thread-data remapping via GPU shared memory is so far the most suitable solution. However, existing solution cannot handle inner branches directly due to undefined behavior of GPU barrier function when executed within branch statements. Race condition needs to be prevented without using barrier function. Targeting nested divergence, we propose NeX as a nested extension scheme featuring an inter-thread protocol that supports sub-workgroup synchronization. We further exploit the on-the-fly nature of Head-or-Tail ( HoT ) algorithm and propose HoT2 with enhanced flexibility of wavefront scheduling. Evaluated on four GPU models including NVIDIA Volta and Turing, HoT2 confirms to be more efficient. For benchmarks with branch nests up to five-layer-deep, NeX further boosts performance by up to 1.56x.},
  archive      = {J_JPDC},
  author       = {Huanxin Lin and Cho-Li Wang},
  doi          = {10.1016/j.jpdc.2020.02.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {75-86},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {On-GPU thread-data remapping for nested branch divergence},
  volume       = {139},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient OpenMP parallelization to a complex MPI parallel
magnetohydrodynamics code. <em>JPDC</em>, <em>139</em>, 65–74. (<a
href="https://doi.org/10.1016/j.jpdc.2020.02.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The state-of-the-art finite volume/difference magnetohydrodynamics (MHD) code Block Adaptive Tree Solarwind Roe Upwind Scheme (BATS-R-US) was originally designed with pure MPI parallelization . The maximum problem size achievable was limited by the storage requirements of the block tree structure. To mitigate this limitation, we have added multi-threaded OpenMP parallelization to the previous pure MPI implementation. We opted to use a coarse-grained approach by making the loops over grid blocks multi-threaded and have succeeded in making BATS-R-US an efficient hybrid parallel code with modest changes in the source code while preserving the performance. Good weak scalings up to hundreds of thousands of cores were achieved both for explicit and implicit time stepping schemes. This parallelization strategy greatly extended the possible simulation scale from 16,000 cores to more than 500,000 cores with 2GB/core memory on the Blue Waters supercomputer . Our work also revealed significant performance issues for some of the compilers when the code is compiled with the OpenMP library, probably related to the less efficient optimization of a complex multi-threaded region.},
  archive      = {J_JPDC},
  author       = {Hongyang Zhou and Gábor Tóth},
  doi          = {10.1016/j.jpdc.2020.02.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {65-74},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Efficient OpenMP parallelization to a complex MPI parallel magnetohydrodynamics code},
  volume       = {139},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enhancing and simplifying data security and privacy for
multitiered applications. <em>JPDC</em>, <em>139</em>, 53–64. (<a
href="https://doi.org/10.1016/j.jpdc.2020.01.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While databases provide capabilities to enforce security and privacy policies, two major issues still prevent applications from safely delegating such policies to the database. The first one is the loss of user identity in multitiered environments which renders the database security features of little to no value. The second issue is the unsafe coexistence between the security capabilities and fundamental database tenets which creates data leakage vulnerabilities. This paper proposes extensions to database systems to allow applications, such as those used in managing the operations of energy clouds, to safely delegate the security and privacy policies to the database. This delegation reduces complexity for applications and improves overall data security and privacy. Our performance evaluation shows that almost all the TPC-H queries perform the same or better when the security policy is enforced by the database. For the set of queries that performed better, the improvement observed ranges from 8 to 68\%.},
  archive      = {J_JPDC},
  author       = {Walid Rjaibi and Mohammad Hammoudeh},
  doi          = {10.1016/j.jpdc.2020.01.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {53-64},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Enhancing and simplifying data security and privacy for multitiered applications},
  volume       = {139},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distributed bayesian optimization of deep reinforcement
learning algorithms. <em>JPDC</em>, <em>139</em>, 43–52. (<a
href="https://doi.org/10.1016/j.jpdc.2019.07.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Significant strides have been made in supervised learning settings thanks to the successful application of deep learning . Now, recent work has brought the techniques of deep learning to bear on sequential decision processes in the area of deep reinforcement learning (DRL). Currently, little is known regarding hyperparameter optimization for DRL algorithms. Given that DRL algorithms are computationally intensive to train, and are known to be sample inefficient, optimizing model hyperparameters for DRL presents significant challenges to established techniques. We provide an open source, distributed Bayesian model-based optimization algorithm , HyperSpace, and show that it consistently outperforms standard hyperparameter optimization techniques across three DRL algorithms.},
  archive      = {J_JPDC},
  author       = {M. Todd Young and Jacob D. Hinkle and Ramakrishnan Kannan and Arvind Ramanathan},
  doi          = {10.1016/j.jpdc.2019.07.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {43-52},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Distributed bayesian optimization of deep reinforcement learning algorithms},
  volume       = {139},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accelerated serverless computing based on GPU
virtualization. <em>JPDC</em>, <em>139</em>, 32–42. (<a
href="https://doi.org/10.1016/j.jpdc.2020.01.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a platform to support serverless computing for scalable event-driven data processing that features a multi-level elasticity approach combined with virtualization of GPUs . The platform supports the execution of applications based on Docker containers in response to file uploads to a data storage in order to perform the data processing in parallel. This is managed by an elastic Kubernetes cluster whose size automatically grows and shrinks depending on the number of files to be processed. To accelerate the processing time of each file, several approaches involving virtualized access to GPUs , either locally or remote, have been evaluated. A use case that involves the inference based on deep learning techniques on transthoracic echocardiography imaging has been carried out to assess the benefits and limitations of the platform. The results indicate that the combination of serverless computing and GPU virtualization introduce an efficient and cost-effective event-driven accelerated computing approach that can be applied for a wide variety of scientific applications.},
  archive      = {J_JPDC},
  author       = {Diana M. Naranjo and Sebastián Risco and Carlos de Alfonso and Alfonso Pérez and Ignacio Blanquer and Germán Moltó},
  doi          = {10.1016/j.jpdc.2020.01.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {32-42},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Accelerated serverless computing based on GPU virtualization},
  volume       = {139},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). QuickDedup: Efficient VM deduplication in cloud computing
environments. <em>JPDC</em>, <em>139</em>, 18–31. (<a
href="https://doi.org/10.1016/j.jpdc.2020.01.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deduplication is one of the major storage optimisation techniques for Virtual Machines (VMs) in cloud environment. Usually, hashing of blocks helps in identifying duplicate data blocks. This paper proposes a novel deduplication approach, QuickDedup that reduces the overall deduplication time, metadata overhead and the number of hash computations, and subsequent comparisons for the VM disk images. In addition to minimising the deduplication related metadata, which is a necessary by-product useful in checking deduplication, QuickDedup, follows novel byte comparison scheme to prepare various block classes. This way, QuickDedup eliminates or minimises the need for hash calculation and subsequent comparisons. QuickDedup performs the calculation and comparisons of hashes within the respective categories only. QuickDedup saves the space required for hash storage during deduplication and makes deduplication of VM disk images much faster. We conducted a detailed evaluation of QuickDedup on various metrics with different kinds and sizes of VM images taken from publicly available datasets. The evaluation results show a substantial improvement of up to 96\% in the overall deduplication time required to deduplicate VM images apart from significant savings in metadata and storage overhead .},
  archive      = {J_JPDC},
  author       = {Shweta Saharan and Gaurav Somani and Gaurav Gupta and Robin Verma and Manoj Singh Gaur and Rajkumar Buyya},
  doi          = {10.1016/j.jpdc.2020.01.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {18-31},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {QuickDedup: Efficient VM deduplication in cloud computing environments},
  volume       = {139},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A general purpose contention manager for software
transactions on the GPU. <em>JPDC</em>, <em>139</em>, 1–17. (<a
href="https://doi.org/10.1016/j.jpdc.2019.12.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Graphics Processing Unit (GPU) is now used extensively for general purpose GPU programming (GPGPU), allowing for greater exploitation of the multi-core model across many application domains. This is particularly true in cloud/edge/fog computing, where multiple GPU enabled servers support many different end user services. This move away from the naturally parallel domain of graphics can incur significant performance issues. Unlike the CPU, code that is hindered from execution due to blocking/waiting on the GPU can affect thousands of threads , rendering the advantages of a GPU irrelevant and reducing a highly parallel environment down to a serial one in the worst case. In this paper we present a solution that minimises blocking/waiting in GPGPU computing using a contention manager that offsets memory conflicts across threads through thread re-ordering. We consider conflicts of memory not only to avoid corruption (standard for transactional memory) but also in the semantic layer of application logic (e.g., enforcing ordering to ensure money drawn from bank account occurs after all deposits). We demonstrate how our approach is successful across a number of industry benchmarks and compare our approach to the only other related solution. We also demonstrate that our approach is scalable in terms of thread numbers (a key requirement on the GPU). We believe this is the first work of its kind demonstrating a generalised conflict and semantic contention manager suitable for the scale of parallel execution found on a GPU.},
  archive      = {J_JPDC},
  author       = {Qi Shen and Craig Sharp and Richard Davison and Gary Ushaw and Rajiv Ranjan and Albert Y. Zomaya and Graham Morgan},
  doi          = {10.1016/j.jpdc.2019.12.018},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-17},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A general purpose contention manager for software transactions on the GPU},
  volume       = {139},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient convolution pooling on the GPU. <em>JPDC</em>,
<em>138</em>, 222–229. (<a
href="https://doi.org/10.1016/j.jpdc.2019.12.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main contribution of this paper is to show efficient implementations of the convolution-pooling in the GPU , in which the pooling follows the multiple convolution . Since the multiple convolution and the pooling operations are performed alternately in earlier stages of many Convolutional Neural Networks (CNNs), it is very important to accelerate the convolution-pooling. Our new GPU implementation uses two techniques, (1) convolution interchange with direct sum, and (2) conversion to matrix multiplication. By these techniques, the computational and memory access cost are reduced. Further the convolution interchange is converted to matrix multiplication, which can be computed by cuBLAS very efficiently. Experimental results using Tesla V100 GPU show that our new GPU implementation compatible with cuDNN for the convolution-pooling is expected 2.90 times and 1.43 times faster for fp32 and fp16 than the multiple convolution and then the pooling by cuDNN, respectively. the most popular library of primitives to implement the CNNs in the GPU.},
  archive      = {J_JPDC},
  author       = {Shunsuke Suita and Takahiro Nishimura and Hiroki Tokura and Koji Nakano and Yasuaki Ito and Akihiko Kasagi and Tsuguchika Tabaru},
  doi          = {10.1016/j.jpdc.2019.12.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {222-229},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Efficient convolution pooling on the GPU},
  volume       = {138},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Designing an efficient parallel spectral clustering
algorithm on multi-core processors in julia. <em>JPDC</em>,
<em>138</em>, 211–221. (<a
href="https://doi.org/10.1016/j.jpdc.2020.01.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral clustering is widely used in data mining, machine learning and other fields. It can identify the arbitrary shape of a sample space and converge to the global optimal solution . Compared with the traditional k -means algorithm, the spectral clustering algorithm has stronger adaptability to data and better clustering results . However, the computation of the algorithm is quite expensive. In this paper, an efficient parallel spectral clustering algorithm on multi-core processors in the Julia language is proposed, and we refer to it as juPSC. The Julia language is a high-performance, open-source programming language . The juPSC is composed of three procedures: (1) calculating the affinity matrix , (2) calculating the eigenvectors , and (3) conducting k -means clustering. Procedures (1) and (3) are computed by the efficient parallel algorithm , and the COO format is used to compress the affinity matrix . Two groups of experiments are conducted to verify the accuracy and efficiency of the juPSC. Experimental results indicate that (1) the juPSC achieves speedups of approximately 14 × × ∼ 18 × ∼18× on a 24-core CPU and that (2) the serial version of the juPSC is faster than the Python version of scikit-learn . Moreover, the structure and functions of the juPSC are designed considering modularity, which is convenient for combination and further optimization with other parallel computing platforms.},
  archive      = {J_JPDC},
  author       = {Zenan Huo and Gang Mei and Giampaolo Casolla and Fabio Giampaolo},
  doi          = {10.1016/j.jpdc.2020.01.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {211-221},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Designing an efficient parallel spectral clustering algorithm on multi-core processors in julia},
  volume       = {138},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the performance difference between theory and practice
for parallel algorithms. <em>JPDC</em>, <em>138</em>, 199–210. (<a
href="https://doi.org/10.1016/j.jpdc.2019.12.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of parallel algorithms is often inconsistent with their preliminary theoretical analyses. Indeed, the difference is increasing between the ability to theoretically predict the performance of a parallel algorithm and the results measured in practice. This is mainly due to the accelerated development of advanced parallel architectures , whereas there is still no agreed model for parallel computation, which has implications for the design of parallel algorithms and for the manner in which parallel programming should be taught. In this study, we examined the practical performance of Cormen’s Quicksort parallel algorithm. We determined the performance of the algorithm with different parallel programming approaches and examine the capacity of theoretical performance analyses of the algorithm for predicting the actual performance. This algorithm is used for teaching theoretical and practical aspects of parallel programming to undergraduate students. We considered the pedagogic implications that may arise when the algorithm is used as a learning resource for teaching parallel programming.},
  archive      = {J_JPDC},
  author       = {Ami Marowka},
  doi          = {10.1016/j.jpdc.2019.12.020},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {199-210},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {On the performance difference between theory and practice for parallel algorithms},
  volume       = {138},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Subgraph fault tolerance of distance optimally edge
connected hypercubes and folded hypercubes. <em>JPDC</em>, <em>138</em>,
190–198. (<a href="https://doi.org/10.1016/j.jpdc.2019.12.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypercube and folded hypercube are the most fundamental interconnection networks for the attractive topological properties. We assume for any distinct vertices u , v ∈ V , κ ( u , v ) u,v∈V,κ(u,v) defined as local connectivity of u u and v v , is the maximum number of independent ( u , v ) (u,v) -paths in G G . Similarly, λ ( u , v ) λ(u,v) is local edge connectivity of u , v u,v . For some t ∈ [ 1 , D ( G ) ] , ∀ u , v ∈ V , u ≠ v t∈[1,D(G)],∀u,v∈V,u≠v , and d ( u , v ) = t d(u,v)=t , if κ ( u , v ) ( o r λ ( u , v ) ) = m i n { d ( u ) , d ( v ) } κ(u,v)(orλ(u,v))=min{d(u),d(v)} , then G G is t t -distance optimally (edge) connected, where D ( G ) D(G) is the diameter of G G and d ( u ) d(u) is the degree of u u . For all integers 0 0&amp;lt;k≤t , if G G is k k -distance optimally connected, then we call G G is t t -distance local optimally connected. Similarly, we have the definition of t t -distance local optimally edge connected. In this paper, we show that after deleting Q k ( k ≤ n − 1 ) Qk(k≤n−1) , Q n − Q k Qn−Qk and F Q n − Q k FQn−Qk are 2-distance local optimally edge connected.},
  archive      = {J_JPDC},
  author       = {Litao Guo and Chengfu Qin and Liqiong Xu},
  doi          = {10.1016/j.jpdc.2019.12.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {190-198},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Subgraph fault tolerance of distance optimally edge connected hypercubes and folded hypercubes},
  volume       = {138},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient AES implementation on sunway TaihuLight
supercomputer: A systematic approach. <em>JPDC</em>, <em>138</em>,
178–189. (<a href="https://doi.org/10.1016/j.jpdc.2019.12.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Encryption is an important technique to improve information security for many real-world applications. The Advanced Encryption Standard (AES) is a widely-used efficient cryptographic algorithm . Although AES is fast both in software and hardware, it is time-consuming to do data encryption especially for large amount of data. Therefore, it is a lasting effort to accelerate AES operations. This paper presents SW-AES, a parallel AES implementation on the Sunway TaihuLight, one of the fastest supercomputers in the world that takes the SW26010 processor as the basic building block . According to the architectural features of SW26010, SW-AES exploits parallelism from different levels, including (1) inter-CPE (Computing Processing Element) data parallelism that distributes tasks among the 256 on-chip CPEs, (2) intra-CPE data parallelism enabled by the Single-Instruction Multiple-Data (SIMD) instructions inside each CPE, and (3) instruction-level parallelism that pipelines memory access and the computation. In addition, corresponding to the two application scenarios, SW-AES presents scalable ways to efficiently run AES on many nodes. As a result, SW-AES can gain a maximum throughput of 13.50 GB/s on a single SW26010 node, which is 216.23 × × higher than the latest parallel AES implementation on the Sunway TaihuLight, and about 37.3\% higher than the latest AES implementation on the GTX 480 GPU . When running on 1024 computing nodes with each one processing 1 GB data, SW-AES can achieve a throughput of 13819.25 GB/s. On the contrast, only a throughput of 63.91 GB/s can be achieved by the latest related work on the Sunway TaihuLight.},
  archive      = {J_JPDC},
  author       = {Liandeng Li and Jiarui Fang and Jinlei Jiang and Lin Gan and Weijie Zheng and Haohuan Fu and Guangwen Yang},
  doi          = {10.1016/j.jpdc.2019.12.013},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {178-189},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Efficient AES implementation on sunway TaihuLight supercomputer: A systematic approach},
  volume       = {138},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A semantic-based methodology for digital forensics analysis.
<em>JPDC</em>, <em>138</em>, 172–177. (<a
href="https://doi.org/10.1016/j.jpdc.2019.12.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, more than ever, digital forensics activities are involved in any criminal, civil or military investigation and represent a fundamental tool to support cyber-security. Investigators use a variety of techniques and proprietary software forensics applications to examine the copy of digital devices, searching hidden, deleted, encrypted, or damaged files or folders. Any evidence found is carefully analysed and documented in a “finding report” in preparation for legal proceedings that involve discovery, depositions, or actual litigation. The aim is to discover and analyse patterns of fraudulent activities. In this work, a new methodology is proposed to support investigators during the analysis process, correlating evidence found through different forensics tools . The methodology was implemented through a system able to add semantic assertion to data generated by forensics tools during extraction processes. These assertions enable more effective access to relevant information and enhanced retrieval and reasoning capabilities.},
  archive      = {J_JPDC},
  author       = {Flora Amato and Aniello Castiglione and Giovanni Cozzolino and Fabio Narducci},
  doi          = {10.1016/j.jpdc.2019.12.017},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {172-177},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A semantic-based methodology for digital forensics analysis},
  volume       = {138},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SLASs: A fully automatic auto-tuned linear algebra library
based on OpenMP extensions implemented in OmpSs (LASs library).
<em>JPDC</em>, <em>138</em>, 153–171. (<a
href="https://doi.org/10.1016/j.jpdc.2019.12.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we have implemented a novel Linear Algebra Library on top of the task-based runtime OmpSs-2. We have used some of the most advanced OmpSs-2 features; weak dependencies and regions, together with the final clause for the implementation of auto-tunable code for the BLAS-3 trsm routine and the LAPACK routines npgetrf and npgesv . All these implementations are part of the first prototype of sLASs library, a novel library for auto-tunable codes for linear algebra operations based on LASs library. In all these cases, the use of the OmpSs-2 features presents an improvement in terms of execution time against other reference libraries such as, the original LASs library, PLASMA, ATLAS and Intel MKL. These codes are able to reduce the execution time in about 18\% on big matrices, by increasing the IPC on gemm and reducing the time of task instantiation . For a few medium matrices, benefits are also seen. For small matrices and a subset of medium matrices, specific optimizations that allow to increase the degree of parallelism in both, gemm and trsm tasks, are applied. This strategy achieves an increment in performance of up to 40\%.},
  archive      = {J_JPDC},
  author       = {Pedro Valero-Lara and Sandra Catalán and Xavier Martorell and Tetsuzo Usui and Jesús Labarta},
  doi          = {10.1016/j.jpdc.2019.12.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {153-171},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {SLASs: A fully automatic auto-tuned linear algebra library based on OpenMP extensions implemented in OmpSs (LASs library)},
  volume       = {138},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Structured multi-block grid partitioning using balanced cut
trees. <em>JPDC</em>, <em>138</em>, 139–152. (<a
href="https://doi.org/10.1016/j.jpdc.2019.12.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An algorithm to partition structured multi-block hexahedral grids for a load balanced assignment of the partitions to a given number of bins is presented. It uses a balanced hierarchical cut tree data structure to partition the structured blocks into structured partitions. The refinement of the cut tree attempts to generate equally shaped partitions with a low amount of additional surface. A multi-block load balancing approach is presented that guarantees to satisfy an upper bound of load imbalance. The partition quality of the algorithm is compared to established recursive edge bisection approaches and an unstructured partitioning using METIS. Two generic and two turbomachinery test cases demonstrate the superior quality and fast runtime of the present algorithm at generating load balanced structured partitions.},
  archive      = {J_JPDC},
  author       = {Georg Geiser and Wolfgang Schröder},
  doi          = {10.1016/j.jpdc.2019.12.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {139-152},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Structured multi-block grid partitioning using balanced cut trees},
  volume       = {138},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High level programming abstractions for leveraging
hierarchical memories with micro-core architectures. <em>JPDC</em>,
<em>138</em>, 128–138. (<a
href="https://doi.org/10.1016/j.jpdc.2019.11.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro-core architectures combine many low memory, low power computing cores together in a single package. These are attractive for use as accelerators but due to limited on-chip memory and multiple levels of memory hierarchy, the way in which programmers offload kernels needs to be carefully considered. In this paper we use Python as a vehicle for exploring the semantics and abstractions of higher level programming languages to support the offloading of computational kernels to these devices. By moving to a pass by reference model, along with leveraging memory kinds, we demonstrate the ability to easily and efficiently take advantage of multiple levels in the memory hierarchy, even ones that are not directly accessible to the micro-cores. Using a machine learning benchmark, we perform experiments on both Epiphany-III and MicroBlaze based micro-cores, demonstrating the ability to compute with data sets of arbitrarily large size. To provide context of our results, we explore the performance and power efficiency of these technologies, demonstrating that whilst these two micro-core technologies are competitive within their own embedded class of hardware, there is still a way to go to reach HPC class GPUs .},
  archive      = {J_JPDC},
  author       = {Maurice Jamieson and Nick Brown},
  doi          = {10.1016/j.jpdc.2019.11.011},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {128-138},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {High level programming abstractions for leveraging hierarchical memories with micro-core architectures},
  volume       = {138},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scheduling directed acyclic graphs with optimal duplication
strategy on homogeneous multiprocessor systems. <em>JPDC</em>,
<em>138</em>, 115–127. (<a
href="https://doi.org/10.1016/j.jpdc.2019.12.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern applications generally need a large volume of computation and communication to fulfill the goal. These applications are often implemented on multiprocessor systems to meet the requirements in computing capacity and communication bandwidth , whereas, how to obtain a good or even the optimal performance on such systems remains a challenge. When tasks of the application are mapped onto different processors for execution, inter-processor communications become inevitable, which delays some tasks’ execution and deteriorates the schedule performance. To mitigate the overhead incurred by inter-processor communications and improve the schedule performance, task duplication strategy has been employed in the schedule. Most available techniques for the duplication-based scheduling problem utilize heuristic strategies to produce sub-optimal solutions, however, how to find the optimal duplication-based solution with the minimal schedule makespan remains an unsolved issue. To fill in this gap, this paper proposes a novel Mixed Integer Linear Programming (MILP) formulation for this problem, together with a set of key theorems which enable and simplify the MILP formulation. The proposed MILP formulation can optimize the duplication strategy, serialize the execution of task instances on each processor and determine data precedences among different task instances, thus producing the optimal solution. The proposed method is tested on a set of synthesized applications and platforms and compared with the well-known algorithm. The experimental results demonstrate the effectiveness of the proposed method.},
  archive      = {J_JPDC},
  author       = {Qi Tang and Li-Hua Zhu and Li Zhou and Jun Xiong and Ji-Bo Wei},
  doi          = {10.1016/j.jpdc.2019.12.012},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {115-127},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Scheduling directed acyclic graphs with optimal duplication strategy on homogeneous multiprocessor systems},
  volume       = {138},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Blockchain 3.0 applications survey. <em>JPDC</em>,
<em>138</em>, 99–114. (<a
href="https://doi.org/10.1016/j.jpdc.2019.12.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we survey a number of interesting applications of blockchain technology not related to cryptocurrencies . As a matter of fact, after an initial period of application to cryptocurrencies and to the financial world, blockchain technology has been successfully exploited in many other different scenarios, where its unique features allowed the definition of innovative and sometimes disruptive solutions. In particular, this paper takes into account the following application scenarios: end-to-end verifiable electronic voting, healthcare records management, identity management systems , access control systems , decentralized notary (with a focus on intellectual property protection) and supply chain management. For each of these, we firstly analyse the problem, the related requirements and the advantages the adoption of blockchain technology might bring. Then, we present a number of relevant solutions proposed in the literature both by academia and companies.},
  archive      = {J_JPDC},
  author       = {Damiano Di Francesco Maesa and Paolo Mori},
  doi          = {10.1016/j.jpdc.2019.12.019},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {99-114},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Blockchain 3.0 applications survey},
  volume       = {138},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A parallel multilevel feature selection algorithm for
improved cancer classification. <em>JPDC</em>, <em>138</em>, 78–98. (<a
href="https://doi.org/10.1016/j.jpdc.2019.12.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biological data is prone to grow exponentially, which consumes more resources, time and manpower. Parallelization of algorithms could reduce overall execution time. There are two main challenges in parallelizing computational methods. (1) Biological data is multi-dimensional in nature. (2). Parallel algorithms reduce execution time, but with the penalty of reduced prediction accuracy. This research paper targets these two issues and proposes the following approaches. (1) Vertical partitioning of data along feature space and horizontal partitioning along samples in order to ease the task of data parallelism . (2) Parallel Multilevel Feature Selection (M-FS) algorithm to select optimal and important features for improved classification of cancer sub-types. The selected features are evaluated using parallel Random Forest on Spark, compared with previously reported results and also with the results of sequential execution of same algorithms. The proposed parallel M-FS algorithm was compared with existing parallel feature selection algorithms in terms of accuracy and execution time. The results reveal that parallel multilevel feature selection algorithm improved cancer classification resulting into prediction accuracy ranging from ∼ ∼ 85\% to ∼ ∼ 99\% with very high speed up in terms of seconds. On the other hand, existing sequential algorithms yielded prediction accuracy of ∼ ∼ 65\% to ∼ ∼ 99\% with execution time of more than 24 hours.},
  archive      = {J_JPDC},
  author       = {Lokeswari Venkataramana and Shomona Gracia Jacob and Rajavel Ramadoss},
  doi          = {10.1016/j.jpdc.2019.12.015},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {78-98},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A parallel multilevel feature selection algorithm for improved cancer classification},
  volume       = {138},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extending the limits for big data RSA cracking: Towards
cache-oblivious TU decomposition. <em>JPDC</em>, <em>138</em>, 65–77.
(<a href="https://doi.org/10.1016/j.jpdc.2019.12.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, Big Data security processes require mining large amounts of content that was traditionally not typically used for security analysis in the past. The RSA algorithm has become the de facto standard for encryption, especially for data sent over the internet. RSA takes its security from the hardness of the Integer Factorisation Problem. As the size of the modulus of an RSA key grows with the number of bytes to be encrypted, the corresponding linear system to be solved in the adversary integer factorisation algorithm also grows. In the age of big data this makes it compelling to redesign linear solvers over finite fields so that they exploit the memory hierarchy. To this end, we examine several matrix layouts based on space-filling curves that allow for a cache-oblivious adaptation of parallel TU decomposition for rectangular matrices over finite fields. The TU algorithm of Dumas and Roche (2002) requires index conversion routines for which the cost to encode and decode the chosen curve is significant. Using a detailed analysis of the number of bit operations required for the encoding and decoding procedures, and filtering the cost of lookup tables that represent the recursive decomposition of the Hilbert curve, we show that the Morton-hybrid order incurs the least cost for index conversion routines that are required throughout the matrix decomposition as compared to the Hilbert, Peano, or Morton orders. The motivation lies in that cache efficient parallel adaptations for which the natural sequential evaluation order demonstrates lower cache miss rate result in overall faster performance on parallel machines with private or shared caches and on GPU’s.},
  archive      = {J_JPDC},
  author       = {Fatima K. Abu Salem and Mira Al Arab and Laurence T. Yang},
  doi          = {10.1016/j.jpdc.2019.12.016},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {65-77},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Extending the limits for big data RSA cracking: Towards cache-oblivious TU decomposition},
  volume       = {138},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CHAMELEON: Reactive load balancing for hybrid MPI+OpenMP
task-parallel applications. <em>JPDC</em>, <em>138</em>, 55–64. (<a
href="https://doi.org/10.1016/j.jpdc.2019.12.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many applications in high performance computing are designed based on underlying performance and execution models. While these models could successfully be employed in the past for balancing load within and between compute nodes, modern software and hardware increasingly make performance predictability difficult if not impossible. Consequently, balancing computational load becomes much more difficult. Aiming to tackle these challenges in search for a general solution, we present a novel library for fine-granular task-based reactive load balancing in distributed memory based on MPI and OpenMP. With our approach, individual migratable tasks can be executed on any MPI rank. The actual executing rank is determined at run time based on online performance data. We evaluate our approach under an enforced power cap and under enforced clock frequency changes for a synthetic benchmark and show its robustness for work-induced imbalances for a realistic application. Our experiments demonstrate speedups of up to 1 . 31 X 1.31X .},
  archive      = {J_JPDC},
  author       = {Jannis Klinkenberg and Philipp Samfass and Michael Bader and Christian Terboven and Matthias S. Müller},
  doi          = {10.1016/j.jpdc.2019.12.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {55-64},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {CHAMELEON: Reactive load balancing for hybrid MPI+OpenMP task-parallel applications},
  volume       = {138},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Kokkos implementation of an ewald coulomb solver and
analysis of performance portability. <em>JPDC</em>, <em>138</em>, 48–54.
(<a href="https://doi.org/10.1016/j.jpdc.2019.12.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We have implemented the computation of Coulomb interactions in particle systems using the performance portable C++ framework Kokkos. For the computation of the electrostatic interactions in particle systems we used an Ewald summation. This implementation we consider as a basis for a performance portability study. As target architectures we used Intel CPUs, including Intel Xeon Phi , as well as Nvidia GPUs . To provide a measure for performance portability we compute the number of needed operations and required cycles, i.e. runtime, and compare these with the measured runtime. Results indicate a similar quality of performance portability on all investigated architectures.},
  archive      = {J_JPDC},
  author       = {Rene Halver and Jan H. Meinke and Godehard Sutmann},
  doi          = {10.1016/j.jpdc.2019.12.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {48-54},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Kokkos implementation of an ewald coulomb solver and analysis of performance portability},
  volume       = {138},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scalable energy-efficient parallel sorting on a fine-grained
many-core processor array. <em>JPDC</em>, <em>138</em>, 32–47. (<a
href="https://doi.org/10.1016/j.jpdc.2019.12.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three parallel sorting applications and two list output protocols for the first phase of an external sort execute on a fine-grained many-core processor array that contains no algorithm-specific hardware acting as a co-processor with a variety of array sizes. Results are generated using a cycle-accurate model based on measured data from a fabricated many-core chip, and simulated for different processor array sizes. The data shows most energy efficient first-phase many-core sort requires over 65 × × lower energy than GNU C++ standard library sort performed on an Intel laptop-class processor and over 105 × × lower energy than a radix sort running on an Nvidia GPU . In addition, the highest first-phase throughput many-core sort is over 9.8 × × faster than the std::sort and over 14 × × faster than the radix sort. Both phases of a 10 GB external sort require 6.2 × × lower energy × × time energy delay product than the std::sort and over 13 × × lower energy × × time than the radix sort.},
  archive      = {J_JPDC},
  author       = {Aaron Stillmaker and Brent Bohnenstiehl and Lucas Stillmaker and Bevan Baas},
  doi          = {10.1016/j.jpdc.2019.12.011},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {32-47},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Scalable energy-efficient parallel sorting on a fine-grained many-core processor array},
  volume       = {138},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On demand clock synchronization for live VM migration in
distributed cloud data centers. <em>JPDC</em>, <em>138</em>, 15–31. (<a
href="https://doi.org/10.1016/j.jpdc.2019.11.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Live migration of virtual machines (VMs) has become an extremely powerful tool for cloud data center management and provides significant benefits of seamless VM mobility among physical hosts within a data center or across multiple data centers without interrupting the running service. However, with all the enhanced techniques that ensure a smooth and flexible migration, the down-time of any VM during a live migration could still be in a range of few milliseconds to seconds. But many time-sensitive applications and services cannot afford this extended down-time, and their clocks must be perfectly synchronized to ensure no loss of events or information. In such a virtualized environment , clock synchronization with minute precision and error boundedness are one of the most complex and tedious tasks for system performance. In this paper, we propose enhanced DTP and wireless PTP based clock synchronization algorithms to achieve high precision at intra and inter-cloud data center networks. We thoroughly analyze the performance of the proposed algorithms using different clock measurements. Through simulation and real-time experiments, we also show the effect of various performance parameters on the data center networking architectures.},
  archive      = {J_JPDC},
  author       = {Yashwant Singh Patel and Aditi Page and Manvi Nagdev and Anurag Choubey and Rajiv Misra and Sajal K. Das},
  doi          = {10.1016/j.jpdc.2019.11.012},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {15-31},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {On demand clock synchronization for live VM migration in distributed cloud data centers},
  volume       = {138},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DQPFS: Distributed quadratic programming based feature
selection for big data. <em>JPDC</em>, <em>138</em>, 1–14. (<a
href="https://doi.org/10.1016/j.jpdc.2019.12.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of the Big data, the scalability of the machine learning algorithms has become more crucial than ever before. Furthermore, Feature selection as an essential preprocessing technique can improve the performance of the learning algorithms in confront with large-scale dataset by removing the irrelevant and redundant features. Owing to the lack of scalability, most of the classical feature selection algorithms are not so proper to deal with the voluminous data in the Big Data era. QPFS is a traditional feature weighting algorithm that has been used in lots of feature selection applications. By inspiring the classical QPFS , in this paper, a scalable algorithm called DQPFS is proposed based on the novel Apache Spark cluster computing model. The experimental study is performed on three big datasets that have a large number of instances and features at the same time. Then some assessment criteria such as accuracy, execution time, speed-up and scale-out are figured. Moreover, to study more deeply, the results of the proposed algorithm are compared with the classical version QPFS and the DiRelief, a distributed feature selection algorithm proposed recently. The empirical results illustrate that proposed method has (a) better scale-out than DiRelief, (b) significantly lower execution time than DiRelief, (c) lower execution time than QPFS, (d) better accuracy of the Naïve Bayes classifier in two of three datasets than DiRelief.},
  archive      = {J_JPDC},
  author       = {Majid Soheili and Amir Masoud Eftekhari-Moghadam},
  doi          = {10.1016/j.jpdc.2019.12.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-14},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {DQPFS: Distributed quadratic programming based feature selection for big data},
  volume       = {138},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Intrusion detection in edge-of-things computing.
<em>JPDC</em>, <em>137</em>, 259–265. (<a
href="https://doi.org/10.1016/j.jpdc.2019.12.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge-of-Things (EoT) is a new evolving computing model driven by the Internet of Things (IoT). It enables data processing, storage, and service to be shifted from the Cloud to nearby Edge devices/systems such as smartphones, routers, and base stations on the IoT paradigm. However, this architectural shift causes the security and privacy issues to migrate to the different layers of the Edge architecture. Therefore, detecting intrusion in such a distributed environment is difficult. In this scenario, an Intrusion Detection Systems is necessary. Here, we propose an approach to quickly and accurately detect intrusive activities in the EoT network, to realize the full potential of the IoT. Specifically, we propose a deep belief network (DBN) based on an advanced intrusion detection approach. We studied different detection models, by using different structures of DBNs, and compared them with existing detection techniques. Test results show that the proposed methodology performs essentially superior to the current state-of-the-art approaches.},
  archive      = {J_JPDC},
  author       = {Ahmad S. Almogren},
  doi          = {10.1016/j.jpdc.2019.12.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {259-265},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Intrusion detection in edge-of-things computing},
  volume       = {137},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parallel tiled cache and energy efficient codes for o(n4)
RNA folding algorithms. <em>JPDC</em>, <em>137</em>, 252–258. (<a
href="https://doi.org/10.1016/j.jpdc.2019.12.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider two O ( n 4 ) O(n4) RNA folding algorithms, Zuker’s recurrence and the maximum expected accuracy prediction (MEA), which are challenging dynamic programming tasks to optimize because they are computationally intensive and have a large number of non-uniform dependences. We apply our previously published approach to automatically tile and parallelize each loop in the studied algorithms by means of the polyhedral model. First, for each loop nest statement, rectangular tiles are formed within the iteration space of the loop nest. Then, those tiles are corrected to honor all dependences exposed for the original loop nest. Correction is based on applying the exact transitive closure of a dependence graph . We implemented our approach as a part of the source-to-source TRACO compiler, generated target code, and compare the performance and energy consumption of generated code with those of code obtained with the state-of-the-art PluTo compiler based on the affine transformation framework as well as with those of code generated by means of the manual cache-efficient Transpose method. Experiments were carried out on a modern multi-core processor to achieve the significant locality improvement and energy saving for generated code.},
  archive      = {J_JPDC},
  author       = {Marek Palkowski and Wlodzimierz Bielecki},
  doi          = {10.1016/j.jpdc.2019.12.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {252-258},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Parallel tiled cache and energy efficient codes for o(n4) RNA folding algorithms},
  volume       = {137},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A precise non-asymptotic complexity analysis of parallel
hash functions without tree topology constraints. <em>JPDC</em>,
<em>137</em>, 246–251. (<a
href="https://doi.org/10.1016/j.jpdc.2019.10.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A recent work shows how we can optimize a tree based mode of operation for a hash function where the sizes of input message blocks and digest are the same, subject to the constraint that the involved tree structure has all its leaves at the same depth. In this work, we show that we can further optimize the running time of such a mode by using a tree having leaves at all its levels. We make the assumption that the input message block has a size a multiple of that of the digest and denote by d d the ratio block size over digest size. The running time is evaluated in terms of number of operations performed by the hash function , i.e. the number of calls to its underlying function. It turns out that a digest can be computed in ⌈ log d + 1 ( l ∕ 2 ) ⌉ + 2 ⌈logd+1(l∕2)⌉+2 evaluations of the underlying function using ⌈ l ∕ 2 ⌉ ⌈l∕2⌉ processors, where l l is the number of blocks of the message. Other results of interest are discussed, such as the optimization of the parallel running time for a tree of restricted height.},
  archive      = {J_JPDC},
  author       = {Kevin Atighehchi},
  doi          = {10.1016/j.jpdc.2019.10.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {246-251},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A precise non-asymptotic complexity analysis of parallel hash functions without tree topology constraints},
  volume       = {137},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Algorithmic and language-based optimization of marsa-LFIB4
pseudorandom number generator using OpenMP, OpenACC and CUDA.
<em>JPDC</em>, <em>137</em>, 238–245. (<a
href="https://doi.org/10.1016/j.jpdc.2019.12.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this paper is to present new high-performance implementations of Marsa-LFIB4 which is an example of high-quality multiple recursive pseudorandom number generators . We propose an algorithmic approach that combines language-based vectorization techniques together with a new divide-and-conquer parallel method that exploits a special sparse structure of the matrix obtained from the recursive formula that defines the generator. Our portable OpenACC implementation achieves the performance comparable to those achieved by our CUDA-based and OpenMP-based implementations on GPUs and multicore CPUs, respectively.},
  archive      = {J_JPDC},
  author       = {Przemysław Stpiczyński},
  doi          = {10.1016/j.jpdc.2019.12.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {238-245},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Algorithmic and language-based optimization of marsa-LFIB4 pseudorandom number generator using OpenMP, OpenACC and CUDA},
  volume       = {137},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Blockchain-based accountability for multi-party oblivious
RAM. <em>JPDC</em>, <em>137</em>, 224–237. (<a
href="https://doi.org/10.1016/j.jpdc.2019.10.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, oblivious random access machine (ORAM) has been widely used to prevent privacy leakage from user’s access pattern. However, in multi-user scenarios, the obliviousness property of ORAM facilitates the malicious data modification by unauthorized users, which brings a new security challenge of user accountability to ORAM applications. Moreover, based on our observations, existing user accountability schemes for multi-user ORAM induce the extremely unacceptable overhead in both time and storage. What is worse, it is still inherent the traditional cloud accountability problem that the untrusted cloud server may have misbehavior on storing the outsourced data . In this paper, we focus on the issue that how to do accountability for both malicious users and untrusted cloud server without the independent trusted third party server. To address the above problem, we design and implement a Traceable Oblivious RAM , or T-ORAM for short, a cryptographic system that protects the privacy of users and the integrity of outsourced data based on group signatures . It can detect malicious users quickly by utilizing the traceability property of group signatures , and cost less storage overhead comparing with the existing solutions. Then, we further propose a more secure solution of Blockchain-based Traceable Oblivious RAM (BT-ORAM). Specifically, by introducing the blockchain technology, BT-ORAM can detect the malicious behavior from both malicious users and untrusted cloud server. BT-ORAM is the first accountability work for multi-user ORAM that deal with both malicious users and the untrusted cloud server. Finally, security analysis and experimental results show that our method outperforms the state-of-the-art accountability work for oblivious RAM, S-GORAM, in both security and performance.},
  archive      = {J_JPDC},
  author       = {Huikang Cao and Ruixuan Li and Wenlong Tian and Zhiyong Xu and Weijun Xiao},
  doi          = {10.1016/j.jpdc.2019.10.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {224-237},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Blockchain-based accountability for multi-party oblivious RAM},
  volume       = {137},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joker: Elastic stream processing with organic adaptation.
<em>JPDC</em>, <em>137</em>, 205–223. (<a
href="https://doi.org/10.1016/j.jpdc.2019.10.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problem of auto-parallelization of streaming applications. We propose an online parallelization optimization algorithm that adjusts the degree of pipeline and data parallelism in a joint manner. We define an operator development API and a flexible parallel execution model to form a basis for the optimization algorithm . The operator interface unifies the development of different types of operators and makes operator properties visible in order to enable safe optimizations. The parallel execution model splits a data flow graph into regions . A region contains the longest sequence of compatible operators that are amenable to data parallelism as a whole and can be further parallelized with pipeline parallelism . We also develop a stream processing run-time, named Joker , to scale the execution of streaming applications in a safe, transparent, dynamic, and automatic manner . This ability is called organic adaptation . Joker implements the runtime machinery to execute a data flow graph with any parallelization configuration and most importantly change this configuration at run-time with low cost in the presence of partitioned stateful operators, in a way that is transparent to the application developers. Joker continuously monitors the run-time performance, and runs the optimization algorithm to resolve bottlenecks and scale the application by adjusting the degree of pipeline and data parallelism. The experimental evaluation based on micro-benchmarks and real-world applications showcase that our solution accomplishes elasticity by finding an effective parallelization configuration.},
  archive      = {J_JPDC},
  author       = {Basri Kahveci and Buğra Gedik},
  doi          = {10.1016/j.jpdc.2019.10.012},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {205-223},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Joker: Elastic stream processing with organic adaptation},
  volume       = {137},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards secure big data analytic for cloud-enabled
applications with fully homomorphic encryption. <em>JPDC</em>,
<em>137</em>, 192–204. (<a
href="https://doi.org/10.1016/j.jpdc.2019.10.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing empowers enterprises to efficiently manage big data and discovery of useful information which are the most fundamental challenges for big data enabled applications. The cloud offered unlimited resources that can store, manage and analyze massive and heterogeneous data to improve the quality assurance of application services. Nevertheless, cloud computing is exposed to enormous external and internal privacy breaches and leakage threats. In this paper, we introduce a privacy-preserving distributed analytics framework for big data in cloud. Fully Homomorphic Encryption (FHE) is used as an emerging and powerful cryptosystem that can carry out analysis tasks on encrypted data . The developed distributed approach has the scalability to partition both data and analysis computations into subset cloud computing nodes that can be run independently. This rapidly accelerates the performance of encrypted data processing while preserving a high level of analysis accuracy. Our experimental evaluation demonstrates the efficiency of the proposed framework, in terms of both analysis performance and accuracy, for building a secure analytics cloud-enabled application.},
  archive      = {J_JPDC},
  author       = {Abdulatif Alabdulatif and Ibrahim Khalil and Xun Yi},
  doi          = {10.1016/j.jpdc.2019.10.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {192-204},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Towards secure big data analytic for cloud-enabled applications with fully homomorphic encryption},
  volume       = {137},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient authentication protocol with anonymity and key
protection for mobile internet users. <em>JPDC</em>, <em>137</em>,
179–191. (<a href="https://doi.org/10.1016/j.jpdc.2019.11.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To preserve user privacy and guarantee data confidentiality on the mobile Internet, it is crucial to secure communication between the mobile devices held by users and a remote server. In real applications, a serious threat against communication security is exposure of secret keys, due to the compromise of the mobile devices storing the key. One method of preserving key exposure is to use protected hardware or smart-cards, but they are costly and impractical. Another method is to utilize secret sharing to share secret key across multiple devices. Nevertheless, secret sharing schemes guarantee security only if the adversary cannot access at least one share in its entirety. In this paper, we present a remote authentication protocol , which resists key exposure. Further, we present a zero-knowledge protocol based on SDH assumption that can achieve anonymity. We formally prove our proposed solution is secure under the decision linear assumption and the q s qs -mSDH assumption in the random oracle model . Finally, we show our solution can achieve higher efficiency and stronger anonymity comparing with existing schemes, and thus the proposed solution is more suitable for real-world environments.},
  archive      = {J_JPDC},
  author       = {Yan Jiang and Youwen Zhu and Jian Wang and Yong Xiang},
  doi          = {10.1016/j.jpdc.2019.11.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {179-191},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Efficient authentication protocol with anonymity and key protection for mobile internet users},
  volume       = {137},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving the performance of physics applications in
atom-based clusters with rCUDA. <em>JPDC</em>, <em>137</em>, 160–178.
(<a href="https://doi.org/10.1016/j.jpdc.2019.11.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditionally, High-Performance Computing (HPC) has been associated with large power requirements. The reason was that chip makers of the processors typically employed in HPC deployments have always focused on getting the highest performance from their designs, regardless of the energy their processors may consume. Actually, for many years only heat dissipation was the real barrier for achieving higher performance, at the cost of higher energy consumption. However, a new trend has recently appeared consisting on the use of low-power processors for HPC purposes. The MontBlanc and Isambard projects are good examples of this trend. These proposals, however, do not consider the use of GPUs . In this paper we propose to use GPUs in this kind of low-power processor based HPC deployments by making use of the remote GPU virtualization mechanism. To that end, we leverage the rCUDA middleware in a hybrid cluster composed of low-power Atom-based nodes and regular Xeon-based nodes equipped with GPUs. Our experiments show that, by making use of rCUDA, the execution time of applications belonging to the physics domain is noticeably reduced, achieving a speed up of up to 140x with just one remote NVIDIA V100 GPU with respect to the execution of the same applications using 8 Atom-based nodes. Additionally, a rough energy consumption estimation reports improvements in energy demands of up to 37x.},
  archive      = {J_JPDC},
  author       = {Federico Silla and Javier Prades and Elvira Baydal and Carlos Reaño},
  doi          = {10.1016/j.jpdc.2019.11.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {160-178},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Improving the performance of physics applications in atom-based clusters with rCUDA},
  volume       = {137},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Batched transpose-free ADI-type preconditioners for a
poisson solver on GPGPUs. <em>JPDC</em>, <em>137</em>, 148–159. (<a
href="https://doi.org/10.1016/j.jpdc.2019.11.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the iterative solution of a symmetric positive definite linear system involving the shifted Laplacian as the system matrix on General Purpose Graphics Processing Units (GPGPUs). We consider in particular the Chebyshev iteration for its reduced global communication. The ADI-type preconditioner involves solving multiple (batched) symmetric positive tridiagonal Toeplitz systems along each coordinate direction. We investigate several variants how to solve these tridiagonal systems , the Thomas algorithm , the Thomas combined with the SPIKE algorithm, and a polynomial approximation of the inverse. We test the various implementations numerically by means of two- and three-dimensional examples. It turns out that a combination of the Thomas algorithm and the approximate inverse leads to a solution that does not need either tiling or transpositions. As such none of the kernels uses an extensive amount of shared memory which yields a very high GPU utilization and more importantly optimal coalesced global memory access patterns.},
  archive      = {J_JPDC},
  author       = {Peter Arbenz and Lubomír Říha},
  doi          = {10.1016/j.jpdc.2019.11.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {148-159},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Batched transpose-free ADI-type preconditioners for a poisson solver on GPGPUs},
  volume       = {137},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parallel implementation of the image block representation
using OpenMP. <em>JPDC</em>, <em>137</em>, 134–147. (<a
href="https://doi.org/10.1016/j.jpdc.2019.11.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Herein, a parallel implementation in OpenMP of the Image Block Representation (IBR) for binary imagesis investigated. The IBR is a region-based image representation scheme that represents the binary image as a set of non-overlapping rectangular areas with object level, called blocks . The IBR permits the execution of operations on image areas instead of image points and therefore leads to a substantial reduction of the required computational complexity . The experimental and the analytically derived results from parallel implementation in OpenMP, on a multicore computer, proved that a very good overall performance can be achieved.},
  archive      = {J_JPDC},
  author       = {Iraklis M. Spiliotis and Michael P. Bekakos and Yiannis S. Boutalis},
  doi          = {10.1016/j.jpdc.2019.11.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {134-147},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Parallel implementation of the image block representation using OpenMP},
  volume       = {137},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Performance evaluation of decision making under uncertainty
for low power heterogeneous platforms. <em>JPDC</em>, <em>137</em>,
119–133. (<a href="https://doi.org/10.1016/j.jpdc.2019.11.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Value Iteration (VI) is a core method to find optimal policies , allowing a robot to act autonomously in environments where the effects of its actions are not deterministic. Although there are extensive studies on VI’s theoretical properties and computational cost, its energy performance — an essential indicator for its use in the physical world — has not been evaluated. In this paper, we explore both the energy and runtime performance of five parallel implementation strategies of VI on representative low-power heterogeneous computing platforms that integrate CPUs and GPUs , for the use-case scenario of indoor autonomous robot navigation. We provide a statistical analysis of their performance depending on the problem size, parallel implementation and computing platform. Our study shows that CPU–GPU heterogeneous strategies reduce computation time and energy considerably, given large enough problem sizes, regardless of the computation platform. This work also provides practical guidelines to assist in the application of the most efficient implementation, either in terms of energy consumption or time, to a low-power heterogeneous platform .},
  archive      = {J_JPDC},
  author       = {Denisa-Andreea Constantinescu and Angeles Navarro and Juan-Antonio Fernández-Madrigal and Rafael Asenjo},
  doi          = {10.1016/j.jpdc.2019.11.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {119-133},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Performance evaluation of decision making under uncertainty for low power heterogeneous platforms},
  volume       = {137},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). HDOT — an approach towards productive programming of hybrid
applications. <em>JPDC</em>, <em>137</em>, 104–118. (<a
href="https://doi.org/10.1016/j.jpdc.2019.11.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A wealth of important scientific and engineering applications are configured for use on high performance computing architectures using functionality found in the MPI specification. This specification provides application developers with a straightforward means for implementing their ideas for execution on distributed-memory parallel processing computers. OpenMP directives provide a means for operating on shared-memory regions of those computers. With the advent of machines composed of many-core processors, the strict synchronisation required by the bulk synchronous parallel (BSP) communication model can hinder performance increases. This is due to the complexity to handle load imbalances, to reduce serialisation imposed by blocking communication patterns, to overlap communication with computation and, finally, to deal with increasing memory overheads. The MPI specification provides advanced features such as non-blocking calls or shared memory to mitigate some of these factors. However, applying these features efficiently usually requires significant changes on the application structure. Task parallel programming models are being developed as a means of mitigating the abovementioned issues but without requiring extensive changes on the application code. In this work, we present a methodology to develop hybrid applications based on tasks called hierarchical domain over-decomposition with tasking (HDOT). This methodology overcomes most of the issues found on MPI-only and traditional hybrid MPI+OpenMP applications. However, by emphasising the reuse of data partition schemes from process-level and applying them to task-level, it enables a natural coexistence between MPI and shared-memory programming models. The proposed methodology shows promising results in terms of programmability and performance measured on a set of applications.},
  archive      = {J_JPDC},
  author       = {Jan Ciesko and Pedro J. Martínez-Ferrer and Raúl Peñacoba Veigas and Xavier Teruel and Vicenç Beltran},
  doi          = {10.1016/j.jpdc.2019.11.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {104-118},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {HDOT — an approach towards productive programming of hybrid applications},
  volume       = {137},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Securing instruction interaction for hierarchical
management. <em>JPDC</em>, <em>137</em>, 91–103. (<a
href="https://doi.org/10.1016/j.jpdc.2019.10.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical management is a typical approach to managing complex information networks (CINs). In hierarchical management, as the only avenue to control information storage, transmission and usage, instruction interaction is often exploited by attackers to threaten CINs security . Now it has become a major challenge to ensure the security of instruction interaction. To address this challenge, in this paper, we propose a security model to manage instruction interaction for the hierarchical management in CINs. First, considering instruction lifecycle, we design five basic instruction operations ( i.e. , instruction generation, distribution, decomposition, execution, and execution outcome feedback) and formally define their semantics. Then a series of security rules for the basic operations are proposed to monitor and control the instruction interactions. We prove that, through these rules, both controllability and confidentiality can be provided. Finally, case studies demonstrate the feasibility of the proposed model.},
  archive      = {J_JPDC},
  author       = {Fenghua Li and Zifu Li and Liang Fang and Yongjun Li and Yaobing Xu and Yunchuan Guo},
  doi          = {10.1016/j.jpdc.2019.10.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {91-103},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Securing instruction interaction for hierarchical management},
  volume       = {137},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). KASLR-MT: Kernel address space layout randomization for
multi-tenant cloud systems. <em>JPDC</em>, <em>137</em>, 77–90. (<a
href="https://doi.org/10.1016/j.jpdc.2019.11.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing has completely changed our lives. This technology dramatically impacted on how we play, work and live. It has been widely adopted in many sectors mainly because it reduces the cost of performing tasks in a flexible, scalable and reliable way. To provide a secure cloud computing architecture, the highest possible level of protection must be applied. Unfortunately, the cloud computing paradigm introduces new scenarios where security protection techniques are weakened or disabled to obtain a better performance and resources exploitation. Kernel ASLR (KASLR) is a widely adopted protection technique present in all modern operating systems . KASLR is a very effective technique that thwarts unknown attacks but unfortunately its randomness have a significant impact on memory deduplication savings. Both techniques are very desired by the industry, the first one because of the high level of security that it provides and the latter to obtain better performance and resources exploitation. In this paper, we propose KASLR-MT, a new Linux kernel randomization approach compatible with memory deduplication . We identify why the most widely and effective technique used to mitigate attacks at kernel level, KASLR, fails to provide protection and shareability at the same time. We analyze the current Linux kernel randomization and how it affects to the shared memory of each kernel region. Then, based on the analysis, we propose KASLR-MT, the first effective and practical Kernel ASLR memory protection that maximizes the memory deduplication savings rate while providing a strong security. Our tests reveal that KASLR-MT is not intrusive, very scalable and provides strong protection without sacrificing the shareability.},
  archive      = {J_JPDC},
  author       = {Fernando Vano-Garcia and Hector Marco-Gisbert},
  doi          = {10.1016/j.jpdc.2019.11.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {77-90},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {KASLR-MT: Kernel address space layout randomization for multi-tenant cloud systems},
  volume       = {137},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hitchhiker’s guide on distributed training of deep neural
networks. <em>JPDC</em>, <em>137</em>, 65–76. (<a
href="https://doi.org/10.1016/j.jpdc.2019.10.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has led to tremendous advancements in the field of Artificial Intelligence . One caveat, however, is the substantial amount of compute needed to train these deep learning models. Training a benchmark dataset like ImageNet on a single machine with a modern GPU can take up to a week and distributing training on multiple machines has been observed to drastically bring this time down . Recent work has brought down ImageNet training time to as low as 4 min by using a cluster of 2048 GPUs. This paper surveys the various algorithms and techniques used in distributed training and presents the current state of the art for a modern distributed training framework. More specifically, we explore the synchronous and asynchronous variants of distributed Stochastic Gradient Descent , various All Reduce gradient aggregation strategies and best practices for obtaining higher throughput and lower latency over a cluster such as mixed precision training, large batch training, and gradient compression.},
  archive      = {J_JPDC},
  author       = {Karanbir Singh Chahal and Manraj Singh Grover and Kuntal Dey and Rajiv Ratn Shah},
  doi          = {10.1016/j.jpdc.2019.10.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {65-76},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A hitchhiker’s guide on distributed training of deep neural networks},
  volume       = {137},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved GPU near neighbours performance for multi-agent
simulations. <em>JPDC</em>, <em>137</em>, 53–64. (<a
href="https://doi.org/10.1016/j.jpdc.2019.11.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex systems simulations are well suited to the SIMT paradigm of GPUs , enabling millions of actors to be processed in fractions of a second. At the core of many such simulations, fixed radius near neighbours (FRNN) search provides the actors with spatial awareness of their neighbours. The FRNN search process is frequently the limiting factor of performance, due to the disproportionate level of scattered memory reads demanded by the query stage, leading to FRNN search runtimes exceeding that of simulation logic. In this paper, we propose and evaluate two novel optimisations (Strips and Proportional Bin Width) for improving the performance of uniform spatially partitioned FRNN searches and apply them in combination to demonstrate the impact on the performance of multi-agent simulations. The two approaches aim to reduce latency in search and reduce the amount of data considered (i.e. more efficient searching), respectively. When the two optimisations are combined, the peak obtained speedups observed in a benchmark model are 1.27x and 1.34x in two and three dimensional implementations, respectively. Due to additional non FRNN search computation, the peak speedup obtained when applied to complex system simulations within FLAMEGPU is 1.21x.},
  archive      = {J_JPDC},
  author       = {Robert Chisholm and Steve Maddock and Paul Richmond},
  doi          = {10.1016/j.jpdc.2019.11.002},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {53-64},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Improved GPU near neighbours performance for multi-agent simulations},
  volume       = {137},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiple pattern matching for network security applications:
Acceleration through vectorization. <em>JPDC</em>, <em>137</em>, 34–52.
(<a href="https://doi.org/10.1016/j.jpdc.2019.10.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As both new network attacks emerge and network traffic increases in volume, the need to perform network traffic inspection at high rates is ever increasing. The core of many security applications that inspect network traffic (such as Network Intrusion Detection) is pattern matching . At the same time, pattern matching is a major performance bottleneck for those applications: indeed, it is shown to contribute to more than 70\% of the total running time of Intrusion Detection Systems . Although numerous efficient approaches to this problem have been proposed on custom hardware, it is challenging for pattern matching algorithms to gain benefit from the advances in commodity hardware. This becomes even more relevant with the adoption of Network Function Virtualization , that moves network services , such as Network Intrusion Detection , to the cloud, where scaling on commodity hardware is key for performance. In this paper, we tackle the problem of pattern matching and show how to leverage the architecture features found in commodity platforms. We present efficient algorithmic designs that achieve good cache locality and make use of modern vectorization techniques to utilize data parallelism within each core. We first identify properties of pattern matching that make it fit for vectorization and show how to use them in the algorithmic design. Second, we build on an earlier, cache-aware algorithmic design and show how we apply cache-locality combined with SIMD gather instructions to pattern matching. Third, we complement our algorithms with an analytical model that predicts their performance and that can be used to easily evaluate alternative designs. We evaluate our algorithmic design with open data sets of real-world network traffic: Our results on two different platforms, Haswell and Xeon-Phi, show a speedup of 1.8x and 3.6x, respectively, over Direct Filter Classification (DFC), a recently proposed algorithm by Choi et al. for pattern matching exploiting cache locality, and a speedup of more than 2.3x over Aho–Corasick, a widely used algorithm in today’s Intrusion Detection Systems . Finally, we utilize highly parallel hardware platforms, evaluate the scalability of our algorithms and compare it to parallel implementations of DFC and Aho–Corasick, achieving processing throughput of up to 45Gbps and close to 2 times higher throughput than Aho–Corasick.},
  archive      = {J_JPDC},
  author       = {Charalampos Stylianopoulos and Magnus Almgren and Olaf Landsiedel and Marina Papatriantafilou},
  doi          = {10.1016/j.jpdc.2019.10.011},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {34-52},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Multiple pattern matching for network security applications: Acceleration through vectorization},
  volume       = {137},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Malware detection in mobile environments based on
autoencoders and API-images. <em>JPDC</em>, <em>137</em>, 26–33. (<a
href="https://doi.org/10.1016/j.jpdc.2019.11.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to their open nature and popularity, Android-based devices represent one of the main targets for malware attacks that may adversely affect the privacy of their users. Considering the huge Android market share, it is necessary to build effective tools able to reliably detect zero-day malware on these platforms. Therefore, several static and dynamic analysis methods based on Neural Networks and Deep Learning have been proposed in the literature. Despite machine learning can be considered the most promising approach for classifying applications into malware or legitimate ones, its success strongly depends on the choice of the right features used for building the detection model. This is definitely not an easy task that requires a systematic solution. Accordingly, this work represents the sequences of API calls invoked by apps during their execution as sparse matrices looking like images (API-images), which can be used as fingerprints of the apps’ behavior over time. We also used autoencoders to autonomously extract the most representative and discriminating features from these matrices, that, once provided to an artificial neural network-based classifier have shown to be effective in detecting malware, also when the network is trained on a reduced number of samples. Experimental results show that the resulting framework is able to outperform more complex and sophisticated machine learning approaches in malware classification.},
  archive      = {J_JPDC},
  author       = {Gianni D’Angelo and Massimo Ficco and Francesco Palmieri},
  doi          = {10.1016/j.jpdc.2019.11.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {26-33},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Malware detection in mobile environments based on autoencoders and API-images},
  volume       = {137},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variational approach for privacy funnel optimization on
continuous data. <em>JPDC</em>, <em>137</em>, 17–25. (<a
href="https://doi.org/10.1016/j.jpdc.2019.09.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Here we consider a common data encryption problem encountered by users who want to disclose some data to gain utility but preserve their private information. Specifically, we consider the inference attack, in which an adversary conducts inference on the disclosed data to gain information about users’ private data. Following privacy funnel (Makhdoumi et al., 2014), assuming that the original data X X is transformed into Z Z before disclosing and the log loss is used for both privacy and utility metrics, then the problem can be modeled as finding a mapping X → Z X→Z that maximizes mutual information between X X and Z Z subject to a constraint that the mutual information between Z Z and private data S S is smaller than a predefined threshold ϵ ϵ . In contrast to the original study (Makhdoumi et al., 2014), which only focused on discrete data, we consider the more general and practical setting of continuous and high-dimensional disclosed data (e.g., image data). Most previous work on privacy-preserving representation learning is based on adversarial learning or generative adversarial networks , which has been shown to suffer from the vanishing gradient problem, and it is experimentally difficult to eliminate the relationship with private data Y Y when Z Z is constrained to retain more information about X X . Here we propose a simple but effective variational approach that does not rely on adversarial training . Our experimental results show that our approach is stable and outperforms previous methods in terms of both downstream task accuracy and mutual information estimation.},
  archive      = {J_JPDC},
  author       = {Lihao Nan and Dacheng Tao},
  doi          = {10.1016/j.jpdc.2019.09.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {17-25},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Variational approach for privacy funnel optimization on continuous data},
  volume       = {137},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). COMITMENT: A fog computing trust management approach.
<em>JPDC</em>, <em>137</em>, 1–16. (<a
href="https://doi.org/10.1016/j.jpdc.2019.10.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an extension of cloud computing , fog computing is considered to be relatively more secure than cloud computing due to data being transiently maintained and analyzed on local fog nodes closer to data sources. However, there exist several security and privacy concerns when fog nodes collaborate and share data to execute certain tasks. For example, offloading data to a malicious fog node can result into an unauthorized collection or manipulation of users’ private data. Cryptographic-based techniques can prevent external attacks, but are not useful when fog nodes are already authenticated and part of a networks using legitimate identities. We therefore resort to trust to identify and isolate malicious fog nodes and mitigate security, respectively. In this paper, we present a fog COMputIng Trust manageMENT (COMITMENT) approach that uses quality of service and quality of protection history measures from previous direct and indirect fog node interactions for assessing and managing the trust level of the nodes within the fog computing environment. Using COMITMENT approach, we were able to reduce/identify the malicious attacks/interactions among fog nodes by approximately 66\%, while reducing the service response time by approximately 15 s.},
  archive      = {J_JPDC},
  author       = {Mohammed Al-khafajiy and Thar Baker and Muhammad Asim and Zehua Guo and Rajiv Ranjan and Antonella Longo and Deepak Puthal and Mark Taylor},
  doi          = {10.1016/j.jpdc.2019.10.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-16},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {COMITMENT: A fog computing trust management approach},
  volume       = {137},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Edge-based differential privacy computing for sensor–cloud
systems. <em>JPDC</em>, <em>136</em>, 75–85. (<a
href="https://doi.org/10.1016/j.jpdc.2019.10.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In sensor–cloud systems, with more personal data being hosted in cloud, privacy leakage is becoming one of the most serious concerns. Privacy computing is emerging as a paradigm to systematically enhance privacy protection. In other words, the new paradigm requests us to improve the computing model to provide a general privacy protection service. In this paper, we propose an edge-based model for data collection, in which the raw data from wireless sensor networks (WSNs) is differentially processed by algorithms on edge servers for privacy computing. A small quantity of the core data is stored on edge and local servers while the rest is transmitted to cloud for storage. In this way, the benefits are twofold. First, the data privacy is preserved since the original data cannot be retrieved even if the data stored in the cloud is leaked. Second, implemented by a differential storage method, compared to the state of the art, the edge-based model sends less data to the cloud and reduces the cost of communication and storage. Both theoretical analyses and extensive experiments validate our proposed method.},
  archive      = {J_JPDC},
  author       = {Tian Wang and Yaxin Mei and Weijia Jia and Xi Zheng and Guojun Wang and Mande Xie},
  doi          = {10.1016/j.jpdc.2019.10.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {75-85},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Edge-based differential privacy computing for sensor–cloud systems},
  volume       = {136},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FFT, FMM, and multigrid on the road to exascale: Performance
challenges and opportunities. <em>JPDC</em>, <em>136</em>, 63–74. (<a
href="https://doi.org/10.1016/j.jpdc.2019.09.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {FFT , FMM , and multigrid methods are widely used fast and highly scalable solvers for elliptic PDEs. However, emerging large-scale computing systems are introducing challenges in comparison to current petascale computers. Recent efforts (Dongarra et al. 2011) have identified several constraints in the design of exascale software that include massive concurrency, resilience management, exploiting the high performance of heterogeneous systems , energy efficiency, and utilizing the deeper and more complex memory hierarchy expected at exascale. In this paper, we perform a model-based comparison of the FFT , FMM , and multigrid methods in the context of these projected constraints. In addition we use performance models to offer predictions about the expected performance on upcoming exascale system configurations based on current technology trends.},
  archive      = {J_JPDC},
  author       = {Huda Ibeid and Luke Olson and William Gropp},
  doi          = {10.1016/j.jpdc.2019.09.014},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {63-74},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {FFT, FMM, and multigrid on the road to exascale: Performance challenges and opportunities},
  volume       = {136},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fault-tolerant least squares solvers for wireless sensor
networks based on gossiping. <em>JPDC</em>, <em>136</em>, 52–62. (<a
href="https://doi.org/10.1016/j.jpdc.2019.09.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many applications in large loosely connected distributed networks (such as wireless sensor networks) require the distributed solution of linear least squares (dLLS) problems. Ideally, a truly distributed algorithm should require very little coordination between the nodes. This favours algorithms which do not require a fusion centre, cluster heads or any multi-hop communication. We present the novel dLLS solver GLS-IR for overdetermined linear systems. We investigate two variants of our novel solver, one of them based on the semi-normal equations, the other based on the normal equations. Both are combined with iterative refinement in mixed precision, which not only stabilises the methods but also decreases the communication cost. In GLS-IR, all communication between nodes is contained within a gossip-based algorithm for distributed aggregation, which limits the communication of each node to its immediate neighbourhood. Therefore, GLS-IR benefits directly from efficient and fault-tolerant algorithms for distributed aggregation. We use a fault-tolerant alternative to the push-sum method, the push-flow algorithm, which is able to recover from silent message loss and temporary or permanent link failures. We analytically compare the communication cost of GLS-IR to existing truly distributed algorithms. Since the theoretical analysis contains problem-dependent parameters, numerical experiments are needed in order to get a complete picture. Our simulation experiments illustrate a significantly reduced communication cost of GLS-IR compared to other existing truly distributed least squares solvers. We also illustrate that due to the properties of iterative refinement and push-flow, GLS-IR can achieve a result accurate to machine precision even if a high amount of message loss occurs.},
  archive      = {J_JPDC},
  author       = {Karl E. Prikopa and Wilfried N. Gansterer},
  doi          = {10.1016/j.jpdc.2019.09.006},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {52-62},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Fault-tolerant least squares solvers for wireless sensor networks based on gossiping},
  volume       = {136},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Blockchain for secure location verification. <em>JPDC</em>,
<em>136</em>, 40–51. (<a
href="https://doi.org/10.1016/j.jpdc.2019.10.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In location-sensitive applications, dishonest users may submit fake location claims to illegally access a service or obtain benefit. To address this issue, a number of location proof mechanisms have been proposed in literature. However, they confront various security and privacy challenges, including Prover–Prover collusions (Terrorist Frauds), Prover–Witness collusions, and location privacy threats. In this paper, we utilize the unique features of the blockchain technology to design a decentralized scheme for location proof generation and verification. In the proposed scheme, a user who needs a location proof (called a prover ) broadcasts a request to the neighbor devices through a short-range communication interface, e.g. Bluetooth. Those neighbor devices that decide to respond (called witnesses ) start to authenticate the requesting user. We integrate an incentive mechanism into the proposed scheme to reward such witnesses. Upon successful authentication , a transaction is generated as a location proof and is broadcast onto a peer-to-peer network where it can be picked up by verifiers for final verification. Our security analysis shows that the proposed scheme achieves a reliable performance against Prover–Prover and Prover–Witness collusions. Moreover, our prototype implementation on the Android platform shows that the proposed scheme outperforms other currently deployed location proof schemes.},
  archive      = {J_JPDC},
  author       = {Mohammad Reza Nosouhi and Shui Yu and Wanlei Zhou and Marthie Grobler and Habiba Keshtiar},
  doi          = {10.1016/j.jpdc.2019.10.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {40-51},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Blockchain for secure location verification},
  volume       = {136},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A practical group blind signature scheme for privacy
protection in smart grid. <em>JPDC</em>, <em>136</em>, 29–39. (<a
href="https://doi.org/10.1016/j.jpdc.2019.09.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The leakage of privacy is one of the key factors to restrict the development of smart grid. Currently, research works of protecting privacy in smart grid mainly focused on two aspects: (1) data aggregation based on the mathematical model and algorithm and (2) user anonymous authentication . However, data aggregation is at cost of obtaining fine-grained electricity consumption information to protect privacy. Anonymous authentication cannot identify the malicious user in previous studies. Hence, in this work, we propose a group blind signature scheme in smart grid to accomplish conditional anonymity. Furthermore, the integrity of consumption data can be verified by homomorphic encryption (HE) which can decrease the communication overhead between control center and smart meter remarkably. From the security analysis and experiment simulation, the results show that our scheme is safe and efficient.},
  archive      = {J_JPDC},
  author       = {Wei Kong and Jian Shen and Pandi Vijayakumar and Youngju Cho and Victor Chang},
  doi          = {10.1016/j.jpdc.2019.09.016},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {29-39},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A practical group blind signature scheme for privacy protection in smart grid},
  volume       = {136},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient fault tolerant workflow scheduling approach
using replication heuristics and checkpointing in the cloud.
<em>JPDC</em>, <em>136</em>, 14–28. (<a
href="https://doi.org/10.1016/j.jpdc.2019.09.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scientific workflows have been predominantly used for complex and large scale data analysis and scientific computation/automation and the need for robust workflow scheduling techniques has grown considerably. But, most of the existing workflow scheduling algorithms do not provide the required reliability and robustness. In this paper, a new fault tolerant workflow scheduling algorithm that learns replication heuristics in an unsupervised manner has been proposed. Furthermore, the use of light weight synchronized checkpointing enables efficient resubmission of failed tasks and ensures workflow completion even in precarious environments. The proposed technique improves upon metrics like Resource Wastage and Resource Usage in comparison to the Replicate-All algorithm, while maintaining an acceptable increase in Makespan as compared to the vanilla Heterogeneous Earliest Finish Time (HEFT).},
  archive      = {J_JPDC},
  author       = {Amrith Rajagopal Setlur and S. Jaya Nirmala and Har Simrat Singh and Sudhanshu Khoriya},
  doi          = {10.1016/j.jpdc.2019.09.004},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {14-28},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {An efficient fault tolerant workflow scheduling approach using replication heuristics and checkpointing in the cloud},
  volume       = {136},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Budgeted video replacement policy in mobile crowdsensing.
<em>JPDC</em>, <em>136</em>, 1–13. (<a
href="https://doi.org/10.1016/j.jpdc.2019.10.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile crowdsensing offers a new platform that recruits a suitable set of users to collectively complete an information collection/sensing task through users’ equipped devices. As a special case, video crowdsensing is to collect different video segments of the same event that are taken separately by the built-in cameras of mobile devices , and then combine them into a complete video. Mobile crowdsensing has attracted considerable attention recently due to the rich information that can be provided by videos. However, because of the limited caching space, a suitable video replacement policy is necessary. In this paper, we propose a Budgeted Video replaCement policy in mobile Video crowdsensing (BVCV), which first determines a video segment’s value according to its caching situation and natural attributes. Then, we formulate the video caching problem as a budgeted maximum coverage problem, which is a well-known NP-hard problem. Finally, we propose a practical greedy solution and also infer the approximate ratio, which could be regarded as the lower bound of BVCV to the optimal solution. Our experiments with the real mobility datasets (StudentLife dataset, Buffalo/phonelab-wifi dataset) show that, the proposed budgeted video replacement policy achieves a longer successfully delivered video length, compared with other general replacement policies.},
  archive      = {J_JPDC},
  author       = {En Wang and Yongjian Yang and Jie Wu and Kaihao Lou and Wenbin Liu and Yuanbo Xu},
  doi          = {10.1016/j.jpdc.2019.10.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-13},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Budgeted video replacement policy in mobile crowdsensing},
  volume       = {136},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Full title. <em>JPDC</em>, <em>135</em>, i. (<a
href="https://doi.org/10.1016/S0743-7315(19)30719-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JPDC},
  doi          = {10.1016/S0743-7315(19)30719-1},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {i},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Full title},
  volume       = {135},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimistic scheduling with service guarantees.
<em>JPDC</em>, <em>135</em>, 246–258. (<a
href="https://doi.org/10.1016/j.jpdc.2019.04.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data centers form the core of current information and commercial enterprise. At current scale, any improvement in data center resource utilization leads to substantial savings. We focus on the problem of scheduling jobs in distributed execution environments to improve resource utilization. Cluster schedulers like YARN and Mesos base their scheduling decisions on resource requirements provided by end users. It is hard for end-users to predict the exact amount of resources required for a task/job, especially since resource utilization can vary significantly over time and across tasks. In practice, users make highly conservative estimates of peak utilization across all tasks of a job to ensure job completion, leading to resource fragmentation and severe under utilization in production clusters. We present UBIS, a utilization-aware approach to cluster scheduling, to address resource fragmentation, and to improve cluster utilization and job throughput. UBIS considers actual usage of running tasks and schedules opportunistic work on under-utilized nodes. UBIS monitors resource usage on these nodes and preempts opportunistic containers in the event this over-subscription becomes untenable. In doing so, UBIS effectively utilizes wasted resources, while minimizing adverse effects on regularly scheduled tasks. Our implementation of UBIS on YARN demonstrates improvements of up to 30\% in makespan for a representative workload and 25\% in individual job durations.},
  archive      = {J_JPDC},
  author       = {Karthik Kambatla and Vamsee Yarlagadda and Íñigo Goiri and Ananth Grama},
  doi          = {10.1016/j.jpdc.2019.04.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {246-258},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Optimistic scheduling with service guarantees},
  volume       = {135},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interval stabbing on the automata processor. <em>JPDC</em>,
<em>135</em>, 234–245. (<a
href="https://doi.org/10.1016/j.jpdc.2018.01.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Automata Processor (AP) was designed for string-pattern matching. In this paper, we showcase its use to execute integer and floating-point comparisons and apply the same to accelerate interval stabbing queries . An interval stabbing query determines which of the intervals in a set overlap a query point . Such queries are often used in computational geometry , pattern matching , database management systems , and geographic information systems. The check for each interval is programmed as a single automaton and multiple automata are executed in parallel to provide significant performance gains. While handling 32-bit integers or single-precision floating-point numbers, up to 2.75 trillion comparisons can be executed per second, whereas 0.79 trillion comparisons per second can be completed for 64-bit integers or double-precision floating-point numbers. Additionally, our solution leaves the intervals in the set unordered allowing addition or deletion of an interval in constant time. This is not possible for contemporary solutions wherein the intervals are ordered, making update of intervals complex. Our automata designs are modular allowing them to become constituent parts of larger automata, where the numerical comparisons are part of the overall pattern matching operation. We have validated the designs on the hardware, and the routines to generate the necessary automata and execute them on the AP will be made available as software libraries shortly.},
  archive      = {J_JPDC},
  author       = {Indranil Roy and Ankit Srivastava and Matt Grimm and Srinivas Aluru},
  doi          = {10.1016/j.jpdc.2018.01.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {234-245},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Interval stabbing on the automata processor},
  volume       = {135},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). En-ABC: An ensemble artificial bee colony based anomaly
detection scheme for cloud environment. <em>JPDC</em>, <em>135</em>,
219–233. (<a href="https://doi.org/10.1016/j.jpdc.2019.09.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With an exponential increase in the usage of different types of services and applications in cloud computing environment , the identification of malicious behavior of different nodes becomes challenging due to the diversity of traffic patterns generated from various services and applications. Most of the existing solutions reported in the literature are restricted with respect to the usage of a specific technique applicable to single class datasets. But in real life scenarios, applications and services especially in cloud environment may have multi-class datasets. Moreover, non-linear behavior among the dataset attributes generates additional challenges for identification of nodes behavior, and it has not been exploited to its full potential in the existing solutions. This can lead to performance bottlenecks with respect to the identification of malicious behavior of different nodes. Motivated from these facts, this paper proposes an Ensemble Artificial Bee Colony based Anomaly Detection Scheme (En-ABC) for multi-class datasets in cloud environment. En-ABC has following components for identification of malicious behavior of nodes-(i) feature selection and optimization, (ii) data clustering , and (iii) identification of anomalous behavior of nodes. The feature selection and optimization model in En-ABC has been built using Restricted Boltzmann Machine and Unscented Kalman Filter (to handle the non-linear behavior of dataset attributes) respectively. Moreover, Artificial Bee Colony-based Fuzzy C-means clustering technique is used to obtain an optimal clustering based on two objective functions, i.e. , Mean Square Deviation and Dunn Index (to handle the participation of attributes in multiple clustered datasets). Then, a profile of normal/abnormal behavior has been built using clustering results for detection of the anomalies. Finally, the performance of the proposed scheme has been compared with the existing schemes (CM, SVM, ML-IDS and MSADA) using various parameters such as-detection, false alarm, and accuracy rates. Experimental results on benchmark (NSL-KDD, NAB and IBRL) and synthetic datasets validate the effectiveness of the proposed scheme.},
  archive      = {J_JPDC},
  author       = {Sahil Garg and Kuljeet Kaur and Shalini Batra and Gagangeet Singh Aujla and Graham Morgan and Neeraj Kumar and Albert Y. Zomaya and Rajiv Ranjan},
  doi          = {10.1016/j.jpdc.2019.09.013},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {219-233},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {En-ABC: An ensemble artificial bee colony based anomaly detection scheme for cloud environment},
  volume       = {135},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cost-effective deployment of certified cloud composite
services. <em>JPDC</em>, <em>135</em>, 203–218. (<a
href="https://doi.org/10.1016/j.jpdc.2019.09.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The advent of cloud computing has radically changed the concept of distributed environments, where services can now be composed and reused at high rates. Today, service composition in the cloud is driven by the need of providing stable QoS, where non-functional properties of composite services are proven over time and composite services continuously adapt to both functional and non-functional changes of the component services. This scenario introduces substantial costs on the cloud providers that go beyond the cost of deploying component services, and require to consider the costs of continuously verifying non-functional properties of composite and component services. In this paper, we propose a cost-effective approach to certification-based cloud service composition. This approach is based, on one side, on a portable certification process for the cloud evaluating non-functional properties of composite services and, on the other side, on a cost-evaluation methodology aimed to produce the service composition that minimizes the total cost paid by the cloud providers , taking into account both deployment and certification/verification costs. Our service composition approach is driven by certificates awarded to single services and by a fuzzy-based cost evaluation methodology, and assumes certified properties as must-have requirements for service selection and composition.},
  archive      = {J_JPDC},
  author       = {Marco Anisetti and Claudio A. Ardagna and Ernesto Damiani and Filippo Gaudenzi and Gwanggil Jeon},
  doi          = {10.1016/j.jpdc.2019.09.003},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {203-218},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Cost-effective deployment of certified cloud composite services},
  volume       = {135},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Managing renewable energy and carbon footprint in
multi-cloud computing environments. <em>JPDC</em>, <em>135</em>,
191–202. (<a href="https://doi.org/10.1016/j.jpdc.2019.09.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud computing offers attractive features for both service providers and customers. Users benefit from the pay-as-you-go model by saving expenditures and service providers are deploying their services to cloud data centers to reduce their maintenance efforts. However, due to the fast growth of cloud data centers , the energy consumed by the data centers can lead to a huge amount of carbon emission with environmental impacts, and the carbon intensity of different locations are varied among different power plants according to the sources of energy. Thus, in this paper, to address the carbon emission problem of data centers, we consider shifting the workloads among multi-cloud located in different time zones. We also formulate the energy usage and carbon emission of data centers and model the solar power corresponding to the locations. This helps to reduce the usage of brown energy and maximize the utilization of renewable energy at different locations. We propose an approach for managing carbon footprint and renewable energy for multiple data centers at California, Virginia, and Dublin, which are in different time zones. The results show that our proposed approaches that apply workload shifting can reduce around 40\% carbon emission in comparison to the baseline while ensuring the average response time of user requests.},
  archive      = {J_JPDC},
  author       = {Minxian Xu and Rajkumar Buyya},
  doi          = {10.1016/j.jpdc.2019.09.015},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {191-202},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Managing renewable energy and carbon footprint in multi-cloud computing environments},
  volume       = {135},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Profit-aware application placement for integrated fog–cloud
computing environments. <em>JPDC</em>, <em>135</em>, 177–190. (<a
href="https://doi.org/10.1016/j.jpdc.2019.10.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The marketplace for Internet of Things (IoT)-enabled smart systems is rapidly expanding. The integration of Fog and Cloud paradigm aims at harnessing both edge device and remote datacentre-based computing resources to meet Quality of Service (QoS) requirements of these smart systems. Due to lack of instance pricing and revenue maximizing techniques, it becomes difficult for service providers to make comprehensive profit from such integration. This problem further intensifies when associated expenses and allowances are charged from the revenue. Conversely, the rigid revenue maximizing intention of providers affects user’s budget and system’s service quality. To address these issues, we propose a profit-aware application placement policy for integrated Fog–Cloud environments. It is formulated using constraint Integer Linear Programming model that simultaneously enhances profit and ensures QoS during application placement on computing instances. Furthermore, it provides compensation to users for any violation of Service Level Agreement (SLA) and sets the price of instances according to their ability of reducing service delivery time. The performance of proposed policy is evaluated in a simulated Fog–Cloud environment using iFogSim and the results demonstrate that it outperforms other placement policies in concurrently increasing provider’s profit and user’s QoS satisfaction rate.},
  archive      = {J_JPDC},
  author       = {Redowan Mahmud and Satish Narayana Srirama and Kotagiri Ramamohanarao and Rajkumar Buyya},
  doi          = {10.1016/j.jpdc.2019.10.001},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {177-190},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Profit-aware application placement for integrated Fog–Cloud computing environments},
  volume       = {135},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A secure and efficient outsourced computation on data
sharing scheme for privacy computing. <em>JPDC</em>, <em>135</em>,
169–176. (<a href="https://doi.org/10.1016/j.jpdc.2019.09.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of computer technology, it makes malicious users more easily get data stored in cloud. However, these data are always related to users’ privacy, and it is harmful when the data are acquired by attackers. Ciphertext-policy attribute-based encryption (CP-ABE) is suitable to achieve privacy and security in cloud. In this paper, we put forward a secure and efficient outsourced computation algorithm on data sharing scheme for privacy computing. Existing schemes only outsource decryption computation to the cloud, users still have heavy burden in encryption. In order to reduce the computing burden of users, most encryption and decryption computations are outsourced to the cloud service provider in our construction. At the same time, to apply in practice, we propose efficient user and attribute revocation . Finally, the security analysis and simulation results show that our scheme is secure and efficient compared with existing schemes.},
  archive      = {J_JPDC},
  author       = {Kai Fan and Tingting Liu and Kuan Zhang and Hui Li and Yintang Yang},
  doi          = {10.1016/j.jpdc.2019.09.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {169-176},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A secure and efficient outsourced computation on data sharing scheme for privacy computing},
  volume       = {135},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Privbus: A privacy-enhanced crowdsourced bus service via fog
computing. <em>JPDC</em>, <em>135</em>, 156–168. (<a
href="https://doi.org/10.1016/j.jpdc.2019.09.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowdsourced bus service provides the customized bus for a group of users with similar itineraries by designing the route based on the users’ trip plans. With crowdsourced bus service, the users with similar trips can enjoy the customized bus route efficiently and inexpensively. However, serious privacy concerns (e.g., the exposure of users’ current and future locations) have become a major barrier. To protect users’ itineraries, we propose Privbus, a privacy-enhanced crowdsourced bus service without hampering the functionality of bus route planning. Specifically, Privbus improves the performance of clustering itineraries due to the assistance of fogs. Then, Privbus executes the fog-assisted density peaks clustering operations on ciphertexts of users’ travel plans to protect the users’ trips. By doing so, the clustering operation is removed from users’ smart devices to fog nodes, so as to enable the users to be offline after they submit their travel plans. According to the clustering results , Privbus uses a route planning method to optimize the bus routes. The optimization reduces the time cost on travel of users, while guaranteeing the good profit and the wide coverage of crowdsourced bus service. Finally, through the performance evaluation and extensive experiments, we demonstrate that Privbus has the advantage of low computational and communication overhead , while providing high security and precision guarantees.},
  archive      = {J_JPDC},
  author       = {Yuanyuan He and Jianbing Ni and Ben Niu and Fenghua Li and Xuemin (Sherman) Shen},
  doi          = {10.1016/j.jpdc.2019.09.007},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {156-168},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Privbus: A privacy-enhanced crowdsourced bus service via fog computing},
  volume       = {135},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive data and verified message disjoint security routing
for gathering big data in energy harvesting networks. <em>JPDC</em>,
<em>135</em>, 140–155. (<a
href="https://doi.org/10.1016/j.jpdc.2019.08.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve the data arrival ratio and the transmission delay and considering that the capacity for determining malicious nodes and energy are limited, a security disjoint routing-based verified message (SDRVM) scheme is proposed. The main contributions of SDRVM are as follows: (a) two connected dominating sets (a data CDS and a v-message CDS) are created for disseminating data and verified messages (v-messages), respectively, based on the remaining energy of nodes. (b) Nodes record the ID information in data packets with a specified probability, namely, the marking probability, which is adjusted according to the remaining energy of the nodes. (c) The duty cycle of the nodes is adjusted, and the energy of the nodes is divided into three levels. In the data CDS, the duty cycle of the sensor nodes is the longest and the duty cycle of the nodes that do not belong to either of the CDSs is the shortest. (d) If the energy of the sensor nodes is sufficient, data packets are transmitted several times and the v-messages that are stored in the nodes are transmitted to the destination nodes . The proposed scheme has been evaluated using different parameters where the results obtained prove its effectiveness in comparison to the existing solutions.},
  archive      = {J_JPDC},
  author       = {Xiao Liu and Anfeng Liu and Tian Wang and Kaoru Ota and Mianxiong Dong and Yuxin Liu and Zhiping Cai},
  doi          = {10.1016/j.jpdc.2019.08.012},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {140-155},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Adaptive data and verified message disjoint security routing for gathering big data in energy harvesting networks},
  volume       = {135},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Privacy-preserving range query over multi-source electronic
health records in public clouds. <em>JPDC</em>, <em>135</em>, 127–139.
(<a href="https://doi.org/10.1016/j.jpdc.2019.08.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Range query is an important data search technique in cloud-based electronic healthcare (eHealth) systems. It enables authorized doctors to retrieve target electronic health records (EHRs) that are generated and outsourced by patients from the cloud server. In reality, patients always encrypt their EHRs before outsourcing, making the range query impossible. In this paper, we identify three threats in real cloud-based eHealth systems, i.e., privacy leakage , frequency analysis, and identical data inference. To capture the security properties that resist these threats, we define a security notion of indistinguishability under multi-source ordered chosen plaintext attack (IND-MSOCPA). Then, we propose a multi-source order-preserving encryption (MSOPE) scheme for cloud-based eHealth systems to enable range queries over encrypted EHRs from multiple patients. Security analysis proves that the MSOPE scheme is IND-MSOCPA secure. We also conduct comprehensive performance evaluations, which demonstrate the high efficiency of the MSOPE scheme.},
  archive      = {J_JPDC},
  author       = {Jinwen Liang and Zheng Qin and Sheng Xiao and Jixin Zhang and Hui Yin and Keqin Li},
  doi          = {10.1016/j.jpdc.2019.08.011},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {127-139},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Privacy-preserving range query over multi-source electronic health records in public clouds},
  volume       = {135},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rotorcraft virtual sensors via deep regression.
<em>JPDC</em>, <em>135</em>, 114–126. (<a
href="https://doi.org/10.1016/j.jpdc.2019.08.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Raw sensor data containing high-fidelity information is highly desirable for valuable post-processing. We developed a machine learning model that performs deep regression to infer rotorcraft component vibration spectra from a few flight conditional indicators (CI). The model consists of a deep neural network of fully connected layers (DNN) that performs high-dimensional and non-linear multivariate regression to reconstruct raw accelerometer data. The network architecture hyperparameters were optimized using an evolutionary genetic algorithm (GA) that was more effective than random and manual search methods. The best GA design was further tuned to achieve spectrum reconstruction accuracies above 95\% on validation datasets . An automated model generator workflow was developed to train and evaluate thousands of DNN designs using parallel asynchronous execution on a Cray XC50, which were monitored and studied. Finally, as a verification step of the DNN inference model operation and performance, a detailed sensitivity analysis was performed using a modified Sobol sampling technique to understand response behavior and limitations. The sensitivity analysis method utilized Dask-distributed across multiple nodes on our HPC to evaluate millions of generated samples in parallel.},
  archive      = {J_JPDC},
  author       = {Daniel Martínez and Wesley Brewer and Andrew Strelzoff and Andrew Wilson and Daniel Wade},
  doi          = {10.1016/j.jpdc.2019.08.008},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {114-126},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Rotorcraft virtual sensors via deep regression},
  volume       = {135},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal placement for repair-efficient erasure codes in
geo-diverse storage centres. <em>JPDC</em>, <em>135</em>, 101–113. (<a
href="https://doi.org/10.1016/j.jpdc.2019.08.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Erasure codes are increasingly being used by storage providers to reduce the cost of reliably storing large volumes of data. As compared to the default mechanism of triple replication, erasure codes result in optimal storage efficiency, but require significant network and disk usage during repair of failed data. The repair process is particularly complicated for storage clusters with data centres spread across a wide geographical area. For such geo-diverse clusters, the recovery performance of the code used depends more on the network throughput and latency than on the computations required for decoding. Hence, the recovery performance of most erasure codes can be improved if the surviving blocks for effecting a node repair are placed optimally. This article affirms the idea by proposing an optimization framework for placement of blocks in geo-distributed storage clusters, addressing the open problem posed by Dimakis et al. in their celebrated paper on network coding. To this end, a signomial program is formulated that yields the optimal placement minimizing the average single-block repair cost over large number of files. Though non-convex, the structure of the problem allows us to use a monomial approximation to solve the problem efficiently. MATLAB simulation results and equivalent translation to implementation with popular codes used in Hadoop storage setting are presented, that validate our framework. The idea could be applied to any coded geo-diverse storage system to achieve significant benefits in repair performance during node failures.},
  archive      = {J_JPDC},
  author       = {Lakshmi J. Mohan and Ketan Rajawat and Udaya Parampalli and Aaron Harwood},
  doi          = {10.1016/j.jpdc.2019.08.010},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {101-113},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Optimal placement for repair-efficient erasure codes in geo-diverse storage centres},
  volume       = {135},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SIMD programming using intel vector extensions.
<em>JPDC</em>, <em>135</em>, 83–100. (<a
href="https://doi.org/10.1016/j.jpdc.2019.09.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single instruction multiple data (SIMD) extensions are one of the most significant capabilities of recent General Purpose Processors (GPPs) which improves the performance of applications with less hardware modification. Each GPP vendor such as HP, Sun, Intel, and AMD has its particular Instruction Set Architecture (ISA) and SIMD micro-architecture with different perspectives. Intel expanded SIMD technologies from hardware and software point of view. It has introduced SIMD technologies such as MultiMedia eXtensions (MMX), Streaming SIMD Extensions (SSE), Advanced Vector eXtensions (AVX), Fused Multiply Add (FMA) and AVX-512 sets. During micro-processors developments path, register width has been extended from 64 bits to 512 bits and number of vector registers has been increased from 8 to 32. Wider registers provide more parallelism ways and more registers reduce extra data movement to the cache memory. In order to gain the advantages of SIMD extensions, many programming approaches have been developed. Compiler Automatic Vectorization (CAV) as an implicit vectorization approach, provides simple and easy SIMDization tools. While, performance improvement of CAV is not always granted, most compilers auto-vectorize simple loops. On the other hand, for explicit vectorization , Intrinsic Programming Model (IPM) provides low-level access to vector registers for SIMDizing. However, programming with IPM requires great amount of expertise especially in low-level architecture feature, thus, choosing the suitable instructions and vectorization methodology for mapping to a certain algorithm is important. Moreover, portability, compatibility, scalability and compiler optimization might limit the advantage of IPM. Our goal in this paper is as follows. First, we provide a review of SIMD technology in general and Intel’s SIMD extensions in particular. Second, some SIMD features of Intel SIMD technologies, MMX, SSEs, AVX, and FMA in terms of ISA, vector width, and SIMD programming tools are comparatively discussed. Third, in order to compare the performance of different auto-vectorizers and IPM approaches using Intel C++ compiler (ICC), GNU Compiler Collection (GCC) and Low Level Virtual Machine (LLVM), we map and implement some representative multimedia kernels on AVX and AVX2 extensions. Finally, our experimental results show that although the performance improvement using IPM approach is higher than CAVs, programmer needs more programming efforts and knows different mapping strategists. Therefore, extending auto-vectorizers abilities to generate more efficient vectorized codes is an important issue in different compilers.},
  archive      = {J_JPDC},
  author       = {Hossein Amiri and Asadollah Shahbahrami},
  doi          = {10.1016/j.jpdc.2019.09.012},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {83-100},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {SIMD programming using intel vector extensions},
  volume       = {135},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Privacy preserving classification on local differential
privacy in data centers. <em>JPDC</em>, <em>135</em>, 70–82. (<a
href="https://doi.org/10.1016/j.jpdc.2019.09.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rise of cloud service providers and the continuous virtualization of data centers , data center networks are also developing rapidly. As data centers become more and more complex, the demand for security increases dramatically. This paper discusses the privacy inherent in data centers. However, there is no general solution to the privacy problem in data centers due to the device heterogeneity. In this paper, we proposed a local differential privacy-based classification algorithm for data centers. In data mining of data centers, the differential privacy protection mechanism is added to deal with Laplace noise of sensitive information in the pattern mining process. We designed a method to quantify the quality of privacy protection through strict mathematical proof. Experiments demonstrated that the differential privacy-based classification algorithm proposed in this paper has higher iteration efficiency, better security and feasible accuracy. On the premise of ensuring availability, the algorithm has reliable privacy protection characteristics and excellent timeliness.},
  archive      = {J_JPDC},
  author       = {Weibei Fan and Jing He and Mengjiao Guo and Peng Li and Zhijie Han and Ruchuan Wang},
  doi          = {10.1016/j.jpdc.2019.09.009},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {70-82},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Privacy preserving classification on local differential privacy in data centers},
  volume       = {135},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A fine-grained authorized keyword secure search scheme with
efficient search permission update in cloud computing. <em>JPDC</em>,
<em>135</em>, 56–69. (<a
href="https://doi.org/10.1016/j.jpdc.2019.09.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of cloud computing , secure search has become a hot research spot, which is a promising technique that allows a data user to perform privacy-preserving keyword-based search over encrypted cloud data. In this paper, we further consider the secure search problem based on a practical application scenario that a data owner needs to grant different keyword query permissions for different data users to achieve flexible access control on outsourced encrypted data in the cloud computing environment. To address this problem, we propose a fine-grained authorized keyword secure search scheme by leveraging the ciphertext policy attribute-based encryption (ABE), which not only supports privacy-preserving keyword-based search over encrypted data , but also inherits flexible and fine-grained data privilege control properties of ABE. Moreover, our proposed scheme is able to achieve fine-grained search permission update with very small communication and computation cost. By running the attribute revocation sub-protocol and attribute addition sub-protocol, the data owner can flexibly and efficiently update a data user’s keyword search permissions when the data user’s system role changes. We provide detailed performance analysis and rigorous security proof for our scheme. Extensive experiments demonstrate the correctness and practicality of the proposed scheme.},
  archive      = {J_JPDC},
  author       = {Hui Yin and Zheng Qin and Jixin Zhang and Hua Deng and Fangmin Li and Keqin Li},
  doi          = {10.1016/j.jpdc.2019.09.011},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {56-69},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {A fine-grained authorized keyword secure search scheme with efficient search permission update in cloud computing},
  volume       = {135},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Preparing opportunistic networks for smart cities:
Collecting sensed data with minimal knowledge. <em>JPDC</em>,
<em>135</em>, 21–55. (<a
href="https://doi.org/10.1016/j.jpdc.2019.09.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Opportunistic Networks exploit portable handheld devices to collect delay-tolerant data from sensors to gateways for realizing various Smart City applications . To obtain knowledge for determining suitable routing paths as users go about their daily routine, nodes maintain history of every encounter and exchange the information through summary vectors. Due to large node populations, the size of summary vectors makes it challenging to implement real-world city-scale applications with the technology. In this paper, we take the technology a step towards real-world implementation by proposing a set of adaptive and privacy-preserving mechanisms that can be incorporated into existing encounter-based routing protocols to reduce summary vector sizes without compromising delivery guarantees. We validate our proposals with real-world human movement traces and simulation experiments. In terms of network performance, our proposals reduce the average summary vector size by 75\% to achieve up to 21\% less energy consumption with about 28\% improvement in throughput.},
  archive      = {J_JPDC},
  author       = {Tekenate E. Amah and Maznah Kamat and Kamalrulnizam Abu Bakar and Waldir Moreira and Antonio Oliveira Jr. and Marcos A. Batista},
  doi          = {10.1016/j.jpdc.2019.09.005},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {21-55},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Preparing opportunistic networks for smart cities: Collecting sensed data with minimal knowledge},
  volume       = {135},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accountable privacy preserving attribute based framework for
authenticated encrypted access in clouds. <em>JPDC</em>, <em>135</em>,
1–20. (<a href="https://doi.org/10.1016/j.jpdc.2019.08.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an accountable privacy preserving attribute-based framework, called Ins-PAbAC , that combines attribute based encryption and attribute based signature techniques for securely sharing outsourced data contents via public cloud servers. The proposed framework presents several advantages. First, it provides an encrypted access control feature, enforced at the data owner’s side, while providing the desired expressiveness of access control policies . Second, Ins-PAbAC preserves users’ privacy, relying on an anonymous authentication mechanism, derived from a privacy preserving attribute based signature scheme that hides the users’ identifying information. Furthermore, our proposal introduces an accountable attribute based signature that enables an inspection authority to reveal the identity of the anonymously-authenticated user if needed. Third, Ins-PAbAC is provably secure , as it is resistant to both curious cloud providers and malicious users adversaries. Finally, experimental results, built upon OpenStack Swift testbed , point out the applicability of the proposed scheme in real world scenarios.},
  archive      = {J_JPDC},
  author       = {Sana Belguith and Nesrine Kaaniche and Maryline Laurent and Abderrazak Jemai and Rabah Attia},
  doi          = {10.1016/j.jpdc.2019.08.014},
  journal      = {Journal of Parallel and Distributed Computing},
  pages        = {1-20},
  shortjournal = {J. Parallel Distrib. Comput.},
  title        = {Accountable privacy preserving attribute based framework for authenticated encrypted access in clouds},
  volume       = {135},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
