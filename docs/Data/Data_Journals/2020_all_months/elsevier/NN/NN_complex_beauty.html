<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>NN_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="nn---400">NN - 400</h2>
<ul>
<li><details>
<summary>
(2020). Announcement of the neural networks best paper award.
<em>NN</em>, <em>132</em>, iii. (<a
href="https://doi.org/10.1016/S0893-6080(20)30384-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  author       = {Kenji Doya ( Co-Editors-in-Chief, Neural Networks ) and DeLiang Wang},
  doi          = {10.1016/S0893-6080(20)30384-1},
  journal      = {Neural Networks},
  pages        = {iii},
  shortjournal = {Neural Netw.},
  title        = {Announcement of the neural networks best paper award},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>132</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(20)30370-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(20)30370-1},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Current events. <em>NN</em>, <em>132</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(20)30369-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(20)30369-5},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {Current events},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). An improved lyapunov functional with application to
stability of cohen–grossberg neural networks of neutral-type with
multiple delays. <em>NN</em>, <em>132</em>, 532–539. (<a
href="https://doi.org/10.1016/j.neunet.2020.09.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The essential objective of this research article is to investigate stability issue of neutral-type Cohen–Grossberg neural networks involving multiple time delays in states of neurons and multiple neutral delays in time derivatives of states of neurons in the network. By exploiting a modified and improved version of a previously introduced Lyapunov functional , a new sufficient stability criterion is obtained for global asymptotic stability of Cohen–Grossberg neural networks of neutral-type possessing multiple delays. The proposed new stability condition does not involve the time and neutral delay parameters. The obtained stability criterion is totally dependent on the system elements of Cohen–Grossberg neural network model. Moreover, the validity of this novel global asymptotic stability condition may be tested by only checking simple appropriate algebraic equations established within the parameters of the considered neutral-type neural network. In addition, an instructive numerical example is presented to indicate the advantages of our proposed stability result over the existing literature results obtained for stability of various classes of neutral-type neural networks having multiple delays.},
  archive      = {J_NN},
  author       = {Ozlem Faydasicok},
  doi          = {10.1016/j.neunet.2020.09.023},
  journal      = {Neural Networks},
  pages        = {532-539},
  shortjournal = {Neural Netw.},
  title        = {An improved lyapunov functional with application to stability of Cohen–Grossberg neural networks of neutral-type with multiple delays},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Human interaction behavior modeling using generative
adversarial networks. <em>NN</em>, <em>132</em>, 521–531. (<a
href="https://doi.org/10.1016/j.neunet.2020.09.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, considerable research has focused on personal assistant robots, and robots capable of rich human-like communication are expected. Among humans, non-verbal elements contribute to effective and dynamic communication. However, people use a wide range of diverse gestures, and a robot capable of expressing various human gestures has not been realized. In this study, we address human behavior modeling during interaction using a deep generative model . In the proposed method, to consider interaction motion, three factors, i.e., interaction intensity, time evolution, and time resolution, are embedded in the network structure. Subjective evaluation results suggest that the proposed method can generate high-quality human motions .},
  archive      = {J_NN},
  author       = {Yusuke Nishimura and Yutaka Nakamura and Hiroshi Ishiguro},
  doi          = {10.1016/j.neunet.2020.09.019},
  journal      = {Neural Networks},
  pages        = {521-531},
  shortjournal = {Neural Netw.},
  title        = {Human interaction behavior modeling using generative adversarial networks},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lower dimensional kernels for video discriminators.
<em>NN</em>, <em>132</em>, 506–520. (<a
href="https://doi.org/10.1016/j.neunet.2020.09.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work presents an analysis of the discriminators used in Generative Adversarial Networks (GANs) for Video. We show that unconstrained video discriminator architectures induce a loss surface with high curvature which make optimization difficult. We also show that this curvature becomes more extreme as the maximal kernel dimension of video discriminators increases. With these observations in hand, we propose a methodology for the design of a family of efficient Lower-Dimensional Video Discriminators for GANs (LDVD-GANs). The proposed methodology improves the performance and efficiency of video GAN models it is applied to and demonstrates good performance on complex and diverse datasets such as UCF-101. In particular, we show that LDVDs can double the performance of Temporal-GANs and provide for state-of-the-art performance on a single GPU using the proposed methodology.},
  archive      = {J_NN},
  author       = {Emmanuel Kahembwe and Subramanian Ramamoorthy},
  doi          = {10.1016/j.neunet.2020.09.016},
  journal      = {Neural Networks},
  pages        = {506-520},
  shortjournal = {Neural Netw.},
  title        = {Lower dimensional kernels for video discriminators},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-grouping convolutional neural networks. <em>NN</em>,
<em>132</em>, 491–505. (<a
href="https://doi.org/10.1016/j.neunet.2020.09.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although group convolution operators are increasingly used in deep convolutional neural networks to improve the computational efficiency and to reduce the number of parameters, most existing methods construct their group convolution architectures by a predefined partitioning of the filters of each convolutional layer into multiple regular filter groups with an equal spatial group size and data-independence, which prevents a full exploitation of their potential. To tackle this issue, we propose a novel method of designing self-grouping convolutional neural networks , called SG-CNN, in which the filters of each convolutional layer group themselves based on the similarity of their importance vectors. Concretely, for each filter, we first evaluate the importance value of their input channels to identify the importance vectors, and then group these vectors by clustering. Using the resulting data-dependent centroids , we prune the less important connections, which implicitly minimizes the accuracy loss of the pruning, thus yielding a set of diverse group convolution filters. Subsequently, we develop two fine-tuning schemes, i.e. (1) both local and global fine-tuning and (2) global only fine-tuning, which experimentally deliver comparable results, to recover the recognition capacity of the pruned network. Comprehensive experiments carried out on the CIFAR-10/100 and ImageNet datasets demonstrate that our self-grouping convolution method adapts to various state-of-the-art CNN architectures, such as ResNet and DenseNet, and delivers superior performance in terms of compression ratio, speedup and recognition accuracy. We demonstrate the ability of SG-CNN to generalize by transfer learning , including domain adaption and object detection, showing competitive results. Our source code is available at https://github.com/QingbeiGuo/SG-CNN.git .},
  archive      = {J_NN},
  author       = {Qingbei Guo and Xiao-Jun Wu and Josef Kittler and Zhiquan Feng},
  doi          = {10.1016/j.neunet.2020.09.015},
  journal      = {Neural Networks},
  pages        = {491-505},
  shortjournal = {Neural Netw.},
  title        = {Self-grouping convolutional neural networks},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). AMD-GAN: Attention encoder and multi-branch structure based
generative adversarial networks for fundus disease detection from
scanning laser ophthalmoscopy images. <em>NN</em>, <em>132</em>,
477–490. (<a
href="https://doi.org/10.1016/j.neunet.2020.09.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scanning laser ophthalmoscopy (SLO) has become an important tool for the determination of peripheral retinal pathology, in recent years. However, the collected SLO images are easily interfered by the eyelash and frame of the devices, which heavily affect the key feature extraction of the images. To address this, we propose a generative adversarial network called AMD-GAN based on the attention encoder (AE) and multi-branch (MB) structure for fundus disease detection from SLO images. Specifically, the designed generator consists of two parts: the AE and generation flow network , where the real SLO images are encoded by the AE module to extract features and the generation flow network to handle the random Gaussian noise by a series of residual block with up-sampling (RU) operations to generate fake images with the same size as the real ones, where the AE is also used to mine features for generator. For discriminator , a ResNet network using MB is devised by copying the stage 3 and stage 4 structures of the ResNet-34 model to extract deep features. Furthermore, the depth-wise asymmetric dilated convolution is leveraged to extract local high-level contextual features and accelerate the training process. Besides, the last layer of discriminator is modified to build the classifier to detect the diseased and normal SLO images. In addition, the prior knowledge of experts is utilized to improve the detection results. Experimental results on the two local SLO datasets demonstrate that our proposed method is promising in detecting the diseased and normal SLO images with the experts labeling.},
  archive      = {J_NN},
  author       = {Hai Xie and Haijun Lei and Xianlu Zeng and Yejun He and Guozhen Chen and Ahmed Elazab and Guanghui Yue and Jiantao Wang and Guoming Zhang and Baiying Lei},
  doi          = {10.1016/j.neunet.2020.09.005},
  journal      = {Neural Networks},
  pages        = {477-490},
  shortjournal = {Neural Netw.},
  title        = {AMD-GAN: Attention encoder and multi-branch structure based generative adversarial networks for fundus disease detection from scanning laser ophthalmoscopy images},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Latent dirichlet allocation based generative adversarial
networks. <em>NN</em>, <em>132</em>, 461–476. (<a
href="https://doi.org/10.1016/j.neunet.2020.08.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative adversarial networks have been extensively studied in recent years and powered a wide range of applications, ranging from image generation , image-to-image translation, to text-to-image generation, and visual recognition. These methods typically model the mapping from latent space to image with single or multiple generators. However, they have obvious drawbacks: (i) ignoring the multi-modal structure of images, and (ii) lacking model interpretability . Importantly, the existing methods mostly assume one or more generators can cover all image modes even if we do not know the structure of data. Thus, mode dropping and collapse often take place along with GANs training. Despite the importance of exploring the data structure in generation, it has been almost unexplored. In this work, aiming at generating multi-modal images and interpreting model explicitly, we explore the theory on how to integrate GANs with data structure prior, and propose latent Dirichlet allocation based generative adversarial networks (LDAGAN). This framework is extended to combine with a variety of state-of-the-art single-generator GANs and achieves improved performance. Extensive experiments on synthetic and real datasets demonstrate the efficacy of LDAGAN for multi-modal image generation . An implementation of LDAGAN is available at https://github.com/Sumching/LDAGAN.},
  archive      = {J_NN},
  author       = {Lili Pan and Shen Cheng and Jian Liu and Peijun Tang and Bowen Wang and Yazhou Ren and Zenglin Xu},
  doi          = {10.1016/j.neunet.2020.08.012},
  journal      = {Neural Networks},
  pages        = {461-476},
  shortjournal = {Neural Netw.},
  title        = {Latent dirichlet allocation based generative adversarial networks},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Event-triggered impulsive synchronization of discrete-time
coupled neural networks with stochastic perturbations and multiple
delays. <em>NN</em>, <em>132</em>, 447–460. (<a
href="https://doi.org/10.1016/j.neunet.2020.09.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the synchronization for discrete-time coupled neural networks (DTCNNs), in which stochastic perturbations and multiple delays are simultaneously involved. The multiple delays mean that both discrete time-varying delays and distributed delays are included. Time-triggered impulsive control (TTIC) is proposed to investigate the synchronization issue of the DTCNNs based on the recently proposed impulsive control scheme for continuous neural networks with single time delays . Furthermore, a novel event-triggered impulsive control (ETIC) is designed to further reduce the communication bandwidth . By using linear matrix inequality (LMI) technique and constructing appropriate Lyapunov functions , some sufficient criteria guaranteeing the synchronization of the DTCNNs are obtained. Finally, We propose a simulation example to illustrate the validity and feasibility of the theoretical results obtained.},
  archive      = {J_NN},
  author       = {Huiyuan Li and Jian-an Fang and Xiaofan Li and Leszek Rutkowski and Tingwen Huang},
  doi          = {10.1016/j.neunet.2020.09.012},
  journal      = {Neural Networks},
  pages        = {447-460},
  shortjournal = {Neural Netw.},
  title        = {Event-triggered impulsive synchronization of discrete-time coupled neural networks with stochastic perturbations and multiple delays},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-dimensional dynamics of generalization error in neural
networks. <em>NN</em>, <em>132</em>, 428–446. (<a
href="https://doi.org/10.1016/j.neunet.2020.08.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We perform an analysis of the average generalization dynamics of large neural networks trained using gradient descent . We study the practically-relevant “high-dimensional” regime where the number of free parameters in the network is on the order of or even larger than the number of examples in the dataset. Using random matrix theory and exact solutions in linear models, we derive the generalization error and training error dynamics of learning and analyze how they depend on the dimensionality of data and signal to noise ratio of the learning problem. We find that the dynamics of gradient descent learning naturally protect against overtraining and overfitting in large networks. Overtraining is worst at intermediate network sizes, when the effective number of free parameters equals the number of samples, and thus can be reduced by making a network smaller or larger . Additionally, in the high-dimensional regime, low generalization error requires starting with small initial weights. We then turn to non-linear neural networks, and show that making networks very large does not harm their generalization performance . On the contrary, it can in fact reduce overtraining, even without early stopping or regularization of any sort. We identify two novel phenomena underlying this behavior in overcomplete models: first, there is a frozen subspace of the weights in which no learning occurs under gradient descent; and second, the statistical properties of the high-dimensional regime yield better-conditioned input correlations which protect against overtraining. We demonstrate that standard application of theories such as Rademacher complexity are inaccurate in predicting the generalization performance of deep neural networks, and derive an alternative bound which incorporates the frozen subspace and conditioning effects and qualitatively matches the behavior observed in simulation.},
  archive      = {J_NN},
  author       = {Madhu S. Advani and Andrew M. Saxe and Haim Sompolinsky},
  doi          = {10.1016/j.neunet.2020.08.022},
  journal      = {Neural Networks},
  pages        = {428-446},
  shortjournal = {Neural Netw.},
  title        = {High-dimensional dynamics of generalization error in neural networks},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the robustness of skeleton detection against adversarial
attacks. <em>NN</em>, <em>132</em>, 416–427. (<a
href="https://doi.org/10.1016/j.neunet.2020.09.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human perception of an object’s skeletal structure is particularly robust to diverse perturbations of shape. This skeleton representation possesses substantial advantages for parts-based and invariant shape encoding, which is essential for object recognition. Multiple deep learning-based skeleton detection models have been proposed, while their robustness to adversarial attacks remains unclear. (1) This paper is the first work to study the robustness of deep learning-based skeleton detection against adversarial attacks , which are only slightly unlike the original data but still imperceptible to humans. We systematically analyze the robustness of skeleton detection models through exhaustive adversarial attacking experiments. (2) We propose a novel Frequency attack, which can directly exploit the regular and interpretable perturbations to sharply disrupt skeleton detection models. Frequency attack consists of an excitatory-inhibition waveform with high frequency attribution, which confuses edge-sensitive convolutional filters due to the sudden contrast between crests and troughs. Our comprehensive results verify that skeleton detection models are also vulnerable to adversarial attacks. The meaningful findings will inspire researchers to explore more potential robust models by involving explicit skeleton features.},
  archive      = {J_NN},
  author       = {Xiuxiu Bai and Ming Yang and Zhe Liu},
  doi          = {10.1016/j.neunet.2020.09.018},
  journal      = {Neural Networks},
  pages        = {416-427},
  shortjournal = {Neural Netw.},
  title        = {On the robustness of skeleton detection against adversarial attacks},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neurodynamical classifiers with low model complexity.
<em>NN</em>, <em>132</em>, 405–415. (<a
href="https://doi.org/10.1016/j.neunet.2020.08.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recently proposed Minimal Complexity Machine (MCM) finds a hyperplane classifier by minimizing an upper bound on the Vapnik–Chervonenkis (VC) dimension. The VC dimension measures the capacity or model complexity of a learning machine . Vapnik’s risk formula indicates that models with smaller VC dimension are expected to show improved generalization. On many benchmark datasets, the MCM generalizes better than SVMs and uses far fewer support vectors than the number used by SVMs . In this paper, we describe a neural network that converges to the MCM solution. We employ the MCM neurodynamical system as the final layer of a neural network architecture. Our approach also optimizes the weights of all layers in order to minimize the objective, which is a combination of a bound on the VC dimension and the classification error . We illustrate the use of this model for robust binary and multi-class classification. Numerical experiments on benchmark datasets from the UCI repository show that the proposed approach is scalable and accurate, and learns models with improved accuracies and fewer support vectors.},
  archive      = {J_NN},
  author       = {Himanshu Pant and Sumit Soman and Jayadeva and Amit Bhaya},
  doi          = {10.1016/j.neunet.2020.08.013},
  journal      = {Neural Networks},
  pages        = {405-415},
  shortjournal = {Neural Netw.},
  title        = {Neurodynamical classifiers with low model complexity},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deconvolutional neural network for image super-resolution.
<em>NN</em>, <em>132</em>, 394–404. (<a
href="https://doi.org/10.1016/j.neunet.2020.09.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study builds a fully deconvolutional neural network (FDNN) and addresses the problem of single image super-resolution (SISR) by using the FDNN. Although SISR using deep neural networks has been a major research focus, the problem of reconstructing a high resolution (HR) image with an FDNN has received little attention. A few recent approaches toward SISR are to embed deconvolution operations into multilayer feedforward neural networks . This paper constructs a deep FDNN for SISR that possesses two remarkable advantages compared to existing SISR approaches. The first improves the network performance without increasing the depth of the network or embedding complex structures. The second replaces all convolution operations with deconvolution operations to implement an effective reconstruction. That is, the proposed FDNN only contains deconvolution layers and learns an end-to-end mapping from low resolution (LR) to HR images. Furthermore, to avoid the oversmoothness of the mean squared error loss, the trained image is treated as a probability distribution, and the Kullback–Leibler divergence is introduced into the final loss function to achieve enhanced recovery. Although the proposed FDNN only has 10 layers, it is successfully evaluated through extensive experiments. Compared with other state-of-the-art methods and deep convolution neural networks with 20 or 30 layers, the proposed FDNN achieves better performance for SISR.},
  archive      = {J_NN},
  author       = {Feilong Cao and Kaixuan Yao and Jiye Liang},
  doi          = {10.1016/j.neunet.2020.09.017},
  journal      = {Neural Networks},
  pages        = {394-404},
  shortjournal = {Neural Netw.},
  title        = {Deconvolutional neural network for image super-resolution},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning to select actions shapes recurrent dynamics in the
corticostriatal system. <em>NN</em>, <em>132</em>, 375–393. (<a
href="https://doi.org/10.1016/j.neunet.2020.09.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning to select appropriate actions based on their values is fundamental to adaptive behavior . This form of learning is supported by fronto-striatal systems. The dorsal-lateral prefrontal cortex (dlPFC) and the dorsal striatum (dSTR), which are strongly interconnected, are key nodes in this circuitry. Substantial experimental evidence, including neurophysiological recordings, have shown that neurons in these structures represent key aspects of learning. The computational mechanisms that shape the neurophysiological responses, however, are not clear. To examine this, we developed a recurrent neural network (RNN) model of the dlPFC-dSTR circuit and trained it on an oculomotor sequence learning task. We compared the activity generated by the model to activity recorded from monkey dlPFC and dSTR in the same task. This network consisted of a striatal component which encoded action values, and a prefrontal component which selected appropriate actions. After training, this system was able to autonomously represent and update action values and select actions, thus being able to closely approximate the representational structure in corticostriatal recordings. We found that learning to select the correct actions drove action-sequence representations further apart in activity space, both in the model and in the neural data . The model revealed that learning proceeds by increasing the distance between sequence-specific representations. This makes it more likely that the model will select the appropriate action sequence as learning develops. Our model thus supports the hypothesis that learning in networks drives the neural representations of actions further apart, increasing the probability that the network generates correct actions as learning proceeds. Altogether, this study advances our understanding of how neural circuit dynamics are involved in neural computation , revealing how dynamics in the corticostriatal system support task learning.},
  archive      = {J_NN},
  author       = {Christian D. Márton and Simon R. Schultz and Bruno B. Averbeck},
  doi          = {10.1016/j.neunet.2020.09.008},
  journal      = {Neural Networks},
  pages        = {375-393},
  shortjournal = {Neural Netw.},
  title        = {Learning to select actions shapes recurrent dynamics in the corticostriatal system},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dual-regression model for visual tracking. <em>NN</em>,
<em>132</em>, 364–374. (<a
href="https://doi.org/10.1016/j.neunet.2020.09.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing regression based tracking methods built on correlation filter model or convolution model do not take both accuracy and robustness into account at the same time. In this paper, we propose a dual-regression framework comprising a discriminative fully convolutional module and a fine-grained correlation filter component for visual tracking. The convolutional module trained in a classification manner with hard negative mining ensures the discriminative ability of the proposed tracker, which facilitates the handling of several challenging problems, such as drastic deformation, distractors , and complicated backgrounds. The correlation filter component built on the shallow features with fine-grained features enables accurate localization . By fusing these two branches in a coarse-to-fine manner, the proposed dual-regression tracking framework achieves a robust and accurate tracking performance. Extensive experiments on the OTB2013, OTB2015, and VOT2015 datasets demonstrate that the proposed algorithm performs favorably against the state-of-the-art methods.},
  archive      = {J_NN},
  author       = {Xin Li and Qiao Liu and Nana Fan and Zikun Zhou and Zhenyu He and Xiao-yuan Jing},
  doi          = {10.1016/j.neunet.2020.09.011},
  journal      = {Neural Networks},
  pages        = {364-374},
  shortjournal = {Neural Netw.},
  title        = {Dual-regression model for visual tracking},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-content image generation for drug discovery using
generative adversarial networks. <em>NN</em>, <em>132</em>, 353–363. (<a
href="https://doi.org/10.1016/j.neunet.2020.09.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immense amount of high-content image data generated in drug discovery screening requires computationally driven automated analysis. Emergence of advanced machine learning algorithms , like deep learning models, has transformed the interpretation and analysis of imaging data. However, deep learning methods generally require large number of high-quality data samples, which could be limited during preclinical investigations. To address this issue, we propose a generative modeling based computational framework to synthesize images, which can be used for phenotypic profiling of perturbations induced by drug compounds. We investigated the use of three variants of Generative Adversarial Network (GAN) in our framework, viz., a basic Vanilla GAN, Deep Convolutional GAN (DCGAN) and Progressive GAN (ProGAN), and found DCGAN to be most efficient in generating realistic synthetic images . A pre-trained convolutional neural network (CNN) was used to extract features of both real and synthetic images , followed by a classification model trained on real and synthetic images. The quality of synthesized images was evaluated by comparing their feature distributions with that of real images. The DCGAN-based framework was applied to high-content image data from a drug screen to synthesize high-quality cellular images, which were used to augment the real image data. The augmented dataset was shown to yield better classification performance compared with that obtained using only real images. We also demonstrated the application of proposed method on the generation of bacterial images and computed feature distributions for bacterial images specific to different drug treatments. In summary, our results showed that the proposed DCGAN-based framework can be utilized to generate realistic synthetic high-content images, thus enabling the study of drug-induced effects on cells and bacteria.},
  archive      = {J_NN},
  author       = {Shaista Hussain and Ayesha Anees and Ankit Das and Binh P. Nguyen and Mardiana Marzuki and Shuping Lin and Graham Wright and Amit Singhal},
  doi          = {10.1016/j.neunet.2020.09.007},
  journal      = {Neural Networks},
  pages        = {353-363},
  shortjournal = {Neural Netw.},
  title        = {High-content image generation for drug discovery using generative adversarial networks},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exponential synchronization of neural networks with
time-varying delays and stochastic impulses. <em>NN</em>, <em>132</em>,
342–352. (<a
href="https://doi.org/10.1016/j.neunet.2020.09.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper concentrates on the exponential synchronization problem of the delayed neural networks (DNNs) with stochastic impulses. First, the impulsive Halanay differential inequality is further extended to the case that the impulsive strengths are random variables . Then, based on the generalized inequalities, synchronization criteria are respectively proposed for DNNs with two kinds of stochastic impulses, i.e., impulses with independent property/Markovian property. It should be pointed out that only some basic statistical characteristics are needed to verify the proposed criteria. Numerical examples are provided to show the validation of the obtained theoretical results at the end of this paper.},
  archive      = {J_NN},
  author       = {Yifan Sun and Lulu Li and Xiaoyang Liu},
  doi          = {10.1016/j.neunet.2020.09.014},
  journal      = {Neural Networks},
  pages        = {342-352},
  shortjournal = {Neural Netw.},
  title        = {Exponential synchronization of neural networks with time-varying delays and stochastic impulses},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-label zero-shot learning with graph convolutional
networks. <em>NN</em>, <em>132</em>, 333–341. (<a
href="https://doi.org/10.1016/j.neunet.2020.09.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of zero-shot learning (ZSL) is to build a classifier that recognizes novel categories with no corresponding annotated training data. The typical routine is to transfer knowledge from seen classes to unseen ones by learning a visual-semantic embedding. Existing multi-label zero-shot learning approaches either ignore correlations among labels, suffer from large label combinations, or learn the embedding using only local or global visual features. In this paper, we propose a Graph Convolution Networks based Multi-label Zero-Shot Learning model, abbreviated as MZSL-GCN. Our model first constructs a label relation graph using label co-occurrences and compensates the absence of unseen labels in the training phase by semantic similarity . It then takes the graph and the word embedding of each seen (unseen) label as inputs to the GCN to learn the label semantic embedding , and to obtain a set of inter-dependent object classifiers. MZSL-GCN simultaneously trains another attention network to learn compatible local and global visual features of objects with respect to the classifiers, and thus makes the whole network end-to-end trainable. In addition, the use of unlabeled training data can reduce the bias toward seen labels and boost the generalization ability . Experimental results on benchmark datasets show that our MZSL-GCN competes with state-of-the-art approaches.},
  archive      = {J_NN},
  author       = {Guangjin Ou and Guoxian Yu and Carlotta Domeniconi and Xuequan Lu and Xiangliang Zhang},
  doi          = {10.1016/j.neunet.2020.09.010},
  journal      = {Neural Networks},
  pages        = {333-341},
  shortjournal = {Neural Netw.},
  title        = {Multi-label zero-shot learning with graph convolutional networks},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GP-GAN: Brain tumor growth prediction using stacked 3D
generative adversarial networks from longitudinal MR images.
<em>NN</em>, <em>132</em>, 321–332. (<a
href="https://doi.org/10.1016/j.neunet.2020.09.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain tumors are one of the major common causes of cancer-related death, worldwide. Growth prediction of these tumors, particularly gliomas which are the most dominant type, can be quite useful to improve treatment planning, quantify tumor aggressiveness, and estimate patients’ survival time towards precision medicine . Studying tumor growth prediction basically requires multiple time points of single or multimodal medical images of the same patient. Recent models are based on complex mathematical formulations that basically rely on a system of partial differential equations , e.g. reaction diffusion model, to capture the diffusion and proliferation of tumor cells in the surrounding tissue. However, these models usually have small number of parameters that are insufficient to capture different patterns and other characteristics of the tumors. In addition, such models consider tumor growth independently for each subject, not being able to get benefit from possible common growth patterns existed in the whole population under study. In this paper, we propose a novel data-driven method via stacked 3D generative adversarial networks (GANs), named GP-GAN, for growth prediction of glioma. Specifically, we use stacked conditional GANs with a novel objective function that includes both l 1 l1 and Dice losses. Moreover, we use segmented feature maps to guide the generator for better generated images. Our generator is designed based on a modified 3D U-Net architecture with skip connections to combine hierarchical features and thus have a better generated image. The proposed method is trained and tested on 18 subjects with 3 time points (9 subjects from collaborative hospital and 9 subjects from BRATS 2014 dataset). Results show that our proposed GP-GAN outperforms state-of-the-art methods for glioma growth prediction and attain average Jaccard index and Dice coefficient of 78.97\% and 88.26\%, respectively.},
  archive      = {J_NN},
  author       = {Ahmed Elazab and Changmiao Wang and Syed Jamal Safdar Gardezi and Hongmin Bai and Qingmao Hu and Tianfu Wang and Chunqi Chang and Baiying Lei},
  doi          = {10.1016/j.neunet.2020.09.004},
  journal      = {Neural Networks},
  pages        = {321-332},
  shortjournal = {Neural Netw.},
  title        = {GP-GAN: Brain tumor growth prediction using stacked 3D generative adversarial networks from longitudinal MR images},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybrid tensor decomposition in neural network compression.
<em>NN</em>, <em>132</em>, 309–320. (<a
href="https://doi.org/10.1016/j.neunet.2020.09.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have enabled impressive breakthroughs in various artificial intelligence (AI) applications recently due to its capability of learning high-level features from big data. However, the current demand of DNNs for computational resources especially the storage consumption is growing due to that the increasing sizes of models are being required for more and more complicated applications. To address this problem, several tensor decomposition methods including tensor-train (TT) and tensor-ring (TR) have been applied to compress DNNs and shown considerable compression effectiveness. In this work, we introduce the hierarchical Tucker (HT), a classical but rarely-used tensor decomposition method , to investigate its capability in neural network compression. We convert the weight matrices and convolutional kernels to both HT and TT formats for comparative study, since the latter is the most widely used decomposition method and the variant of HT. We further theoretically and experimentally discover that the HT format has better performance on compressing weight matrices , while the TT format is more suited for compressing convolutional kernels. Based on this phenomenon we propose a strategy of hybrid tensor decomposition by combining TT and HT together to compress convolutional and fully connected parts separately and attain better accuracy than only using the TT or HT format on convolutional neural networks (CNNs). Our work illuminates the prospects of hybrid tensor decomposition for neural network compression.},
  archive      = {J_NN},
  author       = {Bijiao Wu and Dingheng Wang and Guangshe Zhao and Lei Deng and Guoqi Li},
  doi          = {10.1016/j.neunet.2020.09.006},
  journal      = {Neural Networks},
  pages        = {309-320},
  shortjournal = {Neural Netw.},
  title        = {Hybrid tensor decomposition in neural network compression},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time gun detection in CCTV: An open problem.
<em>NN</em>, <em>132</em>, 297–308. (<a
href="https://doi.org/10.1016/j.neunet.2020.09.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detectors have improved in recent years, obtaining better results and faster inference time. However, small object detection is still a problem that has not yet a definitive solution. The autonomous weapons detection on Closed-circuit television (CCTV) has been studied recently, being extremely useful in the field of security, counter-terrorism, and risk mitigation . This article presents a new dataset obtained from a real CCTV installed in a university and the generation of synthetic images , to which Faster R-CNN was applied using Feature Pyramid Network with ResNet-50 resulting in a weapon detection model able to be used in quasi real-time CCTV (90 ms of inference time with an NVIDIA GeForce GTX-1080Ti card) improving the state of the art on weapon detection in a two stages training. In this work, an exhaustive experimental study of the detector with these datasets was performed, showing the impact of synthetic datasets on the training of weapons detection systems, as well as the main limitations that these systems present nowadays. The generated synthetic dataset and the real CCTV dataset are available to the whole research community.},
  archive      = {J_NN},
  author       = {Jose L. Salazar González and Carlos Zaccaro and Juan A. Álvarez-García and Luis M. Soria Morillo and Fernando Sancho Caparrini},
  doi          = {10.1016/j.neunet.2020.09.013},
  journal      = {Neural Networks},
  pages        = {297-308},
  shortjournal = {Neural Netw.},
  title        = {Real-time gun detection in CCTV: An open problem},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysis of the transferability and robustness of GANs
evolved for pareto set approximations. <em>NN</em>, <em>132</em>,
281–296. (<a
href="https://doi.org/10.1016/j.neunet.2020.09.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generative adversarial network (GAN) is a good example of a strong-performing, neural network-based generative model , even though it does have some drawbacks of its own. Mode collapsing and the difficulty in finding the optimal network structure are two of the most concerning issues. In this paper, we address these two issues at the same time by proposing a neuro-evolutionary approach with an agile evaluation method for the fast evolution of robust deep architectures that avoid mode collapsing. The computation of Pareto set approximations with GANs is chosen as a suitable benchmark to evaluate the quality of our approach. Furthermore, we demonstrate the consistency, scalability, and generalization capabilities of the proposed method, which shows its potential applications to many areas. We finally readdress the issue of designing this kind of models by analyzing the characteristics of the best performing GAN specifications, and conclude with a set of general guidelines. This results in a reduction of the many-dimensional problem of structural manual design or automated search.},
  archive      = {J_NN},
  author       = {Unai Garciarena and Alexander Mendiburu and Roberto Santana},
  doi          = {10.1016/j.neunet.2020.09.003},
  journal      = {Neural Networks},
  pages        = {281-296},
  shortjournal = {Neural Netw.},
  title        = {Analysis of the transferability and robustness of GANs evolved for pareto set approximations},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Boundary mittag-leffler stabilization of fractional
reaction–diffusion cellular neural networks. <em>NN</em>, <em>132</em>,
269–280. (<a
href="https://doi.org/10.1016/j.neunet.2020.09.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mittag-Leffler stabilization is studied for fractional reaction–diffusion cellular neural networks (FRDCNNs) in this paper. Different from previous literature, the FRDCNNs in this paper are high-dimensional systems, and boundary control and observed-based boundary control are both used to make FRDCNNs achieve Mittag-Leffler stability. First, a state-dependent boundary controller is designed when system states are available. By employing the spatial integral functional method and some inequalities, a criterion ensuring Mittag-Leffler stability of FRDCNNs is presented. Then, when the information of system states is not fully accessible, an observer is presented to estimate the system states based on boundary output and an observer-based boundary controller is provided aiming to stabilize the considered FRDCNNs. Furthermore, a robust observer-based boundary controller is proposed to ensure the Mittag-Leffler stability for FRDCNNs with uncertainties. Examples are given to illustrate the effectiveness of obtained theoretical results.},
  archive      = {J_NN},
  author       = {Xiao-Zhen Liu and Ze-Tao Li and Kai-Ning Wu},
  doi          = {10.1016/j.neunet.2020.09.009},
  journal      = {Neural Networks},
  pages        = {269-280},
  shortjournal = {Neural Netw.},
  title        = {Boundary mittag-leffler stabilization of fractional reaction–diffusion cellular neural networks},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). A direct approach for function approximation on data
defined manifolds. <em>NN</em>, <em>132</em>, 253–268. (<a
href="https://doi.org/10.1016/j.neunet.2020.08.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In much of the literature on function approximation by deep networks, the function is assumed to be defined on some known domain, such as a cube or a sphere. In practice, the data might not be dense on these domains, and therefore, the approximation theory results are observed to be too conservative. In manifold learning, one assumes instead that the data is sampled from an unknown manifold; i.e., the manifold is defined by the data itself. Function approximation on this unknown manifold is then a two stage procedure: first, one approximates the Laplace–Beltrami operator (and its eigen-decomposition) on this manifold using a graph Laplacian , and next, approximates the target function using the eigen-functions. Alternatively, one estimates first some atlas on the manifold and then uses local approximation techniques based on the local coordinate charts. In this paper, we propose a more direct approach to function approximation on unknown , data defined manifolds without computing the eigen-decomposition of some operator or an atlas for the manifold, and without any kind of training in the classical sense. Our constructions are universal; i.e., do not require the knowledge of any prior on the target function other than continuity on the manifold. We estimate the degree of approximation. For smooth functions, the estimates do not suffer from the so-called saturation phenomenon. We demonstrate via a property called good propagation of errors how the results can be lifted for function approximation using deep networks where each channel evaluates a Gaussian network on a possibly unknown manifold.},
  archive      = {J_NN},
  author       = {H.N. Mhaskar},
  doi          = {10.1016/j.neunet.2020.08.018},
  journal      = {Neural Networks},
  pages        = {253-268},
  shortjournal = {Neural Netw.},
  title        = {A direct approach for function approximation on data defined manifolds},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Low-rank tensor constrained co-regularized multi-view
spectral clustering. <em>NN</em>, <em>132</em>, 245–252. (<a
href="https://doi.org/10.1016/j.neunet.2020.08.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the efficiency of exploiting relationships and complex structures hidden in multi-views data, graph-oriented clustering methods have achieved remarkable progress in recent years. But most existing graph-based spectral methods still have the following demerits: (1) They regularize each view equally, which does not make sense in real applications. (2) By employing different norms, most existing methods calculate the error feature by feature, resulting in neglecting the spatial structure information and the complementary information. To tackle the aforementioned drawbacks, we propose an enhanced multi-view spectral clustering model. Our model characterizes the consistency among indicator matrices by minimizing our proposed weighted tensor nuclear norm , which explicitly exploits the salient different information between singular values of the matrix. Moreover, our model adaptively assigns a reasonable weight to each view, which helps improve robustness of the algorithm. Finally, the proposed tensor nuclear norm well exploits both high-order and complementary information, which helps mine the consistency between indicator matrices . Extensive experiments indicate the efficiency of our method.},
  archive      = {J_NN},
  author       = {Huiling Xu and Xiangdong Zhang and Wei Xia and Quanxue Gao and Xinbo Gao},
  doi          = {10.1016/j.neunet.2020.08.019},
  journal      = {Neural Networks},
  pages        = {245-252},
  shortjournal = {Neural Netw.},
  title        = {Low-rank tensor constrained co-regularized multi-view spectral clustering},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient search for informational cores in complex systems:
Application to brain networks. <em>NN</em>, <em>132</em>, 232–244. (<a
href="https://doi.org/10.1016/j.neunet.2020.08.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important step in understanding the nature of the brain is to identify “cores” in the brain network , where brain areas strongly interact with each other. Cores can be considered as essential sub-networks for brain functions. In the last few decades, an information-theoretic approach to identifying cores has been developed. In this approach, interactions between parts are measured by an information loss function, which quantifies how much information would be lost if interactions between parts were removed. Then, a core called a “complex” is defined as a subsystem wherein the amount of information loss is locally maximal. Although identifying complexes can be a novel and useful approach, its application is practically impossible because computation time grows exponentially with system size. Here we propose a fast and exact algorithm for finding complexes, called Hierarchical Partitioning for Complex search (HPC). HPC hierarchically partitions systems to narrow down candidates for complexes. The computation time of HPC is polynomial, enabling us to find complexes in large systems (up to several hundred) in a practical amount of time. We prove that HPC is exact when an information loss function satisfies a mathematical property , monotonicity. We show that mutual information is one such information loss function. We also show that a broad class of submodular functions can be considered as such information loss functions, indicating the expandability of our framework to the class. We applied HPC to electrocorticogram recordings from a monkey and demonstrated that HPC revealed temporally stable and characteristic complexes.},
  archive      = {J_NN},
  author       = {Jun Kitazono and Ryota Kanai and Masafumi Oizumi},
  doi          = {10.1016/j.neunet.2020.08.020},
  journal      = {Neural Networks},
  pages        = {232-244},
  shortjournal = {Neural Netw.},
  title        = {Efficient search for informational cores in complex systems: Application to brain networks},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stochastic DCA for minimizing a large sum of DC functions
with application to multi-class logistic regression. <em>NN</em>,
<em>132</em>, 220–231. (<a
href="https://doi.org/10.1016/j.neunet.2020.08.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the large sum of DC (Difference of Convex) functions minimization problem which appear in several different areas, especially in stochastic optimization and machine learning . Two DCA (DC Algorithm) based algorithms are proposed: stochastic DCA and inexact stochastic DCA. We prove that the convergence of both algorithms to a critical point is guaranteed with probability one. Furthermore, we develop our stochastic DCA for solving an important problem in multi-task learning, namely group variables selection in multi class logistic regression . The corresponding stochastic DCA is very inexpensive, all computations are explicit. Numerical experiments on several benchmark datasets and synthetic datasets illustrate the efficiency of our algorithms and their superiority over existing methods, with respect to classification accuracy , sparsity of solution as well as running time.},
  archive      = {J_NN},
  author       = {Hoai An Le Thi and Hoai Minh Le and Duy Nhat Phan and Bach Tran},
  doi          = {10.1016/j.neunet.2020.08.024},
  journal      = {Neural Networks},
  pages        = {220-231},
  shortjournal = {Neural Netw.},
  title        = {Stochastic DCA for minimizing a large sum of DC functions with application to multi-class logistic regression},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic event-based state estimation for delayed artificial
neural networks with multiplicative noises: A gain-scheduled approach.
<em>NN</em>, <em>132</em>, 211–219. (<a
href="https://doi.org/10.1016/j.neunet.2020.08.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study is concerned with the state estimation issue for a kind of delayed artificial neural networks with multiplicative noises . The occurrence of the time delay is in a random way that is modeled by a Bernoulli distributed stochastic variable whose occurrence probability is time-varying and confined within a given interval. A gain-scheduled approach is proposed for the estimator design to accommodate the time-varying nature of the occurrence probability . For the sake of utilizing the communication resource as efficiently as possible, a dynamic event triggering mechanism is put forward to orchestrate the data delivery from the sensor to the estimator. Sufficient conditions are established to ensure that, in the simultaneous presence of the external noises, the randomly occurring time delays with time-varying occurrence probability as well as the dynamic event triggering communication protocol, the estimation error is exponentially ultimately bounded in the mean square . Moreover, the estimator gain matrices are explicitly calculated in terms of the solution to certain easy-to-solve matrix inequalities. Simulation examples are provided to show the validity of the proposed state estimation method.},
  archive      = {J_NN},
  author       = {Shuai Liu and Zidong Wang and Yun Chen and Guoliang Wei},
  doi          = {10.1016/j.neunet.2020.08.023},
  journal      = {Neural Networks},
  pages        = {211-219},
  shortjournal = {Neural Netw.},
  title        = {Dynamic event-based state estimation for delayed artificial neural networks with multiplicative noises: A gain-scheduled approach},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quantum-like influence diagrams for decision-making.
<em>NN</em>, <em>132</em>, 190–210. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a novel and comprehensive framework on how to describe the probabilistic nature of decision-making process. We suggest extending the quantum-like Bayesian network formalism to incorporate the notion of maximum expected utility to model human paradoxical, sub-optimal and irrational decisions. What distinguishes this work is that we take advantage of the quantum interference effects produced in quantum-like Bayesian Networks during the inference process to influence the probabilities used to compute the maximum expected utility of some decision. The proposed quantum-like decision model is able to (1) predict the probability distributions found in different experiments reported in the literature by modelling uncertainty through quantum interference , (2) to identify decisions that the decision-makers perceive to be optimal within their belief space, but that are actually irrational with respect to expected utility theory , (3) gain an understanding of how the decision-maker’s beliefs evolve within a decision-making scenario. The proposed model has the potential to provide new insights in decision science, as well as having direct implications for decision support systems that deal with human data, such as in the fields of economics, finance, psychology, etc.},
  archive      = {J_NN},
  author       = {Catarina Moreira and Prayag Tiwari and Hari Mohan Pandey and Peter Bruza and Andreas Wichert},
  doi          = {10.1016/j.neunet.2020.07.009},
  journal      = {Neural Networks},
  pages        = {190-210},
  shortjournal = {Neural Netw.},
  title        = {Quantum-like influence diagrams for decision-making},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MGAT: Multi-view graph attention networks. <em>NN</em>,
<em>132</em>, 180–189. (<a
href="https://doi.org/10.1016/j.neunet.2020.08.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view graph embedding is aimed at learning low-dimensional representations of nodes that capture various relationships in a multi-view network, where each view represents a type of relationship among nodes. Multitudes of existing graph embedding approaches concentrate on single-view networks, that can only characterize one simple type of proximity relationships among objects. However, most of the real-world complex systems possess multiple types of relationships among entities. In this paper, a novel approach of graph embedding for multi-view networks is proposed, named Multi-view Graph Attention Networks (MGAT). We explore an attention-based architecture for learning node representations from each single view, the network parameters of which are constrained by a novel regularization term. In order to collaboratively integrate multiple types of relationships in different views, a view-focused attention method is explored to aggregate the view-wise node representations. We evaluate the proposed algorithm on several real-world datasets, and it demonstrates that the proposed approach outperforms existing state-of-the-art baselines.},
  archive      = {J_NN},
  author       = {Yu Xie and Yuanqiao Zhang and Maoguo Gong and Zedong Tang and Chao Han},
  doi          = {10.1016/j.neunet.2020.08.021},
  journal      = {Neural Networks},
  pages        = {180-189},
  shortjournal = {Neural Netw.},
  title        = {MGAT: Multi-view graph attention networks},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SympNets: Intrinsic structure-preserving symplectic networks
for identifying hamiltonian systems. <em>NN</em>, <em>132</em>, 166–179.
(<a href="https://doi.org/10.1016/j.neunet.2020.08.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose new symplectic networks (SympNets) for identifying Hamiltonian systems from data based on a composition of linear, activation and gradient modules. In particular, we define two classes of SympNets: the LA-SympNets composed of linear and activation modules, and the G-SympNets composed of gradient modules. Correspondingly, we prove two new universal approximation theorems that demonstrate that SympNets can approximate arbitrary symplectic maps based on appropriate activation functions . We then perform several experiments including the pendulum, double pendulum and three-body problems to investigate the expressivity and the generalization ability of SympNets. The simulation results show that even very small size SympNets can generalize well, and are able to handle both separable and non-separable Hamiltonian systems with data points resulting from short or long time steps. In all the test cases, SympNets outperform the baseline models , and are much faster in training and prediction. We also develop an extended version of SympNets to learn the dynamics from irregularly sampled data. This extended version of SympNets can be thought of as a universal model representing the solution to an arbitrary Hamiltonian system.},
  archive      = {J_NN},
  author       = {Pengzhan Jin and Zhen Zhang and Aiqing Zhu and Yifa Tang and George Em Karniadakis},
  doi          = {10.1016/j.neunet.2020.08.017},
  journal      = {Neural Networks},
  pages        = {166-179},
  shortjournal = {Neural Netw.},
  title        = {SympNets: Intrinsic structure-preserving symplectic networks for identifying hamiltonian systems},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Frequency-dependent organization of the brain’s functional
network through delayed-interactions. <em>NN</em>, <em>132</em>,
155–165. (<a
href="https://doi.org/10.1016/j.neunet.2020.08.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The structure of the brain network exhibits modularity at multiple spatial scales . The effect of the modular structure on the brain dynamics has been the focus of several studies in recent years but many aspects remain to be explored. For example, it is not well-known how the delays in the transmission of signals between the neurons and the brain regions interact with the modular structure to determine the brain dynamics. In this paper, we show an important impact of the delays on the collective dynamics of brain networks with modular structure; that is, the degree of the synchrony between different brain regions depends on the oscillating frequency. In particular, we show that when increasing the frequency of the nodes the network transits from a global synchrony state to an asynchronous state, through a transition region over which the local synchrony inside the modules is stronger than the global synchrony. When the delays depend on the distance between the nodes, the modular structure of different spatial scales appears in the correlation matrix over different specific frequency bands, so that, finer spatial modular structures reveal in higher frequency bands. The results are corroborated by a simple theoretical argument and elaborated by simulations on several simplified modular networks and the connectome with different spatial resolutions.},
  archive      = {J_NN},
  author       = {Abolfazl Ziaeemehr and Mina Zarei and Alireza Valizadeh and Claudio R. Mirasso},
  doi          = {10.1016/j.neunet.2020.08.003},
  journal      = {Neural Networks},
  pages        = {155-165},
  shortjournal = {Neural Netw.},
  title        = {Frequency-dependent organization of the brain’s functional network through delayed-interactions},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised spectral mapping and feature selection for
hyperspectral anomaly detection. <em>NN</em>, <em>132</em>, 144–154. (<a
href="https://doi.org/10.1016/j.neunet.2020.08.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exploring techniques that breakthrough the unknown space or material species is of considerable significance to military and civilian fields, and it is a challenging task without any prior information. Nowadays, the use of material-specific spectral information to detect unknowns has received increasing interest. However, affected by noise and interference, high-dimensional hyperspectral anomaly detection is difficult to meet the requirements of high detection accuracy and low false alarm rate . Besides, there is a problem of insufficient and unbalanced samples. To address these problems, we propose a novel hyperspectral anomaly detection framework based on spectral mapping and feature selection (SMFS) in an unsupervised manner . The SMFS introduces the essential properties of hyperspectral data into an unsupervised neural network to construct the nonlinear mapping relationship from high-dimensional spectral space to low-dimensional deep feature space. And it searches the optimal feature subset from the candidate feature space for standing out anomalies. Because of the compelling characterization of the encoder, we develop it specifically for spectral signatures to reveal the hidden data. Quantitative and qualitative experiments on real hyperspectral datasets indicate that the proposed method can provide the compact features overcoming the problems of noise, interference, redundancy and time-consuming caused by high-dimensionality and limited samples. And it has advantages over some state-of-the-art competitors concerning detecting anomalies of different scales.},
  archive      = {J_NN},
  author       = {Weiying Xie and Yunsong Li and Jie Lei and Jian Yang and Jiaojiao Li and Xiuping Jia and Zhen Li},
  doi          = {10.1016/j.neunet.2020.08.010},
  journal      = {Neural Networks},
  pages        = {144-154},
  shortjournal = {Neural Netw.},
  title        = {Unsupervised spectral mapping and feature selection for hyperspectral anomaly detection},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning image features with fewer labels using a
semi-supervised deep convolutional network. <em>NN</em>, <em>132</em>,
131–143. (<a
href="https://doi.org/10.1016/j.neunet.2020.08.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning feature embeddings for pattern recognition is a relevant task for many applications. Deep learning methods such as convolutional neural networks can be employed for this assignment with different training strategies: leveraging pre-trained models as baselines; training from scratch with the target dataset; or fine-tuning from the pre-trained model. Although there are separate systems used for learning features from labelled and unlabelled data , there are few models combining all available information. Therefore, in this paper, we present a novel semi-supervised deep network training strategy that comprises a convolutional network and an autoencoder using a joint classification and reconstruction loss function. We show our network improves the learned feature embedding when including the unlabelled data in the training process. The results using the feature embedding obtained by our network achieve better classification accuracy when compared with competing methods, as well as offering good generalisation in the context of transfer learning . Furthermore, the proposed network ensemble and loss function is highly extensible and applicable in many recognition tasks.},
  archive      = {J_NN},
  author       = {Fernando P. dos Santos and Cemre Zor and Josef Kittler and Moacir A. Ponti},
  doi          = {10.1016/j.neunet.2020.08.016},
  journal      = {Neural Networks},
  pages        = {131-143},
  shortjournal = {Neural Netw.},
  title        = {Learning image features with fewer labels using a semi-supervised deep convolutional network},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). H∞ and l2-l∞ state estimation for delayed memristive neural
networks on finite horizon: The round-robin protocol. <em>NN</em>,
<em>132</em>, 121–130. (<a
href="https://doi.org/10.1016/j.neunet.2020.08.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a protocol-based finite-horizon H ∞ H∞ and l 2 l2 - l ∞ l∞ estimation approach is put forward to solve the state estimation problem for discrete-time memristive neural networks (MNNs) subject to time-varying delays and energy-bounded disturbances. The Round-Robin protocol is utilized to mitigate unnecessary network congestion occurring in the sensor-to-estimator communication channel. For the delayed MNNs, our aim is to devise an estimator that not only ensures a prescribed disturbance attenuation level over a finite time-horizon, but also keeps the peak value of the estimation error within a given range. By resorting to the Lyapunov–Krasovskii functional method, the delay-dependent criteria are formulated that guarantee the existence of the desired estimator. Subsequently, the estimator gains are obtained via figuring out a bank of convex optimization problems . The validity of our estimator is finally shown via a numerical example.},
  archive      = {J_NN},
  author       = {Hongjian Liu and Zidong Wang and Weiyin Fei and Jiahui Li},
  doi          = {10.1016/j.neunet.2020.08.006},
  journal      = {Neural Networks},
  pages        = {121-130},
  shortjournal = {Neural Netw.},
  title        = {H∞ and l2-l∞ state estimation for delayed memristive neural networks on finite horizon: The round-robin protocol},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comparing SNNs and RNNs on neuromorphic vision datasets:
Similarities and differences. <em>NN</em>, <em>132</em>, 108–120. (<a
href="https://doi.org/10.1016/j.neunet.2020.08.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuromorphic data, recording frameless spike events, have attracted considerable attention for the spatiotemporal information components and the event-driven processing fashion. Spiking neural networks (SNNs) represent a family of event-driven models with spatiotemporal dynamics for neuromorphic computing, which are widely benchmarked on neuromorphic data. Interestingly, researchers in the machine learning community can argue that recurrent (artificial) neural networks (RNNs) also have the capability to extract spatiotemporal features although they are not event-driven. Thus, the question of “what will happen if we benchmark these two kinds of models together on neuromorphic data” comes out but remains unclear. In this work, we make a systematic study to compare SNNs and RNNs on neuromorphic data, taking the vision datasets as a case study. First, we identify the similarities and differences between SNNs and RNNs (including the vanilla RNNs and LSTM) from the modeling and learning perspectives. To improve comparability and fairness, we unify the supervised learning algorithm based on backpropagation through time (BPTT), the loss function exploiting the outputs at all timesteps, the network structure with stacked fully-connected or convolutional layers , and the hyper-parameters during training. Especially, given the mainstream loss function used in RNNs, we modify it inspired by the rate coding scheme to approach that of SNNs. Furthermore, we tune the temporal resolution of datasets to test model robustness and generalization. At last, a series of contrast experiments are conducted on two types of neuromorphic datasets: DVS-converted (N-MNIST) and DVS-captured (DVS Gesture). Extensive insights regarding recognition accuracy, feature extraction, temporal resolution and contrast, learning generalization, computational complexity and parameter volume are provided, which are beneficial for the model selection on different workloads and even for the invention of novel neural models in the future.},
  archive      = {J_NN},
  author       = {Weihua He and YuJie Wu and Lei Deng and Guoqi Li and Haoyu Wang and Yang Tian and Wei Ding and Wenhui Wang and Yuan Xie},
  doi          = {10.1016/j.neunet.2020.08.001},
  journal      = {Neural Networks},
  pages        = {108-120},
  shortjournal = {Neural Netw.},
  title        = {Comparing SNNs and RNNs on neuromorphic vision datasets: Similarities and differences},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Emotional EEG classification using connectivity features and
convolutional neural networks. <em>NN</em>, <em>132</em>, 96–107. (<a
href="https://doi.org/10.1016/j.neunet.2020.08.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) are widely used to recognize the user’s state through electroencephalography (EEG) signals. In the previous studies, the EEG signals are usually fed into the CNNs in the form of high-dimensional raw data. However, this approach makes it difficult to exploit the brain connectivity information that can be effective in describing the functional brain network and estimating the perceptual state of the user. We introduce a new classification system that utilizes brain connectivity with a CNN and validate its effectiveness via the emotional video classification by using three different types of connectivity measures. Furthermore, two data-driven methods to construct the connectivity matrix are proposed to maximize classification performance. Further analysis reveals that the level of concentration of the brain connectivity related to the emotional property of the target video is correlated with classification performance.},
  archive      = {J_NN},
  author       = {Seong-Eun Moon and Chun-Jui Chen and Cho-Jui Hsieh and Jane-Ling Wang and Jong-Seok Lee},
  doi          = {10.1016/j.neunet.2020.08.009},
  journal      = {Neural Networks},
  pages        = {96-107},
  shortjournal = {Neural Netw.},
  title        = {Emotional EEG classification using connectivity features and convolutional neural networks},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved dual-scale residual network for image
super-resolution. <em>NN</em>, <em>132</em>, 84–95. (<a
href="https://doi.org/10.1016/j.neunet.2020.08.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, convolutional neural networks have been successfully applied to single image super-resolution (SISR) tasks, making breakthrough progress both in accuracy and speed. In this work, an improved dual-scale residual network (IDSRN), achieving promising reconstruction performance without sacrificing too much calculations, is proposed for SISR. The proposed network extracts features through two independent parallel branches: dual-scale feature extraction branch and texture attention branch. The improved dual-scale residual block (IDSRB) combined with active weighted mapping strategy constitutes the dual-scale feature extraction branch, which aims to capture dual-scale features of the image. As regards the texture attention branch, an encoder–decoder network employing symmetric full convolutional-deconvolution structure acts as a feature selector to enhance the high-frequency details. The integration of two branches reaches the goal of capturing dual-scale features with high-frequency information. Comparative experiments and extensive studies indicate that the proposed IDSRN can catch up with the state-of-the-art approaches in terms of accuracy and efficiency.},
  archive      = {J_NN},
  author       = {Huan Liu and Feilong Cao},
  doi          = {10.1016/j.neunet.2020.08.008},
  journal      = {Neural Networks},
  pages        = {84-95},
  shortjournal = {Neural Netw.},
  title        = {Improved dual-scale residual network for image super-resolution},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploiting bi-directional global transition patterns and
personal preferences for missing POI category identification.
<em>NN</em>, <em>132</em>, 75–83. (<a
href="https://doi.org/10.1016/j.neunet.2020.08.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the increasing popularity of Location-based Social Network (LBSN) services, which provides unparalleled opportunities to build personalized Point-of-Interest (POI) recommender systems . Existing POI recommendation and location prediction tasks utilize past information for future recommendation or prediction from a single direction perspective, while the missing POI category identification task needs to utilize the check-in information both before and after the missing category. Therefore, a long-standing challenge is how to effectively identify the missing POI categories at any time in the real-world check-in data of mobile users. To this end, in this paper, we propose a novel neural network approach to identify the missing POI categories by integrating both bi-directional global non-personal transition patterns and personal preferences of users. Specifically, we delicately design an attention matching cell to model how well the check-in category information matches their non-personal transition patterns and personal preferences. Finally, we evaluate our model on two real-world datasets, which clearly validate its effectiveness compared with the state-of-the-art baselines. Furthermore, our model can be naturally extended to address next POI category recommendation and prediction tasks with competitive performance.},
  archive      = {J_NN},
  author       = {Dongbo Xi and Fuzhen Zhuang and Yanchi Liu and Hengshu Zhu and Pengpeng Zhao and Chang Tan and Qing He},
  doi          = {10.1016/j.neunet.2020.08.015},
  journal      = {Neural Networks},
  pages        = {75-83},
  shortjournal = {Neural Netw.},
  title        = {Exploiting bi-directional global transition patterns and personal preferences for missing POI category identification},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CariGAN: Caricature generation through weakly paired
adversarial learning. <em>NN</em>, <em>132</em>, 66–74. (<a
href="https://doi.org/10.1016/j.neunet.2020.08.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Caricature generation is an interesting yet challenging task. The primary goal is to generate a plausible caricature with reasonable exaggerations given a face image. Conventional caricature generation approaches mainly use low-level geometric transformations such as image warping to generate exaggerated images, which lack richness and diversity in terms of content and style. The recent progress in generative adversarial networks (GANs) makes it possible to learn an image-to-image transformation from data so as to generate diverse images. However, directly applying GAN-based models to this task leads to unsatisfactory results due to the large variance in the caricature distribution. Moreover, conventional models typically require pixel-wisely paired training data which largely limits their usage scenarios . In this paper, we model caricature generation as a weakly paired image-to-image translation task, and propose CariGAN to address these issues. Specifically, to enforce reasonable exaggeration and facial deformation, manually annotated caricature facial landmarks are used as an additional condition to constrain the generated image. Furthermore, an image fusion mechanism is designed to encourage our model to focus on the key facial parts so that more vivid details in these regions can be generated. Finally, a diversity loss is proposed to encourage the model to produce diverse results. Extensive experiments on a large-scale “WebCaricature” dataset show that the proposed CariGAN can generate more visually plausible caricatures with larger diversity compared with the state-of-the-art models.},
  archive      = {J_NN},
  author       = {Wenbin Li and Wei Xiong and Haofu Liao and Jing Huo and Yang Gao and Jiebo Luo},
  doi          = {10.1016/j.neunet.2020.08.011},
  journal      = {Neural Networks},
  pages        = {66-74},
  shortjournal = {Neural Netw.},
  title        = {CariGAN: Caricature generation through weakly paired adversarial learning},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hierarchical fusion of common sense knowledge and classifier
decisions for answer selection in community question answering.
<em>NN</em>, <em>132</em>, 53–65. (<a
href="https://doi.org/10.1016/j.neunet.2020.08.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of answer selection is to select the most applicable answers from an answer candidate pool. It plays an essential role in numerous applications in information retrieval (IR) and natural language processing (NLP). In this paper, we introduce a novel Knowledge-enhanced Hierarchical Attention mechanism for Answer Selection (KHAAS), which fully exploits the common sense knowledge from knowledge bases (KBs) and input textual information. Specifically, we first devise a three-stage knowledge-enhanced hierarchical attention mechanism, including the word-level attention, the phrase-level attention, and the document-level attention to learn the fact-aware intra-document features within questions and answers by fusing the knowledge from both the question/answer and KB. Hence, we can leverage the semantic compositionality of the question/answer and learn more holistic knowledge-enhanced intra-document features of the question/answer at three levels of granularity . Second, after obtaining the knowledge-enhanced question and answer representations, we employ a multi-perspective co-attention network to learn the complex inter-document relationships between the question and answer representations from different representation subspaces, which can capture the interactive semantics of the question and answer representations at three levels. Finally, we propose an adaptive decision fusion method to learn a more effective and robust ensemble answer selection model by adaptively combining multiple classifiers learned with different levels of features. Experimental results on three large-scale answer selection datasets demonstrate that KHAAS consistently outperforms the compared methods.},
  archive      = {J_NN},
  author       = {Min Yang and Lei Chen and Ziyu Lyu and Junhao Liu and Ying Shen and Qingyao Wu},
  doi          = {10.1016/j.neunet.2020.08.005},
  journal      = {Neural Networks},
  pages        = {53-65},
  shortjournal = {Neural Netw.},
  title        = {Hierarchical fusion of common sense knowledge and classifier decisions for answer selection in community question answering},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High tissue contrast image synthesis via multistage
attention-GAN: Application to segmenting brain MR scans. <em>NN</em>,
<em>132</em>, 43–52. (<a
href="https://doi.org/10.1016/j.neunet.2020.08.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic resonance imaging (MRI) presents a detailed image of the internal organs via a magnetic field. Given MRI’s non-invasive advantage in repeated imaging, the low-contrast MR images in the target area make segmentation of tissue a challenging problem. This study shows the potential advantages of synthetic high tissue contrast (HTC) images through image-to-image translation techniques. Mainly, we use a novel cycle generative adversarial network (Cycle-GAN), which provides a mechanism of attention to increase the contrast within the tissue. The attention block and training on HTC images are beneficial to our model to enhance tissue visibility. We use a multistage architecture to concentrate on a single tissue as a preliminary and filter out the irrelevant context in every stage in order to increase the resolution of HTC images. The multistage architecture reduces the gap between source and target domains and alleviates synthetic images’ artefacts. We apply our HTC image synthesising method to two public datasets. In order to validate the effectiveness of these images we use HTC MR images in both end-to-end and two-stage segmentation structures. The experiments on three segmentation baselines on BraTS’18 demonstrate that joining the synthetic HTC images in the multimodal segmentation framework develops the average Dice similarity scores (DSCs) of 0.8\%, 0.6\%, and 0.5\% respectively on the whole tumour (WT), tumour core (TC), and enhancing tumour (ET) while removing one real MRI channels from the segmentation pipeline. Moreover, segmentation of infant brain tissue in T1w MR slices through our framework improves DSCs approximately 1\% in cerebrospinal fluid (CSF), grey matter (GM), and white matter (WM) compared to state-of-the-art segmentation techniques . The source code of synthesising HTC images is publicly available.},
  archive      = {J_NN},
  author       = {Mohammad Hamghalam and Tianfu Wang and Baiying Lei},
  doi          = {10.1016/j.neunet.2020.08.014},
  journal      = {Neural Networks},
  pages        = {43-52},
  shortjournal = {Neural Netw.},
  title        = {High tissue contrast image synthesis via multistage attention-GAN: Application to segmenting brain MR scans},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Event-driven h∞ control with critic learning for nonlinear
systems. <em>NN</em>, <em>132</em>, 30–42. (<a
href="https://doi.org/10.1016/j.neunet.2020.08.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study an event-driven H ∞ H∞ control problem of continuous-time nonlinear systems . Initially, with the introduction of a discounted cost function, we convert the nonlinear H ∞ H∞ control problem into an event-driven nonlinear two-player zero-sum game. Then, we develop an event-driven Hamilton–Jacobi–Isaacs equation (HJIE) related to the two-player zero-sum game. After that, we propose a novel event-triggering condition guaranteeing Zeno behavior not to happen. The triggering threshold in the newly proposed event-triggering condition can be kept positive without requiring to properly choose the prescribed level of disturbance attenuation. To solve the event-driven HJIE, we employ an adaptive critic architecture which contains a unique critic neural network (NN). The weight parameters used in the critic NN are tuned via the gradient descent method . After that, we carry out stability analysis of the hybrid closed-loop system based on Lyapunov’s direct approach. Finally, we provide two nonlinear plants , including the pendulum system , to validate the proposed event-driven H ∞ H∞ control scheme.},
  archive      = {J_NN},
  author       = {Xiong Yang and Zhongke Gao and Jinhui Zhang},
  doi          = {10.1016/j.neunet.2020.08.004},
  journal      = {Neural Networks},
  pages        = {30-42},
  shortjournal = {Neural Netw.},
  title        = {Event-driven h∞ control with critic learning for nonlinear systems},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive balancing of exploration and exploitation around
the edge of chaos in internal-chaos-based learning. <em>NN</em>,
<em>132</em>, 19–29. (<a
href="https://doi.org/10.1016/j.neunet.2020.08.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses learning with exploration driven by chaotic internal dynamics of a neural network . Hoerzer et al. showed that a chaotic reservoir network (RN) can learn with exploration driven by external random noise and a sequential reward. In this paper, we demonstrate that a chaotic RN can learn without external noise because the output fluctuation originated from its internal chaotic dynamics functions as exploration. As learning progresses, the chaoticity decreases and the network can automatically switch from exploration mode to exploitation mode. Furthermore, the network can resume exploration when presented with a new situation. In addition, we found that even when the two parameters that influence the chaoticity are varied, learning performance always improves around the edge of chaos. From these results, we think that exploration is generated from internal chaotic dynamics, and exploitation appears in the process of forming attractors on the chaotic dynamics through learning. Consequently, exploration and exploitation are well-balanced around the edge of chaos, which leads to good learning performance.},
  archive      = {J_NN},
  author       = {Toshitaka Matsuki and Katsunari Shibata},
  doi          = {10.1016/j.neunet.2020.08.002},
  journal      = {Neural Networks},
  pages        = {19-29},
  shortjournal = {Neural Netw.},
  title        = {Adaptive balancing of exploration and exploitation around the edge of chaos in internal-chaos-based learning},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A robust algorithm for explaining unreliable machine
learning survival models using the kolmogorov–smirnov bounds.
<em>NN</em>, <em>132</em>, 1–18. (<a
href="https://doi.org/10.1016/j.neunet.2020.08.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new robust algorithm based on the explanation method SurvLIME called SurvLIME-KS is proposed for explaining machine learning survival models. The algorithm is developed to ensure robustness to cases of a small amount of training data or outliers of survival data. The first idea behind SurvLIME-KS is to apply the Cox proportional hazards model to approximate the black-box survival model at the local area around a test example due to the linear relationship of covariates in the model. The second idea is to incorporate the well-known Kolmogorov–Smirnov bounds for constructing sets of predicted cumulative hazard functions. As a result, the robust maximin strategy is used, which aims to minimize the average distance between cumulative hazard functions of the explained black-box model and of the approximating Cox model , and to maximize the distance over all cumulative hazard functions in the interval produced by the Kolmogorov–Smirnov bounds. The maximin optimization problem is reduced to the quadratic program . Various numerical experiments with synthetic and real datasets demonstrate the SurvLIME-KS efficiency.},
  archive      = {J_NN},
  author       = {Maxim S. Kovalev and Lev V. Utkin},
  doi          = {10.1016/j.neunet.2020.08.007},
  journal      = {Neural Networks},
  pages        = {1-18},
  shortjournal = {Neural Netw.},
  title        = {A robust algorithm for explaining unreliable machine learning survival models using the Kolmogorov–Smirnov bounds},
  volume       = {132},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>131</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(20)30344-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(20)30344-0},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Current events. <em>NN</em>, <em>131</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(20)30343-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(20)30343-9},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {Current events},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Leveraging maximum entropy and correlation on latent factors
for learning representations. <em>NN</em>, <em>131</em>, 312–323. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many tasks involve learning representations from matrices, and Non-negative Matrix Factorization (NMF) has been widely used due to its excellent interpretability . Through factorization, sample vectors are reconstructed as additive combinations of latent factors , which are represented as non-negative distributions over the raw input features. NMF models are significantly affected by latent factors’ distribution characteristics and the correlations among them. And NMF models are faced with the challenge of learning robust latent factor. To this end, we propose to learn representations with an awareness of the semantic quality evaluated from the aspects of intra- and inter-factors. On the one hand, a Maximum Entropy-based function is devised for the intra-factor semantic quality. On the other hand, the semantic uniqueness is evaluated via inter-factor correlation, which reinforces the aim of semantic compactness. Moreover, we present a novel non-linear NMF framework. The learning algorithm is presented and the convergence is theoretically analyzed and proved. Extensive experimental results on multiple datasets demonstrate that our method can be successfully applied to representative NMF models and boost performances over state-of-the-art models.},
  archive      = {J_NN},
  author       = {Zhicheng He and Jie Liu and Kai Dang and Fuzhen Zhuang and Yalou Huang},
  doi          = {10.1016/j.neunet.2020.07.027},
  journal      = {Neural Networks},
  pages        = {312-323},
  shortjournal = {Neural Netw.},
  title        = {Leveraging maximum entropy and correlation on latent factors for learning representations},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Memristor-based LSTM network with in situ training and its
applications. <em>NN</em>, <em>131</em>, 300–311. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial neural networks (ANNs), such as the convolutional neural network (CNN) and long short-term memory (LSTM), have high complexity and contain large numbers of parameters. Memristor-based neural networks, which have the ability of in-memory and parallel computing , are therefore proposed to accelerate the operations of ANNs. In this paper, a memristor-based hardware realization of long short-term memory (LSTM) network with in situ training is presented. The designed memristor-based LSTM (MbLSTM) network is composed of memristor-based LSTM cell and memristor-based dense layer. Sigmoid and tanh (hyperbolic tangent) activation functions are approximately implemented through intentionally designing circuit parameters. A weight update scheme with row-parallel characteristic is put forward to update the conductance of memristors in crossbars. The highlights of MbLSTM include an effective hardware-based inference process and in situ training. The validity of MbLSTM is substantiated through classification tasks . The robustness of MbLSTM to conductance variations is also analyzed.},
  archive      = {J_NN},
  author       = {Xiaoyang Liu and Zhigang Zeng and Donald C. Wunsch II},
  doi          = {10.1016/j.neunet.2020.07.035},
  journal      = {Neural Networks},
  pages        = {300-311},
  shortjournal = {Neural Netw.},
  title        = {Memristor-based LSTM network with in situ training and its applications},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved recurrent neural network-based manipulator control
with remote center of motion constraints: Experimental results.
<em>NN</em>, <em>131</em>, 291–299. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an improved recurrent neural network (RNN) scheme is proposed to perform the trajectory control of redundant robot manipulators using remote center of motion (RCM) constraints. Firstly, learning by demonstration is implemented to model the surgical operation skills in the Cartesian space. After that, considering the kinematic constraints associated with the optimization control of redundant manipulators , we propose a novel RNN-based approach to facilitate accurate task tracking based on the general quadratic performance index, which includes managing the constraints on RCM joint angle, and joint velocity, simultaneously. The results of the conducted theoretical analysis confirm that the RCM constraint has been established successfully, and accordingly. The corresponding end-effector tracking errors asymptotically converge to zero. Finally, demonstration experiments are conducted in a laboratory setup environment using KUKA LWR4+ to validate the effectiveness of the proposed control strategy.},
  archive      = {J_NN},
  author       = {Hang Su and Yingbai Hu and Hamid Reza Karimi and Alois Knoll and Giancarlo Ferrigno and Elena De Momi},
  doi          = {10.1016/j.neunet.2020.07.033},
  journal      = {Neural Networks},
  pages        = {291-299},
  shortjournal = {Neural Netw.},
  title        = {Improved recurrent neural network-based manipulator control with remote center of motion constraints: Experimental results},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SVM-boosting based on markov resampling: Theory and
algorithm. <em>NN</em>, <em>131</em>, 276–290. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article we introduce the idea of Markov resampling for Boosting methods. We first prove that Boosting algorithm with general convex loss function based on uniformly ergodic Markov chain (u.e.M.c.) examples is consistent and establish its fast convergence rate. We apply Boosting algorithm based on Markov resampling to Support Vector Machine (SVM), and introduce two new resampling-based Boosting algorithms: SVM-Boosting based on Markov resampling (SVM-BM) and improved SVM-Boosting based on Markov resampling (ISVM-BM). In contrast with SVM-BM, ISVM-BM uses the support vectors to calculate the weights of base classifiers . The numerical studies based on benchmark datasets show that the proposed two resampling-based SVM Boosting algorithms for linear base classifiers have smaller misclassification rates , less total time of sampling and training compared to three classical AdaBoost algorithms: Gentle AdaBoost , Real AdaBoost, Modest AdaBoost. In addition, we compare the proposed SVM-BM algorithm with the widely used and efficient gradient Boosting algorithm-XGBoost (eXtreme Gradient Boosting), SVM-AdaBoost and present some useful discussions on the technical parameters.},
  archive      = {J_NN},
  author       = {Hongwei Jiang and Bin Zou and Chen Xu and Jie Xu and Yuan Yan Tang},
  doi          = {10.1016/j.neunet.2020.07.036},
  journal      = {Neural Networks},
  pages        = {276-290},
  shortjournal = {Neural Netw.},
  title        = {SVM-boosting based on markov resampling: Theory and algorithm},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep learning on image denoising: An overview. <em>NN</em>,
<em>131</em>, 251–275. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning techniques have received much attention in the area of image denoising . However, there are substantial differences in the various types of deep learning methods dealing with image denoising . Specifically, discriminative learning based on deep learning can ably address the issue of Gaussian noise . Optimization models based on deep learning are effective in estimating the real noise. However, there has thus far been little related research to summarize the different deep learning techniques for image denoising . In this paper, we offer a comparative study of deep techniques in image denoising . We first classify the deep convolutional neural networks (CNNs) for additive white noisy images ; the deep CNNs for real noisy images ; the deep CNNs for blind denoising and the deep CNNs for hybrid noisy images, which represents the combination of noisy, blurred and low-resolution images. Then, we analyze the motivations and principles of the different types of deep learning methods. Next, we compare the state-of-the-art methods on public denoising datasets in terms of quantitative and qualitative analyses. Finally, we point out some potential challenges and directions of future research.},
  archive      = {J_NN},
  author       = {Chunwei Tian and Lunke Fei and Wenxian Zheng and Yong Xu and Wangmeng Zuo and Chia-Wen Lin},
  doi          = {10.1016/j.neunet.2020.07.025},
  journal      = {Neural Networks},
  pages        = {251-275},
  shortjournal = {Neural Netw.},
  title        = {Deep learning on image denoising: An overview},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exponential synchronization of stochastic delayed memristive
neural networks via a novel hybrid control. <em>NN</em>, <em>131</em>,
242–250. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the exponential synchronization issue of stochastic delayed memristive neural networks (SDMNNs) via a novel hybrid control (HC), where impulsive instants are determined by the state-dependent trigger condition. The switching and quantification strategies are applied to the event-based impulsive controller to cope with the challenges induced concurrently by interval parameters, impulses, stochastic disturbance and time-varying delays. Furthermore, the control costs can be reduced and communication channels and bandwidths can be saved by using this designed controller. Then, novel Lyapunov functions and new analytical methods are constructed, which can be used to realize the exponential synchronization of SDMNNs via HC. Finally, a numerical simulation is provided to demonstrate our theoretical results.},
  archive      = {J_NN},
  author       = {Nijing Yang and Yongbin Yu and Shouming Zhong and Xiangxiang Wang and Kaibo Shi and Jingye Cai},
  doi          = {10.1016/j.neunet.2020.07.034},
  journal      = {Neural Networks},
  pages        = {242-250},
  shortjournal = {Neural Netw.},
  title        = {Exponential synchronization of stochastic delayed memristive neural networks via a novel hybrid control},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Synthesis of recurrent neural dynamics for monotone
inclusion with application to bayesian inference. <em>NN</em>,
<em>131</em>, 231–241. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a top-down approach to construct recurrent neural circuit dynamics for the mathematical problem of monotone inclusion (MoI). MoI in a general optimization framework that encompasses a wide range of contemporary problems, including Bayesian inference and Markov decision making. We show that in a recurrent neural circuit/network with Poisson neurons, each neuron’s firing curve can be understood as a proximal operator of a local objective function, while the overall circuit dynamics constitutes an operator-splitting system of ordinary differential equations whose equilibrium point corresponds to the solution of the MoI problem. Our analysis thus establishes that neural circuits are a substrate for solving a broad class of computational tasks. In this regard, we provide an explicit synthesis procedure for building neural circuits for specific MoI problems and demonstrate it for the specific case of Bayesian inference and sparse neural coding .},
  archive      = {J_NN},
  author       = {Peng Yi and ShiNung Ching},
  doi          = {10.1016/j.neunet.2020.07.037},
  journal      = {Neural Networks},
  pages        = {231-241},
  shortjournal = {Neural Netw.},
  title        = {Synthesis of recurrent neural dynamics for monotone inclusion with application to bayesian inference},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Compressing 3DCNNs based on tensor train decomposition.
<em>NN</em>, <em>131</em>, 215–230. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional convolutional neural networks (3DCNNs) have been applied in many tasks, e.g. , video and 3D point cloud recognition. However, due to the higher dimension of convolutional kernels, the space complexity of 3DCNNs is generally larger than that of traditional two-dimensional convolutional neural networks (2DCNNs). To miniaturize 3DCNNs for the deployment in confining environments such as embedded devices, neural network compression is a promising approach. In this work, we adopt the tensor train (TT) decomposition, a straightforward and simple in situ training compression method , to shrink the 3DCNN models. Through proposing tensorizing 3D convolutional kernels in TT format, we investigate how to select appropriate TT ranks for achieving higher compression ratio. We have also discussed the redundancy of 3D convolutional kernels for compression, core significance and future directions of this work, as well as the theoretical computation complexity versus practical executing time of convolution in TT. In the light of multiple contrast experiments based on VIVA challenge, UCF11, UCF101, and ModelNet40 datasets, we conclude that TT decomposition can compress 3DCNNs by around one hundred times without significant accuracy loss, which will enable its applications in extensive real world scenarios.},
  archive      = {J_NN},
  author       = {Dingheng Wang and Guangshe Zhao and Guoqi Li and Lei Deng and Yang Wu},
  doi          = {10.1016/j.neunet.2020.07.028},
  journal      = {Neural Networks},
  pages        = {215-230},
  shortjournal = {Neural Netw.},
  title        = {Compressing 3DCNNs based on tensor train decomposition},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Twin minimax probability machine for pattern classification.
<em>NN</em>, <em>131</em>, 201–214. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new distribution-free Bayes optimal classifier, called the twin minimax probability machine (TWMPM), which combines the benefits of both minimax probability machine(MPM) and twin support vector machine (TWSVM). TWMPM tries to construct two nonparallel hyperplanes such that each hyperplane separates one class samples with maximal probability, and is distant from the other class samples simultaneously. Moreover, the proposed TWMPM can control the misclassification error of samples in a worst-case setting by minimizing the upper bound on misclassification probability. An efficient algorithm for TWMPM is first proposed, which transforms TWMPM into concave fractional programming by applying multivariate Chebyshev inequality . Then the proposed TWMPM is reformulated as a pair of convex quadric programming (QP) by proper mathematical transformations. This guarantees TWMPM to have global optimal solution and be solved simply and effectively. In addition, we develop also an iterative algorithm for the proposed TWMPM. By comparing the two proposed algorithms theoretically, it is easy to know that the convex quadric programming algorithm is with lower computation burden than iterative algorithm for the TWMPM. A linear TWMPM version is first built, and then we show how to exploit mercer kernel to obtain nonlinear TWMPM version. The computation complexity for QP algorithm of TWMPM is in the same order as the traditional twin support vector machine (TWSVM). Experiments are carried out on three databases: UCI benchmark database, a practical application database and an artificial database. With low computation complexity and fewer parameters, experiments show the feasibility and effectiveness of the proposed TWMPM and its QP algorithm.},
  archive      = {J_NN},
  author       = {Liming Yang and Yakun Wen and Min Zhang and Xue Wang},
  doi          = {10.1016/j.neunet.2020.07.030},
  journal      = {Neural Networks},
  pages        = {201-214},
  shortjournal = {Neural Netw.},
  title        = {Twin minimax probability machine for pattern classification},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MetalGAN: Multi-domain label-less image synthesis using
cGANs and meta-learning. <em>NN</em>, <em>131</em>, 185–200. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image synthesis is currently one of the most addressed image processing topic in computer vision and deep learning fields of study. Researchers have tackled this problem focusing their efforts on its several challenging problems, e.g. image quality and size, domain and pose changing, architecture of the networks, and so on. Above all, producing images belonging to different domains by using a single architecture is a very relevant goal for image generation. In fact, a single multi-domain network would allow greater flexibility and robustness in the image synthesis task than other approaches. This paper proposes a novel architecture and a training algorithm , which are able to produce multi-domain outputs using a single network. A small portion of a dataset is intentionally used, and there are no hard-coded labels (or classes). This is achieved by combining a conditional Generative Adversarial Network (cGAN) for image generation and a Meta-Learning algorithm for domain switch, and we called our approach MetalGAN. The approach has proved to be appropriate for solving the multi-domain label-less problem and it is validated on facial attribute transfer, using CelebA dataset.},
  archive      = {J_NN},
  author       = {Tomaso Fontanini and Eleonora Iotti and Luca Donati and Andrea Prati},
  doi          = {10.1016/j.neunet.2020.07.031},
  journal      = {Neural Networks},
  pages        = {185-200},
  shortjournal = {Neural Netw.},
  title        = {MetalGAN: Multi-domain label-less image synthesis using cGANs and meta-learning},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hungarian layer: A novel interpretable neural layer for
paraphrase identification. <em>NN</em>, <em>131</em>, 172–184. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Paraphrase identification serves as an important topic in natural language processing while sequence alignment and matching underlie the principle of this task. Traditional alignment methods take advantage of attention mechanism . Attention mechanism, i.e. weighting technique , could pick out the most similar/dissimilar parts, but is weak in modeling the aligned unmatched parts, which are the crucial evidence to identify paraphrases. In this paper, we empower neural architecture with Hungarian algorithm to extract the aligned unmatched parts. Specifically, first, our model applies BiLSTM/BERT to encode the input sentences into hidden representations. Then, Hungarian layer leverages the hidden representations to extract the aligned unmatched parts. Last, we apply cosine similarity to metric the aligned unmatched parts for a final discrimination. Extensive experiments show that our model outperforms other baselines, substantially and significantly.},
  archive      = {J_NN},
  author       = {Han Xiao},
  doi          = {10.1016/j.neunet.2020.07.024},
  journal      = {Neural Networks},
  pages        = {172-184},
  shortjournal = {Neural Netw.},
  title        = {Hungarian layer: A novel interpretable neural layer for paraphrase identification},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Finite-time stabilization and energy consumption estimation
for delayed neural networks with bounded activation function.
<em>NN</em>, <em>131</em>, 163–171. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper concentrates on finite-time stabilization and energy consumption estimation for one type of delayed neural networks (DNNs) with bounded activation function . Under the bounded activation function condition and using the comparison theorem, a new switch controller is proposed to ensure the finite-time stability of the considered DNNs. Furthermore, the energy consumption produced in system controlling is estimated by inequality techniques. We generalize the previous results about the problem of finite-time stabilization and energy consumption estimation for neural networks . Ultimately, two numerical simulations are carried out to verify the validity of our results.},
  archive      = {J_NN},
  author       = {Chongyang Chen and Song Zhu and Min Wang and Chunyu Yang and Zhigang Zeng},
  doi          = {10.1016/j.neunet.2020.07.032},
  journal      = {Neural Networks},
  pages        = {163-171},
  shortjournal = {Neural Netw.},
  title        = {Finite-time stabilization and energy consumption estimation for delayed neural networks with bounded activation function},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Theory of deep convolutional neural networks II: Spherical
analysis. <em>NN</em>, <em>131</em>, 154–162. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning based on deep neural networks of various structures and architectures has been powerful in many practical applications, but it lacks enough theoretical verifications. In this paper, we consider a family of deep convolutional neural networks applied to approximate functions on the unit sphere S d − 1 Sd−1 of R d Rd . Our analysis presents rates of uniform approximation when the approximated function lies in the Sobolev space W ∞ r ( S d − 1 ) W∞r(Sd−1) with r &gt; 0 r&amp;gt;0 or takes an additive ridge form. Our work verifies theoretically the modelling and approximation ability of deep convolutional neural networks followed by downsampling and one fully connected layer or two. The key idea of our spherical analysis is to use the inner product form of the reproducing kernels of the spaces of spherical harmonics and then to apply convolutional factorizations of filters to realize the generated linear features.},
  archive      = {J_NN},
  author       = {Zhiying Fang and Han Feng and Shuo Huang and Ding-Xuan Zhou},
  doi          = {10.1016/j.neunet.2020.07.029},
  journal      = {Neural Networks},
  pages        = {154-162},
  shortjournal = {Neural Netw.},
  title        = {Theory of deep convolutional neural networks II: Spherical analysis},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Integral reinforcement learning based event-triggered
control with input saturation. <em>NN</em>, <em>131</em>, 144–153. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel integral reinforcement learning (IRL)-based event-triggered adaptive dynamic programming scheme is developed for input-saturated continuous-time nonlinear systems . By using the IRL technique, the learning system does not require the knowledge of the drift dynamics. Then, a single critic neural network is designed to approximate the unknown value function and its learning is not subjected to the requirement of an initial admissible control . In order to reduce computational and communication costs, the event-triggered control law is designed. The triggering threshold is given to guarantee the asymptotic stability of the control system. Two examples are employed in the simulation studies, and the results verify the effectiveness of the developed IRL-based event-triggered control method.},
  archive      = {J_NN},
  author       = {Shan Xue and Biao Luo and Derong Liu},
  doi          = {10.1016/j.neunet.2020.07.016},
  journal      = {Neural Networks},
  pages        = {144-153},
  shortjournal = {Neural Netw.},
  title        = {Integral reinforcement learning based event-triggered control with input saturation},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DANTE: Deep alternations for training neural networks.
<em>NN</em>, <em>131</em>, 127–143. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present DANTE , a novel method for training neural networks using the alternating minimization principle. DANTE provides an alternate perspective to traditional gradient-based backpropagation techniques commonly used to train deep networks. It utilizes an adaptation of quasi-convexity to cast training a neural network as a bi-quasi-convex optimization problem . We show that for neural network configurations with both differentiable (e.g. sigmoid) and non-differentiable (e.g. ReLU) activation functions , we can perform the alternations effectively in this formulation. DANTE can also be extended to networks with multiple hidden layers. In experiments on standard datasets, neural networks trained using the proposed method were found to be promising and competitive to traditional backpropagation techniques, both in terms of quality of the solution, as well as training speed.},
  archive      = {J_NN},
  author       = {Vaibhav B. Sinha and Sneha Kudugunta and Adepu Ravi Sankar and Surya Teja Chavali and Vineeth N. Balasubramanian},
  doi          = {10.1016/j.neunet.2020.07.026},
  journal      = {Neural Networks},
  pages        = {127-143},
  shortjournal = {Neural Netw.},
  title        = {DANTE: Deep alternations for training neural networks},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bifurcations in a fractional-order neural network with
multiple leakage delays. <em>NN</em>, <em>131</em>, 115–126. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper expatiates the stability and bifurcation for a fractional-order neural network (FONN) with double leakage delays. Firstly, the characteristic equation of the developed FONN is circumspectly researched by employing inequable delays as bifurcation parameters . Simultaneously the bifurcation criteria are correspondingly extrapolated. Then, unequal delays-spurred-bifurcation diagrams are primarily delineated to confirm the precision and correctness for the values of bifurcation points . Furthermore, it lavishly illustrates from the evidence that the stability performance of the proposed FONN can be demolished with the presence of leakage delays in accordance with comparative studies. Eventually, two numerical examples are exploited to underpin the feasibility of the developed theory. The results derived in this paper have perfected the retrievable outcomes on bifurcations of FONNs embodying unique leakage delay, which can nicely serve a benchmark deliberation and provide a comparatively credible guidance for the influence of multiple leakage delays on bifurcations of FONNs.},
  archive      = {J_NN},
  author       = {Chengdai Huang and Heng Liu and Xiangyun Shi and Xiaoping Chen and Min Xiao and Zhengxin Wang and Jinde Cao},
  doi          = {10.1016/j.neunet.2020.07.015},
  journal      = {Neural Networks},
  pages        = {115-126},
  shortjournal = {Neural Netw.},
  title        = {Bifurcations in a fractional-order neural network with multiple leakage delays},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved object recognition using neural networks trained to
mimic the brain’s statistical properties. <em>NN</em>, <em>131</em>,
103–114. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current state-of-the-art object recognition algorithms , deep convolutional neural networks (DCNNs), are inspired by the architecture of the mammalian visual system, and are capable of human-level performance on many tasks. As they are trained for object recognition tasks , it has been shown that DCNNs develop hidden representations that resemble those observed in the mammalian visual system (Razavi and Kriegeskorte, 2014; Yamins and Dicarlo, 2016; Gu and van Gerven, 2015; Mcclure and Kriegeskorte, 2016). Moreover, DCNNs trained on object recognition tasks are currently among the best models we have of the mammalian visual system. This led us to hypothesize that teaching DCNNs to achieve even more brain-like representations could improve their performance. To test this, we trained DCNNs on a composite task , wherein networks were trained to: (a) classify images of objects; while (b) having intermediate representations that resemble those observed in neural recordings from monkey visual cortex . Compared with DCNNs trained purely for object categorization, DCNNs trained on the composite task had better object recognition performance and are more robust to label corruption. Interestingly, we found that neural data was not required for this process, but randomized data with the same statistical properties as neural data also boosted performance. While the performance gains we observed when training on the composite task vs the “pure” object recognition task were modest, they were remarkably robust. Notably, we observed these performance gains across all network variations we studied, including: smaller (CORNet-Z) vs larger (VGG-16) architectures; variations in optimizers (Adam vs gradient descent); variations in activation function (ReLU vs ELU); and variations in network initialization. Our results demonstrate the potential utility of a new approach to training object recognition networks , using strategies in which the brain – or at least the statistical properties of its activation patterns – serves as a teacher signal for training DCNNs.},
  archive      = {J_NN},
  author       = {Callie Federer and Haoyan Xu and Alona Fyshe and Joel Zylberberg},
  doi          = {10.1016/j.neunet.2020.07.013},
  journal      = {Neural Networks},
  pages        = {103-114},
  shortjournal = {Neural Netw.},
  title        = {Improved object recognition using neural networks trained to mimic the brain’s statistical properties},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Relation-guided representation learning. <em>NN</em>,
<em>131</em>, 93–102. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep auto-encoders (DAEs) have achieved great success in learning data representations via the powerful representability of neural networks . But most DAEs only focus on the most dominant structures which are able to reconstruct the data from a latent space and neglect rich latent structural information. In this work, we propose a new representation learning method that explicitly models and leverages sample relations, which in turn is used as supervision to guide the representation learning . Different from previous work, our framework well preserves the relations between samples. Since the prediction of pairwise relations themselves is a fundamental problem, our model adaptively learns them from data. This provides much flexibility to encode real data manifold. The important role of relation and representation learning is evaluated on the clustering task . Extensive experiments on benchmark data sets demonstrate the superiority of our approach. By seeking to embed samples into subspace, we further show that our method can address the large-scale and out-of-sample problem. Our source code is publicly available at: https://github.com/nbShawnLu/RGRL .},
  archive      = {J_NN},
  author       = {Zhao Kang and Xiao Lu and Jian Liang and Kun Bai and Zenglin Xu},
  doi          = {10.1016/j.neunet.2020.07.014},
  journal      = {Neural Networks},
  pages        = {93-102},
  shortjournal = {Neural Netw.},
  title        = {Relation-guided representation learning},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Global μ-synchronization of impulsive pantograph neural
networks. <em>NN</em>, <em>131</em>, 78–92. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the problem of global μ μ -synchronization of impulsive pantograph neural networks . In this paper, new concept of ν ν -asymptotic periodic impulsive interval T a s y ν Tasyν is proposed for pantograph networks. By employing the Lyapunov method combined with the mathematical analysis approach for impulsive systems, some useful criteria are derived to guarantee the global μ μ -synchronization of coupled pantograph neural networks when the asymptotic logarithmic periodic impulsive interval T a s y ln Tasyln&amp;lt;∞ and T a s y ln = ∞ Tasyln=∞ , respectively. Especially when T a s y ln = ∞ Tasyln=∞ , as long as the networks are unstable, impulsive control cannot achieve synchronization regardless of the size of the impulse gain. Numerical simulations are exploited to illustrate our theoretical results.},
  archive      = {J_NN},
  author       = {Xuechen Li and Nan Wang and Jungang Lou and Jianquan Lu},
  doi          = {10.1016/j.neunet.2020.07.004},
  journal      = {Neural Networks},
  pages        = {78-92},
  shortjournal = {Neural Netw.},
  title        = {Global μ-synchronization of impulsive pantograph neural networks},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ASSAF: Advanced and slim StegAnalysis detection framework
for JPEG images based on deep convolutional denoising autoencoder and
siamese networks. <em>NN</em>, <em>131</em>, 64–77. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Steganography is the art of embedding a confidential message within a host message. Modern steganography is focused on widely used multimedia file formats , such as images, video files, and Internet protocols . Recently, cyber attackers have begun to include steganography (for communication purposes) in their arsenal of tools for evading detection. Steganalysis is the counter-steganography domain which aims at detecting the existence of steganography within a host file. The presence of steganography in files raises suspicion regarding the file itself, as well as its origin and receiver, and might be an indication of a sophisticated attack. The JPEG file format is one of the most popular image file formats and thus is an attractive and commonly used carrier for steganography embedding. State-of-the-art JPEG steganalysis methods, which are mainly based on neural networks , are limited in their ability to detect sophisticated steganography use cases. In this paper, we propose ASSAF, a novel deep neural network architecture composed of a convolutional denoising autoencoder and a Siamese neural network, specially designed to detect steganography in JPEG images. We focus on detecting the J-UNIWARD method, which is one of the most sophisticated adaptive steganography methods used today. We evaluated our novel architecture using the BOSSBase dataset, which contains 10,000 JPEG images, in eight different use cases which combine different JPEG’s quality factors and embedding rates (bpnzAC). Our results show that ASSAF can detect stenography with high accuracy rates, outperforming, in all eight use cases, the state-of-the-art steganalysis methods by 6\% to 40\%.},
  archive      = {J_NN},
  author       = {Assaf Cohen and Aviad Cohen and Nir Nissim},
  doi          = {10.1016/j.neunet.2020.07.022},
  journal      = {Neural Networks},
  pages        = {64-77},
  shortjournal = {Neural Netw.},
  title        = {ASSAF: Advanced and slim StegAnalysis detection framework for JPEG images based on deep convolutional denoising autoencoder and siamese networks},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised multi-domain multimodal image-to-image
translation with explicit domain-constrained disentanglement.
<em>NN</em>, <em>131</em>, 50–63. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-to-image translation has drawn great attention during the past few years. It aims to translate an image in one domain to a target image in another domain. However, three big challenges remain in image-to-image translation: (1) the lack of large amounts of aligned training pairs for various tasks; (2) the ambiguity of multiple possible outputs from a single input image; and (3) the lack of simultaneous training for multi-domain translation with a single network. Therefore in this paper, we propose a unified framework for learning to generate diverse outputs using unpaired training data and allow for simultaneous multi-domain translation via a single model. Moreover, we also observed from experiments that the implicit disentanglement of content and style could lead to undesirable results. Thus we investigate how to extract domain-level signal as explicit supervision so as to achieve better image-to-image translation. Extensive experiments show that the proposed method outperforms or is comparable with the state-of-the-art methods for various applications.},
  archive      = {J_NN},
  author       = {Weihao Xia and Yujiu Yang and Jing-Hao Xue},
  doi          = {10.1016/j.neunet.2020.07.023},
  journal      = {Neural Networks},
  pages        = {50-63},
  shortjournal = {Neural Netw.},
  title        = {Unsupervised multi-domain multimodal image-to-image translation with explicit domain-constrained disentanglement},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sparse coding with a somato-dendritic rule. <em>NN</em>,
<em>131</em>, 37–49. (<a
href="https://doi.org/10.1016/j.neunet.2020.06.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cortical neurons are silent most of the time: sparse activity enables low-energy computation in the brain, and promises to do the same in neuromorphic hardware. Beyond power efficiency, sparse codes have favourable properties for associative learning , as they can store more information than local codes but are easier to read out than dense codes. Auto-encoders with a sparse constraint can learn sparse codes, and so can single-layer networks that combine recurrent inhibition with unsupervised Hebbian learning . But the latter usually require fast homeostatic plasticity, which could lead to catastrophic forgetting in embodied agents that learn continuously. Here we set out to explore whether plasticity at recurrent inhibitory synapses could take up that role instead, regulating both the population sparseness and the firing rates of individual neurons . We put the idea to the test in a network that employs compartmentalised inputs to solve the task: rate-based dendritic compartments integrate the feedforward input, while spiking integrate-and-fire somas compete through recurrent inhibition. A somato-dendritic learning rule allows somatic inhibition to modulate nonlinear Hebbian learning in the dendrites. Trained on MNIST digits and natural images, the network discovers independent components that form a sparse encoding of the input and support linear decoding. These findings confirm that intrinsic homeostatic plasticity is not strictly required for regulating sparseness: inhibitory synaptic plasticity can have the same effect. Our work illustrates the usefulness of compartmentalised inputs, and makes the case for moving beyond point neuron models in artificial spiking neural networks .},
  archive      = {J_NN},
  author       = {Damien Drix and Verena V. Hafner and Michael Schmuker},
  doi          = {10.1016/j.neunet.2020.06.007},
  journal      = {Neural Networks},
  pages        = {37-49},
  shortjournal = {Neural Netw.},
  title        = {Sparse coding with a somato-dendritic rule},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two-hidden-layer feed-forward networks are universal
approximators: A constructive approach. <em>NN</em>, <em>131</em>,
29–36. (<a href="https://doi.org/10.1016/j.neunet.2020.07.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well-known that artificial neural networks are universal approximators . The classical existence result proves that, given a continuous function on a compact set embedded in an n n -dimensional space, there exists a one-hidden-layer feed-forward network that approximates the function. In this paper, a constructive approach to this problem is given for the case of a continuous function on triangulated spaces. Once a triangulation of the space is given, a two-hidden-layer feed-forward network with a concrete set of weights is computed. The level of the approximation depends on the refinement of the triangulation.},
  archive      = {J_NN},
  author       = {Eduardo Paluzo-Hidalgo and Rocio Gonzalez-Diaz and Miguel A. Gutiérrez-Naranjo},
  doi          = {10.1016/j.neunet.2020.07.021},
  journal      = {Neural Networks},
  pages        = {29-36},
  shortjournal = {Neural Netw.},
  title        = {Two-hidden-layer feed-forward networks are universal approximators: A constructive approach},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast deep stacked networks based on extreme learning machine
applied to regression problems. <em>NN</em>, <em>131</em>, 14–28. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning techniques are commonly used to process large amounts of data, and good results are obtained in many applications. Those methods, however, can lead to long training times. An alternative to simultaneously tune all parameters of a large network is to stack smaller modules, improving the model efficiency. However, methods such as Deep Stacked Network (DSN) have some problems that increase its training time and memory usage. To deal with these problems, Fast DSN (FDSN) was proposed, where the modules are trained using an Extreme Learning Machine (ELM) variant. Nonetheless, to speed-up the FDSN training, the ELM random feature mapping is shared among the modules, which can impact the network performance if the weights are not properly chosen. In this paper, we focus on the weight initialization of FDSN in order to improve its performance. We also propose FKDSN, a kernel-based variant of FDSN, besides discussing the theoretical complexity of the methods. We evaluate three different initialization approaches on ELM-trained neural networks over 50 public real-world regression datasets. Our experiments show that FDSN when combined with a more complex initialization method achieves similar results to ELM algorithms applied to large SLFNs, besides having a shorter training time and memory usage, implying that it can be suitable to be used on systems with restrict resources, such as Internet of Things devices. FKDSN also obtained similar results and training time to the large SLFNs, requiring less memory.},
  archive      = {J_NN},
  author       = {Bruno Légora Souza da Silva and Fernando Kentaro Inaba and Evandro Ottoni Teatini Salles and Patrick Marques Ciarelli},
  doi          = {10.1016/j.neunet.2020.07.018},
  journal      = {Neural Networks},
  pages        = {14-28},
  shortjournal = {Neural Netw.},
  title        = {Fast deep stacked networks based on extreme learning machine applied to regression problems},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Intermittent boundary stabilization of stochastic
reaction–diffusion cohen–grossberg neural networks. <em>NN</em>,
<em>131</em>, 1–13. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cohen–Grossberg neural networks (CGNNs) play an important role in many applications and the stabilization of this system has been well studied. This study considers the exponential stabilization for stochastic reaction–diffusion Cohen–Grossberg neural networks (SRDCGNNs) by means of an aperiodically intermittent boundary control. Both SRDCGNNs without and with time-delays are discussed. By employing the spatial integral functional method and Poincare’s inequality, criteria are derived to ensure the controlled systems achieve mean square exponential stabilization. Based on these criteria, the effects of diffusion item, control gains, the minimum control proportion and time-delays on exponential stability are analyzed. Examples are given to illustrate the effectiveness of the obtained theoretical results.},
  archive      = {J_NN},
  author       = {Xiao-Zhen Liu and Kai-Ning Wu and Weihai Zhang},
  doi          = {10.1016/j.neunet.2020.07.019},
  journal      = {Neural Networks},
  pages        = {1-13},
  shortjournal = {Neural Netw.},
  title        = {Intermittent boundary stabilization of stochastic reaction–diffusion Cohen–Grossberg neural networks},
  volume       = {131},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020c). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>130</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(20)30314-2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(20)30314-2},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020c). Current events. <em>NN</em>, <em>130</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(20)30313-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(20)30313-0},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {Current events},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Investigating object compositionality in generative
adversarial networks. <em>NN</em>, <em>130</em>, 309–325. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep generative models seek to recover the process with which the observed data was generated. They may be used to synthesize new samples or to subsequently extract representations. Successful approaches in the domain of images are driven by several core inductive biases. However, a bias to account for the compositional way in which humans structure a visual scene in terms of objects has frequently been overlooked. In this work, we investigate object compositionality as an inductive bias for Generative Adversarial Networks (GANs). We present a minimal modification of a standard generator to incorporate this inductive bias and find that it reliably learns to generate images as compositions of objects. Using this general design as a backbone, we then propose two useful extensions to incorporate dependencies among objects and background. We extensively evaluate our approach on several multi-object image datasets and highlight the merits of incorporating structure for representation learning purposes. In particular, we find that our structured GANs are better at generating multi-object images that are more faithful to the reference distribution. More so, we demonstrate how, by leveraging the structure of the learned generative process , one can ‘invert’ the learned generative model to perform unsupervised instance segmentation . On the challenging CLEVR dataset, it is shown how our approach is able to improve over other recent purely unsupervised object-centric approaches to image generation .},
  archive      = {J_NN},
  author       = {Sjoerd van Steenkiste and Karol Kurach and Jürgen Schmidhuber and Sylvain Gelly},
  doi          = {10.1016/j.neunet.2020.07.007},
  journal      = {Neural Networks},
  pages        = {309-325},
  shortjournal = {Neural Netw.},
  title        = {Investigating object compositionality in generative adversarial networks},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discretely-constrained deep network for weakly supervised
segmentation. <em>NN</em>, <em>130</em>, 297–308. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An efficient strategy for weakly-supervised segmentation is to impose constraints or regularization priors on target regions. Recent efforts have focused on incorporating such constraints in the training of convolutional neural networks (CNN), however this has so far been done within a continuous optimization framework. Yet, various segmentation constraints and regularization priors can be modeled and optimized more efficiently in a discrete formulation. This paper proposes a method, based on the alternating direction method of multipliers (ADMM) algorithm, to train a CNN with discrete constraints and regularization priors. This method is applied to the segmentation of medical images with weak annotations, where both size constraints and boundary length regularization are enforced. Experiments on two benchmark datasets for medical image segmentation show our method to provide significant improvements compared to existing approaches in terms of segmentation accuracy , constraint satisfaction and convergence speed.},
  archive      = {J_NN},
  author       = {Jizong Peng and Hoel Kervadec and Jose Dolz and Ismail Ben Ayed and Marco Pedersoli and Christian Desrosiers},
  doi          = {10.1016/j.neunet.2020.07.011},
  journal      = {Neural Networks},
  pages        = {297-308},
  shortjournal = {Neural Netw.},
  title        = {Discretely-constrained deep network for weakly supervised segmentation},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Landslide displacement interval prediction using lower upper
bound estimation method with pre-trained random vector functional link
network initialization. <em>NN</em>, <em>130</em>, 286–296. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interval prediction is an efficient approach to quantifying the uncertainties associated with landslide evolution. In this paper, a novel method, termed lower upper bound estimation (LUBE), of constructing prediction intervals (PIs) based on neural networks (NNs) is applied and extended to landslide displacement prediction. A random vector functional link network (RVFLN) is adopted as the NN used in the improved LUBE. A hybrid evolutionary algorithm , termed PSOGSA, that combines particle swarm optimization (PSO) and gravitational search algorithm (GSA) is utilized to train LUBE. The loss function of LUBE is redesigned by considering the quality of PI centre, which allows for a more comprehensive evaluation of PIs. The population initialization in the training process of LUBE is implemented by transferring the weights of a series of pre-trained RVFLNs. The performance of the improved LUBE method is validated by considering a comprehensive set of cases using seven benchmark datasets. In addition, a hybrid method that integrates ensemble empirical mode decomposition (EEMD) with the improved LUBE is proposed for the special case of landslide displacement prediction. Six real-world reservoir-induced landslides are considered to validate the capability and merit of the proposed hybrid method .},
  archive      = {J_NN},
  author       = {Cheng Lian and Zhigang Zeng and Xiaoping Wang and Wei Yao and Yixin Su and Huiming Tang},
  doi          = {10.1016/j.neunet.2020.07.020},
  journal      = {Neural Networks},
  pages        = {286-296},
  shortjournal = {Neural Netw.},
  title        = {Landslide displacement interval prediction using lower upper bound estimation method with pre-trained random vector functional link network initialization},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A pruning feedforward small-world neural network based on
katz centrality for nonlinear system modeling. <em>NN</em>,
<em>130</em>, 269–285. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approaching to the biological neural network , small-world neural networks have been demonstrated to improve the generalization performance of artificial neural networks . However, the architecture of small-world neural networks is typically large and predefined. This may cause the problems of overfitting and time consuming, and cannot obtain an optimal network structure automatically for a given problem. To solve the above problems, this paper proposes a pruning feedforward small-world neural network (PFSWNN), and applies it to nonlinear system modeling. Firstly, a feedforward small-world neural network (FSWNN) is constructed according to the rewiring rule of Watts–Strogatz. Secondly, the importance of each hidden neuron is evaluated based on its Katz centrality. If the Katz centrality of a hidden neuron is below the predefined threshold, this neuron is considered to be an unimportant node and then merged with its most correlated neuron in the same hidden layer. The connection weights are trained using the gradient-based algorithm, and the convergence of the proposed PFSWNN is theoretically analyzed in this paper. Finally, the PFSWNN model is tested on some problems for nonlinear system modeling, including the approximation for a rapidly changing function, CATS missing time-series prediction, four benchmark problems of UCI public datasets and a practical problem for wastewater treatment process . Experimental results demonstrate that PFSWNN exhibits superior generalization performance by small-world property as well as the pruning algorithm, and the training time of PFSWNN is shortened owning to a compact structure.},
  archive      = {J_NN},
  author       = {Wenjing Li and Minghui Chu and Junfei Qiao},
  doi          = {10.1016/j.neunet.2020.07.017},
  journal      = {Neural Networks},
  pages        = {269-285},
  shortjournal = {Neural Netw.},
  title        = {A pruning feedforward small-world neural network based on katz centrality for nonlinear system modeling},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-organizing subspace clustering for high-dimensional and
multi-view data. <em>NN</em>, <em>130</em>, 253–268. (<a
href="https://doi.org/10.1016/j.neunet.2020.06.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A surge in the availability of data from multiple sources and modalities is correlated with advances in how to obtain, compress, store, transfer, and process large amounts of complex high-dimensional data. The clustering challenge increases with the growth of data dimensionality which decreases the discriminate power of the distance metrics. Subspace clustering aims to group data drawn from a union of subspaces. In such a way, there is a large number of state-of-the-art approaches and we divide them into families regarding the method used in the clustering. We introduce a soft subspace clustering algorithm , a Self-organizing Map (SOM) with a time-varying structure, to cluster data without any prior knowledge of the number of categories or of the neural network topology, both determined during the training process. The model also assigns proper relevancies (weights) to different dimensions, capturing from the learning process the influence of each dimension on uncovering clusters. We employ a number of real-world datasets to validate the model. This algorithm presents a competitive performance in a diverse range of contexts among them data mining , gene expression, multi-view, computer vision and text clustering problems which include high-dimensional data. Extensive experiments suggest that our method very often outperforms the state-of-the-art approaches in all types of problems considered.},
  archive      = {J_NN},
  author       = {Aluizio F.R. Araújo and Victor O. Antonino and Karina L. Ponce-Guevara},
  doi          = {10.1016/j.neunet.2020.06.022},
  journal      = {Neural Networks},
  pages        = {253-268},
  shortjournal = {Neural Netw.},
  title        = {Self-organizing subspace clustering for high-dimensional and multi-view data},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feature fusion via deep random forest for facial age
estimation. <em>NN</em>, <em>130</em>, 238–252. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last few years, human age estimation from face images attracted the attention of many researchers in computer vision and machine learning fields. This is due to its numerous applications. In this paper, we propose a new architecture for age estimation based on facial images . It is mainly based on a cascade of classification trees ensembles, which are known recently as a Deep Random Forest . Our architecture is composed of two types of DRF. The first type extends and enhances the feature representation of a given facial descriptor. The second type operates on the fused form of all enhanced representations in order to provide a prediction for the age while taking into account the fuzziness property of the human age. While the proposed methodology is able to work with all kinds of image features , the face descriptors adopted in this work used off-the-shelf deep features allowing to retain both the rich deep features and the powerful enhancement and decision provided by the proposed architecture. Experiments conducted on six public databases prove the superiority of the proposed architecture over other state-of-the-art methods.},
  archive      = {J_NN},
  author       = {O. Guehairia and A. Ouamane and F. Dornaika and A. Taleb-Ahmed},
  doi          = {10.1016/j.neunet.2020.07.006},
  journal      = {Neural Networks},
  pages        = {238-252},
  shortjournal = {Neural Netw.},
  title        = {Feature fusion via deep random forest for facial age estimation},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Asynchronous dissipative filtering for nonhomogeneous markov
switching neural networks with variable packet dropouts. <em>NN</em>,
<em>130</em>, 229–237. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work focuses on the problem of asynchronous filtering for nonhomogeneous Markov switching neural networks with variable packet dropouts (VPDs). The discrete-time nonhomogeneous Markov process is adopted to depict the modes switching of target plant, where time-varying transition probabilities are revealed by utilizing a polytope technology. By means of the Bernoulli distributed sequence, the randomly occurring packet dropouts are presented, where VPD rates are mode-dependent and remain variable. Unlike the existing results, the hidden Markov model scheme is formulated to describe the asynchronization between nonhomogeneous neural networks and filter, and resilient filters are presented, which makes the designed filters more general. Eventually, a simulation example is established to verify the effectiveness of the developed filter scheme.},
  archive      = {J_NN},
  author       = {Xia Zhou and Jun Cheng and Jinde Cao and Minvydas Ragulskis},
  doi          = {10.1016/j.neunet.2020.07.012},
  journal      = {Neural Networks},
  pages        = {229-237},
  shortjournal = {Neural Netw.},
  title        = {Asynchronous dissipative filtering for nonhomogeneous markov switching neural networks with variable packet dropouts},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep clustering with a dynamic autoencoder: From
reconstruction towards centroids construction. <em>NN</em>,
<em>130</em>, 206–228. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In unsupervised learning , there is no apparent straightforward cost function that can capture the significant factors of variations and similarities. Since natural systems have smooth dynamics, an opportunity is lost if an unsupervised objective function remains static. The absence of concrete supervision suggests that smooth dynamics should be integrated during the training process. Compared to classical static cost functions, dynamic objective functions allow to better make use of the gradual and uncertain knowledge acquired through pseudo-supervision. In this paper, we propose Dynamic Autoencoder (DynAE), a novel model for deep clustering that addresses a clustering–reconstruction trade-off, by gradually and smoothly eliminating the reconstruction objective function in favor of a construction one. Experimental evaluations on benchmark datasets show that our approach achieves state-of-the-art results compared to the most relevant deep clustering methods .},
  archive      = {J_NN},
  author       = {Nairouz Mrabah and Naimul Mefraz Khan and Riadh Ksantini and Zied Lachiri},
  doi          = {10.1016/j.neunet.2020.07.005},
  journal      = {Neural Networks},
  pages        = {206-228},
  shortjournal = {Neural Netw.},
  title        = {Deep clustering with a dynamic autoencoder: From reconstruction towards centroids construction},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). K-hop graph neural networks. <em>NN</em>, <em>130</em>,
195–205. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have emerged recently as a powerful architecture for learning node and graph representations . Standard GNNs have the same expressive power as the Weisfeiler–Lehman test of graph isomorphism in terms of distinguishing non-isomorphic graphs. However, it was recently shown that this test cannot identify fundamental graph properties such as connectivity and triangle freeness. We show that GNNs also suffer from the same limitation. To address this limitation, we propose a more expressive architecture, k k -hop GNNs, which updates a node’s representation by aggregating information not only from its direct neighbors, but from its k k -hop neighborhood. We show that the proposed architecture can identify fundamental graph properties. We evaluate the proposed architecture on standard node classification and graph classification datasets. Our experimental evaluation confirms our theoretical findings since the proposed model achieves performance better or comparable to standard GNNs and to state-of-the-art algorithms.},
  archive      = {J_NN},
  author       = {Giannis Nikolentzos and George Dasoulas and Michalis Vazirgiannis},
  doi          = {10.1016/j.neunet.2020.07.008},
  journal      = {Neural Networks},
  pages        = {195-205},
  shortjournal = {Neural Netw.},
  title        = {K-hop graph neural networks},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards explainable deep neural networks (xDNN).
<em>NN</em>, <em>130</em>, 185–194. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an elegant solution that is directly addressing the bottlenecks of the traditional deep learning approaches and offers an explainable internal architecture that can outperform the existing methods, requires very little computational resources (no need for GPUs) and short training times (in the order of seconds). The proposed approach, xDNN is using prototypes. Prototypes are actual training data samples (images), which are local peaks of the empirical data distribution called typicality as well as of the data density. This generative model is identified in a closed form and equates to the pdf but is derived automatically and entirely from the training data with no user- or problem-specific thresholds, parameters or intervention. The proposed xDNN offers a new deep learning architecture that combines reasoning and learning in a synergy. It is non-iterative and non-parametric, which explains its efficiency in terms of time and computational resources. From the user perspective, the proposed approach is clearly understandable to human users. We tested it on challenging problems as the classification of different lighting conditions for driving scenes (iROADS), object detection (Caltech-256, and Caltech-101), and SARS-CoV-2 identification via computed tomography scan (COVID CT-scans dataset). xDNN outperforms the other methods including deep learning in terms of accuracy, time to train and offers an explainable classifier.},
  archive      = {J_NN},
  author       = {Plamen Angelov and Eduardo Soares},
  doi          = {10.1016/j.neunet.2020.07.010},
  journal      = {Neural Networks},
  pages        = {185-194},
  shortjournal = {Neural Netw.},
  title        = {Towards explainable deep neural networks (xDNN)},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybrid neural network with cost-sensitive support vector
machine for class-imbalanced multimodal data. <em>NN</em>, <em>130</em>,
176–184. (<a
href="https://doi.org/10.1016/j.neunet.2020.06.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although deep learning exhibits advantages in various applications involving multimodal data, it cannot effectively solve the class-imbalance problem. Herein, we propose a hybrid neural network with a cost-sensitive support vector machine (hybrid NN-CSSVM) for class-imbalanced multimodal data. We used a fused multiple-network structure obtained by extracting the features of different modality data, and used cost-sensitive support vector machines (SVMs) as a classifier. To alleviate the insufficiency of learning from minority-class data, our proposed cost-sensitive SVM loss function reflects different weights of misclassification errors from both majority and minority classes, by controlling cost parameters. Additionally, we present a theoretical setting of the cost parameters in our model. The proposed model is validated on real datasets that range from low to high imbalance ratios . By exploiting the complementary advantages of two architectures, the hybrid NN-CSSVM performs excellently, even with data having a minor-class proportion of only 2\%.},
  archive      = {J_NN},
  author       = {Kyung Hye Kim and So Young Sohn},
  doi          = {10.1016/j.neunet.2020.06.026},
  journal      = {Neural Networks},
  pages        = {176-184},
  shortjournal = {Neural Netw.},
  title        = {Hybrid neural network with cost-sensitive support vector machine for class-imbalanced multimodal data},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fixed-time synchronization of stochastic memristor-based
neural networks with adaptive control. <em>NN</em>, <em>130</em>,
165–175. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we consider the fixed-time synchronization problem for stochastic memristor-based neural networks (MNNs) via two different controllers. First, a new stochastic differential equation is established using differential inclusions and set-valued maps. Next, two kinds of control protocols are designed, including a nonlinear delayed state feedback control scheme and a novel adaptive control strategy, by which fixed-time synchronization of MNNs can be achieved. Then based on stochastic analysis techniques and a Lyapunov function , some sufficient criteria are obtained to ensure that stochastic MNNs achieve stochastic fixed-time synchronization in probability . In addition, the upper bound of the settling time is estimated. Finally, simulation results are provided to demonstrate the validity of the proposed schemes.},
  archive      = {J_NN},
  author       = {Hongwei Ren and Zhiping Peng and Yu Gu},
  doi          = {10.1016/j.neunet.2020.07.002},
  journal      = {Neural Networks},
  pages        = {165-175},
  shortjournal = {Neural Netw.},
  title        = {Fixed-time synchronization of stochastic memristor-based neural networks with adaptive control},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quantum neural networks model based on swap test and phase
estimation. <em>NN</em>, <em>130</em>, 152–164. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a neural networks model for quantum computer is proposed. The core of this model is quantum neuron. Firstly, the inner product of the input qubits and the weight qubits is mapped to the phase of the control qubits in the neuron by the swap test technology, and then these phases are obtained by the phase estimation method, which are further used as the phase of the output qubit in the neuron. In this way, the mapping of input qubits to output qubit in quantum neuron is completed. The quantum neurons mentioned above can be used to construct quantum neural networks . In this paper, the quantum circuit for each operation step are given. The simulation results on the classic computer verify the effectiveness of the proposed model.},
  archive      = {J_NN},
  author       = {Panchi Li and Bing Wang},
  doi          = {10.1016/j.neunet.2020.07.003},
  journal      = {Neural Networks},
  pages        = {152-164},
  shortjournal = {Neural Netw.},
  title        = {Quantum neural networks model based on swap test and phase estimation},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Delay-distribution-dependent state estimation for neural
networks under stochastic communication protocol with uncertain
transition probabilities. <em>NN</em>, <em>130</em>, 143–151. (<a
href="https://doi.org/10.1016/j.neunet.2020.06.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the protocol-based remote state estimation problem is considered for a kind of delayed artificial neural networks . The random time-varying delays fall into certain intervals with known probability distributions. For the sake of reducing the data collisions in communication channel from the sensors to the estimator, the stochastic communication protocol (SCP) is employed to decide which sensor is allowed to transmit its data to the remote estimator through the channel at each fixed instant. The scheduling principle of the SCP is governed by a Markov chain whose transition probability is allowed to be uncertain so as to reflect the possible imprecision when implementing the SCP. Through a combination of Lyapunov–Krasovskii functional method and the stochastic analysis technique, a sufficient criterion is obtained for the existence of the desired remote state estimator ensuring that the corresponding augmented estimation error dynamics is asymptotically stable with a prescribed H ∞ H∞ performance index. Furthermore, the estimator parameter is acquired by solving a convex optimization problem . Finally, the validity of the established theoretical results is demonstrated via a numerical simulation example.},
  archive      = {J_NN},
  author       = {Jiahui Li and Zidong Wang and Hongli Dong and Weiyin Fei},
  doi          = {10.1016/j.neunet.2020.06.023},
  journal      = {Neural Networks},
  pages        = {143-151},
  shortjournal = {Neural Netw.},
  title        = {Delay-distribution-dependent state estimation for neural networks under stochastic communication protocol with uncertain transition probabilities},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybrid multi-mode machine learning-based fault diagnosis
strategies with application to aircraft gas turbine engines.
<em>NN</em>, <em>130</em>, 126–142. (<a
href="https://doi.org/10.1016/j.neunet.2020.07.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, a novel data-driven fault diagnostic framework is developed by using hybrid multi-mode machine learning strategies to monitor system health status. The coexistence of multi-mode and concurrent faults and their adverse coupling effects pose serious limitations for developing reliable diagnostic methodologies. A novel framework is proposed by exploiting inherent embedded health information contained in the I/O sensor data. The proposed hybrid strategies consist of optimal integration of recurrent neural network-based feature generation and self-organizing map diagnostic modules. To construct reliable fault diagnostic modules, a systematic clustering and modeling methodology is developed that has two primary advantages: (i) it does not require any a priori knowledge of data set characteristics or system mathematical model , and (ii) it does address and resolve the key limitations and challenges in conventional self-organizing map approaches. The effectiveness of our proposed framework is validated by utilizing sensor data including healthy and various degradation modes in application to compressor and turbine of an aircraft gas turbine engine . Comparisons with other machine learning-based methods in the literature are provided to demonstrate the performance and superiority of our proposed framework in fault diagnostic accuracy , false alarm rates , and in dealing with multi-mode and concurrent fault scenarios.},
  archive      = {J_NN},
  author       = {Yanyan Shen and Khashayar Khorasani},
  doi          = {10.1016/j.neunet.2020.07.001},
  journal      = {Neural Networks},
  pages        = {126-142},
  shortjournal = {Neural Netw.},
  title        = {Hybrid multi-mode machine learning-based fault diagnosis strategies with application to aircraft gas turbine engines},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A learning approach with incomplete pixel-level labels for
deep neural networks. <em>NN</em>, <em>130</em>, 111–125. (<a
href="https://doi.org/10.1016/j.neunet.2020.06.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning with incomplete labels in Neural Networks has been actively investigated these last years. Among different kinds of incomplete labels, we investigate incomplete pixel-level labels which are tackled in many concrete problems. One of the challenges for incomplete pixel-level labels is the missing information at local-level. Most of the current researches with incomplete labels in Neural Network focus on the incompleteness of global labels, only a few works focus on the incompleteness of local labels. To deal with the local incompleteness, we propose a learning approach which uses two dynamic weighted maps in parallel: one for object pixels and another one for background pixels . The two maps are integrated into the loss function of the target Neural Networks , to optimize the model by the present labels and to minimize the damage of the missing labels. We validate our approach on the speech balloon extraction problem in comic book images. Our approach uses the output of a balloon extraction algorithm as incomplete labels. The results are comparable with the state of the art supervised approach with manual labels. The results are very promising because our method does not require any manual labels. In addition, we apply our method to the medical image segmentation task to confirm the generalization of our approach.},
  archive      = {J_NN},
  author       = {Nhu-Van Nguyen and Christophe Rigaud and Arnaud Revel and Jean-Christophe Burie},
  doi          = {10.1016/j.neunet.2020.06.025},
  journal      = {Neural Networks},
  pages        = {111-125},
  shortjournal = {Neural Netw.},
  title        = {A learning approach with incomplete pixel-level labels for deep neural networks},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visual interaction networks: A novel bio-inspired
computational model for image classification. <em>NN</em>, <em>130</em>,
100–110. (<a
href="https://doi.org/10.1016/j.neunet.2020.06.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inspired by biological mechanisms and structures in neuroscience , many biologically inspired visual computational models have been presented to provide new solutions for visual recognition task. For example, convolutional neural network (CNN) was proposed according to the hierarchical structure of biological vision, which could achieve superior performance in large-scale image classification . In this paper, we propose a new framework called visual interaction networks (VIN-Net), which is inspired by visual interaction mechanisms. More specifically, self-interaction, mutual-interaction, multi-interaction, and adaptive interaction are proposed in VIN-Net, forming the first interactive completeness of the visual interaction model. To further enhance the representation ability of visual features, the adaptive adjustment mechanism is integrated into the VIN-Net model. Finally, our model is evaluated on three benchmark datasets and two self-built textile defect datasets. The experimental results demonstrate that the proposed model exhibits its efficiency on visual classification tasks . Furthermore, a textile industrial application shows that the proposed architecture outperforms the state-of-the-art approaches in classification performance.},
  archive      = {J_NN},
  author       = {Bing Wei and Haibo He and Kuangrong Hao and Lei Gao and Xue-song Tang},
  doi          = {10.1016/j.neunet.2020.06.019},
  journal      = {Neural Networks},
  pages        = {100-110},
  shortjournal = {Neural Netw.},
  title        = {Visual interaction networks: A novel bio-inspired computational model for image classification},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quantifying the generalization error in deep learning in
terms of data distribution and neural network smoothness. <em>NN</em>,
<em>130</em>, 85–99. (<a
href="https://doi.org/10.1016/j.neunet.2020.06.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accuracy of deep learning , i.e., deep neural networks , can be characterized by dividing the total error into three main types: approximation error, optimization error, and generalization error . Whereas there are some satisfactory answers to the problems of approximation and optimization, much less is known about the theory of generalization. Most existing theoretical works for generalization fail to explain the performance of neural networks in practice. To derive a meaningful bound, we study the generalization error of neural networks for classification problems in terms of data distribution and neural network smoothness. We introduce the cover complexity (CC) to measure the difficulty of learning a data set and the inverse of the modulus of continuity to quantify neural network smoothness. A quantitative bound for expected accuracy/error is derived by considering both the CC and neural network smoothness. Although most of the analysis is general and not specific to neural networks, we validate our theoretical assumptions and results numerically for neural networks by several data sets of images. The numerical results confirm that the expected error of trained networks scaled with the square root of the number of classes has a linear relationship with respect to the CC. We also observe a clear consistency between test loss and neural network smoothness during the training process. In addition, we demonstrate empirically that the neural network smoothness decreases when the network size increases whereas the smoothness is insensitive to training dataset size.},
  archive      = {J_NN},
  author       = {Pengzhan Jin and Lu Lu and Yifa Tang and George Em Karniadakis},
  doi          = {10.1016/j.neunet.2020.06.024},
  journal      = {Neural Networks},
  pages        = {85-99},
  shortjournal = {Neural Netw.},
  title        = {Quantifying the generalization error in deep learning in terms of data distribution and neural network smoothness},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamical system based compact deep hybrid network for
classification of parkinson disease related EEG signals. <em>NN</em>,
<em>130</em>, 75–84. (<a
href="https://doi.org/10.1016/j.neunet.2020.06.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalogram (EEG) signals accumulate the brain’s spiking activities using standardized electrodes placed at the scalp. These cumulative brain signals are chaotic in nature and vary depending upon current physical and/or mental activities. The anatomy of the brain is altered when dopamine releasing neurons die because of Parkinson Disease (PD), a neurodegenerative disorder . The resulting alterations force synchronized neuronal activity in β β frequency components deep within motor region of the brain. This synchronization in the motor region affects the dynamical behavior of the brain activities , which induce motor related impairments in patient’s limbs. Identification of reliable bio-markers for PD is active research area since there are no tests or scans to diagnose PD. We use embedding reconstruction, a tool from chaos theory, to highlight PD-related alterations in dynamical properties of EEG and present it as a potentially reliable bio-marker for PD related classification. We use Individual Component Analysis (ICA) to demonstrate that the strengthened synchronizations can be cumulatively collected from EEG channels over the motor region of the brain. We use this information to select the 12 EEG channels for classification of On and Off medication PD patients. Additionally, there is the strong synchronization between amplitude of higher frequency components and phase of β β components for PD patients. This information is used to improve the performance of this classification. We apply embedding reconstruction to design a new architecture of a deep neural network called Dynamical system Generated Hybrid Network. We report that this network outperforms the state of the art in terms of classification accuracy of 99 . 2\% ( + 0 . 52\% ) 99.2\%(+0.52\%) with approximately 24\% of the computational resources. Apart from classification accuracy , we use well known statistical measures like specificity, sensitivity, Matthews Correlation Coefficient (MCC), F1 score, and Cohen Kappa score for the analysis and comparison of classification performances.},
  archive      = {J_NN},
  author       = {Syed Aamir Ali Shah and Lei Zhang and Abdul Bais},
  doi          = {10.1016/j.neunet.2020.06.018},
  journal      = {Neural Networks},
  pages        = {75-84},
  shortjournal = {Neural Netw.},
  title        = {Dynamical system based compact deep hybrid network for classification of parkinson disease related EEG signals},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Controller design for finite-time and fixed-time
stabilization of fractional-order memristive complex-valued BAM neural
networks with uncertain parameters and time-varying delays. <em>NN</em>,
<em>130</em>, 60–74. (<a
href="https://doi.org/10.1016/j.neunet.2020.06.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we investigate controller design problem for finite-time and fixed-time stabilization of fractional-order memristive complex-valued BAM neural networks (FMCVBAMNNs) with uncertain parameters and time-varying delays. By using the Lyapunov theory , differential inclusion theory, and fractional calculus theory, finite-time stabilization condition for fractional-order memristive complex-valued BAM neural networks and the upper bound of the settling time for stabilization are obtained. The nonlinear complex-valued activation functions are split into two (real and imaginary) components. Moreover, the settling time of fixed time stabilization, that does not depend upon the initial values, is merely calculated. A novel criterion for guaranteeing the fixed-time stabilization of FMCVBAMNNs is derived. Our control scheme achieves system stabilization within bounded time and has an advantage in convergence rate. Numerical simulations are furnished to demonstrate the effectiveness of the theoretical analysis.},
  archive      = {J_NN},
  author       = {Emel Arslan and G. Narayanan and M. Syed Ali and Sabri Arik and Sumit Saroha},
  doi          = {10.1016/j.neunet.2020.06.021},
  journal      = {Neural Networks},
  pages        = {60-74},
  shortjournal = {Neural Netw.},
  title        = {Controller design for finite-time and fixed-time stabilization of fractional-order memristive complex-valued BAM neural networks with uncertain parameters and time-varying delays},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). R-ELMNet: Regularized extreme learning machine network.
<em>NN</em>, <em>130</em>, 49–59. (<a
href="https://doi.org/10.1016/j.neunet.2020.06.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Principal component analysis network (PCANet), as an unsupervised shallow network, demonstrates noticeable effectiveness on datasets of various volumes. It carries a two-layer convolution with PCA as filter learning method, followed by a block-wise histogram post-processing stage. Following the structure of PCANet, extreme learning machine auto-encoder (ELM-AE) variants are employed to replace the PCA’s role, which come from extreme learning machine network (ELMNet) and hierarchical ELMNet. ELMNet emphasizes the importance of orthogonal projection while overlooking non-linearity. The latter introduces complex pre-processing to overcome drawback of non-linear ELM-AE. In this paper, we analyze intrinsic characteristics of ELM-AE variants and accordingly propose a regularized ELM-AE, which combines non-linearity learning capability and approximately orthogonal projection . Experiments on image classification show the effectiveness compared to supervised convolutional neural networks and related shallow networks on unsupervised feature learning .},
  archive      = {J_NN},
  author       = {Guanghao Zhang and Yue Li and Dongshun Cui and Shangbo Mao and Guang-Bin Huang},
  doi          = {10.1016/j.neunet.2020.06.009},
  journal      = {Neural Networks},
  pages        = {49-59},
  shortjournal = {Neural Netw.},
  title        = {R-ELMNet: Regularized extreme learning machine network},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning explicitly transferable representations for domain
adaptation. <em>NN</em>, <em>130</em>, 39–48. (<a
href="https://doi.org/10.1016/j.neunet.2020.06.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation tackles the problem where the training source domain and the test target domain have distinctive data distributions, and therefore improves the generalization ability of deep models. The very popular mechanism of domain adaptation is to learn a new feature representation which is supposed to be domain-invariant, so that the classifiers trained on the source domain can be directly applied to the target domain. However, recent work reveals that learning new feature representations may potentially deteriorate the adaptability of the original features and increase the expected error bound of the target domain. To address this, we propose to adapt classifiers rather than features. Specifically, we fill in the distribution gaps between domains by some additional transferable representations which are explicitly learned from the original features while keeping the original features unchanged. In addition, we argue that transferable representations should be able to be translated from one domain to the other with appropriate mappings. At the same time, we introduce conditional entropy to mitigate the semantic confusion during mapping. Experiments on both standard and large-scale datasets verify that our method is able to achieve the new state-of-the-art results on unsupervised domain adaptation .},
  archive      = {J_NN},
  author       = {Mengmeng Jing and Jingjing Li and Ke Lu and Lei Zhu and Yang Yang},
  doi          = {10.1016/j.neunet.2020.06.016},
  journal      = {Neural Networks},
  pages        = {39-48},
  shortjournal = {Neural Netw.},
  title        = {Learning explicitly transferable representations for domain adaptation},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stability of delayed inertial neural networks on time
scales: A unified matrix-measure approach. <em>NN</em>, <em>130</em>,
33–38. (<a href="https://doi.org/10.1016/j.neunet.2020.06.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This note introduces a unified matrix-measure concept to study the stability of a class of inertial neural networks with bounded time delays on time scales. The novel matrix-measure concept unifies the classic matrix-measure and the generalized matrix-measure concept. One sufficient global exponential stability criterion is obtained based on this key matrix-measure and no Lyapunov function is required. To make the stability performance better, another stability criterion in which more detailed information is involved has been acquired. The theoretical results in this note contain and extend some existing continuous-time and discrete-time works. A numerical example is given to show the validity of the results.},
  archive      = {J_NN},
  author       = {Qiang Xiao and Tingwen Huang},
  doi          = {10.1016/j.neunet.2020.06.020},
  journal      = {Neural Networks},
  pages        = {33-38},
  shortjournal = {Neural Netw.},
  title        = {Stability of delayed inertial neural networks on time scales: A unified matrix-measure approach},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Heart sound classification based on improved MFCC features
and convolutional recurrent neural networks. <em>NN</em>, <em>130</em>,
22–32. (<a href="https://doi.org/10.1016/j.neunet.2020.06.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heart sound classification plays a vital role in the early detection of cardiovascular disorders, especially for small primary health care clinics. Despite that much progress has been made for heart sound classification in recent years, most of them are based on conventional segmented features and shallow structure based classifiers. These conventional acoustic representation and classification methods may be insufficient in characterizing heart sound, and generally suffer from a degraded performance due to the complicated and changeable cardiac acoustic environment. In this paper, we propose a new heart sound classification method based on improved Mel-frequency cepstrum coefficient (MFCC) features and convolutional recurrent neural networks . The Mel-frequency cepstrums are firstly calculated without dividing the heart sound signal. A new improved feature extraction scheme based on MFCC is proposed to elaborate the dynamic characteristics among consecutive heart sound signals. Finally, the MFCC-based features are fed to a deep convolutional and recurrent neural network (CRNN) for feature learning and later classification task . The proposed deep learning framework can take advantage of the encoded local characteristics extracted from the convolutional neural network (CNN) and the long-term dependencies captured by the recurrent neural network (RNN). Comprehensive studies on the performance of different network parameters and different network connection strategies are presented in this paper. Performance comparisons with state-of-the-art algorithms are given for discussions. Experiments show that, for the two-class classification problem (pathological or non-pathological), a classification accuracy of 98\% has been achieved on the 2016 PhysioNet/CinC Challenge database.},
  archive      = {J_NN},
  author       = {Muqing Deng and Tingting Meng and Jiuwen Cao and Shimin Wang and Jing Zhang and Huijie Fan},
  doi          = {10.1016/j.neunet.2020.06.015},
  journal      = {Neural Networks},
  pages        = {22-32},
  shortjournal = {Neural Netw.},
  title        = {Heart sound classification based on improved MFCC features and convolutional recurrent neural networks},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Block-term tensor neural networks. <em>NN</em>,
<em>130</em>, 11–21. (<a
href="https://doi.org/10.1016/j.neunet.2020.05.034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have achieved outstanding performance in a wide range of applications, e.g., image classification , natural language processing , etc. Despite the good performance, the huge number of parameters in DNNs brings challenges to efficient training of DNNs and also their deployment in low-end devices with limited computing resources. In this paper, we explore the correlations in the weight matrices , and approximate the weight matrices with the low-rank block-term tensors. We name the new corresponding structure as block-term tensor layers (BT-layers), which can be easily adapted to neural network models, such as CNNs and RNNs . In particular, the inputs and the outputs in BT-layers are reshaped into low-dimensional high-order tensors with a similar or improved representation power. Sufficient experiments have demonstrated that BT-layers in CNNs and RNNs can achieve a very large compression ratio on the number of parameters while preserving or improving the representation power of the original DNNs.},
  archive      = {J_NN},
  author       = {Jinmian Ye and Guangxi Li and Di Chen and Haiqin Yang and Shandian Zhe and Zenglin Xu},
  doi          = {10.1016/j.neunet.2020.05.034},
  journal      = {Neural Networks},
  pages        = {11-21},
  shortjournal = {Neural Netw.},
  title        = {Block-term tensor neural networks},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel feature representation: Aggregating convolution
kernels for image retrieval. <em>NN</em>, <em>130</em>, 1–10. (<a
href="https://doi.org/10.1016/j.neunet.2020.06.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Activated hidden units in convolutional neural networks (CNNs), known as feature maps, dominate image representation, which is compact and discriminative. For ultra-large datasets, high dimensional feature maps in float format not only result in high computational complexity , but also occupy massive memory space. To this end, a new image representation by aggregating convolution kernels (ACK) is proposed, where some convolution kernels capturing certain patterns are activated. The top-n index numbers of the convolution kernels are extracted directly as image representation in discrete integer values, which rebuild relationship between convolution kernels and image. Furthermore, a distance measurement is defined from the perspective of ordered sets to calculate position-sensitive similarities between image representations. Extensive experiments conducted on Oxford Buildings, Paris, and Holidays, etc., manifest that the proposed ACK achieves competitive performance on image retrieval with much lower computational cost, outperforming the ones using feature maps for image representation.},
  archive      = {J_NN},
  author       = {Qi Wang and Jinxing Lai and Luc Claesen and Zhenguo Yang and Liang Lei and Wenyin Liu},
  doi          = {10.1016/j.neunet.2020.06.010},
  journal      = {Neural Networks},
  pages        = {1-10},
  shortjournal = {Neural Netw.},
  title        = {A novel feature representation: Aggregating convolution kernels for image retrieval},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020d). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>129</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(20)30279-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(20)30279-3},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020d). Current events. <em>NN</em>, <em>129</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(20)30278-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(20)30278-1},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {Current events},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pinning bipartite synchronization for inertial coupled
delayed neural networks with signed digraph via non-reduced order
method. <em>NN</em>, <em>129</em>, 392–402. (<a
href="https://doi.org/10.1016/j.neunet.2020.06.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study investigates bipartite synchronization of inertial coupled delayed neural networks (ICDNNs) with signed digraph by non-reduced order method and pinning control. The second-order CDNNs will not be converted into a first-order differential system by introducing variable substitution. Instead, a novel Lyapunov–Krasovskii functional is proposed which depends on the topology of the ICDNNs. Some sufficient conditions for linear matrix inequalities (LMI) are derived to realize bipartite synchronization, which is based on matrix decomposition theory and Barbalat Lemma in strongly connected signed networks. And then, M-matrix theory is utilized to generalize the results to networks containing directed spanning trees. Finally, two examples are used to verify the validity of the derived theoretical results.},
  archive      = {J_NN},
  author       = {Shanshan Chen and Haijun Jiang and Binglong Lu and Zhiyong Yu and Liang Li},
  doi          = {10.1016/j.neunet.2020.06.017},
  journal      = {Neural Networks},
  pages        = {392-402},
  shortjournal = {Neural Netw.},
  title        = {Pinning bipartite synchronization for inertial coupled delayed neural networks with signed digraph via non-reduced order method},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Prediction of n6-methyladenosine sites using convolution
neural network model based on distributed feature representations.
<em>NN</em>, <em>129</em>, 385–391. (<a
href="https://doi.org/10.1016/j.neunet.2020.05.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {N 6 -methyladenosine (m 6 A) is a well-studied and most common interior messenger RNA (mRNA) modification that plays an important function in cell development. N 6 A is found in all kingdoms​ of life and many other cellular processes such as RNA splicing , immune tolerance, regulatory functions, RNA processing , and cancer. Despite the crucial role of m 6 A in cells, it was targeted computationally, but unfortunately, the obtained results were unsatisfactory. It is imperative to develop an efficient computational model that can truly represent m 6 A sites. In this regard, an intelligent and highly discriminative computational model namely: m6A-word2vec is introduced for the discrimination of m 6 A sites. Here, a concept of natural language processing in the form of word2vec is used to represent the motif of the target class automatically. These motifs (numerical descriptors) are automatically targeted from the human genome without any clear definition. Further, the extracted feature space is then forwarded to the convolution neural network model as input for prediction. The developed computational model obtained 83.17\%, 92.69\%, and 90.50\% accuracy for benchmark datasets S 1 S1 , S 2 S2 , and S 3 S3 , respectively, using a 10-fold cross-validation test. The predictive outcomes validate that the developed intelligent computational model showed better performance compared to existing computational models. It is thus greatly estimated that the introduced computational model “m6A-word2vec” may be a supportive and practical tool for elementary and pharmaceutical research such as in drug design along with academia.},
  archive      = {J_NN},
  author       = {Muhammad Tahir and Maqsood Hayat and Kil To Chong},
  doi          = {10.1016/j.neunet.2020.05.027},
  journal      = {Neural Networks},
  pages        = {385-391},
  shortjournal = {Neural Netw.},
  title        = {Prediction of n6-methyladenosine sites using convolution neural network model based on distributed feature representations},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Noise can speed backpropagation learning and deep
bidirectional pretraining. <em>NN</em>, <em>129</em>, 359–384. (<a
href="https://doi.org/10.1016/j.neunet.2020.04.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that the backpropagation algorithm is a special case of the generalized Expectation–Maximization (EM) algorithm for iterative maximum likelihood estimation. We then apply the recent result that carefully chosen noise can speed the average convergence of the EM algorithm as it climbs a hill of probability or log-likelihood. Then injecting such noise can speed the average convergence of the backpropagation algorithm for both the training and pretraining of multilayer neural networks . The beneficial noise adds to the hidden and visible neurons and related parameters. The noise also applies to regularized regression networks. This beneficial noise is just that noise that makes the current signal more probable. We show that such noise also tends to improve classification accuracy . The geometry of the noise-benefit region depends on the probability structure of the neurons in a given layer. The noise-benefit region in noise space lies above the noisy-EM (NEM) hyperplane for classification and involves a hypersphere for regression. Simulations demonstrate these noise benefits using MNIST digit classification. The NEM noise benefits substantially exceed those of simply adding blind noise to the neural network. We further prove that the noise speed-up applies to the deep bidirectional pretraining of neural-network bidirectional associative memories (BAMs) or their functionally equivalent restricted Boltzmann machines. We then show that learning with basic contrastive divergence also reduces to generalized EM for an energy-based network probability. The optimal noise adds to the input visible neurons of a BAM in stacked layers of trained BAMs. Global stability of generalized BAMs guarantees rapid convergence in pretraining where neural signals feed back between contiguous layers. Bipolar coding of inputs further improves pretraining performance.},
  archive      = {J_NN},
  author       = {Bart Kosko and Kartik Audhkhasi and Osonde Osoba},
  doi          = {10.1016/j.neunet.2020.04.004},
  journal      = {Neural Networks},
  pages        = {359-384},
  shortjournal = {Neural Netw.},
  title        = {Noise can speed backpropagation learning and deep bidirectional pretraining},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast generalization error bound of deep learning without
scale invariance of activation functions. <em>NN</em>, <em>129</em>,
344–358. (<a
href="https://doi.org/10.1016/j.neunet.2020.05.033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the theoretical analysis of deep learning , discovering which features of deep learning lead to good performance is an important task. Using the framework for analyzing the generalization error developed by Suzuki (2018), we derive a fast learning rate for deep neural networks with general activation functions . According to Suzuki (2018), scale invariance of the activation functions is essential to derive tight error bounds. While the rectified linear unit (ReLU; Nair and Hinton, 2010) satisfies scale invariance , the other famous activation functions, such as the sigmoid, the hyperbolic tangent functions, and the exponential linear unit (ELU; Clevert et al., 2016), do not satisfy this condition. The existing analysis indicates the possibility that deep learning with non scale invariant activations may have a slower convergence rate of O ( 1 ∕ n ) O(1∕n) whereas with scale invariant activation functions it can reach a faster rate. In this paper, without scale invariance of activation functions, we derive the tight generalization error bound which is essentially the same as that of Suzuki (2018). From this result, at least in the framework of Suzuki (2018), we show that scale invariance of the activation functions is not essential to obtain a fast rate of convergence. We also conclude that the theoretical framework proposed by Suzuki (2018) can be widely applied to the analysis of deep learning with general activation functions.},
  archive      = {J_NN},
  author       = {Yoshikazu Terada and Ryoma Hirose},
  doi          = {10.1016/j.neunet.2020.05.033},
  journal      = {Neural Networks},
  pages        = {344-358},
  shortjournal = {Neural Netw.},
  title        = {Fast generalization error bound of deep learning without scale invariance of activation functions},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Appearance variation adaptation tracker using adversarial
network. <em>NN</em>, <em>129</em>, 334–343. (<a
href="https://doi.org/10.1016/j.neunet.2020.06.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual trackers using deep neural networks have demonstrated favorable performance in object tracking . However, training a deep classification network using overlapped initial target regions may lead an overfitted model. To increase the model generalization, we propose an appearance variation adaptation (AVA) tracker that aligns the feature distributions of target regions over time by learning an adaptation mask in an adversarial network. The proposed adversarial network consists of a generator and a discriminator network that compete with each other over optimizing a discriminator loss in a mini-max optimization problem . Specifically, the discriminator network aims to distinguish recent target regions from earlier ones by minimizing the discriminator loss, while the generator network aims to produce an adaptation mask to maximize the discriminator loss. We incorporate a gradient reverse layer in the adversarial network to solve the aforementioned mini-max optimization in an end-to-end manner. We compare the performance of the proposed AVA tracker with the most recent state-of-the-art trackers by doing extensive experiments on OTB50, OTB100, and VOT2016 tracking benchmarks. Among the compared methods, AVA yields the highest area under curve (AUC) score of 0.712 and the highest average precision score of 0.951 on the OTB50 tracking benchmark. It achieves the second best AUC score of 0.688 and the best precision score of 0.924 on the OTB100 tracking benchmark. AVA also achieves the second best expected average overlap (EAO) score of 0.366, the best failure rate of 0.68, and the second best accuracy of 0.53 on the VOT2016 tracking benchmark.},
  archive      = {J_NN},
  author       = {Mohammadreza Javanmardi and Xiaojun Qi},
  doi          = {10.1016/j.neunet.2020.06.011},
  journal      = {Neural Networks},
  pages        = {334-343},
  shortjournal = {Neural Netw.},
  title        = {Appearance variation adaptation tracker using adversarial network},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Energy-efficient and damage-recovery slithering gait design
for a snake-like robot based on reinforcement learning and inverse
reinforcement learning. <em>NN</em>, <em>129</em>, 323–333. (<a
href="https://doi.org/10.1016/j.neunet.2020.05.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Similar to real snakes in nature, the flexible trunks of snake-like robots enhance their movement capabilities and adaptabilities in diverse environments. However, this flexibility corresponds to a complex control task involving highly redundant degrees of freedom, where traditional model-based methods usually fail to propel the robots energy-efficiently and adaptively to unforeseeable joint damage. In this work, we present an approach for designing an energy-efficient and damage-recovery slithering gait for a snake-like robot using the reinforcement learning (RL) algorithm and the inverse reinforcement learning (IRL) algorithm. Specifically, we first present an RL-based controller for generating locomotion gaits at a wide range of velocities, which is trained using the proximal policy optimization (PPO) algorithm. Then, by taking the RL-based controller as an expert and collecting trajectories from it, we train an IRL-based controller using the adversarial inverse reinforcement learning (AIRL) algorithm. For the purpose of comparison, a traditional parameterized gait controller is presented as the baseline and the parameter sets are optimized using the grid search and Bayesian optimization algorithm . Based on the analysis of the simulation results, we first demonstrate that this RL-based controller exhibits very natural and adaptive movements, which are also substantially more energy-efficient than the gaits generated by the parameterized controller. We then demonstrate that the IRL-based controller cannot only exhibit similar performances as the RL-based controller, but can also recover from the unpredictable damage body joints and still outperform the model-based controller, which has an undamaged body, in terms of energy efficiency. Videos can be viewed at https://videoviewsite.wixsite.com/rlsnake .},
  archive      = {J_NN},
  author       = {Zhenshan Bing and Christian Lemke and Long Cheng and Kai Huang and Alois Knoll},
  doi          = {10.1016/j.neunet.2020.05.029},
  journal      = {Neural Networks},
  pages        = {323-333},
  shortjournal = {Neural Netw.},
  title        = {Energy-efficient and damage-recovery slithering gait design for a snake-like robot based on reinforcement learning and inverse reinforcement learning},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Partial transfer learning in machinery cross-domain fault
diagnostics using class-weighted adversarial networks. <em>NN</em>,
<em>129</em>, 313–322. (<a
href="https://doi.org/10.1016/j.neunet.2020.06.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, transfer learning has been receiving growing interests in machinery fault diagnosis due to its strong generalization across different industrial scenarios. The existing methods generally assume identical label spaces, and propose minimizing marginal distribution discrepancy between source and target domains. However, this assumption usually does not hold in real industries, where testing data mostly contain a subspace of the source label space. Therefore, transferring diagnosis knowledge from a comprehensive source domain to a target domain with limited machine conditions is motivated. This challenging partial transfer learning problem is addressed in this study using deep learning-based domain adaptation method. A class weighted adversarial neural network is proposed to encourage positive transfer of the shared classes and ignore the source outliers. Experimental results on two rotating machinery datasets suggest the proposed method is promising for partial transfer learning.},
  archive      = {J_NN},
  author       = {Xiang Li and Wei Zhang and Hui Ma and Zhong Luo and Xu Li},
  doi          = {10.1016/j.neunet.2020.06.014},
  journal      = {Neural Networks},
  pages        = {313-322},
  shortjournal = {Neural Netw.},
  title        = {Partial transfer learning in machinery cross-domain fault diagnostics using class-weighted adversarial networks},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Batch process fault detection for multi-stage broad learning
system. <em>NN</em>, <em>129</em>, 298–312. (<a
href="https://doi.org/10.1016/j.neunet.2020.05.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the real industrial production process, some minor faults are difficult to be detected by multivariate statistical analysis methods with mean and variance as detection indicators due to the aging equipment and catalyst deactivation . With structural characteristics, deep neural networks can better extract data features to detect such faults. However, most deep learning models contain a large number of connection parameters between layers, which causes the training time-consuming and thus makes it difficult to achieve a fast-online response. The Broad Learning System (BLS) network structure is expanded without a retraining process and thus saves a lot of training time. Considering that different stages of the batch production process have different production characteristics, we use the Affinity Propagation (AP) algorithm to separate the different stages of the production process. This paper conducts research on a multi-stage process monitoring framework that integrates AP and the BLS. Compared with other monitoring models, the monitoring results in the penicillin fermentation process have verified the superiority of the AP-BLS model.},
  archive      = {J_NN},
  author       = {Chang Peng and RuiWei Lu and Olivia Kang and Wang Kai},
  doi          = {10.1016/j.neunet.2020.05.031},
  journal      = {Neural Networks},
  pages        = {298-312},
  shortjournal = {Neural Netw.},
  title        = {Batch process fault detection for multi-stage broad learning system},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). A new lyapunov functional for stability analysis of
neutral-type hopfield neural networks with multiple delays. <em>NN</em>,
<em>129</em>, 288–297. (<a
href="https://doi.org/10.1016/j.neunet.2020.06.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research paper conducts an investigation into the stability issue for a more general class of neutral-type Hopfield neural networks that involves multiple time delays in the states of neurons and multiple neutral delays in the time derivatives of the states of neurons. By constructing a new proper Lyapunov functional , an alternative easily verifiable algebraic criterion for global asymptotic stability of this type of Hopfield neural systems is derived. This new stability condition is entirely independent of time and neutral delays. Two instructive examples are employed to indicate that the result obtained in this paper reveals a new set of sufficient stability criteria when it is compared with the previously reported stability results. Therefore, the proposed stability result enlarges the application domain of Hopfield neural systems of neutral types.},
  archive      = {J_NN},
  author       = {Ozlem Faydasicok},
  doi          = {10.1016/j.neunet.2020.06.013},
  journal      = {Neural Networks},
  pages        = {288-297},
  shortjournal = {Neural Netw.},
  title        = {A new lyapunov functional for stability analysis of neutral-type hopfield neural networks with multiple delays},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A neurodynamic optimization approach for complex-variables
programming problem. <em>NN</em>, <em>129</em>, 280–287. (<a
href="https://doi.org/10.1016/j.neunet.2020.06.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A neural network model upon differential inclusion is designed for solving the complex-variables convex programming , and the chain rule for real-valued function with the complex-variables is established in this paper. The model does not need to choose penalty parameters when applied to practical problems, which makes it easier to design. The result is obtained that its state reaches the feasible region in finite time. Furthermore, the convergence for its state to an optimal solution is proved. Some typical examples are shown for the effectiveness of the designed model.},
  archive      = {J_NN},
  author       = {Shuxin Liu and Haijun Jiang and Liwei Zhang and Xuehui Mei},
  doi          = {10.1016/j.neunet.2020.06.012},
  journal      = {Neural Networks},
  pages        = {280-287},
  shortjournal = {Neural Netw.},
  title        = {A neurodynamic optimization approach for complex-variables programming problem},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A spiking neural network-based long-term prediction system
for biogas production. <em>NN</em>, <em>129</em>, 271–279. (<a
href="https://doi.org/10.1016/j.neunet.2020.06.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient energy production from biomass is a central issue in the context of clean alternative energy resource. In this work we propose a novel model based on spiking neural networks cubes in order to model the chemical processes that goes on in a digestor for the production of usable biogas . For the implementation of the predictive structure, we have used the NeuCube computational framework. The goals of the proposed model were: develop a tool for real applications (low-cost and efficient), generalize the data when the system presents high sensitivity to small differences on the initial conditions, take in account the “multi-scale” temporal dynamics of the chemical processes occurring in the digestor, since the variations present in the early stages of the processes are very quick, whereas in the later stages are slower. By using the first ten days of observation the implemented system has been proven able to predict the evolution of the chemical process up to the 100th day obtaining a high degree of accuracy with respect to the experimental data measured in laboratory. This is due to the fact that the spiking neural networks have shown to be able to modeling complex information processes and then it has been shown that spiking neurons are able to handle patterns of activity that spans different time scales. Thanks to such properties, our system is able to capture the multi-scale trend of the time series associated to the early-stage evolutions, as well as their interaction, which are crucial in the point of view of the information content to obtain a good long-term prediction.},
  archive      = {J_NN},
  author       = {Giacomo Capizzi and Grazia Lo Sciuto and Christian Napoli and Marcin Woźniak and Gianluca Susi},
  doi          = {10.1016/j.neunet.2020.06.001},
  journal      = {Neural Networks},
  pages        = {271-279},
  shortjournal = {Neural Netw.},
  title        = {A spiking neural network-based long-term prediction system for biogas production},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Contextual encoder–decoder network for visual saliency
prediction. <em>NN</em>, <em>129</em>, 261–270. (<a
href="https://doi.org/10.1016/j.neunet.2020.05.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting salient regions in natural images requires the detection of objects that are present in a scene. To develop robust representations for this challenging task, high-level visual features at multiple spatial scales must be extracted and augmented with contextual information. However, existing models aimed at explaining human fixation maps do not incorporate such a mechanism explicitly. Here we propose an approach based on a convolutional neural network pre-trained on a large-scale image classification task. The architecture forms an encoder–decoder structure and includes a module with multiple convolutional layers at different dilation rates to capture multi-scale features in parallel. Moreover, we combine the resulting representations with global scene information for accurately predicting visual saliency. Our model achieves competitive and consistent results across multiple evaluation metrics on two public saliency benchmarks and we demonstrate the effectiveness of the suggested approach on five datasets and selected examples. Compared to state of the art approaches, the network is based on a lightweight image classification backbone and hence presents a suitable choice for applications with limited computational resources, such as (virtual) robotic systems , to estimate human fixations across complex natural scenes. Our TensorFlow implementation is openly available at https://github.com/alexanderkroner/saliency .},
  archive      = {J_NN},
  author       = {Alexander Kroner and Mario Senden and Kurt Driessens and Rainer Goebel},
  doi          = {10.1016/j.neunet.2020.05.004},
  journal      = {Neural Networks},
  pages        = {261-270},
  shortjournal = {Neural Netw.},
  title        = {Contextual encoder–decoder network for visual saliency prediction},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Missing data imputation with adversarially-trained graph
convolutional networks. <em>NN</em>, <em>129</em>, 249–260. (<a
href="https://doi.org/10.1016/j.neunet.2020.06.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data imputation (MDI) is the task of replacing missing values in a dataset with alternative, predicted ones. Because of the widespread presence of missing data, it is a fundamental problem in many scientific disciplines. Popular methods for MDI use global statistics computed from the entire dataset (e.g., the feature-wise medians), or build predictive models operating independently on every instance. In this paper we propose a more general framework for MDI, leveraging recent work in the field of graph neural networks (GNNs). We formulate the MDI task in terms of a graph denoising autoencoder , where each edge of the graph encodes the similarity between two patterns. A GNN encoder learns to build intermediate representations for each example by interleaving classical projection layers and locally combining information between neighbors, while another decoding GNN learns to reconstruct the full imputed dataset from this intermediate embedding. In order to speed-up training and improve the performance, we use a combination of multiple losses, including an adversarial loss implemented with the Wasserstein metric and a gradient penalty. We also explore a few extensions to the basic architecture involving the use of residual connections between layers, and of global statistics computed from the dataset to improve the accuracy. On a large experimental evaluation with varying levels of artificial noise, we show that our method is on par or better than several alternative imputation methods . On three datasets with pre-existing missing values, we show that our method is robust to the choice of a downstream classifier, obtaining similar or slightly higher results compared to other choices.},
  archive      = {J_NN},
  author       = {Indro Spinelli and Simone Scardapane and Aurelio Uncini},
  doi          = {10.1016/j.neunet.2020.06.005},
  journal      = {Neural Networks},
  pages        = {249-260},
  shortjournal = {Neural Netw.},
  title        = {Missing data imputation with adversarially-trained graph convolutional networks},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A neurodynamic model of the interaction between color
perception and color memory. <em>NN</em>, <em>129</em>, 222–248. (<a
href="https://doi.org/10.1016/j.neunet.2020.06.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The memory color effect and Spanish castle illusion have been taken as evidence of the cognitive penetrability of vision. In the same manner, the successful decoding of color-related brain signals in functional neuroimaging studies suggests the retrieval of memory colors associated with a perceived gray object. Here, we offer an alternative account of these findings based on the design principles of adaptive resonance theory (ART). In ART, conscious perception is a consequence of a resonant state. Resonance emerges in a recurrent cortical circuit when a bottom-up spatial pattern agrees with the top-down expectation. When they do not agree, a special control mechanism is activated that resets the network and clears off erroneous expectation, thus allowing the bottom-up activity to always dominate in perception. We developed a color ART circuit and evaluated its behavior in computer simulations. The model helps to explain how traces of erroneous expectations about incoming color are eventually removed from the color perception, although their transient effect may be visible in behavioral responses or in brain imaging . Our results suggest that the color ART circuit, as a predictive computational system, is almost never penetrable, because it is equipped with computational mechanisms designed to constrain the impact of the top-down predictions on ongoing perceptual processing.},
  archive      = {J_NN},
  author       = {Mateja Marić and Dražen Domijan},
  doi          = {10.1016/j.neunet.2020.06.008},
  journal      = {Neural Networks},
  pages        = {222-248},
  shortjournal = {Neural Netw.},
  title        = {A neurodynamic model of the interaction between color perception and color memory},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A gentle introduction to deep learning for graphs.
<em>NN</em>, <em>129</em>, 203–221. (<a
href="https://doi.org/10.1016/j.neunet.2020.06.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The adaptive processing of graph data is a long-standing research topic that has been lately consolidated as a theme of major interest in the deep learning community. The snap increase in the amount and breadth of related research has come at the price of little systematization of knowledge and attention to earlier literature. This work is a tutorial introduction to the field of deep learning for graphs. It favors a consistent and progressive presentation of the main concepts and architectural aspects over an exposition of the most recent literature, for which the reader is referred to available surveys. The paper takes a top-down view of the problem, introducing a generalized formulation of graph representation learning based on a local and iterative approach to structured information processing . Moreover, it introduces the basic building blocks that can be combined to design novel and effective neural models for graphs. We complement the methodological exposition with a discussion of interesting research challenges and applications in the field.},
  archive      = {J_NN},
  author       = {Davide Bacciu and Federico Errica and Alessio Micheli and Marco Podda},
  doi          = {10.1016/j.neunet.2020.06.006},
  journal      = {Neural Networks},
  pages        = {203-221},
  shortjournal = {Neural Netw.},
  title        = {A gentle introduction to deep learning for graphs},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Novel results on finite-time stabilization of state-based
switched chaotic inertial neural networks with distributed delays.
<em>NN</em>, <em>129</em>, 193–202. (<a
href="https://doi.org/10.1016/j.neunet.2020.06.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The p p -norm finite-time stabilization (FTS) issue of a class of state-based switched inertial chaotic neural networks (SBSCINNs) with distributed time-varying delays is investigated. By using a suitable variable transformation, such second-order SBSCINNs are turned into the first-order differential equations. Then some novel criteria are obtained to stabilize SBSCINNs in a finite time based on the theory of finite-time control and non-smooth analysis together with designing two proper delay-dependent feedback controllers . Besides, the settling time of FTS is also estimated and discussed. Finally, the validity and practicability of the deduced theoretical results are verified by examples and applications.},
  archive      = {J_NN},
  author       = {Changqing Long and Guodong Zhang and Zhigang Zeng},
  doi          = {10.1016/j.neunet.2020.06.004},
  journal      = {Neural Networks},
  pages        = {193-202},
  shortjournal = {Neural Netw.},
  title        = {Novel results on finite-time stabilization of state-based switched chaotic inertial neural networks with distributed delays},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graph embedded rules for explainable predictions in data
streams. <em>NN</em>, <em>129</em>, 174–192. (<a
href="https://doi.org/10.1016/j.neunet.2020.05.035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the reason why a prediction has been made by a machine is crucial to grant trust to a human decision-maker. However, data mining based decision support systems are, in general, not designed to promote interpretability ; instead, they are developed to improve accuracy. Interpretability becomes a more challenging issue in the context of data stream mining. Where the prediction model has to deal with enormous volumes of data gathered continuously at a fast rate and whose underlying distribution may change over time. On the one hand, the majority of the methods that address classification in a data stream are black-box models or white-box models into ensembles. Either do not provide a clear view of why a particular decision has been made. On the other hand, white-box models, such as rule-based models, do not provide acceptable accuracy to be considered in many applications. This paper proposes modeling the data as a special graph, which is built over the attribute space, and from which interpretable rules can be extracted. To overcome concept drift and enhance model accuracy, different variants of such graphs are considered within an ensemble that is updated over time. The proposed approach has shown the best overall classification results when compared to six rule-based algorithms in twelve streaming domains.},
  archive      = {J_NN},
  author       = {João Roberto Bertini Junior},
  doi          = {10.1016/j.neunet.2020.05.035},
  journal      = {Neural Networks},
  pages        = {174-192},
  shortjournal = {Neural Netw.},
  title        = {Graph embedded rules for explainable predictions in data streams},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Encoding primitives generation policy learning for robotic
arm to overcome catastrophic forgetting in sequential multi-tasks
learning. <em>NN</em>, <em>129</em>, 163–173. (<a
href="https://doi.org/10.1016/j.neunet.2020.06.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continual learning, a widespread ability in people and animals, aims to learn and acquire new knowledge and skills continuously. Catastrophic forgetting usually occurs in continual learning when an agent attempts to learn different tasks sequentially without storing or accessing previous task information. Unfortunately, current learning systems, e.g., neural networks , are prone to deviate the weights learned in previous tasks after training new tasks, leading to catastrophic forgetting, especially in a sequential multi-tasks scenario. To address this problem, in this paper, we propose to overcome catastrophic forgetting with the focus on learning a series of robotic tasks sequentially. Particularly, a novel hierarchical neural network’s framework called Encoding Primitives Generation Policy Learning (E-PGPL) is developed to enable continual learning with two components. By employing a variational autoencoder to project the original state space into a meaningful low-dimensional feature space, representative state primitives could be sampled to help learn corresponding policies for different tasks. In learning a new task, the feature space is required to be close to the previous ones so that previously learned tasks can be protected. Extensive experiments on several simulated robotic tasks demonstrate our method’s efficacy to learn control policies for handling sequentially arriving multi-tasks, delivering improvement substantially over some other continual learning methods, especially for the tasks with more diversity.},
  archive      = {J_NN},
  author       = {Fangzhou Xiong and Zhiyong Liu and Kaizhu Huang and Xu Yang and Hong Qiao and Amir Hussain},
  doi          = {10.1016/j.neunet.2020.06.003},
  journal      = {Neural Networks},
  pages        = {163-173},
  shortjournal = {Neural Netw.},
  title        = {Encoding primitives generation policy learning for robotic arm to overcome catastrophic forgetting in sequential multi-tasks learning},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-organization of action hierarchy and compositionality
by reinforcement learning with recurrent neural networks. <em>NN</em>,
<em>129</em>, 149–162. (<a
href="https://doi.org/10.1016/j.neunet.2020.06.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent neural networks (RNNs) for reinforcement learning (RL) have shown distinct advantages, e.g., solving memory-dependent tasks and meta-learning. However, little effort has been spent on improving RNN architectures and on understanding the underlying neural mechanisms for performance gain. In this paper, we propose a novel, multiple-timescale, stochastic RNN for RL. Empirical results show that the network can autonomously learn to abstract sub-goals and can self-develop an action hierarchy using internal dynamics in a challenging continuous control task. Furthermore, we show that the self-developed compositionality of the network enhances faster re-learning when adapting to a new task that is a re-composition of previously learned sub-goals, than when starting from scratch. We also found that improved performance can be achieved when neural activities are subject to stochastic rather than deterministic dynamics .},
  archive      = {J_NN},
  author       = {Dongqi Han and Kenji Doya and Jun Tani},
  doi          = {10.1016/j.neunet.2020.06.002},
  journal      = {Neural Networks},
  pages        = {149-162},
  shortjournal = {Neural Netw.},
  title        = {Self-organization of action hierarchy and compositionality by reinforcement learning with recurrent neural networks},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Structure learning with similarity preserving. <em>NN</em>,
<em>129</em>, 138–148. (<a
href="https://doi.org/10.1016/j.neunet.2020.05.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leveraging on the underlying low-dimensional structure of data, low-rank and sparse modeling approaches have achieved great success in a wide range of applications. However, in many applications the data can display structures beyond simply being low-rank or sparse. Fully extracting and exploiting hidden structure information in the data is always desirable and favorable. To reveal more underlying effective manifold structure, in this paper, we explicitly model the data relation. Specifically, we propose a structure learning framework that retains the pairwise similarities between the data points. Rather than just trying to reconstruct the original data based on self-expression, we also manage to reconstruct the kernel matrix, which functions as similarity preserving. Consequently, this technique is particularly suitable for the class of learning problems that are sensitive to sample similarity, e.g., clustering and semisupervised classification. To take advantage of representation power of deep neural network , a deep auto-encoder architecture is further designed to implement our model. Extensive experiments on benchmark data sets demonstrate that our proposed framework can consistently and significantly improve performance on both evaluation tasks. We conclude that the quality of structure learning can be enhanced if similarity information is incorporated.},
  archive      = {J_NN},
  author       = {Zhao Kang and Xiao Lu and Yiwei Lu and Chong Peng and Wenyu Chen and Zenglin Xu},
  doi          = {10.1016/j.neunet.2020.05.030},
  journal      = {Neural Networks},
  pages        = {138-148},
  shortjournal = {Neural Netw.},
  title        = {Structure learning with similarity preserving},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image style transfer with collection representation space
and semantic-guided reconstruction. <em>NN</em>, <em>129</em>, 123–137.
(<a href="https://doi.org/10.1016/j.neunet.2020.05.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image style transfer renders the content of an image into different styles. Current methods made decent progress with transferring the style of single image, however, visual statistics from one image cannot reflect the full scope of an artist. Also, previous work did not put content preservation in the important position, which would result in poor structure integrity, thus deteriorating the comprehensibility of generated image. These two problems would limit the visual quality improvement of style transfer results. Targeting at style resemblance and content preservation problems, we propose a style transfer system composed of collection representation space and semantic-guided reconstruction. We train an encoder–decoder network with art collections to construct a representation space that can reflect the style of the artist. Then, we use semantic information as guidance to reconstruct the target representation of the input image for better content preservation. We conduct both quantitative analysis and qualitative evaluation to assess the proposed method. Experiment results demonstrate that our approach well balanced the trade-off between capturing artistic characteristics and preserving content information in style transfer tasks.},
  archive      = {J_NN},
  author       = {Zhuoqi Ma and Jie Li and Nannan Wang and Xinbo Gao},
  doi          = {10.1016/j.neunet.2020.05.028},
  journal      = {Neural Networks},
  pages        = {123-137},
  shortjournal = {Neural Netw.},
  title        = {Image style transfer with collection representation space and semantic-guided reconstruction},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Phase portraits as movement primitives for fast humanoid
robot control. <em>NN</em>, <em>129</em>, 109–122. (<a
href="https://doi.org/10.1016/j.neunet.2020.04.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, usual approaches for fast robot control are largely reliant on solving online optimal control problems . Such methods are known to be computationally intensive and sensitive to model accuracy. On the other hand, animals plan complex motor actions not only fast but seemingly with little effort even on unseen tasks. This natural sense to infer temporal dynamics and coordination motivates us to approach robot control from a motor skill learning perspective to design fast and computationally light controllers that can be learned autonomously by the robot under mild modeling assumptions. This article introduces Phase Portrait Movement Primitives (PPMP), a primitive that predicts dynamics on a low dimensional phase space which in turn is used to govern the high dimensional kinematics of the task. The stark difference with other primitive formulations is a built-in mechanism for phase prediction in the form of coupled oscillators that replaces model-based state estimators such as Kalman filters . The policy is trained by optimizing the parameters of the oscillators whose output is connected to a kinematic distribution in the form of a phase portrait. The drastic reduction in dimensionality allows us to efficiently train and execute PPMPs on a real human-sized, dual-arm humanoid upper body on a task involving 20 degrees-of-freedom. We demonstrate PPMPs in interactions requiring fast reactions times while generating anticipative pose adaptation in both discrete and cyclic tasks.},
  archive      = {J_NN},
  author       = {Guilherme Maeda and Okan Koç and Jun Morimoto},
  doi          = {10.1016/j.neunet.2020.04.007},
  journal      = {Neural Networks},
  pages        = {109-122},
  shortjournal = {Neural Netw.},
  title        = {Phase portraits as movement primitives for fast humanoid robot control},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Initializing photonic feed-forward neural networks using
auxiliary tasks. <em>NN</em>, <em>129</em>, 103–108. (<a
href="https://doi.org/10.1016/j.neunet.2020.05.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Photonics is among the most promising emerging technologies for providing fast and energy-efficient Deep Learning (DL) implementations. Despite their advantages, these photonic DL accelerators also come with certain important limitations. For example, the majority of existing photonic accelerators do not currently support many of the activation functions that are commonly used in DL, such as the ReLU activation function. Instead, sinusoidal and sigmoidal nonlinearities are usually employed, rendering the training process unstable and difficult to tune, mainly due to vanishing gradient phenomena. Thus, photonic DL models usually require carefully fine-tuning all their training hyper-parameters in order to ensure that the training process will proceed smoothly. Despite the recent advances in initialization schemes, as well as in optimization algorithms , training photonic DL models is still especially challenging. To overcome these limitations, we propose a novel adaptive initialization method that employs auxiliary tasks to estimate the optimal initialization variance for each layer of a network. The effectiveness of the proposed approach is demonstrated using two different datasets, as well as two recently proposed photonic activation functions and three different initialization methods. Apart from significantly increasing the stability of the training process, the proposed method can be directly used with any photonic activation function, without further requiring any other kind of fine-tuning, as also demonstrated through the conducted experiments.},
  archive      = {J_NN},
  author       = {Nikolaos Passalis and George Mourgias-Alexandris and Nikos Pleros and Anastasios Tefas},
  doi          = {10.1016/j.neunet.2020.05.024},
  journal      = {Neural Networks},
  pages        = {103-108},
  shortjournal = {Neural Netw.},
  title        = {Initializing photonic feed-forward neural networks using auxiliary tasks},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep learning for symbols detection and classification in
engineering drawings. <em>NN</em>, <em>129</em>, 91–102. (<a
href="https://doi.org/10.1016/j.neunet.2020.05.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Engineering drawings are commonly used in different industries such as Oil and Gas, construction, and other types of engineering. Digitising these drawings is becoming increasingly important. This is mainly due to the need to improve business practices such as inventory, assets management, risk analysis , and other types of applications. However, processing and analysing these drawings is a challenging task. A typical diagram often contains a large number of different types of symbols belonging to various classes and with very little variation among them. Another key challenge is the class-imbalance problem, where some types of symbols largely dominate the data while others are hardly represented in the dataset. In this paper, we propose methods to handle these two challenges. First, we propose an advanced bounding-box detection method for localising and recognising symbols in engineering diagrams. Our method is end-to-end with no user interaction. Thorough experiments on a large collection of diagrams from an industrial partner proved that our methods accurately recognise more than 94\% of the symbols. Secondly, we present a method based on Deep Generative Adversarial Neural Network for handling class-imbalance. The proposed GAN model proved to be capable of learning from a small number of training examples. Experiment results showed that the proposed method greatly improved the classification of symbols in engineering drawings.},
  archive      = {J_NN},
  author       = {Eyad Elyan and Laura Jamieson and Adamu Ali-Gombe},
  doi          = {10.1016/j.neunet.2020.05.025},
  journal      = {Neural Networks},
  pages        = {91-102},
  shortjournal = {Neural Netw.},
  title        = {Deep learning for symbols detection and classification in engineering drawings},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Group visualization of class-discriminative features.
<em>NN</em>, <em>129</em>, 75–90. (<a
href="https://doi.org/10.1016/j.neunet.2020.05.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research explaining the behavior of convolutional neural networks (CNNs) has gained a lot of attention over the past few years. Although many visualization methods have been proposed to explain network predictions, most fail to provide clear correlations between the target output and the features extracted by convolutional layers . In this work, we define a concept, i.e., class-discriminative feature groups, to specify features that are extracted by groups of convolutional kernels correlated with a particular image class. We propose a detection method to detect class-discriminative feature groups and a visualization method to highlight image regions correlated with particular output and to interpret class-discriminative feature groups intuitively. The experiments showed that the proposed method can disentangle features based on image classes and shed light on what feature groups are extracted from which regions of the image. We also applied this method to visualize “lost” features in adversarial samples and features in an image containing a non-class object to demonstrate its ability to debug why the network failed or succeeded.},
  archive      = {J_NN},
  author       = {Rui Shi and Tianxing Li and Yasushi Yamaguchi},
  doi          = {10.1016/j.neunet.2020.05.026},
  journal      = {Neural Networks},
  pages        = {75-90},
  shortjournal = {Neural Netw.},
  title        = {Group visualization of class-discriminative features},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interpretable and lightweight convolutional neural network
for EEG decoding: Application to movement execution and imagination.
<em>NN</em>, <em>129</em>, 55–74. (<a
href="https://doi.org/10.1016/j.neunet.2020.05.032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs) are emerging as powerful tools for EEG decoding: these techniques, by automatically learning relevant features for class discrimination, improve EEG decoding performances without relying on handcrafted features. Nevertheless, the learned features are difficult to interpret and most of the existing CNNs introduce many trainable parameters. Here, we propose a lightweight and interpretable shallow CNN (Sinc-ShallowNet), by stacking a temporal sinc-convolutional layer (designed to learn band-pass filters, each having only the two cut-off frequencies as trainable parameters), a spatial depthwise convolutional layer (reducing channel connectivity and learning spatial filters tied to each band-pass filter), and a fully-connected layer finalizing the classification. This convolutional module limits the number of trainable parameters and allows direct interpretation of the learned spectral–spatial​ features via simple kernel visualizations. Furthermore, we designed a post-hoc gradient-based technique to enhance interpretation by identifying the more relevant and more class-specific features. Sinc-ShallowNet was evaluated on benchmark motor-execution and motor-imagery datasets and against different design choices and training strategies. Results show that (i) Sinc-ShallowNet outperformed a traditional machine learning algorithm and other CNNs for EEG decoding; (ii) The learned spectral–spatial features matched well-known EEG motor-related activity; (iii) The proposed architecture performed better with a larger number of temporal kernels still maintaining a good compromise between accuracy and parsimony, and with a trialwise rather than a cropped training strategy. In perspective, the proposed approach, with its interpretative capacity, can be exploited to investigate cognitive/motor aspects whose EEG correlates are yet scarcely known, potentially characterizing their relevant features.},
  archive      = {J_NN},
  author       = {Davide Borra and Silvia Fantozzi and Elisa Magosso},
  doi          = {10.1016/j.neunet.2020.05.032},
  journal      = {Neural Networks},
  pages        = {55-74},
  shortjournal = {Neural Netw.},
  title        = {Interpretable and lightweight convolutional neural network for EEG decoding: Application to movement execution and imagination},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An end-to-end exemplar association for unsupervised person
re-identification. <em>NN</em>, <em>129</em>, 43–54. (<a
href="https://doi.org/10.1016/j.neunet.2020.05.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tracklet association methods learn the cross camera retrieval ability though associating underlying cross camera positive samples, which have proven to be successful in unsupervised person re-identification task. However, most of them use poor-efficiency association strategies which costs long training hours but gains the low performance. To solve this, we propose an effective end-to-end exemplar associations (EEA) framework in this work. EEA mainly adapts three strategies to improve efficiency: (1) end-to-end exemplar-based training , (2) exemplar association and (3) dynamic selection threshold . The first one is to accelerate the training process, while the others aim to improve the tracklet association precision. Compared with existing tracklet associating methods, EEA obviously reduces the training cost and achieves the higher performance. Extensive experiments and ablation studies on seven RE-ID datasets demonstrate the superiority of the proposed EEA over most state-of-the-art unsupervised and domain adaptation RE-ID methods.},
  archive      = {J_NN},
  author       = {Jinlin Wu and Yang Yang and Zhen Lei and Jinqiao Wang and Stan Z. Li and Prayag Tiwari and Hari Mohan Pandey},
  doi          = {10.1016/j.neunet.2020.05.015},
  journal      = {Neural Networks},
  pages        = {43-54},
  shortjournal = {Neural Netw.},
  title        = {An end-to-end exemplar association for unsupervised person re-identification},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quasi-bipartite synchronization of signed delayed neural
networks under impulsive effects. <em>NN</em>, <em>129</em>, 31–42. (<a
href="https://doi.org/10.1016/j.neunet.2020.05.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper mainly studies quasi-bipartite synchronization (QBPS) of signed delayed neural networks (SDNNs) under impulsive effects, in which the nodes have cooperative as well as antagonistic interactions. It is assumed that disturbance occurs in the communication channels between some neighboring agents at impulsive occurring instants. Under the balanced network topology , some sufficient criteria to achieve QBPS of SDNNs are proposed by utilizing algebraic graph theory and extended Halanay differential inequality . Moreover, for the QBPS error of SDNNs, the upper bound of the final error state is also provided explicitly. Two numerical examples are presented to demonstrate the correctness of the theoretical results.},
  archive      = {J_NN},
  author       = {Guohong Mu and Lulu Li and Xiaodi Li},
  doi          = {10.1016/j.neunet.2020.05.012},
  journal      = {Neural Networks},
  pages        = {31-42},
  shortjournal = {Neural Netw.},
  title        = {Quasi-bipartite synchronization of signed delayed neural networks under impulsive effects},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-view clustering on data with partial instances and
clusters. <em>NN</em>, <em>129</em>, 19–30. (<a
href="https://doi.org/10.1016/j.neunet.2020.05.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most multi-view clustering algorithms apply to data with complete instances and clusters in the views. Recently, multi-view clustering on data with partial instances has been studied. In this paper, we study the more general version of the problem, i.e., multi-view clustering on data with partial instances and clusters in the views. We propose a non-negative matrix factorization (NMF) based algorithm. For the special case with partial instances, it introduces an instance-view-indicator matrix to indicate whether an instance exists in a view. Then, it maps the instances representing the same object to the same vector, and maps the instances representing different objects to different vectors. For the general case with partial instances and clusters, it further introduces a cluster-view-indicator matrix to indicate whether a cluster exists in a view. In each view, it also maps the instances representing the same object to the same vector, but it further makes the elements of the vector 0 if the elements correspond to missing clusters. Then it minimizes the disagreements between the approximated indicator vectors of instances representing the same object. Experimental results show that the proposed algorithm performs well on data with partial instances and clusters, and outperforms existing algorithms on data with partial instances.},
  archive      = {J_NN},
  author       = {Linlin Zong and Xianchao Zhang and Xinyue Liu and Hong Yu},
  doi          = {10.1016/j.neunet.2020.05.021},
  journal      = {Neural Networks},
  pages        = {19-30},
  shortjournal = {Neural Netw.},
  title        = {Multi-view clustering on data with partial instances and clusters},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DLPNet: A deep manifold network for feature extraction of
hyperspectral imagery. <em>NN</em>, <em>129</em>, 7–18. (<a
href="https://doi.org/10.1016/j.neunet.2020.05.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has received increasing attention in recent years and it has been successfully applied for feature extraction (FE) of hyperspectral images . However, most deep learning methods fail to explore the manifold structure in hyperspectral image (HSI). To tackle this issue, a novel graph-based deep learning model, termed deep locality preserving neural network (DLPNet), was proposed in this paper. Traditional deep learning methods use random initialization to initialize network parameters. Different from that, DLPNet initializes each layer of the network by exploring the manifold structure in hyperspectral data . In the stage of network optimization, it designed a deep-manifold learning joint loss function to exploit graph embedding process while measuring the difference between the predictive value and the actual value, then the proposed model can take into account the extraction of deep features and explore the manifold structure of data simultaneously. Experimental results on real-world HSI datasets indicate that the proposed DLPNet performs significantly better than some state-of-the-art methods.},
  archive      = {J_NN},
  author       = {Zhengying Li and Hong Huang and Yule Duan and Guangyao Shi},
  doi          = {10.1016/j.neunet.2020.05.022},
  journal      = {Neural Networks},
  pages        = {7-18},
  shortjournal = {Neural Netw.},
  title        = {DLPNet: A deep manifold network for feature extraction of hyperspectral imagery},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Error bounds for deep ReLU networks using the
kolmogorov–arnold superposition theorem. <em>NN</em>, <em>129</em>, 1–6.
(<a href="https://doi.org/10.1016/j.neunet.2019.12.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We prove a theorem concerning the approximation of multivariate functions by deep ReLU networks, for which the curse of the dimensionality is lessened. Our theorem is based on a constructive proof of the Kolmogorov–Arnold superposition theorem, and on a subset of multivariate continuous functions whose outer superposition functions can be efficiently approximated by deep ReLU networks.},
  archive      = {J_NN},
  author       = {Hadrien Montanelli and Haizhao Yang},
  doi          = {10.1016/j.neunet.2019.12.013},
  journal      = {Neural Networks},
  pages        = {1-6},
  shortjournal = {Neural Netw.},
  title        = {Error bounds for deep ReLU networks using the Kolmogorov–Arnold superposition theorem},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020e). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>128</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(20)30210-0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(20)30210-0},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020e). Current events. <em>NN</em>, <em>128</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(20)30209-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(20)30209-4},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {Current events},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Progressive learning: A deep learning framework for
continual learning. <em>NN</em>, <em>128</em>, 345–357. (<a
href="https://doi.org/10.1016/j.neunet.2020.05.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continual learning is the ability of a learning system to solve new tasks by utilizing previously acquired knowledge from learning and performing prior tasks without having significant adverse effects on the acquired prior knowledge. Continual learning is key to advancing machine learning and artificial intelligence . Progressive learning is a deep learning framework for continual learning that comprises three procedures: curriculum, progression, and pruning. The curriculum procedure is used to actively select a task to learn from a set of candidate tasks. The progression procedure is used to grow the capacity of the model by adding new parameters that leverage parameters learned in prior tasks, while learning from data available for the new task at hand, without being susceptible to catastrophic forgetting. The pruning procedure is used to counteract the growth in the number of parameters as further tasks are learned, as well as to mitigate negative forward transfer, in which prior knowledge unrelated to the task at hand may interfere and worsen performance. Progressive learning is evaluated on a number of supervised classification tasks in the image recognition and speech recognition domains to demonstrate its advantages compared with baseline methods . It is shown that, when tasks are related, progressive learning leads to faster learning that converges to better generalization performance using a smaller number of dedicated parameters.},
  archive      = {J_NN},
  author       = {Haytham M. Fayek and Lawrence Cavedon and Hong Ren Wu},
  doi          = {10.1016/j.neunet.2020.05.011},
  journal      = {Neural Networks},
  pages        = {345-357},
  shortjournal = {Neural Netw.},
  title        = {Progressive learning: A deep learning framework for continual learning},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time multiple spatiotemporal action localization and
prediction approach using deep learning. <em>NN</em>, <em>128</em>,
331–344. (<a
href="https://doi.org/10.1016/j.neunet.2020.05.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting the locations of multiple actions in videos and classifying them in real-time are challenging problems termed ”action localization and prediction” problem. Convolutional neural networks (ConvNets) have achieved great success for action localization and prediction in still images. A major advance occurred when the AlexNet architecture was introduced in the ImageNet competition. ConvNets have since achieved state-of-the-art performances across a wide variety of machine vision tasks, including object detection, image segmentation , image classification , facial recognition, human pose estimation, and tracking. However, few works exist that address action localization and prediction in videos. The current action localization research primarily focuses on the classification of temporally trimmed videos in which only one action occurs per frame. Moreover, nearly all the current approaches work only offline and are too slow to be useful in real-world environments. In this work, we propose a fast and accurate deep-learning approach to perform real-time action localization and prediction. The proposed approach uses convolutional neural networks to localize multiple actions and predict their classes in real time. This approach starts by using appearance and motion detection networks (known as ”you only look once” (YOLO) networks) to localize and classify actions from RGB frames and optical flow frames using a two-stream model. We then propose a fusion step that increases the localization accuracy of the proposed approach. Moreover, we generate an action tube based on frame level detection. The frame by frame processing introduces an early action detection and prediction with top performance in terms of detection speed and precision. The experimental results demonstrate this superiority of our proposed approach in terms of both processing time and accuracy compared to recent offline and online action localization and prediction approaches on the challenging UCF-101-24 and J-HMDB-21 benchmarks.},
  archive      = {J_NN},
  author       = {Ahmed Ali Hammam and Mona M. Soliman and Aboul Ella Hassanien},
  doi          = {10.1016/j.neunet.2020.05.017},
  journal      = {Neural Networks},
  pages        = {331-344},
  shortjournal = {Neural Netw.},
  title        = {Real-time multiple spatiotemporal action localization and prediction approach using deep learning},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Regularized least squares locality preserving projections
with applications to image recognition. <em>NN</em>, <em>128</em>,
322–330. (<a
href="https://doi.org/10.1016/j.neunet.2020.05.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Locality preserving projection (LPP), as a well-known technique for dimensionality reduction, is designed to preserve the local structure of the original samples which usually lie on a low-dimensional manifold in the real world. However, it suffers from the undersampled or small-sample-size problem, when the dimension of the features is larger than the number of samples which causes the corresponding generalized eigenvalue problem to be ill-posed. To address this problem, we show that LPP is equivalent to a multivariate linear regression under a mild condition, and establish the connection between LPP and a least squares problem with multiple columns on the right-hand side. Based on the developed connection, we propose two regularized least squares methods for solving LPP. Experimental results on real-world databases illustrate the performance of our methods.},
  archive      = {J_NN},
  author       = {Wei Wei and Hua Dai and Weitai Liang},
  doi          = {10.1016/j.neunet.2020.05.023},
  journal      = {Neural Networks},
  pages        = {322-330},
  shortjournal = {Neural Netw.},
  title        = {Regularized least squares locality preserving projections with applications to image recognition},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Approximation rates for neural networks with general
activation functions. <em>NN</em>, <em>128</em>, 313–321. (<a
href="https://doi.org/10.1016/j.neunet.2020.05.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We prove some new results concerning the approximation rate of neural networks with general activation functions . Our first result concerns the rate of approximation of a two layer neural network with a polynomially-decaying non-sigmoidal activation function. We extend the dimension independent approximation rates previously obtained to this new class of activation functions . Our second result gives a weaker, but still dimension independent, approximation rate for a larger class of activation functions, removing the polynomial decay assumption. This result applies to any bounded, integrable activation function. Finally, we show that a stratified sampling approach can be used to improve the approximation rate for polynomially decaying activation functions under mild additional assumptions.},
  archive      = {J_NN},
  author       = {Jonathan W. Siegel and Jinchao Xu},
  doi          = {10.1016/j.neunet.2020.05.019},
  journal      = {Neural Networks},
  pages        = {313-321},
  shortjournal = {Neural Netw.},
  title        = {Approximation rates for neural networks with general activation functions},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sparsity through evolutionary pruning prevents neuronal
networks from overfitting. <em>NN</em>, <em>128</em>, 305–312. (<a
href="https://doi.org/10.1016/j.neunet.2020.05.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern Machine learning techniques take advantage of the exponentially rising calculation power in new generation processor units. Thus, the number of parameters which are trained to solve complex tasks was highly increased over the last decades. However, still the networks fail – in contrast to our brain – to develop general intelligence in the sense of being able to solve several complex tasks with only one network architecture . This could be the case because the brain is not a randomly initialized neural network , which has to be trained from scratch by simply investing a lot of calculation power, but has from birth some fixed hierarchical structure. To make progress in decoding the structural basis of biological neural networks we here chose a bottom-up approach, where we evolutionarily trained small neural networks in performing a maze task. This simple maze task requires dynamic decision making with delayed rewards. We were able to show that during the evolutionary optimization random severance of connections leads to better generalization performance of the networks compared to fully connected networks. We conclude that sparsity is a central property of neural networks and should be considered for modern Machine learning approaches .},
  archive      = {J_NN},
  author       = {Richard C. Gerum and André Erpenbeck and Patrick Krauss and Achim Schilling},
  doi          = {10.1016/j.neunet.2020.05.007},
  journal      = {Neural Networks},
  pages        = {305-312},
  shortjournal = {Neural Netw.},
  title        = {Sparsity through evolutionary pruning prevents neuronal networks from overfitting},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cross-modality paired-images generation and augmentation for
RGB-infrared person re-identification. <em>NN</em>, <em>128</em>,
294–304. (<a
href="https://doi.org/10.1016/j.neunet.2020.05.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {RGB-Infrared (IR) person re-identification is very challenging due to the large cross-modality variations between RGB and IR images. Considering no correspondence labels between every pair of RGB and IR images, most methods try to alleviate the variations with set-level alignment by reducing marginal distribution divergence between the entire RGB and IR sets. However, this set-level alignment strategy may lead to misalignment of some instances, which limit the performance for RGB–IR Re-ID. Different from existing methods, in this paper, we propose to generate cross-modality paired-images and perform both global set-level and fine-grained instance-level alignments. Our proposed method enjoys several merits. First, our method can perform set-level alignment by disentangling modality-specific and modality-invariant features. Compared with conventional methods, ours can explicitly remove the modality-specific features and the modality variation can be better reduced. Second, given cross-modality unpaired-images of a person, our method can generate cross-modality paired images from exchanged features. With them, we can directly perform instance-level alignment by minimizing distances of every pair of images. Third, our method learns a latent manifold space. In the space, we can random sample and generate lots of images of unseen classes. Training with those images, the learned identity feature space is more smooth can generalize better when test. Finally, extensive experimental results on two standard benchmarks demonstrate that the proposed model favorably against state-of-the-art methods.},
  archive      = {J_NN},
  author       = {Guan’an Wang and Yang Yang and Tianzhu Zhang and Jian Cheng and Zengguang Hou and Prayag Tiwari and Hari Mohan Pandey},
  doi          = {10.1016/j.neunet.2020.05.008},
  journal      = {Neural Networks},
  pages        = {294-304},
  shortjournal = {Neural Netw.},
  title        = {Cross-modality paired-images generation and augmentation for RGB-infrared person re-identification},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generalized norm for existence, uniqueness and stability of
hopfield neural networks with discrete and distributed delays.
<em>NN</em>, <em>128</em>, 288–293. (<a
href="https://doi.org/10.1016/j.neunet.2020.05.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the existence, uniqueness and stability criteria of solutions for Hopfield neural networks with discrete and distributed delays (DDD HNNs) are investigated by the definitions of three kinds of generalized norm ( ξ ξ -norm). A general DDD HNN model is firstly introduced, where the discrete delays τ p q ( t ) τpq(t) are asynchronous time-varying delays. Then, { ξ , 1 } {ξ,1} -norm, { ξ , 2 } {ξ,2} -norm and { ξ , ∞ } {ξ,∞} -norm are successively used to derive the existence, uniqueness and stability criteria of solutions for the DDD HNNs. In the proof of theorems, special functions and assumptions are given to deal with discrete and distributed delays. Furthermore, a corollary is concluded for the existence and stability criteria of solutions. The methods given in this paper can also be used to study the synchronization and μ μ -stability of different DDD NNs. Finally, two numerical examples and their simulation figures are given to illustrate the effectiveness of these results.},
  archive      = {J_NN},
  author       = {Huamin Wang and Guoliang Wei and Shiping Wen and Tingwen Huang},
  doi          = {10.1016/j.neunet.2020.05.014},
  journal      = {Neural Networks},
  pages        = {288-293},
  shortjournal = {Neural Netw.},
  title        = {Generalized norm for existence, uniqueness and stability of hopfield neural networks with discrete and distributed delays},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Uni-image: Universal image construction for robust neural
model. <em>NN</em>, <em>128</em>, 279–287. (<a
href="https://doi.org/10.1016/j.neunet.2020.05.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks have shown high performance in prediction, but they are defenseless when they predict on adversarial examples which are generated by adversarial attack techniques. In image classification , those attack techniques usually perturb the pixel of an image to fool the deep neural networks . To improve the robustness of the neural networks, many researchers have introduced several defense techniques against those attack techniques. To the best of our knowledge, adversarial training is one of the most effective defense techniques against the adversarial examples . However, the defense technique could fail against a semantic adversarial image that performs arbitrary perturbation to fool the neural networks, where the modified image semantically represents the same object as the original image. Against this background, we propose a novel defense technique, Uni-Image Procedure (UIP) method. UIP generates a universal-image (uni-image) from a given image, which can be a clean image or a perturbed image by some attacks. The generated uni-image preserves its own characteristics (i.e. color) regardless of the transformations of the original image. Note that those transformations include inverting the pixel value of an image, modifying the saturation, hue, and value of an image, etc. Our experimental results using several benchmark datasets show that our method not only defends well known adversarial attacks and semantic adversarial attack but also boosts the robustness of the neural network.},
  archive      = {J_NN},
  author       = {Jiacang Ho and Byung-Gook Lee and Dae-Ki Kang},
  doi          = {10.1016/j.neunet.2020.05.018},
  journal      = {Neural Networks},
  pages        = {279-287},
  shortjournal = {Neural Netw.},
  title        = {Uni-image: Universal image construction for robust neural model},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accurate and efficient sequential ensemble learning for
highly imbalanced multi-class data. <em>NN</em>, <em>128</em>, 268–278.
(<a href="https://doi.org/10.1016/j.neunet.2020.05.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-class classification for highly imbalanced data is a challenging task in which multiple issues must be resolved simultaneously, including (i) accuracy on classifying highly imbalanced multi-class data; (ii) training efficiency for large data; and (iii) sensitivity to high imbalance ratio (IR). In this paper, a novel sequential ensemble learning (SEL) framework is designed to simultaneously resolve these issues. SEL framework provides a significant property over traditional AdaBoost , in which the majority samples can be divided into multiple small and disjoint subsets for training multiple weak learners without compromising accuracy (while AdaBoost cannot). To ensure the class balance and majority-disjoint property of subsets, a learning strategy called balanced and majority-disjoint subsets division (BMSD) is developed. Unfortunately it is difficult to derive a general learner combination method (LCM) for any kind of weak learner. In this work, LCM is specifically designed for extreme learning machine , called LCM-ELM. The proposed SEL framework with BMSD and LCM-ELM has been compared with state-of-the-art methods over 16 benchmark datasets. In the experiments, under highly imbalanced multi-class data (IR up to 14K; data size up to 493K), (i) the proposed works improve the performance in different measures including G-mean, macro-F, micro-F, MAUC; (ii) training time is significantly reduced.},
  archive      = {J_NN},
  author       = {Chi-Man Vong and Jie Du},
  doi          = {10.1016/j.neunet.2020.05.010},
  journal      = {Neural Networks},
  pages        = {268-278},
  shortjournal = {Neural Netw.},
  title        = {Accurate and efficient sequential ensemble learning for highly imbalanced multi-class data},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generalized guerra’s interpolation schemes for dense
associative neural networks. <em>NN</em>, <em>128</em>, 254–267. (<a
href="https://doi.org/10.1016/j.neunet.2020.05.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we develop analytical techniques to investigate a broad class of associative neural networks set in the high-storage regime. These techniques translate the original statistical–mechanical problem into an analytical–mechanical one which implies solving a set of partial differential equations , rather than tackling the canonical probabilistic route. We test the method on the classical Hopfield model – where the cost function includes only two-body interactions (i.e., quadratic terms) – and on the “relativistic” Hopfield model — where the (expansion of the) cost function includes p p -body (i.e., of degree p p ) contributions. Under the replica symmetric assumption, we paint the phase diagrams of these models by obtaining the explicit expression of their free energy as a function of the model parameters (i.e., noise level and memory storage). Further, since for non-pairwise models ergodicity breaking is non necessarily a critical phenomenon, we develop a fluctuation analysis and find that criticality is preserved in the relativistic model.},
  archive      = {J_NN},
  author       = {Elena Agliari and Francesco Alemanno and Adriano Barra and Alberto Fachechi},
  doi          = {10.1016/j.neunet.2020.05.009},
  journal      = {Neural Networks},
  pages        = {254-267},
  shortjournal = {Neural Netw.},
  title        = {Generalized guerra’s interpolation schemes for dense associative neural networks},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graph transform learning. <em>NN</em>, <em>128</em>,
248–253. (<a
href="https://doi.org/10.1016/j.neunet.2020.05.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transform learning is a new representation learning framework where we learn an operator/transform that analyses the data to generate the coefficient/representation. We propose a variant of it called the graph transform learning; in this we explicitly account for the correlation in the dataset in terms of graph Laplacian . We will give two variants; in the first one the graph is computed from the data and fixed during the operation. In the second, the graph is learnt iteratively from the data during operation. The first technique will be applied for clustering, and the second one for solving inverse problems .},
  archive      = {J_NN},
  author       = {Angshul Majumdar},
  doi          = {10.1016/j.neunet.2020.05.020},
  journal      = {Neural Networks},
  pages        = {248-253},
  shortjournal = {Neural Netw.},
  title        = {Graph transform learning},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Embedding and approximation theorems for echo state
networks. <em>NN</em>, <em>128</em>, 234–247. (<a
href="https://doi.org/10.1016/j.neunet.2020.05.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Echo State Networks (ESNs) are a class of single-layer recurrent neural networks that have enjoyed recent attention. In this paper we prove that a suitable ESN, trained on a series of measurements of an invertible dynamical system , induces a C1 map from the dynamical system’s phase space to the ESN’s reservoir space. We call this the Echo State Map. We then prove that the Echo State Map is generically an embedding with positive probability. Under additional mild assumptions, we further conjecture that the Echo State Map is almost surely an embedding. For sufficiently large, and specially structured, but still randomly generated ESNs, we prove that there exists a linear readout layer that allows the ESN to predict the next observation of a dynamical system arbitrarily well. Consequently, if the dynamical system under observation is structurally stable then the trained ESN will exhibit dynamics that are topologically conjugate to the future behaviour of the observed dynamical system. Our theoretical results connect the theory of ESNs to the delay-embedding literature for dynamical systems, and are supported by numerical evidence from simulations of the traditional Lorenz equations . The simulations confirm that, from a one dimensional observation function, an ESN can accurately infer a range of geometric and topological features of the dynamics such as the eigenvalues of equilibrium points , Lyapunov exponents and homology groups .},
  archive      = {J_NN},
  author       = {Allen Hart and James Hook and Jonathan Dawes},
  doi          = {10.1016/j.neunet.2020.05.013},
  journal      = {Neural Networks},
  pages        = {234-247},
  shortjournal = {Neural Netw.},
  title        = {Embedding and approximation theorems for echo state networks},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). T-net: Nested encoder–decoder architecture for the main
vessel segmentation in coronary angiography. <em>NN</em>, <em>128</em>,
216–233. (<a
href="https://doi.org/10.1016/j.neunet.2020.05.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we proposed nested encoder–decoder architecture named T-Net. T-Net consists of several small encoder–decoders for each block constituting convolutional network . T-Net overcomes the limitation that U-Net can only have a single set of the concatenate layer between encoder and decoder block. To be more precise, the U-Net symmetrically forms the concatenate layers, so the low-level feature of the encoder is connected to the latter part of the decoder, and the high-level feature is connected to the beginning of the decoder. T-Net arranges the pooling and up-sampling appropriately during the encoding process, and likewise during the decoding process so that feature-maps of various sizes are obtained in a single block. As a result, all features from the low-level to the high-level extracted from the encoder are delivered from the beginning of the decoder to predict a more accurate mask. We evaluated T-Net for the problem of segmenting three main vessels in coronary angiography images. The experiment consisted of a comparison of U-Net and T-Nets under the same conditions, and an optimized T-Net for the main vessel segmentation . As a result, T-Net recorded a Dice Similarity Coefficient score ( DSC ) of 83.77\%, 10.69\% higher than that of U-Net, and the optimized T-Net recorded a DSC of 88.97\% which was 15.89\% higher than that of U-Net. In addition, we visualized the weight activation of the convolutional layer of T-Net and U-Net to show that T-Net actually predicts the mask from earlier decoders. Therefore, we expect that T-Net can be effectively applied to other similar medical image segmentation problems.},
  archive      = {J_NN},
  author       = {Tae Joon Jun and Jihoon Kweon and Young-Hak Kim and Daeyoung Kim},
  doi          = {10.1016/j.neunet.2020.05.002},
  journal      = {Neural Networks},
  pages        = {216-233},
  shortjournal = {Neural Netw.},
  title        = {T-net: Nested encoder–decoder architecture for the main vessel segmentation in coronary angiography},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analog neuron hierarchy. <em>NN</em>, <em>128</em>, 199–215.
(<a href="https://doi.org/10.1016/j.neunet.2020.05.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to refine the analysis of the computational power of discrete-time recurrent neural networks (NNs) between the binary-state NNs which are equivalent to finite automata (level 3 in the Chomsky hierarchy), and the analog-state NNs with rational weights which are Turing-complete (Chomsky level 0), we study an intermediate model α α ANN of a binary-state NN that is extended with α ≥ 0 α≥0 extra analog-state neurons. For rational weights, we establish an analog neuron hierarchy 0ANNs ⊂ ⊂ 1ANNs ⊂ ⊂ 2ANNs ⊆ ⊆ 3ANNs and separate its first two levels. In particular, 0ANNs coincide with the binary-state NNs (Chomsky level 3) being a proper subset of 1ANNs which accept at most context-sensitive languages (Chomsky level 1) including some non-context-free ones (above Chomsky level 2). We prove that the deterministic (context-free) language L # = { 0 n 1 n ∣ n ≥ 1 } L#={0n1n∣n≥1} cannot be recognized by any 1ANN even with real weights. In contrast, we show that deterministic pushdown automata accepting deterministic languages can be simulated by 2ANNs with rational weights, which thus constitute a proper superset of 1ANNs. Finally, we prove that the analog neuron hierarchy collapses to 3ANNs by showing that any Turing machine can be simulated by a 3ANN having rational weights, with linear-time overhead.},
  archive      = {J_NN},
  author       = {Jiří Šíma},
  doi          = {10.1016/j.neunet.2020.05.006},
  journal      = {Neural Networks},
  pages        = {199-215},
  shortjournal = {Neural Netw.},
  title        = {Analog neuron hierarchy},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast haar transforms for graph neural networks. <em>NN</em>,
<em>128</em>, 188–198. (<a
href="https://doi.org/10.1016/j.neunet.2020.04.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks (GNNs) have become a topic of intense research recently due to their powerful capability in high-dimensional classification and regression tasks for graph-structured data. However, as GNNs typically define the graph convolution by the orthonormal basis for the graph Laplacian , they suffer from high computational cost when the graph size is large. This paper introduces a Haar basis , which is a sparse and localized orthonormal system for a coarse-grained chain on the graph. The graph convolution under Haar basis , called Haar convolution, can be defined accordingly for GNNs. The sparsity and locality of the Haar basis allow Fast Haar Transforms (FHTs) on the graph, by which one then achieves a fast evaluation of Haar convolution between graph data and filters. We conduct experiments on GNNs equipped with Haar convolution, which demonstrates state-of-the-art results on graph-based regression and node classification tasks .},
  archive      = {J_NN},
  author       = {Ming Li and Zheng Ma and Yu Guang Wang and Xiaosheng Zhuang},
  doi          = {10.1016/j.neunet.2020.04.028},
  journal      = {Neural Networks},
  pages        = {188-198},
  shortjournal = {Neural Netw.},
  title        = {Fast haar transforms for graph neural networks},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sequential vessel segmentation via deep channel attention
network. <em>NN</em>, <em>128</em>, 172–187. (<a
href="https://doi.org/10.1016/j.neunet.2020.05.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately segmenting contrast-filled vessels from X-ray coronary angiography (XCA) image sequence is an essential step for the diagnosis and therapy of coronary artery disease. However, developing automatic vessel segmentation is particularly challenging due to the overlapping structures, low contrast and the presence of complex and dynamic background artifacts in XCA images. This paper develops a novel encoder–decoder deep network architecture which exploits the several contextual frames of 2D+t sequential images in a sliding window centered at current frame to segment 2D vessel masks from the current frame. The architecture is equipped with temporal–spatial feature extraction in encoder stage, feature fusion in skip connection layers and channel attention mechanism in decoder stage. In the encoder stage, a series of 3D convolutional layers are employed to hierarchically extract temporal–spatial features. Skip connection layers subsequently fuse the temporal–spatial feature maps and deliver them to the corresponding decoder stages. To efficiently discriminate vessel features from the complex and noisy backgrounds in the XCA images, the decoder stage effectively utilizes channel attention blocks to refine the intermediate feature maps from skip connection layers for subsequently decoding the refined features in 2D ways to produce the segmented vessel masks. Furthermore, Dice loss function is implemented to train the proposed deep network in order to tackle the class imbalance problem in the XCA data due to the wide distribution of complex background artifacts. Extensive experiments by comparing our method with other state-of-the-art algorithms demonstrate the proposed method’s superior performance over other methods in terms of the quantitative metrics and visual validation. To facilitate the reproductive research in XCA community, we publicly release our dataset and source codes at https://github.com/Binjie-Qin/SVS-net .},
  archive      = {J_NN},
  author       = {Dongdong Hao and Song Ding and Linwei Qiu and Yisong Lv and Baowei Fei and Yueqi Zhu and Binjie Qin},
  doi          = {10.1016/j.neunet.2020.05.005},
  journal      = {Neural Networks},
  pages        = {172-187},
  shortjournal = {Neural Netw.},
  title        = {Sequential vessel segmentation via deep channel attention network},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Impulsive synchronization of coupled delayed neural networks
with actuator saturation and its application to image encryption.
<em>NN</em>, <em>128</em>, 158–171. (<a
href="https://doi.org/10.1016/j.neunet.2020.05.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The actuator of any physical control systems is constrained by amplitude and energy, which causes the control systems to be inevitably affected by actuator saturation. In this paper, impulsive synchronization of coupled delayed neural networks with actuator saturation is presented. A new controller is designed to introduce actuator saturation term into impulsive controller. Based on sector nonlinearity model approach, impulsive controls with actuator saturation and with partial actuator saturation are studied, respectively, and some effective sufficient conditions are obtained. Numerical simulation is presented to verify the validity of the theoretical analysis results. Finally, the impulsive synchronization is applied to image encryption . The experimental results show that the proposed image encryption system has high security properties.},
  archive      = {J_NN},
  author       = {Deqiang Ouyang and Jie Shao and Haijun Jiang and Sing Kiong Nguang and Heng Tao Shen},
  doi          = {10.1016/j.neunet.2020.05.016},
  journal      = {Neural Networks},
  pages        = {158-171},
  shortjournal = {Neural Netw.},
  title        = {Impulsive synchronization of coupled delayed neural networks with actuator saturation and its application to image encryption},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Synchronization criteria for quaternion-valued coupled
neural networks with impulses. <em>NN</em>, <em>128</em>, 150–157. (<a
href="https://doi.org/10.1016/j.neunet.2020.04.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the global exponential synchronization of a category of quaternion-valued coupled neural networks (QVCNNs) with impulses in this article. It makes up for the gap of coupled neural networks with impulses in quaternion. On account of the product of two quaternions cannot be exchanged under normal circumstances, for convenience, we isolate the QVCNN into four real-valued coupled neural networks (RVCNNs) which are converted into an augmented system by defining a new augmented vector. By leveraging a distinctive Lyapunov–Krasovskii function and some matrix inequalities, several sufficient conditions for the global exponential synchronization of the system are attained. Ultimately, two examples are used to prove the validity of the theories in this paper.},
  archive      = {J_NN},
  author       = {Xingnan Qi and Haibo Bao and Jinde Cao},
  doi          = {10.1016/j.neunet.2020.04.027},
  journal      = {Neural Networks},
  pages        = {150-157},
  shortjournal = {Neural Netw.},
  title        = {Synchronization criteria for quaternion-valued coupled neural networks with impulses},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Training memristor-based multilayer neuromorphic networks
with SGD, momentum and adaptive learning rates. <em>NN</em>,
<em>128</em>, 142–149. (<a
href="https://doi.org/10.1016/j.neunet.2020.04.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks implemented with traditional hardware face inherent limitation of memory latency . Specifically, the processing units like GPUs , FPGAs , and customized ASICs , must wait for inputs to read from memory and outputs to write back. This motivates memristor-based neuromorphic computing in which the memory units (i.e., memristors) have computing capabilities. However, training a memristor-based neural network is difficult since memristors work differently from CMOS hardware. This paper proposes a new training approach that enables prevailing neural network training techniques to be applied for memristor-based neuromorphic networks. Particularly, we introduce momentum and adaptive learning rate to the circuit training, both of which are proven methods that significantly accelerate the convergence of neural network parameters. Furthermore, we show that this circuit can be used for neural networks with arbitrary numbers of layers, neurons, and parameters. Simulation results on four classification tasks demonstrate that the proposed circuit achieves both high accuracy and fast speed. Compared with the SGD-based training circuit, on the WBC data set, the training speed of our circuit is increased by 37.2\% while the accuracy is only reduced by 0.77\%. On the MNIST data set, the new circuit even leads to improved accuracy.},
  archive      = {J_NN},
  author       = {Zheng Yan and Jiadong Chen and Rui Hu and Tingwen Huang and Yiran Chen and Shiping Wen},
  doi          = {10.1016/j.neunet.2020.04.025},
  journal      = {Neural Networks},
  pages        = {142-149},
  shortjournal = {Neural Netw.},
  title        = {Training memristor-based multilayer neuromorphic networks with SGD, momentum and adaptive learning rates},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A unified robust framework for multi-view feature extraction
with l2,1-norm constraint. <em>NN</em>, <em>128</em>, 126–141. (<a
href="https://doi.org/10.1016/j.neunet.2020.04.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view feature extraction methods mainly focus on exploiting the consistency and complementary information between multi-view samples, and most of the current methods apply the F-norm or L2-norm as the metric, which are sensitive to the outliers or noises. In this paper, based on L2,1-norm, we propose a unified robust feature extraction framework, which includes four special multi-view feature extraction methods, and extends the state-of-art methods to a more generalized form. The proposed methods are less sensitive to outliers or noises. An efficient iterative algorithm is designed to solve L2,1-norm based methods. Comprehensive analyses, such as convergence analysis , rotational invariance analysis and relationship between our methods and previous F-norm based methods illustrate the effectiveness of our proposed methods. Experiments on two artificial datasets and six real datasets demonstrate that the proposed L2,1-norm based methods have better performance than the related methods.},
  archive      = {J_NN},
  author       = {Jinxin Zhang and Liming Liu and Ling Zhen and Ling Jing},
  doi          = {10.1016/j.neunet.2020.04.024},
  journal      = {Neural Networks},
  pages        = {126-141},
  shortjournal = {Neural Netw.},
  title        = {A unified robust framework for multi-view feature extraction with l2,1-norm constraint},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-projection of unequal dimension optimal transport
theory for generative adversary networks. <em>NN</em>, <em>128</em>,
107–125. (<a
href="https://doi.org/10.1016/j.neunet.2020.04.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a major step forward in machine learning , generative adversarial networks (GANs) employ the Wasserstein distance as a metric between the generative distribution and target data distribution, and thus can be viewed as optimal transport (OT) problems to reflect the underlying geometry of the probability distribution. However, the unequal dimensions between the source random distribution and the target data, result in often instability in the training processes, and lack of diversity in the generative images. To resolve the challenges, we propose here a multiple-projection approach, to project the source and target probability measures into multiple different low-dimensional subspaces. Moreover, we show that the original problem can be transformed into a variant multi-marginal OT problem, and we provide the explicit properties of the solutions. In addition, we employ parameterized approximation for the objective, and study the corresponding differentiability and convergence properties , ensuring that the problem can indeed be computed.},
  archive      = {J_NN},
  author       = {Judy Yangjun Lin and Shaoyan Guo and Longhan Xie and Gu Xu},
  doi          = {10.1016/j.neunet.2020.04.029},
  journal      = {Neural Networks},
  pages        = {107-125},
  shortjournal = {Neural Netw.},
  title        = {Multi-projection of unequal dimension optimal transport theory for generative adversary networks},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep multi-critic network for accelerating policy learning
in multi-agent environments. <em>NN</em>, <em>128</em>, 97–106. (<a
href="https://doi.org/10.1016/j.neunet.2020.04.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans live among other humans, not in isolation. Therefore, the ability to learn and behave in multi-agent environments is essential for any autonomous system that intends to interact with people. Due to the presence of multiple simultaneous learners in a multi-agent learning environment, the Markov assumption used for single-agent environments is not tenable, necessitating the development of new Policy Learning algorithms. Recent Actor–Critic algorithms proposed for multi-agent environments, such as Multi-Agent Deep Deterministic Policy Gradients and Counterfactual Multi-Agent Policy Gradients, find a way to use the same mathematical framework as single agent environments by augmenting the Critic with extra information. However, this extra information can slow down the learning process and afflict the Critic with Curse of Dimensionality . To combat this, we propose a novel Deep Neural Network configuration called Deep Multi-Critic Network. This architecture works by taking a weighted sum over the outputs of multiple critic networks of varying complexity and size. The configuration was tested on data collected from a real-world multi-agent environment. The results illustrate that by using Deep Multi-Critic Network, less data is needed to reach the same level of performance as when not using the configuration. This suggests that as the configuration learns faster from less data, then the Critic may be able to learn Q-values faster, accelerating Actor training as well.},
  archive      = {J_NN},
  author       = {Joosep Hook and Varuna De Silva and Ahmet Kondoz},
  doi          = {10.1016/j.neunet.2020.04.023},
  journal      = {Neural Networks},
  pages        = {97-106},
  shortjournal = {Neural Netw.},
  title        = {Deep multi-critic network for accelerating policy learning in multi-agent environments},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). BPGAN: Bidirectional CT-to-MRI prediction using
multi-generative multi-adversarial nets with spectral normalization and
localization. <em>NN</em>, <em>128</em>, 82–96. (<a
href="https://doi.org/10.1016/j.neunet.2020.05.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Magnetic Resonance Imaging (MRI) and Computed Tomography (CT) are widely used detection technology in screening, diagnosis, and image-guided therapy for both clinical and research. However, CT imposes ionizing radiation to patients during acquisition. Compared to CT, MRI is much safer and does not involve any radiations, but it is more expensive and has prolonged acquisition time. Therefore, it is necessary to estimate one modal image from another given modal image of the same subject for the case of radiotherapy planning. Considering that there is currently no bidirectional prediction model between MRI and CT images, we propose a bidirectional prediction by using multi-generative multi-adversarial nets (BPGAN) for the prediction of any modal from another modal image in paired and unpaired fashion. In BPGAN, two nonlinear maps are learned by projecting same pathological features from one domain to another with cycle consistency strategy. Technologically, pathological prior information is introduced to constrain the feature generation to attack the potential risk of pathological variance, and edge retention metric is adopted to preserve geometrically distortion and anatomical structure. Algorithmically, spectral normalization is designed to control the performance of discriminator and to make predictor learn better and faster, and the localization is proposed to impose regularizer on predictor to reduce generalization error . Experimental results show that BPGAN generates better predictions than recently state-of-the-art methods. Specifically, BPGAN achieves average increment of MAE 33.2\% and 37.4\%, and SSIM 24.5\% and 44.6\% on two baseline datasets than comparisons.},
  archive      = {J_NN},
  author       = {Liming Xu and Xianhua Zeng and He Zhang and Weisheng Li and Jianbo Lei and Zhiwei Huang},
  doi          = {10.1016/j.neunet.2020.05.001},
  journal      = {Neural Networks},
  pages        = {82-96},
  shortjournal = {Neural Netw.},
  title        = {BPGAN: Bidirectional CT-to-MRI prediction using multi-generative multi-adversarial nets with spectral normalization and localization},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep learning from label proportions with labeled samples.
<em>NN</em>, <em>128</em>, 73–81. (<a
href="https://doi.org/10.1016/j.neunet.2020.04.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from label proportions (LLP), where the training data is in form of bags, and only the proportions of classes in each bag are available, has attracted wide interest in machine learning community. In general, most LLP algorithms adopt random sampling to obtain the proportional information of different categories, which correspondingly obtains some labeled samples in each bag. However, LLP training process always fails to leverage these labeled samples, which may contain essential data distribution information. To address this issue, in this paper, we propose end-to-end LLP solver based on convolutional neural networks (ConvNets), called LLP with labeled samples (LLP-LS). First, we reshape the cross entropy loss in ConvNets, so that it can combine the proportional information and labeled samples in each bag. Second, in order to comply with the training data in a bag manner, ADAM based on batch is employed to train LLP-LS. Hence, the batch size in training process is in accordance with the bag size. Compared with up-to-date methods on multi-class problem, our algorithm can obtain the state-of-the-art on several image datasets.},
  archive      = {J_NN},
  author       = {Yong Shi and Jiabin Liu and Bo Wang and Zhiquan Qi and YingJie Tian},
  doi          = {10.1016/j.neunet.2020.04.026},
  journal      = {Neural Networks},
  pages        = {73-81},
  shortjournal = {Neural Netw.},
  title        = {Deep learning from label proportions with labeled samples},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust image classification against adversarial attacks
using elastic similarity measures between edge count sequences.
<em>NN</em>, <em>128</em>, 61–72. (<a
href="https://doi.org/10.1016/j.neunet.2020.04.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to their unprecedented capacity to learn patterns from raw data, deep neural networks have become the de facto modeling choice to address complex machine learning tasks. However, recent works have emphasized the vulnerability of deep neural networks when being fed with intelligently manipulated adversarial data instances tailored to confuse the model. In order to overcome this issue, a major effort has been made to find methods capable of making deep learning models robust against adversarial inputs. This work presents a new perspective for improving the robustness of deep neural networks in image classification . In computer vision scenarios, adversarial images are crafted by manipulating legitimate inputs so that the target classifier is eventually fooled, but the manipulation is not visually distinguishable by an external observer. The reason for the imperceptibility of the attack is that the human visual system fails to detect minor variations in color space , but excels at detecting anomalies in geometric shapes . We capitalize on this fact by extracting color gradient features from input images at multiple sensitivity levels to detect possible manipulations. We resort to a deep neural classifier to predict the category of unseen images, whereas a discrimination model analyzes the extracted color gradient features with time series techniques to determine the legitimacy of input images. The performance of our method is assessed over experiments comprising state-of-the-art techniques for crafting adversarial attacks . Results corroborate the increased robustness of the classifier when using our discrimination module, yielding drastically reduced success rates of adversarial attacks that operate on the whole image rather than on localized regions or around the existing shapes of the image. Future research is outlined towards improving the detection accuracy of the proposed method for more general attack strategies.},
  archive      = {J_NN},
  author       = {Izaskun Oregi and Javier Del Ser and Aritz Pérez and José A. Lozano},
  doi          = {10.1016/j.neunet.2020.04.030},
  journal      = {Neural Networks},
  pages        = {61-72},
  shortjournal = {Neural Netw.},
  title        = {Robust image classification against adversarial attacks using elastic similarity measures between edge count sequences},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automated classification of cells into multiple classes in
epithelial tissue of oral squamous cell carcinoma using transfer
learning and convolutional neural network. <em>NN</em>, <em>128</em>,
47–60. (<a href="https://doi.org/10.1016/j.neunet.2020.05.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of tissue of a tumor in the oral cavity is essential for the pathologist to ascertain its grading. Recent studies using biopsy images reveal computer-aided diagnosis for oral sub-mucous fibrosis (OSF) carried out using machine learning algorithms , but no research has yet been outlined for multi-class grading of oral squamous cell carcinoma (OSCC). Pertinently, with the advent of deep learning in digital imaging and computational aid in the diagnosis, multi-class classification of OSCC biopsy images can help in timely and effective prognosis and multi-modal treatment protocols for oral cancer patients , thus reducing the operational workload of pathologists while enhancing management of the disease. With this motivation, this study attempts to classify OSCC into its four classes as per the Broder’s system of histological grading. The study is conducted on oral biopsy images applying two methods: (i) through the application of transfer learning using pre-trained deep convolutional neural network (CNN) wherein four candidate pre-trained models, namely Alexnet, VGG-16, VGG-19 and Resnet-50, were chosen to find the most suitable model for our classification problem, and (ii) by a proposed CNN model. Although the highest classification accuracy of 92.15\% is achieved by Resnet-50 model, the experimental findings highlight that the proposed CNN model outperformed the transfer learning approaches displaying accuracy of 97.5\%. It can be concluded that the proposed CNN based multi-class grading method of OSCC could be used for diagnosis of patients with OSCC.},
  archive      = {J_NN},
  author       = {Navarun Das and Elima Hussain and Lipi B. Mahanta},
  doi          = {10.1016/j.neunet.2020.05.003},
  journal      = {Neural Networks},
  pages        = {47-60},
  shortjournal = {Neural Netw.},
  title        = {Automated classification of cells into multiple classes in epithelial tissue of oral squamous cell carcinoma using transfer learning and convolutional neural network},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Theory of adaptive SVD regularization for deep neural
networks. <em>NN</em>, <em>128</em>, 33–46. (<a
href="https://doi.org/10.1016/j.neunet.2020.04.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep networks can learn complex problems, however, they suffer from overfitting. To solve this problem, regularization methods have been proposed that are not adaptable to the dynamic changes in the training process. With a different approach, this paper presents a regularization method based on the Singular Value Decomposition (SVD) that adjusts the learning model adaptively. To this end, the overfitting can be evaluated by condition numbers of the synaptic matrices. When the overfitting is high, the matrices are substituted with their SVD approximations . Some theoretical results are derived to show the performance of this regularization method. It is proved that SVD approximation cannot solve overfitting after several iterations. Thus, a new Tikhonov term is added to the loss function to converge the synaptic weights to the SVD approximation of the best-found results. Following this approach, an Adaptive SVD Regularization (ASR) is proposed to adjust the learning model with respect to the dynamic training characteristics. ASR results are visualized to show how ASR overcomes overfitting. The different configurations of Convolutional Neural Networks (CNN) are implemented with different augmentation schemes to compare ASR with state-of-the-art regularization methods. The results show that on MNIST, F-MNIST, SVHN, CIFAR-10 and CIFAR-100, the accuracies of ASR are 99.4\%, 95.7\%, 97.1\%, 93.2\% and 55.6\%, respectively. Although ASR improves the overfitting and validation loss, its elapsed time is not significantly greater than the learning without regularization.},
  archive      = {J_NN},
  author       = {Mohammad Mahdi Bejani and Mehdi Ghatee},
  doi          = {10.1016/j.neunet.2020.04.021},
  journal      = {Neural Networks},
  pages        = {33-46},
  shortjournal = {Neural Netw.},
  title        = {Theory of adaptive SVD regularization for deep neural networks},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Performance metrics for online seizure prediction.
<em>NN</em>, <em>128</em>, 22–32. (<a
href="https://doi.org/10.1016/j.neunet.2020.04.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many recent studies on online seizure prediction from iEEG signal describe various prediction algorithms and their prediction performance. In contrast, this paper focuses on proper specification of system parameters, such as prediction period, prediction horizon and data-driven characterization of lead seizures. Whereas prediction performance clearly depends on these system parameters many researchers simply set the values of these parameters in an ad hoc manner. Our paper investigates the effect of these system parameters on online prediction performance, using both synthetic and real-life data sets. Therefore, meaningful comparison of methods/algorithms (for online seizure prediction) should consider proper specification of system parameters.},
  archive      = {J_NN},
  author       = {Hsiang-Han Chen and Vladimir Cherkassky},
  doi          = {10.1016/j.neunet.2020.04.022},
  journal      = {Neural Networks},
  pages        = {22-32},
  shortjournal = {Neural Netw.},
  title        = {Performance metrics for online seizure prediction},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Common stochastic inputs induce neuronal transient
synchronization with partial reset. <em>NN</em>, <em>128</em>, 13–21.
(<a href="https://doi.org/10.1016/j.neunet.2020.04.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuronal synchronization plays important roles in information encoding and transmission in the brain. Mathematical models of neurons have been widely used to simulate synchronization behavior and analyze its mechanisms. Common stochastic inputs are considered to be effective in facilitating synchronization. However, the mechanisms of how partial reset affects neuronal synchronization are still not well understood. In this paper, the synchronization of Stein’s model neurons with partial reset is studied. The differences in synchronization mechanisms between neurons with full reset and those with partial reset are analyzed, and the findings lead to the novel concept of transient synchronization. Furthermore, it is proven analytically that due to common stochastic inputs, Stein’s model neurons with different initial membrane potentials and partial reset achieve transient synchronization with probability 1. Additionally, a systematic numerical analysis is performed to explore the similarities and differences between full reset and partial reset regarding model parameters, synchronization time, and desynchronization behavior . Thus, partial reset is a powerful and flexible tool that facilitates neuronal synchronization while reserving the possibility of desynchronization. Our analysis also provides an alternative approach to analyze neurons of the integrate-and-fire family and a theoretical complement implying possible information encoding mechanisms in the brain.},
  archive      = {J_NN},
  author       = {Siyang Leng and Kazuyuki Aihara},
  doi          = {10.1016/j.neunet.2020.04.019},
  journal      = {Neural Networks},
  pages        = {13-21},
  shortjournal = {Neural Netw.},
  title        = {Common stochastic inputs induce neuronal transient synchronization with partial reset},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fixed-time synchronization of delayed cohen–grossberg neural
networks based on a novel sliding mode. <em>NN</em>, <em>128</em>, 1–12.
(<a href="https://doi.org/10.1016/j.neunet.2020.04.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper has discussed fixed-time synchronization of discontinuous Cohen–Grossberg neural networks with time-varying delays and matched disturbances based on sliding mode control technology. First, a novel sliding-mode surface is established. And, the dynamics on the sliding-mode surface can be achieved in the fixed time by employing the Gudermannian function. Then, considering the effect of delay, two different control schemes are introduced to ensure the fixed time reachability of the sliding mode. In addition, some useful criteria are given out for fixed-time synchronization of neural networks , and the setting time is formulated in a straightforward way. Finally, some examples and simulations are presented to verify the validity of the proposed results.},
  archive      = {J_NN},
  author       = {Jian Xiao and Zhigang Zeng and Ailong Wu and Shiping Wen},
  doi          = {10.1016/j.neunet.2020.04.020},
  journal      = {Neural Networks},
  pages        = {1-12},
  shortjournal = {Neural Netw.},
  title        = {Fixed-time synchronization of delayed Cohen–Grossberg neural networks based on a novel sliding mode},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020f). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>127</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(20)30173-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(20)30173-8},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020f). Current events. <em>NN</em>, <em>127</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(20)30172-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(20)30172-6},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {Current events},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A classification-based approach to semi-supervised
clustering with pairwise constraints. <em>NN</em>, <em>127</em>,
193–203. (<a
href="https://doi.org/10.1016/j.neunet.2020.04.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a neural network framework for semi-supervised clustering with pairwise (must-link or cannot-link) constraints. In contrast to existing approaches, we decompose semi-supervised clustering into two simpler classification tasks : the first stage uses a pair of Siamese neural networks to label the unlabeled pairs of points as must-link or cannot-link; the second stage uses the fully pairwise-labeled dataset produced by the first stage in a supervised neural-network-based clustering method . The proposed approach is motivated by the observation that binary classification (such as assigning pairwise relations) is usually easier than multi-class clustering with partial supervision. On the other hand, being classification-based, our method solves only well-defined classification problems, rather than less well specified clustering tasks . Extensive experiments on various datasets demonstrate the high performance of the proposed method.},
  archive      = {J_NN},
  author       = {Marek Śmieja and Łukasz Struski and Mário A.T. Figueiredo},
  doi          = {10.1016/j.neunet.2020.04.017},
  journal      = {Neural Networks},
  pages        = {193-203},
  shortjournal = {Neural Netw.},
  title        = {A classification-based approach to semi-supervised clustering with pairwise constraints},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DART: Domain-adversarial residual-transfer networks for
unsupervised cross-domain image classification. <em>NN</em>,
<em>127</em>, 182–192. (<a
href="https://doi.org/10.1016/j.neunet.2020.03.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accuracy of deep learning (e.g., convolutional neural networks) for an image classification task critically relies on the amount of labeled training data. Aiming to solve an image classification task on a new domain that lacks labeled data but gains access to cheaply available unlabeled data , unsupervised domain adaptation is a promising technique to boost the performance without incurring extra labeling cost, by assuming images from different domains share some invariant characteristics. In this paper, we propose a new unsupervised domain adaptation method named Domain-Adversarial Residual-Transfer (DART) learning of deep neural networks to tackle cross-domain image classification tasks. In contrast to the existing unsupervised domain adaption approaches, the proposed DART not only learns domain-invariant features via adversarial training , but also achieves robust domain-adaptive classification via a residual-transfer strategy, all in an end-to-end training framework. We evaluate the performance of the proposed method for cross-domain image classification tasks on several well-known benchmark data sets, in which our method clearly outperforms the state-of-the-art approaches.},
  archive      = {J_NN},
  author       = {Xianghong Fang and Haoli Bai and Ziyi Guo and Bin Shen and Steven Hoi and Zenglin Xu},
  doi          = {10.1016/j.neunet.2020.03.025},
  journal      = {Neural Networks},
  pages        = {182-192},
  shortjournal = {Neural Netw.},
  title        = {DART: Domain-adversarial residual-transfer networks for unsupervised cross-domain image classification},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Vulnerability of classifiers to evolutionary generated
adversarial examples. <em>NN</em>, <em>127</em>, 168–181. (<a
href="https://doi.org/10.1016/j.neunet.2020.04.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the vulnerability of machine learning models to adversarial examples and its implication for robustness and generalization properties. We propose an evolutionary algorithm that can generate adversarial examples for any machine learning model in the black-box attack scenario. This way, we can find adversarial examples without access to model’s parameters, only by querying the model at hand. We have tested a range of machine learning models including deep and shallow neural networks . Our experiments have shown that the vulnerability to adversarial examples is not only the problem of deep networks, but it spreads through various machine learning architectures. Rather, it depends on the type of computational units. Local units, such as Gaussian kernels , are less vulnerable to adversarial examples.},
  archive      = {J_NN},
  author       = {Petra Vidnerová and Roman Neruda},
  doi          = {10.1016/j.neunet.2020.04.015},
  journal      = {Neural Networks},
  pages        = {168-181},
  shortjournal = {Neural Netw.},
  title        = {Vulnerability of classifiers to evolutionary generated adversarial examples},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graph convolution networks with manifold regularization for
semi-supervised learning. <em>NN</em>, <em>127</em>, 160–167. (<a
href="https://doi.org/10.1016/j.neunet.2020.04.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent times, Graph Convolution Networks (GCN) have been proposed as a powerful tool for graph-based semi-supervised learning. In this paper, we introduce a model that enhances label propagation of Graph Convolution Networks (GCN). More precisely, we propose GCNs with Manifold Regularization (GCNMR). The objective function of the proposed GCNMR is composed by a supervised term and an unsupervised term. The supervised term enforces the fitting term between the predicted labels and the known labels. The unsupervised term imposes the smoothness of the predicted labels of the whole data samples. By learning a Graph Convolution Network with the proposed objective function, we are able to derive a more powerful semi-supervised learning. The proposed model retains the advantages of the classic GCN, yet it can improve it with no increase in time complexity. Experiments on three public image datasets show that the proposed model is superior to the GCN and several competing existing graph-based semi-supervised learning methods.},
  archive      = {J_NN},
  author       = {M. Tavassoli Kejani and F. Dornaika and H. Talebi},
  doi          = {10.1016/j.neunet.2020.04.016},
  journal      = {Neural Networks},
  pages        = {160-167},
  shortjournal = {Neural Netw.},
  title        = {Graph convolution networks with manifold regularization for semi-supervised learning},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Linear embedding by joint robust discriminant analysis and
inter-class sparsity. <em>NN</em>, <em>127</em>, 141–159. (<a
href="https://doi.org/10.1016/j.neunet.2020.04.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear Discriminant Analysis (LDA) and its variants are widely used as feature extraction methods. They have been used for different classification tasks . However, these methods have some limitations that need to be overcome. The main limitation is that the projection obtained by LDA does not provide a good interpretability for the features. In this paper, we propose a novel supervised method used for multi-class classification that simultaneously performs feature selection and extraction. The targeted projection transformation focuses on the most discriminant original features, and at the same time, makes sure that the transformed features (extracted features) belonging to each class have common sparsity . Our proposed method is called Robust Discriminant Analysis with Feature Selection and Inter-class Sparsity (RDA _ _ FSIS). The corresponding model integrates two types of sparsity . The first type is obtained by imposing the ℓ 2 , 1 ℓ2,1 constraint on the projection matrix in order to perform feature selection. The second type of sparsity is obtained by imposing the inter-class sparsity constraint used for ensuring a common sparsity structure in each class. An orthogonal matrix is also introduced in our model in order to guarantee that the extracted features can retain the main variance of the original data and thus improve the robustness to noise. The proposed method retrieves the LDA transformation by taking into account the two types of sparsity. Various experiments are conducted on several image datasets including faces, objects and digits. The projected features are used for multi-class classification. Obtained results show that the proposed method outperforms other competing methods by learning a more compact and discriminative transformation.},
  archive      = {J_NN},
  author       = {F. Dornaika and A. Khoder},
  doi          = {10.1016/j.neunet.2020.04.018},
  journal      = {Neural Networks},
  pages        = {141-159},
  shortjournal = {Neural Netw.},
  title        = {Linear embedding by joint robust discriminant analysis and inter-class sparsity},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Micro-cracks detection of solar cells surface via combining
short-term and long-term deep features. <em>NN</em>, <em>127</em>,
132–140. (<a
href="https://doi.org/10.1016/j.neunet.2020.04.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The machine vision based methods for micro-cracks detection of solar cells surface have become one of the main research directions with its efficiency and convenience. The existed methods are roughly classified into two categories: current viewing information based methods, prior knowledge based methods, however, the former usually adopt hand-designed features with poor generality and lacks the guidance of prior knowledge, the latter are usually implemented through the machine learning , and the generalization ability is also limited since the large-scale annotation dataset is scarce. To resolve above problems, a novel micro-cracks detection method via combining short-term and long-term deep features is proposed in this paper. The short-term deep features which represent the current viewing information are learned from the input image itself through stacked denoising auto encoder (SDAE), the long-term deep features which represent the prior knowledge are learned from a large number of natural scene images that people often see through convolutional neural networks (CNNs). The subjective and objective evaluations demonstrate that: 1) the performance of combining the short-term and long-term deep features is better than any of them alone, 2) the performance of proposed method is superior to the shallow learning based methods, 3) the proposed method can effectively detect various kinds of micro-cracks.},
  archive      = {J_NN},
  author       = {Xiaoliang Qian and Jing Li and Jinde Cao and Yuanyuan Wu and Wei Wang},
  doi          = {10.1016/j.neunet.2020.04.012},
  journal      = {Neural Networks},
  pages        = {132-140},
  shortjournal = {Neural Netw.},
  title        = {Micro-cracks detection of solar cells surface via combining short-term and long-term deep features},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Training of deep neural networks for the generation of
dynamic movement primitives. <em>NN</em>, <em>127</em>, 121–131. (<a
href="https://doi.org/10.1016/j.neunet.2020.04.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic movement primitives (DMPs) have proven to be an effective movement representation for motor skill learning. In this paper, we propose a new approach for training deep neural networks to synthesize dynamic movement primitives. The distinguishing property of our approach is that it can utilize a novel loss function that measures the physical distance between movement trajectories as opposed to measuring the distance between the parameters of DMPs that have no physical meaning. This was made possible by deriving differential equations that can be applied to compute the gradients of the proposed loss function, thus enabling an effective application of backpropagation to optimize the parameters of the underlying deep neural network. While the developed approach is applicable to any neural network architecture, it was evaluated on two different architectures based on encoder–decoder networks and convolutional neural networks. Our results show that the minimization of the proposed loss function leads to better results than when more conventional loss functions are used.},
  archive      = {J_NN},
  author       = {Rok Pahič and Barry Ridge and Andrej Gams and Jun Morimoto and Aleš Ude},
  doi          = {10.1016/j.neunet.2020.04.010},
  journal      = {Neural Networks},
  pages        = {121-131},
  shortjournal = {Neural Netw.},
  title        = {Training of deep neural networks for the generation of dynamic movement primitives},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dendrite p systems. <em>NN</em>, <em>127</em>, 110–120. (<a
href="https://doi.org/10.1016/j.neunet.2020.04.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It was recently found that dendrites are not just a passive channel . They can perform mixed computation of analog and digital signals, and therefore can be abstracted as information processors. Moreover, dendrites possess a feedback mechanism. Motivated by these computational and feedback characteristics, this article proposes a new variant of neural-like P systems , dendrite P (DeP) systems, where neurons simulate the computational function of dendrites and perform a firing–storing process instead of the storing–firing process in spiking neural P (SNP) systems. Moreover, the behavior of the neurons is characterized by dendrite rules that are abstracted by two characteristics of dendrites. Different from the usual firing rules in SNP systems, the firing of a dendrite rule is controlled by the states of the corresponding source neurons. Therefore, DeP systems can provide a collaborative control capability for neurons. We discuss the computational power of DeP systems. In particular, it is proven that DeP systems are Turing-universal number generating/accepting devices. Moreover, we construct a small universal DeP system consisting of 115 neurons for computing functions.},
  archive      = {J_NN},
  author       = {Hong Peng and Tingting Bao and Xiaohui Luo and Jun Wang and Xiaoxiao Song and Agustín Riscos-Núñez and Mario J. Pérez-Jiménez},
  doi          = {10.1016/j.neunet.2020.04.014},
  journal      = {Neural Networks},
  pages        = {110-120},
  shortjournal = {Neural Netw.},
  title        = {Dendrite p systems},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the boundary conditions of avoidance memory
reconsolidation: An attractor network perspective. <em>NN</em>,
<em>127</em>, 96–109. (<a
href="https://doi.org/10.1016/j.neunet.2020.04.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reconsolidation and extinction of aversive memories and their boundary conditions have been extensively studied. Knowing their network mechanisms may lead to the development of better strategies for the treatment of fear and anxiety-related disorders. In 2011, Osan et al. developed a computational model for exploring such phenomena based on attractor dynamics, Hebbian plasticity and synaptic degradation induced by prediction error. This model was able to explain, in a single formalism, experimental findings regarding the freezing behavior of rodents submitted to contextual fear conditioning. In 2017, through the study of inhibitory avoidance in rats, Radiske et al. showed that the previous knowledge of a context as non-aversive is a boundary condition for the reconsolidation of the shock memory subsequently experienced in that context. In the present work, by adapting the model of Osan et al. (2011) to simulate the experimental protocols of Radiske et al. (2017), we show that such boundary condition is compatible with the dynamics of an attractor network that supports synaptic labilization common to reconsolidation and extinction. Additionally, by varying parameters such as the levels of protein synthesis and degradation, we predict behavioral outcomes, and thus boundary conditions that can be tested experimentally.},
  archive      = {J_NN},
  author       = {Rodrigo M.M. Santiago and Adriano B.L. Tort},
  doi          = {10.1016/j.neunet.2020.04.013},
  journal      = {Neural Networks},
  pages        = {96-109},
  shortjournal = {Neural Netw.},
  title        = {On the boundary conditions of avoidance memory reconsolidation: An attractor network perspective},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Novel deep neural network based pattern field classification
architectures. <em>NN</em>, <em>127</em>, 82–95. (<a
href="https://doi.org/10.1016/j.neunet.2020.03.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Field classification is a new extension of traditional classification frameworks that attempts to utilize consistent information from a group of samples (termed fields). By forgoing the independent identically distributed (i.i.d.) assumption, field classification can achieve remarkably improved accuracy compared to traditional classification methods. Most studies of field classification have been conducted on traditional machine learning methods . In this paper, we propose integration with a Bayesian framework, for the first time, in order to extend field classification to deep learning and propose two novel deep neural network architectures: the Field Deep Perceptron (FDP) and the Field Deep Convolutional Neural Network (FDCNN). Specifically, we exploit a deep perceptron structure, typically a 6-layer structure, where the first 3 layers remove (learn) a ‘style’ from a group of samples to map them into a more discriminative space and the last 3 layers are trained to perform classification. For the FDCNN, we modify the AlexNet framework by adding style transformation layers within the hidden layers. We derive a novel learning scheme from a Bayesian framework and design a novel and efficient learning algorithm with guaranteed convergence for training the deep networks. The whole framework is interpreted with visualization features showing that the field deep neural network can better learn the style of a group of samples. Our developed models are also able to achieve transfer learning and learn transformations for newly introduced fields. We conduct extensive comparative experiments on benchmark data (including face, speech, and handwriting data) to validate our learning approach. Experimental results demonstrate that our proposed deep frameworks achieve significant improvements over other state-of-the-art algorithms, attaining new benchmark performance.},
  archive      = {J_NN},
  author       = {Kaizhu Huang and Shufei Zhang and Rui Zhang and Amir Hussain},
  doi          = {10.1016/j.neunet.2020.03.011},
  journal      = {Neural Networks},
  pages        = {82-95},
  shortjournal = {Neural Netw.},
  title        = {Novel deep neural network based pattern field classification architectures},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural memory plasticity for medical anomaly detection.
<em>NN</em>, <em>127</em>, 67–81. (<a
href="https://doi.org/10.1016/j.neunet.2020.04.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the domain of machine learning , Neural Memory Networks (NMNs) have recently achieved impressive results in a variety of application areas including visual question answering, trajectory prediction, object tracking , and language modelling . However, we observe that the attention based knowledge retrieval mechanisms used in current NMNs restrict them from achieving their full potential as the attention process retrieves information based on a set of static connection weights. This is suboptimal in a setting where there are vast differences among samples in the data domain; such as anomaly detection where there is no consistent criteria for what constitutes an anomaly. In this paper, we propose a plastic neural memory access mechanism which exploits both static and dynamic connection weights in the memory read, write and output generation procedures. We demonstrate the effectiveness and flexibility of the proposed memory model in three challenging anomaly detection tasks in the medical domain: abnormal EEG identification, MRI tumour type classification and schizophrenia risk detection in children. In all settings, the proposed approach outperforms the current state-of-the-art. Furthermore, we perform an in-depth analysis demonstrating the utility of neural plasticity for the knowledge retrieval process and provide evidence on how the proposed memory model generates sparse yet informative memory outputs.},
  archive      = {J_NN},
  author       = {Tharindu Fernando and Simon Denman and David Ahmedt-Aristizabal and Sridha Sridharan and Kristin R. Laurens and Patrick Johnston and Clinton Fookes},
  doi          = {10.1016/j.neunet.2020.04.011},
  journal      = {Neural Networks},
  pages        = {67-81},
  shortjournal = {Neural Netw.},
  title        = {Neural memory plasticity for medical anomaly detection},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generative adversarial networks are special cases of
artificial curiosity (1990) and also closely related to predictability
minimization (1991). <em>NN</em>, <em>127</em>, 58–66. (<a
href="https://doi.org/10.1016/j.neunet.2020.04.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I review unsupervised or self-supervised neural networks playing minimax games in game-theoretic settings: (i) Artificial Curiosity (AC, 1990) is based on two such networks. One network learns to generate a probability distribution over outputs, the other learns to predict effects of the outputs. Each network minimizes the objective function maximized by the other. (ii) Generative Adversarial Networks (GANs, 2010-2014) are an application of AC where the effect of an output is 1 if the output is in a given set, and 0 otherwise. (iii) Predictability Minimization (PM, 1990s) models data distributions through a neural encoder that maximizes the objective function minimized by a neural predictor of the code components. I correct a previously published claim that PM is not based on a minimax game.},
  archive      = {J_NN},
  author       = {Jürgen Schmidhuber},
  doi          = {10.1016/j.neunet.2020.04.008},
  journal      = {Neural Networks},
  pages        = {58-66},
  shortjournal = {Neural Netw.},
  title        = {Generative adversarial networks are special cases of artificial curiosity (1990) and also closely related to predictability minimization (1991)},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Further results on finite-time synchronization of delayed
inertial memristive neural networks via a novel analysis method.
<em>NN</em>, <em>127</em>, 47–57. (<a
href="https://doi.org/10.1016/j.neunet.2020.04.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel analysis method to investigate the finite-time synchronization (FTS) control problem of the drive–response inertial memristive neural networks (IMNNs) with mixed time-varying delays (MTVDs). Firstly, an improved control scheme is proposed under the delay-independent conditions, which can work even when the past state cannot be measured or the specific time delay function is unknown. Secondly, based on the assumption of bounded activation functions , we establish a new Lemma, which can effectively deal with the difficulties caused by memristive connection weights and MTVDs. Thirdly, by constructing a suitable Lyapunov functions and using a new inequality method, novel sufficient conditions to ensure the FTS for the discussed IMNNs are obtained. Compared with the existing results, our results obtained in a more general framework are more practical. Finally, some numerical simulations are given to substantiate the effectiveness of the theoretical results.},
  archive      = {J_NN},
  author       = {Lanfeng Hua and Shouming Zhong and Kaibo Shi and Xiaojun Zhang},
  doi          = {10.1016/j.neunet.2020.04.009},
  journal      = {Neural Networks},
  pages        = {47-57},
  shortjournal = {Neural Netw.},
  title        = {Further results on finite-time synchronization of delayed inertial memristive neural networks via a novel analysis method},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Synchronization of coupled neural networks under mixed
impulsive effects: A novel delay inequality approach. <em>NN</em>,
<em>127</em>, 38–46. (<a
href="https://doi.org/10.1016/j.neunet.2020.04.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the synchronization problems of an array of coupled neural networks with mixed impulses are considered. Here mixed impulses contain desynchronizing delay-free impulses, synchronizing delay-free impulses, desynchronizing delayed impulses and synchronizing delayed impulses. A novel concept named average delayed impulsive gain is proposed to quantify the effects of mixed impulses. Besides, we establish a delayed impulsive differential inequality which extends famous Halanay inequality, and apply it to study the synchronization problems of delayed neural networks with mixed impulses. It is interesting to notice that both delay-free impulses and delayed impulses can contribute to the synchronization of coupled neural networks. Meanwhile, we also discuss the synchronization of neural networks only with delay-dependent impulses. Some sufficient conditions are derived to ensure the exponential synchronization of delayed neural networks. Finally, some numerical examples are provided to illustrate the validity and superiority of the obtained results.},
  archive      = {J_NN},
  author       = {Yaqi Wang and Jianquan Lu and Xiaodi Li and Jinling Liang},
  doi          = {10.1016/j.neunet.2020.04.002},
  journal      = {Neural Networks},
  pages        = {38-46},
  shortjournal = {Neural Netw.},
  title        = {Synchronization of coupled neural networks under mixed impulsive effects: A novel delay inequality approach},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Randomized sketches for kernel CCA. <em>NN</em>,
<em>127</em>, 29–37. (<a
href="https://doi.org/10.1016/j.neunet.2020.04.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel canonical correlation analysis (KCCA) is a popular tool as a nonlinear extension of canonical correlation analysis . Consistency and optimal convergence rate have been established in the literature. However, the time complexity of KCCA scales as O ( n 3 ) O(n3) and is thus prohibitive when n n is large. We propose an m m -dimensional randomized sketches approach for KCCA with m m&amp;lt;&amp;lt;n , based on the recent work on randomized sketches for kernel ridge regression (KRR). Technically we establish our theoretical results relying on an interesting connection between KCCA and KRR by utilizing a novel “duality tracking” device that alternates between the infinite-dimensional operator-theory-based view of KCCA and the finite-dimensional kernel-matrix-based view.},
  archive      = {J_NN},
  author       = {Heng Lian and Fode Zhang and Wenqi Lu},
  doi          = {10.1016/j.neunet.2020.04.006},
  journal      = {Neural Networks},
  pages        = {29-37},
  shortjournal = {Neural Netw.},
  title        = {Randomized sketches for kernel CCA},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generative adversarial networks with decoder–encoder output
noises. <em>NN</em>, <em>127</em>, 19–28. (<a
href="https://doi.org/10.1016/j.neunet.2020.04.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, research on image generation has been developing very fast. The generative adversarial network (GAN) emerges as a promising framework, which uses adversarial training to improve the generative ability of its generator. However, since GAN and most of its variants use randomly sampled noises as the input of their generators, they have to learn a mapping function from a whole random distribution to the image manifold. As the structures of the random distribution and the image manifold are generally different, this results in GAN and its variants difficult to train and converge. In this paper, we propose a novel deep model called generative adversarial networks with decoder–encoder output noises (DE-GANs), which take advantage of both the adversarial training and the variational Bayesian inference to improve GAN and its variants on image generation performances. DE-GANs use a pre-trained decoder–encoder architecture to map the random noise vectors to informative ones and feed them to the generator of the adversarial networks. Since the decoder–encoder architecture is trained with the same data set as the generator, its output vectors, as the inputs of the generator, could carry the intrinsic distribution information of the training images, which greatly improves the learnability of the generator and the quality of the generated images. Extensive experiments demonstrate the effectiveness of the proposed model, DE-GANs.},
  archive      = {J_NN},
  author       = {Guoqiang Zhong and Wei Gao and Yongbin Liu and Youzhao Yang and Da-Han Wang and Kaizhu Huang},
  doi          = {10.1016/j.neunet.2020.04.005},
  journal      = {Neural Networks},
  pages        = {19-28},
  shortjournal = {Neural Netw.},
  title        = {Generative adversarial networks with decoder–encoder output noises},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Finite-time synchronization of memristor neural networks via
interval matrix method. <em>NN</em>, <em>127</em>, 7–18. (<a
href="https://doi.org/10.1016/j.neunet.2020.04.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the finite-time synchronization problems of two types of driven-response memristor neural networks (MNNs) without time-delay and with time-varying delays are investigated via interval matrix method, respectively. Based on interval matrix transformation , the driven-response MNNs are transformed into a kind of system with interval parameters, which is different from the previous research approaches. Several sufficient conditions in terms of linear matrix inequalities (LMIs) are driven to guarantee finite-time synchronization for MNNs. Correspondingly, two types of nonlinear feedback controllers are designed. Meanwhile, the upper-bounded of the settling time functions are estimated. Finally, two numerical examples with simulations are given to illustrate the correctness of the theoretical results and the effectiveness of the proposed controllers.},
  archive      = {J_NN},
  author       = {Fei Wei and Guici Chen and Wenbo Wang},
  doi          = {10.1016/j.neunet.2020.04.003},
  journal      = {Neural Networks},
  pages        = {7-18},
  shortjournal = {Neural Netw.},
  title        = {Finite-time synchronization of memristor neural networks via interval matrix method},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A tight upper bound on the generalization error of
feedforward neural networks. <em>NN</em>, <em>127</em>, 1–6. (<a
href="https://doi.org/10.1016/j.neunet.2020.04.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We give a tight upper bound on the generalization error of 2-times continuously differentiable feedforward neural networks if the loss function is 2-times continuously differentiable as well. The upper bound consists of two terms, the first term indicates how well the empirical error estimates the error at the mean of the sample space. The second term indicates the expected sensitivity of the error to the changes of the input. Furthermore, we provide explicit formulas for the calculation of the second term.},
  archive      = {J_NN},
  author       = {Aydin Sarraf},
  doi          = {10.1016/j.neunet.2020.04.001},
  journal      = {Neural Networks},
  pages        = {1-6},
  shortjournal = {Neural Netw.},
  title        = {A tight upper bound on the generalization error of feedforward neural networks},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020g). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>126</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(20)30144-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(20)30144-1},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020g). Current events. <em>NN</em>, <em>126</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(20)30143-X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(20)30143-X},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {Current events},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic detection of tympanic membrane and middle ear
infection from oto-endoscopic images via convolutional neural networks.
<em>NN</em>, <em>126</em>, 384–394. (<a
href="https://doi.org/10.1016/j.neunet.2020.03.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural networks (CNNs), a popular type of deep neural network, have been actively applied to image recognition, object detection, object localization , semantic segmentation , and object instance segmentation . Accordingly, the applicability of deep learning to the analysis of medical images has increased. This paper presents a novel application of state-of-the-art CNN models, such as DenseNet, to the automatic detection of the tympanic membrane (TM) and middle ear (ME) infection. We collected 2,484 oto-endoscopic images (OEIs) and classified them into one of three categories: normal, chronic otitis media (COM) with TM perforation , and otitis media with effusion (OME). Our results indicate that CNN models have significant potential for the automatic recognition of TM and ME infections, demonstrating a competitive accuracy of 95\% in classifying TM and middle ear effusion (MEE) from OEIs. In addition to accuracy measurement, our approach achieves nearly perfect measures of 0.99 in terms of the average area under the receiver operating characteristics curve (AUROC). All these results indicate robust performance when recognizing TM and ME effusions in OEIs. Visualization through a class activation mapping (CAM) heatmap demonstrates that our proposed model performs prediction based on the correct region of OEIs. All these outcomes ensure the reliability of our method; hence, the study can aid otolaryngologists and primary care physicians in real-world scenarios.},
  archive      = {J_NN},
  author       = {Mohammad Azam Khan and Soonwook Kwon and Jaegul Choo and Seok Min Hong and Sung Hun Kang and Il-Ho Park and Sung Kyun Kim and Seok Jin Hong},
  doi          = {10.1016/j.neunet.2020.03.023},
  journal      = {Neural Networks},
  pages        = {384-394},
  shortjournal = {Neural Netw.},
  title        = {Automatic detection of tympanic membrane and middle ear infection from oto-endoscopic images via convolutional neural networks},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hyperlink regression via bregman divergence. <em>NN</em>,
<em>126</em>, 362–383. (<a
href="https://doi.org/10.1016/j.neunet.2020.03.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A collection of U ( ∈ N ) U(∈N) data vectors is called a U U - tuple , and the association strength among the vectors of a tuple is termed as the hyperlink weight , that is assumed to be symmetric with respect to permutation of the entries in the index. We herein propose Bregman hyperlink regression (BHLR), which learns a user-specified symmetric similarity function such that it predicts the tuple’s hyperlink weight from data vectors stored in the U U -tuple. BHLR is a simple and general framework for hyper-relational learning, that minimizes Bregman-divergence (BD) between the hyperlink weights and estimated similarities defined for the corresponding tuples; BHLR encompasses various existing methods, such as logistic regression ( U = 1 U=1 ), Poisson regression ( U = 1 U=1 ), link prediction ( U = 2 U=2 ), and those for representation learning , such as graph embedding ( U = 2 U=2 ), matrix factorization ( U = 2 U=2 ), tensor factorization ( U ≥ 2 U≥2 ), and their variants equipped with arbitrary BD. Nonlinear functions (e.g., neural networks), can be employed for the similarity functions. However, there are theoretical challenges such that some of different tuples of BHLR may share data vectors therein, unlike the i.i.d. setting of classical regression. We address these theoretical issues, and proved that BHLR equipped with arbitrary BD and U ∈ N U∈N is (P-1) statistically consistent, that is, it asymptotically recovers the underlying true conditional expectation of hyperlink weights given data vectors, and (P-2) computationally tractable, that is, it is efficiently computed by stochastic optimization algorithms using a novel generalized minibatch sampling procedure for hyper-relational data. Consequently, theoretical guarantees for BHLR including several existing methods, that have been examined experimentally, are provided in a unified manner.},
  archive      = {J_NN},
  author       = {Akifumi Okuno and Hidetoshi Shimodaira},
  doi          = {10.1016/j.neunet.2020.03.026},
  journal      = {Neural Networks},
  pages        = {362-383},
  shortjournal = {Neural Netw.},
  title        = {Hyperlink regression via bregman divergence},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). A fast conformal predictive system with regularized extreme
learning machine. <em>NN</em>, <em>126</em>, 347–361. (<a
href="https://doi.org/10.1016/j.neunet.2020.03.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A conformal predictive system(CPS) is based on the learning framework of conformal prediction , which outputs cumulative distribution functions(CDFs) for labels in regression problems . The CDFs output by a CPS provide useful information for users, as they not only provide probability for the events related to the test labels, but also can be transformed to prediction intervals with the corresponding quantiles . Moreover, CPSs have the property of validity since the distributions and intervals they output have statistical compatibility with the realizations. This property is very useful for many risk-sensitive applications such as financial time series forecast and weather forecast. However, as based on conformal predictors , CPSs inherit the computational issue. To build a fast CPS, in this paper, we propose a CPS with regularized extreme learning machine as the underlying algorithm. To be specific, we combine the leave-one-out cross-conformal predictive system(Leave-One-Out CCPS), a variant of the original CPS, with regularized extreme learning machine(RELM), which is named as LOO-CCPS-RELM. We analyse the computational complexity of it and prove its asymptotic validity based on some regularity assumptions. We also prove that the error rate of the prediction interval output by LOO-CCPS-RELM is under control in the asymptotic setting. Experiments with 20 public data sets were conducted to test LOO-CCPS-RELM and the results showed that LOO-CCPS-RELM is empirically valid and compared favourably with the other CPSs.},
  archive      = {J_NN},
  author       = {Di Wang and Ping Wang and Yue Yuan and Pingping Wang and Junzhi Shi},
  doi          = {10.1016/j.neunet.2020.03.022},
  journal      = {Neural Networks},
  pages        = {347-361},
  shortjournal = {Neural Netw.},
  title        = {A fast conformal predictive system with regularized extreme learning machine},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-view projected clustering with graph learning.
<em>NN</em>, <em>126</em>, 335–346. (<a
href="https://doi.org/10.1016/j.neunet.2020.03.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph based multi-view learning is well known due to its effectiveness and good clustering performance. However, most existing methods directly construct graph from original high-dimensional data which always contain redundancy, noise and outlying entries in real applications, resulting in unreliable and inaccurate graph. Moreover, they do not effectively select some useful features which are important for graph learning and clustering. To solve these limits, we propose a novel model that combines dimensionality reduction, manifold structure learning and feature selection into a framework. We map high-dimensional data into low-dimensional space to reduce the complexity of the algorithm and reduce the effect of noise and redundance. Therefore, we can adaptively learn a more accurate graph. Further more, ℓ 21 ℓ21 -norm regularization is adopted to adaptively select some important features which help improve clustering performance. Finally, an efficiently algorithm is proposed to solve the optimal solution. Extensive experimental results on some benchmark datasets demonstrate the superiority of the proposed method.},
  archive      = {J_NN},
  author       = {Quanxue Gao and Zhizhen Wan and Ying Liang and Qianqian Wang and Yang Liu and Ling Shao},
  doi          = {10.1016/j.neunet.2020.03.020},
  journal      = {Neural Networks},
  pages        = {335-346},
  shortjournal = {Neural Netw.},
  title        = {Multi-view projected clustering with graph learning},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automata complete computation with hodgkin–huxley neural
networks composed of synfire rings. <em>NN</em>, <em>126</em>, 312–334.
(<a href="https://doi.org/10.1016/j.neunet.2020.03.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synfire rings are neural circuits capable of conveying synchronous, temporally precise and self-sustained activities in a robust manner. We propose a cell assembly based paradigm for abstract neural computation centered on the concept of synfire rings. More precisely, we empirically show that Hodgkin–Huxley neural networks modularly composed of synfire rings are automata complete. We provide an algorithmic construction which, starting from any given finite state automaton , builds a corresponding Hodgkin–Huxley neural network modularly composed of synfire rings and capable of simulating it. We illustrate the correctness of the construction on two specific examples. We further analyze the stability and robustness of the construction as a function of changes in the ring topologies as well as with respect to cell death and synaptic failure mechanisms, respectively. These results establish the possibility of achieving abstract computation with bio-inspired neural networks. They might constitute a theoretical ground for the realization of biological neural computers.},
  archive      = {J_NN},
  author       = {Jérémie Cabessa and Aubin Tchaptchet},
  doi          = {10.1016/j.neunet.2020.03.019},
  journal      = {Neural Networks},
  pages        = {312-334},
  shortjournal = {Neural Netw.},
  title        = {Automata complete computation with Hodgkin–Huxley neural networks composed of synfire rings},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extracting boolean and probabilistic rules from trained
neural networks. <em>NN</em>, <em>126</em>, 300–311. (<a
href="https://doi.org/10.1016/j.neunet.2020.03.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents two approaches to extracting rules from a trained neural network consisting of linear threshold functions. The first one leads to an algorithm that extracts rules in the form of Boolean functions . Compared with an existing one, this algorithm outputs much more concise rules if the threshold functions correspond to 1-decision lists, majority functions, or certain combinations of these. The second one extracts probabilistic rules representing relations between some of the input variables and the output using a dynamic programming algorithm . The algorithm runs in pseudo-polynomial time if each hidden layer has a constant number of neurons. We demonstrate the effectiveness of these two approaches by computational experiments.},
  archive      = {J_NN},
  author       = {Pengyu Liu and Avraham A. Melkman and Tatsuya Akutsu},
  doi          = {10.1016/j.neunet.2020.03.024},
  journal      = {Neural Networks},
  pages        = {300-311},
  shortjournal = {Neural Netw.},
  title        = {Extracting boolean and probabilistic rules from trained neural networks},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning visual features under motion invariance.
<em>NN</em>, <em>126</em>, 275–299. (<a
href="https://doi.org/10.1016/j.neunet.2020.03.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans are continuously exposed to a stream of visual data with a natural temporal structure . However, most successful computer vision algorithms work at image level, completely discarding the precious information carried by motion. In this paper, we claim that processing visual streams naturally leads to formulate the motion invariance principle , which enables the construction of a new theory of learning that originates from variational principles , just like in physics. Such principled approach is well suited for a discussion on a number of interesting questions that arise in vision, and it offers a well-posed computational scheme for the discovery of convolutional filters over the retina. Differently from traditional convolutional networks , which need massive supervision, the proposed theory offers a truly new scenario for the unsupervised processing of video signals, where features are extracted in a multi-layer architecture with motion invariance. While the theory enables the implementation of novel computer vision systems, it also sheds light on the role of information-based principles to drive possible biological solutions.},
  archive      = {J_NN},
  author       = {Alessandro Betti and Marco Gori and Stefano Melacci},
  doi          = {10.1016/j.neunet.2020.03.013},
  journal      = {Neural Networks},
  pages        = {275-299},
  shortjournal = {Neural Netw.},
  title        = {Learning visual features under motion invariance},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Crowding in humans is unlike that in convolutional neural
networks. <em>NN</em>, <em>126</em>, 262–274. (<a
href="https://doi.org/10.1016/j.neunet.2020.03.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object recognition is a primary function of the human visual system . It has recently been claimed that the highly successful ability to recognise objects in a set of emergent computer vision systems—Deep Convolutional Neural Networks (DCNNs)—can form a useful guide to recognition in humans. To test this assertion, we systematically evaluated visual crowding, a dramatic breakdown of recognition in clutter, in DCNNs and compared their performance to extant research in humans. We examined crowding in three architectures of DCNNs with the same methodology as that used among humans. We manipulated multiple stimulus factors including inter-letter spacing, letter colour, size, and flanker location to assess the extent and shape of crowding in DCNNs. We found that crowding followed a predictable pattern across architectures that was different from that in humans. Some characteristic hallmarks of human crowding, such as invariance to size, the effect of target-flanker similarity, and confusions between target and flanker identities, were completely missing, minimised or even reversed. These data show that DCNNs, while proficient in object recognition, likely achieve this competence through a set of mechanisms that are distinct from those in humans. They are not necessarily equivalent models of human or primate object recognition and caution must be exercised when inferring mechanisms derived from their operation.},
  archive      = {J_NN},
  author       = {Ben Lonnqvist and Alasdair D.F. Clarke and Ramakrishna Chakravarthi},
  doi          = {10.1016/j.neunet.2020.03.021},
  journal      = {Neural Networks},
  pages        = {262-274},
  shortjournal = {Neural Netw.},
  title        = {Crowding in humans is unlike that in convolutional neural networks},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-way backpropagation for training compact deep neural
networks. <em>NN</em>, <em>126</em>, 250–261. (<a
href="https://doi.org/10.1016/j.neunet.2020.03.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth is one of the key factors behind the success of convolutional neural networks (CNNs). Since ResNet (He et al., 2016), we are able to train very deep CNNs as the gradient vanishing issue has been largely addressed by the introduction of skip connections. However, we observe that, when the depth is very large, the intermediate layers (especially shallow layers) may fail to receive sufficient supervision from the loss due to severe transformation through long backpropagation path. As a result, the representation power of intermediate layers can be very weak and the model becomes very redundant with limited performance. In this paper, we first investigate the supervision vanishing issue in existing backpropagation (BP) methods. And then, we propose to address it via an effective method, called Multi-way BP (MW-BP), which relies on multiple auxiliary losses added to the intermediate layers of the network. The proposed MW-BP method can be applied to most deep architectures with slight modifications, such as ResNet and MobileNet. Our method often gives rise to much more compact models (denoted by “Mw+Architecture”) than existing methods. For example, MwResNet-44 with 44 layers performs better than ResNet-110 with 110 layers on CIFAR-10 and CIFAR-100. More critically, the resultant models even outperform the light models obtained by state-of-the-art model compression methods . Last, our method inherently produces multiple compact models with different depths at the same time, which is helpful for model selection. Extensive experiments on both image classification and face recognition demonstrate the superiority of the proposed method.},
  archive      = {J_NN},
  author       = {Yong Guo and Jian Chen and Qing Du and Anton Van Den Hengel and Qinfeng Shi and Mingkui Tan},
  doi          = {10.1016/j.neunet.2020.03.001},
  journal      = {Neural Networks},
  pages        = {250-261},
  shortjournal = {Neural Netw.},
  title        = {Multi-way backpropagation for training compact deep neural networks},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning in the machine: To share or not to share?
<em>NN</em>, <em>126</em>, 235–249. (<a
href="https://doi.org/10.1016/j.neunet.2020.03.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weight-sharing is one of the pillars behind Convolutional Neural Networks and their successes. However, in physical neural systems such as the brain, weight-sharing is implausible. This discrepancy raises the fundamental question of whether weight-sharing is necessary. If so, to which degree of precision? If not, what are the alternatives? The goal of this study is to investigate these questions, primarily through simulations where the weight-sharing assumption is relaxed. Taking inspiration from neural circuitry , we explore the use of Free Convolutional Networks and neurons with variable connection patterns. Using Free Convolutional Networks, we show that while weight-sharing is a pragmatic optimization approach, it is not a necessity in computer vision applications. Furthermore, Free Convolutional Networks match the performance observed in standard architectures when trained using properly translated data (akin to video). Under the assumption of translationally augmented data, Free Convolutional Networks learn translationally invariant representations that yield an approximate form of weight-sharing.},
  archive      = {J_NN},
  author       = {Jordan Ott and Erik Linstead and Nicholas LaHaye and Pierre Baldi},
  doi          = {10.1016/j.neunet.2020.03.016},
  journal      = {Neural Networks},
  pages        = {235-249},
  shortjournal = {Neural Netw.},
  title        = {Learning in the machine: To share or not to share?},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Brain MRI analysis using a deep learning based evolutionary
approach. <em>NN</em>, <em>126</em>, 218–234. (<a
href="https://doi.org/10.1016/j.neunet.2020.03.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional neural network (CNN) models have recently demonstrated impressive performance in medical image analysis. However, there is no clear understanding of why they perform so well, or what they have learned. In this paper, a three-dimensional convolutional neural network (3D-CNN) is employed to classify brain MRI scans into two predefined groups. In addition, a genetic algorithm based brain masking (GABM) method is proposed as a visualization technique that provides new insights into the function of the 3D-CNN. The proposed GABM method consists of two main steps. In the first step, a set of brain MRI scans is used to train the 3D-CNN. In the second step, a genetic algorithm (GA) is applied to discover knowledgeable brain regions in the MRI scans. The knowledgeable regions are those areas of the brain which the 3D-CNN has mostly used to extract important and discriminative features from them. For applying GA on the brain MRI scans, a new chromosome encoding approach is proposed. The proposed framework has been evaluated using ADNI (including 140 subjects for Alzheimer’s disease classification) and ABIDE (including 1000 subjects for Autism classification) brain MRI datasets. Experimental results show a 5-fold classification accuracy of 0.85 for the ADNI dataset and 0.70 for the ABIDE dataset. The proposed GABM method has extracted 6 to 65 knowledgeable brain regions in ADNI dataset (and 15 to 75 knowledgeable brain regions in ABIDE dataset). These regions are interpreted as the segments of the brain which are mostly used by the 3D-CNN to extract features for brain disease classification. Experimental results show that besides the model interpretability , the proposed GABM method has increased final performance of the classification model in some cases with respect to model parameters.},
  archive      = {J_NN},
  author       = {Hossein Shahamat and Mohammad Saniee Abadeh},
  doi          = {10.1016/j.neunet.2020.03.017},
  journal      = {Neural Networks},
  pages        = {218-234},
  shortjournal = {Neural Netw.},
  title        = {Brain MRI analysis using a deep learning based evolutionary approach},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Backpropagation algorithms and reservoir computing in
recurrent neural networks for the forecasting of complex spatiotemporal
dynamics. <em>NN</em>, <em>126</em>, 191–217. (<a
href="https://doi.org/10.1016/j.neunet.2020.02.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine the efficiency of Recurrent Neural Networks in forecasting the spatiotemporal dynamics of high dimensional and reduced order complex systems using Reservoir Computing (RC) and Backpropagation through time (BPTT) for gated network architectures . We highlight advantages and limitations of each method and discuss their implementation for parallel computing architectures . We quantify the relative prediction accuracy of these algorithms for the long-term forecasting of chaotic systems using as benchmarks the Lorenz-96 and the Kuramoto–Sivashinsky (KS) equations. We find that, when the full state dynamics are available for training, RC outperforms BPTT approaches in terms of predictive performance and in capturing of the long-term statistics , while at the same time requiring much less training time. However, in the case of reduced order data, large scale RC models can be unstable and more likely than the BPTT algorithms to diverge. In contrast, RNNs trained via BPTT show superior forecasting abilities and capture well the dynamics of reduced order systems. Furthermore, the present study quantifies for the first time the Lyapunov Spectrum of the KS equation with BPTT, achieving similar accuracy as RC. This study establishes that RNNs are a potent computational framework for the learning and forecasting of complex spatiotemporal systems.},
  archive      = {J_NN},
  author       = {P.R. Vlachas and J. Pathak and B.R. Hunt and T.P. Sapsis and M. Girvan and E. Ott and P. Koumoutsakos},
  doi          = {10.1016/j.neunet.2020.02.016},
  journal      = {Neural Networks},
  pages        = {191-217},
  shortjournal = {Neural Netw.},
  title        = {Backpropagation algorithms and reservoir computing in recurrent neural networks for the forecasting of complex spatiotemporal dynamics},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New optimization algorithms for neural network training
using operator splitting techniques. <em>NN</em>, <em>126</em>, 178–190.
(<a href="https://doi.org/10.1016/j.neunet.2020.03.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the following paper we present a new type of optimization algorithms adapted for neural network training. These algorithms are based upon sequential operator splitting technique for some associated dynamical systems . Furthermore, we investigate through numerical simulations the empirical rate of convergence of these iterative schemes toward a local minimum of the loss function, with some suitable choices of the underlying hyper-parameters. We validate the convergence of these optimizers using the results of the accuracy and of the loss function on the MNIST, MNIST-Fashion and CIFAR 10 classification datasets.},
  archive      = {J_NN},
  author       = {Cristian Daniel Alecsa and Titus Pinţa and Imre Boros},
  doi          = {10.1016/j.neunet.2020.03.018},
  journal      = {Neural Networks},
  pages        = {178-190},
  shortjournal = {Neural Netw.},
  title        = {New optimization algorithms for neural network training using operator splitting techniques},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Prediction of admission in pediatric emergency department
with deep neural networks and triage textual data. <em>NN</em>,
<em>126</em>, 170–177. (<a
href="https://doi.org/10.1016/j.neunet.2020.03.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emergency department (ED) overcrowding is a global condition that severely worsens attention to patients, increases clinical risks and affects hospital cost management. A correct and early prediction of ED’s admission is of high value and a motivation to adopt machine learning models. However, several of these studies do not consider data collected in textual form, which is a feature set that contains detailed information about patients and presents great potential for medical health care improvement. To this end, we propose and compare predictive models for admission that use both structured and unstructured data available at triage time. In total, our dataset comprised 499,853 pediatric ED’s presentations (with an admission rate of 5.76\%) of patients with age up to 18 years old observed over 3.5 years. Our best model consists of a 2-stage architecture with a deep neural network (DNN) to extract information from textual data followed by a gradient boosting classifier. This combined model achieved a value of 0.892 for the Area Under the Curve (AUC) in the test data. We highlight the importance of DNN-based text processing for better prediction, since the absence of text features resulted in AUC reduction of approximately two percentage points. Also, the feature importance of text was higher than that of the Manchester Triage System (MTS), which is a widely used risk classification protocol. These results suggest that activations from a trained DNN should be used in transfer learning setups in future studies.},
  archive      = {J_NN},
  author       = {Bruno P. Roquette and Hitoshi Nagano and Ernesto C. Marujo and Alexandre C. Maiorano},
  doi          = {10.1016/j.neunet.2020.03.012},
  journal      = {Neural Networks},
  pages        = {170-177},
  shortjournal = {Neural Netw.},
  title        = {Prediction of admission in pediatric emergency department with deep neural networks and triage textual data},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exponential synchronization of memristive neural networks
with time-varying delays via quantized sliding-mode control.
<em>NN</em>, <em>126</em>, 163–169. (<a
href="https://doi.org/10.1016/j.neunet.2020.03.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the paper, exponential synchronization issue is considered for memristive neural networks (MNNs) with time-varying delays via quantized sliding-mode algorithm. Quantized Sliding-mode controller is introduced to ensure the slave system can be exponentially synchronized with the host system via the super-twisting algorithm, which has been proved in the main results. Quantization function consists of uniform quantizer and logarithmic quantizer. Simulation results are given with comparisons between two quantizers in the end.},
  archive      = {J_NN},
  author       = {Bo Sun and Shengbo Wang and Yuting Cao and Zhenyuan Guo and Tingwen Huang and Shiping Wen},
  doi          = {10.1016/j.neunet.2020.03.014},
  journal      = {Neural Networks},
  pages        = {163-169},
  shortjournal = {Neural Netw.},
  title        = {Exponential synchronization of memristive neural networks with time-varying delays via quantized sliding-mode control},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). NFN＋: A novel network followed network for retinal vessel
segmentation. <em>NN</em>, <em>126</em>, 153–162. (<a
href="https://doi.org/10.1016/j.neunet.2020.02.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the early diagnosis of diabetic retinopathy , the morphological attributes of blood vessels play an essential role to construct a retinal computer-aided diagnosis system. However, due to the challenges including limited densely annotated data, inter-vessel differences and structured prediction problem, it remains challenging to segment accurately the retinal vessels , particularly the capillaries on color fundus images. To address these issues, in this paper, we propose a novel deep learning-based model called NFN＋ to effectively extract multi-scale information and make full use of deep feature maps. In NFN＋, the front network converts an image patch into a probabilistic retinal vessel map, and the followed network further refines the map to achieve a better post-processing module, which helps represent the vessel structures implicitly. We employ the inter-network skip connections to unite two identical multi-scale backbones, which enables the useful multi-scale features to be directly transferred from shallow layers to deeper layers. The refined probabilistic retinal vessel maps produced from the augmented images are then averaged to construct the segmentation results. We evaluated this model on the digital retinal images for vessel extraction (DRIVE), structured analysis of the retina (STARE), and the child heart and health study (CHASE) databases. Our results indicate that the elaborated cascaded designs can produce performance gain and the proposed NFN＋ model, to our best knowledge, achieved the state-of-the-art retinal vessel segmentation accuracy on color fundus images (AUC: 98.30\%, 98.75\% and 98.94\%, respectively).},
  archive      = {J_NN},
  author       = {Yicheng Wu and Yong Xia and Yang Song and Yanning Zhang and Weidong Cai},
  doi          = {10.1016/j.neunet.2020.02.018},
  journal      = {Neural Networks},
  pages        = {153-162},
  shortjournal = {Neural Netw.},
  title        = {NFN＋: A novel network followed network for retinal vessel segmentation},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SDARE: A stacked denoising autoencoder method for game
dynamics network structure reconstruction. <em>NN</em>, <em>126</em>,
143–152. (<a
href="https://doi.org/10.1016/j.neunet.2020.03.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex network is a general model to represent the interactions within technological, social, information, and biological interaction. Often, the direct detection of the interaction relationship is costly. Thus, network structure reconstruction, the inverse problem in complex networked systems, is of utmost importance for understanding many complex systems with unknown interaction structures. In addition, the data collected from real network system is often contaminated by noise, which makes the network structure inference task much more challenging. In this paper, we develop a new framework for the game dynamics network structure reconstruction based on deep learning method . In contrast to the compressive sensing methods that employ computationally complex convex/greedy algorithms to solve the network reconstruction task, we introduce a deep learning framework that can learn a structured representation from nodes data and efficiently reconstruct the game dynamics network structure with few observation data. Specifically, we propose the denoising autoencoders (DAEs) as the unsupervised feature learner to capture statistical dependencies between different nodes. Compared to the compressive sensing based method, the proposed method is a global network structure inference method, which can not only get the state-of-art performance, but also obtain the structure of network directly. Besides, the proposed method is robust to noise in the observation data. Moreover, the proposed method is also effective for the network which is not exactly sparse. Accordingly, the proposed method can extend to a wide scope of network reconstruction task in practice.},
  archive      = {J_NN},
  author       = {Keke Huang and Shuo Li and Penglin Dai and Zhen Wang and Zhaofei Yu},
  doi          = {10.1016/j.neunet.2020.03.008},
  journal      = {Neural Networks},
  pages        = {143-152},
  shortjournal = {Neural Netw.},
  title        = {SDARE: A stacked denoising autoencoder method for game dynamics network structure reconstruction},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cross-modal dual subspace learning with adversarial network.
<em>NN</em>, <em>126</em>, 132–142. (<a
href="https://doi.org/10.1016/j.neunet.2020.03.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal retrieval has recently attracted much interest along with the rapid development of multimodal data, and effectively utilizing the complementary relationship of different modal data and eliminating the heterogeneous gap as much as possible are the two key challenges. In this paper, we present a novel network model termed cross-modal Dual Subspace learning with Adversarial Network (DSAN). The main contributions are as follows: (1) Dual subspaces (visual subspace and textual subspace) are proposed, which can better mine the underlying structure information of different modalities as well as modality-specific information. (2) An improved quadruplet loss is proposed, which takes into account the relative distance and absolute distance between positive and negative samples, together with the introduction of the idea of hard sample mining. (3) Intra-modal constrained loss is proposed to maximize the distance of the most similar cross-modal negative samples and their corresponding cross-modal positive samples. In particular, feature preserving and modality classification act as two antagonists. DSAN tries to narrow the heterogeneous gap between different modalities, and distinguish the original modality of random samples in dual subspaces. Comprehensive experimental results demonstrate that, DSAN significantly outperforms 9 state-of-the-art methods on four cross-modal datasets.},
  archive      = {J_NN},
  author       = {Fei Shang and Huaxiang Zhang and Jiande Sun and Liqiang Nie and Li Liu},
  doi          = {10.1016/j.neunet.2020.03.015},
  journal      = {Neural Networks},
  pages        = {132-142},
  shortjournal = {Neural Netw.},
  title        = {Cross-modal dual subspace learning with adversarial network},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep neural networks with a set of node-wise varying
activation functions. <em>NN</em>, <em>126</em>, 118–131. (<a
href="https://doi.org/10.1016/j.neunet.2020.03.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we present deep neural networks with a set of node-wise varying activation functions . The feature-learning abilities of the nodes are affected by the selected activation functions , where the nodes with smaller indices become increasingly more sensitive during training. As a result, the features learned by the nodes are sorted by the node indices in order of their importance such that more sensitive nodes are related to more important features. The proposed networks learn input features but also the importance of the features. Nodes with lower importance in the proposed networks can be pruned to reduce the complexity of the networks, and the pruned networks can be retrained without incurring performance losses. We validated the feature-sorting property of the proposed method using both shallow and deep networks as well as deep networks transferred from existing networks.},
  archive      = {J_NN},
  author       = {Jinhyeok Jang and Hyunjoong Cho and Jaehong Kim and Jaeyeon Lee and Seungjoon Yang},
  doi          = {10.1016/j.neunet.2020.03.004},
  journal      = {Neural Networks},
  pages        = {118-131},
  shortjournal = {Neural Netw.},
  title        = {Deep neural networks with a set of node-wise varying activation functions},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Chimera states in hybrid coupled neuron populations.
<em>NN</em>, <em>126</em>, 108–117. (<a
href="https://doi.org/10.1016/j.neunet.2020.03.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Here we study the emergence of chimera states, a recently reported phenomenon referring to the coexistence of synchronized and unsynchronized dynamical units, in a population of Morris–Lecar neurons which are coupled by both electrical and chemical synapses , constituting a hybrid synaptic architecture, as in actual brain connectivity. This scheme consists of a nonlocal network where the nearest neighbor neurons are coupled by electrical synapses , while the synapses from more distant neurons are of the chemical type. We demonstrate that peculiar dynamical behaviors , including chimera state and traveling wave , exist in such a hybrid coupled neural system, and analyze how the relative abundance of chemical and electrical synapses affects the features of chimera and different synchrony states (i.e. incoherent, traveling wave and coherent) and the regions in the space of relevant parameters for their emergence. Additionally, we show that, when the relative population of chemical synapses increases further, a new intriguing chaotic dynamical behavior appears above the region for chimera states. This is characterized by the coexistence of two distinct synchronized states with different amplitude, and an unsynchronized state, that we denote as a chaotic amplitude chimera. We also discuss about the computational implications of such state.},
  archive      = {J_NN},
  author       = {Ali Calim and Joaquin J. Torres and Mahmut Ozer and Muhammet Uzuntarla},
  doi          = {10.1016/j.neunet.2020.03.002},
  journal      = {Neural Networks},
  pages        = {108-117},
  shortjournal = {Neural Netw.},
  title        = {Chimera states in hybrid coupled neuron populations},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic resource allocation during reinforcement learning
accounts for ramping and phasic dopamine activity. <em>NN</em>,
<em>126</em>, 95–107. (<a
href="https://doi.org/10.1016/j.neunet.2020.03.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For an animal to learn about its environment with limited motor and cognitive resources, it should focus its resources on potentially important stimuli. However, too narrow focus is disadvantageous for adaptation to environmental changes. Midbrain dopamine neurons are excited by potentially important stimuli, such as reward-predicting or novel stimuli, and allocate resources to these stimuli by modulating how an animal approaches, exploits, explores, and attends. The current study examined the theoretical possibility that dopamine activity reflects the dynamic allocation of resources for learning. Dopamine activity may transition between two patterns: (1) phasic responses to cues and rewards, and (2) ramping activity arising as the agent approaches the reward. Phasic excitation has been explained by prediction errors generated by experimentally inserted cues. However, when and why dopamine activity transitions between the two patterns remain unknown. By parsimoniously modifying a standard temporal difference (TD) learning model to accommodate a mixed presentation of both experimental and environmental stimuli, we simulated dopamine transitions and compared them with experimental data from four different studies. The results suggested that dopamine transitions from ramping to phasic patterns as the agent focuses its resources on a small number of reward-predicting stimuli, thus leading to task dimensionality reduction. The opposite occurs when the agent re-distributes its resources to adapt to environmental changes, resulting in task dimensionality expansion. This research elucidates the role of dopamine in a broader context, providing a potential explanation for the diverse repertoire of dopamine activity that cannot be explained solely by prediction error.},
  archive      = {J_NN},
  author       = {Minryung R. Song and Sang Wan Lee},
  doi          = {10.1016/j.neunet.2020.03.005},
  journal      = {Neural Networks},
  pages        = {95-107},
  shortjournal = {Neural Netw.},
  title        = {Dynamic resource allocation during reinforcement learning accounts for ramping and phasic dopamine activity},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). AdaEn-net: An ensemble of adaptive 2D–3D fully convolutional
networks for medical image segmentation. <em>NN</em>, <em>126</em>,
76–94. (<a href="https://doi.org/10.1016/j.neunet.2020.03.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully Convolutional Networks (FCNs) have emerged as powerful segmentation models but are usually designed manually, which requires extensive time and can result in large and complex architectures. There is a growing interest to automatically design efficient architectures that can accurately segment 3D medical images. However, most approaches either do not fully exploit volumetric information or do not optimize the model’s size. To address these problems, we propose a self-adaptive 2D–3D ensemble of FCNs called AdaEn-Net for 3D medical image segmentation that incorporates volumetric data and adapts to a particular dataset by optimizing both the model’s performance and size. The AdaEn-Net consists of a 2D FCN that extracts intra-slice information and a 3D FCN that exploits inter-slice information. The architecture and hyperparameters of the 2D and 3D architectures are found through a multiobjective evolutionary based algorithm that maximizes the expected segmentation accuracy and minimizes the number of parameters in the network. The main contribution of this work is a model that fully exploits volumetric information and automatically searches for a high-performing and efficient architecture. The AdaEn-Net was evaluated for prostate segmentation on the PROMISE12 Grand Challenge and for cardiac segmentation on the MICCAI ACDC challenge. In the first challenge, the AdaEn-Net ranks 9 out of 297 submissions and surpasses the performance of an automatically-generated segmentation network while producing an architecture with 13 × × fewer parameters. In the second challenge, the proposed model is ranked within the top 8 submissions and outperforms an architecture designed with reinforcement learning while having 1.25 × × fewer parameters.},
  archive      = {J_NN},
  author       = {Maria Baldeon Calisto and Susana K. Lai-Yuen},
  doi          = {10.1016/j.neunet.2020.03.007},
  journal      = {Neural Networks},
  pages        = {76-94},
  shortjournal = {Neural Netw.},
  title        = {AdaEn-net: An ensemble of adaptive 2D–3D fully convolutional networks for medical image segmentation},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variational approximation error in non-negative matrix
factorization. <em>NN</em>, <em>126</em>, 65–75. (<a
href="https://doi.org/10.1016/j.neunet.2020.03.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-negative matrix factorization (NMF) is a knowledge discovery method that is used in many fields. Variational inference and Gibbs sampling methods for it are also well-known. However, the variational approximation error has not been clarified yet, because NMF is not statistically regular and the prior distribution used in variational Bayesian NMF (VBNMF) has zero or divergence points. In this paper, using algebraic geometrical methods, we theoretically analyze the difference in negative log evidence (a.k.a. free energy ) between VBNMF and Bayesian NMF, i.e., the Kullback–Leibler divergence between the variational posterior and the true posterior. We derive an upper bound for the learning coefficient (a.k.a. the real log canonical threshold ) in Bayesian NMF. By using the upper bound, we find a lower bound for the approximation error, asymptotically. The result quantitatively shows how well the VBNMF algorithm can approximate Bayesian NMF; the lower bound depends on the hyperparameters and the true non-negative rank. A numerical experiment demonstrates the theoretical result.},
  archive      = {J_NN},
  author       = {Naoki Hayashi},
  doi          = {10.1016/j.neunet.2020.03.009},
  journal      = {Neural Networks},
  pages        = {65-75},
  shortjournal = {Neural Netw.},
  title        = {Variational approximation error in non-negative matrix factorization},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Recommendation via collaborative autoregressive flows.
<em>NN</em>, <em>126</em>, 52–64. (<a
href="https://doi.org/10.1016/j.neunet.2020.03.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although it is one of the most widely used methods in recommender systems , Collaborative Filtering (CF) still has difficulties in modeling non-linear user–item interactions. Complementary to this, recently developed deep generative model variants (e.g., Variational Autoencoder (VAE)) allowing Bayesian inference and approximation of the variational posterior distributions in these models, have achieved promising performance improvement in many areas. However, the choices of variation distribution – e.g., the popular diagonal-covariance Gaussians – are insufficient to recover the true distributions, often resulting in biased maximum likelihood estimates of the model parameters. Aiming at more tractable and expressive variational families, in this work we extend the flow-based generative model to CF for modeling implicit feedbacks. We present the Collaborative Autoregressive Flows (CAF) for the recommender system, transforming a simple initial density into more complex ones via a sequence of invertible transformations , until a desired level of complexity is attained. CAF is a non-linear probabilistic approach allowing uncertainty representation and exact tractability of latent-variable inference in item recommendations. Compared to the agnostic-presumed prior approximation used in existing deep generative recommendation approaches, CAF is more effective in estimating the probabilistic posterior and achieves better recommendation accuracy. We conducted extensive experimental evaluations demonstrating that CAF can capture more effective representation of latent factors , resulting in a substantial gain on recommendation compared to the state-of-the-art approaches.},
  archive      = {J_NN},
  author       = {Fan Zhou and Yuhua Mo and Goce Trajcevski and Kunpeng Zhang and Jin Wu and Ting Zhong},
  doi          = {10.1016/j.neunet.2020.03.010},
  journal      = {Neural Networks},
  pages        = {52-64},
  shortjournal = {Neural Netw.},
  title        = {Recommendation via collaborative autoregressive flows},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Probabilistic inference of binary markov random fields in
spiking neural networks through mean-field approximation. <em>NN</em>,
<em>126</em>, 42–51. (<a
href="https://doi.org/10.1016/j.neunet.2020.03.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have suggested that the cognitive process of the human brain is realized as probabilistic inference and can be further modeled by probabilistic graphical models like Markov random fields . Nevertheless, it remains unclear how probabilistic inference can be implemented by a network of spiking neurons in the brain. Previous studies have tried to relate the inference equation of binary Markov random fields to the dynamic equation of spiking neural networks through belief propagation algorithm and reparameterization , but they are valid only for Markov random fields with limited network structure. In this paper, we propose a spiking neural network model that can implement inference of arbitrary binary Markov random fields. Specifically, we design a spiking recurrent neural network and prove that its neuronal dynamics are mathematically equivalent to the inference process of Markov random fields by adopting mean-field theory. Furthermore, our mean-field approach unifies previous works. Theoretical analysis and experimental results, together with the application to image denoising , demonstrate that our proposed spiking neural network can get comparable results to that of mean-field inference.},
  archive      = {J_NN},
  author       = {Yajing Zheng and Shanshan Jia and Zhaofei Yu and Tiejun Huang and Jian K. Liu and Yonghong Tian},
  doi          = {10.1016/j.neunet.2020.03.003},
  journal      = {Neural Networks},
  pages        = {42-51},
  shortjournal = {Neural Netw.},
  title        = {Probabilistic inference of binary markov random fields in spiking neural networks through mean-field approximation},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling coherence by ordering paragraphs using pointer
networks. <em>NN</em>, <em>126</em>, 36–41. (<a
href="https://doi.org/10.1016/j.neunet.2020.02.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Coherence is a distinctive feature in well-written documents. One method to study coherence is to analyze how sentences are ordered in a document. In Multi-document Summarization, sentences from different sources need to be ordered. Cluster-based ordering algorithms aim to study various themes or topics that are present in a set of sentences. After the clusters of sentences have been identified, sentences are ordered within each cluster in isolation. One challenge that remains is to order these clusters or paragraphs to obtain a coherent ordering of information. Inspired by the success of deep neural networks in several NLP tasks, we propose an RNN-based encoder–decoder system to predict order for a given set of loose clusters or paragraphs. Universal Sentence Encoder (USE) is used to encode paragraphs into high dimensional embeddings, which are then fed into an LSTM encoder and consecutively passed to a pointer network, which finally outputs the paragraph order. Since Wikipedia is a source of well- structured articles, it is used to generate multiple datasets. Based on our experimental results, the proposed model satisfactorily outperforms the baseline model across multiple datasets. We observe a two-fold increase in Kendall’s tau values for the final paragraph orderings.},
  archive      = {J_NN},
  author       = {Divesh Pandey and C. Ravindranath Chowdary},
  doi          = {10.1016/j.neunet.2020.02.022},
  journal      = {Neural Networks},
  pages        = {36-41},
  shortjournal = {Neural Netw.},
  title        = {Modeling coherence by ordering paragraphs using pointer networks},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). NeuroBayesSLAM: Neurobiologically inspired bayesian
integration of multisensory information for robot navigation.
<em>NN</em>, <em>126</em>, 21–35. (<a
href="https://doi.org/10.1016/j.neunet.2020.02.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial navigation depends on the combination of multiple sensory cues from idiothetic and allothetic sources. The computational mechanisms of mammalian brains in integrating different sensory modalities under uncertainty for navigation is enlightening for robot navigation . We propose a Bayesian attractor network model to integrate visual and vestibular inputs inspired by the spatial memory systems of mammalian brains . In the model, the pose of the robot is encoded separately by two sub-networks, namely head direction network for angle representation and grid cell network for position representation, using similar neural codes of head direction cells and grid cells observed in mammalian brains. The neural codes in each of the sub-networks are updated in a Bayesian manner by a population of integrator cells for vestibular cue integration, as well as a population of calibration cells for visual cue calibration. The conflict between vestibular cue and visual cue is resolved by the competitive dynamics between the two populations. The model, implemented on a monocular visual simultaneous localization and mapping (SLAM) system, termed NeuroBayesSLAM, successfully builds semi-metric topological maps and self-localizes in outdoor and indoor environments of difference characteristics, achieving comparable performance as previous neurobiologically inspired navigation systems but with much less computation complexity. The proposed multisensory integration method constitutes a concise yet robust and biologically plausible method for robot navigation in large environments. The model provides a viable Bayesian mechanism for multisensory integration that may pertain to other neural subsystems beyond spatial cognition.},
  archive      = {J_NN},
  author       = {Taiping Zeng and Fengzhen Tang and Daxiong Ji and Bailu Si},
  doi          = {10.1016/j.neunet.2020.02.023},
  journal      = {Neural Networks},
  pages        = {21-35},
  shortjournal = {Neural Netw.},
  title        = {NeuroBayesSLAM: Neurobiologically inspired bayesian integration of multisensory information for robot navigation},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Global exponential stabilization and lag synchronization
control of inertial neural networks with time delays. <em>NN</em>,
<em>126</em>, 11–20. (<a
href="https://doi.org/10.1016/j.neunet.2020.03.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The global exponential stabilization and lag synchronization control of delayed inertial neural networks (INNs) are investigated. By constructing nonnegative function and employing inequality techniques, several new results about exponential stabilization and exponential lag synchronization are derived via adaptive control . And the theoretical outcomes are developed directly from the INNs themselves without variable substitution. In addition, the synchronization results are also applied to image encryption and decryption. Finally, an example is presented to illustrate the validity of the derived results.},
  archive      = {J_NN},
  author       = {Jichen Shi and Zhigang Zeng},
  doi          = {10.1016/j.neunet.2020.03.006},
  journal      = {Neural Networks},
  pages        = {11-20},
  shortjournal = {Neural Netw.},
  title        = {Global exponential stabilization and lag synchronization control of inertial neural networks with time delays},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Finite-time synchronization of fractional-order gene
regulatory networks with time delay. <em>NN</em>, <em>126</em>, 1–10.
(<a href="https://doi.org/10.1016/j.neunet.2020.02.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As multi-gene networks transmit signals and products by synchronous cooperation, investigating the synchronization of gene regulatory networks may help us to explore the biological rhythm and internal mechanisms at molecular and cellular levels . We aim to induce a type of fractional-order gene regulatory networks to synchronize at finite-time point by designing feedback controls . Firstly, a unique equilibrium point of the network is proved by applying the principle of contraction mapping . Secondly, some sufficient conditions for finite-time synchronization of fractional-order gene regulatory networks with time delay are explored based on two kinds of different control techniques and fractional Lyapunov function approach, and the corresponding setting time is estimated. Finally, some numerical examples are given to demonstrate the effectiveness of the theoretical results.},
  archive      = {J_NN},
  author       = {Yuanhua Qiao and Hongyun Yan and Lijuan Duan and Jun Miao},
  doi          = {10.1016/j.neunet.2020.02.004},
  journal      = {Neural Networks},
  pages        = {1-10},
  shortjournal = {Neural Netw.},
  title        = {Finite-time synchronization of fractional-order gene regulatory networks with time delay},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020h). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>125</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(20)30100-3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(20)30100-3},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020h). Current events. <em>NN</em>, <em>125</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(20)30099-X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(20)30099-X},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {Current events},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SOMprocessor: A high throughput FPGA-based architecture for
implementing self-organizing maps and its application to video
processing. <em>NN</em>, <em>125</em>, 349–362. (<a
href="https://doi.org/10.1016/j.neunet.2020.02.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design of neuromorphic chips aims to develop electronic circuits dedicated to executing artificial neural networks , mainly by exploring parallel processing . Unsupervised learning models, such as Self-organizing Maps (SOM), may benefit from massively concurrent hardware-based implementations to meet the requirements of real-time and embedded applications. This work first presents a theoretical analysis of the algorithms implemented in hardware to compute SOM learning and recall phases. This is important because, albeit similar, the processing steps executed in hardware are not necessarily identical to those executed in software. Then, the proposed FPGA architecture entitled SOMprocessor is shown in detail. The circuit of the processor explores two different computational strategies for increasing the performance of current state-of-the-art works. These computational strategies aim to improve the data flow through the processor and its flexibility to implement different network topologies . Finally, this work presents the application of the SOMprocessor to a video categorization task. The results show that topographic and quantization errors are similar between hardware and software implementations, as well as the overall accuracy. Moreover, the proposed FPGA architecture achieves acceleration of 3 to 4 orders of magnitude as compared to CPU executions.},
  archive      = {J_NN},
  author       = {Miguel Angelo de Abreu de Sousa and Ricardo Pires and Emilio Del-Moral-Hernandez},
  doi          = {10.1016/j.neunet.2020.02.019},
  journal      = {Neural Networks},
  pages        = {349-362},
  shortjournal = {Neural Netw.},
  title        = {SOMprocessor: A high throughput FPGA-based architecture for implementing self-organizing maps and its application to video processing},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Constructing large-scale cortical brain networks from scalp
EEG with bayesian nonnegative matrix factorization. <em>NN</em>,
<em>125</em>, 338–348. (<a
href="https://doi.org/10.1016/j.neunet.2020.02.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A large-scale network provides a high hierarchical level for understanding the adaptive adjustment of the human brain during cognition processes. Since high spatial resolution is required, most of the related works are based on functional magnetic resonance imaging (fMRI); however, fMRI lacks the temporal information that is important in investigating the high cognition processes. Although combining electroencephalography (EEG) inverse solution and independent component analysis (ICA), researchers detected large-scale functional subnetworks recently, few researchers focus on the unreasonable negative activation, which is biased from the nonnegative electrical source activations in the brain. In this study, considering the favorable nonnegative property of Bayesian nonnegative matrix factorization (Bayesian NMF) and combining EEG source imaging, we developed a robust approach for EEG large-scale network construction and applied it to two independent real EEG datasets (i.e., decision-making and P300). Eight and nine best-fit networks, including such important subnetworks as the somatosensory-motor network (SMN), the default mode network (DMN), etc., were successfully identified for decision-making and P300, respectively. Compared to the networks acquired with ICA, these networks not only lacked confusing negative activations but also showed clear spatial distributions that are compatible with specific brain function. Based on the constructed large-scale network, we further probed that the self-referential network (SRN), the primary visual network (PVN), and the visual network (VN) demonstrated different interaction patterns with other networks between different responses in decision-making. Our results confirm the possibility of probing the neural mechanisms of high cognition processes at a very high temporal and spatial resolution level.},
  archive      = {J_NN},
  author       = {Chanlin Yi and Chunli Chen and Yajing Si and Fali Li and Tao Zhang and Yuanyuan Liao and Yuanling Jiang and Dezhong Yao and Peng Xu},
  doi          = {10.1016/j.neunet.2020.02.021},
  journal      = {Neural Networks},
  pages        = {338-348},
  shortjournal = {Neural Netw.},
  title        = {Constructing large-scale cortical brain networks from scalp EEG with bayesian nonnegative matrix factorization},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020c). New criteria for global stability of neutral-type
cohen–grossberg neural networks with multiple delays. <em>NN</em>,
<em>125</em>, 330–337. (<a
href="https://doi.org/10.1016/j.neunet.2020.02.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The significant contribution of this paper is the addressing the stability issue of neutral-type Cohen–Grossberg neural networks possessing multiple time delays in the states of the neurons and multiple neutral delays in time derivative of states of the neurons. By making the use of a novel and enhanced Lyapunov functional , some new sufficient stability criteria are presented for this model of neutral-type neural systems. The obtained stability conditions are completely dependent of the parameters of the neural system and independent of time delays and neutral delays. A constructive numerical example is presented for the sake of proving the key advantages of the proposed stability results over the previously reported corresponding stability criteria for Cohen–Grossberg neural networks of neutral type. Since, stability analysis of Cohen–Grossberg neural networks involving multiple time delays and multiple neutral delays is a difficult problem to overcome, the investigations of the stability conditions of the neutral-type the stability analysis of this class of neural network models have not been given much attention. Therefore, the stability criteria derived in this work can be evaluated as a valuable contribution to the stability analysis of neutral-type Cohen–Grossberg neural systems involving multiple delays.},
  archive      = {J_NN},
  author       = {Ozlem Faydasicok},
  doi          = {10.1016/j.neunet.2020.02.020},
  journal      = {Neural Networks},
  pages        = {330-337},
  shortjournal = {Neural Netw.},
  title        = {New criteria for global stability of neutral-type Cohen–Grossberg neural networks with multiple delays},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved multi-view GEPSVM via inter-view difference
maximization and intra-view agreement minimization. <em>NN</em>,
<em>125</em>, 313–329. (<a
href="https://doi.org/10.1016/j.neunet.2020.02.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview Generalized Eigenvalue Proximal Support Vector Machine (MvGEPSVM) is an effective method for multiview data classification proposed recently. However, it ignores discriminations between different views and the agreement of the same view. Moreover, there is no robustness guarantee. In this paper, we propose an improved multiview GEPSVM (IMvGEPSVM) method, which adds a multi-view regularization that can connect different views of the same class and simultaneously considers the maximization of the samples from different classes in heterogeneous views for promoting discriminations. This makes the classification more effective. In addition, L1-norm rather than squared L2-norm is employed to calculate the distances from each of the sample points to the hyperplane so as to reduce the effect of outliers in the proposed model. To solve the resulting objective, an efficient iterative algorithm is presented. Theoretically, we conduct the proof of the algorithm’s convergence. Experimental results show the effectiveness of the proposed method.},
  archive      = {J_NN},
  author       = {Yawen Cheng and Hang Yin and Qiaolin Ye and Peng Huang and Liyong Fu and Zhangjing Yang and Yuan Tian},
  doi          = {10.1016/j.neunet.2020.02.002},
  journal      = {Neural Networks},
  pages        = {313-329},
  shortjournal = {Neural Netw.},
  title        = {Improved multi-view GEPSVM via inter-view difference maximization and intra-view agreement minimization},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CNN–MHSA: A convolutional neural network and multi-head
self-attention combined approach for detecting phishing websites.
<em>NN</em>, <em>125</em>, 303–312. (<a
href="https://doi.org/10.1016/j.neunet.2020.02.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Increasing phishing sites today have posed great threats due to their terribly imperceptible hazard. They expect users to mistake them as legitimate ones so as to steal user information and properties without notice. The conventional way to mitigate such threats is to set up blacklists. However, it cannot detect one-time Uniform Resource Locators (URL) that have not appeared in the list. As an improvement, deep learning methods are applied to increase detection accuracy and reduce the misjudgment ratio. However, some of them only focus on the characters in URLs but ignore the relationships between characters, which results in that the detection accuracy still needs to be improved. Considering the multi-head self-attention (MHSA) can learn the inner structures of URLs, in this paper, we propose CNN–MHSA, a Convolutional Neural Network (CNN) and the MHSA combined approach for highly-precise. To achieve this goal, CNN–MHSA first takes a URL string as the input data and feeds it into a mature CNN model so as to extract its features. In the meanwhile, MHSA is applied to exploit characters’ relationships in the URL so as to calculate the corresponding weights for the CNN learned features. Finally, CNN–MHSA can produce highly-precise detection result for a URL object by integrating its features and their weights. The thorough experiments on a dataset collected in real environment demonstrate that our method achieves 99.84\% accuracy, which outperforms the classical method CNN–LSTM and at least 6.25\% higher than other similar methods on average.},
  archive      = {J_NN},
  author       = {Xi Xiao and Dianyan Zhang and Guangwu Hu and Yong Jiang and Shutao Xia},
  doi          = {10.1016/j.neunet.2020.02.013},
  journal      = {Neural Networks},
  pages        = {303-312},
  shortjournal = {Neural Netw.},
  title        = {CNN–MHSA: A convolutional neural network and multi-head self-attention combined approach for detecting phishing websites},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiple discrimination and pairwise CNN for view-based 3D
object retrieval. <em>NN</em>, <em>125</em>, 290–302. (<a
href="https://doi.org/10.1016/j.neunet.2020.02.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development and wide application of computer, camera device, network and hardware technology, 3D object (or model) retrieval has attracted widespread attention and it has become a hot research topic in the computer vision domain. Deep learning features already available in 3D object retrieval have been proven to be better than the retrieval performance of hand-crafted features. However, most existing networks do not take into account the impact of multi-view image selection on network training, and the use of contrastive loss alone only forcing the same-class samples to be as close as possible. In this work, a novel solution named Multi-view Discrimination and Pairwise CNN ( MDPCNN ) for 3D object retrieval is proposed to tackle these issues. It can simultaneously input multiple batches and multiple views by adding the Slice layer and the Concat layer. Furthermore, a highly discriminative network is obtained by training samples that are not easy to be classified by clustering. Lastly, we deploy the contrastive-center loss and contrastive loss as the optimization objective that has better intra-class compactness and inter-class separability. Large-scale experiments show that the proposed MDPCNN can achieve a significant performance over the state-of-the-art algorithms in 3D object retrieval.},
  archive      = {J_NN},
  author       = {Zan Gao and Haixin Xue and Shaohua Wan},
  doi          = {10.1016/j.neunet.2020.02.017},
  journal      = {Neural Networks},
  pages        = {290-302},
  shortjournal = {Neural Netw.},
  title        = {Multiple discrimination and pairwise CNN for view-based 3D object retrieval},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parametric deformable exponential linear units for deep
neural networks. <em>NN</em>, <em>125</em>, 281–289. (<a
href="https://doi.org/10.1016/j.neunet.2020.02.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rectified activation units make an important contribution to the success of deep neural networks in many computer vision tasks. In this paper, we propose a Parametric Deformable Exponential Linear Unit (PDELU) and theoretically verify its effectiveness for improving the convergence speed of learning procedure. By means of flexible map shape, the proposed PDELU could push the mean value of activation responses closer to zero, which ensures the steepest descent in training a deep neural network. We verify the effectiveness of the proposed method in the image classification task. Extensive experiments on three classical databases (i.e., CIFAR-10, CIFAR-100, and ImageNet-2015) indicate that the proposed method leads to higher convergence speed and better accuracy when it is embedded into different CNN architectures (i.e., NIN, ResNet , WRN, and DenseNet). Meanwhile, the proposed PDELU outperforms many existing shape-specific activation functions (i.e., Maxout, ReLU, LeakyReLU, ELU, SELU, SoftPlus, Swish) and the shape-adaptive activation functions (i.e., APL, PReLU, MPELU, FReLU).},
  archive      = {J_NN},
  author       = {Qishang Cheng and HongLiang Li and Qingbo Wu and Lei Ma and King Ngi Ngan},
  doi          = {10.1016/j.neunet.2020.02.012},
  journal      = {Neural Networks},
  pages        = {281-289},
  shortjournal = {Neural Netw.},
  title        = {Parametric deformable exponential linear units for deep neural networks},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Supervised learning in spiking neural networks: A review of
algorithms and evaluations. <em>NN</em>, <em>125</em>, 258–280. (<a
href="https://doi.org/10.1016/j.neunet.2020.02.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a new brain-inspired computational model of the artificial neural network , a spiking neural network encodes and processes neural information through precisely timed spike trains . Spiking neural networks are composed of biologically plausible spiking neurons , which have become suitable tools for processing complex temporal or spatiotemporal information. However, because of their intricately discontinuous and implicit nonlinear mechanisms, the formulation of efficient supervised learning algorithms for spiking neural networks is difficult, and has become an important problem in this research field. This article presents a comprehensive review of supervised learning algorithms for spiking neural networks and evaluates them qualitatively and quantitatively. First, a comparison between spiking neural networks and traditional artificial neural networks is provided. The general framework and some related theories of supervised learning for spiking neural networks are then introduced. Furthermore, the state-of-the-art supervised learning algorithms in recent years are reviewed from the perspectives of applicability to spiking neural network architecture and the inherent mechanisms of supervised learning algorithms. A performance comparison of spike train learning of some representative algorithms is also made. In addition, we provide five qualitative performance evaluation criteria for supervised learning algorithms for spiking neural networks and further present a new taxonomy for supervised learning algorithms depending on these five performance evaluation criteria. Finally, some future research directions in this research field are outlined.},
  archive      = {J_NN},
  author       = {Xiangwen Wang and Xianghong Lin and Xiaochao Dang},
  doi          = {10.1016/j.neunet.2020.02.011},
  journal      = {Neural Networks},
  pages        = {258-280},
  shortjournal = {Neural Netw.},
  title        = {Supervised learning in spiking neural networks: A review of algorithms and evaluations},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Low-rank discriminative regression learning for image
classification. <em>NN</em>, <em>125</em>, 245–257. (<a
href="https://doi.org/10.1016/j.neunet.2020.02.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a famous multivariable analysis technique, regression methods , such as ridge regression, are widely used for image representation and dimensionality reduction. However, the metric of ridge regression and its variants is always the Frobenius norm (F-norm), which is sensitive to outliers and noise in data. At the same time, the performance of the ridge regression and its extensions is limited by the class number of the data. To address these problems, we propose a novel regression learning method which named low-rank discriminative regression learning (LDRL) for image representation. LDRL assumes that the input data is corrupted and thus the L 1 L1 norm can be used as a sparse constraint on the noised matrix to recover the clean data for regression, which can improve the robustness of the algorithm. Due to learn a novel project matrix that is not limited by the number of classes, LDRL is suitable for classifying the data set no matter whether there is a small or large number of classes. The performance of the proposed LDRL is evaluated on six public image databases. The experimental results prove that LDRL obtains better performance than existing regression methods .},
  archive      = {J_NN},
  author       = {Yuwu Lu and Zhihui Lai and Wai Keung Wong and Xuelong Li},
  doi          = {10.1016/j.neunet.2020.02.007},
  journal      = {Neural Networks},
  pages        = {245-257},
  shortjournal = {Neural Netw.},
  title        = {Low-rank discriminative regression learning for image classification},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). End-to-end semantic segmentation of personalized deep brain
structures for non-invasive brain stimulation. <em>NN</em>,
<em>125</em>, 233–244. (<a
href="https://doi.org/10.1016/j.neunet.2020.02.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electro-stimulation or modulation of deep brain regions is commonly used in clinical procedures for the treatment of several nervous system disorders . In particular, transcranial direct current stimulation (tDCS) is widely used as an affordable clinical application that is applied through electrodes attached to the scalp. However, it is difficult to determine the amount and distribution of the electric field (EF) in the different brain regions due to anatomical complexity and high inter-subject variability. Personalized tDCS is an emerging clinical procedure that is used to tolerate electrode montage for accurate targeting. This procedure is guided by computational head models generated from anatomical images such as MRI. Distribution of the EF in segmented head models can be calculated through simulation studies. Therefore, fast, accurate, and feasible segmentation of different brain structures would lead to a better adjustment for customized tDCS studies. In this study, a single-encoder multi-decoders convolutional neural network is proposed for deep brain segmentation. The proposed architecture is trained to segment seven deep brain structures using T1-weighted MRI. Network generated models are compared with a reference model constructed using a semi-automatic method, and it presents a high matching especially in Thalamus (Dice Coefficient (DC) = 94.70\%), Caudate (DC = 91.98\%) and Putamen (DC = 90.31\%) structures. Electric field distribution during tDCS in generated and reference models matched well each other, suggesting its potential usefulness in clinical practice.},
  archive      = {J_NN},
  author       = {Essam A. Rashed and Jose Gomez-Tames and Akimasa Hirata},
  doi          = {10.1016/j.neunet.2020.02.006},
  journal      = {Neural Networks},
  pages        = {233-244},
  shortjournal = {Neural Netw.},
  title        = {End-to-end semantic segmentation of personalized deep brain structures for non-invasive brain stimulation},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Synchronization of complex networks with time-varying delay
of unknown bound via delayed impulsive control. <em>NN</em>,
<em>125</em>, 224–232. (<a
href="https://doi.org/10.1016/j.neunet.2020.02.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The synchronization problem for complex networks with time-varying delays of unknown bound is investigated in this paper. From the impulsive control point of view, a novel delayed impulsive differential inequality is proposed, where the bounds of time-varying delays in continuous dynamic and discrete dynamic are both unknown. Based on the inequality, a class of delayed impulsive controllers is designed to achieve the synchronization of complex networks, where the restriction between impulses interval and time-varying delays is dropped. A numerical example is presented to illustrate the effectiveness of the obtained results.},
  archive      = {J_NN},
  author       = {Zhilu Xu and Xiaodi Li and Peiyong Duan},
  doi          = {10.1016/j.neunet.2020.02.003},
  journal      = {Neural Networks},
  pages        = {224-232},
  shortjournal = {Neural Netw.},
  title        = {Synchronization of complex networks with time-varying delay of unknown bound via delayed impulsive control},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hyper-laplacian regularized multi-view subspace clustering
with low-rank tensor constraint. <em>NN</em>, <em>125</em>, 214–223. (<a
href="https://doi.org/10.1016/j.neunet.2020.02.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel hyper-Laplacian regularized multiview subspace clustering with low-rank tensor constraint method, which is referred as HLR-MSCLRT. In the HLR-MSCLRT model, the subspace representation matrices of different views are stacked as a tensor, and then the high order correlations among data can be captured. To reduce the redundancy information of the learned subspace representations, a low-rank constraint is adopted to the constructed tensor. Since data in the real world often reside in multiple nonlinear subspaces, the HLR-MSCLRT model utilizes the hyper-Laplacian graph regularization to preserve the local geometry structure embedded in a high-dimensional ambient space. An efficient algorithm is also presented to solve the optimization problem of the HLR-MSCLRT model. The experimental results on some data sets show that the proposed HLR-MSCLRT model outperforms many state-of-the-art multi-view clustering approaches .},
  archive      = {J_NN},
  author       = {Gui-Fu Lu and Qin-Ru Yu and Yong Wang and Ganyi Tang},
  doi          = {10.1016/j.neunet.2020.02.014},
  journal      = {Neural Networks},
  pages        = {214-223},
  shortjournal = {Neural Netw.},
  title        = {Hyper-laplacian regularized multi-view subspace clustering with low-rank tensor constraint},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Collaborative learning with corrupted labels. <em>NN</em>,
<em>125</em>, 205–213. (<a
href="https://doi.org/10.1016/j.neunet.2020.02.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNNs) have been very successful for supervised learning. However, their high generalization performance often comes with the high cost of annotating data manually. Collecting low-quality labeled dataset is relatively cheap, e.g., using web search engines, while DNNs tend to overfit to corrupted labels easily. In this paper, we propose a collaborative learning (co-learning) approach to improve the robustness and generalization performance of DNNs on datasets with corrupted labels. This is achieved by designing a deep network with two separate branches, coupled with a relabeling mechanism. Co-learning could safely recover the true labels of most mislabeled samples, not only preventing the model from overfitting the noise, but also exploiting useful information from all the samples. Although being very simple, the proposed algorithm is able to achieve high generalization performance even a large portion of the labels are corrupted. Experiments show that co-learning consistently outperforms existing state-of-the-art methods on three widely used benchmark datasets.},
  archive      = {J_NN},
  author       = {Yulin Wang and Rui Huang and Gao Huang and Shiji Song and Cheng Wu},
  doi          = {10.1016/j.neunet.2020.02.010},
  journal      = {Neural Networks},
  pages        = {205-213},
  shortjournal = {Neural Netw.},
  title        = {Collaborative learning with corrupted labels},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Resilient fault-tolerant anti-synchronization for stochastic
delayed reaction–diffusion neural networks with semi-markov jump
parameters. <em>NN</em>, <em>125</em>, 194–204. (<a
href="https://doi.org/10.1016/j.neunet.2020.02.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the anti-synchronization issue for stochastic delayed reaction–diffusion neural networks subject to semi-Markov jump parameters. A resilient fault-tolerant controller is utilized to ensure the anti-synchronization in the presence of actuator failures as well as gain perturbations, simultaneously. Firstly, by means of the Lyapunov functional and stochastic analysis methods, a mean-square exponential stability criterion is derived for the resulting error system. It is shown the obtained criterion improves a previously reported result. Then, based on the present analysis result and using several decoupling techniques, a strategy for designing the desired resilient fault-tolerant controller is proposed. At last, two numerical examples are given to illustrate the superiority of the present stability analysis method and the applicability of the proposed resilient fault-tolerant anti-synchronization control strategy, respectively.},
  archive      = {J_NN},
  author       = {Jianping Zhou and Yamin Liu and Jianwei Xia and Zhen Wang and Sabri Arik},
  doi          = {10.1016/j.neunet.2020.02.015},
  journal      = {Neural Networks},
  pages        = {194-204},
  shortjournal = {Neural Netw.},
  title        = {Resilient fault-tolerant anti-synchronization for stochastic delayed reaction–diffusion neural networks with semi-markov jump parameters},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Causal importance of low-level feature selectivity for
generalization in image recognition. <em>NN</em>, <em>125</em>, 185–193.
(<a href="https://doi.org/10.1016/j.neunet.2020.02.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although our brain and deep neural networks (DNNs) can perform high-level sensory-perception tasks, such as image or speech recognition, the inner mechanism of these hierarchical information-processing systems is poorly understood in both neuroscience and machine learning . Recently, Morcos et al. (2018) examined the effect of class-selective units in DNNs, i.e., units with high-level selectivity, on network generalization, concluding that hidden units that are selectively activated by specific input patterns may harm the network’s performance. In this study, we revisited their hypothesis, considering units with selectivity for lower-level features, and argue that selective units are not always harmful to the network performance. Specifically, by using DNNs trained for image classification , we analyzed the orientation selectivity of individual units, a low-level selectivity widely studied in visual neuroscience. We found that orientation-selective units exist in both lower and higher layers of these DNNs, as in our brain. In particular, units in lower layers became more orientation-selective as the generalization performance improved during the course of training. Consistently, networks that generalized better were more orientation-selective in the lower layers. We finally revealed that ablating these selective units in the lower layers substantially degraded the generalization performance of the networks, at least by disrupting the shift-invariance of the higher layers. These results suggest that orientation selectivity can play a causally important role in object recognition, and that, contrary to the triviality of units with high-level selectivity, lower-layer units with selectivity for low-level features may be indispensable for generalization, at least for the several network architectures .},
  archive      = {J_NN},
  author       = {Jumpei Ukita},
  doi          = {10.1016/j.neunet.2020.02.009},
  journal      = {Neural Networks},
  pages        = {185-193},
  shortjournal = {Neural Netw.},
  title        = {Causal importance of low-level feature selectivity for generalization in image recognition},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Chaos in fractional-order discrete neural networks with
application to image encryption. <em>NN</em>, <em>125</em>, 174–184. (<a
href="https://doi.org/10.1016/j.neunet.2020.02.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a three-dimensional fractional-order (FO) discrete Hopfield neural network (FODHNN) in the left Caputo discrete delta’s sense is proposed, the dynamic behavior and synchronization of FODHNN are studied, and the system is applied to image encryption . First, FODHNN is shown to exhibit rich nonlinear dynamics behaviors. Phase portraits, bifurcation diagrams and Lyapunov exponents are carried out to verify chaotic dynamics in this system. Moreover, by using stability theorem of FO discrete linear systems, a suitable control scheme is designed to achieve synchronization of the FODHNN. Finally, image encryption system based on the chaotic FODHNN is presented. Some security analysis and tests are given to show the effective of the encryption system .},
  archive      = {J_NN},
  author       = {Liping Chen and Hao Yin and Tingwen Huang and Liguo Yuan and Song Zheng and Lisheng Yin},
  doi          = {10.1016/j.neunet.2020.02.008},
  journal      = {Neural Networks},
  pages        = {174-184},
  shortjournal = {Neural Netw.},
  title        = {Chaos in fractional-order discrete neural networks with application to image encryption},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reachable set bounding for neural networks with mixed
delays: Reciprocally convex approach. <em>NN</em>, <em>125</em>,
165–173. (<a
href="https://doi.org/10.1016/j.neunet.2020.02.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper discusses the reachable set estimation problem of neural networks with mixed delays. Firstly, by means of the maximal Lyapunov–Krasovskii functional, we obtain a non-ellipsoid form of the reachable set. Further more, when calculating the derivative of the maximum Lyapunov functional , the lower bound lemma and reciprocally convex approach method are used to solve the reciprocally convex combination term, which reduce the related decision variables. Secondly, we extend the results to polytopic uncertainties neural networks and consider the case of uncertain differentiable parameters. Finally, two numerical examples and one application example are listed to show the validity of our methods.},
  archive      = {J_NN},
  author       = {Ruihan Chen and Song Zhu and Yongqiang Qi and Yuxin Hou},
  doi          = {10.1016/j.neunet.2020.02.005},
  journal      = {Neural Networks},
  pages        = {165-173},
  shortjournal = {Neural Netw.},
  title        = {Reachable set bounding for neural networks with mixed delays: Reciprocally convex approach},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust min–max optimal control design for systems with
uncertain models: A neural dynamic programming approach. <em>NN</em>,
<em>125</em>, 153–164. (<a
href="https://doi.org/10.1016/j.neunet.2020.01.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The design of an artificial neural network (ANN) based sub-optimal controller to solve the finite-horizon optimization problem for a class of systems with uncertainties is the main outcome of this study. The optimization problem considers a convex performance index in the Bolza form . The dynamic uncertain restriction is considered as a linear system affected by modeling uncertainties, as well as by external bounded perturbations. The proposed controller implements a min–max approach based on the dynamic neural programming approximate solution. An ANN approximates the Value function to get the estimate of the Hamilton–Jacobi–Bellman (HJB) equation solution. The explicit adaptive law for the weights in the ANN is obtained from the approximation of the HJB solution. The stability analysis based on the Lyapunov theory yields to confirm that the approximate Value function serves as a Lyapunov function candidate and to conclude the practical stability of the equilibrium point. A simulation example illustrates the characteristics of the sub-optimal controller. The comparison of the performance indexes obtained with the application of different controllers evaluates the effect of perturbations and the sub-optimal solution.},
  archive      = {J_NN},
  author       = {Mariana Ballesteros and Isaac Chairez and Alexander Poznyak},
  doi          = {10.1016/j.neunet.2020.01.016},
  journal      = {Neural Networks},
  pages        = {153-164},
  shortjournal = {Neural Netw.},
  title        = {Robust min–max optimal control design for systems with uncertain models: A neural dynamic programming approach},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast discrete cross-modal hashing with semantic consistency.
<em>NN</em>, <em>125</em>, 142–152. (<a
href="https://doi.org/10.1016/j.neunet.2020.01.035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Supervised cross-modal hashing has attracted widespread concentrations for large-scale retrieval task due to its promising retrieval performance . However, most existing works suffer from some of following issues. Firstly, most of them only leverage the pair-wise similarity matrix to learn hash codes, which may result in class information loss. Secondly, the pair-wise similarity matrix generally lead to high computing complexity and memory cost. Thirdly, most of them relax the discrete constraints during optimization, which generally results in large cumulative quantization error and consequent inferior hash codes. To address above problems, we present a Fast Discrete Cross-modal Hashing method in this paper, FDCH for short. Specifically, it firstly leverages both class labels and the pair-wise similarity matrix to learn a sharing Hamming space where the semantic consistency can be better preserved. Then we propose an asymmetric hash codes learning model to avoid the challenging issue of symmetric matrix factorization . Finally, an effective and efficient discrete optimal scheme is designed to generate discrete hash codes directly, and the computing complexity and memory cost caused by the pair-wise similarity matrix are reduced from O ( n 2 ) O(n2) to O ( n ) O(n) , where n n denotes the size of training set. Extensive experiments conducted on three real world datasets highlight the superiority of FDCH compared with several cross-modal hashing methods and demonstrate its effectiveness and efficiency.},
  archive      = {J_NN},
  author       = {Tao Yao and Lianshan Yan and Yilan Ma and Hong Yu and Qingtang Su and Gang Wang and Qi Tian},
  doi          = {10.1016/j.neunet.2020.01.035},
  journal      = {Neural Networks},
  pages        = {142-152},
  shortjournal = {Neural Netw.},
  title        = {Fast discrete cross-modal hashing with semantic consistency},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Preserving differential privacy in deep neural networks with
relevance-based adaptive noise imposition. <em>NN</em>, <em>125</em>,
131–141. (<a
href="https://doi.org/10.1016/j.neunet.2020.02.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep learning achieves remarkable results in the field of artificial intelligence . However, the training process of deep neural networks may cause the leakage of individual privacy. Given the model and some background information of the target individual, the adversary can maliciously infer the sensitive feature of the target individual. Therefore, it is imperative to preserve the sensitive information in the training data. Differential privacy is a state-of-the-art paradigm for providing the privacy guarantee of datasets, which protects the private and sensitive information from the attack of adversaries significantly. However, the existing privacy-preserving models based on differential privacy are less than satisfactory since traditional approaches always inject the same amount of noise into parameters to preserve the sensitive information, which may impact the trade-off between the model utility and the privacy guarantee of training data. In this paper, we present a general differentially private deep neural networks learning framework based on relevance analysis, which aims to bridge the gap between private and non-private models while providing an effective privacy guarantee of sensitive information. The proposed model perturbs gradients according to the relevance between neurons in different layers and the model output. Specifically, during the process of backward propagation, more noise is added to gradients of neurons that have less relevance to the model output, and vice-versa. Experiments on five real datasets demonstrate that our mechanism not only bridges the gap between private and non-private models, but also prevents the disclosure of sensitive information effectively.},
  archive      = {J_NN},
  author       = {Maoguo Gong and Ke Pan and Yu Xie and A.K. Qin and Zedong Tang},
  doi          = {10.1016/j.neunet.2020.02.001},
  journal      = {Neural Networks},
  pages        = {131-141},
  shortjournal = {Neural Netw.},
  title        = {Preserving differential privacy in deep neural networks with relevance-based adaptive noise imposition},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the localness modeling for the self-attention based
end-to-end speech synthesis. <em>NN</em>, <em>125</em>, 121–130. (<a
href="https://doi.org/10.1016/j.neunet.2020.01.034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attention based end-to-end speech synthesis achieves better performance in both prosody and quality compared to the conventional “front-end”–“back-end” structure. But training such end-to-end framework is usually time-consuming because of the use of recurrent neural networks . To enable parallel calculation and long-range dependency modeling, a solely self-attention based framework named Transformer is proposed recently in the end-to-end family. However, it lacks position information in sequential modeling, so that the extra position representation is crucial to achieve good performance. Besides, the weighted sum form of self-attention is conducted over the whole input sequence when computing latent representation, which may disperse the attention to the whole input sequence other than focusing on the more important neighboring input states, resulting in generation errors. In this paper, we introduce two localness modeling methods to enhance the self-attention based representation for speech synthesis , which maintain the abilities of parallel computation and global-range dependency modeling in self-attention while improving the generation stability. We systematically analyze the solely self-attention based end-to-end speech synthesis framework, and unveil the importance of local context. Then we add the proposed relative-position-aware method to enhance local edges and experiment with different architectures to examine the effectiveness of localness modeling. In order to achieve query-specific window and discard the hyper-parameter of the relative-position-aware approach, we further conduct Gaussian-based bias to enhance localness. Experimental results indicate that the two proposed localness enhanced methods can both improve the performance of the self-attention model, especially when applied to the encoder part. And the query-specific window of Gaussian bias approach is more robust compared with the fixed relative edges.},
  archive      = {J_NN},
  author       = {Shan Yang and Heng Lu and Shiyin Kang and Liumeng Xue and Jinba Xiao and Dan Su and Lei Xie and Dong Yu},
  doi          = {10.1016/j.neunet.2020.01.034},
  journal      = {Neural Networks},
  pages        = {121-130},
  shortjournal = {Neural Netw.},
  title        = {On the localness modeling for the self-attention based end-to-end speech synthesis},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Weighted discriminative collaborative competitive
representation for robust image classification. <em>NN</em>,
<em>125</em>, 104–120. (<a
href="https://doi.org/10.1016/j.neunet.2020.01.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative representation-based classification (CRC) is a famous representation-based classification method in pattern recognition. Recently, many variants of CRC have been designed for many classification tasks with the good classification performance. However, most of them ignore the inter-class pattern discrimination among the class-specific representations, which is very critical for strengthening the pattern discrimination of collaborative representation (CR). In this article, we propose a novel CR approach for image classification , called weighted discriminative collaborative competitive representation (WDCCR). The proposed WDCCR designs the discriminative and competitive collaborative representation among all the classes by fully considering the class information. On the one hand, we incorporate two discriminative constraints into the unified WDCCR model. Both constraints are the competitive class-specific representation residuals and the pairs of class-specific representations for each query sample. On the other hand, the constraint of the weighted categorical representation coefficients is introduced into the proposed model for further enhancing the power of discriminative and competitive representation. In the weighted constraint, we assume that the different classes of each query sample should have less contribution to the representation with the small representation coefficients, and then two types of weight factors are designed to constrain the representation coefficients. Furthermore, the robust WDCCR (R-WDCCR) is proposed with l 1 l1 -norm representation fidelity for recognizing noisy images . Extensive experiments on six image data sets demonstrate the effective and robust superiorities of the proposed WDCCR and R-WDCCR over the related state-of-the-art representation-based classification methods.},
  archive      = {J_NN},
  author       = {Jianping Gou and Lei Wang and Zhang Yi and Yunhao Yuan and Weihua Ou and Qirong Mao},
  doi          = {10.1016/j.neunet.2020.01.020},
  journal      = {Neural Networks},
  pages        = {104-120},
  shortjournal = {Neural Netw.},
  title        = {Weighted discriminative collaborative competitive representation for robust image classification},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mu-net: Multi-scale u-net for two-photon microscopy image
denoising and restoration. <em>NN</em>, <em>125</em>, 92–103. (<a
href="https://doi.org/10.1016/j.neunet.2020.01.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in two two-photon microscopy (2PM) have made three-dimensional (3D) neural imaging of deep cortical regions possible. However, 2PM often suffers from poor image quality because of various noise factors, including blur, white noise, and photo bleaching. In addition, the effectiveness of the existing image processing methods is limited because of the special features of 2PM images such as deeper tissue penetration but higher image noises owing to rapid laser scanning. To address the denoising problems in 2PM 3D images, we present a new algorithm based on deep convolutional neural networks (CNNs). The proposed model consists of multiple U-nets in which an individual U-net removes noises at different scales and then yields a performance improvement based on a coarse-to-fine strategy. Moreover, the constituent CNNs employ fully 3D convolution operations. Such an architecture enables the proposed model to facilitate end-to-end learning without any pre/post processing. Based on the experiments on 2PM image denoising , we observed that our new algorithm demonstrates substantial performance improvements over other baseline methods .},
  archive      = {J_NN},
  author       = {Sehyung Lee and Makiko Negishi and Hidetoshi Urakubo and Haruo Kasai and Shin Ishii},
  doi          = {10.1016/j.neunet.2020.01.026},
  journal      = {Neural Networks},
  pages        = {92-103},
  shortjournal = {Neural Netw.},
  title        = {Mu-net: Multi-scale U-net for two-photon microscopy image denoising and restoration},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PET image super-resolution using generative adversarial
networks. <em>NN</em>, <em>125</em>, 83–91. (<a
href="https://doi.org/10.1016/j.neunet.2020.01.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The intrinsically low spatial resolution of positron emission tomography (PET) leads to image quality degradation and inaccurate image-based quantitation. Recently developed supervised super-resolution (SR) approaches are of great relevance to PET but require paired low- and high-resolution images for training, which are usually unavailable for clinical datasets. In this paper, we present a self-supervised SR (SSSR) technique for PET based on dual generative adversarial networks (GANs), which precludes the need for paired training data, ensuring wider applicability and adoptability. The SSSR network receives as inputs a low-resolution PET image, a high-resolution anatomical magnetic resonance (MR) image, spatial information (axial and radial coordinates), and a high-dimensional feature set extracted from an auxiliary CNN which is separately-trained in a supervised manner using paired simulation datasets. The network is trained using a loss function which includes two adversarial loss terms, a cycle consistency term, and a total variation penalty on the SR image. We validate the SSSR technique using a clinical neuroimaging dataset. We demonstrate that SSSR is promising in terms of image quality, peak signal-to-noise ratio, structural similarity index, contrast-to-noise ratio, and an additional no-reference metric developed specifically for SR image quality assessment . Comparisons with other SSSR variants suggest that its high performance is largely attributable to simulation guidance.},
  archive      = {J_NN},
  author       = {Tzu-An Song and Samadrita Roy Chowdhury and Fan Yang and Joyita Dutta},
  doi          = {10.1016/j.neunet.2020.01.029},
  journal      = {Neural Networks},
  pages        = {83-91},
  shortjournal = {Neural Netw.},
  title        = {PET image super-resolution using generative adversarial networks},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Training high-performance and large-scale deep neural
networks with full 8-bit integers. <em>NN</em>, <em>125</em>, 70–82. (<a
href="https://doi.org/10.1016/j.neunet.2019.12.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural network (DNN) quantization converting floating-point (FP) data in the network to integers (INT) is an effective way to shrink the model size for memory saving and simplify the operations for compute acceleration. Recently, researches on DNN quantization develop from inference to training, laying a foundation for the online training on accelerators. However, existing schemes leaving batch normalization (BN) untouched during training are mostly incomplete quantization that still adopts high precision FP in some parts of the data paths. Currently, there is no solution that can use only low bit-width INT data during the whole training process of large-scale DNNs with acceptable accuracy. In this work, through decomposing all the computation steps in DNNs and fusing three special quantization functions to satisfy the different precision requirements, we propose a unified complete quantization framework termed as “WAGEUBN” to quantize DNNs involving all data paths including W (Weights), A (Activation), G (Gradient), E (Error), U (Update), and BN. Moreover, the Momentum optimizer is also quantized to realize a completely quantized framework. Experiments on ResNet18/34/50 models demonstrate that WAGEUBN can achieve competitive accuracy on the ImageNet dataset . For the first time, the study of quantization in large-scale DNNs is advanced to the full 8-bit INT level. In this way, all the operations in the training and inference can be bit-wise operations, pushing towards faster processing speed, decreased memory cost, and higher energy efficiency. Our throughout quantization framework has great potential for future efficient portable devices with online learning ability.},
  archive      = {J_NN},
  author       = {Yukuan Yang and Lei Deng and Shuang Wu and Tianyi Yan and Yuan Xie and Guoqi Li},
  doi          = {10.1016/j.neunet.2019.12.027},
  journal      = {Neural Networks},
  pages        = {70-82},
  shortjournal = {Neural Netw.},
  title        = {Training high-performance and large-scale deep neural networks with full 8-bit integers},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neuromodulated attention and goal-driven perception in
uncertain domains. <em>NN</em>, <em>125</em>, 56–69. (<a
href="https://doi.org/10.1016/j.neunet.2020.01.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In uncertain domains, the goals are often unknown and need to be predicted by the organism or system. In this paper, contrastive Excitation Backprop (c-EB) was used in two goal-driven perception tasks – one with pairs of noisy MNIST digits and the other with a robot in an action-based attention scenario. The first task included attending to even, odd, low, and high digits, whereas the second task included action goals, such as “eat”, “work-on-computer”, “read”, and “say-hi” that led to attention to objects associated with those actions. The system needed to increase attention to target items and decrease attention to distractor items and background noise. Because the valid goal was unknown, an online learning model based on the cholinergic and noradrenergic neuromodulatory systems was used to predict a noisy goal ( expected uncertainty ) and re-adapt when the goal changed ( unexpected uncertainty ). This neurobiologically plausible model demonstrates how neuromodulatory systems can predict goals in uncertain domains and how attentional mechanisms can enhance the perception for that goal.},
  archive      = {J_NN},
  author       = {Xinyun Zou and Soheil Kolouri and Praveen K. Pilly and Jeffrey L. Krichmar},
  doi          = {10.1016/j.neunet.2020.01.031},
  journal      = {Neural Networks},
  pages        = {56-69},
  shortjournal = {Neural Netw.},
  title        = {Neuromodulated attention and goal-driven perception in uncertain domains},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Skeleton-based chinese sign language recognition and
generation for bidirectional communication between deaf and hearing
people. <em>NN</em>, <em>125</em>, 41–55. (<a
href="https://doi.org/10.1016/j.neunet.2020.01.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chinese sign language (CSL) is one of the most widely used sign language systems in the world. As such, the automatic recognition and generation of CSL is a key technology enabling bidirectional communication between deaf and hearing people. Most previous studies have focused solely on sign language recognition (SLR), which only addresses communication in a single direction. As such, there is a need for sign language generation (SLG) to enable communication in the other direction (i.e., from hearing people to deaf people). To achieve a smoother exchange of ideas between these two groups, we propose a skeleton-based CSL recognition and generation framework based on a recurrent neural network (RNN), to support bidirectional CSL communication. This process can also be extended to other sequence-to-sequence information interactions. The core of the proposed framework is a two-level probability generative model . Compared with previous techniques, this approach offers a more flexible approximate posterior distribution , which can produce skeletal sequences of varying styles that are recognizable to humans. In addition, the proposed generation method compensated for a lack of training data. A series of experiments in bidirectional communication were conducted on the large 500 CSL dataset. The proposed algorithm achieved high recognition accuracy for both real and synthetic data, with a reduced runtime. Furthermore, the generated data improved the performance of the discriminator . These results suggest the proposed bidirectional communication framework and generation algorithm to be an effective new approach to CSL recognition.},
  archive      = {J_NN},
  author       = {Qinkun Xiao and Minying Qin and Yuting Yin},
  doi          = {10.1016/j.neunet.2020.01.030},
  journal      = {Neural Networks},
  pages        = {41-55},
  shortjournal = {Neural Netw.},
  title        = {Skeleton-based chinese sign language recognition and generation for bidirectional communication between deaf and hearing people},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Event-triggered synchronization of discrete-time neural
networks: A switching approach. <em>NN</em>, <em>125</em>, 31–40. (<a
href="https://doi.org/10.1016/j.neunet.2020.01.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the event-triggered synchronization control of discrete-time neural networks . The main highlights are threefold: (1) a new event-triggered mechanism (ETM) is presented, which can be regarded as a switching between the discrete-time periodic sampled-data control and a continuous ETM; (2) a saturating controller which is equipped with two switching gains is designed to match the switching property of the proposed ETM; (3) a dedicated switching Lyapunov–Krasovskii functional is constructed, which takes the sawtooth constraints of control input into account. Based on these ingredients, the synchronization criteria are derived such that the considered error systems are locally stable. Whereafter, two co-design problems are discussed to maximize the set of admissible initial conditions and the triggering threshold, respectively. Finally, the effectiveness and advantages of the proposed method are validated by two numerical examples.},
  archive      = {J_NN},
  author       = {Sanbo Ding and Zhanshan Wang},
  doi          = {10.1016/j.neunet.2020.01.024},
  journal      = {Neural Networks},
  pages        = {31-40},
  shortjournal = {Neural Netw.},
  title        = {Event-triggered synchronization of discrete-time neural networks: A switching approach},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reconstruction of natural visual scenes from neural spikes
with deep neural networks. <em>NN</em>, <em>125</em>, 19–30. (<a
href="https://doi.org/10.1016/j.neunet.2020.01.033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural coding is one of the central questions in systems neuroscience for understanding how the brain processes stimulus from the environment, moreover, it is also a cornerstone for designing algorithms of brain–machine interface, where decoding incoming stimulus is highly demanded for better performance of physical devices. Traditionally researchers have focused on functional magnetic resonance imaging (fMRI) data as the neural signals of interest for decoding visual scenes. However, our visual perception operates in a fast time scale of millisecond in terms of an event termed neural spike. There are few studies of decoding by using spikes. Here we fulfill this aim by developing a novel decoding framework based on deep neural networks , named spike-image decoder (SID), for reconstructing natural visual scenes, including static images and dynamic videos, from experimentally recorded spikes of a population of retinal ganglion cells. The SID is an end-to-end decoder with one end as neural spikes and the other end as images, which can be trained directly such that visual scenes are reconstructed from spikes in a highly accurate fashion. Our SID also outperforms on the reconstruction of visual stimulus compared to existing fMRI decoding models. In addition, with the aid of a spike encoder, we show that SID can be generalized to arbitrary visual scenes by using the image datasets of MNIST, CIFAR10, and CIFAR100. Furthermore, with a pre-trained SID, one can decode any dynamic videos to achieve real-time encoding and decoding of visual scenes by spikes. Altogether, our results shed new light on neuromorphic computing for artificial visual systems, such as event-based visual cameras and visual neuroprostheses .},
  archive      = {J_NN},
  author       = {Yichen Zhang and Shanshan Jia and Yajing Zheng and Zhaofei Yu and Yonghong Tian and Siwei Ma and Tiejun Huang and Jian K. Liu},
  doi          = {10.1016/j.neunet.2020.01.033},
  journal      = {Neural Networks},
  pages        = {19-30},
  shortjournal = {Neural Netw.},
  title        = {Reconstruction of natural visual scenes from neural spikes with deep neural networks},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling uncertainty-seeking behavior mediated by
cholinergic influence on dopamine. <em>NN</em>, <em>125</em>, 10–18. (<a
href="https://doi.org/10.1016/j.neunet.2020.01.032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent findings suggest that acetylcholine mediates uncertainty-seeking behaviors through its projection to dopamine neurons — another neuromodulatory system known for its major role in reinforcement learning and decision-making. In this paper, we propose a leaky-integrate-and-fire model of this mechanism. It implements a softmax-like selection with an uncertainty bonus by a cholinergic drive to dopaminergic neurons , which in turn influence synaptic currents of downstream neurons. The model is able to reproduce experimental data in two decision-making tasks. It also predicts that: (i) in the absence of cholinergic input, dopaminergic activity would not correlate with uncertainty, and that (ii) the adaptive advantage brought by the implemented uncertainty-seeking mechanism is most useful when sources of reward are not highly uncertain. Moreover, this modeling work allows us to propose novel experiments which might shed new light on the role of acetylcholine in both random and directed exploration. Overall, this study contributes to a more comprehensive understanding of the role of the cholinergic system and, in particular, its involvement in decision-making.},
  archive      = {J_NN},
  author       = {Marwen Belkaid and Jeffrey L. Krichmar},
  doi          = {10.1016/j.neunet.2020.01.032},
  journal      = {Neural Networks},
  pages        = {10-18},
  shortjournal = {Neural Netw.},
  title        = {Modeling uncertainty-seeking behavior mediated by cholinergic influence on dopamine},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Transductive LSTM for time-series prediction: An application
to weather forecasting. <em>NN</em>, <em>125</em>, 1–9. (<a
href="https://doi.org/10.1016/j.neunet.2019.12.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long Short-Term Memory (LSTM) has shown significant performance on many real-world applications due to its ability to capture long-term dependencies. In this paper, we utilize LSTM to obtain a data-driven forecasting model for an application of weather forecasting. Moreover, we propose Transductive LSTM (T-LSTM) which exploits the local information in time-series prediction. In transductive learning , the samples in the test point vicinity are considered to have higher impact on fitting the model. In this study, a quadratic cost function is considered for the regression problem . Localizing the objective function is done by considering a weighted quadratic cost function at which point the samples in the neighborhood of the test point have larger weights. We investigate two weighting schemes based on the cosine similarity between the training samples and the test point. In order to assess the performance of the proposed method in different weather conditions, the experiments are conducted on two different time periods of a year. The results show that T-LSTM results in better performance in the prediction task.},
  archive      = {J_NN},
  author       = {Zahra Karevan and Johan A.K. Suykens},
  doi          = {10.1016/j.neunet.2019.12.030},
  journal      = {Neural Networks},
  pages        = {1-9},
  shortjournal = {Neural Netw.},
  title        = {Transductive LSTM for time-series prediction: An application to weather forecasting},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020i). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>124</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(20)30062-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(20)30062-9},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020i). Current events. <em>NN</em>, <em>124</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(20)30061-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(20)30061-7},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {Current events},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Universal approximation with quadratic deep networks.
<em>NN</em>, <em>124</em>, 383–392. (<a
href="https://doi.org/10.1016/j.neunet.2020.01.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, deep learning has achieved huge successes in many important applications. In our previous studies, we proposed quadratic/second-order neurons and deep quadratic neural networks . In a quadratic neuron, the inner product of a vector of data and the corresponding weights in a conventional neuron is replaced with a quadratic function . The resultant quadratic neuron enjoys an enhanced expressive capability over the conventional neuron. However, how quadratic neurons improve the expressing capability of a deep quadratic network has not been studied up to now, preferably in relation to that of a conventional neural network. Specifically, we ask four basic questions in this paper: (1) for the one-hidden-layer network structure, is there any function that a quadratic network can approximate much more efficiently than a conventional network? (2) for the same multi-layer network structure, is there any function that can be expressed by a quadratic network but cannot be expressed with conventional neurons in the same structure? (3) Does a quadratic network give a new insight into universal approximation? (4) To approximate the same class of functions with the same error bound, could a quantized quadratic network have a lower number of weights than a quantized conventional network? Our main contributions are the four interconnected theorems shedding light upon these four questions and demonstrating the merits of a quadratic network in terms of expressive efficiency, unique capability, compact architecture and computational capacity respectively.},
  archive      = {J_NN},
  author       = {Fenglei Fan and Jinjun Xiong and Ge Wang},
  doi          = {10.1016/j.neunet.2020.01.007},
  journal      = {Neural Networks},
  pages        = {383-392},
  shortjournal = {Neural Netw.},
  title        = {Universal approximation with quadratic deep networks},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The feature extraction of resting-state EEG signal from
amnestic mild cognitive impairment with type 2 diabetes mellitus based
on feature-fusion multispectral image method. <em>NN</em>, <em>124</em>,
373–382. (<a
href="https://doi.org/10.1016/j.neunet.2020.01.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, combining feature extraction and classification method of electroencephalogram (EEG) signals has been widely used in identifying mild cognitive impairment . However, it remains unclear which feature of EEG signals is best effective in assessing amnestic mild cognitive impairment (aMCI) with type 2 diabetes mellitus (T2DM) when combining one classifier. This study proposed a novel feature extraction method of EEG signals named feature-fusion multispectral image method (FMIM) for diagnosis of aMCI with T2DM. The FMIM was integrated with convolutional neural network (CNN) to classify the processed multispectral image data. The results showed that FMIM could effectively identify aMCI with T2DM from the control group compared to existing multispectral image method (MIM), with improvements including the type and quantity of feature extraction. Meanwhile, part of the invalid calculation could be avoided during the classification process. In addition, the classification evaluation indexes were best under the combination of Alpha2-Beta1-Beta2 frequency bands in data set based on FMIM-1, and were also best under the combination of the Theta-Alpha1-Alpha2-Beta1-Beta2 frequency bands in data set based on FMIM-2. Therefore, FMIM can be used as an effective feature extraction method of aMCI with T2DM, and as a valuable biomarker in clinical applications.},
  archive      = {J_NN},
  author       = {Dong Wen and Peng Li and Xiaoli Li and Zhenhao Wei and Yanhong Zhou and Huan Pei and Fengnian Li and Zhijie Bian and Lei Wang and Shimin Yin},
  doi          = {10.1016/j.neunet.2020.01.025},
  journal      = {Neural Networks},
  pages        = {373-382},
  shortjournal = {Neural Netw.},
  title        = {The feature extraction of resting-state EEG signal from amnestic mild cognitive impairment with type 2 diabetes mellitus based on feature-fusion multispectral image method},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A deep CNN approach to decode motor preparation of upper
limbs from time–frequency maps of EEG signals at source level.
<em>NN</em>, <em>124</em>, 357–372. (<a
href="https://doi.org/10.1016/j.neunet.2020.01.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A system that can detect the intention to move and decode the planned movement could help all those subjects that can plan motion but are unable to implement it. In this paper, motor planning activity is investigated by using electroencephalographic (EEG) signals with the aim to decode motor preparation phases. A publicly available database of 61-channels EEG signals recorded from 15 healthy subjects during the execution of different movements (elbow flexion/extension, forearm pronation/supination, hand open/close) of the right upper limb was employed to generate a dataset of EEG epochs preceding resting and movement’s onset. A novel system is introduced for the classification of premovement vs resting and of premovement vs premovement epochs. For every epoch, the proposed system generates a time–frequency (TF) map of every source signal in the motor cortex , through beamforming and Continuous Wavelet Transform (CWT), then all the maps are embedded in a volume and used as input to a deep CNN . The proposed system succeeded in discriminating premovement from resting with an average accuracy of 90.3\% (min 74.6\%, max 100\%), outperforming comparable methods in the literature, and in discriminating premovement vs premovement with an average accuracy of 62.47\%. The achieved results encourage to investigate motor planning at source level in the time–frequency domain through deep learning approaches.},
  archive      = {J_NN},
  author       = {Nadia Mammone and Cosimo Ieracitano and Francesco C. Morabito},
  doi          = {10.1016/j.neunet.2020.01.027},
  journal      = {Neural Networks},
  pages        = {357-372},
  shortjournal = {Neural Netw.},
  title        = {A deep CNN approach to decode motor preparation of upper limbs from time–frequency maps of EEG signals at source level},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A unified model of rule-set learning and selection.
<em>NN</em>, <em>124</em>, 343–356. (<a
href="https://doi.org/10.1016/j.neunet.2020.01.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to focus on relevant information and ignore irrelevant information is a fundamental part of intelligent behavior . It not only allows faster acquisition of new tasks by reducing the size of the problem space but also allows for generalizations to novel stimuli. Task-switching, task-sets, and rule-set learning are all intertwined with this ability. There are many models that attempt to individually describe these cognitive abilities. However, there are few models that try to capture the breadth of these topics in a unified model and fewer still that do it while adhering to the biological constraints imposed by the findings from the field of neuroscience . Presented here is a comprehensive model of rule-set learning and selection that can capture the learning curve results, error-type data, and transfer effects found in rule-learning studies while also replicating the reaction time data and various related effects of task-set and task-switching experiments. The model also factors in many disparate neurological findings, several of which are often disregarded by similar models.},
  archive      = {J_NN},
  author       = {Pierson Fleischer and Sébastien Hélie},
  doi          = {10.1016/j.neunet.2020.01.028},
  journal      = {Neural Networks},
  pages        = {343-356},
  shortjournal = {Neural Netw.},
  title        = {A unified model of rule-set learning and selection},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A model for navigation in unknown environments based on a
reservoir of hippocampal sequences. <em>NN</em>, <em>124</em>, 328–342.
(<a href="https://doi.org/10.1016/j.neunet.2020.01.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hippocampal place cell populations are activated in sequences on multiple time scales during active behavior , resting and sleep states, suggesting that these sequences are the genuine dynamical motifs of the hippocampal circuit. Recently, prewired hippocampal place cell sequences have even been reported to correlate to future behaviors, but so far there is no explanation of what could be the computational benefits of such a mapping between intrinsic dynamical structure and external sensory inputs. Here, I propose a computational model in which a set of predefined internal sequences is used as a dynamical reservoir to construct a spatial map of a large unknown maze based on only a small number of salient landmarks. The model is based on a new variant of temporal difference learning and implements a simultaneous localization and mapping algorithm. As a result sequences during intermittent replay periods can be decoded as spatial trajectories and improve navigation performance, which supports the functional interpretation of replay to consolidate memories of motor actions.},
  archive      = {J_NN},
  author       = {Christian Leibold},
  doi          = {10.1016/j.neunet.2020.01.014},
  journal      = {Neural Networks},
  pages        = {328-342},
  shortjournal = {Neural Netw.},
  title        = {A model for navigation in unknown environments based on a reservoir of hippocampal sequences},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Theory of deep convolutional neural networks: downsampling.
<em>NN</em>, <em>124</em>, 319–327. (<a
href="https://doi.org/10.1016/j.neunet.2020.01.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Establishing a solid theoretical foundation for structured deep neural networks is greatly desired due to the successful applications of deep learning in various practical domains. This paper aims at an approximation theory of deep convolutional neural networks whose structures are induced by convolutions . To overcome the difficulty in theoretical analysis of the networks with linearly increasing widths arising from convolutions , we introduce a downsampling operator to reduce the widths. We prove that the downsampled deep convolutional neural networks can be used to approximate ridge functions nicely, which hints some advantages of these structured networks in terms of approximation or modeling. We also prove that the output of any multi-layer fully-connected neural network can be realized by that of a downsampled deep convolutional neural network with free parameters of the same order, which shows that in general, the approximation ability of deep convolutional neural networks is at least as good as that of fully-connected networks. Finally, a theorem for approximating functions on Riemannian manifolds is presented, which demonstrates that deep convolutional neural networks can be used to learn manifold features of data.},
  archive      = {J_NN},
  author       = {Ding-Xuan Zhou},
  doi          = {10.1016/j.neunet.2020.01.018},
  journal      = {Neural Networks},
  pages        = {319-327},
  shortjournal = {Neural Netw.},
  title        = {Theory of deep convolutional neural networks: Downsampling},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Effective metric learning with co-occurrence embedding for
collaborative recommendations. <em>NN</em>, <em>124</em>, 308–318. (<a
href="https://doi.org/10.1016/j.neunet.2020.01.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recommender systems , matrix factorization and its variants have grown up to be dominant in collaborative filtering due to their simplicity and effectiveness. In matrix factorization based methods, dot product which is actually used as a measure of distance from users to items, does not satisfy the inequality property, and thus may fail to capture the inner grained preference information and further limits the performance of recommendations. Metric learning produces distance functions that capture the essential relationships among rating data and has been successfully explored in collaborative recommendations . However, without the global statistical information of user–user pairs and item–item pairs, it makes the model easy to achieve a suboptimal metric. For this, we present a co-occurrence embedding regularized metric learning model (CRML) for collaborative recommendations . We consider the optimization problem as a multi-task learning problem which includes optimizing a primary task of metric learning and two auxiliary tasks of representation learning . In particular, we develop an effective approach for learning the embedding representations of both users and items, and then exploit the strategy of soft parameter sharing to optimize the model parameters. Empirical experiments on four datasets demonstrate that the CRML model can enhance the naive metric learning model and significantly outperforms the state-of-the-art methods in terms of accuracy of collaborative recommendations.},
  archive      = {J_NN},
  author       = {Hao Wu and Qimin Zhou and Rencan Nie and Jinde Cao},
  doi          = {10.1016/j.neunet.2020.01.021},
  journal      = {Neural Networks},
  pages        = {308-318},
  shortjournal = {Neural Netw.},
  title        = {Effective metric learning with co-occurrence embedding for collaborative recommendations},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). K-anonymity inspired adversarial attack and multiple
one-class classification defense. <em>NN</em>, <em>124</em>, 296–307.
(<a href="https://doi.org/10.1016/j.neunet.2020.01.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel adversarial attack methodology for fooling deep neural network classifiers in image classification tasks is proposed, along with a novel defense mechanism to counter such attacks. Two concepts are introduced, namely the K-Anonymity-inspired Adversarial Attack (K-A 3 3 ) and the Multiple Support Vector Data Description Defense (M-SVDD-D). The proposed K-A 3 3 introduces novel optimization criteria to standard adversarial attack methodologies, inspired by the K-Anonymity principles. Its generated adversarial examples are not only misclassified by the neural network classifier, but are uniformly spread along K K different ranked output positions. The proposed M-SVDD-D consists of a deep neural architecture layer consisting of multiple non-linear one-class classifiers based on Support Vector Data Description that can be used to replace the final linear classification layer of a deep neural architecture, and an additional class verification mechanism. Its application decreases the effectiveness of adversarial attacks, by increasing the noise energy required to deceive the protected model, attributed to the introduced non-linearity. In addition, M-SVDD-D can be used to prevent adversarial attacks in black-box attack settings.},
  archive      = {J_NN},
  author       = {Vasileios Mygdalis and Anastasios Tefas and Ioannis Pitas},
  doi          = {10.1016/j.neunet.2020.01.015},
  journal      = {Neural Networks},
  pages        = {296-307},
  shortjournal = {Neural Netw.},
  title        = {K-anonymity inspired adversarial attack and multiple one-class classification defense},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved value iteration for neural-network-based stochastic
optimal control design. <em>NN</em>, <em>124</em>, 280–295. (<a
href="https://doi.org/10.1016/j.neunet.2020.01.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel value iteration adaptive dynamic programming (ADP) algorithm is presented, which is called an improved value iteration ADP algorithm, to obtain the optimal policy for discrete stochastic processes . In the improved value iteration ADP algorithm, for the first time we propose a new criteria to verify whether the obtained policy is stable or not for stochastic processes . By analyzing the convergence properties of the proposed algorithm, it is shown that the iterative value functions can converge to the optimum. In addition, our algorithm allows the initial value function to be an arbitrary positive semi-definite function. Finally, two simulation examples are presented to validate the effectiveness of the developed method.},
  archive      = {J_NN},
  author       = {Mingming Liang and Ding Wang and Derong Liu},
  doi          = {10.1016/j.neunet.2020.01.004},
  journal      = {Neural Networks},
  pages        = {280-295},
  shortjournal = {Neural Netw.},
  title        = {Improved value iteration for neural-network-based stochastic optimal control design},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning deformable registration of medical images with
anatomical constraints. <em>NN</em>, <em>124</em>, 269–279. (<a
href="https://doi.org/10.1016/j.neunet.2020.01.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deformable image registration is a fundamental problem in the field of medical image analysis. During the last years, we have witnessed the advent of deep learning-based image registration methods which achieve state-of-the-art performance, and drastically reduce the required computational time. However, little work has been done regarding how can we encourage our models to produce not only accurate, but also anatomically plausible results, which is still an open question in the field. In this work, we argue that incorporating anatomical priors in the form of global constraints into the learning process of these models, will further improve their performance and boost the realism of the warped images after registration. We learn global non-linear representations of image anatomy using segmentation masks, and employ them to constraint the registration process. The proposed AC-RegNet architecture is evaluated in the context of chest X-ray image registration using three different datasets, where the high anatomical variability makes the task extremely challenging. Our experiments show that the proposed anatomically constrained registration model produces more realistic and accurate results than state-of-the-art methods, demonstrating the potential of this approach.},
  archive      = {J_NN},
  author       = {Lucas Mansilla and Diego H. Milone and Enzo Ferrante},
  doi          = {10.1016/j.neunet.2020.01.023},
  journal      = {Neural Networks},
  pages        = {269-279},
  shortjournal = {Neural Netw.},
  title        = {Learning deformable registration of medical images with anatomical constraints},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deterministic learning of hybrid fuzzy cognitive maps and
network reduction approaches. <em>NN</em>, <em>124</em>, 258–268. (<a
href="https://doi.org/10.1016/j.neunet.2020.01.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hybrid artificial intelligence deals with the construction of intelligent systems by relying on both human knowledge and historical data records. In this paper, we approach this problem from a neural perspective, particularly when modeling and simulating dynamic systems. Firstly, we propose a Fuzzy Cognitive Map architecture in which experts are requested to define the interaction among the input neurons. As a second contribution, we introduce a fast and deterministic learning rule to compute the weights among input and output neurons. This parameterless learning method is based on the Moore–Penrose inverse and it can be performed in a single step. In addition, we discuss a model to determine the relevance of weights, which allows us to better understand the system. Last but not least, we introduce two calibration methods to adjust the model after the removal of potentially superfluous weights.},
  archive      = {J_NN},
  author       = {Gonzalo Nápoles and Agnieszka Jastrzębska and Carlos Mosquera and Koen Vanhoof and Władysław Homenda},
  doi          = {10.1016/j.neunet.2020.01.019},
  journal      = {Neural Networks},
  pages        = {258-268},
  shortjournal = {Neural Netw.},
  title        = {Deterministic learning of hybrid fuzzy cognitive maps and network reduction approaches},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep feature transfer learning for trusted and automated
malware signature generation in private cloud environments. <em>NN</em>,
<em>124</em>, 243–257. (<a
href="https://doi.org/10.1016/j.neunet.2020.01.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents TrustSign , a novel, trusted automatic malware signature generation method based on high-level deep features transferred from a VGG-19 neural network model pretrained on the ImageNet dataset . While traditional automatic malware signature generation techniques rely on static or dynamic analysis of the malware’s executable, our method overcomes the limitations associated with these techniques by producing signatures based on the presence of the malicious process in the volatile memory . By leveraging the cloud’s virtualization technology, TrustSign analyzes the malicious process in a trusted manner, since the malware is unaware and cannot interfere with the inspection procedure. Additionally, by removing the dependency on the malware’s executable, our method is fully capable of signing fileless malware as well. TrustSign’s signature generation process does not require feature engineering or any additional model training, and it is done in a completely unsupervised manner , eliminating the need for a human expert. Because of this, our method has the advantage of dramatically reducing signature generation and distribution time. In fact, in this paper we rethink the typical use of deep convolutional neural networks and use the VGG-19 model as a topological feature extractor for a vastly different task from the one it was trained for. The results of our experimental evaluation demonstrate TrustSign’s ability to generate signatures impervious to the process state over time. By using the signatures generated by TrustSign as input for various supervised classifiers, we achieved up to 99.5\% classification accuracy .},
  archive      = {J_NN},
  author       = {Daniel Nahmias and Aviad Cohen and Nir Nissim and Yuval Elovici},
  doi          = {10.1016/j.neunet.2020.01.003},
  journal      = {Neural Networks},
  pages        = {243-257},
  shortjournal = {Neural Netw.},
  title        = {Deep feature transfer learning for trusted and automated malware signature generation in private cloud environments},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive complex-valued stepsize based fast learning of
complex-valued neural networks. <em>NN</em>, <em>124</em>, 233–242. (<a
href="https://doi.org/10.1016/j.neunet.2020.01.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex-valued gradient descent algorithm is a popular tool to optimize functions of complex variables, especially for the training of complex-valued neural networks . However, the choice of suitable learning stepsize is a challenging task during the training process. In this paper, an adaptive complex-valued stepsize design method is proposed for complex-valued neural networks by generalizing the adaptable learning rate tree technique to the complex domain. The scaling and rotation factors are introduced to simultaneously adjust the amplitude and phase of complex-valued stepsize. The search range is thus expanded from half line to half plane such that better search direction is obtained at each iteration. We analyze the dynamics of the algorithm near a saddle point and find that it is very easy to escape from the saddle point to guarantee fast convergence and high accuracy. Some experimental results on function approximation and pattern classification tasks are presented to illustrate the advantages of the proposed algorithm over some previous ones.},
  archive      = {J_NN},
  author       = {Yongliang Zhang and He Huang},
  doi          = {10.1016/j.neunet.2020.01.011},
  journal      = {Neural Networks},
  pages        = {233-242},
  shortjournal = {Neural Netw.},
  title        = {Adaptive complex-valued stepsize based fast learning of complex-valued neural networks},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Person re-identification with feature pyramid optimization
and gradual background suppression. <em>NN</em>, <em>124</em>, 223–232.
(<a href="https://doi.org/10.1016/j.neunet.2020.01.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared with face recognition, the performance of person re-identification (re-ID) is still far from practical application. Among various interferences, there are two factors seriously limiting the performance improvement, i.e. , the feature discriminability determined by “external network effectiveness”, and the image quality determined by “internal background clutters”. Target at the “external network effectiveness” problem, feature pyramids are effective to learn discriminative features because they can learn both detailed features from high-resolution shallow layers and semantical features from low-resolution deep layers, however, it can only achieve slight improvement on re-ID tasks because of the error back propagation problem. To handle the problem and utilize the effectiveness of feature pyramids, we propose a strategy called Feature Pyramid Optimization (FPO). Instead of concatenating features directly, the selected layers are optimized independently in a top–bottom order. Target at the “internal background clutters” problem, background suppression is generally considered for removing the environmental interference and improving the image quality. Several mask-based methods are used attempting to totally remove background clutters but achieve limited promotion because of the mask sharpening effect. We propose a novel strategy, i.e. , Gradual Background Suppression (GBS) to reduce the background clutters and keep the smoothness of images simultaneously. Extensive experiments have been conducted and the results demonstrate the effectiveness of both FPO and GBS.},
  archive      = {J_NN},
  author       = {Yingzhi Tang and Xi Yang and Nannan Wang and Bin Song and Xinbo Gao},
  doi          = {10.1016/j.neunet.2020.01.012},
  journal      = {Neural Networks},
  pages        = {223-232},
  shortjournal = {Neural Netw.},
  title        = {Person re-identification with feature pyramid optimization and gradual background suppression},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Directed EEG neural network analysis by LAPPS (p≤1)
penalized sparse granger approach. <em>NN</em>, <em>124</em>, 213–222.
(<a href="https://doi.org/10.1016/j.neunet.2020.01.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The conventional multivariate Granger Analysis (GA) of directed interactions has been widely applied in brain network construction based on EEG recordings as well as fMRI. Nevertheless, EEG is usually inevitably contaminated by strong noise, which may cause network distortion due to the L2-norm used in GAs for directed network recovery. The Lp (p ≤ ≤ 1) norm has been shown to be more robust to outliers as compared to LASSO and L2-GAs. Motivated to construct the sparse brain networks under strong noise condition, we hereby introduce a new approach for GA analysis, termed LAPPS (Least Absolute LP (0= 0.5) to obtain the sparse connections while suppressing the spurious linkages in the networks. The simulation results reveal that LAPPS obtained the best performance under various noise conditions. In a real EEG data test when subjects performed the left and right hand Motor Imagery (MI) for brain network estimation, LAPPS also obtained a sparse network pattern with the hub at the contralateral brain primary motor areas consistent with the physiological basis of MI.},
  archive      = {J_NN},
  author       = {Joyce Chelangat Bore and Peiyang Li and Dennis Joe Harmah and Fali Li and Dezhong Yao and Peng Xu},
  doi          = {10.1016/j.neunet.2020.01.022},
  journal      = {Neural Networks},
  pages        = {213-222},
  shortjournal = {Neural Netw.},
  title        = {Directed EEG neural network analysis by LAPPS (p≤1) penalized sparse granger approach},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). EEG based multi-class seizure type classification using
convolutional neural network and transfer learning. <em>NN</em>,
<em>124</em>, 202–212. (<a
href="https://doi.org/10.1016/j.neunet.2020.01.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognition of epileptic seizure type is essential for the neurosurgeon to understand the cortical connectivity of the brain. Though automated early recognition of seizures from normal electroencephalogram (EEG) was existing, no attempts have been made towards the classification of variants of seizures. Therefore, this study attempts to classify seven variants of seizures with non-seizure EEG through the application of convolutional neural networks (CNN) and transfer learning by making use of the Temple University Hospital EEG corpus. The objective of our study is to perform a multi-class classification of epileptic seizure type , which includes simple partial, complex partial, focal non-specific, generalized non-specific, absence, tonic, and tonic–clonic, and non-seizures. The 19 channels EEG time series was converted into a spectrogram stack before feeding as input to CNN. The following two different modalities were proposed using CNN: (1) Transfer learning using pretrained network, (2) Extract image features using pretrained network and classify using the support vector machine classifier. The following ten pretrained networks were used to identify the optimal network for the proposed study: Alexnet, Vgg16, Vgg19, Squeezenet, Googlenet, Inceptionv3, Densenet201, Resnet18, Resnet50 , and Resnet101. The highest classification accuracy of 82.85\% (using Googlenet) and 88.30\% (using Inceptionv3) was achieved using transfer learning and extract image features approach respectively. Comparison results showed that CNN based approach outperformed conventional feature and clustering based approaches. It can be concluded that the EEG based classification of seizure type using CNN model could be used in pre-surgical evaluation for treating patients with epilepsy.},
  archive      = {J_NN},
  author       = {S. Raghu and Natarajan Sriraam and Yasin Temel and Shyam Vasudeva Rao and Pieter L. Kubben},
  doi          = {10.1016/j.neunet.2020.01.017},
  journal      = {Neural Networks},
  pages        = {202-212},
  shortjournal = {Neural Netw.},
  title        = {EEG based multi-class seizure type classification using convolutional neural network and transfer learning},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). FOM: Fourth-order moment based causal direction
identification on the heteroscedastic data. <em>NN</em>, <em>124</em>,
193–201. (<a
href="https://doi.org/10.1016/j.neunet.2020.01.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identification of the causal direction is a fundamental problem in many scientific research areas. The independence between the noise and the cause variable is a widely used assumption to identify the causal direction. However, such an independence assumption is usually violated due to heteroscedasticity of the real-world data. In this paper, we propose a new criterion for the causal direction identification which is robust to the heteroscedasticity of the data. In detail, the fourth-order moment of noise is proposed to measure the asymmetry between the cause and effect. A heteroscedastic Gaussian process regression-based estimation of the fourth-order moment is proposed accordingly. Under some commonly used assumptions of the causal mechanism , we theoretically show that the noise’s fourth-order moment of the causal direction is smaller than that of the anti-causal direction. Experimental results on both simulated and real-world data illustrate the efficiency of the proposed approach.},
  archive      = {J_NN},
  author       = {Ruichu Cai and Jincheng Ye and Jie Qiao and Huiyuan Fu and Zhifeng Hao},
  doi          = {10.1016/j.neunet.2020.01.006},
  journal      = {Neural Networks},
  pages        = {193-201},
  shortjournal = {Neural Netw.},
  title        = {FOM: Fourth-order moment based causal direction identification on the heteroscedastic data},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A neurodynamic approach to nonsmooth constrained
pseudoconvex optimization problem. <em>NN</em>, <em>124</em>, 180–192.
(<a href="https://doi.org/10.1016/j.neunet.2019.12.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new neurodynamic approach for solving the constrained pseudoconvex optimization problem based on more general assumptions. The proposed neural network is equipped with a hard comparator function and a piecewise linear function, which make the state solution not only stay in the feasible region, but also converge to an optimal solution of the constrained pseudoconvex optimization problem. Compared with other related existing conclusions, the neurodynamic approach here enjoys global convergence and lower dimension of the solution space. Moreover, the neurodynamic approach does not depend on some additional assumptions, such as the feasible region is bounded, the objective function is lower bounded over the feasible region or the objective function is coercive. Finally, both numerical illustrations and simulation results in support vector regression problem show the well performance and the viability of the proposed neurodynamic approach.},
  archive      = {J_NN},
  author       = {Chen Xu and Yiyuan Chai and Sitian Qin and Zhenkun Wang and Jiqiang Feng},
  doi          = {10.1016/j.neunet.2019.12.015},
  journal      = {Neural Networks},
  pages        = {180-192},
  shortjournal = {Neural Netw.},
  title        = {A neurodynamic approach to nonsmooth constrained pseudoconvex optimization problem},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). L2–l∞ state estimation for delayed artificial neural
networks under high-rate communication channels with round-robin
protocol. <em>NN</em>, <em>124</em>, 170–179. (<a
href="https://doi.org/10.1016/j.neunet.2020.01.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the l 2 l2 – l ∞ l∞ state estimation problem is addressed for a class of delayed artificial neural networks under high-rate communication channels with Round-Robin (RR) protocol. To estimate the state of the artificial neural networks , numerous sensors are deployed to measure the artificial neural networks. The sensors communicate with the remote state estimator through a shared high-rate communication channel. In the high-rate communication channel, the RR protocol is utilized to schedule the transmission sequence of the numerous sensors. The aim of this paper is to design an estimator such that, under the high-rate communication channel and the RR protocol, the exponential stability of the estimation error dynamics as well as the l 2 l2 – l ∞ l∞ performance constraint are ensured. First, sufficient conditions are given which guarantee the existence of the desired l 2 l2 – l ∞ l∞ state estimator . Then, the estimator gains are obtained by solving two sets of matrix inequalities. Finally, numerical examples are provided to verify the effectiveness of the developed l 2 l2 – l ∞ l∞ state estimation scheme.},
  archive      = {J_NN},
  author       = {Yuxuan Shen and Zidong Wang and Bo Shen and Fuad E. Alsaadi and Abdullah M. Dobaie},
  doi          = {10.1016/j.neunet.2020.01.013},
  journal      = {Neural Networks},
  pages        = {170-179},
  shortjournal = {Neural Netw.},
  title        = {L2–l∞ state estimation for delayed artificial neural networks under high-rate communication channels with round-robin protocol},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Performance boost of time-delay reservoir computing by
non-resonant clock cycle. <em>NN</em>, <em>124</em>, 158–169. (<a
href="https://doi.org/10.1016/j.neunet.2020.01.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The time-delay-based reservoir computing setup has seen tremendous success in both experiment and simulation. It allows for the construction of large neuromorphic computing systems with only few components. However, until now the interplay of the different timescales has not been investigated thoroughly. In this manuscript, we investigate the effects of a mismatch between the time-delay and the clock cycle for a general model. Typically, these two time scales are considered to be equal. Here we show that the case of equal or resonant time-delay and clock cycle could be actively detrimental and leads to an increase of the approximation error of the reservoir. In particular, we can show that non-resonant ratios of these time scales have maximal memory capacities. We achieve this by translating the periodically driven delay-dynamical system into an equivalent network. Networks that originate from a system with resonant delay-times and clock cycles fail to utilize all of their degrees of freedom, which causes the degradation of their performance.},
  archive      = {J_NN},
  author       = {Florian Stelzer and André Röhm and Kathy Lüdge and Serhiy Yanchuk},
  doi          = {10.1016/j.neunet.2020.01.010},
  journal      = {Neural Networks},
  pages        = {158-169},
  shortjournal = {Neural Netw.},
  title        = {Performance boost of time-delay reservoir computing by non-resonant clock cycle},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive tracking synchronization for coupled
reaction–diffusion neural networks with parameter mismatches.
<em>NN</em>, <em>124</em>, 146–157. (<a
href="https://doi.org/10.1016/j.neunet.2019.12.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, tracking synchronization for coupled reaction–diffusion neural networks with parameter mismatches is investigated. For such a networked control system , only local neighbor information is used to compensate the mismatch characteristic termed as parameter mismatch, uncertainty or external disturbance . Different from the general boundedness hypothesis, the parameter mismatches are permitted to be unbounded. For the known parameter mismatches, parameter-dependent controller and parameter-independent adaptive controller are respectively designed. While for fully unknown network parameters and parameter mismatches, a distributed adaptive controller is proposed. By means of partial differential equation theories and differential inequality techniques, the tracking synchronization errors driven by these nonlinear controllers are proved to be uniformly ultimately bounded and exponentially convergent to some adjustable bounded domains . Finally, three numerical examples are given to test the effectiveness of the proposed controllers.},
  archive      = {J_NN},
  author       = {Hao Zhang and Zhixia Ding and Zhigang Zeng},
  doi          = {10.1016/j.neunet.2019.12.025},
  journal      = {Neural Networks},
  pages        = {146-157},
  shortjournal = {Neural Netw.},
  title        = {Adaptive tracking synchronization for coupled reaction–diffusion neural networks with parameter mismatches},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A causal discovery algorithm based on the prior selection of
leaf nodes. <em>NN</em>, <em>124</em>, 130–145. (<a
href="https://doi.org/10.1016/j.neunet.2019.12.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, Linear Non-Gaussian Acyclic Model (LiNGAM) has been widely used for the discovery of causal network. However, solutions based on LiNGAM usually yield high computational complexity as well as unsatisfied accuracy when the data is high-dimensional or the sample size is too small. Such complexity or accuracy problems here are often originated from their prior selection of root nodes when estimating a causal ordering. Thus, a causal discovery algorithm termed as GPL algorithm (the LiNGAM algorithm of Giving Priority to Leaf-nodes) under a mild assumption is proposed in this paper. It assigns priority to leaf nodes other than root nodes . Since leaf nodes do not affect others in a structure, we can directly estimate a causal ordering in a bottom-up way without performing additional operations like data updating process. Corresponding proofs for both feasibility and superiority are offered based on the properties of leaf nodes. Aside from theoretical analyses, practical experiments are conducted on both synthetic and real-world data, which confirm that GPL algorithm outperforms the other two state-of-the-art algorithms in computational complexity and accuracy, especially when dealing with high-dimensional data (up to 200) or small sample size (down to 100 for the dimension of 70).},
  archive      = {J_NN},
  author       = {Yan Zeng and Zhifeng Hao and Ruichu Cai and Feng Xie and Liang Ou and Ruihui Huang},
  doi          = {10.1016/j.neunet.2019.12.020},
  journal      = {Neural Networks},
  pages        = {130-145},
  shortjournal = {Neural Netw.},
  title        = {A causal discovery algorithm based on the prior selection of leaf nodes},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Attention-guided CNN for image denoising. <em>NN</em>,
<em>124</em>, 117–129. (<a
href="https://doi.org/10.1016/j.neunet.2019.12.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks (CNNs) have attracted considerable interest in low-level computer vision . Researches are usually devoted to improving the performance via very deep CNNs. However, as the depth increases, influences of the shallow layers on deep layers are weakened. Inspired by the fact, we propose an attention-guided denoising convolutional neural network (ADNet), mainly including a sparse block (SB), a feature enhancement block (FEB), an attention block (AB) and a reconstruction block (RB) for image denoising . Specifically, the SB makes a tradeoff between performance and efficiency by using dilated and common convolutions to remove the noise. The FEB integrates global and local features information via a long path to enhance the expressive ability of the denoising model. The AB is used to finely extract the noise information hidden in the complex background, which is very effective for complex noisy images , especially real noisy images and bind denoising. Also, the FEB is integrated with the AB to improve the efficiency and reduce the complexity for training a denoising model. Finally, a RB aims to construct the clean image through the obtained noise mapping and the given noisy image. Additionally, comprehensive experiments show that the proposed ADNet performs very well in three tasks (i.e. synthetic and real noisy images, and blind denoising) in terms of both quantitative and qualitative evaluations. The code of ADNet is accessible at https://github.com/hellloxiaotian/ADNet .},
  archive      = {J_NN},
  author       = {Chunwei Tian and Yong Xu and Zuoyong Li and Wangmeng Zuo and Lunke Fei and Hong Liu},
  doi          = {10.1016/j.neunet.2019.12.024},
  journal      = {Neural Networks},
  pages        = {117-129},
  shortjournal = {Neural Netw.},
  title        = {Attention-guided CNN for image denoising},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Medi-care AI: Predicting medications from billing codes via
robust recurrent neural networks. <em>NN</em>, <em>124</em>, 109–116.
(<a href="https://doi.org/10.1016/j.neunet.2020.01.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present an effective deep prediction framework based on robust recurrent neural networks (RNNs) to predict the likely therapeutic classes of medications a patient is taking, given a sequence of diagnostic billing codes in their record. Accurately capturing the list of medications currently taken by a given patient is extremely challenging due to undefined errors and omissions. We present a general robust framework that explicitly models the possible contamination through overtime decay mechanism on the input billing codes and noise injection into the recurrent hidden states, respectively. By doing this, billing codes are reformulated into its temporal patterns with decay rates on each medical variable, and the hidden states of RNNs are regularized by random noises which serve as dropout to improved RNNs robustness towards data variability in terms of missing values and multiple errors. The proposed method is extensively evaluated on real health care data to demonstrate its effectiveness in suggesting medication orders from contaminated values.},
  archive      = {J_NN},
  author       = {Deyin Liu and Yuanbo Lin Wu and Xue Li and Lin Qi},
  doi          = {10.1016/j.neunet.2020.01.001},
  journal      = {Neural Networks},
  pages        = {109-116},
  shortjournal = {Neural Netw.},
  title        = {Medi-care AI: Predicting medications from billing codes via robust recurrent neural networks},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Differential-game for resource aware approximate optimal
control of large-scale nonlinear systems with multiple players.
<em>NN</em>, <em>124</em>, 95–108. (<a
href="https://doi.org/10.1016/j.neunet.2019.12.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel differential-game based neural network (NN) control architecture to solve an optimal control problem for a class of large-scale nonlinear systems involving N N -players. We focus on optimizing the usage of the computational resources along with the system performance simultaneously. In particular, the N N -players’ control policies are desired to be designed such that they cooperatively optimize the large-scale system performance, and the sampling intervals for each player are desired to reduce the frequency of feedback execution. To develop a unified design framework that achieves both these objectives, we propose an optimal control problem by integrating both the design requirements, which leads to a multi-player differential-game. A solution to this problem is numerically obtained by solving the associated Hamilton-Jacobi (HJ) equation using event-driven approximate dynamic programming (E-ADP) and artificial NNs online and forward-in-time. We employ the critic neural networks to approximate the solution to the HJ equation, i.e., the optimal value function, with aperiodically available feedback information. Using the NN approximated value function, we design the control policies and the sampling schemes. Finally, the event-driven N N -player system is remodeled as a hybrid dynamical system with impulsive weight update rules for analyzing its stability and convergence properties . The closed-loop practical stability of the system and Zeno free behavior of the sampling scheme are demonstrated using the Lyapunov method . Simulation results using a numerical example are also included to substantiate the analytical results.},
  archive      = {J_NN},
  author       = {Avimanyu Sahoo and Vignesh Narayanan},
  doi          = {10.1016/j.neunet.2019.12.031},
  journal      = {Neural Networks},
  pages        = {95-108},
  shortjournal = {Neural Netw.},
  title        = {Differential-game for resource aware approximate optimal control of large-scale nonlinear systems with multiple players},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-context aware user–item embedding for recommendation.
<em>NN</em>, <em>124</em>, 86–94. (<a
href="https://doi.org/10.1016/j.neunet.2020.01.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real recommender systems usually contain various auxiliary information. Some of the most recent works make meaningful exploration of incorporating auxiliary information into the representation model for competitive recommendation. However, learning user and item representations still faces two challenges: (1) existing works do not well address the problem of integrating multi-type auxiliary information; (2) learning representations for inactive users is still challenging due to the high sparsity of explicit user–item associations. In order to tackle these problems, in this paper, the attributed heterogeneous network and bipartite interaction network are employed to incorporate various auxiliary information and user–item associations. A joint objective function and an efficient algorithm are devised for the representation learning. Experimental results show that the proposed algorithm has significant advantages over the state-of-the-art baselines. What is remarkable is that our proposed method is demonstrated to be especially useful for dealing with low-active users in the system.},
  archive      = {J_NN},
  author       = {Biao Wu and Wen Wen and Zhifeng Hao and Ruichu Cai},
  doi          = {10.1016/j.neunet.2020.01.008},
  journal      = {Neural Networks},
  pages        = {86-94},
  shortjournal = {Neural Netw.},
  title        = {Multi-context aware user–item embedding for recommendation},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A 3D deep supervised densely network for small organs of
human temporal bone segmentation in CT images. <em>NN</em>,
<em>124</em>, 75–85. (<a
href="https://doi.org/10.1016/j.neunet.2020.01.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computed Tomography (CT) has become an important way for examining the critical anatomical organs of the human temporal bone in the diagnosis and treatment of ear diseases. Segmentation of the critical anatomical organs is an important fundamental step for the computer assistant analysis of human temporal bone CT images. However, it is challenging to segment sophisticated and small organs. To deal with this issue, a novel 3D Deep Supervised Densely Network (3D-DSD Net) is proposed in this paper. The network adopts a dense connection design and a 3D multi-pooling feature fusion strategy in the encoding stage of the 3D-Unet, and a 3D deep supervised mechanism is employed in the decoding stage. The experimental results show that our method achieved competitive performance in the CT data segmentation task of the small organs in the temporal bone.},
  archive      = {J_NN},
  author       = {Xiaoguang Li and Zhaopeng Gong and Hongxia Yin and Hui Zhang and Zhenchang Wang and Li Zhuo},
  doi          = {10.1016/j.neunet.2020.01.005},
  journal      = {Neural Networks},
  pages        = {75-85},
  shortjournal = {Neural Netw.},
  title        = {A 3D deep supervised densely network for small organs of human temporal bone segmentation in CT images},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust adaptation regularization based on within-class
scatter for domain adaptation. <em>NN</em>, <em>124</em>, 60–74. (<a
href="https://doi.org/10.1016/j.neunet.2020.01.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many practical applications, the assumption that the distributions of the data employed for training and test are identical is rarely valid, which would result in a rapid decline in performance. To address this problem, the domain adaptation strategy has been developed in recent years. In this paper, we propose a novel unsupervised domain adaptation method, referred to as Robust Adaptation Regularization based on Within-Class Scatter (WCS-RAR), to simultaneously optimize the regularized loss, the within-class scatter, the joint distribution between domains, and the manifold consistency. On the one hand, to make the model robust against outliers, we adopt an l 2 , 1 l2,1 -norm based loss function in virtue of its row sparsity , instead of the widely-used l 2 l2 -norm based squared loss or hinge loss function to determine the residual. On the other hand, to well preserve the structure knowledge of the source data within the same class and strengthen the discriminant ability of the classifier, we incorporate the minimum within-class scatter into the process of domain adaptation . Lastly, to efficiently solve the resulting optimization problem , we extend the form of the Representer Theorem through the kernel trick, and thus derive an elegant solution for the proposed model. The extensive comparison experiments with the state-of-the-art methods on multiple benchmark data sets demonstrate the superiority of the proposed method.},
  archive      = {J_NN},
  author       = {Liran Yang and Ping Zhong},
  doi          = {10.1016/j.neunet.2020.01.009},
  journal      = {Neural Networks},
  pages        = {60-74},
  shortjournal = {Neural Netw.},
  title        = {Robust adaptation regularization based on within-class scatter for domain adaptation},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exponential and adaptive synchronization of inertial
complex-valued neural networks: A non-reduced order and non-separation
approach. <em>NN</em>, <em>124</em>, 50–59. (<a
href="https://doi.org/10.1016/j.neunet.2020.01.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper mainly deals with the problem of exponential and adaptive synchronization for a type of inertial complex-valued neural networks via directly constructing Lyapunov functionals without utilizing standard reduced-order transformation for inertial neural systems and common separation approach for complex-valued systems. At first, a complex-valued feedback control scheme is designed and a nontrivial Lyapunov functional , composed of the complex-valued state variables and their derivatives, is proposed to analyze exponential synchronization. Some criteria involving multi-parameters are derived and a feasible method is provided to determine these parameters so as to clearly show how to choose control gains in practice. In addition, an adaptive control strategy in complex domain is developed to adjust control gains and asymptotic synchronization is ensured by applying the method of undeterminated coefficients in the construction of Lyapunov functional and utilizing Barbalat Lemma. Lastly, a numerical example along with simulation results is provided to support the theoretical work.},
  archive      = {J_NN},
  author       = {Juan Yu and Cheng Hu and Haijun Jiang and Leimin Wang},
  doi          = {10.1016/j.neunet.2020.01.002},
  journal      = {Neural Networks},
  pages        = {50-59},
  shortjournal = {Neural Netw.},
  title        = {Exponential and adaptive synchronization of inertial complex-valued neural networks: A non-reduced order and non-separation approach},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bipartite synchronization for inertia memristor-based neural
networks on coopetition networks. <em>NN</em>, <em>124</em>, 39–49. (<a
href="https://doi.org/10.1016/j.neunet.2019.11.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the bipartite synchronization problem of coupled inertia memristor-based neural networks with both cooperative and competitive interactions. Generally, coopetition interaction networks are modeled by a signed graph, and the corresponding Laplacian matrix is different from the nonnegative graph. The coopetition networks with structural balance can reach a final state with identical magnitude but opposite sign, which is called bipartite synchronization. Additionally, an inertia system is a second-order differential system. In this paper, firstly, by using suitable variable substitutions, the inertia memristor-based neural networks (IMNNs) are transformed into the first-order differential equations. Secondly, by designing suitable discontinuous controllers, the bipartite synchronization criteria for IMNNs with or without a leader node on coopetition networks are obtained. Finally, two illustrative examples with simulations are provided to validate the effectiveness of the proposed discontinuous control strategies for achieving bipartite synchronization.},
  archive      = {J_NN},
  author       = {Ning Li and Wei Xing Zheng},
  doi          = {10.1016/j.neunet.2019.11.010},
  journal      = {Neural Networks},
  pages        = {39-49},
  shortjournal = {Neural Netw.},
  title        = {Bipartite synchronization for inertia memristor-based neural networks on coopetition networks},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive neural tree exploiting expert nodes to classify
high-dimensional data. <em>NN</em>, <em>124</em>, 20–38. (<a
href="https://doi.org/10.1016/j.neunet.2019.12.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification of high dimensional data suffers from curse of dimensionality and over-fitting. Neural tree is a powerful method which combines a local feature selection and recursive partitioning to solve these problems, but it leads to high depth trees in classifying high dimensional data . On the other hand, if less depth trees are used, the classification accuracy decreases or over-fitting increases. This paper introduces a novel N eural T ree exploiting E xpert N odes (NTEN) to classify high-dimensional data. It is based on a decision tree structure, whose internal nodes are expert nodes performing multi-dimensional splitting. Any expert node has three decision-making abilities. Firstly, they can select the most eligible neural network with respect to the data complexity. Secondly, they evaluate the over-fitting. Thirdly, they can cluster the features to jointly minimize redundancy and overlapping. To this aim, metaheuristic optimization algorithms including GA , NSGA-II, PSO and ACO are applied. Based on these concepts, any expert node splits a class when the over-fitting is low, and clusters the features when the over-fitting is high. Some theoretical results on NTEN are derived, and experiments on 35 standard data show that NTEN reaches good classification results , reduces tree depth without over-fitting and degrading accuracy.},
  archive      = {J_NN},
  author       = {Shadi Abpeikar and Mehdi Ghatee and Gian Luca Foresti and Christian Micheloni},
  doi          = {10.1016/j.neunet.2019.12.029},
  journal      = {Neural Networks},
  pages        = {20-38},
  shortjournal = {Neural Netw.},
  title        = {Adaptive neural tree exploiting expert nodes to classify high-dimensional data},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cluster stochastic synchronization of complex dynamical
networks via fixed-time control scheme. <em>NN</em>, <em>124</em>,
12–19. (<a href="https://doi.org/10.1016/j.neunet.2019.12.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By means of fixed-time (FDT) control technique, cluster stochastic synchronization of complex networks (CNs) is investigated. Quantized controller is designed to realize the synchronization of CNs within a settling time. FDT synchronization criteria are established with the help of Lyapunov functional and comparison system methods. It should be noted that the convergence of synchronization is further improved by comparing with existing FDT synchronization results. Numerical simulations are given to illustrate our results.},
  archive      = {J_NN},
  author       = {Wanli Zhang and Chuandong Li and Hongfei Li and Xinsong Yang},
  doi          = {10.1016/j.neunet.2019.12.019},
  journal      = {Neural Networks},
  pages        = {12-19},
  shortjournal = {Neural Netw.},
  title        = {Cluster stochastic synchronization of complex dynamical networks via fixed-time control scheme},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Abstractive summarization of long texts by representing
multiple compositionalities with temporal hierarchical pointer generator
network. <em>NN</em>, <em>124</em>, 1–11. (<a
href="https://doi.org/10.1016/j.neunet.2019.12.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to tackle the problem of abstractive summarization of long multi-sentence texts, it is critical to construct an efficient model, which can learn and represent multiple compositionalities better. In this paper, we introduce a temporal hierarchical pointer generator network that can represent multiple compositionalities in order to handle longer sequences of texts with a deep structure. We demonstrate how a multilayer gated recurrent neural network organizes itself with the help of an adaptive timescale in order to represent the compositions. The temporal hierarchical network is implemented with a multiple timescale architecture where the timescale of each layer is also learned during the training process through error backpropagation through time. We evaluate our proposed model using an Introduction-Abstract summarization dataset from scientific articles and the CNN/Daily Mail summarization benchmark dataset. The results illustrate that, we successfully implement a summary generation system for long texts by using the multiple timescale with adaptation concept. We also show that we have improved the summary generation system with our proposed model on the benchmark dataset.},
  archive      = {J_NN},
  author       = {Dennis Singh Moirangthem and Minho Lee},
  doi          = {10.1016/j.neunet.2019.12.022},
  journal      = {Neural Networks},
  pages        = {1-11},
  shortjournal = {Neural Netw.},
  title        = {Abstractive summarization of long texts by representing multiple compositionalities with temporal hierarchical pointer generator network},
  volume       = {124},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020j). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>123</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(20)30027-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(20)30027-7},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020j). Current events. <em>NN</em>, <em>123</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(20)30026-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(20)30026-5},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {Current events},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spacial sampled-data control for h∞ output synchronization
of directed coupled reaction–diffusion neural networks with mixed
delays. <em>NN</em>, <em>123</em>, 429–440. (<a
href="https://doi.org/10.1016/j.neunet.2019.12.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work investigates the H ∞ H∞ output synchronization (HOS) of the directed coupled reaction–diffusion (R–D) neural networks (NNs) with mixed delays. Firstly, a model of the directed state coupled R–D NNs is introduced, which not only contains some discrete and distributed time delays , but also obeys a mixed Dirichlet–Neumann boundary condition. Secondly, a spacial sampled-data controller is proposed to achieve the HOS of the considered networks. This type of controller can reduce the update rate in the process of control by measuring the state of networks at some fixed sampling points in the space region. Moreover, some criteria for the HOS are established by designing an appropriate Lyapunov functional , and some quantitative relations between diffusion coefficients , mixed delays, coupling strength and control parameters are given accurately by these criteria. Thirdly, the case of directed spatial diffusion coupled networks is also studied and, the following finding is obtained: the spatial diffusion coupling can suppress the HOS while the state coupling can promote it. Finally, one example is simulated as the verification of the theoretical results.},
  archive      = {J_NN},
  author       = {Binglong Lu and Haijun Jiang and Cheng Hu and Abdujelil Abdurahman},
  doi          = {10.1016/j.neunet.2019.12.026},
  journal      = {Neural Networks},
  pages        = {429-440},
  shortjournal = {Neural Netw.},
  title        = {Spacial sampled-data control for h∞ output synchronization of directed coupled reaction–diffusion neural networks with mixed delays},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian deep matrix factorization network for multiple
images denoising. <em>NN</em>, <em>123</em>, 420–428. (<a
href="https://doi.org/10.1016/j.neunet.2019.12.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper aims at proposing a robust and fast low rank matrix factorization model for multiple images denoising . To this end, a novel model, Bayesian deep matrix factorization network (BDMF), is presented, where a deep neural network (DNN) is designed to model the low rank components and the model is optimized via stochastic gradient variational Bayes. By the virtue of deep learning and Bayesian modeling, BDMF makes significant improvement on synthetic experiments and real-world tasks (including shadow removal and hyperspectral image denoising), compared with existing state-of-the-art models.},
  archive      = {J_NN},
  author       = {Shuang Xu and Chunxia Zhang and Jiangshe Zhang},
  doi          = {10.1016/j.neunet.2019.12.023},
  journal      = {Neural Networks},
  pages        = {420-428},
  shortjournal = {Neural Netw.},
  title        = {Bayesian deep matrix factorization network for multiple images denoising},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new fixed-time stability theorem and its application to
the fixed-time synchronization of neural networks. <em>NN</em>,
<em>123</em>, 412–419. (<a
href="https://doi.org/10.1016/j.neunet.2019.12.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we derive a new fixed-time stability theorem based on definite integral, variable substitution and some inequality techniques. The fixed-time stability criterion and the upper bound estimate formula for the settling time are different from those in the existing fixed-time stability theorems. Based on the new fixed-time stability theorem, the fixed-time synchronization of neural networks is investigated by designing feedback controller , and sufficient conditions are derived to guarantee the fixed-time synchronization of neural networks . To show the usability and superiority of the obtained theoretical results, we propose a secure communication scheme based on the fixed-time synchronization of neural networks. Numerical simulations illustrate that the new upper bound estimate formula for the settling time is much tighter than those in the existing fixed-time stability theorems. Moreover, the plaintext signals can be recovered according to the new fixed-time stability theorem, while the plaintext signals cannot be recovered according to the existing fixed-time stability theorems.},
  archive      = {J_NN},
  author       = {Chuan Chen and Lixiang Li and Haipeng Peng and Yixian Yang and Ling Mi and Hui Zhao},
  doi          = {10.1016/j.neunet.2019.12.028},
  journal      = {Neural Networks},
  pages        = {412-419},
  shortjournal = {Neural Netw.},
  title        = {A new fixed-time stability theorem and its application to the fixed-time synchronization of neural networks},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-task learning for the prediction of wind power ramp
events with deep neural networks. <em>NN</em>, <em>123</em>, 401–411.
(<a href="https://doi.org/10.1016/j.neunet.2019.12.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Machine Learning , the most common way to address a given problem is to optimize an error measure by training a single model to solve the desired task. However, sometimes it is possible to exploit latent information from other related tasks to improve the performance of the main one, resulting in a learning paradigm known as Multi-Task Learning (MTL). In this context, the high computational capacity of deep neural networks (DNN) can be combined with the improved generalization performance of MTL, by designing independent output layers for every task and including a shared representation for them. In this paper we exploit this theoretical framework on a problem related to Wind Power Ramps Events (WPREs) prediction in wind farms . Wind energy is one of the fastest growing industries in the world, with potential global spreading and deep penetration in developed and developing countries. One of the main issues with the majority of renewable energy resources is their intrinsic intermittency, which makes it difficult to increase the penetration of these technologies into the energetic mix. In this case, we focus on the specific problem of WPREs prediction, which deeply affect the wind speed and power prediction, and they are also related to different turbines damages. Specifically, we exploit the fact that WPREs are spatially-related events, in such a way that predicting the occurrence of WPREs in different wind farms can be taken as related tasks, even when the wind farms are far away from each other. We propose a DNN-MTL architecture, receiving inputs from all the wind farms at the same time to predict WPREs simultaneously in each of the farms locations. The architecture includes some shared layers to learn a common representation for the information from all the wind farms, and it also includes some specification layers , which refine the representation to match the specific characteristics of each location. Finally we modified the Adam optimization algorithm for dealing with imbalanced data , adding costs which are updated dynamically depending on the worst classified class. We compare the proposal against a baseline approach based on building three different independent models (one for each wind farm considered), and against a state-of-the-art reservoir computing approach. The DNN-MTL proposal achieves very good performance in WPREs prediction, obtaining a good balance for all the classes included in the problem (negative ramp, no ramp and positive ramp).},
  archive      = {J_NN},
  author       = {M. Dorado-Moreno and N. Navarin and P.A. Gutiérrez and L. Prieto and A. Sperduti and S. Salcedo-Sanz and C. Hervás-Martínez},
  doi          = {10.1016/j.neunet.2019.12.017},
  journal      = {Neural Networks},
  pages        = {401-411},
  shortjournal = {Neural Netw.},
  title        = {Multi-task learning for the prediction of wind power ramp events with deep neural networks},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Global collaboration through local interaction in
competitive learning. <em>NN</em>, <em>123</em>, 393–400. (<a
href="https://doi.org/10.1016/j.neunet.2019.12.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature maps, that preserve the global topology of arbitrary datasets, can be formed by self-organizing competing agents. So far, it has been presumed that global interaction of agents is necessary for this process. We establish that this is not the case, and that global topology can be uncovered through strictly local interactions. Enforcing uniformity of map quality across all agents results in an algorithm that is able to consistently uncover the global topology of diversely challenging datasets. The applicability and scalability of this approach is further tested on a large point cloud dataset, revealing a linear relation between map training time and size. The presented work not only reduces algorithmic complexity but also constitutes first step towards a distributed self organizing map .},
  archive      = {J_NN},
  author       = {Abbas Siddiqui and Dionysios Georgiadis},
  doi          = {10.1016/j.neunet.2019.12.018},
  journal      = {Neural Networks},
  pages        = {393-400},
  shortjournal = {Neural Netw.},
  title        = {Global collaboration through local interaction in competitive learning},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The role of coupling connections in a model of the
cortico-basal ganglia-thalamocortical neural loop for the generation of
beta oscillations. <em>NN</em>, <em>123</em>, 381–392. (<a
href="https://doi.org/10.1016/j.neunet.2019.12.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Excessive neural synchronization in the cortico-basal ganglia-thalamocortical circuits in the beta ( β β ) frequency range (12–35 Hz) is closely associated with dopamine depletion in Parkinson’s disease (PD) and correlated with movement impairments, but the neural basis remains unclear. In this work, we establish a double-oscillator neural mass model for the cortico-basal ganglia-thalamocortical closed-loop system and explore the impacts of dopamine depletion induced changes in coupling connections within or between the two oscillators on neural activities within the loop. Spectral analysis of the neural mass activities revealed that the power and frequency of their principal components are greatly dependent on the coupling strengths between nuclei. We found that the increased intra-coupling in the basal ganglia-thalamic (BG-Th) oscillator contributes to increased oscillations in the lower β β frequency band (12–25 Hz), while increased intra-coupling in the cortical oscillator mainly contributes to increased oscillations in the upper β β frequency band (26–35 Hz). Interestingly, pathological upper β β oscillations in the cortical oscillator may be another origin of the lower β β oscillations in the BG-Th oscillator, in addition to increased intra-coupling strength within the BG-Th network. Lower β β oscillations in the BG-Th oscillator can also change the dominant oscillation frequency of a cortical nucleus from the upper to the lower β β band. Thus, this work may pave the way towards revealing a possible neural basis underlying the Parkinsonian state.},
  archive      = {J_NN},
  author       = {Chen Liu and Changsong Zhou and Jiang Wang and Chris Fietkiewicz and Kenneth A. Loparo},
  doi          = {10.1016/j.neunet.2019.12.021},
  journal      = {Neural Networks},
  pages        = {381-392},
  shortjournal = {Neural Netw.},
  title        = {The role of coupling connections in a model of the cortico-basal ganglia-thalamocortical neural loop for the generation of beta oscillations},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Synchronization of hindmarsh rose neurons. <em>NN</em>,
<em>123</em>, 372–380. (<a
href="https://doi.org/10.1016/j.neunet.2019.11.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling and implementation of biological neurons are key to the fundamental understanding of neural network architectures in the brain and its cognitive behavior . Synchronization of neuronal models play a significant role in neural signal processing as it is very difficult to identify the actual interaction between neurons in living brain. Therefore, the synchronization study of these neuronal architectures has received extensive attention from researchers. Higher biological accuracy of these neuronal units demands more computational overhead and requires more hardware resources for implementation. This paper presents a two coupled hardware implementation of Hindmarsh Rose neuron model which is mathematically simpler model and yet mimics several behaviors of a real biological neuron. These neurons are synchronized using an exponential function. The coupled system shows several behaviors depending upon the parameters of HR model and coupling function. An approximation of coupling function is also provided to reduce the hardware cost. Both simulations and a low cost hardware implementations of exponential synaptic coupling function and its approximation are carried out for comparison. Hardware implementation on field programmable gate array (FPGA) of approximated coupling function shows that the coupled network produces different dynamical behaviors with acceptable error. Hardware implementation shows that the approximated coupling function has significantly lower implementation cost. A spiking neural network based on HR neuron is also shown as a practical application of this coupled HR neural networks. The spiking network successfully encodes and decodes a time varying input.},
  archive      = {J_NN},
  author       = {Malik S.A. and Mir A.H.},
  doi          = {10.1016/j.neunet.2019.11.024},
  journal      = {Neural Networks},
  pages        = {372-380},
  shortjournal = {Neural Netw.},
  title        = {Synchronization of hindmarsh rose neurons},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Global synchronization of coupled delayed memristive
reaction–diffusion neural networks. <em>NN</em>, <em>123</em>, 362–371.
(<a href="https://doi.org/10.1016/j.neunet.2019.12.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the global exponential synchronization of multiple memristive reaction–diffusion neural networks (MRDNNs) with time delay . Due to introducing the influences of space as well as time on state variables and replacing resistors with memristors in circuit realization , the state-dependent partial differential mathematical model of MRDNN is more general and realistic than traditional neural network model. Based on Lyapunov functional theory, Divergence theorem and inequality techniques, global exponential synchronization criteria of coupled delayed MRDNNs are derived via directed and undirected nonlinear coupling. Finally, three numerical simulation examples are presented to verify the feasibility of our main results.},
  archive      = {J_NN},
  author       = {Shiqin Wang and Zhenyuan Guo and Shiping Wen and Tingwen Huang},
  doi          = {10.1016/j.neunet.2019.12.016},
  journal      = {Neural Networks},
  pages        = {362-371},
  shortjournal = {Neural Netw.},
  title        = {Global synchronization of coupled delayed memristive reaction–diffusion neural networks},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the minimax optimality and superiority of deep neural
network learning over sparse parameter spaces. <em>NN</em>,
<em>123</em>, 343–361. (<a
href="https://doi.org/10.1016/j.neunet.2019.12.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning has been applied to various tasks in the field of machine learning and has shown superiority to other common procedures such as kernel methods. To provide a better theoretical understanding of the reasons for its success, we discuss the performance of deep learning and other methods on a nonparametric regression problem with a Gaussian noise. Whereas existing theoretical studies of deep learning have been based mainly on mathematical theories of well-known function classes such as Hölder and Besov classes, we focus on function classes with discontinuity and sparsity, which are those naturally assumed in practice. To highlight the effectiveness of deep learning, we compare deep learning with a class of linear estimators representative of a class of shallow estimators. It is shown that the minimax risk of a linear estimator on the convex hull of a target function class does not differ from that of the original target function class. This results in the suboptimality of linear methods over a simple but non-convex function class, on which deep learning can attain nearly the minimax-optimal rate. In addition to this extreme case, we consider function classes with sparse wavelet coefficients. On these function classes, deep learning also attains the minimax rate up to log factors of the sample size, and linear methods are still suboptimal if the assumed sparsity is strong. We also point out that the parameter sharing of deep neural networks can remarkably reduce the complexity of the model in our setting.},
  archive      = {J_NN},
  author       = {Satoshi Hayakawa and Taiji Suzuki},
  doi          = {10.1016/j.neunet.2019.12.014},
  journal      = {Neural Networks},
  pages        = {343-361},
  shortjournal = {Neural Netw.},
  title        = {On the minimax optimality and superiority of deep neural network learning over sparse parameter spaces},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ELM embedded discriminative dictionary learning for image
classification. <em>NN</em>, <em>123</em>, 331–342. (<a
href="https://doi.org/10.1016/j.neunet.2019.11.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dictionary learning is a widely adopted approach for image classification . Existing methods focus either on finding a dictionary that produces discriminative sparse representation , or on enforcing priors that best describe the dataset distribution. In many cases, the dataset size is often small with large intra-class variability and nondiscriminative feature space. In this work we propose a simple and effective framework called ELM-DDL to address these issues. Specifically, we represent input features with Extreme Learning Machine (ELM) with orthogonal output projection, which enables diverse representation on nonlinear hidden space and task specific feature learning on output space. The embeddings are further regularized via a maximum margin criterion (MMC) to maximize the inter-class variance and minimize intra-class variance. For dictionary learning, we design a novel weighted class specific ℓ 1 , 2 ℓ1,2 norm to regularize the sparse coding vectors , which promotes uniformity of the sparse patterns of samples belonging to the same class and suppresses support overlaps of different classes. We show that such regularization is robust, discriminative and easy to optimize. The proposed method is combined with a sparse representation classifier (SRC) to evaluate on benchmark datasets. Results show that our approach achieves state-of-the-art performance compared to other dictionary learning methods.},
  archive      = {J_NN},
  author       = {Yijie Zeng and Yue Li and Jichao Chen and Xiaofan Jia and Guang-Bin Huang},
  doi          = {10.1016/j.neunet.2019.11.015},
  journal      = {Neural Networks},
  pages        = {331-342},
  shortjournal = {Neural Netw.},
  title        = {ELM embedded discriminative dictionary learning for image classification},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Finite-time nonfragile time-varying proportional retarded
synchronization for markovian inertial memristive NNs with
reaction–diffusion items. <em>NN</em>, <em>123</em>, 317–330. (<a
href="https://doi.org/10.1016/j.neunet.2019.12.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The issue of synchronization for a class of inertial memristive neural networks over a finite-time interval is investigated in this paper. Specifically, reaction–diffusion items and Markovian jump parameters are both considered in the system model, meanwhile, a novel nonfragile time-varying proportional retarded control strategy is proposed. First, a befitting variable substitution is invoked to transform the original second-order differential system into a first-order one so that the corresponding synchronization error system that is represented by a first-order differential form is established. Second, by utilizing the integral inequality technique, reciprocally convex combination approach and free-weighting matrix method, a less conservative synchronization criterion in terms of linear matrix inequalities is obtained. Finally, three simulations are exploited to illustrate the feasibility, practicability and superiority of the designed controller so that the acquired theoretical results are supported.},
  archive      = {J_NN},
  author       = {Xiaona Song and Jingtao Man and Shuai Song and Zhen Wang},
  doi          = {10.1016/j.neunet.2019.12.011},
  journal      = {Neural Networks},
  pages        = {317-330},
  shortjournal = {Neural Netw.},
  title        = {Finite-time nonfragile time-varying proportional retarded synchronization for markovian inertial memristive NNs with reaction–diffusion items},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient network architecture search via multiobjective
particle swarm optimization based on decomposition. <em>NN</em>,
<em>123</em>, 305–316. (<a
href="https://doi.org/10.1016/j.neunet.2019.12.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The efforts devoted to manually increasing the width and depth of convolutional neural network (CNN) usually require a large amount of time and expertise. It has stimulated a rising demand of neural architecture search (NAS) over these years. However, most popular NAS approaches solely optimize for low prediction error without penalizing high structure complexity. To this end, this paper proposes MOPSO/D-Net, a CNN architecture search method with multiobjective particle swarm optimization based on decomposition (MOPSO/D). The main goal is to reformulate NAS as a multiobjective evolutionary optimization problem , where the optimal architecture is learned by minimizing two conflicting objectives, namely the error rate of classification and number of parameters of the network. Along with the hybrid binary encoding and adaptive penalty-based boundary intersection, an improved MOPSO/D is further proposed to solve the formulated multiobjective NAS and provide diverse tradeoff solutions. Experimental studies verify the effectiveness of MOPSO/D-Net compared with current manual and automated CNN generation methods. The proposed algorithm achieves impressive classification performance with a small number of parameters on each of two benchmark datasets, particularly, 0.4\% error rate with 0.16 M params on MNIST and 5.88\% error rate with 8.1 M params on CIFAR-10, respectively.},
  archive      = {J_NN},
  author       = {Jing Jiang and Fei Han and Qinghua Ling and Jie Wang and Tiange Li and Henry Han},
  doi          = {10.1016/j.neunet.2019.12.005},
  journal      = {Neural Networks},
  pages        = {305-316},
  shortjournal = {Neural Netw.},
  title        = {Efficient network architecture search via multiobjective particle swarm optimization based on decomposition},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Liver disease screening based on densely connected deep
neural networks. <em>NN</em>, <em>123</em>, 299–304. (<a
href="https://doi.org/10.1016/j.neunet.2019.11.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Liver disease is an important public health problem. Liver Function Tests (LFT) is the most achievable test for liver disease diagnosis. Most liver diseases are manifested as abnormal LFT. Liver disease screening by LFT data is helpful for computer aided diagnosis . In this paper, we propose a densely connected deep neural network (DenseDNN), on 13 most commonly used LFT indicators and demographic information of subjects for liver disease screening. The algorithm was tested on a dataset of 76,914 samples (more than 100 times of data than the previous datasets). The Area Under Curve (AUC) of DenseDNN is 0.8919, that of DNN is 0.8867, that of random forest is 0.8790, and that of logistic regression is 0.7974. The performance of deep learning models are significantly better than conventional methods. As for the deep learning methods, DenseDNN shows better performance than DNN.},
  archive      = {J_NN},
  author       = {Zhenjie Yao and Jiangong Li and Zhaoyu Guan and Yancheng Ye and Yixin Chen},
  doi          = {10.1016/j.neunet.2019.11.005},
  journal      = {Neural Networks},
  pages        = {299-304},
  shortjournal = {Neural Netw.},
  title        = {Liver disease screening based on densely connected deep neural networks},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extreme learning machine for a new hybrid
morphological/linear perceptron. <em>NN</em>, <em>123</em>, 288–298. (<a
href="https://doi.org/10.1016/j.neunet.2019.12.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Morphological neural networks (MNNs) can be characterized as a class of artificial neural networks that perform an operation of mathematical morphology at every node, possibly followed by the application of an activation function . Morphological perceptrons (MPs) and (gray-scale) morphological associative memories are among the most widely known MNN models. Since their neuronal aggregation functions are not differentiable, classical methods of non-linear optimization can in principle not be directly applied in order to train these networks. The same observation holds true for hybrid morphological/linear perceptrons and other related models. Circumventing these problems of non-differentiability, this paper introduces an extreme learning machine approach for training a hybrid morphological/linear perceptron, whose morphological components were drawn from previous MP models. We apply the resulting model to a number of well-known classification problems from the literature and compare the performance of our model with the ones of several related models, including some recent MNNs and hybrid morphological/linear neural networks .},
  archive      = {J_NN},
  author       = {Peter Sussner and Israel Campiotti},
  doi          = {10.1016/j.neunet.2019.12.003},
  journal      = {Neural Networks},
  pages        = {288-298},
  shortjournal = {Neural Netw.},
  title        = {Extreme learning machine for a new hybrid morphological/linear perceptron},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cuneate spiking neural network learning to classify
naturalistic texture stimuli under varying sensing conditions.
<em>NN</em>, <em>123</em>, 273–287. (<a
href="https://doi.org/10.1016/j.neunet.2019.11.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We implemented a functional neuronal network that was able to learn and discriminate haptic features from biomimetic tactile sensor inputs using a two-layer spiking neuron model and homeostatic synaptic learning mechanism. The first order neuron model was used to emulate biological tactile afferents and the second order neuron model was used to emulate biological cuneate neurons. We have evaluated 10 naturalistic textures using a passive touch protocol, under varying sensing conditions. Tactile sensor data acquired with five textures under five sensing conditions were used for a synaptic learning process, to tune the synaptic weights between tactile afferents and cuneate neurons. Using post-learning synaptic weights, we evaluated the individual and population cuneate neuron responses by decoding across 10 stimuli, under varying sensing conditions. This resulted in a high decoding performance. We further validated the decoding performance across stimuli, irrespective of sensing velocities using a set of 25 cuneate neuron responses. This resulted in a median decoding performance of 96\% across the set of cuneate neurons. Being able to learn and perform generalized discrimination across tactile stimuli , makes this functional spiking tactile system effective and suitable for further robotic applications .},
  archive      = {J_NN},
  author       = {Udaya B. Rongala and Alberto Mazzoni and Anton Spanne and Henrik Jörntell and Calogero M. Oddo},
  doi          = {10.1016/j.neunet.2019.11.020},
  journal      = {Neural Networks},
  pages        = {273-287},
  shortjournal = {Neural Netw.},
  title        = {Cuneate spiking neural network learning to classify naturalistic texture stimuli under varying sensing conditions},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust face alignment by cascaded regression and
de-occlusion. <em>NN</em>, <em>123</em>, 261–272. (<a
href="https://doi.org/10.1016/j.neunet.2019.12.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face alignment is a typical facial behavior analysis task in computer vision . However, the performance of face alignment is degraded greatly when the face image is partially occluded. In order to achieve better mapping between facial appearance features and shape increments, we propose a robust and occlusion-free face alignment algorithm in which a face de-occlusion module and a deep regression module are integrated into a cascaded deep generative regression model. The face de-occlusion module is a disentangled representation learning Generative Adversarial Networks (GANs) which aims to locate occlusions and recover the genuine appearance from partially occluded face image. The deep regression module can enhance facial appearance representation by utilizing the recovered faces to obtain more accurate regressors . Then, by the cascaded deep generative regression model, we recover the partially occluded face image and achieve accurate locating of landmarks gradually. It is interesting to show that the cascaded deep generative regression model can effectively locate occlusions and recover more genuine faces, which can be further used to improve the performance of face alignment. Experimental results conducted on four challenging occluded face datasets demonstrate that our method outperforms state-of-the-art methods.},
  archive      = {J_NN},
  author       = {Jun Wan and Jing Li and Zhihui Lai and Bo Du and Lefei Zhang},
  doi          = {10.1016/j.neunet.2019.12.009},
  journal      = {Neural Networks},
  pages        = {261-272},
  shortjournal = {Neural Netw.},
  title        = {Robust face alignment by cascaded regression and de-occlusion},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Existence and finite-time stability of discrete
fractional-order complex-valued neural networks with time delays.
<em>NN</em>, <em>123</em>, 248–260. (<a
href="https://doi.org/10.1016/j.neunet.2019.12.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Without decomposing complex-valued systems into real-valued systems, the existence and finite-time stability for discrete fractional-order complex-valued neural networks with time delays are discussed in this paper. First of all, in order to obtain the main results, a new discrete Caputo fractional difference equation is proposed in complex field based on the theory of discrete fractional calculus, which generalizes the fractional-order neural networks in the real domain. Additionally, by utilizing Arzela–Ascoli’s theorem, inequality scaling skills and fixed point theorem , some sufficient criteria of delay-dependent are deduced to ensure the existence and finite-time stability of solutions for proposed networks. Finally, the validity and feasibility of the derived theoretical results are indicated by two numerical examples with simulations. Furthermore, we have drawn the following facts: with the lower order, the discrete fractional-order complex-valued neural networks will achieve the finite-time stability more easily.},
  archive      = {J_NN},
  author       = {Xingxing You and Qiankun Song and Zhenjiang Zhao},
  doi          = {10.1016/j.neunet.2019.12.012},
  journal      = {Neural Networks},
  pages        = {248-260},
  shortjournal = {Neural Netw.},
  title        = {Existence and finite-time stability of discrete fractional-order complex-valued neural networks with time delays},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New h∞ state estimation criteria of delayed static neural
networks via the lyapunov–krasovskii functional with negative definite
terms. <em>NN</em>, <em>123</em>, 236–247. (<a
href="https://doi.org/10.1016/j.neunet.2019.12.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the estimation problem for delayed static neural networks (SNNs), constructing a proper Lyapunov–Krasovskii functional (LKF) is crucial for deriving less conservative estimation criteria . In this paper, a delay-product-type LKF with negative definite terms is proposed. Based on the third-order Bessel–Legendre (B–L) integral inequality and mixed convex combination approaches, a less conservative estimator design criterion is derived. Furthermore, the desired estimator gain matrices and the H ∞ H∞ performance index are obtained by solving a set of linear matrix inequalities (LMIs). Finally, a numerical example is given to demonstrate the effectiveness of the proposed method.},
  archive      = {J_NN},
  author       = {Jing He and Yan Liang and Feisheng Yang and Feng Yang},
  doi          = {10.1016/j.neunet.2019.12.008},
  journal      = {Neural Networks},
  pages        = {236-247},
  shortjournal = {Neural Netw.},
  title        = {New h∞ state estimation criteria of delayed static neural networks via the Lyapunov–Krasovskii functional with negative definite terms},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A counterexample regarding “new study on neural networks:
The essential order of approximation.” <em>NN</em>, <em>123</em>,
234–235. (<a
href="https://doi.org/10.1016/j.neunet.2019.12.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper “New study on neural networks : the essential order of approximation” by Jianjun Wang and Zongben Xu, which appeared in Neural Networks 23 (2010), deals with upper and lower estimates for the error of best approximation with sums of nearly exponential type activation functions in terms of moduli of smoothness. In particular, the presented lower bound is astonishingly good. However, the proof is incorrect and the bound is wrong.},
  archive      = {J_NN},
  author       = {Steffen Goebbels},
  doi          = {10.1016/j.neunet.2019.12.007},
  journal      = {Neural Networks},
  pages        = {234-235},
  shortjournal = {Neural Netw.},
  title        = {A counterexample regarding “New study on neural networks: The essential order of approximation”},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CS-MRI reconstruction based on analysis dictionary learning
and manifold structure regularization. <em>NN</em>, <em>123</em>,
217–233. (<a
href="https://doi.org/10.1016/j.neunet.2019.12.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compressed sensing (CS) significantly accelerates magnetic resonance imaging (MRI) by allowing the exact reconstruction of image from highly undersampling k-space data. In this process, the high sparsity obtained by the learned dictionary and exploitation of correlation among patches are essential to the reconstructed image quality. In this paper, by a use of these two aspects, we propose a novel CS-MRI model based on analysis dictionary learning and manifold structure regularization (ADMS). Furthermore, a proper tight frame constraint is used to obtain an effective overcomplete analysis dictionary with a high sparsifying capacity. The constructed manifold structure regularization nonuniformly enforces the correlation of each group formed by similar patches, which is more consistent with the diverse nonlocal similarity in realistic images. The proposed model is efficiently solved by the alternating direction method of multipliers (ADMM), in which the fast algorithm for each sub-problem is separately developed. The experimental results demonstrate that main components in the proposed method contribute to the final reconstruction performance and the effectiveness of the proposed model.},
  archive      = {J_NN},
  author       = {Jianxin Cao and Shujun Liu and Hongqing Liu and Hongwei Lu},
  doi          = {10.1016/j.neunet.2019.12.010},
  journal      = {Neural Networks},
  pages        = {217-233},
  shortjournal = {Neural Netw.},
  title        = {CS-MRI reconstruction based on analysis dictionary learning and manifold structure regularization},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Minimum variance-embedded deep kernel regularized least
squares method for one-class classification and its applications to
biomedical data. <em>NN</em>, <em>123</em>, 191–216. (<a
href="https://doi.org/10.1016/j.neunet.2019.12.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep kernel learning has been well explored for multi-class classification tasks ; however, relatively less work is done for one-class classification (OCC). OCC needs samples from only one class to train the model. Most recently, kernel regularized least squares (KRL) method-based deep architecture is developed for the OCC task. This paper introduces a novel extension of this method by embedding minimum variance information within this architecture. This embedding improves the generalization capability of the classifier by reducing the intra-class variance. In contrast to traditional deep learning methods , this method can effectively work with small-size datasets. We conduct a comprehensive set of experiments on 18 benchmark datasets (13 biomedical and 5 other datasets) to demonstrate the performance of the proposed classifier. We compare the results with 16 state-of-the-art one-class classifiers. Further, we also test our method for 2 real-world biomedical datasets viz.; detection of Alzheimer’s disease from structural magnetic resonance imaging data and detection of breast cancer from histopathological images . Proposed method exhibits more than 5\% F 1 F1 score compared to existing state-of-the-art methods for various biomedical benchmark datasets. This makes it viable for application in biomedical fields where relatively less amount of data is available.},
  archive      = {J_NN},
  author       = {Chandan Gautam and Pratik K. Mishra and Aruna Tiwari and Bharat Richhariya and Hari Mohan Pandey and Shuihua Wang and M. Tanveer and Alzheimer’s Disease Neuroimaging Initiative},
  doi          = {10.1016/j.neunet.2019.12.001},
  journal      = {Neural Networks},
  pages        = {191-216},
  shortjournal = {Neural Netw.},
  title        = {Minimum variance-embedded deep kernel regularized least squares method for one-class classification and its applications to biomedical data},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel multi-modal machine learning based approach for
automatic classification of EEG recordings in dementia. <em>NN</em>,
<em>123</em>, 176–190. (<a
href="https://doi.org/10.1016/j.neunet.2019.12.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalographic (EEG) recordings generate an electrical map of the human brain that are useful for clinical inspection of patients and in biomedical smart Internet-of-Things (IoT) and Brain-Computer Interface (BCI) applications. From a signal processing perspective, EEGs yield a nonlinear and nonstationary, multivariate representation of the underlying neural circuitry interactions. In this paper, a novel multi-modal Machine Learning (ML) based approach is proposed to integrate EEG engineered features for automatic classification of brain states. EEGs are acquired from neurological patients with Mild Cognitive Impairment (MCI) or Alzheimer’s disease (AD) and the aim is to discriminate Healthy Control (HC) subjects from patients. Specifically, in order to effectively cope with nonstationarities, 19-channels EEG signals are projected into the time–frequency (TF) domain by means of the Continuous Wavelet Transform (CWT) and a set of appropriate features (denoted as CWT features ) are extracted from δ δ , θ θ , α 1 α1 , α 2 α2 , β β EEG sub-bands. Furthermore, to exploit nonlinear phase-coupling information of EEG signals, higher order statistics (HOS) are extracted from the bispectrum (BiS) representation. BiS generates a second set of features (denoted as BiS features ) which are also evaluated in the five EEG sub-bands. The CWT and BiS features are fed into a number of ML classifiers to perform both 2-way (AD vs. HC, AD vs. MCI, MCI vs. HC) and 3-way (AD vs. MCI vs. HC) classifications. As an experimental benchmark, a balanced EEG dataset that includes 63 AD, 63 MCI and 63 HC is analyzed. Comparative results show that when the concatenation of CWT and BiS features (denoted as multi-modal (CWT+BiS) features ) is used as input, the Multi-Layer Perceptron (MLP) classifier outperforms all other models, specifically, the Autoencoder (AE), Logistic Regression (LR) and Support Vector Machine (SVM). Consequently, our proposed multi-modal ML scheme can be considered a viable alternative to state-of-the-art computationally intensive deep learning approaches.},
  archive      = {J_NN},
  author       = {Cosimo Ieracitano and Nadia Mammone and Amir Hussain and Francesco C. Morabito},
  doi          = {10.1016/j.neunet.2019.12.006},
  journal      = {Neural Networks},
  pages        = {176-190},
  shortjournal = {Neural Netw.},
  title        = {A novel multi-modal machine learning based approach for automatic classification of EEG recordings in dementia},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discriminative structure learning of sum–product networks
for data stream classification. <em>NN</em>, <em>123</em>, 163–175. (<a
href="https://doi.org/10.1016/j.neunet.2019.12.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sum–product network (SPN) is a deep probabilistic representation that allows for exact and tractable inference. There has been a trend of online SPN structure learning from massive and continuous data streams. However, online structure learning of SPNs has been introduced only for the generative settings so far. In this paper, we present an online discriminative approach for SPNs for learning both the structure and parameters. The basic idea is to keep track of informative and representative examples to capture the trend of time-changing class distributions. Specifically, by estimating the goodness of model fitting of data points and dynamically maintaining a certain amount of informative examples over time, we generate new sub-SPNs in a recursive and top-down manner. Meanwhile, an outlier-robust margin-based log-likelihood loss is applied locally to each data point and the parameters of SPN are updated continuously using most probable explanation (MPE) inference. This leads to a fast yet powerful optimization procedure and improved discrimination capability between the genuine class and rival classes. Empirical results show that the proposed approach achieves better prediction performance than the state-of-the-art online structure learner for SPNs, while promising order-of-magnitude speedup. Comparison with state-of-the-art stream classifiers further proves the superiority of our approach.},
  archive      = {J_NN},
  author       = {Zhengya Sun and Cheng-Lin Liu and Jinghao Niu and Wensheng Zhang},
  doi          = {10.1016/j.neunet.2019.12.002},
  journal      = {Neural Networks},
  pages        = {163-175},
  shortjournal = {Neural Netw.},
  title        = {Discriminative structure learning of sum–product networks for data stream classification},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evolving artificial neural networks with feedback.
<em>NN</em>, <em>123</em>, 153–162. (<a
href="https://doi.org/10.1016/j.neunet.2019.12.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks in the brain are dominated by sometimes more than 60\% feedback connections , which most often have small synaptic weights . Different from this, little is known how to introduce feedback into artificial neural networks . Here we use transfer entropy in the feed-forward paths of deep networks to identify feedback candidates between the convolutional layers and determine their final synaptic weights using genetic programming . This adds about 70\% more connections to these layers all with very small weights. Nonetheless performance improves substantially on different standard benchmark tasks and in different networks. To verify that this effect is generic we use 36000 configurations of small (2–10 hidden layer) conventional neural networks in a non-linear classification task and select the best performing feed-forward nets. Then we show that feedback reduces total entropy in these networks always leading to performance increase. This method may, thus, supplement standard techniques (e.g. error backprop) adding a new quality to network learning.},
  archive      = {J_NN},
  author       = {Sebastian Herzog and Christian Tetzlaff and Florentin Wörgötter},
  doi          = {10.1016/j.neunet.2019.12.004},
  journal      = {Neural Networks},
  pages        = {153-162},
  shortjournal = {Neural Netw.},
  title        = {Evolving artificial neural networks with feedback},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Dimension independent bounds for general shallow networks.
<em>NN</em>, <em>123</em>, 142–152. (<a
href="https://doi.org/10.1016/j.neunet.2019.11.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proves an abstract theorem addressing in a unified manner two important problems in function approximation : avoiding curse of dimensionality and estimating the degree of approximation for out-of-sample extension in manifold learning. We consider an abstract (shallow) network that includes, for example, neural networks , radial basis function networks , and kernels on data defined manifolds used for function approximation in various settings. A deep network is obtained by a composition of the shallow networks according to a directed acyclic graph, representing the architecture of the deep network. In this paper, we prove dimension independent bounds for approximation by shallow networks in the very general setting of what we have called G G -networks on a compact metric measure space, where the notion of dimension is defined in terms of the cardinality of maximal distinguishable sets, generalizing the notion of dimension of a cube or a manifold. Our techniques give bounds that improve without saturation with the smoothness of the kernel involved in an integral representation of the target function. In the context of manifold learning, our bounds provide estimates on the degree of approximation for an out-of-sample extension of the target function to the ambient space. One consequence of our theorem is that without the requirement of robust parameter selection, deep networks using a non-smooth activation function such as the ReLU, do not provide any significant advantage over shallow networks in terms of the degree of approximation alone.},
  archive      = {J_NN},
  author       = {H.N. Mhaskar},
  doi          = {10.1016/j.neunet.2019.11.006},
  journal      = {Neural Networks},
  pages        = {142-152},
  shortjournal = {Neural Netw.},
  title        = {Dimension independent bounds for general shallow networks},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Structured pruning of recurrent neural networks through
neuron selection. <em>NN</em>, <em>123</em>, 134–141. (<a
href="https://doi.org/10.1016/j.neunet.2019.11.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent neural networks (RNNs) have recently achieved remarkable successes in a number of applications. However, the huge sizes and computational burden of these models make it difficult for their deployment on edge devices. A practically effective approach is to reduce the overall storage and computation costs of RNNs by network pruning techniques. Despite their successful applications, those pruning methods based on Lasso either produce irregular sparse patterns in weight matrices , which is not helpful in practical speedup. To address these issues, we propose a structured pruning method through neuron selection which can remove the independent neuron of RNNs. More specifically, we introduce two sets of binary random variables , which can be interpreted as gates or switches to the input neurons and the hidden neurons , respectively. We demonstrate that the corresponding optimization problem can be addressed by minimizing the L 0 norm of the weight matrix. Finally, experimental results on language modeling and machine reading comprehension tasks have indicated the advantages of the proposed method in comparison with state-of-the-art pruning competitors. In particular, nearly 20 × practical speedup during inference was achieved without losing performance for the language model on the Penn TreeBank dataset, indicating the promising performance of the proposed method.},
  archive      = {J_NN},
  author       = {Liangjian Wen and Xuanyang Zhang and Haoli Bai and Zenglin Xu},
  doi          = {10.1016/j.neunet.2019.11.018},
  journal      = {Neural Networks},
  pages        = {134-141},
  shortjournal = {Neural Netw.},
  title        = {Structured pruning of recurrent neural networks through neuron selection},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploiting the stimuli encoding scheme of evolving spiking
neural networks for stream learning. <em>NN</em>, <em>123</em>, 118–133.
(<a href="https://doi.org/10.1016/j.neunet.2019.11.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stream data processing has lately gained momentum with the arrival of new Big Data scenarios and applications dealing with continuously produced information flows. Unfortunately, traditional machine learning algorithms are not prepared to tackle the specific challenges imposed by data stream processing, such as the need for learning incrementally, limited memory and processing time requirements, and adaptation to non-stationary data, among others. To face these paradigms, Spiking Neural Networks have emerged as one of the most promising stream learning techniques, with variants such as Evolving Spiking Neural Networks capable of efficiently addressing many of these challenges. Interestingly, these networks resort to a particular population encoding scheme – Gaussian Receptive Fields – to transform the incoming stimuli into temporal spikes. The study presented in this manuscript sheds light on the predictive potential of this encoding scheme, focusing on how it can be applied as a computationally lightweight, model-agnostic preprocessing step for data stream learning. We provide informed intuition to unveil under which circumstances the aforementioned population encoding method yields effective prediction gains in data stream classification with respect to the case where no preprocessing is performed. Results obtained for a variety of stream learning models and both synthetic and real stream datasets are discussed to empirically buttress the capability of Gaussian Receptive Fields to boost the predictive performance of stream learning methods, spanning further research towards extrapolating our findings to other machine learning problems.},
  archive      = {J_NN},
  author       = {Jesus L. Lobo and Izaskun Oregi and Albert Bifet and Javier Del Ser},
  doi          = {10.1016/j.neunet.2019.11.021},
  journal      = {Neural Networks},
  pages        = {118-133},
  shortjournal = {Neural Netw.},
  title        = {Exploiting the stimuli encoding scheme of evolving spiking neural networks for stream learning},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A scalable multi-signal approach for the parallelization of
self-organizing neural networks. <em>NN</em>, <em>123</em>, 108–117. (<a
href="https://doi.org/10.1016/j.neunet.2019.11.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-Organizing Neural Networks (SONNs) have a wide range of applications with massive computational requirements that often need to be satisfied with optimized parallel algorithms and implementations. In literature, SONN have been generally parallelized with GPU computing according to a single-signal paradigm: each GPU thread manages one or more nodes of the network and works concurrently on one input signal at the time. This paper presents two contributions. The first one is the experimental proof that the single-signal approach for SONNs is not optimal for the task, as it is intrinsically sequential at its core and thus inherently limited in its performance. The non-optimality of the single-signal paradigm is illustrated via a specific and simplified benchmark. The second contribution is the introduction of a new multi-signal paradigm for the parallelization of SONNs, whereby multiple signals are processed at once in each iteration hence allowing different GPU threads to work on different signals. The advantages of the multi-signal approach are shown through several benchmarks involving the Self-Organizing Adaptive Map (SOAM) algorithm as a basis for evaluation. Having a graph-based termination condition that depends on the features of the network being grown, the SOAM algorithm allows assessing both functional equivalence and performances of the paradigm proposed without relying on arbitrary thresholds. Nonetheless, the evaluation proposed has a broader scope since it refers to a unified framework for the GPU parallelization of a generic SONN.},
  archive      = {J_NN},
  author       = {Mirto Musci and Giacomo Parigi and Virginio Cantoni and Marco Piastra},
  doi          = {10.1016/j.neunet.2019.11.016},
  journal      = {Neural Networks},
  pages        = {108-117},
  shortjournal = {Neural Netw.},
  title        = {A scalable multi-signal approach for the parallelization of self-organizing neural networks},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discriminative margin-sensitive autoencoder for collective
multi-view disease analysis. <em>NN</em>, <em>123</em>, 94–107. (<a
href="https://doi.org/10.1016/j.neunet.2019.11.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical prediction is always collectively determined based on bioimages collected from different sources or various clinical characterizations described from multiple physiological features. Notably, learning intrinsic structures from multiple heterogeneous features is significant but challenging in multi-view disease understanding. Different from existing methods that separately deal with each single view, this paper proposes a discriminative Margin-Sensitive Autoencoder (MSAE) framework for automated Alzheimer’s disease (AD) diagnosis and accurate protein fold recognition. Generally, our MSAE aims to collaboratively explore the complementary properties of multi-view bioimage features in a semantic-sensitive encoder–decoder paradigm, where the discriminative semantic space is explicitly constructed in a margin-scalable regression model. Specifically, we develop a semantic-sensitive autoencoder, where an encoder projects multi-view visual features into the common semantic-aware latent space, and a decoder is exerted as an additional constraint to reconstruct the respective visual features. In particular, the importance of different views is adaptively weighted by self-adjusting learning scheme, such that their underlying correlations and complementary characteristics across multiple views are simultaneously preserved into the latent common representations. Moreover, a flexible semantic space is formulated by a margin-scalable support vector machine to improve the discriminability of the learning model. Importantly, correntropy induced metric is exploited as a robust regularization measurement to better control outliers for effective classification. A half-quadratic minimization and alternating learning strategy are devised to optimize the resulting framework such that each subproblem exists a closed-form solution in each iterative minimization phase. Extensive experimental results performed on the Alzheimer’s Disease Neuroimaging Initiative (ADNI) datasets show that our MSAE can achieve superior performances for both binary and multi-class classification in AD diagnosis, and evaluations on protein folds demonstrate that our method can achieve very encouraging performance on protein structure recognition, outperforming the state-of-the-art methods.},
  archive      = {J_NN},
  author       = {Zheng Zhang and Qi Zhu and Guo-Sen Xie and Yi Chen and Zhengming Li and Shuihua Wang},
  doi          = {10.1016/j.neunet.2019.11.013},
  journal      = {Neural Networks},
  pages        = {94-107},
  shortjournal = {Neural Netw.},
  title        = {Discriminative margin-sensitive autoencoder for collective multi-view disease analysis},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning physical properties in complex visual scenes: An
intelligent machine for perceiving blood flow dynamics from static CT
angiography imaging. <em>NN</em>, <em>123</em>, 82–93. (<a
href="https://doi.org/10.1016/j.neunet.2019.11.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Humans perceive physical properties such as motion and elastic force by observing objects in visual scenes. Recent research has proven that computers are capable of inferring physical properties from camera images like humans. However, few studies perceive the physical properties in more complex environment, i.e. humans have difficulty estimating physical quantities directly from the visual observation, or encounter difficulty visualizing the physical process in mind according to their daily experiences. As an appropriate example, fractional flow reserve (FFR), which measures the blood pressure difference across the vessel stenosis, becomes an important physical quantitative value determining the likelihood of myocardial ischemia in clinical coronary intervention procedure. In this study, we propose a novel deep neural network solution (TreeVes-Net) that allows machines to perceive FFR values directly from static coronary CT angiography images. Our framework fully utilizes a tree-structured recurrent neural network (RNN) with a coronary representation encoder. The encoder captures coronary geometric information providing the blood fluid-related representation. The tree-structured RNN builds a long-distance spatial dependency of blood flow information inside the coronary tree. The experiments performed on 13000 synthetic coronary trees and 180 real coronary trees from clinical patients show that the values of the area under ROC curve (AUC) are 0.92 and 0.93 under two clinical criterions. These results can demonstrate the effectiveness of our framework and its superiority to seven FFR computation methods based on machine learning .},
  archive      = {J_NN},
  author       = {Zhifan Gao and Xin Wang and Shanhui Sun and Dan Wu and Junjie Bai and Youbing Yin and Xin Liu and Heye Zhang and Victor Hugo C. de Albuquerque},
  doi          = {10.1016/j.neunet.2019.11.017},
  journal      = {Neural Networks},
  pages        = {82-93},
  shortjournal = {Neural Netw.},
  title        = {Learning physical properties in complex visual scenes: An intelligent machine for perceiving blood flow dynamics from static CT angiography imaging},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Global exponential synchronization of delayed memristive
neural networks with reaction–diffusion terms. <em>NN</em>,
<em>123</em>, 70–81. (<a
href="https://doi.org/10.1016/j.neunet.2019.11.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the global exponential synchronization problem of delayed memristive neural networks (MNNs) with reaction–diffusion terms. First, by utilizing the pinning control technique, two novel kinds of control methods are introduced to achieve synchronization of delayed MNNs with reaction–diffusion terms. Then, with the help of inequality techniques, pinning control technique, the drive–response concept and Lyapunov functional method, two sufficient conditions are obtained in the form of algebraic inequalities, which can be used for ensuring the exponential synchronization of the proposed delayed MNNs with reaction–diffusion terms. Moreover, the obtained results based on algebraic inequality complement and improve the previously known results. Finally, two illustrative examples are given to support the effectiveness and validity of the obtained theoretical results.},
  archive      = {J_NN},
  author       = {Yanyi Cao and Yuting Cao and Zhenyuan Guo and Tingwen Huang and Shiping Wen},
  doi          = {10.1016/j.neunet.2019.11.008},
  journal      = {Neural Networks},
  pages        = {70-81},
  shortjournal = {Neural Netw.},
  title        = {Global exponential synchronization of delayed memristive neural networks with reaction–diffusion terms},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling functional resting-state brain networks through
neural message passing on the human connectome. <em>NN</em>,
<em>123</em>, 52–69. (<a
href="https://doi.org/10.1016/j.neunet.2019.11.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a natural model for information flow in the brain through a neural message-passing dynamics on a structural network of macroscopic regions, such as the human connectome (HC). In our model, each brain region is assumed to have a binary behavior (active or not), the strengths of interactions among them are encoded in the anatomical connectivity matrix defined by the HC, and the dynamics of the system is defined by the Belief Propagation (BP) algorithm, working near the critical point of the network. We show that in the absence of direct external stimuli the BP algorithm converges to a spatial map of activations that is similar to the Default Mode Network (DMN) of the brain, which has been defined from the analysis of functional MRI data. Moreover, we use Susceptibility Propagation (SP) to compute the matrix of long-range correlations between the different regions and show that the modules defined by a clustering of this matrix resemble several Resting State Networks (RSN) determined experimentally. Both results suggest that the functional DMN and RSNs can be seen as simple consequences of the anatomical structure of the brain and a neural message-passing dynamics between macroscopic regions. With the new model, we explore predictions on how functional maps change when the anatomical brain network suffers structural alterations, like in Alzheimer’s disease and in lesions of the Corpus Callosum . The implications and novel interpretations suggested by the model, as well as the role of criticality , are discussed.},
  archive      = {J_NN},
  author       = {Julio A. Peraza-Goicolea and Eduardo Martínez-Montes and Eduardo Aubert and Pedro A. Valdés-Hernández and Roberto Mulet},
  doi          = {10.1016/j.neunet.2019.11.014},
  journal      = {Neural Networks},
  pages        = {52-69},
  shortjournal = {Neural Netw.},
  title        = {Modeling functional resting-state brain networks through neural message passing on the human connectome},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simplified calcium signaling cascade for synaptic
plasticity. <em>NN</em>, <em>123</em>, 38–51. (<a
href="https://doi.org/10.1016/j.neunet.2019.11.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a model for synaptic plasticity based on a calcium signaling cascade. The model simplifies the full signaling pathways from a calcium influx to the phosphorylation (potentiation) and dephosphorylation (depression) of glutamate receptors that are gated by fictive C 1 and C 2 catalysts, respectively. This model is based on tangible chemical reactions, including fictive catalysts, for long-term plasticity rather than the conceptual theories commonplace in various models, such as preset thresholds of calcium concentration. Our simplified model successfully reproduced the experimental synaptic plasticity induced by different protocols such as (i) a synchronous pairing protocol and (ii) correlated presynaptic and postsynaptic action potentials (APs). Further, the ocular dominance plasticity (or the experimental verification of the celebrated Bienenstock—Cooper—Munro theory) was reproduced by two model synapses that compete by means of back-propagating APs (bAPs). The key to this competition is synapse-specific bAPs with reference to bAP-boosting on the physiological grounds.},
  archive      = {J_NN},
  author       = {Vladimir Kornijcuk and Dohun Kim and Guhyun Kim and Doo Seok Jeong},
  doi          = {10.1016/j.neunet.2019.11.022},
  journal      = {Neural Networks},
  pages        = {38-51},
  shortjournal = {Neural Netw.},
  title        = {Simplified calcium signaling cascade for synaptic plasticity},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiple partial empirical kernel learning with instance
weighting and boundary fitting. <em>NN</em>, <em>123</em>, 26–37. (<a
href="https://doi.org/10.1016/j.neunet.2019.11.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By dividing the original data set into several sub-sets, Multiple Partial Empirical Kernel Learning (MPEKL) constructs multiple kernel matrixes corresponding to the sub-sets, and these kernel matrixes are decomposed to provide the explicit kernel functions . Then, the instances in the original data set are mapped into multiple kernel spaces, which provide better performance than single kernel space. It is known that the instances in different locations and distributions behave differently. Therefore, this paper defines the weight of instance in accordance with the location and distribution of the instances. According to the location, the instances can be categorized into intrinsic instances, boundary instances and noise instances. Generally, the boundary instances, as well as the minority instances in the imbalanced data set, are assigned high weight. Meanwhile, a regularization term, which regulates the classification hyperplane to fit the distribution trend of the class boundary, is constructed by the boundary instances. Then, the weight of instance and the regularization term are introduced into MPEKL to form an algorithm named Multiple Partial Empirical Kernel Learning with Instance Weighting and Boundary Fitting (IBMPEKL). Experiments demonstrate the good performance of IBMPEKL and validate the effectiveness of the instance weighting and boundary fitting.},
  archive      = {J_NN},
  author       = {Zonghai Zhu and Zhe Wang and Dongdong Li and Wenli Du and Yangming Zhou},
  doi          = {10.1016/j.neunet.2019.11.019},
  journal      = {Neural Networks},
  pages        = {26-37},
  shortjournal = {Neural Netw.},
  title        = {Multiple partial empirical kernel learning with instance weighting and boundary fitting},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neonatal seizure detection from raw multi-channel EEG using
a fully convolutional architecture. <em>NN</em>, <em>123</em>, 12–25.
(<a href="https://doi.org/10.1016/j.neunet.2019.11.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A deep learning classifier for detecting seizures in neonates is proposed. This architecture is designed to detect seizure events from raw electroencephalogram (EEG) signals as opposed to the state-of-the-art hand engineered feature-based representation employed in traditional machine learning based solutions. The seizure detection system utilises only convolutional layers in order to process the multichannel time domain signal and is designed to exploit the large amount of weakly labelled data in the training stage. The system performance is assessed on a large database of continuous EEG recordings of 834h in duration; this is further validated on a held-out publicly available dataset and compared with two baseline SVM based systems. The developed system achieves a 56\% relative improvement with respect to a feature-based state-of-the art baseline, reaching an AUC of 98.5\%; this also compares favourably both in terms of performance and run-time. The effect of varying architectural parameters is thoroughly studied. The performance improvement is achieved through novel architecture design which allows more efficient usage of available training data and end-to-end optimisation from the front-end feature extraction to the back-end classification. The proposed architecture opens new avenues for the application of deep learning to neonatal EEG, where the performance becomes a function of the amount of training data with less dependency on the availability of precise clinical labels.},
  archive      = {J_NN},
  author       = {Alison O’Shea and Gordon Lightbody and Geraldine Boylan and Andriy Temko},
  doi          = {10.1016/j.neunet.2019.11.023},
  journal      = {Neural Networks},
  pages        = {12-25},
  shortjournal = {Neural Netw.},
  title        = {Neonatal seizure detection from raw multi-channel EEG using a fully convolutional architecture},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Finite-time and fixed-time anti-synchronization of markovian
neural networks with stochastic disturbances via switching control.
<em>NN</em>, <em>123</em>, 1–11. (<a
href="https://doi.org/10.1016/j.neunet.2019.11.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a unified theoretical framework to study the problem of finite/fixed-time drive–response anti-synchronization for a class of Markovian stochastic neural networks . State feedback switching controllers without the sign function are designed to achieve the finite/fixed-time anti-synchronization of the addressed systems. Compared with the existing synchronization criteria, our results indicate that the controllers via the switching control without the sign function are given with less conservativeness , and the controllers without any sign function can deal with the chattering problem. By employing Lyapunov functional method and properties of the Weiner process, several finite/fixed-time synchronization criteria are presented and the corresponding settling times are calculated as well. Finally, three numerical examples are provided to illustrate the effectiveness of the theoretical results.},
  archive      = {J_NN},
  author       = {Peng Wan and Dihua Sun and Min Zhao},
  doi          = {10.1016/j.neunet.2019.11.012},
  journal      = {Neural Networks},
  pages        = {1-11},
  shortjournal = {Neural Netw.},
  title        = {Finite-time and fixed-time anti-synchronization of markovian neural networks with stochastic disturbances via switching control},
  volume       = {123},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020k). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>122</em>, II. (<a
href="https://doi.org/10.1016/S0893-6080(19)30410-1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(19)30410-1},
  journal      = {Neural Networks},
  pages        = {II},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {122},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020k). Current events. <em>NN</em>, <em>122</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(19)30409-5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(19)30409-5},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {Current events},
  volume       = {122},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Realistic spiking neural network: Non-synaptic mechanisms
improve convergence in cell assembly. <em>NN</em>, <em>122</em>,
420–433. (<a
href="https://doi.org/10.1016/j.neunet.2019.09.038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning in neural networks inspired by brain tissue has been studied for machine learning applications. However, existing works primarily focused on the concept of synaptic weight modulation, and other aspects of neuronal interactions, such as non-synaptic mechanisms, have been neglected. Non-synaptic interaction mechanisms have been shown to play significant roles in the brain, and four classes of these mechanisms can be highlighted: (i) electrotonic coupling; (ii) ephaptic interactions; (iii) electric field effects; and iv) extracellular ionic fluctuations. In this work, we proposed simple rules for learning inspired by recent findings in machine learning adapted to a realistic spiking neural network. We show that the inclusion of non-synaptic interaction mechanisms improves cell assembly convergence. By including extracellular ionic fluctuation represented by the extracellular electrodiffusion in the network, we showed the importance of these mechanisms to improve cell assembly convergence. Additionally, we observed a variety of electrophysiological patterns of neuronal activity , particularly bursting and synchronism when the convergence is improved.},
  archive      = {J_NN},
  author       = {Damien Depannemaecker and Luiz Eduardo Canton Santos and Antônio Márcio Rodrigues and Carla Alessandra Scorza and Fulvio Alexandre Scorza and Antônio-Carlos Guimarães de Almeida},
  doi          = {10.1016/j.neunet.2019.09.038},
  journal      = {Neural Networks},
  pages        = {420-433},
  shortjournal = {Neural Netw.},
  title        = {Realistic spiking neural network: Non-synaptic mechanisms improve convergence in cell assembly},
  volume       = {122},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Person identification using fusion of iris and periocular
deep features. <em>NN</em>, <em>122</em>, 407–419. (<a
href="https://doi.org/10.1016/j.neunet.2019.11.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel method for person identification based on the fusion of iris and periocular biometrics has been proposed in this paper. The challenges for image acquisition for Near-Infrared or Visual Wavelength lights under constrained and unconstrained environments have been considered here. The proposed system is divided into image preprocessing data augmentation followed by feature learning for classification components. In image preprocessing an annular iris, the portion is segmented out from an eyeball image and then transformed into a fixed-sized image region. The parameters of iris localization have been used to extract the local periocular region. Due to different imaging environments, the images suffer from various noise artifacts which create data insufficiency and complicate the recognition task. To overcome this situation, a novel method for data augmentation technique has been introduced here. For features extraction and classification tasks well-known, VGG16, ResNet50 , and Inception-v3 CNN architectures have been employed. The performance due to iris and periocular are fused together to increase the performance of the recognition system. The extensive experimental results have been demonstrated in four benchmark iris databases namely: MMU1, UPOL, CASIA-Iris-distance, and UBIRIS.v2. The comparison with the state-of-the-art methods with respect to these databases shows the robustness and effectiveness of the proposed approach.},
  archive      = {J_NN},
  author       = {Saiyed Umer and Alamgir Sardar and Bibhas Chandra Dhara and Ranjeet Kumar Rout and Hari Mohan Pandey},
  doi          = {10.1016/j.neunet.2019.11.009},
  journal      = {Neural Networks},
  pages        = {407-419},
  shortjournal = {Neural Netw.},
  title        = {Person identification using fusion of iris and periocular deep features},
  volume       = {122},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simultaneously learning affinity matrix and data
representations for machine fault diagnosis. <em>NN</em>, <em>122</em>,
395–406. (<a
href="https://doi.org/10.1016/j.neunet.2019.11.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, preserving geometry information of data while learning representations have attracted increasing attention in intelligent machine fault diagnosis. Existing geometry preserving methods require to predefine the similarities between data points in the original data space. The predefined affinity matrix , which is also known as the similarity matrix , is then used to preserve geometry information during the process of representations learning. Hence, the data representations are learned under the assumption of a fixed and known prior knowledge, i.e., similarities between data points. However, the assumed prior knowledge is difficult to precisely determine the real relationships between data points, especially in high dimensional space. Also, using two separated steps to learn affinity matrix and data representations may not be optimal and universal for data classification . In this paper, based on the extreme learning machine autoencoder (ELM-AE), we propose to learn the data representations and the affinity matrix simultaneously. The affinity matrix is treated as a variable and unified in the objective function of ELM-AE. Instead of predefining and fixing the affinity matrix, the proposed method adjusts the similarities by taking into account its capability of capturing the geometry information in both original data space and non-linearly mapped representation space. Meanwhile, the geometry information of original data can be preserved in the embedded representations with the help of the affinity matrix. Experimental results on several benchmark datasets demonstrate the effectiveness of the proposed method, and the empirical study also shows it is an efficient tool on machine fault diagnosis.},
  archive      = {J_NN},
  author       = {Yue Li and Yijie Zeng and Tianchi Liu and Xiaofan Jia and Guang-Bin Huang},
  doi          = {10.1016/j.neunet.2019.11.007},
  journal      = {Neural Networks},
  pages        = {395-406},
  shortjournal = {Neural Netw.},
  title        = {Simultaneously learning affinity matrix and data representations for machine fault diagnosis},
  volume       = {122},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Global mittag-leffler stability and synchronization of
discrete-time fractional-order complex-valued neural networks with time
delay. <em>NN</em>, <em>122</em>, 382–394. (<a
href="https://doi.org/10.1016/j.neunet.2019.11.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Without decomposing complex-valued systems into real-valued systems, this paper investigates existence, uniqueness, global Mittag-Leffler stability and global Mittag-Leffler synchronization of discrete-time fractional-order complex-valued neural networks (FCVNNs) with time delay . Inspired by Lyapunov’s direct method on continuous-time systems, a class of discrete-time FCVNNs is further discussed by employing the fractional-order extension of Lyapunov’s direct method. Firstly, by means of contraction mapping theory and Cauchy’s inequality, a sufficient condition is presented to ascertain the existence and uniqueness of the equilibrium point for discrete-time FCVNNs. Then, based on the theory of discrete fractional calculus, discrete Laplace transform , the theory of complex functions and discrete Mittag-Leffler functions, a sufficient condition is established for global Mittag-Leffler stability of the proposed networks. Additionally, by applying the Lyapunov’s direct method and designing a effective control scheme, the sufficient criterion is derived to ensure the global Mittag-Leffler synchronization of discrete-time FCVNNs. Finally, two numerical examples are also presented to manifest the feasibility and validity of the obtained results.},
  archive      = {J_NN},
  author       = {Xingxing You and Qiankun Song and Zhenjiang Zhao},
  doi          = {10.1016/j.neunet.2019.11.004},
  journal      = {Neural Networks},
  pages        = {382-394},
  shortjournal = {Neural Netw.},
  title        = {Global mittag-leffler stability and synchronization of discrete-time fractional-order complex-valued neural networks with time delay},
  volume       = {122},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generative adversarial networks with mixture of
t-distributions noise for diverse image generation. <em>NN</em>,
<em>122</em>, 374–381. (<a
href="https://doi.org/10.1016/j.neunet.2019.11.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image generation is a long-standing problem in the machine learning and computer vision areas. In order to generate images with high diversity, we propose a novel model called generative adversarial networks with mixture of t-distributions noise (tGANs). In tGANs, the latent generative space is formulated using a mixture of t-distributions. Particularly, the parameters of the components in the mixture of t-distributions can be learned along with others in the model. To improve the diversity of the generated images in each class, each noise vector and a class codeword are concatenated as the input of the generator of tGANs. In addition, a classification loss is added to both the generator and the discriminator losses to strengthen their performances. We have conducted extensive experiments to compare tGANs with a state-of-the-art pixel by pixel image generation approach, pixelCNN, and related GAN-based models. The experimental results and statistical comparisons demonstrate that tGANs perform significantly better than pixleCNN and related GAN-based models for diverse image generation.},
  archive      = {J_NN},
  author       = {Jinxuan Sun and Guoqiang Zhong and Yang Chen and Yongbin Liu and Tao Li and Kaizhu Huang},
  doi          = {10.1016/j.neunet.2019.11.003},
  journal      = {Neural Networks},
  pages        = {374-381},
  shortjournal = {Neural Netw.},
  title        = {Generative adversarial networks with mixture of t-distributions noise for diverse image generation},
  volume       = {122},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local distinguishability aggrandizing network for human
anomaly detection. <em>NN</em>, <em>122</em>, 364–373. (<a
href="https://doi.org/10.1016/j.neunet.2019.11.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the growing demand for an intelligent system to prevent abnormal events, many methods have been proposed to detect and locate anomalous behaviors in surveillance videos . However, most of these methods contain two shortcomings mainly: distraction of the network and insufficient discriminating ability. In this paper, we propose a local distinguishability aggrandizing network (LDA-Net) in a supervised manner, consisting of a human detection module and an anomaly detection module. In the human detection module, we obtain segmented patches of specific human subjects and take them as the input of the latter module to focus the network on learning motion characteristics of each person. In addition, considering that the auxiliary information, such as the specific type of an action, can aggrandize the whole network to extract distinguishable detail features of normal and abnormal behaviors , the proposed anomaly detection module comprises a primary binary classification sub-branch and an auxiliary distinguishability aggrandizing sub-branch, through which we can jointly detect anomalies and recognize actions. To further reduce the misclassification of the extremely imbalanced datasets, we design a novel inhibition loss function and embed it into the auxiliary sub-branch of the anomaly detection module. Experiments on several public benchmark datasets for frame-level and pixel-level anomaly detection show that the proposed supervised LDA-Net achieves state-of-the-art results on UCSD Ped2 and Subway Exit datasets.},
  archive      = {J_NN},
  author       = {Maoguo Gong and Huimin Zeng and Yu Xie and Hao Li and Zedong Tang},
  doi          = {10.1016/j.neunet.2019.11.002},
  journal      = {Neural Networks},
  pages        = {364-373},
  shortjournal = {Neural Netw.},
  title        = {Local distinguishability aggrandizing network for human anomaly detection},
  volume       = {122},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A review on neural network models of schizophrenia and
autism spectrum disorder. <em>NN</em>, <em>122</em>, 338–363. (<a
href="https://doi.org/10.1016/j.neunet.2019.10.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This survey presents the most relevant neural network models of autism spectrum disorder and schizophrenia, from the first connectionist models to recent deep neural network architectures. We analyzed and compared the most representative symptoms with its neural model counterpart, detailing the alteration introduced in the network that generates each of the symptoms, and identifying their strengths and weaknesses. We additionally cross-compared Bayesian and free-energy approaches, as they are widely applied to model psychiatric disorders and share basic mechanisms with neural networks. Models of schizophrenia mainly focused on hallucinations and delusional thoughts using neural dysconnections or inhibitory imbalance as the predominating alteration. Models of autism rather focused on perceptual difficulties, mainly excessive attention to environment details, implemented as excessive inhibitory connections or increased sensory precision. We found an excessively tight view of the psychopathologies around one specific and simplified effect, usually constrained to the technical idiosyncrasy of the used network architecture. Recent theories and evidence on sensorimotor integration and body perception combined with modern neural network architectures could offer a broader and novel spectrum to approach these psychopathologies. This review emphasizes the power of artificial neural networks for modeling some symptoms of neurological disorders but also calls for further developing of these techniques in the field of computational psychiatry.},
  archive      = {J_NN},
  author       = {Pablo Lanillos and Daniel Oliva and Anja Philippsen and Yuichi Yamashita and Yukie Nagai and Gordon Cheng},
  doi          = {10.1016/j.neunet.2019.10.014},
  journal      = {Neural Networks},
  pages        = {338-363},
  shortjournal = {Neural Netw.},
  title        = {A review on neural network models of schizophrenia and autism spectrum disorder},
  volume       = {122},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New approach to global mittag-leffler synchronization
problem of fractional-order quaternion-valued BAM neural networks based
on a new inequality. <em>NN</em>, <em>122</em>, 320–337. (<a
href="https://doi.org/10.1016/j.neunet.2019.10.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel kind of neural networks named fractional-order quaternion-valued bidirectional associative memory neural networks (FQVBAMNNs) is formulated. On one hand, applying Hamilton rules in quaternion multiplication which is essentially non-commutative, the system of FQVBAMNNs is separated into eight fractional-order real-valued systems. Meanwhile, the activation functions are considered to be quaternion-valued linear threshold ones which help to reduce the unnecessary computational complexity . On the other hand, based on fractional-order Lyapunov technology, a new fractional-order derivative inequality is established. Mainly by employing the new inequality technique, constructing three novel Lyapunov–Krasovskii functionals (LKFs) and designing simple linear controllers , the global Mittag-Leffler synchronization problems are investigated and the corresponding criteria are acquired for the system of FQVBAMNNs and its special cases such as fractional-order complex-valued BAM neural networks (FCVBAMNNs) and fractional-order real-valued BAM neural networks (FRVBAMNNs), respectively. Finally, two numerical examples are given to show the effectiveness and availability of the proposed results.},
  archive      = {J_NN},
  author       = {Jianying Xiao and Shiping Wen and Xujun Yang and Shouming Zhong},
  doi          = {10.1016/j.neunet.2019.10.017},
  journal      = {Neural Networks},
  pages        = {320-337},
  shortjournal = {Neural Netw.},
  title        = {New approach to global mittag-leffler synchronization problem of fractional-order quaternion-valued BAM neural networks based on a new inequality},
  volume       = {122},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model-based optimized phase-deviation deep brain stimulation
for parkinson ’s disease. <em>NN</em>, <em>122</em>, 308–319. (<a
href="https://doi.org/10.1016/j.neunet.2019.11.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-frequency deep brain stimulation (HF-DBS) of the subthalamic nucleus (STN), globus pallidus interna (GPi) and globus pallidus externa (GPe) are often considered as effective methods for the treatment of Parkinson’s disease (PD). However, the stimulation of a single nucleus by HF-DBS can cause specific physical damage, produce side effects and usually consume more electrical energy. Therefore, we use a biophysically-based model of basal ganglia-thalamic circuits to explore more effective stimulation patterns to reduce adverse effects and save energy. In this paper, we computationally investigate the combined DBS of two nuclei with the phase deviation between two stimulation waveforms (CDBS). Three different stimulation combination strategies are proposed, i.e., STN and GPe CDBS (SED), STN and GPi CDBS (SID), as well as GPi and GPe CDBS (GGD). Resultantly, it is found that anti-phase CDBS is more effective in improving parkinsonian dynamical properties, including desynchronization of neurons and the recovery of the thalamus relay ability. Detailed simulation investigation shows that anti-phase SED and GGD are superior to SID. Besides, the energy consumption can be largely reduced by SED and GGD (72.5\% and 65.5\%), compared to HF-DBS. These results provide new insights into the optimal stimulation parameter and target choice of PD, which may be helpful for the clinical practice.},
  archive      = {J_NN},
  author       = {Ying Yu and Yuqing Hao and Qingyun Wang},
  doi          = {10.1016/j.neunet.2019.11.001},
  journal      = {Neural Networks},
  pages        = {308-319},
  shortjournal = {Neural Netw.},
  title        = {Model-based optimized phase-deviation deep brain stimulation for parkinson ’s disease},
  volume       = {122},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Affinity and class probability-based fuzzy support vector
machine for imbalanced data sets. <em>NN</em>, <em>122</em>, 289–307.
(<a href="https://doi.org/10.1016/j.neunet.2019.10.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The learning problem from imbalanced data sets poses a major challenge in data mining community. Although conventional support vector machine can generally show relatively robust performance in dealing with the classification problems of imbalanced data sets, it treats all training samples with the same contribution for learning, which results in the final decision boundary biasing toward the majority class especially in the presence of outliers or noises. In this paper, we propose a new affinity and class probability-based fuzzy support vector machine technique (ACFSVM). The affinity of a majority class sample is calculated according to support vector description domain (SVDD) model trained only by the given majority class training samples in kernel space similar to that used for FSVM learning. The obtained affinity can be used for identifying possible outliers and some border samples existing in the majority class training samples. In order to eliminate the effect of noises, we employ the kernel k k -nearest neighbor method to determine the class probability of the majority class samples in the same kernel space as before. The samples with lower class probabilities are more likely to be noises and their contribution for learning seems to be reduced by their low memberships constructed by combining the affinities and the class probabilities. Thus, ACFSVM can pay more attention to the majority class samples with higher affinities and class probabilities while reducing their effects of the ones with lower affinities and class probabilities, eventually skewing the final classification boundary toward the majority class. In addition, the minority class samples are assigned relative high memberships to guarantee their importance for the model learning. The extensive experimental results on the different imbalanced datasets from UCI repository demonstrate that the proposed approach can achieve better generalization performance in terms of G-Mean, F-Measure, and AUC as compared to the other existing imbalanced dataset classification techniques .},
  archive      = {J_NN},
  author       = {Xinmin Tao and Qing Li and Chao Ren and Wenjie Guo and Qing He and Rui Liu and Junrong Zou},
  doi          = {10.1016/j.neunet.2019.10.016},
  journal      = {Neural Networks},
  pages        = {289-307},
  shortjournal = {Neural Netw.},
  title        = {Affinity and class probability-based fuzzy support vector machine for imbalanced data sets},
  volume       = {122},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Partition level multiview subspace clustering. <em>NN</em>,
<em>122</em>, 279–288. (<a
href="https://doi.org/10.1016/j.neunet.2019.10.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiview clustering has gained increasing attention recently due to its ability to deal with multiple sources (views) data and explore complementary information between different views. Among various methods, multiview subspace clustering methods provide encouraging performance. They mainly integrate the multiview information in the space where the data points lie. Hence, their performance may be deteriorated because of noises existing in each individual view or inconsistent between heterogeneous features. For multiview clustering, the basic premise is that there exists a shared partition among all views. Therefore, the natural space for multiview clustering should be all partitions. Orthogonal to existing methods, we propose to fuse multiview information in partition level following two intuitive assumptions: (i) each partition is a perturbation of the consensus clustering; (ii) the partition that is close to the consensus clustering should be assigned a large weight. Finally, we propose a unified multiview subspace clustering model which incorporates the graph learning from each view, the generation of basic partitions, and the fusion of consensus partition . These three components are seamlessly integrated and can be iteratively boosted by each other towards an overall optimal solution. Experiments on four benchmark datasets demonstrate the efficacy of our approach against the state-of-the-art techniques.},
  archive      = {J_NN},
  author       = {Zhao Kang and Xinjia Zhao and Chong Peng and Hongyuan Zhu and Joey Tianyi Zhou and Xi Peng and Wenyu Chen and Zenglin Xu},
  doi          = {10.1016/j.neunet.2019.10.010},
  journal      = {Neural Networks},
  pages        = {279-288},
  shortjournal = {Neural Netw.},
  title        = {Partition level multiview subspace clustering},
  volume       = {122},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Perceptrons from memristors. <em>NN</em>, <em>122</em>,
273–278. (<a
href="https://doi.org/10.1016/j.neunet.2019.10.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Memristors , resistors with memory whose outputs depend on the history of their inputs, have been used with success in neuromorphic architectures, particularly as synapses and non-volatile memories. However, to the best of our knowledge, no model for a network in which both the synapses and the neurons are implemented using memristors has been proposed so far. In the present work we introduce models for single and multilayer perceptrons based exclusively on memristors. We adapt the delta rule to the memristor-based single-layer perceptron and the backpropagation algorithm to the memristor-based multilayer perceptron. Our results show that both perform as expected for perceptrons , including satisfying Minsky–Papert’s theorem. As a consequence of the Universal Approximation Theorem , they also show that memristors are universal function approximators . By using memristors for both the neurons and the synapses, our models pave the way for novel memristor-based neural network architectures and algorithms. A neural network based on memristors could show advantages in terms of energy conservation and open up possibilities for other learning systems to be adapted to a memristor-based paradigm, both in the classical and quantum learning realms.},
  archive      = {J_NN},
  author       = {Francisco Silva and Mikel Sanz and João Seixas and Enrique Solano and Yasser Omar},
  doi          = {10.1016/j.neunet.2019.10.013},
  journal      = {Neural Networks},
  pages        = {273-278},
  shortjournal = {Neural Netw.},
  title        = {Perceptrons from memristors},
  volume       = {122},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A review of learning in biologically plausible spiking
neural networks. <em>NN</em>, <em>122</em>, 253–272. (<a
href="https://doi.org/10.1016/j.neunet.2019.09.036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial neural networks have been used as a powerful processing tool in various areas such as pattern recognition, control, robotics, and bioinformatics. Their wide applicability has encouraged researchers to improve artificial neural networks by investigating the biological brain. Neurological research has significantly progressed in recent years and continues to reveal new characteristics of biological neurons. New technologies can now capture temporal changes in the internal activity of the brain in more detail and help clarify the relationship between brain activity and the perception of a given stimulus. This new knowledge has led to a new type of artificial neural network, the Spiking Neural Network (SNN), that draws more faithfully on biological properties to provide higher processing abilities. A review of recent developments in learning of spiking neurons is presented in this paper. First the biological background of SNN learning algorithms is reviewed. The important elements of a learning algorithm such as the neuron model, synaptic plasticity , information encoding and SNN topologies are then presented. Then, a critical review of the state-of-the-art learning algorithms for SNNs using single and multiple spikes is presented. Additionally, deep spiking neural networks are reviewed, and challenges and opportunities in the SNN field are discussed.},
  archive      = {J_NN},
  author       = {Aboozar Taherkhani and Ammar Belatreche and Yuhua Li and Georgina Cosma and Liam P. Maguire and T.M. McGinnity},
  doi          = {10.1016/j.neunet.2019.09.036},
  journal      = {Neural Networks},
  pages        = {253-272},
  shortjournal = {Neural Netw.},
  title        = {A review of learning in biologically plausible spiking neural networks},
  volume       = {122},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multistability of switched neural networks with sigmoidal
activation functions under state-dependent switching. <em>NN</em>,
<em>122</em>, 239–252. (<a
href="https://doi.org/10.1016/j.neunet.2019.10.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents theoretical results on the multistability of switched neural networks with commonly used sigmoidal activation functions under state-dependent switching. The multistability analysis with such an activation function is difficult because state–space partition is not as straightforward as that with piecewise-linear activations. Sufficient conditions are derived for ascertaining the existence and stability of multiple equilibria. It is shown that the number of stable equilibria of an n n -neuron switched neural networks is up to 3 n 3n under given conditions. In contrast to existing multistability results with piecewise-linear activation functions , the results herein are also applicable to the equilibria at switching points. Four examples are discussed to substantiate the theoretical results.},
  archive      = {J_NN},
  author       = {Zhenyuan Guo and Shiqin Ou and Jun Wang},
  doi          = {10.1016/j.neunet.2019.10.012},
  journal      = {Neural Networks},
  pages        = {239-252},
  shortjournal = {Neural Netw.},
  title        = {Multistability of switched neural networks with sigmoidal activation functions under state-dependent switching},
  volume       = {122},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multistability and attraction basins of discrete-time neural
networks with nonmonotonic piecewise linear activation functions.
<em>NN</em>, <em>122</em>, 231–238. (<a
href="https://doi.org/10.1016/j.neunet.2019.10.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with multistability and attraction basins of discrete-time neural networks with nonmonotonic piecewise linear activation functions . Under some reasonable conditions, the addressed networks have ( 2 m + 1 ) n (2m+1)n equilibrium points. ( m + 1 ) n (m+1)n of which are locally asymptotically stable, and the others are unstable. The attraction basins of the locally asymptotically stable equilibrium points are given in the form of hyperspherical regions. These results here, which include existence, uniqueness, locally asymptotical stability, instability and attraction basins of the multiple equilibrium points, generalize and improve the earlier publications. Finally, an illustrative example with numerical simulation is given to show the feasibility and the effectiveness of the theoretical results. The theoretical results and illustrative example indicate that the activation functions improve the storage capacity of neural networks significantly.},
  archive      = {J_NN},
  author       = {Peng Wan and Dihua Sun and Min Zhao and Li Wan and Shuang Jin},
  doi          = {10.1016/j.neunet.2019.10.005},
  journal      = {Neural Networks},
  pages        = {231-238},
  shortjournal = {Neural Netw.},
  title        = {Multistability and attraction basins of discrete-time neural networks with nonmonotonic piecewise linear activation functions},
  volume       = {122},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A complementary learning systems approach to temporal
difference learning. <em>NN</em>, <em>122</em>, 218–230. (<a
href="https://doi.org/10.1016/j.neunet.2019.10.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complementary Learning Systems (CLS) theory suggests that the brain uses a ’neocortical’ and a ’hippocampal’ learning system to achieve complex behaviour . These two systems are complementary in that the ’neocortical’ system relies on slow learning of distributed representations while the ’hippocampal’ system relies on fast learning of pattern-separated representations. Both of these systems project to the striatum, which is a key neural structure in the brain’s implementation of Reinforcement Learning (RL). Current deep RL approaches share similarities with a ’neocortical’ system because they slowly learn distributed representations through backpropagation in Deep Neural Networks (DNNs). An ongoing criticism of such approaches is that they are data inefficient and lack flexibility. CLS theory suggests that the addition of a ’hippocampal’ system could address these criticisms. In the present study we propose a novel algorithm known as Complementary Temporal Difference Learning (CTDL), which combines a DNN with a Self-Organizing Map (SOM) to obtain the benefits of both a ’neocortical’ and a ’hippocampal’ system. Key features of CTDL include the use of Temporal Difference (TD) error to update a SOM and the combination of a SOM and DNN to calculate action values. We evaluate CTDL on Grid World, Cart–Pole and Continuous Mountain Car tasks and show several benefits over the classic Deep Q-Network (DQN) approach. These results demonstrate (1) the utility of complementary learning systems for the evaluation of actions, (2) that the TD error signal is a useful form of communication between the two systems and (3) that our approach extends to both discrete and continuous state and action spaces.},
  archive      = {J_NN},
  author       = {Sam Blakeman and Denis Mareschal},
  doi          = {10.1016/j.neunet.2019.10.011},
  journal      = {Neural Networks},
  pages        = {218-230},
  shortjournal = {Neural Netw.},
  title        = {A complementary learning systems approach to temporal difference learning},
  volume       = {122},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the accuracy and computational cost of spiking neuron
implementation. <em>NN</em>, <em>122</em>, 196–217. (<a
href="https://doi.org/10.1016/j.neunet.2019.09.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since more than a decade ago, three statements about spiking neuron (SN) implementations have been widely accepted: 1) Hodgkin and Huxley (HH) model is computationally prohibitive, 2) Izhikevich (IZH) artificial neuron is as efficient as Leaky Integrate-and-Fire (LIF) model, and 3) IZH model is more efficient than HH model ( Izhikevich, 2004 ). As suggested by Hodgkin and Huxley (1952) , their model operates in two modes: by using the α α ’s and β β ’s rate functions directly (HH model) and by storing them into tables (HHT model) for computational cost reduction. Recently, it has been stated that: 1) HHT model (HH using tables) is not prohibitive, 2) IZH model is not efficient, and 3) both HHT and IZH models are comparable in computational cost ( Skocik &amp; Long, 2014 ). That controversy shows that there is no consensus concerning SN simulation capacities. Hence, in this work, we introduce a refined approach, based on the multiobjective optimization theory, describing the SN simulation capacities and ultimately choosing optimal simulation parameters. We have used normalized metrics to define the capacity levels of accuracy, computational cost, and efficiency. Normalized metrics allowed comparisons between SNs at the same level or scale. We conducted tests for balanced, lower, and upper boundary conditions under a regular spiking mode with constant and random current stimuli. We found optimal simulation parameters leading to a balance between computational cost and accuracy. Importantly, and, in general, we found that 1) HH model (without using tables) is the most accurate, computationally inexpensive, and efficient, 2) IZH model is the most expensive and inefficient, 3) both LIF and HHT models are the most inaccurate, 4) HHT model is more expensive and inaccurate than HH model due to α α ’s and β β ’s table discretization , and 5) HHT model is not comparable in computational cost to IZH model. These results refute the theory formulated over a decade ago ( Izhikevich, 2004 ) and go more in-depth in the statements formulated by Skocik and Long (2014) . Our statements imply that the number of dimensions or FLOPS in the SNs are theoretical but not practical indicators of the true computational cost. The metric we propose for the computational cost is more precise than FLOPS and was found to be invariant to computer architecture . Moreover, we found that the firing frequency used in previous works is a necessary but an insufficient metric to evaluate the simulation accuracy. We also show that our results are consistent with the theory of numerical methods and the theory of SN discontinuity. Discontinuous SNs, such LIF and IZH models, introduce a considerable error every time a spike is generated. In addition, compared to the constant input current, the random input current increases the computational cost and inaccuracy. Besides, we found that the search for optimal simulation parameters is problem-specific. That is important because most of the previous works have intended to find a general and unique optimal simulation. Here, we show that this solution could not exist because it is a multiobjective optimization problem that depends on several factors. This work sets up a renewed thesis concerning the SN simulation that is useful to several related research areas, including the emergent Deep Spiking Neural Networks .},
  archive      = {J_NN},
  author       = {Sergio Valadez-Godínez and Humberto Sossa and Raúl Santiago-Montero},
  doi          = {10.1016/j.neunet.2019.09.026},
  journal      = {Neural Networks},
  pages        = {196-217},
  shortjournal = {Neural Netw.},
  title        = {On the accuracy and computational cost of spiking neuron implementation},
  volume       = {122},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The robustness-fidelity trade-off in grow when required
neural networks performing continuous novelty detection. <em>NN</em>,
<em>122</em>, 183–195. (<a
href="https://doi.org/10.1016/j.neunet.2019.10.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Novelty detection allows robots to recognise unexpected data in their sensory field and can thus be utilised in applications such as reconnaissance , surveillance, self-monitoring, etc. We assess the suitability of Grow When Required Neural Networks (GWRNNs) for detecting novel features in a robot’s visual input in the context of randomised physics-based simulation environments. We compare, for the first time, several GWRNN architectures, including new Plastic architectures in which the number of activated input connections for individual neurons is adjusted dynamically as the robot senses a varying number of salient environmental features. The networks are studied in both one-shot and continuous novelty reporting tasks and we demonstrate that there is a trade-off, not unique to this type of novelty detector, between robustness and fidelity. Robustness is achieved through generalisation over the input space which minimises the impact of network parameters on performance, whereas high fidelity results from learning detailed models of the input space and is especially important when a robot encounters multiple novelties consecutively or must detect that previously encountered objects have disappeared from the environment. We propose a number of improvements that could mitigate the robustness-fidelity trade-off and demonstrate one of them, where localisation information is added to the input data stream being monitored.},
  archive      = {J_NN},
  author       = {Lenka Pitonakova and Seth Bullock},
  doi          = {10.1016/j.neunet.2019.10.015},
  journal      = {Neural Networks},
  pages        = {183-195},
  shortjournal = {Neural Netw.},
  title        = {The robustness-fidelity trade-off in grow when required neural networks performing continuous novelty detection},
  volume       = {122},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning cascade attention for fine-grained image
classification. <em>NN</em>, <em>122</em>, 174–182. (<a
href="https://doi.org/10.1016/j.neunet.2019.10.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained image classification is a challenging task due to the large inter-class difference and small intra-class difference. In this paper, we propose a novel Cascade Attention Model using the Deep Convolutional Neural Network to address this problem. Our method first leverages the Spatial Confusion Attention to identify ambiguous areas of the input image. Two constraint loss functions are proposed: the Spatial Mask loss and the Spatial And loss; Second, the Cross-network Attention, applying different pre-train parameters to the two stream architecture. Also, two novel loss functions called Cross-network Similarity loss and Satisfied Rank loss are proposed to make the two-stream networks reinforce each other and get better results. Finally, the Network Fusion Attention merges intermediate results with the novel entropy add strategy to obtain the final predictions. All of these modules can work together and can be trained end to end. Besides, different from previous works, our model is fully weak-supervised and fully paralleled, which leads to easier generalization and faster computation. We obtain the state-of-the-art performance on three challenge benchmark datasets (CUB-200-2011, FGVC-Aircraft and Flower 102) with results of 90.8\%, 92.1\%, and 98.5\%, respectively. The model will be publicly available at https://github.com/billzyx/LCA-CNN.},
  archive      = {J_NN},
  author       = {Youxiang Zhu and Ruochen Li and Yin Yang and Ning Ye},
  doi          = {10.1016/j.neunet.2019.10.009},
  journal      = {Neural Networks},
  pages        = {174-182},
  shortjournal = {Neural Netw.},
  title        = {Learning cascade attention for fine-grained image classification},
  volume       = {122},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A human-in-the-loop deep learning paradigm for synergic
visual evaluation in children. <em>NN</em>, <em>122</em>, 163–173. (<a
href="https://doi.org/10.1016/j.neunet.2019.10.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual development during early childhood is a vital process. Examining the visual acuity of children is essential for early detection of visual abnormalities, but performing visual examination in children is challenging. Here, we developed a human-in-the-loop deep learning (DL) paradigm that combines traditional vision examination and DL with integration of software and hardware, thus facilitating the execution of vision examinations, offsetting the shortcomings of human doctors, and improving the abilities of both DL and doctors to evaluate the vision of children. Because this paradigm contains two rounds (a human round and DL round), doctors can learn from DL and the two can mutually supervise each other such that the precision of the DL system in evaluating the visual acuity of children is improved. Based on DL-based object localization and image identification, the experiences of doctors and the videos captured in the first round, the DL system in the second round can simulate doctors in evaluating the visual acuity of children with a final accuracy of 75.54\%. For comparison, we also assessed an automatic deep learning method that did not consider the experiences of doctors, but its performance was not satisfactory. This entire paradigm can evaluate the visual acuity of children more accurately than humans alone. Furthermore, the paradigm facilitates automatic evaluation of the vision of children with a wearable device .},
  archive      = {J_NN},
  author       = {Kai Zhang and Xiaoyan Li and Lin He and Chong Guo and Yahan Yang and Zhou Dong and Haoqing Yang and Yi Zhu and Chuan Chen and Xiaojing Zhou and Wangting Li and Zhenzhen Liu and Xiaohang Wu and Xiyang Liu and Haotian Lin},
  doi          = {10.1016/j.neunet.2019.10.003},
  journal      = {Neural Networks},
  pages        = {163-173},
  shortjournal = {Neural Netw.},
  title        = {A human-in-the-loop deep learning paradigm for synergic visual evaluation in children},
  volume       = {122},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ℓ1-gain filter design of discrete-time positive neural
networks with mixed delays. <em>NN</em>, <em>122</em>, 152–162. (<a
href="https://doi.org/10.1016/j.neunet.2019.10.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper mainly focuses on the filter design with ℓ 1 ℓ1 -gain disturbance attenuation performance for a class of discrete-time positive neural networks . Discrete and distributed time-varying delays occurring in neuron transmission are taken into account. Especially, the probabilistic distribution of distributed delays is described by a Bernoulli random process in the system model. First, criteria on the positiveness and the unique equilibrium of discrete-time neural networks are presented. Second, through linear Lyapunov method , sufficient conditions for globally asymptotic stability with ℓ 1 ℓ1 -gain disturbance attenuation performance of positive neural networks are proposed. Third, using the results obtained above, criteria on ℓ 1 ℓ1 -gain stability of the established filtering error system are presented, based on which a linear programming (LP) approach is put forward to design the desired positive filter. Finally, two examples of applications to water distribution network and genetic regulatory network are given to demonstrate the effectiveness and applicability of the derived results.},
  archive      = {J_NN},
  author       = {Shunyuan Xiao and Yijun Zhang and Baoyong Zhang},
  doi          = {10.1016/j.neunet.2019.10.004},
  journal      = {Neural Networks},
  pages        = {152-162},
  shortjournal = {Neural Netw.},
  title        = {ℓ1-gain filter design of discrete-time positive neural networks with mixed delays},
  volume       = {122},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A consensus algorithm based on collective neurodynamic
system for distributed optimization with linear and bound constraints.
<em>NN</em>, <em>122</em>, 144–151. (<a
href="https://doi.org/10.1016/j.neunet.2019.10.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an algorithm based on collective neurodynamic system is investigated for distributed constrained convex optimization , whose objective function is a sum of smooth convex functions and non-smooth L 1 L1 -norm functions. Inspired by recent advances in distributed convex optimization , the continuous-time and discrete-time distributed optimization algorithms described by collective neurodynamic systems are proposed. In the systems, each of the smooth objective functions is allocated to each node as well as each of the L 1 L1 -norm functions. However, the L 1 L1 -norm functions are realized by projection operators. Meanwhile, each node satisfies the local linear and bound constraints . Then a connected network is constituted from all the nodes with consensus to find the optimal solutions.},
  archive      = {J_NN},
  author       = {Yan Zhao and Qingshan Liu},
  doi          = {10.1016/j.neunet.2019.10.008},
  journal      = {Neural Networks},
  pages        = {144-151},
  shortjournal = {Neural Netw.},
  title        = {A consensus algorithm based on collective neurodynamic system for distributed optimization with linear and bound constraints},
  volume       = {122},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spiking neural networks applied to the classification of
motor tasks in EEG signals. <em>NN</em>, <em>122</em>, 130–143. (<a
href="https://doi.org/10.1016/j.neunet.2019.09.037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the recent progress of Spiking Neural Network (SNN) models in pattern recognition, we report on the development and evaluation of brain signal classifiers based on SNNs. The work shows the capabilities of this type of Spiking Neurons in the recognition of motor imagery tasks from EEG signals and compares their performance with other traditional classifiers commonly used in this application. This work includes two stages: the first stage consists of comparing the performance of the SNN models against some traditional neural network models. The second stage, compares the SNN models performance in two input conditions: input features with constant values and input features with temporal information. The EEG signals employed in this work represent five motor imagery tasks: i.e. rest, left hand, right hand, foot and tongue movements. These EEG signals were obtained from a public database provided by the Technological University of Graz (Brunner et al., 2008). The feature extraction stage was performed by applying two algorithms: power spectral density and wavelet decomposition . Likewise, this work uses raw EEG signals for the second stage of the problem solution. All of the models were evaluated in the classification between two motor imagery tasks. This work demonstrates that with a smaller number of Spiking neurons, simple problems can be solved. Better results are obtained by using patterns with temporal information, thereby exploiting the capabilities of the SNNs.},
  archive      = {J_NN},
  author       = {Carlos D. Virgilio G. and Juan H. Sossa A. and Javier M. Antelis and Luis E. Falcón},
  doi          = {10.1016/j.neunet.2019.09.037},
  journal      = {Neural Networks},
  pages        = {130-143},
  shortjournal = {Neural Netw.},
  title        = {Spiking neural networks applied to the classification of motor tasks in EEG signals},
  volume       = {122},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Regularized correntropy criterion based semi-supervised ELM.
<em>NN</em>, <em>122</em>, 117–129. (<a
href="https://doi.org/10.1016/j.neunet.2019.09.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Along with the explosive growing of data, semi-supervised learning attracts increasing attention in the past years due to its powerful capability in labeling unlabeled data and knowledge mining. As an emerging method, the semi-supervised extreme learning machine (SSELM), that builds on ELM, has been developed for data classification and shown superiorities in learning efficiency and accuracy. However, the optimization of SSELM as well as most of the other ELMs is generally based on the mean square error (MSE) criterion, which has been shown less effective in dealing with non-Gaussian noises. In this paper, a robust regularized correntropy criterion based SSELM (RC-SSELM) is developed. The optimization of the output weight matrix of RC-SSELM is derived by the fixed-point iteration based approach. A convergent analysis of the proposed RC-SSELM is presented based on the half-quadratic optimization technique. Experimental results on 4 synthetic datasets and 13 benchmark UCI datasets are provided to show the superiorities of the proposed RC-SSELM over SSELM and other state-of-the-art methods.},
  archive      = {J_NN},
  author       = {Jie Yang and Jiuwen Cao and Tianlei Wang and Anke Xue and Badong Chen},
  doi          = {10.1016/j.neunet.2019.09.030},
  journal      = {Neural Networks},
  pages        = {117-129},
  shortjournal = {Neural Netw.},
  title        = {Regularized correntropy criterion based semi-supervised ELM},
  volume       = {122},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Effects of infinite occurrence of hybrid impulses with
quasi-synchronization of parameter mismatched neural networks.
<em>NN</em>, <em>122</em>, 106–116. (<a
href="https://doi.org/10.1016/j.neunet.2019.10.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article is deeply concerned with the effects of hybrid impulses on quasi-synchronization of neural networks with mixed time-varying delays and parameter mismatches. Hybrid impulses allow synchronizing as well as desynchronizing impulses in one impulsive sequence, so their infinite time occurrence with the system may destroy the synchronization process . Therefore, the effective hybrid impulsive controller has been designed to deal with the difficulties in achieving the quasi-synchronization under the effects of hybrid impulses, which occur all the time, but their density of occurrence gradually decrease. In addition, the new concepts of average impulsive interval and average impulsive gain have been applied to cope with the simultaneous existence of synchronizing and desynchronizing impulses. Based on the Lyapunov method together with the extended comparison principle and the formula of variation of parameters for mixed time-varying delayed impulsive system, the delay-dependent sufficient criteria of quasi-synchronization have been derived for two separate cases, viz., T a Ta&amp;lt;∞ and T a = ∞ Ta=∞ . Finally, the efficiency of the theoretical results has been illustrated by providing two numerical examples.},
  archive      = {J_NN},
  author       = {Rakesh Kumar and Subir Das and Yang Cao},
  doi          = {10.1016/j.neunet.2019.10.007},
  journal      = {Neural Networks},
  pages        = {106-116},
  shortjournal = {Neural Netw.},
  title        = {Effects of infinite occurrence of hybrid impulses with quasi-synchronization of parameter mismatched neural networks},
  volume       = {122},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new learning paradigm for random vector functional-link
network: RVFL+. <em>NN</em>, <em>122</em>, 94–105. (<a
href="https://doi.org/10.1016/j.neunet.2019.09.039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In school, a teacher plays an important role in various classroom teaching patterns. Likewise to this human learning activity , the learning using privileged information (LUPI) paradigm provides additional information generated by the teacher to ’teach’ learning models during the training stage. Therefore, this novel learning paradigm is a typical Teacher–Student Interaction mechanism. This paper is the first to present a random vector functional link (RVFL) network based on the LUPI paradigm, called RVFL+. The novel RVFL+ incorporates the LUPI paradigm that can leverage additional source of information into the RVFL, which offers an alternative way to train the RVFL. Rather than simply combining two existing approaches, the newly-derived RVFL+ fills the gap between classical randomized neural networks and the newfashioned LUPI paradigm. Moreover, the proposed RVFL+ can perform in conjunction with the kernel trick for highly complicated nonlinear feature learning, termed KRVFL+. Furthermore, the statistical property of the proposed RVFL+ is investigated, and the authors present a sharp and high-quality generalization error bound based on the Rademacher complexity. Competitive experimental results on 14 real-world datasets illustrate the great effectiveness and efficiency of the novel RVFL+ and KRVFL+, which can achieve better generalization performance than state-of-the-art methods.},
  archive      = {J_NN},
  author       = {Peng-Bo Zhang and Zhi-Xin Yang},
  doi          = {10.1016/j.neunet.2019.09.039},
  journal      = {Neural Networks},
  pages        = {94-105},
  shortjournal = {Neural Netw.},
  title        = {A new learning paradigm for random vector functional-link network: RVFL+},
  volume       = {122},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Constructing multilayered neural networks with sparse,
data-driven connectivity using biologically-inspired, complementary,
homeostatic mechanisms. <em>NN</em>, <em>122</em>, 68–93. (<a
href="https://doi.org/10.1016/j.neunet.2019.09.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The immense complexity of the brain requires that it be built and controlled by intrinsic, self-regulating mechanisms. One such mechanism, the formation of new connections via synaptogenesis , plays a central role in neuronal connectivity and, ultimately, performance. Adaptive synaptogenesis networks combine synaptogenesis, associative synaptic modification, and synaptic shedding to construct sparse networks. Here, inspired by neuroscientific observations, novel aspects of brain development are incorporated into adaptive synaptogenesis. The extensions include: (i) multiple layers, (ii) neuron survival and death based on information transmission, and (iii) bigrade growth factor signaling to control the onset of synaptogenesis in succeeding layers and to control neuron survival and death in preceding layers. Also guiding this research is the assumption that brains must achieve a compromise between good performance and low energy expenditures. Simulations of the network model demonstrate the parametric and functional control of both performance and energy expenditures, where performance is measured in terms of information loss and classification errors , and energy expenditures are assumed to be a monotonically increasing function of the number of neurons. Major insights from this study include (a) the key role a neural layer between two other layers has in controlling synaptogenesis and neuron elimination, (b) the performance and energy-savings benefits of delaying the onset of synaptogenesis in a succeeding layer, and (c) how the elimination of neurons in a preceding layer provides energy savings , code compression , and can be accomplished without significantly degrading information transfer or classification performance.},
  archive      = {J_NN},
  author       = {Robert A. Baxter and William B Levy},
  doi          = {10.1016/j.neunet.2019.09.025},
  journal      = {Neural Networks},
  pages        = {68-93},
  shortjournal = {Neural Netw.},
  title        = {Constructing multilayered neural networks with sparse, data-driven connectivity using biologically-inspired, complementary, homeostatic mechanisms},
  volume       = {122},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A broad class of discrete-time hypercomplex-valued hopfield
neural networks. <em>NN</em>, <em>122</em>, 54–67. (<a
href="https://doi.org/10.1016/j.neunet.2019.09.040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the stability of a broad class of discrete-time hypercomplex-valued Hopfield-type neural networks . To ensure the neural networks belonging to this class always settle down at a stationary state, we introduce novel hypercomplex number systems referred to as real-part associative hypercomplex number systems. Real-part associative hypercomplex number systems generalize the well-known Cayley–Dickson algebras and real Clifford algebras and include the systems of real numbers, complex numbers, dual numbers, hyperbolic numbers, quaternions, tessarines, and octonions as particular instances. Apart from the novel hypercomplex number systems, we introduce a family of hypercomplex-valued activation functions called B B -projection functions. Broadly speaking, a B B -projection function projects the activation potential onto the set of all possible states of a hypercomplex-valued neuron. Using the theory presented in this paper, we confirm the stability analysis of several discrete-time hypercomplex-valued Hopfield-type neural networks from the literature. Moreover, we introduce and provide the stability analysis of a general class of Hopfield-type neural networks on Cayley–Dickson algebras.},
  archive      = {J_NN},
  author       = {Fidelis Zanetti de Castro and Marcos Eduardo Valle},
  doi          = {10.1016/j.neunet.2019.09.040},
  journal      = {Neural Networks},
  pages        = {54-67},
  shortjournal = {Neural Netw.},
  title        = {A broad class of discrete-time hypercomplex-valued hopfield neural networks},
  volume       = {122},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A smoothing neural network for minimization l1-lp in sparse
signal reconstruction with measurement noises. <em>NN</em>,
<em>122</em>, 40–53. (<a
href="https://doi.org/10.1016/j.neunet.2019.10.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates a smoothing neural network (SNN) to solve a robust sparse signal reconstruction in compressed sensing (CS), where the objective function is nonsmooth l 1 l1 -norm and the feasible set satisfies an inequality of l p lp -norm 2 &amp;#x2265; p &amp;#x2265; 1 2&amp;#x26;#x2265;p&amp;#x26;#x2265;1 which is used for measuring residual errors . With a smoothing approximate technique, the non-smooth and non-Lipschitz continuous issues of the l 1 l1 -norm and the gradient of l p lp -norm can be solved efficiently. We propose a SNN which is modeled by a differential equation and give its circuit implementation. In this case, we prove the proposed SNN converges to the optimal of considered problem. Simulation results are discussed to demonstrate the efficiency of the proposed algorithm.},
  archive      = {J_NN},
  author       = {You Zhao and Xing He and Tingwen Huang and Junjian Huang and Peng Li},
  doi          = {10.1016/j.neunet.2019.10.006},
  journal      = {Neural Networks},
  pages        = {40-53},
  shortjournal = {Neural Netw.},
  title        = {A smoothing neural network for minimization l1-lp in sparse signal reconstruction with measurement noises},
  volume       = {122},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint ranking SVM and binary relevance with robust low-rank
learning for multi-label classification. <em>NN</em>, <em>122</em>,
24–39. (<a href="https://doi.org/10.1016/j.neunet.2019.10.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label classification studies the task where each example belongs to multiple labels simultaneously. As a representative method, Ranking Support Vector Machine (Rank-SVM) aims to minimize the Ranking Loss and can also mitigate the negative influence of the class-imbalance issue. However, due to its stacking-style way for thresholding , it may suffer error accumulation and thus reduces the final classification performance. Binary Relevance (BR) is another typical method, which aims to minimize the Hamming Loss and only needs one-step learning. Nevertheless, it might have the class-imbalance issue and does not take into account label correlations. To address the above issues, we propose a novel multi-label classification model , which joints Ranking support vector machine and Binary Relevance with robust Low-rank learning (RBRL). RBRL inherits the ranking loss minimization advantages of Rank-SVM, and thus overcomes the disadvantages of BR suffering the class-imbalance issue and ignoring the label correlations. Meanwhile, it utilizes the hamming loss minimization and one-step learning advantages of BR, and thus tackles the disadvantages of Rank-SVM including another thresholding learning step. Besides, a low-rank constraint is utilized to further exploit high-order label correlations under the assumption of low dimensional label space. Furthermore, to achieve nonlinear multi-label classifiers, we derive the kernelization RBRL. Two accelerated proximal gradient methods (APG) are used to solve the optimization problems efficiently. Extensive comparative experiments with several state-of-the-art methods illustrate a highly competitive or superior performance of our method RBRL.},
  archive      = {J_NN},
  author       = {Guoqiang Wu and Ruobing Zheng and Yingjie Tian and Dalian Liu},
  doi          = {10.1016/j.neunet.2019.10.002},
  journal      = {Neural Networks},
  pages        = {24-39},
  shortjournal = {Neural Netw.},
  title        = {Joint ranking SVM and binary relevance with robust low-rank learning for multi-label classification},
  volume       = {122},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-label zero-shot human action recognition via joint
latent ranking embedding. <em>NN</em>, <em>122</em>, 1–23. (<a
href="https://doi.org/10.1016/j.neunet.2019.09.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action recognition is one of the most challenging tasks in computer vision . Most of the existing works in human action recognition are limited to single-label classification. A real-world video stream, however, often contains multiple human actions. Such a video stream is usually annotated collectively with a set of relevant human action labels, which leads to a multi-label learning problem. Furthermore, there are a great number of meaningful human actions in reality but it would be extremely difficult, if not impossible, to collect/annotate sufficient video clips regarding all these human actions for training a supervised learning model. In this paper, we formulate a real-world human action recognition task as a multi-label zero-shot learning problem. To address this problem, a joint latent ranking embedding framework is proposed. Our framework holistically tackles the issue of unknown temporal boundaries between different actions within a video clip for multi-label learning and exploits the side information regarding the semantic relationship between different human actions for zero-shot learning. Specifically, our framework consists of two component neural networks for visual and semantic embedding respectively. Thus, multi-label zero-shot recognition is done by measuring relatedness scores of concerned action labels to a test video clip in the joint latent visual and semantic embedding spaces. We evaluate our framework in different settings, including a novel data split scheme designed especially for evaluating multi-label zero-shot learning. The experimental results on two weakly annotated multi-label human action datasets (i.e. Breakfast and Charades ) demonstrate the effectiveness of our framework.},
  archive      = {J_NN},
  author       = {Qian Wang and Ke Chen},
  doi          = {10.1016/j.neunet.2019.09.029},
  journal      = {Neural Networks},
  pages        = {1-23},
  shortjournal = {Neural Netw.},
  title        = {Multi-label zero-shot human action recognition via joint latent ranking embedding},
  volume       = {122},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Growing to meet the publishing demands of the deep learning
era. <em>NN</em>, <em>121</em>, xv–xvi. (<a
href="https://doi.org/10.1016/S0893-6080(19)30372-7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(19)30372-7},
  journal      = {Neural Networks},
  pages        = {xv-xvi},
  shortjournal = {Neural Netw.},
  title        = {Growing to meet the publishing demands of the deep learning era},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural networks referees in 2019. <em>NN</em>, <em>121</em>,
x–xiv. (<a href="https://doi.org/10.1016/S0893-6080(19)30380-6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(19)30380-6},
  journal      = {Neural Networks},
  pages        = {x-xiv},
  shortjournal = {Neural Netw.},
  title        = {Neural networks referees in 2019},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020l). INN/ENNS/JNNS - membership applic. form. <em>NN</em>,
<em>121</em>, III. (<a
href="https://doi.org/10.1016/S0893-6080(19)30382-X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(19)30382-X},
  journal      = {Neural Networks},
  pages        = {III},
  shortjournal = {Neural Netw.},
  title        = {INN/ENNS/JNNS - membership applic. form},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020l). Current events. <em>NN</em>, <em>121</em>, I. (<a
href="https://doi.org/10.1016/S0893-6080(19)30381-8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_NN},
  doi          = {10.1016/S0893-6080(19)30381-8},
  journal      = {Neural Networks},
  pages        = {I},
  shortjournal = {Neural Netw.},
  title        = {Current events},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep CovDenseSNN: A hierarchical event-driven dynamic
framework with spiking neurons in noisy environment. <em>NN</em>,
<em>121</em>, 512–519. (<a
href="https://doi.org/10.1016/j.neunet.2019.08.034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurons in the brain use an event signal, termed spike, encode temporal information for neural computation. Spiking neural networks (SNNs) take this advantage to serve as biological relevant models. However, the effective encoding of sensory information and also its integration with downstream neurons of SNNs are limited by the current shallow structures and learning algorithms. To tackle this limitation, this paper proposes a novel hybrid framework combining the feature learning ability of continuous-valued convolutional neural networks (CNNs) and SNNs, named deep CovDenseSNN, such that SNNs can make use of feature extraction ability of CNNs during the encoding stage, but still process features with unsupervised learning rule of spiking neurons . We evaluate them on MNIST and its variations to show that our model can extract and transmit more important information than existing models, especially for anti-noise ability in the noisy environment . The proposed architecture provides efficient ways to perform feature representation and recognition in a consistent temporal learning framework, which is easily adapted to neuromorphic hardware implementations and bring more biological realism into modern image classification models, with the hope that the proposed framework can inform us how sensory information is transmitted and represented in the brain.},
  archive      = {J_NN},
  author       = {Qi Xu and Jianxin Peng and Jiangrong Shen and Huajin Tang and Gang Pan},
  doi          = {10.1016/j.neunet.2019.08.034},
  journal      = {Neural Networks},
  pages        = {512-519},
  shortjournal = {Neural Netw.},
  title        = {Deep CovDenseSNN: A hierarchical event-driven dynamic framework with spiking neurons in noisy environment},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic behaviors of the FitzHugh–nagumo neuron model with
state-dependent impulsive effects. <em>NN</em>, <em>121</em>, 497–511.
(<a href="https://doi.org/10.1016/j.neunet.2019.09.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In present work, in order to reproduce spiking and bursting behavior of real neurons , a new hybrid biological neuron model is established and analyzed by combining the FitzHugh–Nagumo (FHN) neuron model, the threshold for spike initiation and the state-dependent impulsive effects (impulse resetting process). Firstly, we construct Poincaré mappings under different conditions by means of geometric analysis, and then obtain some sufficient criteria for the existence and stability of order-1 or order-2 periodic solution to the impulsive neuron model by finding the fixed point of Poincaré mapping and some geometric analysis techniques. Numerical simulations are given to illustrate and verify our theoretical results. The bifurcation diagrams are presented to describe the phenomena of period-doubling route to chaos, which implies that the dynamic behavior of the neuron model become more complex due to impulsive effects. Furthermore, the correctness and effectiveness of the proposed FitzHugh–Nagumo neuron model with state-dependent impulsive effects are verified by circuit simulation. Finally, the conclusions of this paper are analyzed and summarized, and the effects of random factors on the electrophysiological activities of neuron are discussed by numerical simulation.},
  archive      = {J_NN},
  author       = {Zhilong He and Chuandong Li and Ling Chen and Zhengran Cao},
  doi          = {10.1016/j.neunet.2019.09.031},
  journal      = {Neural Networks},
  pages        = {497-511},
  shortjournal = {Neural Netw.},
  title        = {Dynamic behaviors of the FitzHugh–Nagumo neuron model with state-dependent impulsive effects},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Privacy-enhanced multi-party deep learning. <em>NN</em>,
<em>121</em>, 484–496. (<a
href="https://doi.org/10.1016/j.neunet.2019.10.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-party deep learning , multiple participants jointly train a deep learning model through a central server to achieve common objectives without sharing their private data. Recently, a significant amount of progress has been made toward the privacy issue of this emerging multi-party deep learning paradigm. In this paper, we mainly focus on two problems in multi-party deep learning. The first problem is that most of the existing works are incapable of defending simultaneously against the attacks of honest-but-curious participants and an honest-but-curious server without a manager trusted by all participants. To tackle this problem, we design a privacy-enhanced multi-party deep learning framework, which integrates differential privacy and homomorphic encryption to prevent potential privacy leakage to other participants and a central server without requiring a manager that all participants trust. The other problem is that existing frameworks consume high total privacy budget when applying differential privacy for preserving privacy, which leads to a high risk of privacy leakage . In order to alleviate this problem, we propose three strategies for dynamically allocating privacy budget at each epoch to further enhance privacy guarantees without compromising the model utility. Moreover, it provides participants with an intuitive handle to strike a balance between the privacy level and the training efficiency by choosing different strategies. Both analytical and experimental evaluations demonstrate the promising performance of our proposed framework.},
  archive      = {J_NN},
  author       = {Maoguo Gong and Jialun Feng and Yu Xie},
  doi          = {10.1016/j.neunet.2019.10.001},
  journal      = {Neural Networks},
  pages        = {484-496},
  shortjournal = {Neural Netw.},
  title        = {Privacy-enhanced multi-party deep learning},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive fault-tolerant consensus for a class of
leader-following systems using neural network learning strategy.
<em>NN</em>, <em>121</em>, 474–483. (<a
href="https://doi.org/10.1016/j.neunet.2019.09.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the leader-following consensus problem of a class of nonlinearly multi-dimensional multi-agent systems with actuator faults is addressed by developing a novel neural network learning strategy. In order to achieve the desirable consensus results, a neural network learning algorithm composed of adaptive technique is proposed to on-line approximate the unknown nonlinear functions and estimate the unknown bounds of actuator faults . Then, on the basis of the approximations and estimations, a robust adaptive distributed fault-tolerant consensus control scheme is investigated so that the bounded results of all signals of the resulting closed-loop leader-following system can be achieved by using Lyapunov stability theorem. Finally, efficiency of the proposed adaptive neural network learning strategy-based consensus control strategies is demonstrated by a coupled nonlinear forced pendulums system .},
  archive      = {J_NN},
  author       = {Xiaozheng Jin and Xianfeng Zhao and Jiguo Yu and Xiaoming Wu and Jing Chi},
  doi          = {10.1016/j.neunet.2019.09.028},
  journal      = {Neural Networks},
  pages        = {474-483},
  shortjournal = {Neural Netw.},
  title        = {Adaptive fault-tolerant consensus for a class of leader-following systems using neural network learning strategy},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image denoising using deep CNN with batch renormalization.
<em>NN</em>, <em>121</em>, 461–473. (<a
href="https://doi.org/10.1016/j.neunet.2019.08.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep convolutional neural networks (CNNs) have attracted great attention in the field of image denoising . However, there are two drawbacks: (1) it is very difficult to train a deeper CNN for denoising tasks, and (2) most of deeper CNNs suffer from performance saturation. In this paper, we report the design of a novel network called a batch-renormalization denoising network (BRDNet). Specifically, we combine two networks to increase the width of the network, and thus obtain more features. Because batch renormalization is fused into BRDNet, we can address the internal covariate shift and small mini-batch problems. Residual learning is also adopted in a holistic way to facilitate the network training. Dilated convolutions are exploited to extract more information for denoising tasks. Extensive experimental results show that BRDNet outperforms state-of-the-art image-denoising methods. The code of BRDNet is accessible at http://www.yongxu.org/lunwen.html .},
  archive      = {J_NN},
  author       = {Chunwei Tian and Yong Xu and Wangmeng Zuo},
  doi          = {10.1016/j.neunet.2019.08.022},
  journal      = {Neural Networks},
  pages        = {461-473},
  shortjournal = {Neural Netw.},
  title        = {Image denoising using deep CNN with batch renormalization},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Synchronization in an array of coupled neural networks with
delayed impulses: Average impulsive delay method. <em>NN</em>,
<em>121</em>, 452–460. (<a
href="https://doi.org/10.1016/j.neunet.2019.09.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the paper, synchronization of coupled neural networks with delayed impulses is investigated. In order to overcome the difficulty that time delays can be flexible and even larger than impulsive interval, we propose a new method of average impulsive delay (AID). By the methods of average impulsive interval (AII) and AID, some sufficient synchronization criteria for coupled neural networks with delayed impulses are obtained. We prove that the time delay in impulses can play double roles, namely, it may desynchronize a synchronous network or synchronize a nonsynchronized network. Moreover, a unified relationship is established among AII, AID and rate coefficients of the impulsive dynamical network such that the network is globally exponentially synchronized (GES). Further, we discuss the case that time delays in impulses may be unbounded, which has not been considered in existing results. Finally, two examples are presented to demonstrate the validity of the derived results.},
  archive      = {J_NN},
  author       = {Bangxin Jiang and Jianquan Lu and Jungang Lou and Jianlong Qiu},
  doi          = {10.1016/j.neunet.2019.09.019},
  journal      = {Neural Networks},
  pages        = {452-460},
  shortjournal = {Neural Netw.},
  title        = {Synchronization in an array of coupled neural networks with delayed impulses: Average impulsive delay method},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Integrating joint feature selection into subspace learning:
A formulation of 2DPCA for outliers robust feature selection.
<em>NN</em>, <em>121</em>, 441–451. (<a
href="https://doi.org/10.1016/j.neunet.2019.08.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the principal component analysis and its variants are sensitive to outliers that affect their performance and applicability in real world, several variants have been proposed to improve the robustness. However, most of the existing methods are still sensitive to outliers and are unable to select useful features. To overcome the issue of sensitivity of PCA against outliers, in this paper, we introduce two-dimensional outliers-robust principal component analysis (ORPCA) by imposing the joint constraints on the objective function. ORPCA relaxes the orthogonal constraints and penalizes the regression coefficient , thus, it selects important features and ignores the same features that exist in other principal components. It is commonly known that square Frobenius norm is sensitive to outliers. To overcome this issue, we have devised an alternative way to derive objective function. Experimental results on four publicly available benchmark datasets show the effectiveness of joint feature selection and provide better performance as compared to state-of-the-art dimensionality-reduction methods.},
  archive      = {J_NN},
  author       = {Imran Razzak and Raghib Abu Saris and Michael Blumenstein and Guandong Xu},
  doi          = {10.1016/j.neunet.2019.08.030},
  journal      = {Neural Networks},
  pages        = {441-451},
  shortjournal = {Neural Netw.},
  title        = {Integrating joint feature selection into subspace learning: A formulation of 2DPCA for outliers robust feature selection},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A sparse deep belief network with efficient fuzzy learning
framework. <em>NN</em>, <em>121</em>, 430–440. (<a
href="https://doi.org/10.1016/j.neunet.2019.09.035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep belief network (DBN) is one of the most feasible ways to realize deep learning (DL) technique, and it has been attracting more and more attentions in nonlinear system modeling. However, DBN cannot provide satisfactory results in learning speed, modeling accuracy and robustness, which is mainly caused by dense representation and gradient diffusion. To address these problems and promote DBN’s development in cross-models, we propose a Sparse Deep Belief Network with Fuzzy Neural Network (SDBFNN) for nonlinear system modeling. In this novel framework, the sparse DBN is considered as a pre-training technique to realize fast weight-initialization and to obtain feature vectors. It can balance the dense representation to improve its robustness. A fuzzy neural network is developed for supervised modeling so as to eliminate the gradient diffusion. Its input happens to be the obtained feature vector. As a novel cross-model, SDBFNN combines the advantages of both pre-training technique and fuzzy neural network to improve modeling capability. Its convergence is also analyzed as well. A benchmark problem and a practical problem in wastewater treatment are conducted to demonstrate the superiority of SDBFNN. The extensive experimental results show that SDBFNN achieves better performance than the existing methods in learning speed, modeling accuracy and robustness.},
  archive      = {J_NN},
  author       = {Gongming Wang and Qing-Shan Jia and Junfei Qiao and Jing Bi and Caixia Liu},
  doi          = {10.1016/j.neunet.2019.09.035},
  journal      = {Neural Networks},
  pages        = {430-440},
  shortjournal = {Neural Netw.},
  title        = {A sparse deep belief network with efficient fuzzy learning framework},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distinct role of flexible and stable encodings in sequential
working memory. <em>NN</em>, <em>121</em>, 419–429. (<a
href="https://doi.org/10.1016/j.neunet.2019.09.034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The serial-position effect in working memory is considered important for studying how a sequence of sensory information can be retained and manipulated simultaneously in neural memory circuits. Here, via a precise analysis of the primacy and recency effects in human psychophysical experiments, we propose that stable and flexible codings take distinct roles of retaining and updating information in working memory, and that their combination induces serial-position effects spontaneously. We found that stable encoding retains memory to induce the primacy effect, while flexible encoding used for learning new inputs induces the recency effect. A model simulation based on human data, confirmed that a neural network with both flexible and stable synapses could reproduce the major characteristics of serial-position effects. Our new prediction, that the control of resource allocation by flexible–stable coding balance can modulate memory performance in sequence-specific manner, was supported by pre-cued memory performance data in humans.},
  archive      = {J_NN},
  author       = {Hyeonsu Lee and Woochul Choi and Youngjin Park and Se-Bum Paik},
  doi          = {10.1016/j.neunet.2019.09.034},
  journal      = {Neural Networks},
  pages        = {419-429},
  shortjournal = {Neural Netw.},
  title        = {Distinct role of flexible and stable encodings in sequential working memory},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive latent similarity learning for multi-view
clustering. <em>NN</em>, <em>121</em>, 409–418. (<a
href="https://doi.org/10.1016/j.neunet.2019.09.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing clustering methods employ the original multi-view data as input to learn the similarity matrix which characterizes the underlying cluster structure shared by multiple views. This reduces the flexibility of multi-view clustering methods due to the fact that multi-view data usually contains noise or the variation between multi-view data points, which should belong to the same cluster, is larger than the variation between data points belonging to different clusters. To address these problems, we propose a novel multi-view clustering model, namely adaptive latent similarity learning (ALSL) for multi-view clustering. ALSL employs the adaptively learned graph, which characterizes the relationship between clusters, as the new input to learn the latent data representation and integrates the latent similarity representation learning , manifold learning and spectral clustering into a unified framework. With the complementarity of multiple views, the latent similarity representation characterizes the underlying cluster structure shared by multiple views. Our model is intuitive and can be optimized efficiently by using the Augmented Lagrangian Multiplier with Alternating Direction Minimization (ALM-ADM) algorithm. Extensive experiments on benchmark datasets have demonstrated the superiority of the proposed method.},
  archive      = {J_NN},
  author       = {Deyan Xie and Quanxue Gao and Qianqian Wang and Xiangdong Zhang and Xinbo Gao},
  doi          = {10.1016/j.neunet.2019.09.013},
  journal      = {Neural Networks},
  pages        = {409-418},
  shortjournal = {Neural Netw.},
  title        = {Adaptive latent similarity learning for multi-view clustering},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Using a deep convolutional neural network to predict 2017
ozone concentrations, 24 hours in advance. <em>NN</em>, <em>121</em>,
396–408. (<a
href="https://doi.org/10.1016/j.neunet.2019.09.033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we use a deep convolutional neural network (CNN) to develop a model that predicts ozone concentrations 24 h in advance. We have evaluated the model for 21 continuous ambient monitoring stations (CAMS) across Texas. The inputs for the CNN model consist of meteorology (e.g., wind field, temperature) and air pollution concentrations (NO x x and ozone) from the previous day. The model is trained for predicting next-day, 24-hour ozone concentrations. We acquired meteorological and air pollution data from 2014 to 2017 from the Texas Commission on Environmental Quality (TCEQ). For 19 of the 21 stations in the study, results show that the yearly index of agreement (IOA) is above 0.85, confirming the acceptable accuracy of the CNN model. The results also show the model performed well, even for stations with varying monthly trends of ozone concentrations (specifically CAMS-012, located in El-Paso, and CAMS-013, located in Fort Worth, both with IOA=0.89). In addition, to ensure that the model was robust, we tested it on stations where fewer meteorological variables are monitored. Although these stations have fewer input features, their performance is similar to that of other stations. However, despite its success at capturing daily trends, the model mostly underpredicts the daily maximum ozone, which provides a direction for future study and improvement. As this model predicts ozone concentrations 24 h in advance with greater accuracy and computationally fewer resources, it can serve as an early warning system for individuals susceptible to ozone and those engaging in outdoor activities.},
  archive      = {J_NN},
  author       = {Alqamah Sayeed and Yunsoo Choi and Ebrahim Eslami and Yannic Lops and Anirban Roy and Jia Jung},
  doi          = {10.1016/j.neunet.2019.09.033},
  journal      = {Neural Networks},
  pages        = {396-408},
  shortjournal = {Neural Netw.},
  title        = {Using a deep convolutional neural network to predict 2017 ozone concentrations, 24 hours in advance},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A biologically plausible supervised learning method for
spiking neural networks using the symmetric STDP rule. <em>NN</em>,
<em>121</em>, 387–395. (<a
href="https://doi.org/10.1016/j.neunet.2019.09.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking neural networks (SNNs) possess energy-efficient potential due to event-based computation. However, supervised training of SNNs remains a challenge as spike activities are non-differentiable. Previous SNNs training methods can be generally categorized into two basic classes, i.e., backpropagation-like training methods and plasticity-based learning methods. The former methods are dependent on energy-inefficient real-valued computation and non-local transmission, as also required in artificial neural networks (ANNs), whereas the latter are either considered to be biologically implausible or exhibit poor performance. Hence, biologically plausible (bio-plausible) high-performance supervised learning (SL) methods for SNNs remain deficient. In this paper, we proposed a novel bio-plausible SNN model for SL based on the symmetric spike-timing dependent plasticity (sym-STDP) rule found in neuroscience . By combining the sym-STDP rule with bio-plausible synaptic scaling and intrinsic plasticity of the dynamic threshold, our SNN model implemented SL well and achieved good performance in the benchmark recognition task (MNIST dataset). To reveal the underlying mechanism of our SL model, we visualized both layer-based activities and synaptic weights using the t-distributed stochastic neighbor embedding (t-SNE) method after training and found that they were well clustered, thereby demonstrating excellent classification ability. Furthermore, to verify the robustness of our model, we trained it on another more realistic dataset (Fashion-MNIST), which also showed good performance. As the learning rules were bio-plausible and based purely on local spike events, our model could be easily applied to neuromorphic hardware for online training and may be helpful for understanding SL information processing at the synaptic level in biological neural systems.},
  archive      = {J_NN},
  author       = {Yunzhe Hao and Xuhui Huang and Meng Dong and Bo Xu},
  doi          = {10.1016/j.neunet.2019.09.007},
  journal      = {Neural Networks},
  pages        = {387-395},
  shortjournal = {Neural Netw.},
  title        = {A biologically plausible supervised learning method for spiking neural networks using the symmetric STDP rule},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design space exploration of hardware spiking neurons for
embedded artificial intelligence. <em>NN</em>, <em>121</em>, 366–386.
(<a href="https://doi.org/10.1016/j.neunet.2019.09.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning is yielding unprecedented interest in research and industry, due to recent success in many applied contexts such as image classification and object recognition. However, the deployment of these systems requires huge computing capabilities, thus making them unsuitable for embedded systems . To deal with this limitation, many researchers are investigating brain-inspired computing, which would be a perfect alternative to the conventional Von Neumann architecture based computers (CPU/GPU) that meet the requirements for computing performance, but not for energy-efficiency. Therefore, neuromorphic hardware circuits that are adaptable for both parallel and distributed computations need to be designed. In this paper, we focus on Spiking Neural Networks (SNNs) with a comprehensive study of neural coding methods and hardware exploration. In this context, we propose a framework for neuromorphic hardware design space exploration , which allows to define a suitable architecture based on application-specific constraints and starting from a wide variety of possible architectural choices . For this framework, we have developed a behavioral level simulator for neuromorphic hardware architectural exploration named NAXT. Moreover, we propose modified versions of the standard Rate Coding technique to make trade-offs with the Time Coding paradigm, which is characterized by the low number of spikes propagating in the network. Thus, we are able to reduce the number of spikes while keeping the same neuron’s model, which results in an SNN with fewer events to process. By doing so, we seek to reduce the amount of power consumed by the hardware. Furthermore, we present three neuromorphic hardware architectures in order to quantitatively study the implementation of SNNs. One of these architectures integrates a novel hybrid structure: a highly-parallel computation core for most solicited layers, and time-multiplexed computation units for deeper layers. These architectures are derived from a novel funnel-like Design Space Exploration framework for neuromorphic hardware.},
  archive      = {J_NN},
  author       = {Nassim Abderrahmane and Edgar Lemaire and Benoît Miramond},
  doi          = {10.1016/j.neunet.2019.09.024},
  journal      = {Neural Networks},
  pages        = {366-386},
  shortjournal = {Neural Netw.},
  title        = {Design space exploration of hardware spiking neurons for embedded artificial intelligence},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Finite-time resilient h∞ state estimation for discrete-time
delayed neural networks under dynamic event-triggered mechanism.
<em>NN</em>, <em>121</em>, 356–365. (<a
href="https://doi.org/10.1016/j.neunet.2019.09.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the finite-time resilient H ∞ H∞ state estimation problem is investigated for a class of discrete-time delayed neural networks . For the sake of energy saving , a dynamic event-triggered mechanism is employed in the design of state estimator for the discrete-time delayed neural networks . In order to handle the possible fluctuation of the estimator gain parameters when the state estimator is implemented, a resilient state estimator is adopted. By constructing a Lyapunov–Krasovskii functional, a sufficient condition is established, which guarantees that the estimation error system is bounded and the H ∞ H∞ performance requirement is satisfied within the finite time. Then, the desired estimator gains are obtained via solving a set of linear matrix inequalities. Finally, a numerical example is employed to illustrate the usefulness of the proposed state estimation scheme.},
  archive      = {J_NN},
  author       = {Yufei Liu and Bo Shen and Huisheng Shu},
  doi          = {10.1016/j.neunet.2019.09.006},
  journal      = {Neural Networks},
  pages        = {356-365},
  shortjournal = {Neural Netw.},
  title        = {Finite-time resilient h∞ state estimation for discrete-time delayed neural networks under dynamic event-triggered mechanism},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interfering with a memory without erasing its trace.
<em>NN</em>, <em>121</em>, 339–355. (<a
href="https://doi.org/10.1016/j.neunet.2019.09.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous research has shown that performance of a novice skill can be easily interfered with by subsequent training of another skill. We address the open questions whether extensively trained skills show the same vulnerability to interference as novice skills and which memory mechanism regulates interference between expert skills. We developed a recurrent neural network model of V1 able to learn from feedback experienced over the course of a long-term orientation discrimination experiment. After first exposing the model to one discrimination task for 3480 consecutive trials, we assessed how its performance was affected by subsequent training in a second, similar task. Training the second task strongly interfered with the first (highly trained) discrimination skill. The magnitude of interference depended on the relative amounts of training devoted to the different tasks. We used these and other model outcomes as predictions for a perceptual learning experiment in which human participants underwent the same training protocol as our model. Specifically, over the course of three months participants underwent baseline training in one orientation discrimination task for 15 sessions before being trained for 15 sessions on a similar task and finally undergoing another 15 sessions of training on the first task (to assess interference). Across all conditions, the pattern of interference observed empirically closely matched model predictions. According to our model, behavioral interference can be explained by antagonistic changes in neuronal tuning induced by the two tasks. Remarkably, this did not stem from erasing connections due to earlier learning but rather from a reweighting of lateral inhibition .},
  archive      = {J_NN},
  author       = {Gesa Lange and Mario Senden and Alexandra Radermacher and Peter De Weerd},
  doi          = {10.1016/j.neunet.2019.09.027},
  journal      = {Neural Networks},
  pages        = {339-355},
  shortjournal = {Neural Netw.},
  title        = {Interfering with a memory without erasing its trace},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). A waiting-time-based event-triggered scheme for
stabilization of complex-valued neural networks. <em>NN</em>,
<em>121</em>, 329–338. (<a
href="https://doi.org/10.1016/j.neunet.2019.09.032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the global stabilization of complex-valued neural networks (CVNNs) via event-triggered control. First, a waiting-time-based event-triggered scheme is designed to reduce the data transmission rate . Therein, an exponential decay term is introduced into the predefined threshold function, which may postpone the triggering instant of the necessary data and therefore reduce the frequency of data transmission. Then, with the help of the input delay approach, a time-dependent piecewise-defined Lyapunov–Krasovskii functional is constructed for closed-loop system to formulate a less conservative stability criterion. In addition, by resorting to matrix transformation , the co-design method for both the feedback gains and the trigger parameters is derived. Finally, a numerical example is given to illustrate the feasibility and superiority of the proposed event-triggered scheme and the obtained theoretical results.},
  archive      = {J_NN},
  author       = {Xiaohong Wang and Zhen Wang and Qiankun Song and Hao Shen and Xia Huang},
  doi          = {10.1016/j.neunet.2019.09.032},
  journal      = {Neural Networks},
  pages        = {329-338},
  shortjournal = {Neural Netw.},
  title        = {A waiting-time-based event-triggered scheme for stabilization of complex-valued neural networks},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Event-driven implementation of deep spiking convolutional
neural networks for supervised classification using the SpiNNaker
neuromorphic platform. <em>NN</em>, <em>121</em>, 319–328. (<a
href="https://doi.org/10.1016/j.neunet.2019.09.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks have enabled great advances in recent times due mainly to improved parallel computing capabilities in accordance to Moore’s Law, which allowed reducing the time needed for the parameter learning of complex, multi-layered neural architectures. However, with silicon technology reaching its physical limits, new types of computing paradigms are needed to increase the power efficiency of learning algorithms, especially for dealing with deep spatio-temporal knowledge on embedded applications. With the goal of mimicking the brain’s power efficiency, new hardware architectures such as the SpiNNaker board have been built. Furthermore, recent works have shown that networks using spiking neurons as learning units can match classical neural networks in supervised tasks. In this paper, we show that the implementation of state-of-the-art models on both the MNIST and the event-based NMNIST digit recognition datasets is possible on neuromorphic hardware. We use two approaches, by directly converting a classical neural network to its spiking version and by training a spiking network from scratch. For both cases, software simulations and implementations into a SpiNNaker 103 machine were performed. Numerical results approaching the state of the art on digit recognition are presented, and a new method to decrease the spike rate needed for the task is proposed, which allows a significant reduction of the spikes (up to 34 times for a fully connected architecture) while preserving the accuracy of the system. With this method, we provide new insights on the capabilities offered by networks of spiking neurons to efficiently encode spatio-temporal information.},
  archive      = {J_NN},
  author       = {Alberto Patiño-Saucedo and Horacio Rostro-Gonzalez and Teresa Serrano-Gotarredona and Bernabé Linares-Barranco},
  doi          = {10.1016/j.neunet.2019.09.008},
  journal      = {Neural Networks},
  pages        = {319-328},
  shortjournal = {Neural Netw.},
  title        = {Event-driven implementation of deep spiking convolutional neural networks for supervised classification using the SpiNNaker neuromorphic platform},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Embedding topological features into convolutional neural
network salient object detection. <em>NN</em>, <em>121</em>, 308–318.
(<a href="https://doi.org/10.1016/j.neunet.2019.09.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient object detection can be applied as a critical preprocessing step in many computer vision tasks . Recent studies of salient object detection mainly employed convolutional neural networks (CNNs) for mining high-level semantic properties. However, the existing methods can still be improved to find precise semantic information in different scenarios. In particular, in the two main methods employed for salient object detection, the patchwise detection models might ignore spatial structures among regions and the fully convolution-based models mainly consider semantic features in a global manner. In this paper, we proposed a salient object detection framework by embedding topological features into a deep neural network for extracting semantics. We segment the input image and compute weight for each region with low-level features. The weighted segmentation result is called a topological map and it provides an additional channel for the CNN to emphasize the structural integrity and locality during the extraction of semantic features . We also utilize the topological map for saliency refinement based on a conditional random field at the end of our model. Experimental results on six benchmark datasets demonstrated that our proposed framework achieves competitive performance compared to other state-of-the-art methods.},
  archive      = {J_NN},
  author       = {Lecheng Zhou and Xiaodong Gu},
  doi          = {10.1016/j.neunet.2019.09.009},
  journal      = {Neural Networks},
  pages        = {308-318},
  shortjournal = {Neural Netw.},
  title        = {Embedding topological features into convolutional neural network salient object detection},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rethinking the performance comparison between SNNS and ANNS.
<em>NN</em>, <em>121</em>, 294–307. (<a
href="https://doi.org/10.1016/j.neunet.2019.09.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial neural networks (ANNs), a popular path towards artificial intelligence , have experienced remarkable success via mature models, various benchmarks, open-source datasets, and powerful computing platforms. Spiking neural networks (SNNs), a category of promising models to mimic the neuronal dynamics of the brain, have gained much attention for brain inspired computing and been widely deployed on neuromorphic devices. However, for a long time, there are ongoing debates and skepticisms about the value of SNNs in practical applications. Except for the low power attribute benefit from the spike-driven processing, SNNs usually perform worse than ANNs especially in terms of the application accuracy. Recently, researchers attempt to address this issue by borrowing learning methodologies from ANNs, such as backpropagation , to train high-accuracy SNN models. The rapid progress in this domain continuously produces amazing results with ever-increasing network size, whose growing path seems similar to the development of deep learning . Although these ways endow SNNs the capability to approach the accuracy of ANNs, the natural superiorities of SNNs and the way to outperform ANNs are potentially lost due to the use of ANN-oriented workloads and simplistic evaluation metrics . In this paper, we take the visual recognition task as a case study to answer the questions of “ what workloads are ideal for SNNs and how to evaluate SNNs makes sense ”. We design a series of contrast tests using different types of datasets (ANN-oriented and SNN-oriented), diverse processing models, signal conversion methods, and learning algorithms. We propose comprehensive metrics on the application accuracy and the cost of memory &amp; compute to evaluate these models, and conduct extensive experiments. We evidence the fact that on ANN-oriented workloads, SNNs fail to beat their ANN counterparts; while on SNN-oriented workloads, SNNs can fully perform better. We further demonstrate that in SNNs there exists a trade-off between the application accuracy and the execution cost, which will be affected by the simulation time window and firing threshold. Based on these abundant analyses, we recommend the most suitable model for each scenario. To the best of our knowledge, this is the first work using systematical comparisons to explicitly reveal that the straightforward workload porting from ANNs to SNNs is unwise although many works are doing so and a comprehensive evaluation indeed matters. Finally, we highlight the urgent need to build a benchmarking framework for SNNs with broader tasks, datasets, and metrics.},
  archive      = {J_NN},
  author       = {Lei Deng and Yujie Wu and Xing Hu and Ling Liang and Yufei Ding and Guoqi Li and Guangshe Zhao and Peng Li and Yuan Xie},
  doi          = {10.1016/j.neunet.2019.09.005},
  journal      = {Neural Networks},
  pages        = {294-307},
  shortjournal = {Neural Netw.},
  title        = {Rethinking the performance comparison between SNNS and ANNS},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-output parameter-insensitive kernel twin SVR model.
<em>NN</em>, <em>121</em>, 276–293. (<a
href="https://doi.org/10.1016/j.neunet.2019.09.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-output regression aims at mapping a multivariate input feature space to a multivariate output space. Currently, it is effective to extend the traditional support vector regression (SVR) mechanism to solve the multi-output case. However, some methods adopting a combination of single-output SVR models exhibit the severe drawback of not considering the possible correlations between outputs, and other multi-output SVRs show high computational complexity and are typically sensitive to parameters due to the influence of noise. To handle these problems, in this study, we determine the multi-output regression function through a pair of nonparallel up- and down-bound functions solved by two smaller-sized quadratic programming problems , which results in a fast learning speed. This method is named multi-output twin support vector regression (M-TSVR). Moreover, when the noise is heteroscedastic, based on our M-TSVR, we introduce a pair of multi-input/output nonparallel parameter insensitive up- and down-bound functions to evaluate a regression model named multi-output parameter-insensitive twin support vector regression (M-PITSVR). To handle the nonlinear case , we derive the kernelized extensions of M-TSVR and M-PITSVR. Finally, a series of comparative experiments with several other multi-output-based methods are performed on twelve multi-output datasets. The experimental results indicate that the proposed multi-output regressors yield fast learning speed as well as a better and more stable prediction performance.},
  archive      = {J_NN},
  author       = {Yanmeng Li and Huaijiang Sun and Wenzhu Yan and Xiaoqian Zhang},
  doi          = {10.1016/j.neunet.2019.09.022},
  journal      = {Neural Networks},
  pages        = {276-293},
  shortjournal = {Neural Netw.},
  title        = {Multi-output parameter-insensitive kernel twin SVR model},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Event-triggered passivity and synchronization of delayed
multiple-weighted coupled reaction–diffusion neural networks with
non-identical nodes. <em>NN</em>, <em>121</em>, 259–275. (<a
href="https://doi.org/10.1016/j.neunet.2019.08.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper solves the event-triggered passivity and synchronization problems for delayed multiple-weighted coupled reaction–diffusion neural networks (DMWCRDNNs) composed of non-identical nodes with and without parameter uncertainties. On one side, by designing appropriate event-triggered controllers, several passivity and synchronization criteria for DMWCRDNNs with certain parameters under the designed event-triggered conditions are derived based on the Lyapunov functional method and some inequality techniques. On the other side, we consider that the external perturbations may lead the parameters in network model to containing uncertainties, robust event-triggered passivity and synchronization for DMWCRDNNs with parameter uncertainties are investigated. Finally, two examples with numerical simulation results are provided to illustrate the effectiveness of the obtained theoretical results.},
  archive      = {J_NN},
  author       = {Shanrong Lin and Yanli Huang and Shunyan Ren},
  doi          = {10.1016/j.neunet.2019.08.031},
  journal      = {Neural Networks},
  pages        = {259-275},
  shortjournal = {Neural Netw.},
  title        = {Event-triggered passivity and synchronization of delayed multiple-weighted coupled reaction–diffusion neural networks with non-identical nodes},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Gated spiking neural network using iterative free-energy
optimization and rank-order coding for structure learning in memory
sequences (INFERNO GATE). <em>NN</em>, <em>121</em>, 242–258. (<a
href="https://doi.org/10.1016/j.neunet.2019.09.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a framework based on iterative free-energy optimization with spiking neural networks for modeling the fronto-striatal system (PFC-BG) for the generation and recall of audio memory sequences. In line with neuroimaging studies carried out in the PFC, we propose a genuine coding strategy using the gain-modulation mechanism to represent abstract sequences based solely on the rank and location of items within them. Based on this mechanism, we show that we can construct a repertoire of neurons sensitive to the temporal structure in sequences from which we can represent any novel sequences. Free-energy optimization is then used to explore and to retrieve the missing indices of the items in the correct order for executive control and compositionality. We show that the gain-modulation mechanism permits the network to be robust to variabilities and to have long-term dependencies as it implements a gated recurrent neural network. This model, called Inferno Gate, is an extension of the neural architecture Inferno standing for Iterative Free-Energy Optimization of Recurrent Neural Networks with Gating or Gain-modulation. In experiments performed with an audio database of ten thousand MFCC vectors, Inferno Gate is capable of encoding efficiently and retrieving chunks of fifty items length. We then discuss the potential of our network to model the features of working memory in the PFC-BG loop for structural learning, goal-direction and hierarchical reinforcement learning .},
  archive      = {J_NN},
  author       = {Alexandre Pitti and Mathias Quoy and Catherine Lavandier and Sofiane Boucenna},
  doi          = {10.1016/j.neunet.2019.09.023},
  journal      = {Neural Networks},
  pages        = {242-258},
  shortjournal = {Neural Netw.},
  title        = {Gated spiking neural network using iterative free-energy optimization and rank-order coding for structure learning in memory sequences (INFERNO GATE)},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An analysis of training and generalization errors in shallow
and deep networks. <em>NN</em>, <em>121</em>, 229–241. (<a
href="https://doi.org/10.1016/j.neunet.2019.08.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is motivated by an open problem around deep networks, namely, the apparent absence of over-fitting despite large over-parametrization which allows perfect fitting of the training data. In this paper, we analyze this phenomenon in the case of regression problems when each unit evaluates a periodic activation function . We argue that the minimal expected value of the square loss is inappropriate to measure the generalization error in approximation of compositional functions in order to take full advantage of the compositional structure . Instead, we measure the generalization error in the sense of maximum loss, and sometimes, as a pointwise error. We give estimates on exactly how many parameters ensure both zero training error as well as a good generalization error. We prove that a solution of a regularization problem is guaranteed to yield a good training error as well as a good generalization error and estimate how much error to expect at which test data.},
  archive      = {J_NN},
  author       = {H.N. Mhaskar and T. Poggio},
  doi          = {10.1016/j.neunet.2019.08.028},
  journal      = {Neural Networks},
  pages        = {229-241},
  shortjournal = {Neural Netw.},
  title        = {An analysis of training and generalization errors in shallow and deep networks},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distributed dual vigilance fuzzy adaptive resonance theory
learns online, retrieves arbitrarily-shaped clusters, and mitigates
order dependence. <em>NN</em>, <em>121</em>, 208–228. (<a
href="https://doi.org/10.1016/j.neunet.2019.08.033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel adaptive resonance theory (ART)-based modular architecture for unsupervised learning , namely the distributed dual vigilance fuzzy ART (DDVFA). DDVFA consists of a global ART system whose nodes are local fuzzy ART modules. It is equipped with distributed higher-order activation and match functions and a dual vigilance mechanism. Together, these allow DDVFA to perform unsupervised modularization, create multi-prototype cluster representations, retrieve arbitrarily-shaped clusters, and reduce category proliferation. Another important contribution is the reduction of order-dependence, an issue that affects any agglomerative clustering method. This paper demonstrates two approaches for mitigating order-dependence: pre-processing using visual assessment of cluster tendency (VAT) or post-processing using a novel Merge ART module. The former is suitable for batch processing, whereas the latter also works for online learning. Experimental results in online mode carried out on 30 benchmark data sets show that DDVFA cascaded with Merge ART statistically outperformed the best other ART-based systems when samples were randomly presented. Conversely, they were found to be statistically equivalent in offline mode when samples were pre-processed using VAT. Remarkably, performance comparisons to non-ART-based clustering algorithms show that DDVFA (which learns incrementally) was also statistically equivalent to the non-incremental (offline) methods of density-based spatial clustering of applications with noise (DBSCAN), single linkage hierarchical agglomerative clustering (SL-HAC), and k-means, while retaining the appealing properties of ART. Links to the source code and data are provided. Considering the algorithm’s simplicity, online learning capability, and performance, it is an ideal choice for many agglomerative clustering applications.},
  archive      = {J_NN},
  author       = {Leonardo Enzo Brito da Silva and Islam Elnabarawy and Donald C. Wunsch II},
  doi          = {10.1016/j.neunet.2019.08.033},
  journal      = {Neural Networks},
  pages        = {208-228},
  shortjournal = {Neural Netw.},
  title        = {Distributed dual vigilance fuzzy adaptive resonance theory learns online, retrieves arbitrarily-shaped clusters, and mitigates order dependence},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Recognition of words from brain-generated signals of
speech-impaired people: Application of autoencoders as a neural turing
machine controller in deep neural networks. <em>NN</em>, <em>121</em>,
186–207. (<a
href="https://doi.org/10.1016/j.neunet.2019.07.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is an essential requirement to support people with speech and communication disabilities. A brain–computer interface using electroencephalography (EEG) is applied to satisfy this requirement. A number of research studies to recognize brain signals using machine learning and deep neural networks (DNNs) have been performed to increase the brain signal detection rate, yet there are several defects and limitations in the techniques. Among them is the use in specific circumstances of machine learning . On the one hand, DNNs extract the features well and automatically. On the other hand, their use results in overfitting and vanishing problems. Consequently, in this research, a deep network is designed on the basis of an autoencoder neural Turing machine (DN-AE-NTM) to resolve the problems by the use of NTM external memory. In addition, the DN-AE-NTM copes with all kinds of signals with high detection rates. The data were collected by P300 EEG devices from several individuals under the same conditions. During the test, each individual was requested to skim images with one to six labels and focus on only one of the images. Not to focus on some images is analogous to producing unimportant information in the individual’s brain, which provides unfamiliar signals. Besides the main P300 EEG dataset, EEG recordings of individuals with alcoholism and control individuals and the EEGMMIDB, MNIST, and ORL datasets were implemented and tested. The proposed DN-AE-NTM method classifies data with an average detection rate of 97.5\%, 95\%, 98\%, 99.4\%, and 99.1\%, respectively, in situations where the signals are noisy so that only 20\% of the data are reliable and include useful information.},
  archive      = {J_NN},
  author       = {Behzad Boloukian and Faramarz Safi-Esfahani},
  doi          = {10.1016/j.neunet.2019.07.012},
  journal      = {Neural Networks},
  pages        = {186-207},
  shortjournal = {Neural Netw.},
  title        = {Recognition of words from brain-generated signals of speech-impaired people: Application of autoencoders as a neural turing machine controller in deep neural networks},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep learning and deep knowledge representation in spiking
neural networks for brain-computer interfaces. <em>NN</em>,
<em>121</em>, 169–185. (<a
href="https://doi.org/10.1016/j.neunet.2019.08.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Objective: This paper argues that Brain-Inspired Spiking Neural Network (BI-SNN) architectures can learn and reveal deep in time-space functional and structural patterns from spatio-temporal data. These patterns can be represented as deep knowledge, in a partial case in the form of deep spatio-temporal rules. This is a promising direction for building new types of Brain-Computer Interfaces called Brain-Inspired Brain–Computer Interfaces (BI-BCI). A theoretical framework and its experimental validation on deep knowledge extraction and representation using SNN are presented. Results: The proposed methodology was applied in a case study to extract deep knowledge of the functional and structural organisation of the brain’s neural network during the execution of a Grasp and Lift task. The BI-BCI successfully extracted the neural trajectories that represent the dorsal and ventral visual information processing streams as well as its connection to the motor cortex in the brain. Deep spatiotemporal rules on functional and structural interaction of distinct brain areas were then used for event prediction in BI-BCI. Significance: The computational framework can be used for unveiling the topological patterns of the brain and such knowledge can be effectively used to enhance the state-of-the-art in BCI.},
  archive      = {J_NN},
  author       = {Kaushalya Kumarasinghe and Nikola Kasabov and Denise Taylor},
  doi          = {10.1016/j.neunet.2019.08.029},
  journal      = {Neural Networks},
  pages        = {169-185},
  shortjournal = {Neural Netw.},
  title        = {Deep learning and deep knowledge representation in spiking neural networks for brain-computer interfaces},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Operation-aware neural networks for user response
prediction. <em>NN</em>, <em>121</em>, 161–168. (<a
href="https://doi.org/10.1016/j.neunet.2019.09.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User response prediction makes a crucial contribution to the rapid development of online advertising system and recommendation system. The importance of learning feature interactions has been emphasized by many works. Many deep models are proposed to automatically learn high-order feature interactions. Since most features in advertising systems and recommendation systems are high-dimensional sparse features, deep models usually learn a low-dimensional distributed representation for each feature in the bottom layer. Besides traditional fully-connected architectures, some new operations, such as convolutional operations and product operations, are proposed to learn feature interactions better. In these models, the representation is shared among different operations. However, the best representation for each operation may be different. In this paper, we propose a new neural model named Operation-aware Neural Networks (ONN) which learns different representations for different operations. Our experimental results on two large-scale real-world ad click/conversion datasets demonstrate that ONN consistently outperforms the state-of-the-art models in both offline-training environment and online-training environment.},
  archive      = {J_NN},
  author       = {Yi Yang and Baile Xu and Shaofeng Shen and Furao Shen and Jian Zhao},
  doi          = {10.1016/j.neunet.2019.09.020},
  journal      = {Neural Networks},
  pages        = {161-168},
  shortjournal = {Neural Netw.},
  title        = {Operation-aware neural networks for user response prediction},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tree-CNN: A hierarchical deep convolutional neural network
for incremental learning. <em>NN</em>, <em>121</em>, 148–160. (<a
href="https://doi.org/10.1016/j.neunet.2019.09.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past decade, Deep Convolutional Neural Networks (DCNNs) have shown remarkable performance in most computer vision tasks. These tasks traditionally use a fixed dataset, and the model, once trained, is deployed as is. Adding new information to such a model presents a challenge due to complex training issues, such as “catastrophic forgetting”, and sensitivity to hyper-parameter tuning. However, in this modern world, data is constantly evolving, and our deep learning models are required to adapt to these changes. In this paper, we propose an adaptive hierarchical network structure composed of DCNNs that can grow and learn as new data becomes available. The network grows in a tree-like fashion to accommodate new classes of data, while preserving the ability to distinguish the previously trained classes. The network organizes the incrementally available data into feature-driven super-classes and improves upon existing hierarchical CNN models by adding the capability of self-growth. The proposed hierarchical model, when compared against fine-tuning a deep network, achieves significant reduction of training effort, while maintaining competitive accuracy on CIFAR-10 and CIFAR-100.},
  archive      = {J_NN},
  author       = {Deboleena Roy and Priyadarshini Panda and Kaushik Roy},
  doi          = {10.1016/j.neunet.2019.09.010},
  journal      = {Neural Networks},
  pages        = {148-160},
  shortjournal = {Neural Netw.},
  title        = {Tree-CNN: A hierarchical deep convolutional neural network for incremental learning},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sliding mode control of neural networks via continuous or
periodic sampling event-triggering algorithm. <em>NN</em>, <em>121</em>,
140–147. (<a
href="https://doi.org/10.1016/j.neunet.2019.09.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the theoretical results on sliding mode control (SMC) of neural networks via continuous or periodic sampling event-triggered algorithm. Firstly, SMC with continuous sampling event-triggered scheme is developed and the practical sliding mode can be achieved. In addition, there is a consistent positive lower bound for the time interval between two successive trigger events which implies that the Zeno phenomenon will not occur. Next, a more economical and realistic SMC technique is presented with periodic sampling event-triggered algorithm, which guarantees the robust stability of the augmented system . Finally, two illustrative examples are presented to substantiate the effectiveness of the derived theoretical results.},
  archive      = {J_NN},
  author       = {Shiqin Wang and Yuting Cao and Tingwen Huang and Yiran Chen and Peng Li and Shiping Wen},
  doi          = {10.1016/j.neunet.2019.09.001},
  journal      = {Neural Networks},
  pages        = {140-147},
  shortjournal = {Neural Netw.},
  title        = {Sliding mode control of neural networks via continuous or periodic sampling event-triggering algorithm},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Named entity recognition in electronic health records using
transfer learning bootstrapped neural networks. <em>NN</em>,
<em>121</em>, 132–139. (<a
href="https://doi.org/10.1016/j.neunet.2019.08.032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural networks (NNs) have become the state of the art in many machine learning applications, such as image, sound (LeCun et al., 2015) and natural language processing (Young et al., 2017; Linggard et al., 2012). However, the success of NNs remains dependent on the availability of large labelled datasets, such as in the case of electronic health records (EHRs). With scarce data, NNs are unlikely to be able to extract this hidden information with practical accuracy. In this study, we develop an approach that solves these problems for named entity recognition , obtaining 94.6 F1 score in I2B2 2009 Medical Extraction Challenge (Uzuner et al., 2010), 4.3 above the architecture that won the competition. To achieve this, we bootstrap our NN models through transfer learning by pretraining word embeddings on a secondary task performed on a large pool of unannotated EHRs and using the output embeddings as a foundation of a range of NN architectures. Beyond the official I2B2 challenge, we further achieve 82.4 F1 on extracting relationships between medical terms using attention-based seq2seq models bootstrapped in the same manner.},
  archive      = {J_NN},
  author       = {Luka Gligic and Andrey Kormilitzin and Paul Goldberg and Alejo Nevado-Holgado},
  doi          = {10.1016/j.neunet.2019.08.032},
  journal      = {Neural Networks},
  pages        = {132-139},
  shortjournal = {Neural Netw.},
  title        = {Named entity recognition in electronic health records using transfer learning bootstrapped neural networks},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). How are response properties in the middle temporal area
related to inference on visual motion patterns? <em>NN</em>,
<em>121</em>, 122–131. (<a
href="https://doi.org/10.1016/j.neunet.2019.08.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurons in the primate middle temporal area (MT) respond to moving stimuli, with strong tuning for motion speed and direction. These responses have been characterized in detail, but the functional significance of these details (e.g. shapes and widths of speed tuning curves) is unclear, because they cannot be selectively manipulated. To estimate their functional significance, we used a detailed model of MT population responses as input to convolutional networks that performed sophisticated motion processing tasks (visual odometry and gesture recognition). We manipulated the distributions of speed and direction tuning widths, and studied the effects on task performance. We also studied performance with random linear mixtures of the responses, and with responses that had the same representational dissimilarity as the model populations, but were otherwise randomized. The width of speed and direction tuning both affected task performance, despite the networks having been optimized individually for each tuning variation, but the specific effects were different in each task. Random linear mixing improved performance of the odometry task, but not the gesture recognition task. Randomizing the responses while maintaining representational dissimilarity resulted in poor odometry performance. In summary, despite full optimization of the deep networks in each case, each manipulation of the representation affected performance of sophisticated visual tasks. Representation properties such as tuning width and representational similarity have been studied extensively from other perspectives, but this work provides new insight into their possible roles in sophisticated visual inference.},
  archive      = {J_NN},
  author       = {Omid Rezai and Lucas Stoffl and Bryan Tripp},
  doi          = {10.1016/j.neunet.2019.08.027},
  journal      = {Neural Networks},
  pages        = {122-131},
  shortjournal = {Neural Netw.},
  title        = {How are response properties in the middle temporal area related to inference on visual motion patterns?},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep neural network and data augmentation methodology for
off-axis iris segmentation in wearable headsets. <em>NN</em>,
<em>121</em>, 101–121. (<a
href="https://doi.org/10.1016/j.neunet.2019.07.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A data augmentation methodology is presented and applied to generate a large dataset of off-axis iris regions and train a low-complexity deep neural network . Although of low complexity the resulting network achieves a high level of accuracy in iris region segmentation for challenging off-axis eye-patches. Interestingly, this network is also shown to achieve high levels of performance for regular, frontal, segmentation of iris regions, comparing favourably with state-of-the-art techniques of significantly higher complexity. Due to its lower complexity this network is well suited for deployment in embedded applications such as augmented and mixed reality headsets.},
  archive      = {J_NN},
  author       = {Viktor Varkarakis and Shabab Bazrafkan and Peter Corcoran},
  doi          = {10.1016/j.neunet.2019.07.020},
  journal      = {Neural Networks},
  pages        = {101-121},
  shortjournal = {Neural Netw.},
  title        = {Deep neural network and data augmentation methodology for off-axis iris segmentation in wearable headsets},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spiking neural networks and online learning: An overview and
perspectives. <em>NN</em>, <em>121</em>, 88–100. (<a
href="https://doi.org/10.1016/j.neunet.2019.09.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Applications that generate huge amounts of data in the form of fast streams are becoming increasingly prevalent, being therefore necessary to learn in an online manner. These conditions usually impose memory and processing time restrictions, and they often turn into evolving environments where a change may affect the input data distribution. Such a change causes that predictive models trained over these stream data become obsolete and do not adapt suitably to new distributions. Specially in these non-stationary scenarios, there is a pressing need for new algorithms that adapt to these changes as fast as possible, while maintaining good performance scores. Unfortunately, most off-the-shelf classification models need to be retrained if they are used in changing environments, and fail to scale properly. Spiking Neural Networks have revealed themselves as one of the most successful approaches to model the behavior and learning potential of the brain, and exploit them to undertake practical online learning tasks. Besides, some specific flavors of Spiking Neural Networks can overcome the necessity of retraining after a drift occurs. This work intends to merge both fields by serving as a comprehensive overview, motivating further developments that embrace Spiking Neural Networks for online learning scenarios, and being a friendly entry point for non-experts.},
  archive      = {J_NN},
  author       = {Jesus L. Lobo and Javier Del Ser and Albert Bifet and Nikola Kasabov},
  doi          = {10.1016/j.neunet.2019.09.004},
  journal      = {Neural Networks},
  pages        = {88-100},
  shortjournal = {Neural Netw.},
  title        = {Spiking neural networks and online learning: An overview and perspectives},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MultiResUNet: Rethinking the u-net architecture for
multimodal biomedical image segmentation. <em>NN</em>, <em>121</em>,
74–87. (<a href="https://doi.org/10.1016/j.neunet.2019.08.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years Deep Learning has brought about a breakthrough in Medical Image Segmentation . In this regard, U-Net has been the most popular architecture in the medical imaging community. Despite outstanding overall performance in segmenting multimodal medical images, through extensive experimentations on some challenging datasets, we demonstrate that the classical U-Net architecture seems to be lacking in certain aspects. Therefore, we propose some modifications to improve upon the already state-of-the-art U-Net model. Following these modifications, we develop a novel architecture, MultiResUNet, as the potential successor to the U-Net architecture. We have tested and compared MultiResUNet with the classical U-Net on a vast repertoire of multimodal medical images. Although only slight improvements in the cases of ideal images are noticed, remarkable gains in performance have been attained for the challenging ones. We have evaluated our model on five different datasets, each with their own unique challenges, and have obtained a relative improvement in performance of 10.15\%, 5.07\%, 2.63\%, 1.41\%, and 0.62\% respectively. We have also discussed and highlighted some qualitatively superior aspects of MultiResUNet over classical U-Net that are not really reflected in the quantitative measures.},
  archive      = {J_NN},
  author       = {Nabil Ibtehaz and M. Sohel Rahman},
  doi          = {10.1016/j.neunet.2019.08.025},
  journal      = {Neural Networks},
  pages        = {74-87},
  shortjournal = {Neural Netw.},
  title        = {MultiResUNet: Rethinking the U-net architecture for multimodal biomedical image segmentation},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The GH-EXIN neural network for hierarchical clustering.
<em>NN</em>, <em>121</em>, 57–73. (<a
href="https://doi.org/10.1016/j.neunet.2019.07.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical clustering is an important tool for extracting information from data in a multi-resolution way. It is more meaningful if driven by data, as in the case of divisive algorithms , which split data until no more division is allowed. However, they have the drawback of the splitting threshold setting. The neural networks can address this problem, because they basically depend on data. The growing hierarchical GH-EXIN neural network builds a hierarchical tree in an incremental (data-driven architecture) and self-organized way. It is a top-down technique which defines the horizontal growth by means of an anisotropic region of influence, based on the novel idea of neighborhood convex hull . It also reallocates data and detects outliers by using a novel approach on all the leaves, simultaneously. Its complexity is estimated and an analysis of its user-dependent parameters is given. The advantages of the proposed approach, with regard to the best existing networks, are shown and analyzed, qualitatively and quantitatively, both in benchmark synthetic problems and in a real application (image recognition from video), in order to test the performance in building hierarchical trees. Furthermore, an important and very promising application of GH-EXIN in two-way hierarchical clustering, for the analysis of gene expression data in the study of the colorectal cancer is described.},
  archive      = {J_NN},
  author       = {Giansalvo Cirrincione and Gabriele Ciravegna and Pietro Barbiero and Vincenzo Randazzo and Eros Pasero},
  doi          = {10.1016/j.neunet.2019.07.018},
  journal      = {Neural Networks},
  pages        = {57-73},
  shortjournal = {Neural Netw.},
  title        = {The GH-EXIN neural network for hierarchical clustering},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the validity of memristor modeling in the neural network
literature. <em>NN</em>, <em>121</em>, 52–56. (<a
href="https://doi.org/10.1016/j.neunet.2019.08.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An analysis of the literature shows that there are two types of non-memristive models that have been widely used in the modeling of so-called “memristive” neural networks . Here, we demonstrate that such models have nothing in common with the concept of memristive elements: they describe either non-linear resistors or certain bi-state systems , which all are devices without memory. Therefore, the results presented in a significant number of publications are at least questionable, if not completely irrelevant to the actual field of memristive neural networks .},
  archive      = {J_NN},
  author       = {Yuriy V. Pershin and Massimiliano Di Ventra},
  doi          = {10.1016/j.neunet.2019.08.026},
  journal      = {Neural Networks},
  pages        = {52-56},
  shortjournal = {Neural Netw.},
  title        = {On the validity of memristor modeling in the neural network literature},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling place cells and grid cells in multi-compartment
environments: Entorhinal–hippocampal loop as a multisensory integration
circuit. <em>NN</em>, <em>121</em>, 37–51. (<a
href="https://doi.org/10.1016/j.neunet.2019.09.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hippocampal place cells and entorhinal grid cells are thought to form a representation of space by integrating internal and external sensory cues. Experimental data show that different subsets of place cells are controlled by vision, self-motion or a combination of both. Moreover, recent studies in environments with a high degree of visual aliasing suggest that a continuous interaction between place cells and grid cells can result in a deformation of hexagonal grids or in a progressive loss of visual cue control over grid fields. The computational nature of such a bidirectional interaction remains unclear. In this work we present a neural network model of the dynamic interaction between place cells and grid cells within the entorhinal–hippocampal processing loop. The model was tested in two recent experimental paradigms involving environments with visually similar compartments that provided conflicting evidence about visual cue control over self-motion-based spatial codes. Analysis of the model behavior suggests that the strength of entorhinal–hippocampal dynamical loop is the key parameter governing differential cue control in multi-compartment environments. Moreover, construction of separate spatial representations of visually identical compartments required a progressive weakening of visual cue control over place fields in favor of self-motion based mechanisms. More generally our results suggest a functional segregation between plastic and dynamic processes in hippocampal processing.},
  archive      = {J_NN},
  author       = {Tianyi Li and Angelo Arleo and Denis Sheynikhovich},
  doi          = {10.1016/j.neunet.2019.09.002},
  journal      = {Neural Networks},
  pages        = {37-51},
  shortjournal = {Neural Netw.},
  title        = {Modeling place cells and grid cells in multi-compartment environments: Entorhinal–hippocampal loop as a multisensory integration circuit},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Indirect and direct training of spiking neural networks for
end-to-end control of a lane-keeping vehicle. <em>NN</em>, <em>121</em>,
21–36. (<a href="https://doi.org/10.1016/j.neunet.2019.05.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Building spiking neural networks (SNNs) based on biological synaptic plasticities holds a promising potential for accomplishing fast and energy-efficient computing, which is beneficial to mobile robotic applications . However, the implementations of SNNs in robotic fields are limited due to the lack of practical training methods. In this paper, we therefore introduce both indirect and direct end-to-end training methods of SNNs for a lane-keeping vehicle. First, we adopt a policy learned using the Deep Q-Learning (DQN) algorithm and then subsequently transfer it to an SNN using supervised learning. Second, we adopt the reward-modulated spike-timing-dependent plasticity (R-STDP) for training SNNs directly, since it combines the advantages of both reinforcement learning and the well-known spike-timing-dependent plasticity (STDP). We examine the proposed approaches in three scenarios in which a robot is controlled to keep within lane markings by using an event-based neuromorphic vision sensor. We further demonstrate the advantages of the R-STDP approach in terms of the lateral localization accuracy and training time steps by comparing them with other three algorithms presented in this paper.},
  archive      = {J_NN},
  author       = {Zhenshan Bing and Claus Meschede and Guang Chen and Alois Knoll and Kai Huang},
  doi          = {10.1016/j.neunet.2019.05.019},
  journal      = {Neural Networks},
  pages        = {21-36},
  shortjournal = {Neural Netw.},
  title        = {Indirect and direct training of spiking neural networks for end-to-end control of a lane-keeping vehicle},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Centralized/decentralized event-triggered pinning
synchronization of stochastic coupled networks with noise and incomplete
transitional rate. <em>NN</em>, <em>121</em>, 10–20. (<a
href="https://doi.org/10.1016/j.neunet.2019.09.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the synchronous problem of Markovian switching complex networks associated with partly unknown transitional rates, stochastic noise, and randomly coupling strength. In order to achieve the synchronization for these array networks, event-triggered pinning control is established and developed, in which the pinning node undergoes a self-adapted switch, governed by a Markov chain . Two types of event-triggered sampling controls, centralized and decentralized event-triggered sampling, respectively, are established. Sufficient conditions for synchronization are developed by constructing a desirable stochastic Lyapunov functional as well as by employing the properties of Markov chain and It o ˆ oˆ integration. Numerical simulations are provided to demonstrate the effectiveness of the proposed approach.},
  archive      = {J_NN},
  author       = {Hailing Dong and Jiamu Zhou and Mingqing Xiao},
  doi          = {10.1016/j.neunet.2019.09.003},
  journal      = {Neural Networks},
  pages        = {10-20},
  shortjournal = {Neural Netw.},
  title        = {Centralized/decentralized event-triggered pinning synchronization of stochastic coupled networks with noise and incomplete transitional rate},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Label-activating framework for zero-shot learning.
<em>NN</em>, <em>121</em>, 1–9. (<a
href="https://doi.org/10.1016/j.neunet.2019.08.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing zero-shot learning (ZSL) models usually learn mappings between visual space and semantic space. However, few of them take the label information into account. Indirect Attribute Prediction (IAP) learns the posterior probability of each attribute by label space, but labels of seen and unseen classes are defined in different spaces, which is not suitable for Generalized ZSL (GZSL). We propose a Label-Activating Framework (LAF) for semantic-based classification. The purpose of the proposed framework is to activate the label space by learning mappings from vision and semantics to labels. In the training phase, the original label space made up of one-hot vectors is used as common space, on which visual features and semantic information are embedded. After the label space is activated, labels of unseen classes can be regarded as the linear combination of labels of seen classes. In this case, seen and unseen labels are defined in the same space, and the label space has specific meanings rather than only signs of each class. Doing so makes the activated label space become very discriminative, especially for GZSL, which is therefore more challenging and reasonable for real-world tasks. In addition, we develop a specific model based on the framework, which effectively mitigate the projection domain shift problem. Extensive experiments show our framework outperforms state-of-the-art methods and also its suitability for GZSL.},
  archive      = {J_NN},
  author       = {Yang Liu and Xinbo Gao and Quanxue Gao and Jungong Han and Ling Shao},
  doi          = {10.1016/j.neunet.2019.08.023},
  journal      = {Neural Networks},
  pages        = {1-9},
  shortjournal = {Neural Netw.},
  title        = {Label-activating framework for zero-shot learning},
  volume       = {121},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
