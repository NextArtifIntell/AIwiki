<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JOE_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="joe---154">JOE - 154</h2>
<ul>
<li><details>
<summary>
(2020). Econometric analysis of production networks with dominant
units. <em>JOE</em>, <em>219</em>(2), 507–541. (<a
href="https://doi.org/10.1016/j.jeconom.2020.03.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces the notions of strongly and weakly dominant units for networks, and shows that pervasiveness of shocks to a network is measured by the degree of dominance of its most pervasive unit; shown to be equivalent to the inverse of the shape parameter of the power law fitted to the network outdegrees. New cross-section and panel extremum estimators of the degree of dominance in networks are proposed, and their asymptotic properties investigated. The small sample properties of the proposed estimators are examined by Monte Carlo experiments, and their use is illustrated by an empirical application to US input–output tables.},
  archive      = {J_JOE},
  author       = {M. Hashem Pesaran and Cynthia Fan Yang},
  doi          = {10.1016/j.jeconom.2020.03.014},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {507-541},
  shortjournal = {J. Econ.},
  title        = {Econometric analysis of production networks with dominant units},
  volume       = {219},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adjusted QMLE for the spatial autoregressive parameter.
<em>JOE</em>, <em>219</em>(2), 488–506. (<a
href="https://doi.org/10.1016/j.jeconom.2020.03.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One simple , and often very effective, way to attenuate the impact of nuisance parameters on maximum likelihood estimation of a parameter of interest is to recenter the profile score for that parameter. We apply this general principle to the quasi-maximum likelihood estimator (QMLE) of the autoregressive parameter λ λ in a spatial autoregression . The resulting estimator for λ λ has better finite sample properties compared to the QMLE for λ λ , especially in the presence of a large number of covariates . It can also solve the incidental parameter problem that arises, for example, in social interaction models with network fixed effects. However, spatial autoregressions present specific challenges for this type of adjustment, because recentering the profile score may cause the adjusted estimate to be outside the usual parameter space for λ λ . Conditions for this to happen are given, and implications are discussed. For inference, we propose confidence intervals based on a Lugannani–Rice approximation to the distribution of the adjusted QMLE of λ λ . Based on our simulations, the coverage properties of these intervals are excellent even in models with a large number of covariates .},
  archive      = {J_JOE},
  author       = {Federico Martellosio and Grant Hillier},
  doi          = {10.1016/j.jeconom.2020.03.013},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {488-506},
  shortjournal = {J. Econ.},
  title        = {Adjusted QMLE for the spatial autoregressive parameter},
  volume       = {219},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-frequency jump tests: Which test should we use?
<em>JOE</em>, <em>219</em>(2), 478–487. (<a
href="https://doi.org/10.1016/j.jeconom.2020.03.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We conduct an extensive evaluation of price jump tests based on high-frequency financial data. After providing a concise review of multiple alternative tests, we document the size and power of all tests in a range of empirically relevant scenarios. Particular focus is given to the robustness of test performance to the presence of jumps in volatility and microstructure noise, and to the impact of sampling frequency. The paper concludes by providing guidelines for empirical researchers about which test to choose in any given setting.},
  archive      = {J_JOE},
  author       = {Worapree Maneesoonthorn and Gael M. Martin and Catherine S. Forbes},
  doi          = {10.1016/j.jeconom.2020.03.012},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {478-487},
  shortjournal = {J. Econ.},
  title        = {High-frequency jump tests: Which test should we use?},
  volume       = {219},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-dimensional predictive regression in the presence of
cointegration. <em>JOE</em>, <em>219</em>(2), 456–477. (<a
href="https://doi.org/10.1016/j.jeconom.2020.03.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a Least Absolute Shrinkage and Selection Operator (LASSO) estimator of a predictive regression in which stock returns are conditioned on a large set of lagged covariates , some of which are highly persistent and potentially cointegrated. We establish the asymptotic properties of the proposed LASSO estimator and validate our theoretical findings using simulation studies. The application of this proposed LASSO approach to forecasting stock returns suggests that a cointegrating relationship among the persistent predictors leads to a significant improvement in the prediction of stock returns over various competing forecasting methods with respect to mean squared error.},
  archive      = {J_JOE},
  author       = {Bonsoo Koo and Heather M. Anderson and Myung Hwan Seo and Wenying Yao},
  doi          = {10.1016/j.jeconom.2020.03.011},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {456-477},
  shortjournal = {J. Econ.},
  title        = {High-dimensional predictive regression in the presence of cointegration},
  volume       = {219},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hypothesis testing based on a vector of statistics.
<em>JOE</em>, <em>219</em>(2), 425–455. (<a
href="https://doi.org/10.1016/j.jeconom.2020.03.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new approach to hypothesis testing based on a vector of statistics . It involves simulating the statistics under the null hypothesis and then estimating the joint density of the statistics. This allows the p p -value of the smallest acceptance region test to be estimated. We prove this p p -value is a consistent estimate under some regularity conditions . The small-sample properties of the proposed procedure are investigated in the context of testing for autocorrelation , testing for normality, and testing for model misspecification through the information matrix. We find that our testing procedure has appropriate sizes and good powers.},
  archive      = {J_JOE},
  author       = {Maxwell L. King and Xibin Zhang and Muhammad Akram},
  doi          = {10.1016/j.jeconom.2020.03.010},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {425-455},
  shortjournal = {J. Econ.},
  title        = {Hypothesis testing based on a vector of statistics},
  volume       = {219},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparametric estimation of infinite order regression and
its application to the risk-return tradeoff. <em>JOE</em>,
<em>219</em>(2), 389–424. (<a
href="https://doi.org/10.1016/j.jeconom.2020.03.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies nonparametric estimation of the infinite order regression E ( Y t k | F t − 1 ) E(Ytk|Ft−1) , k ∈ Z k∈Z with stationary and weakly dependent data. We propose a Nadaraya–Watson type estimator that operates with an infinite number of conditioning variables. We propose a bandwidth sequence that shrinks the effects of long lags, so the influence of all conditioning information is modelled in a natural and flexible way, and the issues of omitted information bias and specification error are effectively handled. We establish the asymptotic properties of the estimator under a wide range of static and dynamic regressions frameworks, thereby allowing various kinds of conditioning variables to be used. We establish pointwise/uniform consistency and CLTs. We show that the convergence rates are at best logarithmic, and depend on the smoothness of the regression, the distribution of the marginal regressors and their dependence structure in a non-trivial way via the Lambert W function. We apply our methodology to examine the intertemporal risk-return relation for the aggregate stock market, and some new empirical evidence is reported. For the S&amp;P 500 daily data from 1950 to 2017 using our estimator we report an overall positive risk-return relation. We also find evidence of strong time variation and counter-cyclical behaviour in risk aversion . These conclusions are possibly attributable to the allowance of further flexibility and the inclusion of otherwise neglected information in our method.},
  archive      = {J_JOE},
  author       = {Seok Young Hong and Oliver Linton},
  doi          = {10.1016/j.jeconom.2020.03.009},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {389-424},
  shortjournal = {J. Econ.},
  title        = {Nonparametric estimation of infinite order regression and its application to the risk-return tradeoff},
  volume       = {219},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Level shift estimation in the presence of non-stationary
volatility with an application to the unit root testing problem.
<em>JOE</em>, <em>219</em>(2), 354–388. (<a
href="https://doi.org/10.1016/j.jeconom.2020.03.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper focuses on the estimation of the location of level breaks in time series whose shocks display non-stationary volatility (permanent changes in unconditional volatility). We propose a new feasible weighted least squares (WLS) estimator, based on an adaptive estimate of the volatility path of the shocks. We show that this estimator belongs to a generic class of weighted residual sum of squares which also contains the ordinary least squares (OLS) and WLS estimators, the latter based on the true volatility process. For fixed magnitude breaks we show that the consistency rate of the generic estimator is unaffected by non-stationary volatility. We also provide local limiting distribution theory for cases where the break magnitude is either local-to-zero at some polynomial rate in the sample size or is exactly zero. The former includes the Pitman drift rate which is shown via Monte Carlo experiments to predict well the key features of the finite sample behaviour of both the OLS and our feasible WLS estimators. The simulations highlight the importance of the break location, break magnitude, and the form of non-stationary volatility for the finite sample performance of these estimators, and show that our proposed feasible WLS estimator can deliver significant improvements over the OLS estimator under heteroskedasticity. We discuss how these results can be applied, by using level break fraction estimators on the first differences of the data, when testing for a unit root in the presence of trend breaks and/or non-stationary volatility. Methods to select between the break and no break cases, using standard information criteria and feasible weighted information criteria based on our adaptive volatility estimator, are also discussed. Simulation evidence suggests that unit root tests based on these weighted quantities can display significantly improved finite sample behaviour under heteroskedasticity relative to their unweighted counterparts. An empirical illustration to U.S. and U.K. real GDP is also considered.},
  archive      = {J_JOE},
  author       = {David Harris and Hsein Kew and A.M. Robert Taylor},
  doi          = {10.1016/j.jeconom.2020.03.008},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {354-388},
  shortjournal = {J. Econ.},
  title        = {Level shift estimation in the presence of non-stationary volatility with an application to the unit root testing problem},
  volume       = {219},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Heterogeneous panel data models with cross-sectional
dependence. <em>JOE</em>, <em>219</em>(2), 329–353. (<a
href="https://doi.org/10.1016/j.jeconom.2020.03.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers a semiparametric panel data model with heterogeneous coefficients and individual-specific trending functions, where the random errors are assumed to be serially correlated and cross-sectionally dependent. We propose mean group estimators for the coefficients and trending functions involved in the model. It can be shown that the proposed estimators can achieve an asymptotic consistency with rates of root − N T −NT and root − N T h −NTh , respectively as ( N , T ) → ( ∞ , ∞ ) (N,T)→(∞,∞) , where N N is allowed to increase faster than T T . Furthermore, a statistic for testing homogeneous coefficients is constructed based on the difference between the mean group estimator and a pooled estimator. Its asymptotic distributions are established under both the null and a sequence of local alternatives, even if the difference between these estimators vanishes considerably fast (can achieve root- N T 2 NT2 rate at most under the null) and no consistent estimator for the covariance matrix is required. The finite sample performance of the proposed estimators together with the size and local power properties of the test are demonstrated by simulated data examples, and an empirical application with the OECD health care expenditure dataset is also provided.},
  archive      = {J_JOE},
  author       = {Jiti Gao and Kai Xia and Huanjun Zhu},
  doi          = {10.1016/j.jeconom.2020.03.007},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {329-353},
  shortjournal = {J. Econ.},
  title        = {Heterogeneous panel data models with cross-sectional dependence},
  volume       = {219},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Testing for a trend with persistent errors. <em>JOE</em>,
<em>219</em>(2), 314–328. (<a
href="https://doi.org/10.1016/j.jeconom.2020.03.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop new tests for the coefficient on a time trend in a regression of a variable on a constant and time trend where there is potentially strong serial correlation . This serial correlation can also include a unit root. We obtain tests under two different assumptions on the initial value for the stochastic component of the variable being examined, either this being zero asymptotically and also allowing the initial condition to be drawn from its unconditional distribution. We find that statistics perform better under the second of these assumptions, which is the more natural assumption to make.},
  archive      = {J_JOE},
  author       = {Graham Elliott},
  doi          = {10.1016/j.jeconom.2020.03.006},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {314-328},
  shortjournal = {J. Econ.},
  title        = {Testing for a trend with persistent errors},
  volume       = {219},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Asymptotic theory for time series with changing mean and
variance. <em>JOE</em>, <em>219</em>(2), 281–313. (<a
href="https://doi.org/10.1016/j.jeconom.2020.03.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper develops point estimation and asymptotic theory with respect to a semiparametric model for time series with moving mean and unconditional heteroscedasticity . These two features are modelled nonparametrically, whereas autocorrelations are described by a short memory stationary parametric time series model . We first study the usual least squares estimate of the coefficient of the first-order autoregressive model based on constant but unknown mean and variance. Allowing for both the latter to vary over time in a general way we establish its probability limit and a central limit theorem for a suitably normed and centred statistic , giving explicit bias and variance formulae. As expected mean variation is the main source of inconsistency and heteroscedasticity the main source of inefficiency, though we discuss circumstances in which the estimate is consistent for, and asymptotically normal about, the autoregressive coefficient , albeit inefficient. We then consider standard implicitly-defined Whittle estimates of a more general class of short memory parametric time series model , under otherwise more restrictive conditions. When the mean is correctly assumed to be constant, estimates that ignore the heteroscedasticity are again found to be asymptotically normal but inefficient. Allowing a slowly time-varying mean we resort to trimming out of low frequencies to achieve the same outcome. Returning to finite order autoregression , nonparametric estimates of the varying mean and variance are given asymptotic justification, and forecasting formulae developed. Finite sample properties are studied by a small Monte Carlo simulation , and an empirical example is also included.},
  archive      = {J_JOE},
  author       = {Violetta Dalla and Liudas Giraitis and Peter M. Robinson},
  doi          = {10.1016/j.jeconom.2020.03.005},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {281-313},
  shortjournal = {J. Econ.},
  title        = {Asymptotic theory for time series with changing mean and variance},
  volume       = {219},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Score tests in GMM: Why use implied probabilities?
<em>JOE</em>, <em>219</em>(2), 260–280. (<a
href="https://doi.org/10.1016/j.jeconom.2020.03.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While simple to implement and thus attractive in practice, the GMM score test of Newey and West (1987) often displays upward size distortion under common scenarios involving skewed moment vectors or models with weak identification. Inference based on the Generalized Empirical Likelihood (GEL) is seen as a general solution to this problem. Kleibergen (2005) and, more generally, Guggenberger and Smith (2005) devised an elegant theory for the GEL score tests. However, strictly speaking, the GEL score tests do not nest the Newey–West score test. Our paper provides a unified framework for score tests in GMM that nests all of the above as special cases and helps us to understand the precise mechanism by which the standard first order asymptotic theory on size and power well approximates the finite sample behavior of some score tests (namely, a subset of the GEL score tests) but not others. Special attention is paid to models with weak identification. We also argue that the apparent computational burden of GEL can be overcome in practice by recognizing the fundamental common role played by the GEL implied probabilities under all special cases of our framework. In particular, we show that all the GEL implied probabilities are asymptotically equivalent at a higher order – both under the null and under appropriate sequences of alternatives – and thus are exchangeable across computationally burdensome (e.g. Empirical Likelihood) and easy (e.g. Euclidean Empirical Likelihood) GEL score tests without affecting the first order asymptotics. Extensive simulation evidence is provided to corroborate our theoretical results. The simulation results also support a simple and yet important insight on the power of the tests: the use of implied probabilities to efficiently estimate the components of the score statistic, namely, the Jacobian and the asymptotic variance of the moment vector, can significantly improve the power of the score test in finite samples.},
  archive      = {J_JOE},
  author       = {Saraswata Chaudhuri and Eric Renault},
  doi          = {10.1016/j.jeconom.2020.03.004},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {260-280},
  shortjournal = {J. Econ.},
  title        = {Score tests in GMM: Why use implied probabilities?},
  volume       = {219},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Point optimal testing with roots that are functionally local
to unity. <em>JOE</em>, <em>219</em>(2), 231–259. (<a
href="https://doi.org/10.1016/j.jeconom.2020.03.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Limit theory for regressions involving local to unit roots (LURs) is now used extensively in time series econometric work, establishing power properties for unit root and cointegration tests, assisting the construction of uniform confidence intervals for autoregressive coefficients , and enabling the development of methods robust to departures from unit roots. The present paper shows how to generalize LUR asymptotics to cases where the localized departure from unity is a time varying function rather than a constant. Such a functional local unit root (FLUR) model has much greater generality and encompasses many cases of additional interest that appear in practical work, including structural break formulations that admit subperiods of unit root, local stationary and local explosive behavior within a given sample. Point optimal FLUR tests are constructed in the paper to accommodate such cases and demonstrate how the power envelope changes in situations of practical interest. Against FLUR alternatives, conventional constant point optimal tests can be asymptotically infinitely deficient in power, with poor finite sample power performance particularly when the departure from unity occurs early in the sample period. New analytic explanation for this phenomenon is provided. Simulation results are reported and some implications for empirical practice are examined.},
  archive      = {J_JOE},
  author       = {Anna Bykhovskaya and Peter C.B. Phillips},
  doi          = {10.1016/j.jeconom.2020.03.003},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {231-259},
  shortjournal = {J. Econ.},
  title        = {Point optimal testing with roots that are functionally local to unity},
  volume       = {219},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The term structure of equity and variance risk premia.
<em>JOE</em>, <em>219</em>(2), 204–230. (<a
href="https://doi.org/10.1016/j.jeconom.2020.03.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the term structure of variance swaps, equity and variance risk premia. A model-free analysis reveals a significant price jump component in variance swaps. A model-based analysis shows that investors’ willingness to ensure against volatility risk increases after a market drop. This effect is stronger for short maturities, but more persistent for long maturities. During the financial crisis investors demanded large risk premia to hold equities, but the risk premia largely depended on and strongly decreased with the holding horizon. The term structure of equity and variance risk premia responds differently to various economic indicators.},
  archive      = {J_JOE},
  author       = {Yacine Aït-Sahalia and Mustafa Karaman and Loriano Mancini},
  doi          = {10.1016/j.jeconom.2020.03.002},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {204-230},
  shortjournal = {J. Econ.},
  title        = {The term structure of equity and variance risk premia},
  volume       = {219},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Special issue of the journal of econometrics on “econometric
estimation and testing: Essays in honour of maxwell king.” <em>JOE</em>,
<em>219</em>(2), 201–203. (<a
href="https://doi.org/10.1016/j.jeconom.2020.03.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOE},
  author       = {Jiti Gao and Heather Anderson and Tong Li},
  doi          = {10.1016/j.jeconom.2020.03.001},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {201-203},
  shortjournal = {J. Econ.},
  title        = {Special issue of the journal of econometrics on “Econometric estimation and testing: Essays in honour of maxwell king”},
  volume       = {219},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ill-posed estimation in high-dimensional models with
instrumental variables. <em>JOE</em>, <em>219</em>(1), 171–200. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with inference about low-dimensional components of a high-dimensional parameter vector β 0 β0 which is identified through instrumental variables . We allow for eigenvalues of the expected outer product of included and excluded covariates , denoted by M M , to shrink to zero as the sample size increases. We propose a novel estimator based on desparsification of an instrumental variable Lasso estimator, which is a regularized version of 2SLS with an additional correction term. This estimator converges to β 0 β0 at a rate depending on the mapping properties of M M . Linear combinations of our estimator of β 0 β0 are shown to be asymptotically normally distributed. Based on consistent covariance estimation, our method allows for constructing confidence intervals and statistical tests for single or low-dimensional components of β 0 β0 . In Monte-Carlo simulations we analyze the finite sample behavior of our estimator. We apply our method to estimate a logit model of demand for automobiles using real market share data.},
  archive      = {J_JOE},
  author       = {Christoph Breunig and Enno Mammen and Anna Simoni},
  doi          = {10.1016/j.jeconom.2020.04.043},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {171-200},
  shortjournal = {J. Econ.},
  title        = {Ill-posed estimation in high-dimensional models with instrumental variables},
  volume       = {219},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Panel threshold models with interactive fixed effects.
<em>JOE</em>, <em>219</em>(1), 137–170. (<a
href="https://doi.org/10.1016/j.jeconom.2020.05.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies estimation and inference in a panel threshold model in the presence of interactive fixed effects. We study the asymptotic properties of the least squares estimators of the regression parameters in the shrinking-threshold-effect framework. We find that under some regularity conditions , the threshold parameter estimator possesses super-consistency in the sense that its estimation error has an asymptotically negligible effect on the asymptotic properties of the slope coefficients . The inference on the threshold parameter can be conducted based on a likelihood ratio test statistic as in the cross-sectional or time series setup. We also propose a test for the presence of the threshold effect. Monte Carlo simulations suggest that our estimators and test statistics perform well in finite samples. We apply our method to study the effect of financial development on economic growth and find that there is indeed a turning point in the effect for all three measures of financial development when the cross-sectional dependence is properly accounted for.},
  archive      = {J_JOE},
  author       = {Ke Miao and Kunpeng Li and Liangjun Su},
  doi          = {10.1016/j.jeconom.2020.05.018},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {137-170},
  shortjournal = {J. Econ.},
  title        = {Panel threshold models with interactive fixed effects},
  volume       = {219},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Testing-optimal kernel choice in HAR inference.
<em>JOE</em>, <em>219</em>(1), 123–136. (<a
href="https://doi.org/10.1016/j.jeconom.2020.06.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper investigates the optimal kernel choice in heteroskedasticity and autocorrelation robust tests based on the fixed-b asymptotics. In parallel with the optimality of the quadratic spectral kernel under the asymptotic mean squared error criterion of the point estimator of the long run variance as considered in Andrews (1991) , we show that the optimality of the quadratic spectral kernel continues to hold under the testing-oriented criterion of Sun, Philips and Jin (2008) which takes a weighted average of the probabilities of type I and type II errors of the fixed-b asymptotic test.},
  archive      = {J_JOE},
  author       = {Yixiao Sun and Jingjing Yang},
  doi          = {10.1016/j.jeconom.2020.06.007},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {123-136},
  shortjournal = {J. Econ.},
  title        = {Testing-optimal kernel choice in HAR inference},
  volume       = {219},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Doubly robust difference-in-differences estimators.
<em>JOE</em>, <em>219</em>(1), 101–122. (<a
href="https://doi.org/10.1016/j.jeconom.2020.06.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes doubly robust estimators for the average treatment effect on the treated (ATT) in difference-in-differences (DID) research designs. In contrast to alternative DID estimators, the proposed estimators are consistent if either (but not necessarily both) a propensity score or outcome regression working models are correctly specified. We also derive the semiparametric efficiency bound for the ATT in DID designs when either panel or repeated cross-section data are available, and show that our proposed estimators attain the semiparametric efficiency bound when the working models are correctly specified. Furthermore, we quantify the potential efficiency gains of having access to panel data instead of repeated cross-section data. Finally, by paying particular attention to the estimation method used to estimate the nuisance parameters , we show that one can sometimes construct doubly robust DID estimators for the ATT that are also doubly robust for inference. Simulation studies and an empirical application illustrate the desirable finite-sample performance of the proposed estimators. Open-source software for implementing the proposed policy evaluation tools is available.},
  archive      = {J_JOE},
  author       = {Pedro H.C. Sant’Anna and Jun Zhao},
  doi          = {10.1016/j.jeconom.2020.06.003},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {101-122},
  shortjournal = {J. Econ.},
  title        = {Doubly robust difference-in-differences estimators},
  volume       = {219},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation and inference of change points in
high-dimensional factor models. <em>JOE</em>, <em>219</em>(1), 66–100.
(<a href="https://doi.org/10.1016/j.jeconom.2019.08.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the estimation of break points in high-dimensional factor models where the unobserved factors are estimated by principal component analysis (PCA). The factor loading matrix is assumed to have a structural break at an unknown time. We establish the conditions under which the least squares (LS) estimator is consistent for the break date. Our consistency result holds for both large and small breaks. We also find the LS estimator’s asymptotic distribution . Simulation results confirm that the break date can be accurately estimated by the LS even if the magnitudes of breaks are small. In two empirical applications, we implement the method to estimate break points in the U.S. stock market and U.S. macroeconomy, respectively.},
  archive      = {J_JOE},
  author       = {Jushan Bai and Xu Han and Yutang Shi},
  doi          = {10.1016/j.jeconom.2019.08.013},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {66-100},
  shortjournal = {J. Econ.},
  title        = {Estimation and inference of change points in high-dimensional factor models},
  volume       = {219},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust inference for spurious regressions and cointegrations
involving processes moderately deviated from a unit root. <em>JOE</em>,
<em>219</em>(1), 52–65. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies spurious regressions involving processes moderately deviated from a unit root ( PMDUR s), and establishes the limiting distributions for the least squares estimator , the associated t -statistic, the coefficient of determination R2 and the Durbin–Watson statistic . We find that these limiting distributions critically depend on nuisance parameters that characterize the local deviations from unity, making inference for spurious regressions practically impossible using the conventional t -statistic. As a cure, we propose robust inference based on the balanced regression model, where the lagged regressor and the lagged dependent variable are augmented to the original regression. The induced t -statistic via such an augmentation is shown to be asymptotically standard normal and is therefore free of nuisance parameters , which turns out to be a robust and simple-to-implement tool for spurious regressions inference. Moreover, the limiting properties of other statistics are investigated. The balanced regression based inference is further shown to continue to work for cointegration models with PMDUR s, which is therefore robust to whether the PMDUR s are spuriously related or cointegrated. Finally, the finite sample properties of the robust method are demonstrated through both Monte Carlo and real data examples.},
  archive      = {J_JOE},
  author       = {Yingqian Lin and Yundong Tu},
  doi          = {10.1016/j.jeconom.2020.04.038},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {52-65},
  shortjournal = {J. Econ.},
  title        = {Robust inference for spurious regressions and cointegrations involving processes moderately deviated from a unit root},
  volume       = {219},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Uniform nonparametric inference for time series.
<em>JOE</em>, <em>219</em>(1), 38–51. (<a
href="https://doi.org/10.1016/j.jeconom.2019.09.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides the first result for the uniform inference based on nonparametric series estimators in a general time-series setting. We develop a strong approximation theory for sample averages of mixingales with dimensions growing with the sample size. We use this result to justify the asymptotic validity of a uniform confidence band for series estimators and show that it can also be used to conduct nonparametric specification test for conditional moment restrictions. New results on the validity of heteroskedasticity and autocorrelation consistent (HAC) estimators with increasing dimension are established for making feasible inference. An empirical application on the unemployment volatility puzzle for the search and matching model is provided as an illustration.},
  archive      = {J_JOE},
  author       = {Jia Li and Zhipeng Liao},
  doi          = {10.1016/j.jeconom.2019.09.011},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {38-51},
  shortjournal = {J. Econ.},
  title        = {Uniform nonparametric inference for time series},
  volume       = {219},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Specification test on mixed logit models. <em>JOE</em>,
<em>219</em>(1), 19–37. (<a
href="https://doi.org/10.1016/j.jeconom.2020.03.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a specification test of the mixed logit models, by generalizing Hausman and McFadden (1984) test. We generalize the test even further by considering a model developed by Berry et al. (1995).},
  archive      = {J_JOE},
  author       = {Jinyong Hahn and Jerry Hausman and Josh Lustig},
  doi          = {10.1016/j.jeconom.2020.03.015},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {19-37},
  shortjournal = {J. Econ.},
  title        = {Specification test on mixed logit models},
  volume       = {219},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparametric identification of an interdependent value
model with buyer covariates from first-price auction bids. <em>JOE</em>,
<em>219</em>(1), 1–18. (<a
href="https://doi.org/10.1016/j.jeconom.2019.12.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a version of the interdependent value model of Milgrom and Weber (1982), where the signals are given by an index gathering signal shifters observed by the econometrician and private ones specific to each bidders. The model primitives are shown to be nonparametrically identified from first-price auction bids under a testable mild rank condition. Identification holds for all possible signal values. This allows to consider a wide range of counterfactuals where this is important, as expected revenue in second-price auction. An estimation procedure is briefly discussed.},
  archive      = {J_JOE},
  author       = {Nathalie Gimenes and Emmanuel Guerre},
  doi          = {10.1016/j.jeconom.2019.12.018},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {1-18},
  shortjournal = {J. Econ.},
  title        = {Nonparametric identification of an interdependent value model with buyer covariates from first-price auction bids},
  volume       = {219},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Small-sample tests for stock return predictability with
possibly non-stationary regressors and GARCH-type effects. <em>JOE</em>,
<em>218</em>(2), 750–770. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a simulation-based procedure to test for stock return predictability with multiple regressors. The process governing the regressors is left completely free and the test procedure remains valid in small samples even in the presence of non-normalities and GARCH-type effects in the stock returns. The usefulness of the new procedure is demonstrated in a simulation study and by examining the ability of a group of financial variables to predict excess stock returns. We find some evidence of predictability during the period 1948–2014, driven entirely by the term spread. This empirical evidence, however, is much weaker over subsamples.},
  archive      = {J_JOE},
  author       = {Sermin Gungor and Richard Luger},
  doi          = {10.1016/j.jeconom.2020.04.037},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {750-770},
  shortjournal = {J. Econ.},
  title        = {Small-sample tests for stock return predictability with possibly non-stationary regressors and GARCH-type effects},
  volume       = {218},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A simple r-estimation method for semiparametric duration
models. <em>JOE</em>, <em>218</em>(2), 736–749. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling nonnegative financial variables (e.g. durations between trades or volatilities) is central to a number of studies across econometrics , and still poses several statistical challenges. Among them, the efficiency aspects of semiparametric estimation remain pivotal. In this paper, we concentrate on estimation problems in autoregressive conditional duration models with unspecified innovation densities. Exponential quasi-likelihood estimators (QMLE) are the usual practice in that context, since they are easy-to-implement and preserve Fisher-consistency. However, the efficiency of the QMLE rapidly deteriorates away from the reference exponential density. To cope with the QMLE’s lack of accuracy, semiparametrically efficient procedures have been introduced. These procedures are obtained using the classical tangent space approach; they require kernel estimation and quite large sample sizes. We propose rank-based estimators (R-estimators) as a substitute. Just as the QMLE, R-estimators remain root- n n consistent, irrespective of the underlying density, and rely on the choice of a reference density (which, however, needs not be the exponential one), under which they achieve semiparametric efficiency. Moreover, R-estimators neither require tangent space calculations nor kernel estimation. Numerical results illustrate that R-estimators based on the exponential reference density outperform the QMLE under a large class of actual innovation densities, such as the Weibull or Burr densities. A real-data example about modeling the price range of the Swiss stock market index concludes the paper.},
  archive      = {J_JOE},
  author       = {Marc Hallin and Davide La Vecchia},
  doi          = {10.1016/j.jeconom.2020.04.036},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {736-749},
  shortjournal = {J. Econ.},
  title        = {A simple R-estimation method for semiparametric duration models},
  volume       = {218},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stationary bubble equilibria in rational expectation models.
<em>JOE</em>, <em>218</em>(2), 714–735. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A linear rational expectation model with current expectations admits a unique linear stationary dynamic equilibrium only under specific restrictions on the parameter values. This paper shows that, in general, there is a multiplicity of stationary dynamic equilibria due to the existence of nonlinear stationary equilibria. These nonlinear stationary equilibria are consistent with the self-fulfilling prophecies that characterize the rational expectation equilibria, and can display speculative bubbles, volatility induced mean reversion and/or stochastic autoregressive patterns. They are also compatible with the transversality conditions when the model involves intertemporal optimization. The stationary nonlinear dynamic equilibria are economically relevant. Their analysis requires revised methods of identification for the stationary equilibrium, impulse response analysis, and estimation techniques, which are presented in this paper. Standard econometric and economic methods, which ignore the nonlinear stationary solutions provide misleading outcomes, which may affect the validity of an economic policy or portfolio strategy.},
  archive      = {J_JOE},
  author       = {C. Gourieroux and J. Jasiak and A. Monfort},
  doi          = {10.1016/j.jeconom.2020.04.035},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {714-735},
  shortjournal = {J. Econ.},
  title        = {Stationary bubble equilibria in rational expectation models},
  volume       = {218},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Volatility regressions with fat tails. <em>JOE</em>,
<em>218</em>(2), 690–713. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, a common method to forecast integrated variance is to use the fitted value of a simple OLS autoregression of the realized variance. However, non-parametric estimates of the tail index of this realized variance process reveal that its second moment is possibly unbounded. In this case, the behavior of the OLS estimators and the corresponding statistics are unclear. We prove that when the second moment of the spot variance is unbounded, the slope of the spot variance’s autoregression converges to a random variable as the sample size diverges. The same result holds when one uses the integrated or realized variance instead of the spot variance. We then consider the class of diffusion variance models with an affine drift, a class which includes GARCH and CEV processes, and we prove that IV estimation with adequate instruments provide consistent estimators of the drift parameters as long as the variance process has a finite first moment regardless of the existence of the second moment. In particular, for the GARCH diffusion model with fat tails, an IV estimation where the instrument equals the sign of the centered lagged value of the variable of interest provides consistent estimators . Simulation results corroborate the theoretical findings of the paper.},
  archive      = {J_JOE},
  author       = {Jihyun Kim and Nour Meddahi},
  doi          = {10.1016/j.jeconom.2020.04.034},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {690-713},
  shortjournal = {J. Econ.},
  title        = {Volatility regressions with fat tails},
  volume       = {218},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Testing distributional assumptions using a continuum of
moments. <em>JOE</em>, <em>218</em>(2), 655–689. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose specification tests for parametric distributions that compare the potentially complex theoretical and empirical characteristic functions using the continuum of moment conditions analogue to an overidentifying restrictions test, which takes into account the correlation between influence functions for different argument values. We derive its asymptotic distribution for fixed regularization parameter and when this vanishes with the sample size. We show its consistency against any deviation from the null , study its local power and compare it with existing tests. An extensive Monte Carlo exercise confirms that our proposed tests display good power in finite samples against a variety of alternatives.},
  archive      = {J_JOE},
  author       = {Dante Amengual and Marine Carrasco and Enrique Sentana},
  doi          = {10.1016/j.jeconom.2020.04.033},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {655-689},
  shortjournal = {J. Econ.},
  title        = {Testing distributional assumptions using a continuum of moments},
  volume       = {218},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Testing a large set of zero restrictions in regression
models, with an application to mixed frequency granger causality.
<em>JOE</em>, <em>218</em>(2), 633–654. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new test for a large set of zero restrictions in regression models based on a seemingly overlooked, but simple, dimension reduction technique. The procedure involves multiple parsimonious regression models where key regressors are split across simple regressions. Each parsimonious regression model has one key regressor and other regressors not associated with the null hypothesis. The test is based on the maximum of the squared parameters of the key regressors. Parsimony ensures sharper estimates and therefore improves power in small sample. We present the general theory of our test and focus on mixed frequency Granger causality as a prominent application involving many zero restrictions.},
  archive      = {J_JOE},
  author       = {Eric Ghysels and Jonathan B. Hill and Kaiji Motegi},
  doi          = {10.1016/j.jeconom.2020.04.032},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {633-654},
  shortjournal = {J. Econ.},
  title        = {Testing a large set of zero restrictions in regression models, with an application to mixed frequency granger causality},
  volume       = {218},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simple and reliable estimators of coefficients of interest
in a model with high-dimensional confounding effects. <em>JOE</em>,
<em>218</em>(2), 609–632. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Often an investigator is interested in a single parameter or low-dimensional parameter vector (e.g. a treatment effect) in a regression model, rather than in the full set of regression coefficients . There may also be a relatively high-dimensional set of potential explanatory variables other than the effect of interest which, if omitted, could bias the estimate of the parameter of interest. However there may be too many such variables to include all of them without substantial efficiency loss. We suggest a simple, easily computed estimator for this case, using principal components to compute auxiliary regressors from the set of potential controls, and establish the limit properties of the estimator allowing for dependence and heterogeneity as well as increasing dimension of the set of controls. We also provide finite-sample evidence on the performance of the estimator where principal components are selected in a one-dimensional search using an appropriate information criterion as stopping rule for the number of components. The results suggest that the estimator has practical usefulness in small samples.},
  archive      = {J_JOE},
  author       = {John W. Galbraith and Victoria Zinde-Walsh},
  doi          = {10.1016/j.jeconom.2020.04.031},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {609-632},
  shortjournal = {J. Econ.},
  title        = {Simple and reliable estimators of coefficients of interest in a model with high-dimensional confounding effects},
  volume       = {218},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Regression discontinuity designs, white noise models, and
minimax. <em>JOE</em>, <em>218</em>(2), 587–608. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies an approximation of sharp regression discontinuity (RD) designs with normal homoskedastic errors by ideal Gaussian white noise models with discontinuous drifts. After establishing an asymptotic equivalence of the two models, we first provide a simple derivation of a lower bound on the minimax optimal convergence rate for estimating the discontinuity size in a derivative (of an arbitrary order) of the regression function . We show that the optimal convergence rate is attained by the local polynomial estimator. We also establish an analogous correspondence between the RD designs with an unknown threshold and a convolution white noise model, and derive the minimax optimal rate for estimating the discontinuity location in a derivative of the regression function .},
  archive      = {J_JOE},
  author       = {Purevdorj Tuvaandorj},
  doi          = {10.1016/j.jeconom.2020.04.030},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {587-608},
  shortjournal = {J. Econ.},
  title        = {Regression discontinuity designs, white noise models, and minimax},
  volume       = {218},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Likelihood ratio testing in linear state space models: An
application to dynamic stochastic general equilibrium models.
<em>JOE</em>, <em>218</em>(2), 561–586. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the problem of hypothesis testing in linear Gaussian state space models . We consider two hypotheses of interest: a simple null and a hypothesis of explicit parameter restrictions. We derive the asymptotic distributions of the corresponding likelihood ratio test statistics and compute the Bartlett adjustments. The results are non-trivial because the unrestricted state space model is not (even locally) identified. We apply our analysis to test the validity of the Dynamic Stochastic General Equilibrium (DSGE) models. A Monte Carlo exercise illustrates our findings and confirms the importance of Bartlett corrections at sample sizes typically encountered in macroeconomics .},
  archive      = {J_JOE},
  author       = {Ivana Komunjer and Yinchu Zhu},
  doi          = {10.1016/j.jeconom.2020.04.029},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {561-586},
  shortjournal = {J. Econ.},
  title        = {Likelihood ratio testing in linear state space models: An application to dynamic stochastic general equilibrium models},
  volume       = {218},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inference of local regression in the presence of nuisance
parameters. <em>JOE</em>, <em>218</em>(2), 532–560. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider inference based on local estimating equations in the presence of nuisance parameters . The framework is useful for a number of applications including those in economic policy evaluation based on discontinuities or kinks and in real-time financial risk management . We focus on the criterion-function-based (in particular, empirical likelihood-based) inference, and establish conditions under which the test statistic has a pivotal asymptotic distribution . In the key step of eliminating nuisance parameters in the (possibly non-smooth) criterion function, we consider two different approaches based on either concentration or Laplace-type plug-in estimation. The former is natural, and the latter does not require optimization and can be computationally attractive in applications. Our framework can easily incorporate bias correction induced by localization, and the inference is robust to the identification strength of the parameter of interest. The high-level assumptions are illustrated with several examples. We also conduct Monte Carlo simulations and provide an empirical application which assesses heterogeneous effects of academic probation in college and gender differences under the quantile regression discontinuity design.},
  archive      = {J_JOE},
  author       = {Ke-Li Xu},
  doi          = {10.1016/j.jeconom.2020.04.028},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {532-560},
  shortjournal = {J. Econ.},
  title        = {Inference of local regression in the presence of nuisance parameters},
  volume       = {218},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generic results for establishing the asymptotic size of
confidence sets and tests. <em>JOE</em>, <em>218</em>(2), 496–531. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides a set of results that can be used to establish the asymptotic size and/or similarity in a uniform sense of confidence sets and tests. The results are generic in that they can be applied to a broad range of problems. They are most useful in scenarios where the pointwise asymptotic distribution of a test statistic is a discontinuous function of a parameter. The results are illustrated in several examples. These are: (i) the conditional likelihood ratio test of Moreira (2003) for linear instrumental variables models with instruments that may be weak, extended to the case of heteroskedastic errors; (ii) the grid bootstrap confidence interval of Hansen (1999) for the sum of the AR coefficients in a k k th order autoregressive model with unknown innovation distribution, and (iii) the standard quasi-likelihood ratio test in a nonlinear regression model where identification is lost when the coefficient on the nonlinear regressor is zero. In addition, as a simple running example, we consider a two-sided equal-tailed CI for the AR coefficient in an AR(1) model, which is a simplified version of the CI in Andrews and Guggenberger (2014).},
  archive      = {J_JOE},
  author       = {Donald W.K. Andrews and Xu Cheng and Patrik Guggenberger},
  doi          = {10.1016/j.jeconom.2020.04.027},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {496-531},
  shortjournal = {J. Econ.},
  title        = {Generic results for establishing the asymptotic size of confidence sets and tests},
  volume       = {218},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bootstrapping factor models with cross sectional dependence.
<em>JOE</em>, <em>218</em>(2), 476–495. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider bootstrap methods for factor-augmented regressions with cross sectional dependence among idiosyncratic errors. This is important to capture the bias of the OLS estimator derived recently by Gonçalves and Perron (2014). We first show that a common approach of resampling cross sectional vectors over time is invalid in this context because it induces a zero bias. We then propose the cross-sectional dependent (CSD) bootstrap where bootstrap samples are obtained by taking a random vector and multiplying it by the square root of a consistent estimator of the covariance matrix of the idiosyncratic errors. We show that if the covariance matrix estimator is consistent in the spectral norm, then the CSD bootstrap is consistent, and we verify this condition for the thresholding estimator of Bickel and Levina (2008). Finally, we apply our new bootstrap procedure to forecasting inflation using convenience yields as recently explored by Gospodinov and Ng (2013).},
  archive      = {J_JOE},
  author       = {Sílvia Gonçalves and Benoit Perron},
  doi          = {10.1016/j.jeconom.2020.04.026},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {476-495},
  shortjournal = {J. Econ.},
  title        = {Bootstrapping factor models with cross sectional dependence},
  volume       = {218},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The fast iterated bootstrap. <em>JOE</em>, <em>218</em>(2),
451–475. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The standard forms of bootstrap iteration are very computationally demanding. As a result, there have been several attempts to alleviate the computational burden by use of approximations. In this paper, we extend the fast double bootstrap of Davidson and MacKinnon (2007) to higher orders of iteration, and provide algorithms for their implementation. The new methods make computational demands that increase only linearly with the level of iteration, unlike standard procedures, whose demands increase exponentially. In a series of simulation experiments, we show that the fast triple bootstrap improves on both the standard and fast double bootstraps, in the sense that it suffers from less size distortion under the null with no accompanying loss of power.},
  archive      = {J_JOE},
  author       = {Russell Davidson and Mirza Trokić},
  doi          = {10.1016/j.jeconom.2020.04.025},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {451-475},
  shortjournal = {J. Econ.},
  title        = {The fast iterated bootstrap},
  volume       = {218},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Randomization inference for difference-in-differences with
few treated clusters. <em>JOE</em>, <em>218</em>(2), 435–450. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inference using difference-in-differences with clustered data requires care. Previous research has shown that, when there are few treated clusters, t t -tests based on cluster-robust variance estimators (CRVEs) severely overreject, and different variants of the wild cluster bootstrap can either overreject or underreject dramatically. We study two randomization inference (RI) procedures. A procedure based on estimated coefficients may be unreliable when clusters are heterogeneous. A procedure based on t t -statistics typically performs better (although by no means perfectly) under the null , but at the cost of some power loss. An empirical example demonstrates that RI procedures can yield inferences that differ dramatically from those of other methods.},
  archive      = {J_JOE},
  author       = {James G. MacKinnon and Matthew D. Webb},
  doi          = {10.1016/j.jeconom.2020.04.024},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {435-450},
  shortjournal = {J. Econ.},
  title        = {Randomization inference for difference-in-differences with few treated clusters},
  volume       = {218},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Monte carlo two-stage indirect inference (2SIF) for
autoregressive panels. <em>JOE</em>, <em>218</em>(2), 419–434. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Persistence and mean-nonstationarity often undermine reliability of asymptotically justified inference in dynamic panels. We combine the Monte Carlo test (MCT) and the indirect inference estimation (IIE) principles to construct confidence regions for autoregressive panel parameters with valid coverage whether mean-stationarity is imposed or relaxed and whether autoregressive roots are at or close to the unit boundary or far from unity. Procedures are based on the standard least squares dummy variables (LSDV) estimator and an augmented counterpart which we introduce to restore finite sample exactness in mean-nonstationary settings. We also put forth a ‘Durbin–Wu–Hausman-type’ test for mean-stationarity given a tested autoregressive parameter, based on the distance between the LSDV and its augmented counterpart. Location-scale invariance is shown analytically, and the MCT methods involve multiple stages that preserve exchangeability . The above are formally shown to control size exactly for finite N and T, and provide a new perspective to a literature that is primarily asymptotic. The advantages of the proposed approaches are also illustrated via comparative simulation studies. The identification problems arising from relaxing mean-stationarity are assessed and addressed; we also consider heteroskedastic directions. Results show concrete promise even with very small sample sizes and more broadly, underscore the usefulness of combining MCT and IIE principles.},
  archive      = {J_JOE},
  author       = {Lynda Khalaf and Charles J. Saunders},
  doi          = {10.1016/j.jeconom.2020.04.023},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {419-434},
  shortjournal = {J. Econ.},
  title        = {Monte carlo two-stage indirect inference (2SIF) for autoregressive panels},
  volume       = {218},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exogeneity tests, incomplete models, weak identification and
non-gaussian distributions: Invariance and finite-sample distributional
theory. <em>JOE</em>, <em>218</em>(2), 390–418. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the distribution of Durbin–Wu–Hausman (DWH) and Revankar–Hartley (RH) tests for exogeneity from a finite-sample viewpoint, under the null and alternative hypotheses. We consider linear structural models with possibly non-Gaussian errors, where structural parameters may not be identified and where reduced forms can be incompletely specified (or nonparametric). On level control, we characterize the null distributions of all the test statistics. Through conditioning and invariance arguments, we show that these distributions do not involve nuisance parameters . In particular, this applies to several test statistics for which no finite-sample distributional theory is yet available, such as the standard statistic proposed by Hausman (1978). The distributions of the test statistics may be non-standard – so corrections to usual asymptotic critical values are needed – but the characterizations are sufficiently explicit to yield finite-sample (Monte-Carlo) tests of the exogeneity hypothesis. The procedures so obtained are robust to weak identification, missing instruments or misspecified reduced forms, and can easily be adapted to allow for parametric non-Gaussian error distributions. We give a general invariance result ( block triangular invariance ) for exogeneity test statistics. This property yields a convenient exogeneity canonical form and a parsimonious reduction of the parameters on which power depends. In the extreme case where no structural parameter is identified, the distributions under the alternative hypothesis and the null hypothesis are identical, so the power function is flat, for all the exogeneity statistics. However, as soon as identification does not fail completely, this phenomenon typically disappears. We present simulation experiments which confirm the finite-sample theory. The theoretical results are illustrated with an empirical example : the relation between trade and economic growth.},
  archive      = {J_JOE},
  author       = {Firmin Doko Tchatoka and Jean-Marie Dufour},
  doi          = {10.1016/j.jeconom.2020.04.022},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {390-418},
  shortjournal = {J. Econ.},
  title        = {Exogeneity tests, incomplete models, weak identification and non-gaussian distributions: Invariance and finite-sample distributional theory},
  volume       = {218},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A geometric approach to inference in set-identified entry
games. <em>JOE</em>, <em>218</em>(2), 373–389. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider inference procedures for entry games with complete information. Due to the presence of multiple equilibria, we know that such a model may be set-identified without imposing further restrictions. We complete the model with the unknown selection mechanism and characterize geometrically the set of predicted choice probabilities , in our case, a convex polytope with many facets. Testing whether a parameter belongs to the identified set is equivalent to testing whether the true choice probability vector belongs to this convex set . Using tools from the convex analysis , we calculate the support function and the extreme points. The calculation yields a finite number of inequalities , when the explanatory variables are discrete, and we characterized them once for all. We also propose a procedure that selects the moment inequalities without having to evaluate all of them. This procedure is computationally feasible for any number of players and is based on the geometry of the set. Furthermore, we exploit the specific structure of the test statistic used to test whether a point belongs to a convex set to propose the calculation of critical values that are computed once and independent of the value of the parameter tested, which drastically improves the calculation time. Simulations in a separate section suggest that our procedure performs well compared with existing methods.},
  archive      = {J_JOE},
  author       = {Christian Bontemps and Rohit Kumar},
  doi          = {10.1016/j.jeconom.2020.04.021},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {373-389},
  shortjournal = {J. Econ.},
  title        = {A geometric approach to inference in set-identified entry games},
  volume       = {218},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inference in second-order identified models. <em>JOE</em>,
<em>218</em>(2), 346–372. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore the local power properties of different test statistics for conducting inference in moment condition models that only identify the parameters locally to second order. We consider the conventional Wald and LM statistics, and also the Generalized Anderson–Rubin (GAR) statistic (Anderson and Rubin, 1949; Dufour, 1997; Staiger and Stock, 1997; Stock and Wright, 2000), KLM statistic (Kleibergen, 2002; Kleibergen, 2005) and the GMM extension of Moreira (2003) (GMM-M) conditional likelihood ratio statistic. The GAR, KLM and GMM-M statistics are so-called “identification robust” since their (conditional) limiting distribution is the same under first-order, weak and therefore also second order identification. For inference about the model specification, we consider the identification-robust J statistic (Kleibergen, 2005), and the GAR statistic. Interestingly, we find that the limiting distribution of the Wald statistic under local alternatives not only depends on the distance to the null hypothesis but also on the convergence rate of the Jacobian. We specifically analyse two empirically relevant models with second order identification. In the panel autoregressive model of order one, our analysis indicates that the Wald test of a unit root value of the autoregressive parameter has better power compared to the corresponding GAR test which, in turn, dominates the KLM, GMM-M and LM tests . For the conditionally heteroskedastic factor model, we compare Kleibergen (2005) J and the GAR statistics to Hansen (1982) overidentifying restrictions test (previously analysed in this context by Dovonon and Renault, 2013) and find the power ranking depends on the sample size. Collectively, our results suggest that tests with meaningful power can be conducted in second-order identified models.},
  archive      = {J_JOE},
  author       = {Prosper Dovonon and Alastair R. Hall and Frank Kleibergen},
  doi          = {10.1016/j.jeconom.2020.04.020},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {346-372},
  shortjournal = {J. Econ.},
  title        = {Inference in second-order identified models},
  volume       = {218},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inference in partially identified heteroskedastic
simultaneous equations models. <em>JOE</em>, <em>218</em>(2), 317–345.
(<a href="https://doi.org/10.1016/j.jeconom.2020.04.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identification through heteroskedasticity in heteroskedastic simultaneous equations models (HSEMs) is considered. The possibility that heteroskedasticity identifies structural parameters only partially is explicitly allowed for. The asymptotic properties of the identified parameters are derived. Moreover, tests for identification through heteroskedasticity are developed and their asymptotic distributions are provided. Monte Carlo simulations are used to explore the small sample properties of the asymptotically valid methods. Finally, the approach is applied to investigate the relation between the degree of economic openness and inflation .},
  archive      = {J_JOE},
  author       = {Helmut Lütkepohl and George Milunovich and Minxian Yang},
  doi          = {10.1016/j.jeconom.2020.04.019},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {317-345},
  shortjournal = {J. Econ.},
  title        = {Inference in partially identified heteroskedastic simultaneous equations models},
  volume       = {218},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Testing the impossible: Identifying exclusion restrictions.
<em>JOE</em>, <em>218</em>(2), 294–316. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Method of moments estimation usually implies exploiting presumed uncorrelatedness of model disturbances and identifying instrumental variables . Here, utilizing non-orthogonality conditions is examined for linear multiple cross-section simultaneous regression models. Employing flexible bounds on the correlations between disturbances and regressors one avoids: (i) adoption of often incredible and unverifiable strictly zero correlation assumptions, and (ii) imprecise inference due to possibly weak or invalid external instruments. The suggested alternative form of inference is within constraints endogeneity robust; its asymptotic validity is proved and its accuracy in finite samples demonstrated by simulation. Next to offering an attractive alternative as such, it permits a sensitivity analysis of inference based on orthogonality conditions. Moreover, it yields statistical inference on the validity of exclusion restrictions regarding candidate external instruments, whereas these unavoidable restrictions were always supposed to be non-testable. The practical relevance is illustrated in a few applications borrowed from the textbook literature.},
  archive      = {J_JOE},
  author       = {Jan F. Kiviet},
  doi          = {10.1016/j.jeconom.2020.04.018},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {294-316},
  shortjournal = {J. Econ.},
  title        = {Testing the impossible: Identifying exclusion restrictions},
  volume       = {218},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Testing identification strength. <em>JOE</em>,
<em>218</em>(2), 271–293. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider models defined by a set of moment restrictions that may be subject to weak identification. We propose a testing procedure to assess whether instruments are “too weak” for standard (Gaussian) asymptotic theory to be reliable. Since the validity of standard asymptotics for GMM rests upon a Taylor expansion of the first order conditions, we distinguish two cases: (i) models that are either linear or separable in the parameters of interest; (ii) general models that are neither linear nor separable. Our testing procedure is similar in both cases, but our null hypothesis of weak identification for a nonlinear model is broader than the popular one. Our test is straightforward to apply and allows to test the null hypothesis of weak identification of specific subvectors without assuming identification of the components not under test. In the linear case, it can be seen as a generalization of the popular first-stage F-test but allows us to fix its shortcomings in case of heteroskedasticity. In simulations, our test is well behaved when compared to contenders, both in terms of size and power. In particular, the focus on subvectors allows us to have power to reject the null of weak identification on some components of interest. This observation may explain why, when applied to the estimation of the Elasticity of Intertemporal Substitution, our test is the only one to find matching results for every country under the two symmetric popular specifications: the intercept parameter is always found strongly identified, whereas the slope parameter is always found weakly identified.},
  archive      = {J_JOE},
  author       = {Bertille Antoine and Eric Renault},
  doi          = {10.1016/j.jeconom.2020.04.017},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {271-293},
  shortjournal = {J. Econ.},
  title        = {Testing identification strength},
  volume       = {218},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Impossible inference in econometrics: Theory and
applications. <em>JOE</em>, <em>218</em>(2), 247–270. (<a
href="https://doi.org/10.1016/j.jeconom.2020.04.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies models in which hypothesis tests have trivial power, that is, power smaller than size. This testing impossibility, or impossibility type A, arises when any alternative is not distinguishable from the null . We also study settings in which it is impossible to have almost surely bounded confidence sets for a parameter of interest. This second type of impossibility (type B) occurs under a condition weaker than the condition for type A impossibility: the parameter of interest must be nearly unidentified . Our theoretical framework connects many existing publications on impossible inference that rely on different notions of topologies to show models are not distinguishable or nearly unidentified. We also derive both types of impossibility using the weak topology induced by convergence in distribution . Impossibility in the weak topology is often easier to prove, it is applicable for many widely-used tests, and it is useful for robust hypothesis testing . We conclude by demonstrating impossible inference in multiple economic applications of models with discontinuity and time-series models.},
  archive      = {J_JOE},
  author       = {Marinho Bertanha and Marcelo J. Moreira},
  doi          = {10.1016/j.jeconom.2020.04.016},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {247-270},
  shortjournal = {J. Econ.},
  title        = {Impossible inference in econometrics: Theory and applications},
  volume       = {218},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Editors’ introduction: Special issue in honor of jean-marie
dufour on identification, inference, and causality. <em>JOE</em>,
<em>218</em>(2), 243–246. (<a
href="https://doi.org/10.1016/j.jeconom.2020.07.040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOE},
  author       = {Marine Carrasco and Marcelo Moreira and Benoit Perron and Victoria Zinde-Walsh},
  doi          = {10.1016/j.jeconom.2020.07.040},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {243-246},
  shortjournal = {J. Econ.},
  title        = {Editors’ introduction: Special issue in honor of jean-marie dufour on identification, inference, and causality},
  volume       = {218},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Regression discontinuity design with many thresholds.
<em>JOE</em>, <em>218</em>(1), 216–241. (<a
href="https://doi.org/10.1016/j.jeconom.2019.09.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerous empirical studies employ regression discontinuity designs with multiple cutoffs and heterogeneous treatments. A common practice is to normalize all the cutoffs to zero and estimate one effect. This procedure identifies the average treatment effect (ATE) on the observed distribution of individuals local to existing cutoffs. However, researchers often want to make inferences on more meaningful ATEs, computed over general counterfactual distributions of individuals, rather than simply the observed distribution of individuals local to existing cutoffs. This paper proposes a consistent and asymptotically normal estimator for such ATEs when heterogeneity follows a non-parametric function of cutoff characteristics in the sharp case. The proposed estimator converges at the minimax optimal rate of root- n n for a specific choice of tuning parameters. Identification in the fuzzy case, with multiple cutoffs, is impossible unless heterogeneity follows a finite-dimensional function of cutoff characteristics. Under parametric heterogeneity, this paper proposes an ATE estimator for the fuzzy case that optimally combines observations to maximize its precision.},
  archive      = {J_JOE},
  author       = {Marinho Bertanha},
  doi          = {10.1016/j.jeconom.2019.09.010},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {216-241},
  shortjournal = {J. Econ.},
  title        = {Regression discontinuity design with many thresholds},
  volume       = {218},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the unbiased asymptotic normality of quantile regression
with fixed effects. <em>JOE</em>, <em>218</em>(1), 178–215. (<a
href="https://doi.org/10.1016/j.jeconom.2019.12.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonlinear panel data models with fixed individual effects provide an important set of tools for describing microeconometric data. In a large class of such models (including probit, proportional hazard and quantile regression to name just a few) it is impossible to difference out the individual effects, and inference is usually justified in a ‘large n n large T T ’ asymptotic framework. However, there is a considerable gap in the type of assumptions that are currently imposed in models with smooth score functions (such as probit, and proportional hazard) and quantile regression . In the present paper we show that this gap can be bridged and establish unbiased asymptotic normality for fixed effects quantile regression panels under conditions on n , T n,T that are very close to what is typically assumed in standard nonlinear panels. Our results considerably improve upon existing theory and show that quantile regression is applicable to the same type of panel data (in terms of n , T n,T ) as other commonly used nonlinear panel data models . Thorough numerical experiments confirm our theoretical findings.},
  archive      = {J_JOE},
  author       = {Antonio F. Galvao and Jiaying Gu and Stanislav Volgushev},
  doi          = {10.1016/j.jeconom.2019.12.017},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {178-215},
  shortjournal = {J. Econ.},
  title        = {On the unbiased asymptotic normality of quantile regression with fixed effects},
  volume       = {218},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Asymptotic f tests under possibly weak identification.
<em>JOE</em>, <em>218</em>(1), 140–177. (<a
href="https://doi.org/10.1016/j.jeconom.2019.10.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops asymptotic F tests robust to weak identification and temporal dependence. The test statistics we focus on are modified versions of the S statistic of Stock and Wright (2000) and the K statistic of Kleibergen (2005). In the former case, the modification involves only a multiplicative degree-of-freedom adjustment, and the modified S statistic is asymptotically F distributed under fixed-smoothing asymptotics regardless of the strength of the model identification. In the latter case, the modification involves an additional multiplicative adjustment that uses a J statistic for testing overidentification. We show that the modified K statistic is asymptotically F-distributed when the model parameters are completely unidentified or nearly-weakly identified. When the model parameters are weakly identified, the F approximation for the K statistic can be justified under the conventional asymptotics. The F approximations account for the estimation errors in the underlying heteroskedasticity and autocorrelation robust variance estimators , which the chi-squared approximations ignore. Monte Carlo simulations show that the F approximations are much more accurate than the corresponding chi-squared approximations in finite samples.},
  archive      = {J_JOE},
  author       = {Julián Martínez-Iriarte and Yixiao Sun and Xuexin Wang},
  doi          = {10.1016/j.jeconom.2019.10.011},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {140-177},
  shortjournal = {J. Econ.},
  title        = {Asymptotic f tests under possibly weak identification},
  volume       = {218},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A projection-based conditional dependence measure with
applications to high-dimensional undirected graphical models.
<em>JOE</em>, <em>218</em>(1), 119–139. (<a
href="https://doi.org/10.1016/j.jeconom.2019.12.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measuring conditional dependence is an important topic in econometrics with broad applications including graphical models . Under a factor model setting, a new conditional dependence measure based on projection is proposed. The corresponding conditional independence test is developed with the asymptotic null distribution unveiled where the number of factors could be high-dimensional. It is also shown that the new test has control over the asymptotic type I error and can be calculated efficiently. A generic method for building dependency graphs without Gaussian assumption using the new test is elaborated. We show the superiority of the new method, implemented in the R package pgraph pgraph , through simulation and real data studies.},
  archive      = {J_JOE},
  author       = {Jianqing Fan and Yang Feng and Lucy Xia},
  doi          = {10.1016/j.jeconom.2019.12.016},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {119-139},
  shortjournal = {J. Econ.},
  title        = {A projection-based conditional dependence measure with applications to high-dimensional undirected graphical models},
  volume       = {218},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reducing the state space dimension in a large TVP-VAR.
<em>JOE</em>, <em>218</em>(1), 105–118. (<a
href="https://doi.org/10.1016/j.jeconom.2019.11.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new approach to estimating high dimensional time varying parameter structural vector autoregressive models (TVP-SVARs) by taking advantage of an empirical feature of TVP-(S)VARs. TVP-(S)VAR models are rarely used with more than 4–5 variables. However recent work has shown the advantages of modelling VARs with large numbers of variables and interest has naturally increased in modelling large dimensional TVP-VARs. A feature that has not yet been utilized is that the covariance matrix for the state equation, when estimated freely, is often near singular. We propose a specification that uses this singularity to develop a factor-like structure to estimate a TVP-SVAR for many variables. Using a generalization of the recentering approach, a rank reduced state covariance matrix and judicious parameter expansions, we obtain efficient and simple computation of a high dimensional TVP-SVAR. An advantage of our approach is that we retain a formal inferential framework such that we can propose formal inference on impulse responses, variance decompositions and, important for our model, the rank of the state equation covariance matrix. In a system with 15 variables, we show clear empirical evidence in favour of our model and improvements in estimates of impulse responses.},
  archive      = {J_JOE},
  author       = {Joshua C.C. Chan and Eric Eisenstat and Rodney W. Strachan},
  doi          = {10.1016/j.jeconom.2019.11.006},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {105-118},
  shortjournal = {J. Econ.},
  title        = {Reducing the state space dimension in a large TVP-VAR},
  volume       = {218},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spatial dynamic models with intertemporal optimization:
Specification and estimation. <em>JOE</em>, <em>218</em>(1), 82–104. (<a
href="https://doi.org/10.1016/j.jeconom.2019.10.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a dynamic spatial interaction econometric model . There are n forward-looking agents of whom each has a parametric linear-quadratic payoff, and they interact with their neighbors through a spatial network. Considering a Markov perfect equilibrium (MPE), we derive a unique Nash equilibrium equation and construct a new spatial dynamic panel data (SDPD) model. For estimation, we suggest mainly the quasi-maximum likelihood (QML) method. Asymptotic properties of the QML estimator are investigated. In a Monte Carlo study , we estimate the model’s parameters and compare the results with those from traditional SDPD models. The model is applied to an empirical study on counties’ public safety spending in North Carolina.},
  archive      = {J_JOE},
  author       = {Hanbat Jeong and Lung-fei Lee},
  doi          = {10.1016/j.jeconom.2019.10.012},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {82-104},
  shortjournal = {J. Econ.},
  title        = {Spatial dynamic models with intertemporal optimization: Specification and estimation},
  volume       = {218},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Statistical inferences for price staleness. <em>JOE</em>,
<em>218</em>(1), 32–81. (<a
href="https://doi.org/10.1016/j.jeconom.2020.01.021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a nonparametric theory for statistical inferences on zero returns of high-frequency asset prices. Using an infill asymptotic design, we derive limit theorems for the percentage of zero returns observed on a finite time interval and for other related quantities. Within this framework, we develop two nonparametric tests . First, we test whether intra-day zero returns are independent and identically distributed. Second, we test whether intra-day variation of the likelihood of occurrence of zero returns can be solely explained by a deterministic diurnal pattern. In an empirical application to ten representative stocks of the NYSE , we provide evidence that the null of independent and identically distributed intra-day zero returns can be conclusively rejected. We further find that a deterministic diurnal pattern is not sufficient to explain the intra-day variability of the distribution of zero returns.},
  archive      = {J_JOE},
  author       = {Aleksey Kolokolov and Giulia Livieri and Davide Pirino},
  doi          = {10.1016/j.jeconom.2020.01.021},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {32-81},
  shortjournal = {J. Econ.},
  title        = {Statistical inferences for price staleness},
  volume       = {218},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating latent asset-pricing factors. <em>JOE</em>,
<em>218</em>(1), 1–31. (<a
href="https://doi.org/10.1016/j.jeconom.2019.08.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop an estimator for latent factors in a large-dimensional panel of financial data that can explain expected excess returns. Statistical factor analysis based on Principal Component Analysis (PCA) has problems identifying factors with a small variance that are important for asset pricing. We generalize PCA with a penalty term accounting for the pricing error in expected returns. Our estimator searches for factors that can explain both the expected return and covariance structure . We derive the statistical properties of the new estimator and show that our estimator can find asset-pricing factors, which cannot be detected with PCA, even if a large amount of data is available. Applying the approach to portfolio data we find factors with Sharpe-ratios more than twice as large as those based on conventional PCA and with smaller pricing errors.},
  archive      = {J_JOE},
  author       = {Martin Lettau and Markus Pelger},
  doi          = {10.1016/j.jeconom.2019.08.012},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {1-31},
  shortjournal = {J. Econ.},
  title        = {Estimating latent asset-pricing factors},
  volume       = {218},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonlinearities and regimes in conditional correlations with
different dynamics. <em>JOE</em>, <em>217</em>(2), 496–522. (<a
href="https://doi.org/10.1016/j.jeconom.2019.12.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {New parameterizations of the dynamic conditional correlation (DCC) model and of the regime-switching dynamic correlation (RSDC) model are introduced, such that these models provide a specific dynamics for each correlation. They imply a nonlinear autoregressive form of dependence on lagged correlations and are based on properties of the Hadamard exponential matrix . The new models are applied to a data set of twenty stock market indices and a data set of the thirty Dow Jones components, comparing them to the classical DCC and RSDC models. The empirical results show that the new models improve their classical versions in terms of several criteria.},
  archive      = {J_JOE},
  author       = {Luc Bauwens and Edoardo Otranto},
  doi          = {10.1016/j.jeconom.2019.12.014},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {496-522},
  shortjournal = {J. Econ.},
  title        = {Nonlinearities and regimes in conditional correlations with different dynamics},
  volume       = {217},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Incorporating overnight and intraday returns into
multivariate GARCH volatility models. <em>JOE</em>, <em>217</em>(2),
471–495. (<a
href="https://doi.org/10.1016/j.jeconom.2019.12.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose and evaluate mixed-frequency multivariate GARCH models for forecasting low-frequency (weekly) volatility based on high-frequency intraday returns (at 5-minute intervals) and on the overnight returns. The low-frequency conditional volatility matrix is modeled as a weighted sum of an intraday and an overnight component. The components are specified as multivariate GARCH processes of the BEKK type, adapted to the mixed-frequency data setting, and may enter the model as two separate components or as a single one. The models may further be extended by a nonparametrically estimated slowly-varying long-run volatility matrix. We evaluate the models in and out of sample using the 5-minute and overnight returns on four DJIA stocks (AXP, GE, HD, and IBM) from January 1988 to November 2014 and find that they systematically dominate a variety of models that only use lower-frequency data (weekly, daily, or close-to-open and open-to-close returns).},
  archive      = {J_JOE},
  author       = {Geert Dhaene and Jianbin Wu},
  doi          = {10.1016/j.jeconom.2019.12.013},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {471-495},
  shortjournal = {J. Econ.},
  title        = {Incorporating overnight and intraday returns into multivariate GARCH volatility models},
  volume       = {217},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation of a multiplicative correlation structure in the
large dimensional case. <em>JOE</em>, <em>217</em>(2), 431–470. (<a
href="https://doi.org/10.1016/j.jeconom.2019.12.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a Kronecker product model for correlation or covariance matrices in the large dimensional case. The number of parameters of the model increases logarithmically with the dimension of the matrix. We propose a minimum distance (MD) estimator based on a log-linear property of the model, as well as a one-step estimator, which is a one-step approximation to the quasi-maximum likelihood estimator (QMLE). We establish rates of convergence and central limit theorems (CLT) for our estimators in the large dimensional case. A specification test and tools for Kronecker product model selection and inference are provided. In a Monte Carlo study where a Kronecker product model is correctly specified, our estimators exhibit superior performance. In an empirical application to portfolio choice for S&amp;P500 daily returns, we demonstrate that certain Kronecker product models are good approximations to the general covariance matrix.},
  archive      = {J_JOE},
  author       = {Christian M. Hafner and Oliver B. Linton and Haihan Tang},
  doi          = {10.1016/j.jeconom.2019.12.012},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {431-470},
  shortjournal = {J. Econ.},
  title        = {Estimation of a multiplicative correlation structure in the large dimensional case},
  volume       = {217},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multivariate leverage effects and realized semicovariance
GARCH models. <em>JOE</em>, <em>217</em>(2), 411–430. (<a
href="https://doi.org/10.1016/j.jeconom.2019.12.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose new asymmetric multivariate volatility models. The models exploit estimates of variances and covariances based on the signs of high-frequency returns, measures known as realized semivariances, semicovariances, and semicorrelations, to allow for more nuanced responses to positive and negative return shocks than threshold “leverage effect” terms traditionally used in the literature. Our empirical implementations of the new models, including extensions of widely-used bivariate GARCH specifications for a number of individual stocks and the aggregate market portfolio as well as larger dimensional dynamic conditional correlation type formulations for a cross-section of individual stocks, provide clear evidence of improved model fit and reveal new and interesting asymmetric joint dynamic dependencies.},
  archive      = {J_JOE},
  author       = {Tim Bollerslev and Andrew J. Patton and Rogier Quaedvlieg},
  doi          = {10.1016/j.jeconom.2019.12.011},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {411-430},
  shortjournal = {J. Econ.},
  title        = {Multivariate leverage effects and realized semicovariance GARCH models},
  volume       = {217},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Flexible multivariate hill estimators. <em>JOE</em>,
<em>217</em>(2), 398–410. (<a
href="https://doi.org/10.1016/j.jeconom.2019.12.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dominicy et al. (2017) introduce a family of Hill estimators for elliptically distributed and heavy tailed random vectors. They propose to use the univariate Hill to a norm of order h of the data. The norms are homogeneous functions of order one. We show that the family of estimators can be generalized to homogeneous functions of any order and, more importantly, that ellipticity is not required. Only multivariate regular variation is needed, as it is preserved under well-behaved homogeneous functions. This enables us to have flexibility in terms of the estimator and the underlying distribution. Consistency and asymptotic normality are shown, and a Monte Carlo study is conducted to assess the finite sample properties under different asymmetric and heavy tailed multivariate distributions . We illustrate the estimators with an application to 10 years of daily data of paid claims from property insurance policies across 15 regions of Belgium.},
  archive      = {J_JOE},
  author       = {Yves Dominicy and Matias Heikkilä and Pauliina Ilmonen and David Veredas},
  doi          = {10.1016/j.jeconom.2019.12.010},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {398-410},
  shortjournal = {J. Econ.},
  title        = {Flexible multivariate hill estimators},
  volume       = {217},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nearest comoment estimation with unobserved factors.
<em>JOE</em>, <em>217</em>(2), 381–397. (<a
href="https://doi.org/10.1016/j.jeconom.2019.12.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a minimum distance estimator for the higher-order comoments of a multivariate distribution exhibiting a lower dimensional latent factor structure. We derive the influence function of the proposed estimator and prove its consistency and asymptotic normality . The simulation study confirms the large gains in accuracy compared to the traditional sample comoments. The empirical usefulness of the novel framework is shown in applications to portfolio allocation under non-Gaussian objective functions and to the extraction of factor loadings in a dataset with mental ability scores.},
  archive      = {J_JOE},
  author       = {Kris Boudt and Dries Cornilly and Tim Verdonck},
  doi          = {10.1016/j.jeconom.2019.12.009},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {381-397},
  shortjournal = {J. Econ.},
  title        = {Nearest comoment estimation with unobserved factors},
  volume       = {217},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Virtual historical simulation for estimating the conditional
VaR of large portfolios. <em>JOE</em>, <em>217</em>(2), 356–380. (<a
href="https://doi.org/10.1016/j.jeconom.2019.12.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to estimate the conditional risk of a portfolio’s return, two strategies can be advocated. A multivariate strategy requires estimating a dynamic model for the vector of risk factors, which is often challenging, when at all possible, for large portfolios. A univariate approach based on a dynamic model for the portfolio’s return seems more attractive. However, when the combination of the individual returns is time varying, the portfolio’s return series is typically non stationary which may invalidate statistical inference . An alternative approach consists in reconstituting a ”virtual portfolio”, whose returns are built using the current composition of the portfolio and for which a stationary dynamic model can be estimated. This paper establishes the asymptotic properties of this method, that we call Virtual Historical Simulation. Numerical illustrations on simulated and real data are provided.},
  archive      = {J_JOE},
  author       = {Christian Francq and Jean-Michel Zakoïan},
  doi          = {10.1016/j.jeconom.2019.12.008},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {356-380},
  shortjournal = {J. Econ.},
  title        = {Virtual historical simulation for estimating the conditional VaR of large portfolios},
  volume       = {217},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Partially censored posterior for robust and efficient risk
evaluation. <em>JOE</em>, <em>217</em>(2), 335–355. (<a
href="https://doi.org/10.1016/j.jeconom.2019.12.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel approach to inference for a specific region of the predictive distribution is introduced. An important domain of application is accurate prediction of financial risk measures, where the area of interest is the left tail of the predictive density of logreturns. Our proposed approach originates from the Bayesian approach to parameter estimation and time series forecasting, however it is robust in the sense that it provides a more accurate estimation of the predictive density in the region of interest in case of misspecification. The first main contribution of the paper is the novel concept of the Partially Censored Posterior (PCP), where the set of model parameters is partitioned into two subsets: for the first subset of parameters we consider the standard marginal posterior , for the second subset of parameters (that are particularly related to the region of interest) we consider the conditional censored posterior. The censoring means that observations outside the region of interest are censored: for those observations only the probability of being outside the region of interest matters. This quasi-Bayesian approach yields more precise parameter estimation than a fully censored posterior for all parameters, and has more focus on the region of interest than a standard Bayesian approach. The second main contribution is that we introduce two novel methods for computationally efficient simulation: Conditional MitISEM, a Markov chain Monte Carlo method to simulate model parameters from the Partially Censored Posterior, and PCP-QERMit, an Importance Sampling method that is introduced to further decrease the numerical standard errors of the Value-at-Risk and Expected Shortfall estimators. The third main contribution is that we consider the effect of using a time-varying boundary of the region of interest. Extensive simulation and empirical studies show the ability of the introduced method to outperform standard approaches.},
  archive      = {J_JOE},
  author       = {Agnieszka Borowska and Lennart Hoogerheide and Siem Jan Koopman and Herman K. van Dijk},
  doi          = {10.1016/j.jeconom.2019.12.007},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {335-355},
  shortjournal = {J. Econ.},
  title        = {Partially censored posterior for robust and efficient risk evaluation},
  volume       = {217},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamics of variance risk premia: A new model for
disentangling the price of risk. <em>JOE</em>, <em>217</em>(2), 312–334.
(<a href="https://doi.org/10.1016/j.jeconom.2019.12.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper formulates a new dynamic model for the variance risk premium based on a state space representation of a bivariate system for the observable ex-post realized variance and the ex-ante option implied variance expectation. A regime switching structure accommodates for periods of unusually high volatility, heterogeneous dynamics and changes in the dependence between the latent states. The model allows separating the continuous component of the variance risk premium from the impact of jumps on option implied variance expectations. Using options and high frequency returns for the S&amp;P500 index, we explain what is generating return predictability by disentangling the part of the variance risk premium associated with normal sized price fluctuations from that associated with tail events. The latter component predicts to a significant extent, and asymmetrically with respect to their sign, future market return variations.},
  archive      = {J_JOE},
  author       = {Jeroen V.K. Rombouts and Lars Stentoft and Francesco Violante},
  doi          = {10.1016/j.jeconom.2019.12.006},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {312-334},
  shortjournal = {J. Econ.},
  title        = {Dynamics of variance risk premia: A new model for disentangling the price of risk},
  volume       = {217},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spanning tests for markowitz stochastic dominance.
<em>JOE</em>, <em>217</em>(2), 291–311. (<a
href="https://doi.org/10.1016/j.jeconom.2019.12.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive properties of the cdf of random variables defined as saddle-type points of real valued continuous stochastic processes. This facilitates the derivation of the first-order asymptotic properties of tests for stochastic spanning given some stochastic dominance relation. We define the concept of Markowitz stochastic dominance spanning, and develop an analytical representation of the spanning property. We construct a non-parametric test for spanning based on subsampling, and derive its asymptotic exactness and consistency. The spanning methodology determines whether introducing new securities or relaxing investment constraints improves the investment opportunity set of investors driven by Markowitz stochastic dominance. In an application to standard datasets of historical stock market returns, we reject market portfolio Markowitz efficiency as well as two-fund separation. Hence, we find evidence that equity management through base assets can outperform the market, for investors with Markowitz type preferences.},
  archive      = {J_JOE},
  author       = {Stelios Arvanitis and Olivier Scaillet and Nikolas Topaloglou},
  doi          = {10.1016/j.jeconom.2019.12.005},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {291-311},
  shortjournal = {J. Econ.},
  title        = {Spanning tests for markowitz stochastic dominance},
  volume       = {217},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Volatility estimation and jump detection for drift–diffusion
processes. <em>JOE</em>, <em>217</em>(2), 259–290. (<a
href="https://doi.org/10.1016/j.jeconom.2019.12.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The logarithmic prices of financial assets are conventionally assumed to follow a drift–diffusion process. While the drift term is typically ignored in the infill asymptotic theory and applications, the presence of temporary nonzero drifts is an undeniable fact. The finite sample theory for integrated variance estimators and extensive simulations provided in this paper reveal that the drift component has a nonnegligible impact on the estimation accuracy of volatility, which leads to a dramatic power loss for a class of jump identification procedures. We propose an alternative construction of volatility estimators and observe significant improvement in the estimation accuracy in the presence of nonnegligible drift. The analytical formulas of the finite sample bias of the realized variance, bipower variation, and their modified versions take simple and intuitive forms. The new jump tests, which are constructed from the modified volatility estimators, show satisfactory performance. As an illustration, we apply the new volatility estimators and jump tests, along with their original versions, to 21 years of 5-minute log returns of the NASDAQ stock price index.},
  archive      = {J_JOE},
  author       = {Sébastien Laurent and Shuping Shi},
  doi          = {10.1016/j.jeconom.2019.12.004},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {259-290},
  shortjournal = {J. Econ.},
  title        = {Volatility estimation and jump detection for drift–diffusion processes},
  volume       = {217},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The leverage effect puzzle revisited: Identification in
discrete time. <em>JOE</em>, <em>217</em>(2), 230–258. (<a
href="https://doi.org/10.1016/j.jeconom.2019.12.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The term “leverage effect,” as coined by Black (1976), refers to the tendency of an asset’s volatility to be negatively correlated with the asset’s return. Aït-Sahalia et al. (2013) refer to the “leverage effect puzzle” as the fact that, in spite of a broad agreement that the effect should be present, it is hard to identify empirically. For this purpose, we propose an extension with leverage effect of the discrete time stochastic volatility model of Darolles et al. (2006). This extension is shown to be the natural discrete time analog of the Heston (1993) option pricing model. It shares with Heston (1993) the advantage of structure preserving change of measure: with an exponentially affine stochastic discount factor, the historical and the risk neutral models belong to the same family of joint probability distributions for return and volatility processes. The discrete time approach allows us to make the role of various parameters more transparent: leverage versus volatility feedback effect, connection with daily realized volatility, impact of leverage on the volatility smile, etc. Even more importantly it sheds some new light on the identification of leverage effect and of the various risk premium parameters through link functions in closed form. The price of volatility risk is identified from underlying asset return data, even without option price data, if and only if leverage effect is present. However, the link functions are almost flat if the leverage effect is close to zero, making estimation of the volatility risk price difficult and paving the way for identification robust inference.},
  archive      = {J_JOE},
  author       = {Hyojin Han and Stanislav Khrapov and Eric Renault},
  doi          = {10.1016/j.jeconom.2019.12.003},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {230-258},
  shortjournal = {J. Econ.},
  title        = {The leverage effect puzzle revisited: Identification in discrete time},
  volume       = {217},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Liquidity and volatility in the u.s. Treasury market.
<em>JOE</em>, <em>217</em>(2), 207–229. (<a
href="https://doi.org/10.1016/j.jeconom.2019.12.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We model the joint dynamics of intraday liquidity, volume, and volatility in the U.S. Treasury market, especially through the 2007–09 financial crisis and around important economic announcements. Using various specifications based on Bauwens and Giot (2000)’s Log-ACD(1,1) model, we find that liquidity, volume, and volatility are highly persistent, with volatility having a lower short-term persistence than the other two. Market liquidity and volume are important to explaining volatility dynamics but not vice versa. In addition, market dynamics change during the financial crisis, with all variables exhibiting increased responsiveness to their most recent realizations. Our models also reveal different market dynamics around announcements. Finally, we introduce new measures of liquidity risk that are useful for continually monitoring liquidity conditions and the risk of liquidity stress in the market.},
  archive      = {J_JOE},
  author       = {Giang Nguyen and Robert Engle and Michael Fleming and Eric Ghysels},
  doi          = {10.1016/j.jeconom.2019.12.002},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {207-229},
  shortjournal = {J. Econ.},
  title        = {Liquidity and volatility in the U.S. treasury market},
  volume       = {217},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonlinear financial econometrics JoE special issue
introduction. <em>JOE</em>, <em>217</em>(2), 203–206. (<a
href="https://doi.org/10.1016/j.jeconom.2019.12.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOE},
  author       = {Jeroen V.K. Rombouts and Olivier Scaillet and David Veredas and Jean-Michel Zakoian},
  doi          = {10.1016/j.jeconom.2019.12.001},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {203-206},
  shortjournal = {J. Econ.},
  title        = {Nonlinear financial econometrics JoE special issue introduction},
  volume       = {217},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A coupled component DCS-EGARCH model for intraday and
overnight volatility. <em>JOE</em>, <em>217</em>(1), 176–201. (<a
href="https://doi.org/10.1016/j.jeconom.2019.12.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a semi-parametric coupled component exponential GARCH model for intraday and overnight returns that allows the two series to have different dynamical properties. We adopt a dynamic conditional score model with t-distributed innovations that captures the very heavy tails of overnight returns. We propose a several-step estimation procedure that captures the nonparametric slowly moving components by kernel estimation and the dynamic parameters by maximum likelihood. We establish the consistency, asymptotic normality , and semiparametric efficiency of our semiparametric estimation procedures. We extend the modelling to the multivariate case where we allow time varying correlation between stocks. We apply our model to the study of Dow Jones industrial average component stocks and CRSP size-based portfolios over the period 1993–2017. We show that the ratio of overnight to intraday volatility has actually increased in importance for Dow Jones stocks during the last two decades. This ratio has also increased for large stocks in the CRSP database, but decreased for small stocks in CRSP.},
  archive      = {J_JOE},
  author       = {Oliver Linton and Jianbin Wu},
  doi          = {10.1016/j.jeconom.2019.12.015},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {176-201},
  shortjournal = {J. Econ.},
  title        = {A coupled component DCS-EGARCH model for intraday and overnight volatility},
  volume       = {217},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Posterior distribution of nondifferentiable functions.
<em>JOE</em>, <em>217</em>(1), 161–175. (<a
href="https://doi.org/10.1016/j.jeconom.2019.10.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper examines the asymptotic behavior of the posterior distribution of a possibly nondifferentiable function g ( θ ) g(θ) , where θ θ is a finite-dimensional parameter of either a parametric or semiparametric model. The main assumption is that the distribution of a suitable estimator θ ̂ n θ̂n , its bootstrap approximation, and the Bayesian posterior for θ θ all agree asymptotically. It is shown that whenever g g is locally Lipschitz, though not necessarily differentiable, the posterior distribution of g ( θ ) g(θ) and the bootstrap distribution of g ( θ ̂ n ) g(θ̂n) coincide asymptotically. One implication is that Bayesians can interpret bootstrap inference for g ( θ ) g(θ) as approximately valid posterior inference in a large sample. Another implication—built on known results about bootstrap inconsistency—is that credible intervals for a nondifferentiable parameter g ( θ ) g(θ) cannot be presumed to be approximately valid confidence intervals (even when this relation holds true for θ θ ).},
  archive      = {J_JOE},
  author       = {Toru Kitagawa and José Luis Montiel Olea and Jonathan Payne and Amilcar Velez},
  doi          = {10.1016/j.jeconom.2019.10.009},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {161-175},
  shortjournal = {J. Econ.},
  title        = {Posterior distribution of nondifferentiable functions},
  volume       = {217},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Frequency domain estimation of cointegrating vectors with
mixed frequency and mixed sample data. <em>JOE</em>, <em>217</em>(1),
140–160. (<a
href="https://doi.org/10.1016/j.jeconom.2019.10.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a simple method for exploiting the information contained in mixed frequency and mixed sample data in the estimation of cointegrating vectors. The asymptotic properties of easy-to-compute spectral regression estimators of the cointegrating vectors are derived and these estimators are shown to belong to the class of optimal cointegration estimators. Furthermore, Wald statistics based on these estimators have asymptotic chi-square distributions which enable inferences to be made straightforwardly. Simulation experiments suggest that the spectral regression estimators considered perform well in finite samples and are at least as good as time domain fully modified estimators. The finite sample size and power properties of the spectral regression-based Wald statistic are also found to be good.},
  archive      = {J_JOE},
  author       = {Marcus J. Chambers},
  doi          = {10.1016/j.jeconom.2019.10.010},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {140-160},
  shortjournal = {J. Econ.},
  title        = {Frequency domain estimation of cointegrating vectors with mixed frequency and mixed sample data},
  volume       = {217},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparametric analysis of a duration model with stochastic
unobserved heterogeneity. <em>JOE</em>, <em>217</em>(1), 112–139. (<a
href="https://doi.org/10.1016/j.jeconom.2019.06.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops nonparametric identification and estimation results for a single-spell hazard model, where the unobserved heterogeneity is specified as a Lévy subordinator. The identification approach solves a nonlinear Volterra integral equation of the first kind with an unknown kernel function. Both the kernel of the integral operator, which models the distribution of the unobserved heterogeneity, and the functions that enter it are identified given regularity conditions and minimal variation in the observed covariates . The paper proposes a shape-constrained nonparametric two-step sieve minimum distance estimator. Rates of convergence are derived and Monte Carlo experiments show the finite sample performance of the estimator.},
  archive      = {J_JOE},
  author       = {Irene Botosaru},
  doi          = {10.1016/j.jeconom.2019.06.006},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {112-139},
  shortjournal = {J. Econ.},
  title        = {Nonparametric analysis of a duration model with stochastic unobserved heterogeneity},
  volume       = {217},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inference for high-dimensional instrumental variables
regression. <em>JOE</em>, <em>217</em>(1), 79–111. (<a
href="https://doi.org/10.1016/j.jeconom.2019.09.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper concerns statistical inference for the components of a high-dimensional regression parameter despite possible endogeneity of each regressor . Given a first-stage linear model for the endogenous regressors and a second-stage linear model for the dependent variable, we develop a novel adaptation of the parametric one-step update to a generic second-stage estimator. We provide conditions under which the scaled update is asymptotically normal. We then introduce a two-stage Lasso procedure and show that the second-stage Lasso estimator satisfies the aforementioned conditions. Using these results, we construct asymptotically valid confidence intervals for the components of the second-stage regression coefficients . We complement our asymptotic theory with simulation studies, which demonstrate the performance of our method in finite samples.},
  archive      = {J_JOE},
  author       = {David Gold and Johannes Lederer and Jing Tao},
  doi          = {10.1016/j.jeconom.2019.09.009},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {79-111},
  shortjournal = {J. Econ.},
  title        = {Inference for high-dimensional instrumental variables regression},
  volume       = {217},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Relevant parameter changes in structural break models.
<em>JOE</em>, <em>217</em>(1), 46–78. (<a
href="https://doi.org/10.1016/j.jeconom.2019.10.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural break time series models , which are commonly used in macroeconomics and finance , capture unknown structural changes by allowing for abrupt changes to model parameters. However, many specifications suffer from an over-parametrization issue, since typically all parameters have to change when a break occurs. We introduce a sparse change-point model to detect which parameters change over time. We propose a shrinkage prior distribution, which controls model parsimony by limiting the number of parameters that change from one structural break to another. We develop a Bayesian sampler for inference on the sparse change-point model. An extensive simulation study based on AR, ARMA and GARCH processes highlights the excellent performance of the sampler. We provide several empirical applications including an out-of-sample forecasting exercise showing that the Sparse change-point framework compares favorably with other recent time-varying parameter processes.},
  archive      = {J_JOE},
  author       = {Arnaud Dufays and Jeroen V.K. Rombouts},
  doi          = {10.1016/j.jeconom.2019.10.008},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {46-78},
  shortjournal = {J. Econ.},
  title        = {Relevant parameter changes in structural break models},
  volume       = {217},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High frequency traders and the price process. <em>JOE</em>,
<em>217</em>(1), 20–45. (<a
href="https://doi.org/10.1016/j.jeconom.2019.11.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using a dataset that uniquely identifies counterparties to each S&amp;P500 eMini transaction, we classify each market participant as high or low frequency, and each transaction, by the speed of the traders involved. We investigate empirically the comparative influence of high and low frequency traders on the price process, and conversely the influence of the price process on the trading of high and low frequency traders. We find that high frequency traders have a particularly high success rate on each transaction, measured by the likelihood that the following price change will go in their direction as well as by the amount of time they have to wait to realize their gain, when trading against low frequency traders. Contrary to common wisdom, we find that high frequency traders’ activity does not induce volatility or jumps. In fact, it is their absence that is problematic: volatility and jumps are more prevalent in periods when they trade less intensely. Conversely, we find that spikes in volatility and jumps cause high frequency traders to trade less intensely, decreasing their provision of liquidity. Finally, looking at the market microstructure noise component to the price model, we find that higher level of noise generates trading opportunities for high frequency traders and lead them to increase their trading activity.},
  archive      = {J_JOE},
  author       = {Yacine Aït-Sahalia and Celso Brunetti},
  doi          = {10.1016/j.jeconom.2019.11.005},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {20-45},
  shortjournal = {J. Econ.},
  title        = {High frequency traders and the price process},
  volume       = {217},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating derivatives of function-valued parameters in a
class of moment condition models. <em>JOE</em>, <em>217</em>(1), 1–19.
(<a href="https://doi.org/10.1016/j.jeconom.2019.11.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a general approach to estimating the derivative of a function-valued parameter θ o ( u ) that is identified for every value of u as the solution to a moment condition. This setup in particular covers interesting models for conditional distributions, such as quantile regression or distribution regression. Exploiting that θ o ( u ) solves a moment condition, we obtain an explicit expression for its derivative from the Implicit Function Theorem , and then estimate the components of this expression by suitable sample analogues. The last step generally involves (local linear) smoothing of the empirical moment condition. Our estimators can then be used for a variety of purposes, including the estimation of conditional density functions , quantile partial effects, and the distribution of bidders’ valuations in structural auction models.},
  archive      = {J_JOE},
  author       = {Christoph Rothe and Dominik Wied},
  doi          = {10.1016/j.jeconom.2019.11.004},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {1-19},
  shortjournal = {J. Econ.},
  title        = {Estimating derivatives of function-valued parameters in a class of moment condition models},
  volume       = {217},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Survey weighted estimating equation inference with nuisance
functionals. <em>JOE</em>, <em>216</em>(2), 516–536. (<a
href="https://doi.org/10.1016/j.jeconom.2019.11.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides a rigorous treatment on design-based estimating equation inference using complex survey data in the presence of nuisance functionals. The proposed design-based framework covers parameters from inequality measures and measures on performance evaluation in economic, business and financial studies but the scope of the paper is more broad. Unlike nuisance parameters in other settings where profile analysis is commonly used, the nuisance functionals are typically handled by using a “plug-in” estimator. We establish root- n n consistency and asymptotic normality for parameters defined through survey weighted smooth or non-differentiable estimating functions even if the “plug-in” estimator for the nuisance functional has a convergence rate slower than root- n n . A multiplier bootstrap procedure is proposed for three types of commonly used survey designs to tackle the challenge task of variance estimation. Results from a simulation study and an application to a real survey data set demonstrate the effectiveness of the proposed design-based inference on the Lorenz curve .},
  archive      = {J_JOE},
  author       = {Puying Zhao and David Haziza and Changbao Wu},
  doi          = {10.1016/j.jeconom.2019.11.003},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {516-536},
  shortjournal = {J. Econ.},
  title        = {Survey weighted estimating equation inference with nuisance functionals},
  volume       = {216},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The dynamic factor network model with an application to
international trade. <em>JOE</em>, <em>216</em>(2), 494–515. (<a
href="https://doi.org/10.1016/j.jeconom.2019.10.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a dynamic network model with probabilistic link functions that depend on stochastically time-varying parameters. We adopt a blockmodel framework and allow the high-dimensional vector of link probabilities to be a function of a low-dimensional set of dynamic factors . The resulting dynamic factor network model has a basic and transparent structure. However, parameter estimation, signal extraction of stochastic loadings and dynamic factors , and the econometric analysis generally are intricate tasks for which simulation-based methods are needed. We provide feasible and practical solutions to these challenging tasks, based on a computationally efficient importance sampling procedure to evaluate the likelihood function. An extensive Monte Carlo study demonstrates the performance of our method in finite samples, both under correct and incorrect model specifications. In an empirical study, we use the novel framework to analyze global patterns of banana exports and imports. We identify groups of heavy and light traders in this highly active commodity market and their time-varying trade probabilities .},
  archive      = {J_JOE},
  author       = {Falk Bräuning and Siem Jan Koopman},
  doi          = {10.1016/j.jeconom.2019.10.007},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {494-515},
  shortjournal = {J. Econ.},
  title        = {The dynamic factor network model with an application to international trade},
  volume       = {216},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deviance information criterion for latent variable models
and misspecified models. <em>JOE</em>, <em>216</em>(2), 450–493. (<a
href="https://doi.org/10.1016/j.jeconom.2019.11.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deviance information criterion (DIC) has been widely used for Bayesian model comparison, especially after Markov chain Monte Carlo (MCMC) is used to estimate candidate models. This paper first studies the problem of using DIC to compare latent variable models when DIC is calculated from the conditional likelihood . In particular, it is shown that the conditional likelihood approach undermines theoretical underpinnings of DIC. A new version of DIC, namely DIC L L , is proposed to compare latent variable models . The large sample properties of DIC L L are studied. A frequentist justification of DIC L L is provided. Like AIC , DIC L L provides an asymptotically unbiased estimator to the expected Kullback–Leibler (KL) divergence between the DGP and a predictive distribution . Some popular algorithms, such as the EM, Kalman and particle filtering algorithms, are introduced to compute DIC L L for latent variable models. Moreover, this paper studies the problem of using DIC to compare misspecified models . A new version of DIC, namely DIC M M , is proposed and it can be regarded as a Bayesian version of TIC. A frequentist justification of DIC M M is provided under misspecification. DIC L L and DIC M M are illustrated using asset pricing models and stochastic volatility models .},
  archive      = {J_JOE},
  author       = {Yong Li and Jun Yu and Tao Zeng},
  doi          = {10.1016/j.jeconom.2019.11.002},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {450-493},
  shortjournal = {J. Econ.},
  title        = {Deviance information criterion for latent variable models and misspecified models},
  volume       = {216},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Option market trading activity and the estimation of the
pricing kernel: A bayesian approach. <em>JOE</em>, <em>216</em>(2),
430–449. (<a
href="https://doi.org/10.1016/j.jeconom.2019.11.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a nonparametric Bayesian approach for the estimation of the pricing kernel. Historical stock returns and option market data are combined through the Dirichlet Process (DP) to construct an option-adjusted physical measure. The precision parameter of the DP process is calibrated to the amount of trading activity in deep-out-of-the-money options. We use the option-adjusted physical measure to construct an option-adjusted pricing kernel. An empirical investigation on the S&amp;P 500 Index from 2002 to 2015 shows that the option-adjusted pricing kernel is consistently monotonically decreasing, regardless of the level of volatility, thus providing an explanation to the well known U-shaped pricing kernel puzzle.},
  archive      = {J_JOE},
  author       = {Giovanni Barone-Adesi and Nicola Fusari and Antonietta Mira and Carlo Sala},
  doi          = {10.1016/j.jeconom.2019.11.001},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {430-449},
  shortjournal = {J. Econ.},
  title        = {Option market trading activity and the estimation of the pricing kernel: A bayesian approach},
  volume       = {216},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Counterfactual prediction in complete information games:
Point prediction under partial identification. <em>JOE</em>,
<em>216</em>(2), 394–429. (<a
href="https://doi.org/10.1016/j.jeconom.2019.02.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of counterfactual prediction in discrete decision games with complete information, pure strategies, and Nash equilibria : the presence of multiple equilibria poses unique challenges. We introduce multiple types of counterfactuals to establish sharp identified bounds for their prediction probabilities . We propose and compare various point prediction methods, namely midpoint prediction, an approach using a Dirichlet-based prior, a maximum entropy method, and minmax with an entropy constraint. On balance, we conclude that the maximum-entropy approach is the least of several evils. Our results have implications for counterfactual prediction in other models with partial identification.},
  archive      = {J_JOE},
  author       = {Sung Jae Jun and Joris Pinkse},
  doi          = {10.1016/j.jeconom.2019.02.009},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {394-429},
  shortjournal = {J. Econ.},
  title        = {Counterfactual prediction in complete information games: Point prediction under partial identification},
  volume       = {216},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive inference on pure spatial models. <em>JOE</em>,
<em>216</em>(2), 375–393. (<a
href="https://doi.org/10.1016/j.jeconom.2019.10.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a general class of semiparametric pure spatial models (having no explanatory variables) allowing nonlinearity in the parameter and the weight matrix , we propose adaptive tests and estimates which are asymptotically efficient in the presence of unknown, nonparametric distributional form. Feasibility of adaptive estimation is verified and its efficiency improvement over Gaussian pseudo maximum likelihood is shown to be either less than, or more than, for models with explanatory variables , depending on properties of the spatial weight matrix . An adaptive Lagrange Multiplier testing procedure for lack of spatial dependence is proposed and this, and our adaptive parameter estimate, are extended to cover regression with spatially correlated errors .},
  archive      = {J_JOE},
  author       = {Jungyoon Lee and Peter M. Robinson},
  doi          = {10.1016/j.jeconom.2019.10.006},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {375-393},
  shortjournal = {J. Econ.},
  title        = {Adaptive inference on pure spatial models},
  volume       = {216},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unobserved heterogeneity in auctions under restricted
stochastic dominance. <em>JOE</em>, <em>216</em>(2), 354–374. (<a
href="https://doi.org/10.1016/j.jeconom.2019.07.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the identification of first-price auctions with nonseparable unobserved heterogeneity. In particular, we extend Hu et al. (2013) by relaxing the first-order stochastic dominance condition. Instead, we assume restricted stochastic dominance relations among value quantile functions and show that the same relations pass to bid quantile functions . An ordered tree summarizes these relations and provides a total ordering. Relying on the proposed restricted stochastic dominance ordering, we extend a list of identification results in the empirical auction literature.},
  archive      = {J_JOE},
  author       = {Yao Luo},
  doi          = {10.1016/j.jeconom.2019.07.009},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {354-374},
  shortjournal = {J. Econ.},
  title        = {Unobserved heterogeneity in auctions under restricted stochastic dominance},
  volume       = {216},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient estimation of heterogeneous coefficients in panel
data models with common shocks. <em>JOE</em>, <em>216</em>(2), 327–353.
(<a href="https://doi.org/10.1016/j.jeconom.2019.08.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the estimation and inference issues of heterogeneous coefficients in panel data models with common shocks. We propose a novel two-step method to estimate the heterogeneous coefficients. We establish the asymptotic theory of our estimators, including consistency, asymptotic representation, and limiting distribution. Our two-step method can effectively address the limitations of the existing methods, such as the common correlated effects method proposed by Pesaran (2006) and the iterated principal components method proposed by Song (2013). The two-step estimator is as efficient as the two existing competitors in the basic model, and more efficient in the model with zero restrictions. Intensive Monte Carlo simulations show that the proposed estimator performs robustly in a variety of data setups.},
  archive      = {J_JOE},
  author       = {Kunpeng Li and Guowei Cui and Lina Lu},
  doi          = {10.1016/j.jeconom.2019.08.011},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {327-353},
  shortjournal = {J. Econ.},
  title        = {Efficient estimation of heterogeneous coefficients in panel data models with common shocks},
  volume       = {216},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiscale clustering of nonparametric regression curves.
<em>JOE</em>, <em>216</em>(1), 305–325. (<a
href="https://doi.org/10.1016/j.jeconom.2020.01.020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a wide range of modern applications, one observes a large number of time series rather than only a single one. It is often natural to suppose that there is some group structure in the observed time series. When each time series is modeled by a nonparametric regression equation , one may in particular assume that the observed time series can be partitioned into a small number of groups whose members share the same nonparametric regression function . We develop a bandwidth-free clustering method to estimate the unknown group structure from the data. More precisely speaking, we construct multiscale estimators of the unknown groups and their unknown number which are free of classical bandwidth or smoothing parameters. In the theoretical part of the paper, we analyze the statistical properties of our estimators. Our theoretical results are derived under general conditions which allow the data to be dependent both in time series direction and across different time series. The technical analysis of the paper is complemented by simulated and real-data examples.},
  archive      = {J_JOE},
  author       = {Michael Vogt and Oliver Linton},
  doi          = {10.1016/j.jeconom.2020.01.020},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {305-325},
  shortjournal = {J. Econ.},
  title        = {Multiscale clustering of nonparametric regression curves},
  volume       = {216},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pairwise local fisher and naive bayes: Improving two
standard discriminants. <em>JOE</em>, <em>216</em>(1), 284–304. (<a
href="https://doi.org/10.1016/j.jeconom.2020.01.019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Fisher discriminant is probably the best known likelihood discriminant for continuous data. Another benchmark discriminant is the naive Bayes , which is based on marginals only. In this paper we extend both discriminants by modeling dependence between pairs of variables. In the continuous case this is done by local Gaussian versions of the Fisher discriminant. In the discrete case the naive Bayes is extended by taking geometric averages of pairwise joint probabilities . We also indicate how the two approaches can be combined for mixed continuous and discrete data. The new discriminants show promising results in a number of simulation experiments and real data illustrations.},
  archive      = {J_JOE},
  author       = {Håkon Otneim and Martin Jullum and Dag Tjøstheim},
  doi          = {10.1016/j.jeconom.2020.01.019},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {284-304},
  shortjournal = {J. Econ.},
  title        = {Pairwise local fisher and naive bayes: Improving two standard discriminants},
  volume       = {216},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Double machine learning with gradient boosting and its
application to the big n audit quality effect. <em>JOE</em>,
<em>216</em>(1), 268–283. (<a
href="https://doi.org/10.1016/j.jeconom.2020.01.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the double machine learning (DML) approach of Chernozhukov et al. (2018) for estimating average treatment effect and apply this approach to examine the Big N audit quality effect in the accounting literature. This approach relies on machine learning methods and is suitable when a high dimensional nuisance function with many covariates is present in the model. This approach does not suffer from the “regularization bias” when a learning method with a proper convergence rate is used. We demonstrate by simulations that, for the DML approach, the gradient boosting method is fairly robust and to be preferred to other methods, such as regression tree , random forest, support vector regression machine, and the conventional Nadaraya–Watson nonparametric estimator. We then apply the DML approach with gradient boosting to estimate the Big N effect. We find that Big N auditors have a positive effect on audit quality and that this effect is not only statistically significant but also economically important. We further show that, in contrast to the results of propensity score matching , our estimates of said effect are quite robust to the hyper-parameters in the gradient boosting algorithm.},
  archive      = {J_JOE},
  author       = {Jui-Chung Yang and Hui-Ching Chuang and Chung-Ming Kuan},
  doi          = {10.1016/j.jeconom.2020.01.018},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {268-283},
  shortjournal = {J. Econ.},
  title        = {Double machine learning with gradient boosting and its application to the big n audit quality effect},
  volume       = {216},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Noncausal vector AR processes with application to economic
time series. <em>JOE</em>, <em>216</em>(1), 246–267. (<a
href="https://doi.org/10.1016/j.jeconom.2020.01.017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inference procedures for noncausal autoregressive (AR) models have been well studied and applied in a variety of applications from environmental to financial. For such processes, the observation at time t may depend on both past and future shocks in the system. In this paper, we consider extension of the univariate noncausal AR models to the vector AR (VAR) case. The extension presents several interesting challenges since even a first-order VAR can possess both causal and noncausal components. Assuming a non-Gaussian distribution for the noise, we show how to compute an approximation to the likelihood function. Under suitable conditions, it is shown that the maximum likelihood estimator (MLE) of the vector of AR parameters is asymptotically normal. The estimation procedure is illustrated with a simulation study for a VAR(1) process and with two real data examples.},
  archive      = {J_JOE},
  author       = {Richard A. Davis and Li Song},
  doi          = {10.1016/j.jeconom.2020.01.017},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {246-267},
  shortjournal = {J. Econ.},
  title        = {Noncausal vector AR processes with application to economic time series},
  volume       = {216},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust causality test of infinite variance processes.
<em>JOE</em>, <em>216</em>(1), 235–245. (<a
href="https://doi.org/10.1016/j.jeconom.2020.01.016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a robust causality test for time series with infinite variance innovation processes. First, we introduce a measure of dependence for vector nonparametric linear processes , and derive the asymptotic distribution of the test statistic by Taniguchi et al. (1996) in the infinite variance case. Second, we construct a weighted version of the generalized empirical likelihood (GEL) test statistic , called the self-weighted GEL statistic in the time domain. The limiting distribution of the self-weighted GEL test statistic is shown to be the usual chi-squared one regardless of whether the model has finite variance or not. Some simulation experiments illustrate satisfactory finite sample performances of the proposed test.},
  archive      = {J_JOE},
  author       = {Fumiya Akashi and Masanobu Taniguchi and Anna Clara Monti},
  doi          = {10.1016/j.jeconom.2020.01.016},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {235-245},
  shortjournal = {J. Econ.},
  title        = {Robust causality test of infinite variance processes},
  volume       = {216},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inference for the degree distributions of preferential
attachment networks with zero-degree nodes. <em>JOE</em>,
<em>216</em>(1), 220–234. (<a
href="https://doi.org/10.1016/j.jeconom.2020.01.015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tail of the logarithmic degree distribution of networks decays linearly with respect to the logarithmic degree is known as the power law and is ubiquitous in daily lives. A commonly used technique in modeling the power law is preferential attachment (PA), which sequentially joins each new node to the existing nodes according to the conditional probability law proportional to a linear function of their degrees. Although effective, it is tricky to apply PA to real networks because the number of nodes and that of edges have to satisfy a linear constraint . This paper enables real application of PA by making each new node as an isolated node that attaches to other nodes according to PA scheme in some later epochs. This simple and novel strategy provides an additional degree of freedom to relax the aforementioned constraint to the observed data and uses the PA scheme to compute the implied proportion of the unobserved zero-degree nodes. By using martingale convergence theory, the degree distribution of the proposed model is shown to follow the power law and its asymptotic variance is proved to be the solution of a Sylvester matrix equation , a class of equations frequently found in the control theory (see Hansen and Sargent (2008, 2014)). These results give a strongly consistent estimator for the power-law parameter and its asymptotic normality . Note that this statistical inference procedure is non-iterative and is particularly applicable for big networks such as the World Wide Web presented in Section 6 . Moreover, the proposed model offers a theoretically coherent framework that can be used to study other network features, such as clustering and connectedness, as given in Cheung (2016).},
  archive      = {J_JOE},
  author       = {N.H. Chan and Simon K.C. Cheung and Samuel P.S. Wong},
  doi          = {10.1016/j.jeconom.2020.01.015},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {220-234},
  shortjournal = {J. Econ.},
  title        = {Inference for the degree distributions of preferential attachment networks with zero-degree nodes},
  volume       = {216},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two-mode network autoregressive model for large-scale
networks. <em>JOE</em>, <em>216</em>(1), 203–219. (<a
href="https://doi.org/10.1016/j.jeconom.2020.01.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A two-mode network refers to a network where the nodes are classified into two distinct types, and edges can only exist between nodes of different types. In analysis of two-mode networks, one important objective is to explore the relationship between responses of two types of nodes. To this end, we propose a network autoregressive model for two-mode networks. Different network autocorrelation coefficients are allowed. To estimate the model, a quasi-maximum likelihood estimator is developed with high computational cost. To alleviate the computational burden, a least squares estimator is proposed, which is applicable in large-scale networks. The least squares estimator can be viewed as one particular type of generalized methods of moments estimator . The theoretical properties of both estimators are investigated. The finite sample performances are assessed through simulations and a real data example.},
  archive      = {J_JOE},
  author       = {Danyang Huang and Feifei Wang and Xuening Zhu and Hansheng Wang},
  doi          = {10.1016/j.jeconom.2020.01.014},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {203-219},
  shortjournal = {J. Econ.},
  title        = {Two-mode network autoregressive model for large-scale networks},
  volume       = {216},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Asymptotic theory for near integrated processes driven by
tempered linear processes. <em>JOE</em>, <em>216</em>(1), 192–202. (<a
href="https://doi.org/10.1016/j.jeconom.2020.01.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an early article on near-unit root autoregression , Ahtola and Tiao (1984) studied the behavior of the score function in a stationary first order autoregression driven by independent Gaussian innovations as the autoregressive coefficient approached unity from below. The present paper develops asymptotic theory for near-integrated random processes and associated regressions including the score function in more general settings where the errors are tempered linear processes . Tempered processes are stationary time series that have a semi-long memory property in the sense that the autocovariogram of the process resembles that of a long memory model for moderate lags but eventually diminishes exponentially fast according to the presence of a decay factor governed by a tempering parameter. When the tempering parameter is sample size dependent, the resulting class of processes admits a wide range of behavior that includes both long memory, semi-long memory, and short memory processes. The paper develops asymptotic theory for such processes and associated regression statistics thereby extending earlier findings that fall within certain subclasses of processes involving near-integrated time series. The limit results relate to tempered fractional processes that include tempered fractional Brownian motion and tempered fractional diffusions of the second kind. The theory is extended to provide the limiting distribution for autoregressions with such tempered near-integrated time series, thereby enabling analysis of the limit properties of statistics of particular interest in econometrics , such as unit root tests, under more general conditions than existing theory. Some extensions of the theory to the multivariate case are reported.},
  archive      = {J_JOE},
  author       = {Farzad Sabzikar and Qiying Wang and Peter C.B. Phillips},
  doi          = {10.1016/j.jeconom.2020.01.013},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {192-202},
  shortjournal = {J. Econ.},
  title        = {Asymptotic theory for near integrated processes driven by tempered linear processes},
  volume       = {216},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation for double-nonlinear cointegration. <em>JOE</em>,
<em>216</em>(1), 175–191. (<a
href="https://doi.org/10.1016/j.jeconom.2020.01.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years statistical inference for nonlinear cointegration has attracted attention from both academics and practitioners. This paper proposes a new type of cointegration in the sense that two univariate time series y t and x t are cointegrated via two (unknown) smooth nonlinear transformations, further generalizing the notion of cointegration initially revealed by Box and Tiao (1977), and more systematically studied by Engle and Granger (1987). More precisely, it holds that G ( y t , β 0 ) = g ( x t ) + u t , where G ( ⋅ , β 0 ) is strictly increasing and known up to an unknown parameter β 0 , g ( ⋅ ) is unknown and smooth, x t is I ( 1 ) , and u t is the stationary disturbance. This setting nests the nonlinear cointegration model of Wang and Phillips (2009b) as a special case with G ( y , β 0 ) = y . It extends the model of Linton et al. (2008) to the cases with a unit-root nonstationary regressor . Sieve approximations to the smooth nonparametric function g are applied, leading to an extremum estimator for β and a plugging-in estimator for g ( ⋅ ) . Asymptotic properties of the estimators are established, revealing that both the convergence rates and the limiting distributions depend intimately on the properties of the two nonlinear transformation functions. Simulation studies demonstrate that the estimators perform well even with small samples. A real data example on the environmental Kuznets curve portraying the nonlinear impact of per-capita GDP on air-pollution illustrates the practical relevance of the proposed double-nonlinear cointegration.},
  archive      = {J_JOE},
  author       = {Yingqian Lin and Yundong Tu and Qiwei Yao},
  doi          = {10.1016/j.jeconom.2020.01.012},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {175-191},
  shortjournal = {J. Econ.},
  title        = {Estimation for double-nonlinear cointegration},
  volume       = {216},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Twisted probabilities, uncertainty, and prices.
<em>JOE</em>, <em>216</em>(1), 151–174. (<a
href="https://doi.org/10.1016/j.jeconom.2020.01.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A decision maker constructs a convex set of nonnegative martingales to use as likelihood ratios that represent alternatives that are statistically close to a decision maker’s baseline model. The set is twisted to include some specific models of interest. Max–min expected utility over that set gives rise to equilibrium prices of model uncertainty expressed as worst-case distortions to drifts in a representative investor’s baseline model. Three quantitative illustrations start with baseline models having exogenous long-run risks in technology shocks. These put endogenous long-run risks into consumption dynamics that differ in details that depend on how shocks affect returns to capital stocks. We describe sets of alternatives to a baseline model that generate countercyclical prices of uncertainty.},
  archive      = {J_JOE},
  author       = {Lars Peter Hansen and Bálint Szőke and Lloyd S. Han and Thomas J. Sargent},
  doi          = {10.1016/j.jeconom.2020.01.011},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {151-174},
  shortjournal = {J. Econ.},
  title        = {Twisted probabilities, uncertainty, and prices},
  volume       = {216},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic conditional angular correlation. <em>JOE</em>,
<em>216</em>(1), 137–150. (<a
href="https://doi.org/10.1016/j.jeconom.2020.01.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the concept of angular correlation for estimating the instantaneous correlation matrix with a single multivariate realization. The proposed estimator is generally a positive definite correlation matrix and robust in that for bivariate normal data, the sample angular correlation is equally likely to be above or below the population correlation coefficient . We then generalize the dynamic conditional correlation (DCC) model to the dynamic conditional angular correlation (DCAC) model. We demonstrate the efficacy and robustness of the proposed methods against leptokurticity, with some numerical experiments. In particular, a real application illustrates the better performance of the DCAC model than the DCC model in portfolio construction.},
  archive      = {J_JOE},
  author       = {Riad Jarjour and Kung-Sik Chan},
  doi          = {10.1016/j.jeconom.2020.01.010},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {137-150},
  shortjournal = {J. Econ.},
  title        = {Dynamic conditional angular correlation},
  volume       = {216},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variable selection for high-dimensional regression models
with time series and heteroscedastic errors. <em>JOE</em>,
<em>216</em>(1), 118–136. (<a
href="https://doi.org/10.1016/j.jeconom.2020.01.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although existing literature on high-dimensional regression models is rich, the vast majority of studies have focused on independent and homogeneous error terms. In this article, we consider the problem of selecting high-dimensional regression models with heteroscedastic and time series errors, which have broad applications in economics, quantitative finance , environmental science, and many other fields. The error term in our model is the product of two components: one time series component, allowing for a short-memory, long-memory, or conditional heteroscedasticity effect, and a high-dimensional dispersion function accounting for exogenous heteroscedasticity . By making use of the orthogonal greedy algorithm and the high-dimensional information criterion, we propose a new model selection procedure that consistently chooses the relevant variables in both the regression and the dispersion functions. The finite sample performance of the proposed procedure is also illustrated via simulations and real data analysis.},
  archive      = {J_JOE},
  author       = {Hai-Tang Chiou and Meihui Guo and Ching-Kang Ing},
  doi          = {10.1016/j.jeconom.2020.01.009},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {118-136},
  shortjournal = {J. Econ.},
  title        = {Variable selection for high-dimensional regression models with time series and heteroscedastic errors},
  volume       = {216},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Testing serial correlations in high-dimensional time series
via extreme value theory. <em>JOE</em>, <em>216</em>(1), 106–117. (<a
href="https://doi.org/10.1016/j.jeconom.2020.01.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a simple test for detecting serial correlations in high-dimensional time series. The proposed test makes use of the robust properties of Spearman’s rank correlation and the theory of extreme values. Asymptotic properties of the test statistics are derived under some minor conditions as both the sample size and dimension go to infinity. The test is not sensitive to the underlying distribution of the time series so long as the data are continuously distributed. In particular, the existence of finite-order moments of the underlying distribution is not required, and asymptotic critical values of the test statistics are available in closed form. In finite samples, we correct biases of the sample autocorrelations and conduct simulations to study the performance of the proposed test statistics. Simulation results show that the proposed test statistics enjoy good properties of size and power in finite samples. We apply the proposed test to a 92-dimensional series of asset returns . Finally, a simple R code is available to obtain finite-sample critical values of the test statistics if needed.},
  archive      = {J_JOE},
  author       = {Ruey S. Tsay},
  doi          = {10.1016/j.jeconom.2020.01.008},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {106-117},
  shortjournal = {J. Econ.},
  title        = {Testing serial correlations in high-dimensional time series via extreme value theory},
  volume       = {216},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-frequency factor models and regressions. <em>JOE</em>,
<em>216</em>(1), 86–105. (<a
href="https://doi.org/10.1016/j.jeconom.2020.01.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a nonparametric time series regression model. Our framework allows precise estimation of betas without the usual assumption of betas being piecewise constant. This property makes our framework particularly suitable to study individual stocks. We provide an inference framework for all components of the model, including idiosyncratic volatility and idiosyncratic jumps. Our empirical analysis investigates the largest dataset in the high-frequency literature. First, we use all traded stocks from NYSE, AMEX, and NASDAQ stock markets for 1996–2017 to construct the five Fama–French factors and the momentum factor at the 5-minute frequency. Second, we document the key empirical properties across all the stocks and the new factors, and apply the nonparametric time series regression model with the new high-frequency Fama–French factors. We find that this factor model is effective in explaining the systematic component of the risk of individual stocks. In addition, we provide evidence that idiosyncratic jumps are related to idiosyncratic events such as earnings disappointments.},
  archive      = {J_JOE},
  author       = {Yacine Aït-Sahalia and Ilze Kalnina and Dacheng Xiu},
  doi          = {10.1016/j.jeconom.2020.01.007},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {86-105},
  shortjournal = {J. Econ.},
  title        = {High-frequency factor models and regressions},
  volume       = {216},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Factor-adjusted regularized model selection. <em>JOE</em>,
<em>216</em>(1), 71–85. (<a
href="https://doi.org/10.1016/j.jeconom.2020.01.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies model selection consistency for high dimensional sparse regression when data exhibits both cross-sectional and serial dependency. Most commonly-used model selection methods fail to consistently recover the true model when the covariates are highly correlated. Motivated by econometric and financial studies, we consider the case where covariate dependence can be reduced through the factor model, and propose a consistency strategy named Factor-Adjusted Regularized Model Selection (FarmSelect). By learning the latent factors and idiosyncratic components and using both of them as predictors, FarmSelect transforms the problem from model selection with highly correlated covariates to that with weakly correlated ones via lifting. Model selection consistency, as well as optimal rates of convergence, are obtained under mild conditions. Numerical studies demonstrate the nice finite sample performance in terms of both model selection and out-of-sample prediction. Moreover, our method is flexible in the sense that it pays no price for weakly correlated and uncorrelated cases. Our method is applicable to a wide range of high dimensional sparse regression problems. An R-package FarmSelect is also provided for implementation.},
  archive      = {J_JOE},
  author       = {Jianqing Fan and Yuan Ke and Kaizheng Wang},
  doi          = {10.1016/j.jeconom.2020.01.006},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {71-85},
  shortjournal = {J. Econ.},
  title        = {Factor-adjusted regularized model selection},
  volume       = {216},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Threshold factor models for high-dimensional time series.
<em>JOE</em>, <em>216</em>(1), 53–70. (<a
href="https://doi.org/10.1016/j.jeconom.2020.01.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a threshold factor model for high-dimensional time series in which the dynamics of the time series is assumed to switch between different regimes according to the value of a threshold variable. This is an extension of threshold modeling to a high-dimensional time series setting under a factor structure. Specifically, within each threshold regime, the time series is assumed to follow a factor model. The regime switching mechanism creates structural changes in the factor loading matrices . It provides flexibility in dealing with situations that the underlying states may be changing over time, as often observed in economic time series and other applications. We develop the procedures for the estimation of the loading spaces, the number of factors and the threshold value, as well as the identification of the threshold variable, which governs the regime change mechanism. The theoretical properties are investigated. Simulated and real data examples are presented to illustrate the performance of the proposed method.},
  archive      = {J_JOE},
  author       = {Xialu Liu and Rong Chen},
  doi          = {10.1016/j.jeconom.2020.01.005},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {53-70},
  shortjournal = {J. Econ.},
  title        = {Threshold factor models for high-dimensional time series},
  volume       = {216},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A robust procedure to build dynamic factor models with
cluster structure. <em>JOE</em>, <em>216</em>(1), 35–52. (<a
href="https://doi.org/10.1016/j.jeconom.2020.01.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic factor models provide a useful way to model large sets of time series. These data often have heterogeneity and cluster structure and the formulation and estimation of dynamic factor models should be adapted to these features. This article presents a procedure to fit Dynamic Factor Models with Cluster Structure (DFMCS), where some of the factors are global and others group-specific, to heterogeneous data that may include multivariate additive outliers and level shifts. The procedure starts with an initial cleaning of the times series from outlying effects. Then a first estimation of the possible factors is applied to the cleaned data and these factors are used to build the common component of each series. The groups are found by studying the joint dependency of these common components. Then, additional factors are estimated by using the series in each cluster and, finally, all the factors found are classified as global or group-specific. We show in a Monte Carlo study that the procedure works well and seems to be better than other alternatives in terms of estimation of factors and loadings as well as in terms of misclassification rates for the series. An example of an electricity market is presented to illustrate the advantages of cleaning for outliers and taking into account the cluster structure for understanding and forecasting.},
  archive      = {J_JOE},
  author       = {Andrés M. Alonso and Pedro Galeano and Daniel Peña},
  doi          = {10.1016/j.jeconom.2020.01.004},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {35-52},
  shortjournal = {J. Econ.},
  title        = {A robust procedure to build dynamic factor models with cluster structure},
  volume       = {216},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generalized dynamic factor models and volatilities:
Consistency, rates, and prediction intervals. <em>JOE</em>,
<em>216</em>(1), 4–34. (<a
href="https://doi.org/10.1016/j.jeconom.2020.01.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Volatilities, in high-dimensional panels of economic time series with a dynamic factor structure on the levels or returns, typically also admit a dynamic factor decomposition. We consider a two-stage dynamic factor model method recovering the common and idiosyncratic components of both levels and log-volatilities. Specifically, in a first estimation step, we extract the common and idiosyncratic shocks for the levels, from which a log-volatility proxy is computed. In a second step, we estimate a dynamic factor model, which is equivalent to a multiplicative factor structure for volatilities, for the log-volatility panel. By exploiting this two-stage factor approach, we build one-step-ahead conditional prediction intervals for large n × T n×T panels of returns. Those intervals are based on empirical quantiles , not on conditional variances ; they can be either equal- or unequal-tailed. We provide uniform consistency and consistency rates results for the proposed estimators as both n n and T T tend to infinity. We study the finite-sample properties of our estimators by means of Monte Carlo simulations . Finally, we apply our methodology to a panel of asset returns belonging to the S&amp;P100 index in order to compute one-step-ahead conditional prediction intervals for the period 2006–2013. A comparison with the componentwise GARCH benchmark (which does not take advantage of cross-sectional information) demonstrates the superiority of our approach, which is genuinely multivariate (and high-dimensional), nonparametric, and model-free.},
  archive      = {J_JOE},
  author       = {Matteo Barigozzi and Marc Hallin},
  doi          = {10.1016/j.jeconom.2020.01.003},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {4-34},
  shortjournal = {J. Econ.},
  title        = {Generalized dynamic factor models and volatilities: Consistency, rates, and prediction intervals},
  volume       = {216},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Introduction of the annals issue: Statistical learning for
dependent data — a celebration of the 85th birthday of professor george
c. tiao. <em>JOE</em>, <em>216</em>(1), 1–3. (<a
href="https://doi.org/10.1016/j.jeconom.2020.01.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOE},
  author       = {Rong Chen and Ruey S. Tsay},
  doi          = {10.1016/j.jeconom.2020.01.002},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {1-3},
  shortjournal = {J. Econ.},
  title        = {Introduction of the annals issue: Statistical learning for dependent data — a celebration of the 85th birthday of professor george c. tiao},
  volume       = {216},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Kernel-based inference in time-varying coefficient
cointegrating regression. <em>JOE</em>, <em>215</em>(2), 607–632. (<a
href="https://doi.org/10.1016/j.jeconom.2019.10.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies nonlinear cointegrating models with time-varying coefficients and multiple nonstationary regressors using classic kernel smoothing methods to estimate the coefficient functions . Extending earlier work on nonstationary kernel regression to take account of practical features of the data, we allow the regressors to be cointegrated and to embody a mixture of stochastic and deterministic trends, complications which result in asymptotic degeneracy of the kernel-weighted signal matrix. To address these complications new local and global rotation techniques are introduced to transform the covariate space to accommodate multiple scenarios of induced degeneracy. Under regularity conditions we derive asymptotic results that differ substantially from existing kernel regression asymptotics, leading to new limit theory under multiple convergence rates. For the practically important case of endogenous nonstationary regressors we propose a fully-modified kernel estimator whose limit distribution theory corresponds to the prototypical pure cointegration case (i.e., with exogenous covariates), thereby facilitating inference using a generalized Wald-type test statistic. These results substantially generalize econometric estimation and testing techniques in the cointegration literature to accommodate time variation and complications of co-moving regressors. Finally, Monte-Carlo simulation studies as well as an empirical illustration to aggregate US data on consumption, income, and interest rates are provided to illustrate the methodology and evaluate the numerical performance of the proposed methods in finite samples.},
  archive      = {J_JOE},
  author       = {Degui Li and Peter C.B. Phillips and Jiti Gao},
  doi          = {10.1016/j.jeconom.2019.10.005},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {607-632},
  shortjournal = {J. Econ.},
  title        = {Kernel-based inference in time-varying coefficient cointegrating regression},
  volume       = {215},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multivariate spatial autoregressive model for large scale
social networks. <em>JOE</em>, <em>215</em>(2), 591–606. (<a
href="https://doi.org/10.1016/j.jeconom.2018.11.018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid growth of social network platforms generates a large amount of social network data, where multivariate responses are frequently collected from users. To statistically model such type of data, the multivariate spatial autoregressive (MSAR) model is studied. To estimate the model, the quasi maximum likelihood estimator (QMLE) is obtained under certain technical conditions. However, it is found that the computational cost of QMLE is expensive. To fix this problem, a least squares estimator (LSE) is developed. The corresponding identification conditions and asymptotic properties are investigated. To gauge the finite sample performance of various estimators, a number of simulation studies are conducted. Lastly, a Sina Weibo dataset is analyzed for illustration purpose.},
  archive      = {J_JOE},
  author       = {Xuening Zhu and Danyang Huang and Rui Pan and Hansheng Wang},
  doi          = {10.1016/j.jeconom.2018.11.018},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {591-606},
  shortjournal = {J. Econ.},
  title        = {Multivariate spatial autoregressive model for large scale social networks},
  volume       = {215},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Identification and estimation in panel models with
overspecified number of groups. <em>JOE</em>, <em>215</em>(2), 574–590.
(<a href="https://doi.org/10.1016/j.jeconom.2019.09.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a simple and fast approach to identify and estimate the unknown group structure in panel models by adapting the M-estimation method. We consider both linear and nonlinear panel models where the regression coefficients are heterogeneous across groups but homogeneous within a group and the group membership is unknown to researchers. The main result of the paper is that under certain assumptions, our approach is able to provide uniformly consistent estimation as long as the number of groups used in estimation is not smaller than the true number of groups. We also show that, asymptotically, our method may partition some true groups into further subgroups, but cannot mix units from different groups. When the true number of groups is used in estimation, all units can be categorized correctly with probability approaching one, and we establish the limiting distribution for the estimators of the group parameters. In addition, we provide an information criterion to select the number of groups, and establish the consistency of the selection criterion under some mild conditions. Monte Carlo simulations are conducted to examine the finite sample performance of the proposed method. The findings in the simulation confirm our theoretical results in the paper. Applications to two real datasets also highlight the necessity to consider both individual heterogeneity and group heterogeneity in the model.},
  archive      = {J_JOE},
  author       = {Ruiqi Liu and Zuofeng Shang and Yonghui Zhang and Qiankun Zhou},
  doi          = {10.1016/j.jeconom.2019.09.008},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {574-590},
  shortjournal = {J. Econ.},
  title        = {Identification and estimation in panel models with overspecified number of groups},
  volume       = {215},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Issues in the estimation of mis-specified models of
fractionally integrated processes. <em>JOE</em>, <em>215</em>(2),
559–573. (<a
href="https://doi.org/10.1016/j.jeconom.2019.09.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This short paper provides a comprehensive set of new theoretical results on the impact of mis-specifying the short run dynamics in fractionally integrated processes. We show that four alternative parametric estimators – frequency domain maximum likelihood, Whittle, time domain maximum likelihood and conditional sum of squares – converge to the same pseudo-true value under common mis-specification, and that they possess a common asymptotic distribution . The results are derived assuming the true data generating mechanism is a fractional linear process driven by a martingale difference innovation. A completely general parametric specification for the short run dynamics of the estimated (mis-specified) fractional model is considered, and with long memory, short memory and antipersistence in both the model and the data generating mechanism accommodated. The paper can be seen as extending an existing line of research on mis-specification in fractional models, important contributions to which have appeared in Journal of Econometrics . It also complements a range of existing asymptotic results on estimation in correctly specified fractional models. Open problems in the area are the subject of the final discussion.},
  archive      = {J_JOE},
  author       = {Gael M. Martin and K. Nadarajah and D.S. Poskitt},
  doi          = {10.1016/j.jeconom.2019.09.007},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {559-573},
  shortjournal = {J. Econ.},
  title        = {Issues in the estimation of mis-specified models of fractionally integrated processes},
  volume       = {215},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dependent microstructure noise and integrated volatility
estimation from high-frequency data. <em>JOE</em>, <em>215</em>(2),
536–558. (<a
href="https://doi.org/10.1016/j.jeconom.2019.10.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we develop econometric tools to analyze the integrated volatility (IV) of the efficient price and the dynamic properties of microstructure noise in high-frequency data under general dependent noise. We first develop consistent estimators of the variance and autocovariances of noise using a variant of realized volatility. Next, we employ these estimators to adapt the pre-averaging method and derive consistent estimators of the IV, which converge stably to a mixed Gaussian distribution at the optimal rate n 1 ∕ 4 n1∕4 . To improve the finite sample performance, we propose a multi-step approach that corrects the finite sample bias, which turns out to be crucial in applications. Our extensive simulation studies demonstrate the excellent performance of our multi-step estimators. In an empirical study, we analyze the dependence structures of microstructure noise and provide intuitive economic interpretations; we also illustrate the importance of accounting for both the serial dependence in noise and the finite sample bias when estimating IV.},
  archive      = {J_JOE},
  author       = {Z. Merrick Li and Roger J.A. Laeven and Michel H. Vellekoop},
  doi          = {10.1016/j.jeconom.2019.10.004},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {536-558},
  shortjournal = {J. Econ.},
  title        = {Dependent microstructure noise and integrated volatility estimation from high-frequency data},
  volume       = {215},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variance risk: A bird’s eye view. <em>JOE</em>,
<em>215</em>(2), 517–535. (<a
href="https://doi.org/10.1016/j.jeconom.2019.09.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The literature documents a significantly negative average variance swap payoff (VSP) for the S&amp;P 500 index but generally not for the constituent stocks. We show that this result is affected by biases arising from (i) an intraday momentum effect and (ii) the use of an incoherent measure of return variation. Accounting for these issues, we find stronger evidence of a significant average VSP both at the index level and also for equities. We decompose the index variance risk premium (VRP) into factors related to the VRP of equities and the correlation risk premium (CRP) and assess their predictive power for aggregate stock returns .},
  archive      = {J_JOE},
  author       = {Fabian Hollstein and Chardin Wese Simen},
  doi          = {10.1016/j.jeconom.2019.09.006},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {517-535},
  shortjournal = {J. Econ.},
  title        = {Variance risk: A bird’s eye view},
  volume       = {215},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Time-invariant restrictions of volatility functionals:
Efficient estimation and specification tests. <em>JOE</em>,
<em>215</em>(2), 486–516. (<a
href="https://doi.org/10.1016/j.jeconom.2019.10.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the estimation and inference problems of time-invariant restrictions on certain known functions of the stochastic volatility process. We first develop a more efficient GMM estimator and derive the efficiency bound under such restrictions. Then we construct an integrated Hausman-type test by summing up the squared differences between this more efficient estimator and the unrestricted estimator computed at different time points . Although less efficient under the null , the latter estimator is consistent under both the null and the alternative. The efficient GMM estimator can also be used to update an existing Bierens-type test and simplify the calculation of the asymptotic variance . Since the quadratic function puts more weight on large values, the Hausman-type test can have superior power than the Bierens-type test, which is based on a linear function of the differences. The simulation study shows that except for very small local window sizes, the Hausman-type test has good size and superior power. We finally apply these tests to studying the constant beta hypothesis using empirical data and find substantial evidence against this hypothesis.},
  archive      = {J_JOE},
  author       = {Xiye Yang},
  doi          = {10.1016/j.jeconom.2019.10.003},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {486-516},
  shortjournal = {J. Econ.},
  title        = {Time-invariant restrictions of volatility functionals: Efficient estimation and specification tests},
  volume       = {215},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Identifying dynamic discrete choice models off short panels.
<em>JOE</em>, <em>215</em>(2), 473–485. (<a
href="https://doi.org/10.1016/j.jeconom.2018.12.025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper analyzes the identification of flow payoffs and counterfactual choice probabilities (CCPs) in single-agent dynamic discrete choice models . We develop new results on non-stationary models where the time horizon for the agent extends beyond the length of the data (short panels). We show that counterfactual CCPs in short panels are identified when induced by temporary policy changes affecting payoffs, even though the utility flows are not. Counterfactual CCPs induced by innovations to state transitions are generally not identified unless the model exhibits single action finite dependence, and the payoffs of those actions establishing single action finite dependence are known.},
  archive      = {J_JOE},
  author       = {Peter Arcidiacono and Robert A. Miller},
  doi          = {10.1016/j.jeconom.2018.12.025},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {473-485},
  shortjournal = {J. Econ.},
  title        = {Identifying dynamic discrete choice models off short panels},
  volume       = {215},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The uniform validity of impulse response inference in
autoregressions. <em>JOE</em>, <em>215</em>(2), 450–472. (<a
href="https://doi.org/10.1016/j.jeconom.2019.10.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing proofs of the asymptotic validity of conventional methods of impulse response inference based on higher-order autoregressions are pointwise only. In this paper, we establish the uniform asymptotic validity of conventional asymptotic and bootstrap inference about individual impulse responses and vectors of impulse responses when the horizon is fixed with respect to the sample size. For inference about vectors of impulse responses based on Wald test statistics to be uniformly valid, lag-augmented autoregressions are required, whereas inference about individual impulse responses is uniformly valid under weak conditions even without lag augmentation. We introduce a new rank condition that ensures the uniform validity of inference on impulse responses and show that this condition holds under weak conditions. Simulations show that the highest finite-sample accuracy is achieved when bootstrapping the lag-augmented autoregression using the bias adjustments of Kilian (1999). The conventional bootstrap percentile interval for impulse responses based on this approach remains accurate even at long horizons. We provide a formal asymptotic justification for this result.},
  archive      = {J_JOE},
  author       = {Atsushi Inoue and Lutz Kilian},
  doi          = {10.1016/j.jeconom.2019.10.001},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {450-472},
  shortjournal = {J. Econ.},
  title        = {The uniform validity of impulse response inference in autoregressions},
  volume       = {215},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating permanent price impact via machine learning.
<em>JOE</em>, <em>215</em>(2), 414–449. (<a
href="https://doi.org/10.1016/j.jeconom.2019.10.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we show that vector auto-regression (VAR) models, which are commonly used to estimate permanent price impact, are misspecified and can produce conflicting and incorrect inferences when the price impact function is nonlinear. We propose an alternative method to estimate permanent price impact by modifying a reinforcement learning (RL) framework. Our approach assumes the data is stationary and Markov, but is otherwise unrestrictive. We obtain empirical estimates for our model using an iterative learning rule and demonstrate that our model captures nonlinearities and makes correct inferences.},
  archive      = {J_JOE},
  author       = {R. Philip},
  doi          = {10.1016/j.jeconom.2019.10.002},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {414-449},
  shortjournal = {J. Econ.},
  title        = {Estimating permanent price impact via machine learning},
  volume       = {215},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparametric identification in index models of link
formation. <em>JOE</em>, <em>215</em>(2), 399–413. (<a
href="https://doi.org/10.1016/j.jeconom.2019.09.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a dyadic link formation model with a homophily effect index and a degree heterogeneity index. We provide nonparametric identification results for the potentially nonparametric homophily effect function, the realizations of unobserved individual fixed effects and the unknown distribution of idiosyncratic shocks, up to normalization. We propose a novel form of scale normalization on an arbitrary interquantile range, which is not only theoretically general but also proves particularly convenient for the identification analysis. We then use an inductive “in-fill and out-expansion” algorithm to establish our main results.},
  archive      = {J_JOE},
  author       = {Wayne Yuan Gao},
  doi          = {10.1016/j.jeconom.2019.09.005},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {399-413},
  shortjournal = {J. Econ.},
  title        = {Nonparametric identification in index models of link formation},
  volume       = {215},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating production functions with robustness against
errors in the proxy variables. <em>JOE</em>, <em>215</em>(2), 375–398.
(<a href="https://doi.org/10.1016/j.jeconom.2019.05.024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new approach to the identification and estimation of production functions. It extends the literature on the structural estimation of production functions, which dates back to the seminal work of Olley and Pakes (1996), by relaxing the scalar-unobservable assumption about the proxy variables. The key additional assumption needed in the identification argument is the existence of two conditionally independent proxy variables. The proposed generalized method of moment (GMM) estimator is flexible and straightforward to apply. The method is applied to study how rapidly firms in the Chilean food-product industry adjust their inputs in response to shocks to their productivity.},
  archive      = {J_JOE},
  author       = {Yingyao Hu and Guofang Huang and Yuya Sasaki},
  doi          = {10.1016/j.jeconom.2019.05.024},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {375-398},
  shortjournal = {J. Econ.},
  title        = {Estimating production functions with robustness against errors in the proxy variables},
  volume       = {215},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Testing for stationarity at high frequency. <em>JOE</em>,
<em>215</em>(2), 341–374. (<a
href="https://doi.org/10.1016/j.jeconom.2019.09.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high frequency behavior of the KPSS test, which is most commonly used to test for stationarity , is analyzed in a continuous time framework. Our asymptotics show that the test has no discriminatory power at high frequency: It either always rejects stationarity or has no nontrivial power at high frequency. The test becomes valid at high frequency only when the bandwidth of its long-run variance estimate is chosen suitably in our framework. We also analyze the residual-based KPSS test for cointegration.},
  archive      = {J_JOE},
  author       = {Bibo Jiang and Ye Lu and Joon Y. Park},
  doi          = {10.1016/j.jeconom.2019.09.004},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {341-374},
  shortjournal = {J. Econ.},
  title        = {Testing for stationarity at high frequency},
  volume       = {215},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). N-prediction of generalized heteroscedastic transformation
regression models. <em>JOE</em>, <em>215</em>(2), 305–340. (<a
href="https://doi.org/10.1016/j.jeconom.2019.09.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chen (2010) and Khan (2001) consider quantile regression estimation subject to general transformation that permits general heteroscedasticity , but the resulting conditional quantile predictors converge at rates slower than the parametric rate. In this paper, we consider the estimation of a general transformation model subject to a multiplicative form of heteroscedasticity. Our estimators for the finite dimensional parameters, the transformation function and the resulting conditional quantile predictor all converge at the parametric rate. Monte Carlo simulation experiments show that our estimators and conditional quantile predictor perform well in finite samples.},
  archive      = {J_JOE},
  author       = {Songnian Chen and Hanghui Zhang},
  doi          = {10.1016/j.jeconom.2019.09.003},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {305-340},
  shortjournal = {J. Econ.},
  title        = {N-prediction of generalized heteroscedastic transformation regression models},
  volume       = {215},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparametric identification of discrete choice models with
lagged dependent variables. <em>JOE</em>, <em>215</em>(1), 286–304. (<a
href="https://doi.org/10.1016/j.jeconom.2019.08.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variation in covariates can be used to nonparametrically identify a discrete choice model with a lagged dependent variable and discrete unobserved heterogeneity (Kasahara and Shimotsu, 2009; Browning and Carro, 2014). In some cases the number of support points of the unobserved heterogeneity distribution is restricted only by the number of points of support in the distribution of the covariates . This paper provides conditions under which these models can be identified with continuous heterogeneity using continuous variation in the covariates. The identification argument is related to that of Honore and Lewbel (2002) in that it requires a “special regressor” (Lewbel, 1998), but it does not assume an additively separable latent index. Identification requires only 3 time periods, neither stationarity nor time homogeneity is imposed, and the distribution of the initial condition is not restricted apart from conditions required for the special regressor . I demonstrate the result through a Monte Carlo simulation .},
  archive      = {J_JOE},
  author       = {Benjamin Williams},
  doi          = {10.1016/j.jeconom.2019.08.005},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {286-304},
  shortjournal = {J. Econ.},
  title        = {Nonparametric identification of discrete choice models with lagged dependent variables},
  volume       = {215},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybrid stochastic local unit roots. <em>JOE</em>,
<em>215</em>(1), 257–285. (<a
href="https://doi.org/10.1016/j.jeconom.2019.05.023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two approaches have dominated formulations designed to capture small departures from unit root autoregressions . The first involves deterministic departures that include local-to-unity (LUR) and mildly (or moderately) integrated (MI) specifications where departures shrink to zero as the sample size n → ∞ n→∞ . The second approach allows for stochastic departures from unity, leading to stochastic unit root (STUR) specifications. This paper introduces a hybrid local stochastic unit root (LSTUR) specification that has both LUR and STUR components and allows for endogeneity in the time varying coefficient that introduces structural elements to the autoregression. This hybrid model generates trajectories that, upon normalization, have non-linear diffusion limit processes that link closely to models that have been studied in mathematical finance, particularly with respect to option pricing. It is shown that some LSTUR parameterizations have a mean and variance which are the same as a random walk process but with a kurtosis exceeding 3, a feature which is consistent with much financial data. We develop limit theory and asymptotic expansions for the process and document how inference in LUR and STUR autoregressions is affected asymptotically by ignoring one or the other component in the more general hybrid generating mechanism. In particular, we show how confidence belts constructed from the LUR model are affected by the presence of a STUR component in the generating mechanism. The import of these findings for empirical research is explored in an application to the spreads on US investment grade corporate debt.},
  archive      = {J_JOE},
  author       = {Offer Lieberman and Peter C.B. Phillips},
  doi          = {10.1016/j.jeconom.2019.05.023},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {257-285},
  shortjournal = {J. Econ.},
  title        = {Hybrid stochastic local unit roots},
  volume       = {215},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semiparametric estimation of a censored regression model
with endogeneity. <em>JOE</em>, <em>215</em>(1), 239–256. (<a
href="https://doi.org/10.1016/j.jeconom.2019.08.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Censoring and endogeneity are common in empirical applications. However, the existing semiparametric estimation methods for the censored regression model with endogeneity under an independence restriction are associated with some drawbacks. In this paper we propose a new semiparametric estimator that overcomes these drawbacks. We derive conditional quantile moment conditions for all the conditional quantiles and propose a moment-based estimator. In particular, we construct two types of moment conditions and develop a computationally attractive estimator. We show that our estimator is consistent and asymptotic normal. A Monte Carlo study indicates that our estimator performs well in finite samples and compares favorably with existing methods.},
  archive      = {J_JOE},
  author       = {Songnian Chen and Qian Wang},
  doi          = {10.1016/j.jeconom.2019.08.006},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {239-256},
  shortjournal = {J. Econ.},
  title        = {Semiparametric estimation of a censored regression model with endogeneity},
  volume       = {215},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sequential monitoring for changes from stationarity to mild
non-stationarity. <em>JOE</em>, <em>215</em>(1), 209–238. (<a
href="https://doi.org/10.1016/j.jeconom.2019.08.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop and study sequential testing procedures á la Chu et al. (1996) for on-line detection of changes in a time series from stationarity to mild forms of non-stationarity. The proposed tests are based on sequential CUSUM and KPSS-type detector processes, and are shown to provide consistent detection under a wide range of change point models, including changes in the parameters of ARMA and GARCH series from values within the model’s stationarity parameter region to values close (converging) to the stationarity boundary. Local asymptotic results are established giving precise descriptions of the time to detection under several of these models, which show that such procedures are powerful to detect a wide range of non-stationary characteristics, including changes in mean, volatility, and unit root behaviour. The proposed methods are investigated by means of a simulation study and in applications to monitoring for changes in trend and unit root behaviour in macroeconomic production series, and to detect changes in volatility of the S&amp;P-500 stock market index.},
  archive      = {J_JOE},
  author       = {Lajos Horváth and Zhenya Liu and Gregory Rice and Shixuan Wang},
  doi          = {10.1016/j.jeconom.2019.08.010},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {209-238},
  shortjournal = {J. Econ.},
  title        = {Sequential monitoring for changes from stationarity to mild non-stationarity},
  volume       = {215},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Identification and estimation of time-varying nonseparable
panel data models without stayers. <em>JOE</em>, <em>215</em>(1),
184–208. (<a
href="https://doi.org/10.1016/j.jeconom.2019.08.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the identification and estimation of nonseparable panel data models . We show that the structural function is nonparametrically identified when it is strictly increasing in a scalar unobservable variable, the conditional distributions of unobservable variables do not change over time, and the joint support of explanatory variables satisfies some weak assumptions . To identify the target parameters, existing studies assume that the structural function does not change over time, and that there are “stayers”, namely individuals with the same regressor values in two time periods. Our approach, by contrast, allows the structural function to depend on the time period in an arbitrary manner and does not require the existence of stayers. In estimation part of the paper, we propose parametric and nonparametric estimators that implement our identification results. Monte Carlo studies indicate that our parametric estimator performs well in finite samples. Finally, we extend our identification results to models with discrete outcomes, and show that the structural function is partially identified.},
  archive      = {J_JOE},
  author       = {Takuya Ishihara},
  doi          = {10.1016/j.jeconom.2019.08.008},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {184-208},
  shortjournal = {J. Econ.},
  title        = {Identification and estimation of time-varying nonseparable panel data models without stayers},
  volume       = {215},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Non-standard inference for augmented double autoregressive
models with null volatility coefficients. <em>JOE</em>, <em>215</em>(1),
165–183. (<a
href="https://doi.org/10.1016/j.jeconom.2019.08.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers an augmented double autoregressive (DAR) model, which allows null volatility coefficients to circumvent the over-parameterization problem in the DAR model. Since the volatility coefficients might be on the boundary, the statistical inference methods based on the Gaussian quasi-maximum likelihood estimation (GQMLE) become non-standard, and their asymptotics require the data to have a finite sixth moment, which narrows the applicable scope in studying heavy-tailed data. To overcome this deficiency, this paper develops a systematic statistical inference procedure based on the self-weighted GQMLE for the augmented DAR model. Except for the Lagrange multiplier test statistic, the Wald, quasi-likelihood ratio and portmanteau test statistics are all shown to have non-standard asymptotics. The entire procedure is valid as long as the data are stationary, and its usefulness is illustrated by simulation studies and one real example.},
  archive      = {J_JOE},
  author       = {Feiyu Jiang and Dong Li and Ke Zhu},
  doi          = {10.1016/j.jeconom.2019.08.009},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {165-183},
  shortjournal = {J. Econ.},
  title        = {Non-standard inference for augmented double autoregressive models with null volatility coefficients},
  volume       = {215},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inference on distribution functions under measurement error.
<em>JOE</em>, <em>215</em>(1), 131–164. (<a
href="https://doi.org/10.1016/j.jeconom.2019.09.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with inference on the cumulative distribution function (cdf) F X ∗ FX∗ in the classical measurement error model X = X ∗ + ϵ X=X∗+ϵ . We consider the case where the density of the measurement error ϵ ϵ is unknown and estimated by repeated measurements, and show validity of a bootstrap approximation for the distribution of the deviation in the sup-norm between the deconvolution cdf estimator and F X ∗ FX∗ . We allow the density of ϵ ϵ to be ordinary or super smooth. We also provide several theoretical results on the bootstrap and asymptotic Gumbel approximations of the sup-norm deviation for the case where the density of ϵ ϵ is known. Our approximation results are applicable to various contexts, such as confidence bands for F X ∗ FX∗ and its quantiles , and for performing various cdf-based tests such as goodness-of-fit tests for parametric models of X ∗ X∗ , two sample homogeneity tests, and tests for stochastic dominance . Simulation and real data examples illustrate satisfactory performance of the proposed methods.},
  archive      = {J_JOE},
  author       = {Karun Adusumilli and Daisuke Kurisu and Taisuke Otsu and Yoon-Jae Whang},
  doi          = {10.1016/j.jeconom.2019.09.002},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {131-164},
  shortjournal = {J. Econ.},
  title        = {Inference on distribution functions under measurement error},
  volume       = {215},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ultrahigh dimensional precision matrix estimation via
refitted cross validation. <em>JOE</em>, <em>215</em>(1), 118–130. (<a
href="https://doi.org/10.1016/j.jeconom.2019.08.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a new estimation procedure for ultrahigh dimensional sparse precision matrix, the inverse of covariance matrix . Regularization methods have been proposed for sparse precision matrix estimation, but they may not perform well with ultrahigh dimensional data due to the spurious correlation . We propose a refitted cross validation (RCV) method for sparse precision matrix estimation based on its Cholesky decomposition , which does not require the Gaussian assumption. The proposed RCV procedure can be easily implemented with existing software for ultrahigh dimensional linear regression. We establish the consistency of the proposed RCV estimation and show that the rate of convergence of the RCV estimation without assuming banded structure is the same as that of those assuming the banded structure in Bickel and Levina (2008b). Monte Carlo studies were conducted to access the finite sample performance of the RCV estimation. Our numerical comparison shows that the RCV estimation outperforms the existing ones in various scenarios. We further apply the RCV estimation for an empirical analysis of asset allocation.},
  archive      = {J_JOE},
  author       = {Luheng Wang and Zhao Chen and Christina Dan Wang and Runze Li},
  doi          = {10.1016/j.jeconom.2019.08.004},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {118-130},
  shortjournal = {J. Econ.},
  title        = {Ultrahigh dimensional precision matrix estimation via refitted cross validation},
  volume       = {215},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A goodness-of-fit test for copulas based on martingale
transformation. <em>JOE</em>, <em>215</em>(1), 84–117. (<a
href="https://doi.org/10.1016/j.jeconom.2019.08.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an asymptotically distribution-free test for copulas with dynamic marginal distributions, such as GARCH and ARMA processes. The test is based on the empirical copula process with parametrically estimated marginal distributions. By applying the Khmaladze (1982, 1988, 1993) martingale transformation method, the transformed empirical process converges to a standard Gaussian process , so the resulting test statistics are asymptotically distribution-free. Monte Carlo simulations show that the test performs well in finite samples. An empirical application to test copulas between EUR/USD and GBP/USD exchange rates is provided.},
  archive      = {J_JOE},
  author       = {Xiaohui Lu and Xu Zheng},
  doi          = {10.1016/j.jeconom.2019.08.007},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {84-117},
  shortjournal = {J. Econ.},
  title        = {A goodness-of-fit test for copulas based on martingale transformation},
  volume       = {215},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Determining individual or time effects in panel data models.
<em>JOE</em>, <em>215</em>(1), 60–83. (<a
href="https://doi.org/10.1016/j.jeconom.2019.07.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose a jackknife method to determine individual and time effects in linear panel data models . We first show that when both the serial and cross-sectional correlations among the idiosyncratic error terms are weak, our jackknife method can pick up the correct model with probability approaching one (w.p.a.1). In the presence of moderate or strong degree of serial correlation , we modify our jackknife criterion function and show that the modified jackknife method can also select the correct model w.p.a.1. We conduct Monte Carlo simulations to show that our new methods perform remarkably well in finite samples. We apply our methods to study i i the crime rates in North Carolina, i i ii the determinants of saving rates across countries, and i i i iii the relationship between guns and crime rates in the U.S.},
  archive      = {J_JOE},
  author       = {Xun Lu and Liangjun Su},
  doi          = {10.1016/j.jeconom.2019.07.008},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {60-83},
  shortjournal = {J. Econ.},
  title        = {Determining individual or time effects in panel data models},
  volume       = {215},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Does modeling a structural break improve forecast accuracy?
<em>JOE</em>, <em>215</em>(1), 35–59. (<a
href="https://doi.org/10.1016/j.jeconom.2019.07.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mean square forecast error loss implies a bias–variance trade-off that suggests that structural breaks of small magnitude should be ignored. In this paper, we provide a test to determine whether modeling a structural break improves forecast accuracy. The test is near optimal even when the date of a local-to-zero break is not consistently estimable. The results extend to forecast combinations that weight the post-break sample and the full sample forecasts by our test statistic. In a large number of macroeconomic time series, we find that structural breaks that are relevant for forecasting occur much less frequently than existing tests indicate.},
  archive      = {J_JOE},
  author       = {Tom Boot and Andreas Pick},
  doi          = {10.1016/j.jeconom.2019.07.007},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {35-59},
  shortjournal = {J. Econ.},
  title        = {Does modeling a structural break improve forecast accuracy?},
  volume       = {215},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inference for local distributions at high sampling
frequencies: A bootstrap approach. <em>JOE</em>, <em>215</em>(1), 1–34.
(<a href="https://doi.org/10.1016/j.jeconom.2019.09.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study inference for the local innovations of Itô semimartingales. Specifically, we construct a resampling procedure for the empirical CDF of high-frequency innovations that have been standardized using a nonparametric estimate of its stochastic scale (volatility) and truncated to rid the effect of “large” jumps. Our locally dependent wild bootstrap (LDWB) accommodate issues related to the stochastic scale and jumps as well as account for a special block-wise dependence structure induced by sampling errors. We show that the LDWB replicates first and second-order limit theory from the usual empirical process and the stochastic scale estimate, respectively, in addition to an asymptotic bias . Moreover, we design the LDWB sufficiently general to establish asymptotic equivalence between it and a nonparametric local block bootstrap , also introduced here, up to second-order distribution theory. Finally, we introduce LDWB-aided Kolmogorov–Smirnov tests for local Gaussianity as well as local von-Mises statistics , with and without bootstrap inference, and establish their asymptotic validity using the second-order distribution theory. The finite sample performance of CLT and LDWB-aided local Gaussianity tests is assessed in a simulation study and an empirical application. Whereas the CLT test is oversized, even in large samples, the size of the LDWB tests is accurate, even in small samples. The empirical analysis verifies this pattern, in addition to providing new insights about the fine scale distributional properties of innovations to equity indices, commodities and exchange rates.},
  archive      = {J_JOE},
  author       = {Ulrich Hounyo and Rasmus T. Varneskov},
  doi          = {10.1016/j.jeconom.2019.09.001},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {1-34},
  shortjournal = {J. Econ.},
  title        = {Inference for local distributions at high sampling frequencies: A bootstrap approach},
  volume       = {215},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Annual award announcement. <em>JOE</em>, <em>214</em>(2),
iii. (<a href="https://doi.org/10.1016/S0304-4076(19)30268-4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOE},
  doi          = {10.1016/S0304-4076(19)30268-4},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {iii},
  shortjournal = {J. Econ.},
  title        = {Annual award announcement},
  volume       = {214},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modelling regional patterns of inefficiency: A bayesian
approach to geoadditive panel stochastic frontier analysis with an
application to cereal production in england and wales. <em>JOE</em>,
<em>214</em>(2), 513–539. (<a
href="https://doi.org/10.1016/j.jeconom.2019.07.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a flexible Bayesian approach to inefficiency modelling that accounts for regional patterns of local performance. The model allows for a separated treatment of individual heterogeneity and determinants of inefficiency. Regional dependence structures and location-specific unobserved spatial heterogeneity are modelled via geoadditive predictors in the inefficiency term of the stochastic frontier model. Inference becomes feasible through Markov chain Monte Carlo simulation techniques. In an empirical illustration we find that regional patterns of inefficiency characterize cereal production in England and Wales. Neglecting common performance patterns of farms located in the same region induces systematic biases to inefficiency estimates.},
  archive      = {J_JOE},
  author       = {Nadja Klein and Helmut Herwartz and Thomas Kneib},
  doi          = {10.1016/j.jeconom.2019.07.003},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {513-539},
  shortjournal = {J. Econ.},
  title        = {Modelling regional patterns of inefficiency: A bayesian approach to geoadditive panel stochastic frontier analysis with an application to cereal production in england and wales},
  volume       = {214},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust estimation with many instruments. <em>JOE</em>,
<em>214</em>(2), 495–512. (<a
href="https://doi.org/10.1016/j.jeconom.2019.04.040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear instrumental variables models are widely used in empirical work, but often associated with low estimator precision. This paper proposes an estimator that is robust to outliers and shows that the estimator is minimax optimal in a class of estimators that includes the limited maximum likelihood estimator (LIML). Intuitively, this optimal robust estimator combines LIML with Winsorization of the structural residuals and the Winsorization leads to improved precision under thick-tailed error distributions. Consistency and asymptotic normality of the estimator are established under many instruments asymptotics and a consistent variance estimator which allows for asymptotically valid inference is provided.},
  archive      = {J_JOE},
  author       = {Mikkel Sølvsten},
  doi          = {10.1016/j.jeconom.2019.04.040},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {495-512},
  shortjournal = {J. Econ.},
  title        = {Robust estimation with many instruments},
  volume       = {214},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-dimensional minimum variance portfolio estimation based
on high-frequency data. <em>JOE</em>, <em>214</em>(2), 482–494. (<a
href="https://doi.org/10.1016/j.jeconom.2019.04.039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the estimation of high-dimensional minimum variance portfolio (MVP) based on the high frequency returns which can exhibit heteroscedasticity and possibly be contaminated by microstructure noise. Under certain sparsity assumptions on the precision matrix, we propose estimators of the MVP and prove that our portfolios asymptotically achieve the minimum variance in a sharp sense. In addition, we introduce consistent estimators of the minimum variance, which provide reference targets. Simulation and empirical studies demonstrate the favorable performance of the proposed portfolios.},
  archive      = {J_JOE},
  author       = {T. Tony Cai and Jianchang Hu and Yingying Li and Xinghua Zheng},
  doi          = {10.1016/j.jeconom.2019.04.039},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {482-494},
  shortjournal = {J. Econ.},
  title        = {High-dimensional minimum variance portfolio estimation based on high-frequency data},
  volume       = {214},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Panel threshold regressions with latent group structures.
<em>JOE</em>, <em>214</em>(2), 451–481. (<a
href="https://doi.org/10.1016/j.jeconom.2019.07.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the least squares estimation of a panel structure threshold regression (PSTR) model where both the slope coefficients and threshold parameters may exhibit latent group structures. We study the asymptotic properties of the estimators of the latent group structure and the slope and threshold coefficients. We show that we can estimate the latent group structure correctly with probability approaching 1 and the estimators of the slope and threshold coefficients are asymptotically equivalent to the infeasible estimators that are obtained as if the true group structures were known. We study likelihood-ratio-based inferences on the group-specific threshold parameters under the shrinking-threshold-effect framework. We also propose two specification tests: one tests whether the threshold parameters are homogeneous across groups, and the other tests whether the threshold effects are present. When the number of latent groups is unknown, we propose a BIC-type information criterion to determine the number of groups in the data. Simulations demonstrate that our estimators and tests perform reasonably well in finite samples. We apply our model to revisit the relationship between capital market imperfection and the investment behavior of firms and to examine the impact of bank deregulation on income inequality. We document a large degree of heterogeneous effects in both applications that cannot be captured by conventional panel threshold regressions.},
  archive      = {J_JOE},
  author       = {Ke Miao and Liangjun Su and Wendun Wang},
  doi          = {10.1016/j.jeconom.2019.07.006},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {451-481},
  shortjournal = {J. Econ.},
  title        = {Panel threshold regressions with latent group structures},
  volume       = {214},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inference in heavy-tailed vector error correction models.
<em>JOE</em>, <em>214</em>(2), 433–450. (<a
href="https://doi.org/10.1016/j.jeconom.2019.03.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper first studies the full rank least squares estimator (FLSE) of the heavy-tailed vector error correction (VEC) models. It is shown that the rate of convergence of the FLSE related to the long-run parameters is n n (sample size) and its limiting distribution is a stochastic integral in terms of two stable random processes when the tail index α ∈ ( 0 , 2 ) α∈(0,2) . Furthermore, we show that the rate of convergence of the FLSE related to the short-term parameters is n 1 ∕ α L ̃ ( n ) n1∕αL̃(n) and its limiting distribution is a functional of two stable processes when α ∈ ( 1 , 2 ) α∈(1,2) , where L ̃ ( n ) L̃(n) is a slowly varying function . However, when α ∈ ( 0 , 1 ) α∈(0,1) , we show that the rate of convergence of the FLSE related to the short-term parameters is n n and its limiting distribution not only depends on the stationary component itself but also depends on the unit root component. Based on the FLSE, we then study the limiting behavior of the reduced rank LSE (RLSE). The results related to the short-term parameters of both FLSE and RLSE are significantly different from those of heavy-tailed time series in the literature, and it may provide new insights in the area for future research. Simulation study is carried out to demonstrate the performance of both estimators. A real example with application to 3-month Treasury Bill rate, 1-year Treasury Bill rate and Federal Fund rate is given.},
  archive      = {J_JOE},
  author       = {Rui She and Shiqing Ling},
  doi          = {10.1016/j.jeconom.2019.03.008},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {433-450},
  shortjournal = {J. Econ.},
  title        = {Inference in heavy-tailed vector error correction models},
  volume       = {214},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Measurement error in multiple equations: Tobin’s q and
corporate investment, saving, and debt. <em>JOE</em>, <em>214</em>(2),
413–432. (<a
href="https://doi.org/10.1016/j.jeconom.2019.08.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We characterize the sharp identification regions for the coefficients in a system of linear equations that share an explanatory variable measured with classical error. We demonstrate the identification gain from analyzing the equations jointly. We derive the sharp identification regions under any configuration of three auxiliary assumptions. These restrict the “noise-to-signal” ratio, the coefficients of determination , and the signs of the correlations among the cross-equation disturbances. For inference, we implement results on intersection bounds. The application studies the effects of cash flow on the investment, saving, and debt of firms when Tobin’s q serves as a proxy for marginal q.},
  archive      = {J_JOE},
  author       = {Karim Chalak and Daniel Kim},
  doi          = {10.1016/j.jeconom.2019.08.001},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {413-432},
  shortjournal = {J. Econ.},
  title        = {Measurement error in multiple equations: Tobin’s q and corporate investment, saving, and debt},
  volume       = {214},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On rank estimators in increasing dimensions. <em>JOE</em>,
<em>214</em>(2), 379–412. (<a
href="https://doi.org/10.1016/j.jeconom.2019.08.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The family of rank estimators, including Han’s maximum rank correlation (Han, 1987) as a notable example, has been widely exploited in studying regression problems. For these estimators, although the linear index is introduced for alleviating the impact of dimensionality, the effect of large dimension on inference is rarely studied. This paper fills this gap via studying the statistical properties of a larger family of M-estimators, whose objective functions are formulated as U-processes and may be discontinuous in increasing dimension set-up where the number of parameters, p n pn , in the model is allowed to increase with the sample size, n n . First, we find that often in estimation, as p n ∕ n → 0 pn∕n→0 , ( p n ∕ n ) 1 ∕ 2 (pn∕n)1∕2 rate of convergence is obtainable. Second, we establish Bahadur-type bounds and study the validity of normal approximation, which we find often requires a much stronger scaling requirement than p n 2 ∕ n → 0 pn2∕n→0 . Third, we state conditions under which the numerical derivative estimator of asymptotic covariance matrix is consistent, and show that the step size in implementing the covariance estimator has to be adjusted with respect to p n pn . All theoretical results are further backed up by simulation studies.},
  archive      = {J_JOE},
  author       = {Yanqin Fan and Fang Han and Wei Li and Xiao-Hua Zhou},
  doi          = {10.1016/j.jeconom.2019.08.003},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {379-412},
  shortjournal = {J. Econ.},
  title        = {On rank estimators in increasing dimensions},
  volume       = {214},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparametric assessment of hedge fund performance.
<em>JOE</em>, <em>214</em>(2), 349–378. (<a
href="https://doi.org/10.1016/j.jeconom.2019.08.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new class of performance measures for Hedge Fund (HF) returns based on a family of empirically identifiable stochastic discount factors (SDFs). The SDF-based measures incorporate no-arbitrage pricing restrictions and naturally embed information about higher-order mixed moments between HF and benchmark factors returns. We provide a full asymptotic theory for our SDF estimators to test for the statistical significance of each fund’s performance and for the relevance of individual benchmark factors within each proposed measure. We apply our methodology to a panel of 4815 individual hedge funds. Our empirical analysis reveals that fewer funds have a statistically significant positive alpha compared to the Jensen’s alpha obtained by the traditional linear regression approach. Moreover, the funds’ rankings vary considerably between the two approaches. Performance also varies between the members of our family because of a different fund exposure to higher-order moments of the benchmark factors, highlighting the potential heterogeneity across investors in evaluating performance.},
  archive      = {J_JOE},
  author       = {Caio Almeida and Kym Ardison and René Garcia},
  doi          = {10.1016/j.jeconom.2019.08.002},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {349-378},
  shortjournal = {J. Econ.},
  title        = {Nonparametric assessment of hedge fund performance},
  volume       = {214},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variance disparity and market frictions. <em>JOE</em>,
<em>214</em>(2), 326–348. (<a
href="https://doi.org/10.1016/j.jeconom.2019.07.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new model-free approach to measuring the expectation of market variance using VIX derivatives. This approach shows that VIX derivatives carry different information about future variance than S&amp;P 500 (SPX) options, especially during the 2008 financial crisis. I find that the segmentation is associated with frictions such as funding illiquidity, market illiquidity, and asymmetric information. When they are segmented, VIX derivatives contribute more to the variance discovery process than SPX options. These findings imply that VIX derivatives would offer a better estimate of expected variance than SPX options, and that a measure of segmentation may be useful for policymakers as it signals the severity of frictions.},
  archive      = {J_JOE},
  author       = {Yang-Ho Park},
  doi          = {10.1016/j.jeconom.2019.07.005},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {326-348},
  shortjournal = {J. Econ.},
  title        = {Variance disparity and market frictions},
  volume       = {214},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparametric filtering of conditional state-price
densities. <em>JOE</em>, <em>214</em>(2), 295–325. (<a
href="https://doi.org/10.1016/j.jeconom.2019.05.022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the use of noisy high-frequency data to estimate the time-varying state-price density implicit in European option prices. A dynamic kernel estimator of the conditional pricing function and its derivatives is proposed that can be used for model-free risk measurement. Infill asymptotic theory is derived that applies when the pricing function is either smoothly varying or driven by diffusive state variables. Trading times and moneyness levels are modeled by marked point processes that capture intraday trading patterns. A simulation study investigates the performance of the estimator using a varying plug-in bandwidth in various scenarios. Empirical analysis using S&amp;P 500 E-mini European option quotes reveals significant time-variation at intraday frequencies. An application towards delta- and minimum variance-hedging further illustrates the use of the estimator.},
  archive      = {J_JOE},
  author       = {Jeroen Dalderop},
  doi          = {10.1016/j.jeconom.2019.05.022},
  journal      = {Journal of Econometrics},
  number       = {2},
  pages        = {295-325},
  shortjournal = {J. Econ.},
  title        = {Nonparametric filtering of conditional state-price densities},
  volume       = {214},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evaluating trends in time series of distributions: A spatial
fingerprint of human effects on climate. <em>JOE</em>, <em>214</em>(1),
274–294. (<a
href="https://doi.org/10.1016/j.jeconom.2019.05.014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze a time series of global temperature anomaly distributions to identify and estimate persistent features in climate change. We employ a formal test for the existence of functional unit roots in the time series of these densities, and we develop a new test to distinguish functional unit roots from functional deterministic trends or explosive behavior. Results suggest that temperature anomalies contain stochastic trends (as opposed to deterministic trends or explosive roots), two trends are present in the Northern Hemisphere while one stochastic trend is present in the Southern Hemisphere, and the probabilities of observing moderately positive anomalies have increased. We postulate that differences in the pattern and number of unit roots in each hemisphere may be due to a natural experiment which causes human emissions of greenhouse gases and sulfur to be greater in the Northern Hemisphere, decreasing the mean temperature anomaly but increasing the spatial variance relative to the Southern Hemisphere. Together, these results are consistent with the theory of anthropogenic climate change.},
  archive      = {J_JOE},
  author       = {Yoosoon Chang and Robert K. Kaufmann and Chang Sik Kim and J. Isaac Miller and Joon Y. Park and Sungkeun Park},
  doi          = {10.1016/j.jeconom.2019.05.014},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {274-294},
  shortjournal = {J. Econ.},
  title        = {Evaluating trends in time series of distributions: A spatial fingerprint of human effects on climate},
  volume       = {214},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Econometric modelling of climate systems: The equivalence of
energy balance models and cointegrated vector autoregressions.
<em>JOE</em>, <em>214</em>(1), 256–273. (<a
href="https://doi.org/10.1016/j.jeconom.2019.05.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimates of both the human impact on climate as well as the economic impacts of climate change are crucial to inform policy decisions. Econometric modelling allows us to quantify these impacts and their uncertainties, but models have to be consistent with the underlying physics and the time series properties of the data. Here I show that energy-balance models of climate are equivalent to an econometric cointegrated system and can be estimated in discrete time. This equivalence provides a basis for the use of cointegration methods to estimate climate responses and test their feedback. Further, it is possible to use the estimated parameters to quantify uncertainties in integrated assessment models of the economic impacts of climate change. In an application I estimate a system of temperatures, ocean heat content, and radiative forcing including greenhouse gases, and find statistical support for the cointegrated energy balance model. Accounting for structural breaks from volcanic eruptions highlights large parameter uncertainties and shows that previous empirical estimates of the temperature response to increased CO 2 2 concentrations may be misleadingly low due to model-misspecification.},
  archive      = {J_JOE},
  author       = {Felix Pretis},
  doi          = {10.1016/j.jeconom.2019.05.013},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {256-273},
  shortjournal = {J. Econ.},
  title        = {Econometric modelling of climate systems: The equivalence of energy balance models and cointegrated vector autoregressions},
  volume       = {214},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fully modified OLS estimation and inference for seemingly
unrelated cointegrating polynomial regressions and the environmental
kuznets curve for carbon dioxide emissions. <em>JOE</em>,
<em>214</em>(1), 216–255. (<a
href="https://doi.org/10.1016/j.jeconom.2019.05.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops two fully modified OLS (FM-OLS) estimators for systems of seemingly unrelated cointegrating polynomial regressions , i.e., systems of regressions that include deterministic variables, integrated processes, and integer powers of integrated processes as explanatory variables . The stationary errors are allowed to be serially correlated and the regressors to be endogenous. Furthermore, the errors and regressors are allowed to be dynamically cross-sectionally correlated. The developed estimators have zero mean Gaussian mixture limiting distributions that allow for asymptotic chi-squared inference. The Wald-type hypothesis test results are the basis for considering detailed tests for general forms of group-wise poolability. We provide the corresponding group-wise pooled variants of our estimators, in case the poolability restrictions are not rejected. Our simulations indicate that appropriate pooling leads, as expected, to improved performance of the estimators and tests. Data-driven group-wise pooling turns out to be crucial in our illustrative application that analyzes the environmental Kuznets curve for CO 2 2 emissions for six early industrialized countries.},
  archive      = {J_JOE},
  author       = {Martin Wagner and Peter Grabarczyk and Seung Hyun Hong},
  doi          = {10.1016/j.jeconom.2019.05.012},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {216-255},
  shortjournal = {J. Econ.},
  title        = {Fully modified OLS estimation and inference for seemingly unrelated cointegrating polynomial regressions and the environmental kuznets curve for carbon dioxide emissions},
  volume       = {214},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Global hemispheric temperatures and co-shifting: A vector
shifting-mean autoregressive analysis. <em>JOE</em>, <em>214</em>(1),
198–215. (<a
href="https://doi.org/10.1016/j.jeconom.2019.05.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper examines local changes in annual temperature data for the northern and southern hemispheres (1850–2017) by using a multivariate generalization of the shifting-meanautoregressive model of González and Teräsvirta (2008). Univariate models are first fitted to each series by using the QuickShift methodology. Full information maximum likelihood estimates of a bivariate system of temperature equations are then obtained and asymptotic properties of the corresponding estimators considered. The system is then used to perform formal tests of co-movements, called co-shifting, in the series. The results show evidence of co-shifting in the two series.},
  archive      = {J_JOE},
  author       = {Matthew T. Holt and Timo Teräsvirta},
  doi          = {10.1016/j.jeconom.2019.05.011},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {198-215},
  shortjournal = {J. Econ.},
  title        = {Global hemispheric temperatures and co-shifting: A vector shifting-mean autoregressive analysis},
  volume       = {214},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multicointegration model of global climate change.
<em>JOE</em>, <em>214</em>(1), 175–197. (<a
href="https://doi.org/10.1016/j.jeconom.2019.05.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We model the role of the ocean in climate change, using the concept of multicointegration. Surface temperature and radiative forcing cointegrate and the accumulated cointegration disequilibria represent the change in Earth system heat content, which is predominantly stored in the ocean. System heat content in turn cointegrates with surface temperature. Using a multicointegrating I(2) model, we find that the climate sensitivity is 2.8 °C and the rate of adjustment to equilibrium is realistically slow. These results contrast strongly with those from I(1) cointegration models and are more consistent with global circulation models. We also estimate Earth system heat content as a latent variable for the full period, 1850–2014, and this predicted heat content cointegrates with available ocean heat content observations for 1940–2014.},
  archive      = {J_JOE},
  author       = {Stephan B. Bruns and Zsuzsanna Csereklyei and David I. Stern},
  doi          = {10.1016/j.jeconom.2019.05.010},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {175-197},
  shortjournal = {J. Econ.},
  title        = {A multicointegration model of global climate change},
  volume       = {214},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Trends in distributional characteristics: Existence of
global warming. <em>JOE</em>, <em>214</em>(1), 153–174. (<a
href="https://doi.org/10.1016/j.jeconom.2019.05.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {What type of global warming exists? This study introduces a novel methodology to answer this question, which is the starting point for all issues related to climate change analyses. Global warming is defined as an increasing trend in certain distributional characteristics (moments, quantiles , etc.) of global temperatures, in addition to simply examining the average values. Temperatures are viewed as a functional stochastic process from which we obtain distributional characteristics as time series objects. Here, we present a simple robust trend test and prove that it is able to detect the existence of an unknown trend component (deterministic or stochastic) in these characteristics. Applying this trend test to daily temperatures in Central England (for the period 1772–2017) and to global cross-sectional temperatures (1880–2015), we obtain the same strong conclusions: (i) there is an increasing trend in all distributional characteristics (time series and cross-sectional), and this trend is larger in the lower quantiles than it is in the mean, median, and upper quantiles; (ii) there is a negative trend in the characteristics that measure dispersion (i.e., lower temperatures approach the median faster than higher temperatures do). This type of global warming has more serious consequences than those found by analyzing only the average.},
  archive      = {J_JOE},
  author       = {María Dolores Gadea Rivas and Jesús Gonzalo},
  doi          = {10.1016/j.jeconom.2019.05.009},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {153-174},
  shortjournal = {J. Econ.},
  title        = {Trends in distributional characteristics: Existence of global warming},
  volume       = {214},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inference related to common breaks in a multivariate system
with joined segmented trends with applications to global and hemispheric
temperatures. <em>JOE</em>, <em>214</em>(1), 130–152. (<a
href="https://doi.org/10.1016/j.jeconom.2019.05.008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {What transpires from recent research is that temperatures and radiative forcing seem to be characterized by a linear trend with two changes in the rate of growth. The first occurs in the early 60s and indicates a very large increase in the rate of growth of both temperature and radiative forcing series. This was termed as the “onset of sustained global warming”. The second is related to the more recent so-called hiatus period, which suggests that temperatures and total radiative forcing have increased less rapidly since the mid-90s compared to the larger rate of increase from 1960 to 1990. There are two issues that remain unresolved. The first is whether the breaks in the slope of the trend functions of temperatures and radiative forcing are common. This is important because common breaks coupled with the basic science of climate change would strongly suggest a causal effect from anthropogenic factors to temperatures. The second issue relates to establishing formally via a proper testing procedure that takes into account the noise in the series, whether there was indeed a ‘hiatus period’ for temperatures since the mid 90s. This is important because such a test would counter the widely held view that the hiatus is the product of natural internal variability. Our paper provides tests related to both issues. The results show that the breaks in temperatures and radiative forcing are common and that the hiatus is characterized by a significant decrease in their rate of growth. The statistical results are of independent interest and applicable more generally.},
  archive      = {J_JOE},
  author       = {Dukpa Kim and Tatsushi Oka and Francisco Estrada and Pierre Perron},
  doi          = {10.1016/j.jeconom.2019.05.008},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {130-152},
  shortjournal = {J. Econ.},
  title        = {Inference related to common breaks in a multivariate system with joined segmented trends with applications to global and hemispheric temperatures},
  volume       = {214},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Expected utility and catastrophic risk in a stochastic
economy–climate model. <em>JOE</em>, <em>214</em>(1), 110–129. (<a
href="https://doi.org/10.1016/j.jeconom.2019.05.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze a stochastic dynamic finite-horizon economic model with climate change, in which the social planner faces uncertainty about future climate change and its economic damages. Our model (SDICE*) incorporates, possibly heavy-tailed, stochasticity in Nordhaus’ deterministic DICE model. We develop a regression-based numerical method for solving a general class of dynamic finite-horizon economy–climate models with potentially heavy-tailed uncertainty and general utility functions. We then apply this method to SDICE* and examine the effects of light- and heavy-tailed uncertainty. The results indicate that the effects can be substantial, depending on the nature and extent of the uncertainty and the social planner’s preferences.},
  archive      = {J_JOE},
  author       = {Masako Ikefuji and Roger J.A. Laeven and Jan R. Magnus and Chris Muris},
  doi          = {10.1016/j.jeconom.2019.05.007},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {110-129},
  shortjournal = {J. Econ.},
  title        = {Expected utility and catastrophic risk in a stochastic economy–climate model},
  volume       = {214},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Autoregressive wild bootstrap inference for nonparametric
trends. <em>JOE</em>, <em>214</em>(1), 81–109. (<a
href="https://doi.org/10.1016/j.jeconom.2019.05.006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose an autoregressive wild bootstrap method to construct confidence bands around a smooth deterministic trend. The bootstrap method is easy to implement and does not require any adjustments in the presence of missing data, which makes it particularly suitable for climatological applications. We establish the asymptotic validity of the bootstrap method for both pointwise and simultaneous confidence bands under general conditions, allowing for general patterns of missing data, serial dependence and heteroskedasticity. The finite sample properties of the method are studied in a simulation study. We use the method to study the evolution of trends in daily measurements of atmospheric ethane obtained from a weather station in the Swiss Alps, where the method can easily deal with the many missing observations due to adverse weather conditions.},
  archive      = {J_JOE},
  author       = {Marina Friedrich and Stephan Smeekes and Jean-Pierre Urbain},
  doi          = {10.1016/j.jeconom.2019.05.006},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {81-109},
  shortjournal = {J. Econ.},
  title        = {Autoregressive wild bootstrap inference for nonparametric trends},
  volume       = {214},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Statistical approximation of high-dimensional climate
models. <em>JOE</em>, <em>214</em>(1), 67–80. (<a
href="https://doi.org/10.1016/j.jeconom.2019.05.005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a general emulation method for constructing low-dimensional approximations of complex dynamic climate models. Our method uses artificially designed uncorrelated CO 2 emissions scenarios, which are much better suited for the construction of an emulator than are conventional emissions scenarios. We apply our method to the climate model MAGICC to approximate the impact of emissions on global temperature. Comparing the temperature forecasts of MAGICC and our emulator, we show that the average relative out-of-sample forecast errors in the low-dimensional emulation models are below 2\%. Our emulator offers an avenue to merge modern macroeconomic models with complex dynamic climate models.},
  archive      = {J_JOE},
  author       = {Alena Miftakhova and Kenneth L. Judd and Thomas S. Lontzek and Karl Schmedders},
  doi          = {10.1016/j.jeconom.2019.05.005},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {67-80},
  shortjournal = {J. Econ.},
  title        = {Statistical approximation of high-dimensional climate models},
  volume       = {214},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Long-term forecasting of el niño events via dynamic factor
simulations. <em>JOE</em>, <em>214</em>(1), 46–66. (<a
href="https://doi.org/10.1016/j.jeconom.2019.05.004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new forecasting procedure which particularly explores opportunities for improving the precision of medium and long-term forecasts of the Niño3.4 time series that is linked with the well-known El Niño phenomenon. This important climatic time series is subject to an intricate dynamic structure and is interrelated to other climatological variables. The procedure consists of three steps. First, a univariate time series model is considered for producing prediction errors. Second, signal paths of the prediction errors are simulated via a dynamic factor model for the errors and explanatory variables . From these simulated errors, ensemble time series for Niño3.4 are constructed. Third, forecasts are generated from the ensemble time series and their sample average is our final forecast. As part of these dynamic factor simulations, we also obtain the forecast of the El Niño event which is a categorical variable . We present empirical evidence that our procedure can be superior in its forecasting performance when compared to other econometric forecasting methods.},
  archive      = {J_JOE},
  author       = {Mengheng Li and Siem Jan Koopman and Rutger Lit and Desislava Petrova},
  doi          = {10.1016/j.jeconom.2019.05.004},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {46-66},
  shortjournal = {J. Econ.},
  title        = {Long-term forecasting of el niño events via dynamic factor simulations},
  volume       = {214},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling time series when some observations are zero.
<em>JOE</em>, <em>214</em>(1), 33–45. (<a
href="https://doi.org/10.1016/j.jeconom.2019.05.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sometimes a significant proportion of observations in a time series are zero, but the remaining observations are positive and measured on a continuous scale. We propose a new dynamic model in which the conditional distribution of the observations is constructed by shifting a distribution for non-zero observations to the left and censoring negative values. The key to generalizing the censoring approach to the dynamic case is to have (the logarithm of) the location/scale parameter driven by a filter that depends on the score of the conditional distribution. An exponential link function means that seasonal effects can be incorporated into the model and this is done by means of a cubic spline (which can potentially be time-varying). The model is fitted to daily rainfall in locations in northern Australia and England and compared with a dynamic zero-augmented model.},
  archive      = {J_JOE},
  author       = {Andrew Harvey and Ryoko Ito},
  doi          = {10.1016/j.jeconom.2019.05.003},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {33-45},
  shortjournal = {J. Econ.},
  title        = {Modeling time series when some observations are zero},
  volume       = {214},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Econometric estimates of earth’s transient climate
sensitivity. <em>JOE</em>, <em>214</em>(1), 6–32. (<a
href="https://doi.org/10.1016/j.jeconom.2019.05.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How sensitive is Earth’s climate to a given increase in atmospheric greenhouse gas (GHG) concentrations? This long-standing question in climate science was recently analyzed by dynamic panel data methods using extensive spatio-temporal data of global surface temperatures, solar radiation, and GHG concentrations over the last half century to 2010 (Storelvmo et al, 2016). Those methods revealed that atmospheric aerosol effects masked approximately one-third of the continental warming due to increasing GHG concentrations over this period, thereby implying greater climate sensitivity to GHGs than previously thought. The present study provides regularity conditions and asymptotic theory justifying the use of time series cointegration-based methods of estimation when there are both stochastic process and deterministic trends in the global forcing variables, such as GHGs, and station-level trend effects from such sources as local aerosol pollutants. The asymptotics validate estimation and confidence interval construction for econometric measures of Earth’s transient climate sensitivity (TCS). The methods are applied to observational data and to data generated from several groups of global climate models (GCMs) that are sampled spatio-temporally and aggregated in the same way as the empirical observations for the time period 1964–2005. The findings indicate that 7 out of 9 of the GCM reported TCS values lie within the 95\% empirical confidence interval computed econometrically from the GCM output. The analysis shows the potential of econometric methods to provide empirical estimates and confidence limits for TCS, to calibrate GCM simulation output against observational data in terms of the implied TCS estimates obtained via the econometric model , and to reveal the respective sensitivity parameters (GHG and non-GHG related) governing GCM temperature trends.},
  archive      = {J_JOE},
  author       = {Peter C.B. Phillips and Thomas Leirvik and Trude Storelvmo},
  doi          = {10.1016/j.jeconom.2019.05.002},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {6-32},
  shortjournal = {J. Econ.},
  title        = {Econometric estimates of earth’s transient climate sensitivity},
  volume       = {214},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Econometric models of climate change: Introduction by the
guest editors. <em>JOE</em>, <em>214</em>(1), 1–5. (<a
href="https://doi.org/10.1016/j.jeconom.2019.05.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOE},
  author       = {Eric Hillebrand and Felix Pretis and Tommaso Proietti},
  doi          = {10.1016/j.jeconom.2019.05.001},
  journal      = {Journal of Econometrics},
  number       = {1},
  pages        = {1-5},
  shortjournal = {J. Econ.},
  title        = {Econometric models of climate change: Introduction by the guest editors},
  volume       = {214},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
