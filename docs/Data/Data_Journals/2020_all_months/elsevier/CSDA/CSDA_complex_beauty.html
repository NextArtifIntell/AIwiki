<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>CSDA_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="csda---169">CSDA - 169</h2>
<ul>
<li><details>
<summary>
(2020). Model detection and estimation for varying coefficient panel
data models with fixed effects. <em>CSDA</em>, <em>152</em>, 107054. (<a
href="https://doi.org/10.1016/j.csda.2020.107054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the model detection and estimation for varying coefficient panel data models with fixed effects. We first propose a data transformation approach to eliminate fixed effects. Then, using the basis function approximations and the group SCAD penalty, we develop a combined penalization procedure to select the significant covariates , detect the true structure of the model, i.e., identify the nonzero constant coefficients and the varying coefficients, and estimate the unknown regression coefficients simultaneously. Under some mild conditions, we show that the proposed procedure can identify the true model structure consistently, and the penalized estimators have the oracle properties. At last, we illustrate the finite sample performance of the proposed methods with some simulation studies and a real data application.},
  archive      = {J_CSDA},
  author       = {Sanying Feng and Wenqi He and Feng Li},
  doi          = {10.1016/j.csda.2020.107054},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107054},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Model detection and estimation for varying coefficient panel data models with fixed effects},
  volume       = {152},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semiparametric estimation for linear regression with
symmetric errors. <em>CSDA</em>, <em>152</em>, 107053. (<a
href="https://doi.org/10.1016/j.csda.2020.107053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To avoid the effect of distributional misspecification in the model-based regression, we propose an essentially nonparametric symmetric error distribution and construct a so-called doubly smoothed (DS) likelihood function by applying the same amount of smoothing to both the model and given data. To compute the DS maximum likelihood estimator based on the DS likelihood, we propose an approximated DS likelihood which has the form of a semiparametric mixture likelihood and apply some existing algorithms in the nonparametric mixture literature. The consistency of the DS maximum likelihood estimator is also established with any fixed smoothing parameter. Through numerical studies , we demonstrate that the proposed regression coefficient estimator has relatively good performance in terms of efficiency across a wide range of error distributions and robustness against outliers.},
  archive      = {J_CSDA},
  author       = {Chew-Seng Chee and Byungtae Seo},
  doi          = {10.1016/j.csda.2020.107053},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107053},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Semiparametric estimation for linear regression with symmetric errors},
  volume       = {152},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Varying-coefficient models for dynamic networks.
<em>CSDA</em>, <em>152</em>, 107052. (<a
href="https://doi.org/10.1016/j.csda.2020.107052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic networks are commonly used to model relational data that are observed over time. Statistical models for such data should capture both the temporal variation of the relational system as well as the structural dependencies within each network. As a consequence, effectively making inference on dynamic networks is a computationally challenging task, and many models are intractable even for moderately sized systems. In light of these challenges, a family of dynamic network models known as varying-coefficient exponential random graph models (VCERGMs) is proposed to characterize the evolution of network topology through smoothly varying parameters. The VCERGM provides an interpretable dynamic network model that enables the inference of temporal heterogeneity in dynamic networks. Estimation of the VCERGM is achieved via maximum pseudo-likelihood techniques, thereby providing a computationally tractable strategy for statistical inference of complex dynamic networks. Furthermore, a bootstrap hypothesis testing framework is presented for testing the temporal heterogeneity of an observed dynamic network sequence. Application to the U.S. Senate co-voting network and comprehensive simulation studies both reveal that the VCERGM provides relevant and interpretable patterns and has significant advantages over existing methods.},
  archive      = {J_CSDA},
  author       = {Jihui Lee and Gen Li and James D. Wilson},
  doi          = {10.1016/j.csda.2020.107052},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107052},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Varying-coefficient models for dynamic networks},
  volume       = {152},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inference for a generalised stochastic block model with
unknown number of blocks and non-conjugate edge models. <em>CSDA</em>,
<em>152</em>, 107051. (<a
href="https://doi.org/10.1016/j.csda.2020.107051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The stochastic block model (SBM) is a popular model for capturing community structure and interaction within a network. Network data with non-Boolean edge weights is becoming commonplace; however, existing analysis methods convert such data to a binary representation to apply the SBM, leading to a loss of information. A generalisation of the SBM is considered, which allows edge weights to be modelled in their recorded state. An effective reversible jump Markov chain Monte Carlo sampler is proposed for estimating the parameters and the number of blocks for this generalised SBM. The methodology permits non-conjugate distributions for edge weights, which enable more flexible modelling than current methods as illustrated on synthetic data, a network of brain activity and an email communication network.},
  archive      = {J_CSDA},
  author       = {Matthew Ludkin},
  doi          = {10.1016/j.csda.2020.107051},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107051},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Inference for a generalised stochastic block model with unknown number of blocks and non-conjugate edge models},
  volume       = {152},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two new matrix-variate distributions with application in
model-based clustering. <em>CSDA</em>, <em>152</em>, 107050. (<a
href="https://doi.org/10.1016/j.csda.2020.107050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two matrix-variate distributions, both elliptical heavy-tailed generalization of the matrix-variate normal distribution , are introduced. They belong to the normal scale mixture family, and are respectively obtained by choosing a convenient shifted exponential or uniform as mixing distribution. Moreover, they have a closed-form for the probability density function that is characterized by only one additional parameter, with respect to the nested matrix-variate normal, governing the tail-weight. Both distributions are then used for model-based clustering via finite mixture models . The resulting mixtures, being able to handle data with atypical observations in a better way than the matrix-variate normal mixture, can avoid the disruption of the true underlying group structure. Different EM-based algorithms are implemented for parameter estimation and tested in terms of computational times and parameter recovery. Furthermore, these mixture models are fitted to simulated and real datasets, and their fitting and clustering performances are analyzed and compared to those obtained by other well-established competitors.},
  archive      = {J_CSDA},
  author       = {Salvatore D. Tomarchio and Antonio Punzo and Luca Bagnato},
  doi          = {10.1016/j.csda.2020.107050},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107050},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Two new matrix-variate distributions with application in model-based clustering},
  volume       = {152},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Jackknife empirical likelihood inference for the pietra
ratio. <em>CSDA</em>, <em>152</em>, 107049. (<a
href="https://doi.org/10.1016/j.csda.2020.107049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Pietra ratio (Pietra index) is also known as the Robin Hood index or Schutz coefficient (Ricci–Schutz index). It is a measure of statistical heterogeneity in positive random variables . In this paper, we propose the jackknife empirical likelihood (JEL), the adjusted JEL, the extended JEL, and the balanced adjusted JEL method , for interval estimation of the Pietra ratio. We compare the performance of the proposed methods with the normal approximation (NA), bootstrap based methods and NA jackknife method . Simulation results indicate that under both symmetric and skewed distributions, the extended JEL method gives the best performance in terms of coverage probability . We illustrate the proposed methods by applying our methods to investigate the income data from the 2013 Current Population Survey conducted by the US Census Bureau.},
  archive      = {J_CSDA},
  author       = {Yichuan Zhao and Yueju Su and Hanfang Yang},
  doi          = {10.1016/j.csda.2020.107049},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107049},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Jackknife empirical likelihood inference for the pietra ratio},
  volume       = {152},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sparse recovery via nonconvex regularized m-estimators over
ℓq-balls. <em>CSDA</em>, <em>152</em>, 107047. (<a
href="https://doi.org/10.1016/j.csda.2020.107047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The recovery properties of nonconvex regularized M M -estimators are analysed, under the general sparsity assumption on the true parameter. In the statistical aspect, the recovery bound for any stationary point of the nonconvex regularized M M -estimator is established under some regularity conditions . In the computational aspect, the proximal gradient method is used to solve the nonconvex optimization problem and is proved to achieve a linear convergence rate, by virtue of a slight decomposition of the objective function. In particular, for commonly-used regularizers such as SCAD and MCP , a simpler decomposition is applicable thanks to the assumption on the regularizer, which helps to construct the estimator with better recovery performance. In the aspect of application, theoretical consequences are obtained on the corrupted error-in-variables linear regression model by verifying the required conditions. Finally, statistical and computational results as well as advantages of the assumptions are demonstrated by several numerical experiments. Simulation results show remarkable consistency with the theory under high-dimensional scaling.},
  archive      = {J_CSDA},
  author       = {Xin Li and Dongya Wu and Chong Li and Jinhua Wang and Jen-Chih Yao},
  doi          = {10.1016/j.csda.2020.107047},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107047},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Sparse recovery via nonconvex regularized M-estimators over ℓq-balls},
  volume       = {152},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On empirical estimation of mode based on weakly dependent
samples. <em>CSDA</em>, <em>152</em>, 107046. (<a
href="https://doi.org/10.1016/j.csda.2020.107046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a large sample of observations from an unknown univariate continuous distribution, it is often of interest to empirically estimate the global mode of the underlying density. Applications include samples obtained by Monte Carlo methods with independent observations, or Markov Chain Monte Carlo methods with weakly dependent samples from the underlying stationary density. In either case, often the generating density is not available in closed form and only empirical determination of the mode is possible. Assuming that the generating density has a unique global mode, a non-parametric estimate of the density is proposed based on a sequence of mixtures of Beta densities which allows for the estimation of the mode even when the mode is possibly located on the boundary of the support of the density. Furthermore, the estimated mode is shown to be strongly universally consistent under a set of mild regularity conditions . The proposed method is compared with other empirical estimates of the mode based on popular kernel density estimates. Numerical results based on extensive simulation studies show benefits of the proposed methods in terms of empirical bias, standard errors and computation time. An R package implementing the method is also made available online.},
  archive      = {J_CSDA},
  author       = {Bowen Liu and Sujit K. Ghosh},
  doi          = {10.1016/j.csda.2020.107046},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107046},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {On empirical estimation of mode based on weakly dependent samples},
  volume       = {152},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An advanced hidden markov model for hourly rainfall time
series. <em>CSDA</em>, <em>152</em>, 107045. (<a
href="https://doi.org/10.1016/j.csda.2020.107045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hidden Markov framework is adapted to construct a compelling model for simulation of sub-daily rainfall, capable of capturing important characteristics of sub-daily rainfall well, including: long dry periods or droughts; seasonal and temporal variation in occurrence and intensity; and propensity for extreme values. These adaptations include both clone states and temporally non-homogeneous state persistence probabilities. Set in the Bayesian framework, a rich quantification of parametric and predictive uncertainty is available, and thorough model checking is made possible through posterior predictive analyses. Results from the model are highly interpretable, allowing for meaningful examination of diurnal, seasonal and annual variation in sub-daily rainfall occurrence and intensity. To demonstrate the effectiveness of this approach, both in terms of model fit and interpretability, the model is applied to an 8-year long time series of hourly observations.},
  archive      = {J_CSDA},
  author       = {Oliver Stoner and Theo Economou},
  doi          = {10.1016/j.csda.2020.107045},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107045},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {An advanced hidden markov model for hourly rainfall time series},
  volume       = {152},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian nonparametric clustering as a community detection
problem. <em>CSDA</em>, <em>152</em>, 107044. (<a
href="https://doi.org/10.1016/j.csda.2020.107044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A wide class of Bayesian nonparametric priors leads to the representation of the distribution of the observable variables as a mixture density with an infinite number of components. Such a representation induces a clustering structure in the data. However, due to label switching, cluster identification is not straightforward a posteriori and some post-processing of the MCMC output is usually required. Alternatively, observations can be mapped on a weighted undirected graph , where each node represents a sample item and edge weights are given by the posterior pairwise similarities . It is shown how, after building a particular random walk on such a graph, it is possible to apply a community detection algorithm , known as map equation, leading to the minimisation of the expected description length of the partition. A relevant feature of this method is that it allows for the quantification of the posterior uncertainty of the classification.},
  archive      = {J_CSDA},
  author       = {Stefano F. Tonellato},
  doi          = {10.1016/j.csda.2020.107044},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107044},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayesian nonparametric clustering as a community detection problem},
  volume       = {152},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new correlation coefficient between categorical, ordinal
and interval variables with pearson characteristics. <em>CSDA</em>,
<em>152</em>, 107043. (<a
href="https://doi.org/10.1016/j.csda.2020.107043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A prescription is presented for a new and practical correlation coefficient , ϕ K ϕK , based on several refinements to Pearson’s hypothesis test of independence of two variables. The combined features of ϕ K ϕK form an advantage over existing coefficients. Primarily, it works consistently between categorical, ordinal and interval variables, in essence by treating each variable as categorical, and can therefore be used to calculate correlations between variables of mixed type. Second, it captures nonlinear dependency. The strength of ϕ K ϕK is similar to Pearson’s correlation coefficient, and is equivalent in case of a bivariate normal input distribution. These are useful properties when studying the correlations between variables with mixed types, where some are categorical. Two more innovations are presented: to the proper evaluation of statistical significance of correlations, and to the interpretation of variable relationships in a contingency table , in particular in case of sparse or low statistics samples and significant dependencies. Two practical applications are discussed. The presented algorithms are easy to use and available through a public Python library. 1},
  archive      = {J_CSDA},
  author       = {M. Baak and R. Koopman and H. Snoek and S. Klous},
  doi          = {10.1016/j.csda.2020.107043},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107043},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A new correlation coefficient between categorical, ordinal and interval variables with pearson characteristics},
  volume       = {152},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model-free variable selection for conditional mean in
regression. <em>CSDA</em>, <em>152</em>, 107042. (<a
href="https://doi.org/10.1016/j.csda.2020.107042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel test statistic is proposed to identify important predictors for the conditional mean function in regression. The stepwise regression algorithm based on the proposed test statistic guarantees variable selection consistency without specifying the functional form of the conditional mean. When the predictors are ultrahigh dimensional, a model-free screening procedure is introduced to precede the stepwise regression algorithm. The screening procedure has the sure screening property when the number of predictors grows at an exponential rate of the available sample size. The finite-sample performances of our proposals are demonstrated via numerical studies .},
  archive      = {J_CSDA},
  author       = {Yuexiao Dong and Zhou Yu and Liping Zhu},
  doi          = {10.1016/j.csda.2020.107042},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107042},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Model-free variable selection for conditional mean in regression},
  volume       = {152},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust estimation for semi-functional linear regression
models. <em>CSDA</em>, <em>152</em>, 107041. (<a
href="https://doi.org/10.1016/j.csda.2020.107041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-functional linear regression models postulate a linear relationship between a scalar response and a functional covariate , and also include a non-parametric component involving a univariate explanatory variable . It is of practical importance to obtain estimators for these models that are robust against high-leverage outliers, which are generally difficult to identify and may cause serious damage to least squares and Huber-type M M -estimators. For that reason, robust estimators for semi-functional linear regression models are constructed combining B B -splines to approximate both the functional regression parameter and the nonparametric component with robust regression estimators based on a bounded loss function and a preliminary residual scale estimator . Consistency and rates of convergence for the proposed estimators are derived under mild regularity conditions . The reported numerical experiments show the advantage of the proposed methodology over the classical least squares and Huber-type M M -estimators for finite samples. The analysis of real examples illustrates that the robust estimators provide better predictions for non-outlying points than the classical ones, and that when potential outliers are removed from the training and test sets both methods behave very similarly.},
  archive      = {J_CSDA},
  author       = {Graciela Boente and Matías Salibian-Barrera and Pablo Vena},
  doi          = {10.1016/j.csda.2020.107041},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107041},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Robust estimation for semi-functional linear regression models},
  volume       = {152},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian clustering of skewed and multimodal data using
geometric skewed normal distributions. <em>CSDA</em>, <em>152</em>,
107040. (<a href="https://doi.org/10.1016/j.csda.2020.107040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model-based clustering approaches generally assume that the observations to be clustered are generated from a mixture of distributions, each component of the mixture corresponding to a particular parametric distribution. Most commonly, the underlying distribution is assumed to be normal, which is inadequate for many situations, for example when skewness or multimodality is present within the components. The problem is intensified when the data dimension increases, leading to inaccurate groupings and incorrect inference. A new Bayesian model-based clustering approach is proposed, that can handle a variety of complexities in the data, based on a recently introduced family of geometric skew normal distributions. The performance of this methodology is illustrated through a number of simulation studies and applications to a number of datasets from genomics and medicine.},
  archive      = {J_CSDA},
  author       = {Edoardo Redivo and Hien D. Nguyen and Mayetri Gupta},
  doi          = {10.1016/j.csda.2020.107040},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107040},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayesian clustering of skewed and multimodal data using geometric skewed normal distributions},
  volume       = {152},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A reproducing kernel hilbert space approach to high
dimensional partially varying coefficient model. <em>CSDA</em>,
<em>152</em>, 107039. (<a
href="https://doi.org/10.1016/j.csda.2020.107039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partially varying coefficient model (PVCM) provides a useful class of tools for modeling complex data by incorporating a combination of constant and time-varying covariate effects. One natural question is that how to decide which covariates correspond to constant coefficients and which correspond to time-dependent coefficient functions . To handle this two-type structure selection problem on PVCM, those existing methods are either based on a finite truncation way of coefficient functions , or based on a two-phase procedure to estimate the constant and function parts separately. This paper attempts to provide a complete theoretical characterization for estimation and structure selection issues of PVCM, via proposing two new penalized methods for PVCM within a reproducing kernel Hilbert space (RKHS). The proposed strategy is partially motivated by the so-called “Non-Constant Theorem” of radial kernels, which ensures a unique and unified representation of each candidate component in the hypothesis space. Within a high-dimensional framework, minimax convergence rates for the prediction risk of the first method is established when each unknown time-dependent coefficient can be well approximated within a specified RKHS. On the other hand, under certain regularity conditions , it is shown that the second proposed estimator is able to identify the underlying structure correctly with high probability . Several simulated experiments are implemented to examine the finite sample performance of the proposed methods.},
  archive      = {J_CSDA},
  author       = {Shaogao Lv and Zengyan Fan and Heng Lian and Taiji Suzuki and Kenji Fukumizu},
  doi          = {10.1016/j.csda.2020.107039},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107039},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A reproducing kernel hilbert space approach to high dimensional partially varying coefficient model},
  volume       = {152},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An adapted linear discriminant analysis with variable
selection for the classification in high-dimension, and an application
to medical data. <em>CSDA</em>, <em>152</em>, 107031. (<a
href="https://doi.org/10.1016/j.csda.2020.107031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The classification of normally distributed data in a high-dimensional setting when variables are more numerous than observations is considered. Under the assumption that the inverse covariance matrices (the precision matrices) are the same over all groups, the method of the linear discriminant analysis (LDA) is adapted by including a sparse estimate of these matrices. Furthermore, a variable selection procedure is developed based on the graph associated to the estimated precision matrix. For that, a discriminant capacity is defined for each connected component of the graph, and variables of the most discriminant components are kept. The adapted LDA and the variable selection procedure are both evaluated on synthetic data, and applied to real data from PET brain images for the classification of patients with Alzheimer’s disease.},
  archive      = {J_CSDA},
  author       = {Khuyen T. Le and Caroline Chaux and Frédéric J.P. Richard and Eric Guedj},
  doi          = {10.1016/j.csda.2020.107031},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107031},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {An adapted linear discriminant analysis with variable selection for the classification in high-dimension, and an application to medical data},
  volume       = {152},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The delaunay triangulation learner and its ensembles.
<em>CSDA</em>, <em>152</em>, 107030. (<a
href="https://doi.org/10.1016/j.csda.2020.107030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Delaunay triangulation learner (DTL), which is a new piecewise linear learner, is proposed for both regression and classification tasks. Based on the data samples in a p p -dimensional feature space, the Delaunay triangulation algorithm provides a unique way of triangulating the space. The triangulation separates the convex hull of the samples into a series of disjoint p p -simplices, where the samples are the vertices of the p p -simplices. The DTL is constructed by fitting the responses through linear interpolation functions on each of the Delaunay simplices, and thus it approximates the whole functional by a piecewise linear function. In the ensemble learning approaches, bagging DTLs, random crystal and the boosting DTL are introduced, where the DTLs are constructed on the subspaces of the features, and the feature interactions can be captured by Delaunay triangle meshes . Extensive numerical studies are conducted to compare the proposed DTL and its ensembles with tree-based counterparts, K-nearest neighbors and the multivariate adaptive regression spline . The DTL methods show competitive performances in various settings, and particularly the DTL demonstrates its superiority over others for smooth functionals.},
  archive      = {J_CSDA},
  author       = {Yehong Liu and Guosheng Yin},
  doi          = {10.1016/j.csda.2020.107030},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107030},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {The delaunay triangulation learner and its ensembles},
  volume       = {152},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Large-scale estimation of random graph models with local
dependence. <em>CSDA</em>, <em>152</em>, 107029. (<a
href="https://doi.org/10.1016/j.csda.2020.107029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A class of random graph models is considered, combining features of exponential-family models and latent structure models, with the goal of retaining the strengths of both of them while reducing the weaknesses of each of them. An open problem is how to estimate such models from large networks. A novel approach to large-scale estimation is proposed, taking advantage of the local structure of such models for the purpose of local computing. The main idea is that random graphs with local dependence can be decomposed into subgraphs, which enables parallel computing on subgraphs and suggests a two-step estimation approach. The first step estimates the local structure underlying random graphs. The second step estimates parameters given the estimated local structure of random graphs. Both steps can be implemented in parallel, which enables large-scale estimation. The advantages of the two-step estimation approach are demonstrated by simulation studies with up to 10,000 nodes and an application to a large Amazon product recommendation network with more than 10,000 products.},
  archive      = {J_CSDA},
  author       = {Sergii Babkin and Jonathan R. Stewart and Xiaochen Long and Michael Schweinberger},
  doi          = {10.1016/j.csda.2020.107029},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107029},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Large-scale estimation of random graph models with local dependence},
  volume       = {152},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sparse functional principal component analysis in a new
regression framework. <em>CSDA</em>, <em>152</em>, 107016. (<a
href="https://doi.org/10.1016/j.csda.2020.107016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The functional principal component analysis is widely used to explore major sources of variation in a sample of random curves. These major sources of variation are represented by functional principal components (FPCs). The FPCs from the conventional FPCA method are often nonzero in the whole domain, and are hard to interpret in practice. The main focus is to estimate functional principal components (FPCs), which are only nonzero in subregions and are referred to as sparse FPCs. These sparse FPCs not only represent the major variation sources but also can be used to identify the subregions where those major variations exist. The current methods obtain sparse FPCs by adding a penalty term on the length of nonzero regions of FPCs in the conventional eigendecomposition framework. However, these methods become an NP-hard optimization problem . To overcome this issue, a novel regression framework is proposed to estimate FPCs and the corresponding optimization is not NP-hard. The FPCs estimated using the proposed sparse FPCA method is shown to be equivalent to the FPCs using the conventional FPCA method when the sparsity parameter is zero. Simulation studies illustrate that the proposed sparse FPCA method can provide more accurate estimates for FPCs than other available methods when those FPCs are only nonzero in subregions. The proposed method is demonstrated by exploring the major variations among the acceleration rate curves of 107 diesel trucks, where the nonzero regions of the estimated sparse FPCs are found well separated.},
  archive      = {J_CSDA},
  author       = {Yunlong Nie and Jiguo Cao},
  doi          = {10.1016/j.csda.2020.107016},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107016},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Sparse functional principal component analysis in a new regression framework},
  volume       = {152},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model averaging assisted sufficient dimension reduction.
<em>CSDA</em>, <em>152</em>, 106993. (<a
href="https://doi.org/10.1016/j.csda.2020.106993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sufficient dimension reduction that replaces original predictors with their low- dimensional linear combinations without loss of information is a critical tool in modern statistics and has gained considerable research momentum in the past decades since the two pioneers sliced inverse regression and principal Hessian directions. The classical sufficient dimension reduction methods do not handle sparse case well since the estimated linear reductions involve all of the original predictors. Sparse sufficient dimension reduction methods rely on sparsity assumption which may not be true in practice. Motivated by the least squares formulation of the classical sliced inverse regression and principal Hessian directions, several model averaging assisted sufficient dimension reduction methods are proposed. They are applicable to both dense and sparse cases even with weak signals since model averaging adaptively assigns weights to different candidate models. Based on the model averaging assisted sufficient dimension reduction methods, how to estimate the structural dimension is further studied. Theoretical justifications are given and empirical results show that the proposed methods compare favorably with the classical sufficient dimension reduction methods and popular sparse sufficient dimension reduction methods.},
  archive      = {J_CSDA},
  author       = {Fang Fang and Zhou Yu},
  doi          = {10.1016/j.csda.2020.106993},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106993},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Model averaging assisted sufficient dimension reduction},
  volume       = {152},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparametric bayesian inference for the spectral density
based on irregularly spaced data. <em>CSDA</em>, <em>151</em>, 107019.
(<a href="https://doi.org/10.1016/j.csda.2020.107019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various approaches for spectral analysis based on regularly spaced data have already been well-established, but the spectral inference based on irregularly spaced data are still essentially limited. Under the Bayesian framework, a detouring approach for spectral estimation is proposed for analyzing irregularly spaced data. The detouring process is accomplished by three steps: (1) normalizing the data in some sense on frequency domain by a time-scale change, (2) estimating the spectral density of the time-scale changed process, and (3) solving the estimated spectrum by the relation of spectral densities between the model and its time-scale-changed version. The proposed approach uses a Hamiltonian Monte Carlo—within Gibbs technique to fit smoothing splines to the periodogram . Our technique produces an automatically smoothed spectral estimate. The time-scale-change not only allows basis functions in the smoothing splines to be independent of sampling design, but also makes the proposed estimation need not to adjust tuning parameters according to different irregularly spaced data.},
  archive      = {J_CSDA},
  author       = {Shibin Zhang},
  doi          = {10.1016/j.csda.2020.107019},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107019},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Nonparametric bayesian inference for the spectral density based on irregularly spaced data},
  volume       = {151},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Marginally parameterized spatio-temporal models and stepwise
maximum likelihood estimation. <em>CSDA</em>, <em>151</em>, 107018. (<a
href="https://doi.org/10.1016/j.csda.2020.107018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to learn the complex features of large spatio-temporal data, models with a large number of parameters are often required. However, inference is often infeasible due to the computational and memory costs of maximum likelihood estimation (MLE). The class of marginally parameterized (MP) models is introduced, where estimation can be performed efficiently with a sequence of marginal likelihood functions with stepwise maximum likelihood estimation (SMLE). The conditions under which the stepwise estimators are consistent are provided, and it is shown that this class of models includes the diagonal vector autoregressive moving average model . It is demonstrated that the parameters of this model can be obtained at least three orders of magnitude faster with SMLE compared to MLE, with only a small loss in statistical efficiency. A MP model is applied to a spatio-temporal global climate data set consisting of over five million data points, and it is demonstrated how estimation can be achieved in less than one hour on a laptop with a dual core at 2.9 Ghz.},
  archive      = {J_CSDA},
  author       = {Matthew Edwards and Stefano Castruccio and Dorit Hammerling},
  doi          = {10.1016/j.csda.2020.107018},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107018},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Marginally parameterized spatio-temporal models and stepwise maximum likelihood estimation},
  volume       = {151},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Regression models using shapes of functions as predictors.
<em>CSDA</em>, <em>151</em>, 107017. (<a
href="https://doi.org/10.1016/j.csda.2020.107017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional variables are often used as predictors in regression problems . A commonly used parametric approach, called scalar-on-function regression , uses the L 2 L2 inner product to map functional predictors into scalar responses. This method can perform poorly when predictor functions contain undesired phase variability, causing phases to have disproportionately large influence on the response variable. One past solution has been to perform phase–amplitude separation (as a pre-processing step) and then use only the amplitudes in the regression model. Here we propose a more integrated approach, termed elastic functional regression model (EFRM), where phase-separation is performed inside the regression model, rather than as a pre-processing step. This approach generalizes the notion of phase in functional data, and is based on the norm-preserving time warping of predictors. Due to its invariance properties , this representation provides robustness to predictor phase variability and results in improved predictions of the response variable over traditional models. We demonstrate this framework using a number of datasets involving gait signals, NMR data, and stock market prices.},
  archive      = {J_CSDA},
  author       = {Kyungmin Ahn and J. Derek Tucker and Wei Wu and Anuj Srivastava},
  doi          = {10.1016/j.csda.2020.107017},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107017},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Regression models using shapes of functions as predictors},
  volume       = {151},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A rank-based approach to estimating monotone individualized
two treatment regimes. <em>CSDA</em>, <em>151</em>, 107015. (<a
href="https://doi.org/10.1016/j.csda.2020.107015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing effective individualized treatment rules (ITRs) for diseases is an important goal of clinical research. Much effort has been devoted to estimating individualized treatment effects in the recent literature. However, there have not been systematic studies on the robust inference for individualized treatment effects when there exist potential outliers. We propose a monotone ITR in the framework of a semiparametric generalized regression with two treatments and estimate the treatment effects via a smoothed maximum rank correlation procedure. We provide sufficient conditions under which the proposed estimator has an asymptotically normal distribution whose variance can be consistently estimated based on a resampling procedure. We evaluate the finite-sample properties of our proposed approach via simulation studies. We also illustrate the proposed method by applying it to a data set from an AIDS clinical trials study.},
  archive      = {J_CSDA},
  author       = {Haixiang Zhang and Jian Huang and Liuquan Sun},
  doi          = {10.1016/j.csda.2020.107015},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107015},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A rank-based approach to estimating monotone individualized two treatment regimes},
  volume       = {151},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive semiparametric estimation for single index models
with jumps. <em>CSDA</em>, <em>151</em>, 107013. (<a
href="https://doi.org/10.1016/j.csda.2020.107013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The single index model is one of the most popular semiparametric models in applied quantitative sciences. This paper studies a single index model with unknown jumps (SIMJ) that occur in the link function. An adaptive semiparametric estimation procedure is proposed for estimating the index coefficient and link function. The asymptotic normality of the resulting estimators for both the parametric and nonparametric parts can be established under some mild conditions and without specifying the error distribution. We show that the resulting estimators are robust and efficient for different error distributions. In particular, a modified EM algorithm is developed to implement the adaptive semiparametric estimation in practice. Numerical simulations and real data analysis are conducted to illustrate the finite sample performance of the proposed approach.},
  archive      = {J_CSDA},
  author       = {Zhong-Cheng Han and Jin-Guan Lin and Yan-Yong Zhao},
  doi          = {10.1016/j.csda.2020.107013},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107013},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Adaptive semiparametric estimation for single index models with jumps},
  volume       = {151},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variational inference for high dimensional structured factor
copulas. <em>CSDA</em>, <em>151</em>, 107012. (<a
href="https://doi.org/10.1016/j.csda.2020.107012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Factor copula models have been recently proposed for describing the joint distribution of a large number of variables in terms of a few common latent factors . A Bayesian procedure is employed in order to make fast inferences for multi-factor and structured factor copulas. To deal with the high dimensional structure, a Variational Inference (VI) algorithm is applied to estimate different specifications of factor copula models. Compared to the Markov Chain Monte Carlo (MCMC) approach, the variational approximation is much faster and could handle a sizeable problem in limited time. Another issue of factor copula models is that the bivariate copula functions connecting the variables are unknown in high dimensions . An automatic procedure is derived to recover the hidden dependence structure . By taking advantage of the posterior modes of the latent variables, the bivariate copula functions are selected by minimizing the Bayesian Information Criterion (BIC). Simulation studies in different contexts show that the procedure of bivariate copula selection could be very accurate in comparison to the true generated copula model. The proposed procedure is illustrated with two high dimensional real data sets .},
  archive      = {J_CSDA},
  author       = {Hoang Nguyen and M. Concepción Ausín and Pedro Galeano},
  doi          = {10.1016/j.csda.2020.107012},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107012},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Variational inference for high dimensional structured factor copulas},
  volume       = {151},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Faster monte carlo estimation of joint models for
time-to-event and multivariate longitudinal data. <em>CSDA</em>,
<em>151</em>, 107010. (<a
href="https://doi.org/10.1016/j.csda.2020.107010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quasi-Monte Carlo (QMC) methods using quasi-random sequences, as opposed to pseudo-random samples, are proposed for use in the joint modelling of time-to-event and multivariate longitudinal data . The QMC integration framework extends the Monte Carlo Expectation Maximisation approaches that are commonly adopted, namely using ordinary and antithetic variates. The motivation of QMC integration is to increase the convergence speed by using nodes that are scattered more uniformly. Through simulation, estimates and computational times are compared and this is followed with an application to a clinical dataset. There is a distinct speed advantage in using QMC methods for small sample sizes and QMC is comparable to the antithetic MC method for moderate sample sizes . The new method is available in an updated version of the R package joineRML .},
  archive      = {J_CSDA},
  author       = {Pete Philipson and Graeme L. Hickey and Michael J. Crowther and Ruwanthi Kolamunnage-Dona},
  doi          = {10.1016/j.csda.2020.107010},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107010},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Faster monte carlo estimation of joint models for time-to-event and multivariate longitudinal data},
  volume       = {151},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Competing risk modeling and testing for x-chromosome genetic
association. <em>CSDA</em>, <em>151</em>, 107007. (<a
href="https://doi.org/10.1016/j.csda.2020.107007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complexity of X-chromosome inactivation arouses the X-linked genetic association being overlooked in most of the genetic studies, especially for genetic association analysis on time to event outcomes. To fill this gap, we propose novel methods to analyze the X-linked genetic association for competing risk failure time data based on a subdistribution hazard function. Specifically, we consider two mechanisms for a single genetic variant on X-chromosome: (1) all the subjects in a population undergo the same inactivation process; (2) the subjects randomly undergo different inactivation processes. According to the assumptions, one of the proposed methods can be used to infer the unknown biological process under scenario (1), while another method can be used to estimate the proportion of a certain biological process in the population under scenario (2). Both of the two methods can infer the direction of skewness for skewed X-chromosome inactivation and derive asymptotically unbiased estimates of the model parameters. The asymptotic distributions for the parameter estimates and constructed score tests with nuisance parameters only presented under the alternative hypothesis are illustrated under both assumptions. Finite sample performance of these novel methods is examined via extensive simulation studies. An application is illustrated with implementation on a cancer genetic study with competing risk outcomes.},
  archive      = {J_CSDA},
  author       = {Meiling Hao and Xingqiu Zhao and Wei Xu},
  doi          = {10.1016/j.csda.2020.107007},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107007},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Competing risk modeling and testing for X-chromosome genetic association},
  volume       = {151},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint analysis of semicontinuous data with latent variables.
<em>CSDA</em>, <em>151</em>, 107005. (<a
href="https://doi.org/10.1016/j.csda.2020.107005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A two-part latent variable model is proposed to analyze semicontinuous data in the presence of latent variables. The proposed model comprises two major components. The first component is a structural equation model (SEM), which characterizes latent variables using corresponding multiple attributes and examines the interrelationships among them. The second component is a two-part model to assess a semicontinuous response of interest. The semicontinuous variable is characterized by a mixture of zero values and continuously distributed positive values. The two-part model manages this semicontinuous variable by splitting it into two random variables ; one is a binary indicator to determine whether the response is zero, another is a continuous variable to determine the actual level of the positive response. A full Bayesian approach coupled with spike-and-slab lasso prior is developed for simultaneous variable selection and parameter estimation. The proposed methodology is demonstrated by a simulation study and applied to the analysis of the Chinese General Social Survey dataset. New insights into the interrelationships among non-cognitive ability, education level, and annual income are obtained.},
  archive      = {J_CSDA},
  author       = {Xiaoqing Wang and Xiangnan Feng and Xinyuan Song},
  doi          = {10.1016/j.csda.2020.107005},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107005},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Joint analysis of semicontinuous data with latent variables},
  volume       = {151},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-dimensional two-sample mean vectors test and support
recovery with factor adjustment. <em>CSDA</em>, <em>151</em>, 107004.
(<a href="https://doi.org/10.1016/j.csda.2020.107004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing the equality of two mean vectors is a classical problem in multivariate analysis . In this article, we consider the test in the high-dimensional setting. Existing tests often assume that the covariance matrix (or its inverse) of the underlying variables is sparse, which is rarely true in social science due to the existence of latent common factors. In the article, we introduce a maximum-type test statistic based on the factor-adjusted data. The factor-adjustment step increases the signal-to-noise ratio and thus results in more powerful test. We obtain the limiting null distribution of the maximum-type test statistic, which is the extreme value distribution of type I. To overcome the well-known slow convergence rate of the test statistic’s distribution to the limiting extreme value distribution , we also propose a multiplier bootstrap method to improve the finite-sample performance. In addition, a multiple testing procedure with false discovery rate (FDR) control is proposed for identifying specific locations that differ significantly between the two groups. Thorough numerical studies are conducted to show the superiority of the test over other state-of-the-art tests. The performance of the test is also assessed through a real stock market dataset.},
  archive      = {J_CSDA},
  author       = {Yong He and Mingjuan Zhang and Xinsheng Zhang and Wang Zhou},
  doi          = {10.1016/j.csda.2020.107004},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107004},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {High-dimensional two-sample mean vectors test and support recovery with factor adjustment},
  volume       = {151},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Huber-type principal expectile component analysis.
<em>CSDA</em>, <em>151</em>, 106992. (<a
href="https://doi.org/10.1016/j.csda.2020.106992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In principal component analysis (PCA), principal components are identified by maximizing the component score variance around the mean. However, a practitioner might be interested in capturing the variation in the tail rather than the center of a distribution to, for example, identify the major pollutants from air pollution data. To address this problem, we introduce a new method called Huber-type principal expectile component (HPEC) analysis that uses an asymmetric Huber norm to provide a kind of robust-tail PCA. The statistical properties of HPECs are derived, and a derivative-free optimization approach called particle swarm optimization (PSO) is used to identify HPECs numerically. As a demonstration, HPEC analysis is applied to real and simulated data with encouraging results.},
  archive      = {J_CSDA},
  author       = {Liang-Ching Lin and Ray-Bing Chen and Mong-Na Lo Huang and Meihui Guo},
  doi          = {10.1016/j.csda.2020.106992},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106992},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Huber-type principal expectile component analysis},
  volume       = {151},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comparison of symmetry tests against some skew-symmetric
alternatives in i.i.d. And non-i.i.d. setting. <em>CSDA</em>,
<em>151</em>, 106991. (<a
href="https://doi.org/10.1016/j.csda.2020.106991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A wide set of recent and classical symmetry tests is compared in terms of empirical power against some flexible skew-symmetric alternatives. The comparison is done for i.i.d. data, as well as for linear and GARCH time series models . In addition, the tests are analyzed in terms of computational efficiency. The role of distribution-free tests that avoid time-consuming resampling techniques for determining empirical powers and p-values is pointed out. An asymptotic equivalence of test statistics in the i.i.d. and GARCH models is discussed for a class of distribution-free tests.},
  archive      = {J_CSDA},
  author       = {Blagoje Ivanović and Bojana Milošević and Marko Obradović},
  doi          = {10.1016/j.csda.2020.106991},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106991},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Comparison of symmetry tests against some skew-symmetric alternatives in i.i.d. and non-i.i.d. setting},
  volume       = {151},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Completely monotone distributions: Mixing, approximation and
estimation of number of species. <em>CSDA</em>, <em>150</em>, 107014.
(<a href="https://doi.org/10.1016/j.csda.2020.107014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of species richness estimation using complete monotonicity of the distribution of species abundances is considered. Complete monotonicity is the most natural surrogate for k k -monotonicity when k k is large. The latter model has been considered in the same estimation problem adopting two different approaches which both necessitate selecting the unknown degree of monotonicity k k via some chosen criterion. It is shown that such selection procedures can be avoided by appropriately approximating the true completely monotone distribution by a k n kn -monotone one such that k n kn grows logarithmically as a function of the sample size n n . Furthermore, the proposed estimator of the true total number of species is proved to be asymptotically normal. An extended simulation study indicates that it is quite competitive when compared to other available estimators, and this remains true even when complete monotonicity is not satisfied. It is further illustrated how the method can be applied in practice by using four real datasets.},
  archive      = {J_CSDA},
  author       = {Fadoua Balabdaoui and Yulia Kulagina},
  doi          = {10.1016/j.csda.2020.107014},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107014},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Completely monotone distributions: Mixing, approximation and estimation of number of species},
  volume       = {150},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Superiority of bayes estimators over the MLE in high
dimensional multinomial models and its implication for nonparametric
bayes theory. <em>CSDA</em>, <em>150</em>, 107011. (<a
href="https://doi.org/10.1016/j.csda.2020.107011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of Bayes estimators is examined, in comparison with the MLE , in multinomial models with a relatively large number of cells. The prior for the Bayes estimator is taken to be the conjugate Dirichlet , i.e., the multivariate Beta, with exchangeable distributions over the coordinates, including the non-informative uniform distribution. The choice of the multinomial is motivated by its many applications in business and industry, but also by its use in providing a simple nonparametric estimator of an unknown distribution. It is striking that the Bayes procedure outperforms the asymptotically efficient MLE over most of the parameter spaces for even moderately large dimensional parameter spaces and rather large sample sizes.},
  archive      = {J_CSDA},
  author       = {Rabi Bhattacharya and Rachel Oliver},
  doi          = {10.1016/j.csda.2020.107011},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107011},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Superiority of bayes estimators over the MLE in high dimensional multinomial models and its implication for nonparametric bayes theory},
  volume       = {150},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Functional linear regression model with randomly censored
data: Predicting conversion time to alzheimer ’s disease. <em>CSDA</em>,
<em>150</em>, 107009. (<a
href="https://doi.org/10.1016/j.csda.2020.107009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Predicting the onset time of Alzheimer’s disease is of great importance in preventive medicine. Structural changes in brain regions have been actively investigated in the association study of Alzheimer’s disease diagnosis and prognosis. In this study, we propose a functional linear regression model to predict the conversion time to Alzheimer’s disease among mild cognitive impairment patients. Midsagittal thickness change in corpus callosum is measured from magnetic resonance imaging scan and put into the model as a functional covariate . A synthetic response approach is taken to deal with the censored data . The simulation studies demonstrate that the proposed model successfully predicts the unobserved true survival time but indicate that high censoring rate may cause poor prediction in time. Through ADNI data application, we find that the atrophy in the rear area of corpus callosum is a possible neuroimaging marker on Alzheimer’s disease prognosis.},
  archive      = {J_CSDA},
  author       = {Seong J. Yang and Hyejin Shin and Sang Han Lee and Seokho Lee and Alzheimer’s Disease Neuroimaging Initiative},
  doi          = {10.1016/j.csda.2020.107009},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107009},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Functional linear regression model with randomly censored data: Predicting conversion time to alzheimer ’s disease},
  volume       = {150},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inferring time non-homogeneous ornstein uhlenbeck type
stochastic process. <em>CSDA</em>, <em>150</em>, 107008. (<a
href="https://doi.org/10.1016/j.csda.2020.107008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A generalization of the classical Ornstein Uhlenbeck diffusion process including some deterministic time dependent functions in the infinitesimal moments is considered. The inference based on discrete sampling in time is provided by means of an iterative procedure that, in each step, combines the classical maximum likelihood estimation and a generalized method of moments. The validity of the suggested procedure is evaluated via a simulation study by considering several infinitesimal moments for the Ornstein Uhlenbeck type process and taking different sample size. Finally, an application to P M 10 PM10 daily concentration in Turin metropolitan area in Italy is discussed.},
  archive      = {J_CSDA},
  author       = {G. Albano and V. Giorno},
  doi          = {10.1016/j.csda.2020.107008},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107008},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Inferring time non-homogeneous ornstein uhlenbeck type stochastic process},
  volume       = {150},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semiparametric modeling of time-varying activation and
connectivity in task-based fMRI data. <em>CSDA</em>, <em>150</em>,
107006. (<a href="https://doi.org/10.1016/j.csda.2020.107006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In functional magnetic resonance imaging (fMRI), there is a rise in evidence that time-varying functional connectivity , or dynamic functional connectivity (dFC), which measures changes in the synchronization of brain activity , provides additional information on brain networks not captured by time-invariant (i.e., static) functional connectivity. While there have been many developments for statistical models of dFC in resting-state fMRI, there remains a gap in the literature on how to simultaneously model both dFC and time-varying activation when the study participants are undergoing experimental tasks designed to probe at a cognitive process of interest. A method is proposed to estimate dFC between two regions of interest (ROIs) in task-based fMRI where the activation effects are also allowed to vary over time. The proposed method, called TVAAC (time-varying activation and connectivity), uses penalized splines to model both time-varying activation effects and time-varying functional connectivity and uses the bootstrap for statistical inference . Simulation studies show that TVAAC can estimate both static and time-varying activation and functional connectivity, while ignoring time-varying activation effects would lead to poor estimation of dFC. An empirical illustration is provided by applying TVAAC to analyze two subjects from an event-related fMRI learning experiment.},
  archive      = {J_CSDA},
  author       = {Jun Young Park and Joerg Polzehl and Snigdhansu Chatterjee and André Brechmann and Mark Fiecas},
  doi          = {10.1016/j.csda.2020.107006},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {107006},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Semiparametric modeling of time-varying activation and connectivity in task-based fMRI data},
  volume       = {150},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Block wild bootstrap-based CUSUM tests robust to high
persistence and misspecification. <em>CSDA</em>, <em>150</em>, 106996.
(<a href="https://doi.org/10.1016/j.csda.2020.106996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well-known that the conventional CUSUM tests are over-sized in the presence of high persistence and misspecification. In this article, we propose a block wild bootstrap-based CUSUM test (CUSUM-BWB) for detecting changes in mean and variance shifts under possible high persistence and misspecification. We establish the asymptotic properties of the proposed test and our simulation study shows that CUSUM-BWB tests achieve the correct sizes and comparable powers in finite samples. Our method is also applied to the realized volatility of the KOSPI stock index.},
  archive      = {J_CSDA},
  author       = {Taewook Lee and Changryong Baek},
  doi          = {10.1016/j.csda.2020.106996},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106996},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Block wild bootstrap-based CUSUM tests robust to high persistence and misspecification},
  volume       = {150},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generalized kernel-based inverse regression methods for
sufficient dimension reduction. <em>CSDA</em>, <em>150</em>, 106995. (<a
href="https://doi.org/10.1016/j.csda.2020.106995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The linearity condition and the constant conditional variance assumption popularly used in sufficient dimension reduction are respectively close to elliptical symmetry and normality. However, it is always the concern about their restrictiveness. In this article, we give systematic studies to provide insight into the reasons why the popularly used sliced inverse regression and sliced average variance estimation need these conditions. Then we propose a new framework to relax these conditions and suggest generalized kernel-based inverse regression methods to handle a class of mixture multivariate unified skew-elliptical distributions.},
  archive      = {J_CSDA},
  author       = {Chuanlong Xie and Lixing Zhu},
  doi          = {10.1016/j.csda.2020.106995},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106995},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Generalized kernel-based inverse regression methods for sufficient dimension reduction},
  volume       = {150},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Split sample empirical likelihood. <em>CSDA</em>,
<em>150</em>, 106994. (<a
href="https://doi.org/10.1016/j.csda.2020.106994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Empirical likelihood offers a nonparametric approach to estimation and inference, which replaces the probability density-based likelihood function with a function defined by estimating equations. While this eliminates the need for a parametric specification, the restriction of numerical optimization greatly decreases the applicability of empirical likelihood for large data problems. A solution to this problem is the split sample empirical likelihood; this variant utilizes a divide and conquer approach, allowing for parallel computation of the empirical likelihood function. The results show the asymptotic distribution of the estimators and test statistics derived from the split sample empirical likelihood are the same seen in standard empirical likelihood yet have significantly decreased computational times.},
  archive      = {J_CSDA},
  author       = {Adam Jaeger and Nicole A. Lazar},
  doi          = {10.1016/j.csda.2020.106994},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106994},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Split sample empirical likelihood},
  volume       = {150},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation of the mann–whitney effect in the two-sample
problem under dependent censoring. <em>CSDA</em>, <em>150</em>, 106990.
(<a href="https://doi.org/10.1016/j.csda.2020.106990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Mann–Whitney effect is a nonparametric measure for comparing the distribution between two groups, which can be estimated by right-censored data. However, the traditional estimator of the Mann–Whitney effect based on the Kaplan–Meier estimators can be inconsistent when the independent censoring assumption fails to hold. Investigation is made on the asymptotic bias of the traditional estimator of the Mann–Whitney effect when the independent censoring assumption is violated due to dependence between survival time and censoring time. A new estimator of the Mann–Whitney effect is proposed by applying the copula-graphic estimator to adjust for the effect of dependent censoring. The proposed estimator and test are consistent when the assumed copulas for the two groups are correct. Some consistency properties under misspecified copulas are also given. Simulations are conducted to verify the proposed method under possible misspecification on copulas. The method is illustrated by a real data set . We provide an R function “MW.test” to implement the proposed estimator and test.},
  archive      = {J_CSDA},
  author       = {Takeshi Emura and Jiun-Huang Hsu},
  doi          = {10.1016/j.csda.2020.106990},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106990},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Estimation of the Mann–Whitney effect in the two-sample problem under dependent censoring},
  volume       = {150},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generalized co-clustering analysis via regularized
alternating least squares. <em>CSDA</em>, <em>150</em>, 106989. (<a
href="https://doi.org/10.1016/j.csda.2020.106989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biclustering is an important exploratory analysis tool that simultaneously clusters rows (e.g., samples) and columns (e.g., variables) of a data matrix. Checkerboard-like biclusters reveal intrinsic associations between rows and columns. However, most existing methods rely on Gaussian assumptions and only apply to matrix data. In practice, non-Gaussian and/or multi-way tensor data are frequently encountered. A new CO -clustering method via R egularized A lternating L east S quares (CORALS) is proposed, which generalizes biclustering to non-Gaussian data and multi-way tensor arrays. Non-Gaussian data are modeled with single-parameter exponential family distributions and co-clusters are identified in the natural parameter space via sparse CANDECOMP/PARAFAC tensor decomposition . A regularized alternating (iteratively reweighted) least squares algorithm is devised for model fitting and a deflation procedure is exploited to automatically determine the number of co-clusters. Comprehensive simulation studies and three real data examples demonstrate the efficacy of the proposed method. The data and code are publicly available at https://github.com/reagan0323/CORALS .},
  archive      = {J_CSDA},
  author       = {Gen Li},
  doi          = {10.1016/j.csda.2020.106989},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106989},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Generalized co-clustering analysis via regularized alternating least squares},
  volume       = {150},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Augmented quasi-sudoku designs in field trials.
<em>CSDA</em>, <em>150</em>, 106988. (<a
href="https://doi.org/10.1016/j.csda.2020.106988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Augmented designs play an important role in early-generation plant breeding when many varieties are tested and sufficient seeds are unavailable to permit each variety to be replicated more than once. The key idea is to include some replicated varieties to adjust for environmental heterogeneity and provide a good estimate of error. The simplest form of augmented design is constructed using randomized complete blocks for a few replicated varieties and augmenting each block with unreplicated varieties. This idea is readily extended to incomplete-block designs and row-column designs. A challenge is to ensure good coverage of the experimental field with replicated varieties. A problem with augmented row-column designs is that replicated variety plots may be clustered in parts of the field. One way to improve the evenness of replicated variety plot distribution is to use regions as a third blocking factor, formed by intersection of row groups (i.e. groups of adjacent rows) and column groups (i.e. groups of adjacent columns). First, a strategy to identify the numbers of regions, row groups, and column groups is proposed. Second, a general approach to search for augmented designs with three blocking factors for any numbers of unreplicated varieties and replicated varieties is presented. Finally, the algorithm is illustrated for common scenarios in plant breeding.},
  archive      = {J_CSDA},
  author       = {Nha Vo-Thanh and Hans-Peter Piepho},
  doi          = {10.1016/j.csda.2020.106988},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106988},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Augmented quasi-sudoku designs in field trials},
  volume       = {150},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comparison of nonlinear curves and surfaces. <em>CSDA</em>,
<em>150</em>, 106987. (<a
href="https://doi.org/10.1016/j.csda.2020.106987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimation of nonlinear curves and surfaces has long been the focus of semiparametric and nonparametric regression analysis . What has been less studied is the comparison of nonlinear functions . In lower-dimensional situations, inference typically involves comparisons of curves and surfaces. The existing comparative procedures are subject to various limitations, and few computational tools have been made available for off-the-shelf use. To address these limitations, two modified testing procedures for nonlinear curve and surface comparisons are proposed. The proposed computational tools are implemented in an R package, with a syntax similar to that of the commonly used model fitting packages. An R Shiny application is provided with an interactive interface for analysts who do not use R . The new tests are consistent against fixed alternative hypotheses. Theoretical details are presented in an appendix. Operating characteristics of the proposed tests are assessed against the existing methods. Applications of the methods are illustrated through real data examples.},
  archive      = {J_CSDA},
  author       = {Shi Zhao and Giorgos Bakoyannis and Spencer Lourens and Wanzhu Tu},
  doi          = {10.1016/j.csda.2020.106987},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106987},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Comparison of nonlinear curves and surfaces},
  volume       = {150},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A nested copula duration model for competing risks with
multiple spells. <em>CSDA</em>, <em>150</em>, 106986. (<a
href="https://doi.org/10.1016/j.csda.2020.106986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A copula graphic estimator for the competing risks duration model with multiple spells is presented. By adopting a nested copula structure the dependencies between risks and spells are modelled separately. This breaks up an implicit restriction of popular duration models such as multivariate mixed proportional hazards. It is shown that the dependence structure between spells is identifiable and can be estimated, in contrast to the dependence structure between competing risks. Thus, by allowing these two components to differ, the model is not identifiable. This is an important finding related to the general identifiability of competing risks models. Various features of the model are investigated by simulations and its practicality is illustrated by an application to unemployment duration data.},
  archive      = {J_CSDA},
  author       = {Simon M.S. Lo and Enno Mammen and Ralf A. Wilke},
  doi          = {10.1016/j.csda.2020.106986},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106986},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A nested copula duration model for competing risks with multiple spells},
  volume       = {150},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Partially functional linear regression in reproducing kernel
hilbert spaces. <em>CSDA</em>, <em>150</em>, 106978. (<a
href="https://doi.org/10.1016/j.csda.2020.106978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the partially functional linear regression model in which there are both functional predictors and traditional multivariate predictors. The existing approach is based on approximation using functional principal component analysis which has some limitations. We propose an alternative framework based on reproducing kernel Hilbert spaces (RKHS) which has not been investigated in the literature for the partially functional case. Asymptotic normality of the non-functional part is also shown. Even when reduced to the purely functional linear regression, our results extend the existing results in two aspects: rates are established using both prediction risk and RKHS norm, and faster rates are possible if greater smoothness is assumed. Some simulations are used to demonstrate the performance of the proposed estimator.},
  archive      = {J_CSDA},
  author       = {Xia Cui and Hongmei Lin and Heng Lian},
  doi          = {10.1016/j.csda.2020.106978},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106978},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Partially functional linear regression in reproducing kernel hilbert spaces},
  volume       = {150},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Regularization of bayesian quasi-likelihoods constructed
from complex estimating functions. <em>CSDA</em>, <em>150</em>, 106977.
(<a href="https://doi.org/10.1016/j.csda.2020.106977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian quasi-likelihoods constructed from estimating functions extend the scope of Bayesian inference to a wide range of semi-parametric problems. Nonetheless, when the estimating functions possess complex structure, like containing highly irregular weighting matrices, the quasi-likelihoods constructed from those estimating functions may deform with multiple modes, and thus lose their nice statistical properties. This makes incorporating the information from complex estimating functions, such as composite score functions , into Bayesian inference infeasible. This paper makes two contributions. First, we demonstrate and explain theoretically why the quasi-likelihoods constructed from complex estimating functions deform. Second, we introduce a method of quasi-likelihood regularization which effectively handles the deformation and restores the nice statistical properties of the quasi-likelihoods. The regularization can be easily implemented and asymptotically preserves all information from the original estimating functions, ensuring good estimation accuracy. Numerical studies are conducted to verify the effectiveness of the quasi-likelihood regularization on GARCH(1,1) model and Gaussian random field examples. Direct incorporation of the information from composite likelihoods into the regularized quasi-likelihoods is a highlighted application demonstrated in the numerical studies. We further apply the regularized quasi-likelihood to the spatio-temporal modeling of the ground-level ozone concentration in Hong Kong, and illustrate how our regularized quasi-likelihoods facilitate the simultaneous inference from multiple sources of complex semi-parametric information.},
  archive      = {J_CSDA},
  author       = {Ray S.W. Chung and Mike K.P. So and Amanda M.Y. Chu and Thomas W.C. Chan},
  doi          = {10.1016/j.csda.2020.106977},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106977},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Regularization of bayesian quasi-likelihoods constructed from complex estimating functions},
  volume       = {150},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Testing proportionality of two high-dimensional covariance
matrices. <em>CSDA</em>, <em>150</em>, 106962. (<a
href="https://doi.org/10.1016/j.csda.2020.106962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes three tests for proportionality hypotheses regrading high-dimensional covariance matrices . Compared with currently available tests in the literature that fail in situations involving a “large p p small n n ” or require knowledge of the underlying normal distributions , these tests are nonparametric, and do not require specifying any known distribution to derive asymptotic distributions under both the null hypothesis as well as an alternative hypothesis. The theoretical justification for the proposed tests is provided to ensure their validity, especially when the number of dimensions p p is larger than the sample size n n . Numerical studies show that the proposed tests are adaptively powerful against dense as well as sparse alternatives for a wide range of dimensions and sample sizes. The tests were used to analyze a gene expression dataset to verify their effectiveness.},
  archive      = {J_CSDA},
  author       = {Guanghui Cheng and Baisen Liu and Guoliang Tian and Shurong Zheng},
  doi          = {10.1016/j.csda.2020.106962},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106962},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Testing proportionality of two high-dimensional covariance matrices},
  volume       = {150},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online updating method to correct for measurement error in
big data streams. <em>CSDA</em>, <em>149</em>, 106976. (<a
href="https://doi.org/10.1016/j.csda.2020.106976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When huge amounts of data arrive in streams, online updating is an important method to alleviate both computational and data storage issues. The scope of previous research for online updating is extended in the context of the classical linear measurement error model . In the case where some covariates are unknowingly measured with error at the beginning of the stream, but then are measured without error after a particular point along the data stream, the updated estimators ignoring the measurement error are biased for the true parameters. Once the covariates measured without error are first observed, a method to correct the bias of the estimators, as well as to correct the biases in their variance estimator , is proposed; after correction, the traditional online updating method can then proceed as usual. Further, asymptotic distributions for the corrected and updated estimators are established. Simulation studies and a real data analysis with an airline on-time dataset are provided to illustrate the performance of the proposed method.},
  archive      = {J_CSDA},
  author       = {JooChul Lee and HaiYing Wang and Elizabeth D. Schifano},
  doi          = {10.1016/j.csda.2020.106976},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106976},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Online updating method to correct for measurement error in big data streams},
  volume       = {149},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feature screening under missing indicator imputation with
non-ignorable missing response. <em>CSDA</em>, <em>149</em>, 106975. (<a
href="https://doi.org/10.1016/j.csda.2020.106975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops a model-free variable screening technique with the non-ignorable missing response in ultrahigh-dimensional data analysis. Based on the common logistic model assumption of the propensity function, a novel screening procedure is proposed by borrowing hidden information of missingness indicator such that any variable screening method for ultrahigh-dimensional covariates with full data can be applied to the non-ignorable missing response case. And it is shown that the sure screening property can be kept as long as the corresponding screening method for full data is of sure screening property. The finite sample performances of the proposed method are demonstrated via some simulations and analysis of functional neuroimaging data .},
  archive      = {J_CSDA},
  author       = {Jing Zhang and Qihua Wang and Jian Kang},
  doi          = {10.1016/j.csda.2020.106975},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106975},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Feature screening under missing indicator imputation with non-ignorable missing response},
  volume       = {149},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Degrees of freedom and model selection for k-means
clustering. <em>CSDA</em>, <em>149</em>, 106974. (<a
href="https://doi.org/10.1016/j.csda.2020.106974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A thorough investigation into the model degrees of freedom in k k -means clustering is conducted. An extension of Stein’s lemma is used to obtain an expression for the effective degrees of freedom in the k k -means model. Approximating the degrees of freedom in practice requires simplifications of this expression, however empirical studies evince the appropriateness of the proposed approach. The practical relevance of this new degrees of freedom formulation for k k -means is demonstrated through model selection using the Bayesian Information Criterion . The reliability of this method is then validated through experiments on simulated data as well as on a large collection of publicly available benchmark data sets from diverse application areas. Comparisons with popular existing techniques indicate that this approach is extremely competitive for selecting high quality clustering solutions.},
  archive      = {J_CSDA},
  author       = {David P. Hofmeyr},
  doi          = {10.1016/j.csda.2020.106974},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106974},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Degrees of freedom and model selection for k-means clustering},
  volume       = {149},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sampling a two dimensional matrix. <em>CSDA</em>,
<em>149</em>, 106971. (<a
href="https://doi.org/10.1016/j.csda.2020.106971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new sampling design for populations whose units can be arranged as an N × M N×M matrix is proposed. The sample must satisfy some constraints: row and column sample sizes are set in advance. The proposed sampling method gives the same selection probability to all the sample matrices that satisfy the constraints. Three algorithms to select a sample uniformly in the feasible set are presented: an exact algorithm based on the multivariate hypergeometric distribution , an MCMC algorithm, and the cube method. Their performances are evaluated using Monte Carlo simulations . The designs for sampling elements in a given row or a given column are investigated and the single inclusion and joint selection probabilities under the proposed design are evaluated. Several variance estimators are proposed for the Horvitz–Thompson estimator of the population mean of the survey variable y y and their performances are compared in a Monte Carlo study . A numerical example dealing with a creel survey of fishermen found at 9 sites over 36 days is presented.},
  archive      = {J_CSDA},
  author       = {Louis-Paul Rivest and Sergio Ewane Ebouele},
  doi          = {10.1016/j.csda.2020.106971},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106971},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Sampling a two dimensional matrix},
  volume       = {149},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rank dynamics for functional data. <em>CSDA</em>,
<em>149</em>, 106963. (<a
href="https://doi.org/10.1016/j.csda.2020.106963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of the dynamic behavior of cross-sectional ranks over time for functional data and the ranks of the observed curves at each time point and their temporal evolution can yield valuable insights into the time dynamics of functional data. This approach is of interest in various application areas. For the analysis of the dynamics of ranks, estimation of the cross-sectional ranks of functional data is a first step. Several statistics of interest for ranked functional data are proposed. To quantify the evolution of ranks over time, a model for rank derivatives is introduced, where rank dynamics are decomposed into two components. One component corresponds to population changes and the other to individual changes that both affect the rank trajectories of individuals. The joint asymptotic normality for suitable estimates of these two components is established. The proposed approaches are illustrated with simulations and three longitudinal datasets: Growth curves obtained from the Zürich Longitudinal Growth Study, monthly house price data in the US from 1996 to 2015 and Major League Baseball offensive data for the 2017 season.},
  archive      = {J_CSDA},
  author       = {Yaqing Chen and Matthew Dawson and Hans-Georg Müller},
  doi          = {10.1016/j.csda.2020.106963},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106963},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Rank dynamics for functional data},
  volume       = {149},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Functional outlier detection and taxonomy by sequential
transformations. <em>CSDA</em>, <em>149</em>, 106960. (<a
href="https://doi.org/10.1016/j.csda.2020.106960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional data analysis can be seriously impaired by abnormal observations, which can be classified as either magnitude or shape outliers based on their way of deviating from the bulk of data. Identifying magnitude outliers is relatively easy, while detecting shape outliers is much more challenging. We propose turning the shape outliers into magnitude outliers through data transformation and detecting them using the functional boxplot . Besides easing the detection procedure, applying several transformations sequentially provides a reasonable taxonomy for the flagged outliers. A joint functional ranking, which consists of several transformations, is also defined here. Simulation studies are carried out to evaluate the performance of the proposed method using different functional depth notions. Interesting results are obtained in several practical applications.},
  archive      = {J_CSDA},
  author       = {Wenlin Dai and Tomáš Mrkvička and Ying Sun and Marc G. Genton},
  doi          = {10.1016/j.csda.2020.106960},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106960},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Functional outlier detection and taxonomy by sequential transformations},
  volume       = {149},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian nonparametric test for independence between random
vectors. <em>CSDA</em>, <em>149</em>, 106959. (<a
href="https://doi.org/10.1016/j.csda.2020.106959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A nonparametric approach for testing independence among groups of continuous random variables is proposed. Gaussian-centered multivariate finite Polya tree priors are used to model the underlying probability distributions. Integrating out the random probability measure, a tractable empirical Bayes factor is derived and used as the test statistic. The Bayes factor is consistent in the sense that it tends to infinity under the alternative, and zero under the null . A p p -value is then obtained through a permutation test based on the observed Bayes factor. Through a series of simulation studies, the performance of the proposed approach is examined and compared to several existing approaches based on the power of the test as well as the observed Bayes factor. Lastly, the proposed method is applied to a set of real data in ecology.},
  archive      = {J_CSDA},
  author       = {Zichen Ma and Timothy E. Hanson},
  doi          = {10.1016/j.csda.2020.106959},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106959},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayesian nonparametric test for independence between random vectors},
  volume       = {149},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The LASSO on latent indices for regression modeling with
ordinal categorical predictors. <em>CSDA</em>, <em>149</em>, 106951. (<a
href="https://doi.org/10.1016/j.csda.2020.106951">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many applications of regression models involve ordinal categorical predictors. Two common approaches for handling ordinal predictors are to form a set of dummy variables , or employ a two stage approach where dimension reduction is first applied and then the response is regressed against the predicted latent indices. Both approaches have drawbacks, with the former running into a high-dimensional problem especially if interactions are considered, while the latter separates the prediction of the latent indices from the construction of the regression model. To overcome these challenges, a new approach called the LASSO on Latent Indices (LoLI) for handling ordinal predictors in regression is proposed, which involves jointly constructing latent indices for each or for groups of ordinal predictors and modeling the response directly as a function of these. LoLI borrows strength from the response to more accurately predict the latent indices, leading to better estimation of the corresponding effects. Furthermore, LoLI incorporates a LASSO type penalty to perform hierarchical selection, with interaction terms selected only if both parent main effects are included. Simulations show that LoLI can outperform the dummy variable and two stage approaches in selection and prediction performance. Applying LoLI to an Australian household-based panel identified three dimensions of psychosocial workplace quality (job demands, stress, and security) which affect an individual’s mental health in an additive and pairwise interactive manner.},
  archive      = {J_CSDA},
  author       = {Francis K.C. Hui and Samuel Müller and A.H. Welsh},
  doi          = {10.1016/j.csda.2020.106951},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106951},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {The LASSO on latent indices for regression modeling with ordinal categorical predictors},
  volume       = {149},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extending finite mixtures of t linear mixed-effects models
with concomitant covariates. <em>CSDA</em>, <em>148</em>, 106961. (<a
href="https://doi.org/10.1016/j.csda.2020.106961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The issue of model-based clustering of longitudinal data has attracted increasing attention in past two decades. Finite mixtures of Student’s- t t linear mixed-effects (FM-tLME) models have been considered for implementing this task especially when data contain extreme observations. This paper presents an extended finite mixtures of Student’s- t t linear mixed-effects (EFM-tLME) model, where the categorical component labels are assumed to be influenced by the observed covariates . As compared with the naive methods assuming the mixing proportions to be fixed but unknown, the proposed EFM-tLME model exploits a logistic function to link the relationship between the prior classification probabilities and the covariates of interest. To carry out maximum likelihood estimation , an alternating expectation conditional maximization (AECM) algorithm is developed under several model reduction schemes. The technique for extracting the information-based standard errors of parameter estimates is also investigated. The proposed method is illustrated using simulation experiments and real data from an AIDS clinical study.},
  archive      = {J_CSDA},
  author       = {Yu-Chen Yang and Tsung-I Lin and Luis M. Castro and Wan-Lun Wang},
  doi          = {10.1016/j.csda.2020.106961},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106961},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Extending finite mixtures of t linear mixed-effects models with concomitant covariates},
  volume       = {148},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Primal path algorithm for compositional data analysis.
<em>CSDA</em>, <em>148</em>, 106958. (<a
href="https://doi.org/10.1016/j.csda.2020.106958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the LASSO estimator for compositional data in which covariates are nonnegative, and their sum is always one. Due to the linear constraint of the regression coefficients caused by the sum to one condition, standard algorithms for LASSO cannot be applied directly to compositional data. Hence, a specific regularized regression model with linear constraints is commonly used. However, linear constraints incur additional computational time, which becomes severe in high-dimensional cases. Additionally, the exact computation for the regression is not investigated under existing methods. In this paper, we first propose an exact solution path algorithm for a l 1 l1 regularized regression with high-dimensional compositional data and extend to a classification model . We also compare its computational speed with that of previously developed algorithms and then apply the proposed algorithm to analyzing income inequality data in economics and human gut microbiome data in biology. By analyzing simulated and real data sets , we illustrate that our specialized algorithm is significantly more efficient than the generalized LASSO algorithm for compositional data.},
  archive      = {J_CSDA},
  author       = {Jong-June Jeon and Yongdai Kim and Sungho Won and Hosik Choi},
  doi          = {10.1016/j.csda.2020.106958},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106958},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Primal path algorithm for compositional data analysis},
  volume       = {148},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic identification of curve shapes with applications
to ultrasonic vocalization. <em>CSDA</em>, <em>148</em>, 106956. (<a
href="https://doi.org/10.1016/j.csda.2020.106956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Like human beings, many animals produce sounds for communication and social interactions. The vocalizations of mice have the characteristics of songs, consisting of syllables of different types determined by the frequency modulations and structure variations. To characterize the impact of social environments and genotypes on vocalizations, it is important to identify the patterns of syllables based on the shapes of frequency contours. Using existing hypothesis testing methods to determine the shape classes would require testing various null and alternative hypotheses for each curve, and is impractical for vocalization studies where the interest is on a large number of frequency contours. A new penalization-based method is proposed, which provides function estimation and automatic shape identification simultaneously. The method estimates the functional curve through quadratic B-spline approximation, and captures the shape feature by penalizing the positive and negative parts of the first two derivatives of the spline function in a group manner. It is shown that under some regularity conditions , the proposed method can identify the correct shape with probability approaching one, and the resulting nonparametric estimator can achieve the optimal convergence rate. Simulation shows that the proposed method gives more stable curve estimation and more accurate curve classification than the unconstrained B-spline estimator, and it is competitive to the shape-constrained estimator assuming prior knowledge of the curve shape. The proposed method is applied to the motivating vocalization study to examine the effect of Methyl-CpG binding protein 2 gene on the vocalizations of mice during courtship.},
  archive      = {J_CSDA},
  author       = {Zhikun Gao and Yanlin Tang and Huixia Judy Wang and Guangying K. Wu and Jeff Lin},
  doi          = {10.1016/j.csda.2020.106956},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106956},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Automatic identification of curve shapes with applications to ultrasonic vocalization},
  volume       = {148},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A simple test for zero multiple correlation coefficient in
high-dimensional normal data using random projection. <em>CSDA</em>,
<em>148</em>, 106955. (<a
href="https://doi.org/10.1016/j.csda.2020.106955">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multiple correlation coefficient (MCC) is a measure of linear relationship between a given variable and a set of covariates . Testing the hypothesis of zero MCC has always been important in multiple correlation analysis. For testing this hypothesis, due to the singularity of the sample covariance matrix in high-dimensional data, the classical testing procedures are no longer usable. To test the null hypothesis of zero MCC in high-dimensional normal data, a simple testing procedure is proposed by using the random projection and union-intersection methodologies. Some simulations are carried out to verify the performance evaluation of the proposed test. The results are found to be very convincing. In the end, the experimental validation of the proposed test is carried out on mice tumor data.},
  archive      = {J_CSDA},
  author       = {Dariush Najarzadeh},
  doi          = {10.1016/j.csda.2020.106955},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106955},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A simple test for zero multiple correlation coefficient in high-dimensional normal data using random projection},
  volume       = {148},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Posterior inference for sparse hierarchical non-stationary
models. <em>CSDA</em>, <em>148</em>, 106954. (<a
href="https://doi.org/10.1016/j.csda.2020.106954">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian processes are valuable tools for non-parametric modelling, where typically an assumption of stationarity is employed. While removing this assumption can improve prediction, fitting such models is challenging. Hierarchical models are constructed based on Gaussian Markov random fields with stochastic spatially varying parameters. Importantly, this allows for non-stationarity while also addressing the computational burden through a sparse banded representation of the precision matrix . In this setting, efficient Markov chain Monte Carlo (MCMC) sampling is challenging due to the strong coupling a posteriori of the parameters and hyperparameters. Three adaptive MCMC schemes are developed and compared making use of banded matrix operations for faster inference. Furthermore, a novel extension to higher dimensional input spaces is proposed through an additive structure that retains the flexibility and scalability of the model, while also inheriting interpretability from the additive approach. A thorough assessment of the efficiency and accuracy of the methods in nonstationary settings is presented for both simulated experiments and a computer emulation problem.},
  archive      = {J_CSDA},
  author       = {Karla Monterrubio-Gómez and Lassi Roininen and Sara Wade and Theodoros Damoulas and Mark Girolami},
  doi          = {10.1016/j.csda.2020.106954},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106954},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Posterior inference for sparse hierarchical non-stationary models},
  volume       = {148},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Finding groups in structural equation modeling through the
partial least squares algorithm. <em>CSDA</em>, <em>147</em>, 106957.
(<a href="https://doi.org/10.1016/j.csda.2020.106957">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The identification of different homogeneous groups of observations and their appropriate analysis in PLS-SEM has become a critical issue in many application fields. Usually, both SEM and PLS-SEM assume the homogeneity of units on which the model is applied. The approaches of segmentation proposed in the literature, consist of estimating separate models for each segment of statistical units, assigning these units to segments defined a priori. These approaches are not fully acceptable because no causal structure is postulated among variables. In other words, a model approach should be used, where the clusters obtained are homogeneous, both with respect to the structural causal relationships , and the mean differences between clusters. Therefore, a new methodology is proposed, where simultaneously non-hierarchical clustering and PLS-SEM is applied. This methodology is motivated by the fact that the sequential approach (i.e., the application, first, of SEM or PLS-SEM and subsequently the use of a clustering algorithm on the latent scores obtained) may fail to find the correct clustering structure of data. A simulation study and an application on real data are included to evaluate the performance of the proposed methodology.},
  archive      = {J_CSDA},
  author       = {Mario Fordellone and Maurizio Vichi},
  doi          = {10.1016/j.csda.2020.106957},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106957},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Finding groups in structural equation modeling through the partial least squares algorithm},
  volume       = {147},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cellwise robust m regression. <em>CSDA</em>, <em>147</em>,
106944. (<a href="https://doi.org/10.1016/j.csda.2020.106944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cellwise robust M regression estimator is introduced as the first estimator of its kind that intrinsically yields both a map of cellwise outliers consistent with the linear model, and a vector of regression coefficients that is robust against vertical outliers and leverage points . As a by-product, the method yields a weighted and imputed data set that contains estimates of what the values in cellwise outliers would need to amount to if they had fit the model. The method is illustrated to be equally robust as its casewise counterpart, MM regression. The cellwise regression method discards less information than any casewise robust estimator . Therefore, predictive power can be expected to be at least as good as casewise alternatives. These results are corroborated in a simulation study. Moreover, while the simulations show that predictive performance is at least on par with casewise methods if not better, an application to a data set consisting of compositions of Swiss nutrients, shows that in individual cases, CRM can achieve a much higher predictive accuracy compared to MM regression.},
  archive      = {J_CSDA},
  author       = {P. Filzmoser and S. Höppner and I. Ortner and S. Serneels and T. Verdonck},
  doi          = {10.1016/j.csda.2020.106944},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106944},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Cellwise robust m regression},
  volume       = {147},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Separating variables to accelerate non-convex regularized
optimization. <em>CSDA</em>, <em>147</em>, 106943. (<a
href="https://doi.org/10.1016/j.csda.2020.106943">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel variable separation algorithm stemmed from the idea of orthogonalization EM is proposed to find the minimization of general function with non-convex regularizer. The main idea of our algorithm is to construct a new function by adding an item that allows minimization to be solved separately on each component. Several attractive theoretical properties concerning the new algorithm are established. The new algorithm converges to one of the critical points with the condition that the objective function is coercive or the generated sequence is in a compact set. The convergence rate of the algorithm is also obtained. The Barzilai–Borwein (BB) rule and Nesterov’s method are also used to accelerate our algorithm. The new algorithm can also be used to solve the minimization of general function with group structure regularizer. The simulation and real data results show that these methods can accelerate our method obviously.},
  archive      = {J_CSDA},
  author       = {Wenchen Liu and Yincai Tang and Xianyi Wu},
  doi          = {10.1016/j.csda.2020.106943},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106943},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Separating variables to accelerate non-convex regularized optimization},
  volume       = {147},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint model-free feature screening for ultra-high
dimensional semi-competing risks data. <em>CSDA</em>, <em>147</em>,
106942. (<a href="https://doi.org/10.1016/j.csda.2020.106942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional semi-competing risks data consisting of two probably correlated events, namely terminal event and non-terminal event, arise commonly in many biomedical studies . However, the corresponding statistical analysis is rarely investigated. A joint model-free feature screening procedure for both terminal and non-terminal events is proposed, which could allow the associated covariates to be in an ultra-high dimensional feature space . The joint screening utility is constructed from distance correlation between each predictor’s survival function and joint survival function of terminal and non-terminal events. Under rather mild technical assumptions, it is demonstrated that the proposed joint feature screening procedure enjoys sure screening and consistency in ranking properties. An adaptive threshold rule is further suggested to simultaneously identify important covariates and determine number of these covariates. Extensive numerical studies are conducted to examine the finite-sample performance of the proposed methods. Lastly, the suggested joint feature screening procedure is illustrated through a real example.},
  archive      = {J_CSDA},
  author       = {Shuiyun Lu and Xiaolin Chen and Sheng Xu and Chunling Liu},
  doi          = {10.1016/j.csda.2020.106942},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106942},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Joint model-free feature screening for ultra-high dimensional semi-competing risks data},
  volume       = {147},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). General matching quantiles m-estimation. <em>CSDA</em>,
<em>147</em>, 106941. (<a
href="https://doi.org/10.1016/j.csda.2020.106941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matching quantiles estimation (MQE) is a useful technique that allows one to find a linear combination of a set of random variables that matches the distribution of a target random variable. Since it is based on ordinary least-squares (OLS), it may be sensitive to outlier observations of the target random variable. A general matching quantiles M-estimation (MQME) method is thus proposed, which is resistant to outlier observations of the target random variable. Given that in most applications, the number of variables p p may be large, a ‘sparse’ representation is highly desirable. The MQME is combined with the adaptive Lasso penalty so it can select informative variables. An iterative algorithm based on M-estimation is developed to compute MQME. The proposed matching quantiles M-estimate is consistent, just like the MQE. Extensive simulations are provided, in which efficient finite-sample performance of the new method is demonstrated. In addition, an illustrative real case study is presented.},
  archive      = {J_CSDA},
  author       = {Shanshan Qin and Yuehua Wu},
  doi          = {10.1016/j.csda.2020.106941},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106941},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {General matching quantiles M-estimation},
  volume       = {147},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the inferential implications of decreasing weight
structures in mixture models. <em>CSDA</em>, <em>147</em>, 106940. (<a
href="https://doi.org/10.1016/j.csda.2020.106940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian estimation of nonparametric mixture models strongly relies on available representations of discrete random probability measures. In particular, the order of the mixing weights plays an important role for the identifiability of component-specific parameters which, in turn, affects the convergence properties of posterior samplers. The geometric process mixture model provides a simple alternative to models based on the Dirichlet process that effectively addresses these issues. However, the rate of decay of the mixing weights for this model may be too fast for modeling data with a large number of components. The need for different decay rates arises. Some variants of the geometric process featuring different decay behaviors, while preserving the decreasing structure, are presented and investigated. An asymptotic characterization of the number of distinct values in a sample from the corresponding mixing measure is also given, highlighting the inferential implications of different prior specifications. The analysis is completed by a simulation study in the context of density estimation. It shows that by controlling the decaying rate, the mixture model is able to capture data with a large number of components.},
  archive      = {J_CSDA},
  author       = {Pierpaolo De Blasi and Asael Fabian Martínez and Ramsés H. Mena and Igor Prünster},
  doi          = {10.1016/j.csda.2020.106940},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106940},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {On the inferential implications of decreasing weight structures in mixture models},
  volume       = {147},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Asymptotic distributions and performance of empirical
skewness measures. <em>CSDA</em>, <em>146</em>, 106939. (<a
href="https://doi.org/10.1016/j.csda.2020.106939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A number of skewness measures have been proposed and applied to theoretical distributions. However, the corresponding empirical counterparts have been analyzed only rarely, especially with respect to their asymptotic properties and limit distributions. Six of these empirical measures are considered. After discussing some general properties, the limiting distribution for each measure is derived under weak assumptions . The performance of these estimators is analyzed in simulations using tests and the coverage probabilities of confidence intervals. A particular focus is put on the standardized central third moment as the most popular measure of skewness. Since it turns out to behave poorly, especially when sample sizes are small, the use of alternative and more suitable skewness measures is recommended. A real data application illustrates some of the findings.},
  archive      = {J_CSDA},
  author       = {Andreas Eberl and Bernhard Klar},
  doi          = {10.1016/j.csda.2020.106939},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106939},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Asymptotic distributions and performance of empirical skewness measures},
  volume       = {146},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-scale shotgun stochastic search for large spatial
datasets. <em>CSDA</em>, <em>146</em>, 106931. (<a
href="https://doi.org/10.1016/j.csda.2020.106931">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large spatial datasets often exhibit fine scale features that only occur in sub-domains of the space, coupled with large scale features at much larger ranges. A multi-scale spatial kernel convolution model is developed where fine scale local features are captured by high resolution knots while lower resolution terms are used to describe large scale features. This method achieves parsimony and explicitly identifies the sub-domains of the space that exhibit fine scale attributes by using a form of shotgun stochastic search coupled with a stochastic process prior that induces structured sparsity resulting in spatially varying resolution. In contrast to existing approaches, this approach does not require Markov chain Monte Carlo to produce a fully probabilistic quantification of the prediction uncertainty. In addition, the model does not require a maximum resolution to be specified in advance. The model fitting approach, based on Bayesian model averaging , is computationally feasible on large datasets, as computations for shotgun stochastic search can be performed in parallel, leveraging the availability of convenient formulas for fast updating the coefficients when adding a single knot. Competitive performance for computations, prediction, and interval estimation is demonstrated using simulation experiments and real data.},
  archive      = {J_CSDA},
  author       = {Daniel Kirsner and Bruno Sansó},
  doi          = {10.1016/j.csda.2020.106931},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106931},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Multi-scale shotgun stochastic search for large spatial datasets},
  volume       = {146},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New EM-type algorithms for the heckman selection model.
<em>CSDA</em>, <em>146</em>, 106930. (<a
href="https://doi.org/10.1016/j.csda.2020.106930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Heckman selection model is widely used to analyse data for which the outcome is partially observable, and the missing part is not random. The 2-step method, maximum likelihood estimation (MLE), and EM algorithms have been developed to analyse this model; however, they have certain limitations. Three new algorithms (ECM, ECM(NR), and ECME) will be proposed with the advantages of the EM algorithm: easy implementation and numerical stability. Considering bias and mean squared error (MSE), simulations with different correlation values suggest that MLE performs similarly to the proposed algorithms; however, MLE as well as the proposed algorithms yield better estimations than the 2-step method. A simulation study in which standard error is also considered demonstrates that the new algorithms are more robust than MLE, and yield slightly better estimations than the 2-step and the robust 2-stage methods. Real data analyses are also provided to discuss the performance of MLE, 2-step, and the proposed algorithms. A real data analysis concerning the robustness issue further illustrates that, under certain conditions, the proposed algorithms are more efficient and stable.},
  archive      = {J_CSDA},
  author       = {Jun Zhao and Hea-Jung Kim and Hyoung-Moon Kim},
  doi          = {10.1016/j.csda.2020.106930},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106930},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {New EM-type algorithms for the heckman selection model},
  volume       = {146},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design optimal sampling plans for functional regression
models. <em>CSDA</em>, <em>146</em>, 106925. (<a
href="https://doi.org/10.1016/j.csda.2020.106925">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional regression models are widely considered in practice. To make a precise statistical inference , a good sampling schedule for collecting informative functional data is needed. However, there has not been much research on the optimal sampling schedule design for functional regression model so far. To address this design issue, an efficient computational approach is proposed for generating the best sampling plan in the function-on-function linear regression setting. The obtained sampling plan allows a precise estimation of the predictor function and a precise prediction of the response function. The proposed approach can also be applied to identify the optimal sampling plan for the problem with scalar-on-function linear regression model. Through case studies, this approach is demonstrated to outperform the methods proposed in the previous studies.},
  archive      = {J_CSDA},
  author       = {Hyungmin Rha and Ming-Hung Kao and Rong Pan},
  doi          = {10.1016/j.csda.2020.106925},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106925},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Design optimal sampling plans for functional regression models},
  volume       = {146},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dimensionality determination: A thresholding double ridge
ratio approach. <em>CSDA</em>, <em>146</em>, 106910. (<a
href="https://doi.org/10.1016/j.csda.2020.106910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underdetermination of model dimensionality (order) is a longstanding problem when existing eigendecomposition-based criteria are used. To alleviate this difficulty, we propose a thresholding double ridge ratio criterion in this paper. Unlike all existing eigendecomposition-based criteria, the proposed criterion can provide a consistent estimate even when there are several local minima. For illustration, we present the generic strategy with three important applications: dimension reduction in regressions with fixed and divergent dimensions; model checking with local alternative models; and ultra-high dimensional approximate factor models. Numerical studies are conducted to examine the finite sample performance of the proposed method and a real data example is analyzed for illustration.},
  archive      = {J_CSDA},
  author       = {Xuehu Zhu and Xu Guo and Tao Wang and Lixing Zhu},
  doi          = {10.1016/j.csda.2020.106910},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106910},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Dimensionality determination: A thresholding double ridge ratio approach},
  volume       = {146},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling rate of adaptive trait evolution using
cox–ingersoll–ross process: An approximate bayesian computation
approach. <em>CSDA</em>, <em>145</em>, 106924. (<a
href="https://doi.org/10.1016/j.csda.2020.106924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the past decades, Gaussian processes have been widely used to study trait evolution. In particular, two members of Gaussian processes, Brownian motion and the Ornstein–Uhlenbeck process, have been frequently applied for describing continuous trait evolution. Several models (OUBM, OUOU, OUBMBM, OUOUBM) have been proposed to study the impact on the optimum of a trait by other traits. Applying the Cox–Ingersoll–Ross (CIR) process on rate of evolution, which prevents rates from becoming negative, is a potentially useful extension developed here as the OUBMCIR and OUOUCIR models. Since the likelihood functions of the OUBMCIR and the OUOUCIR models are intractable, a heuristic algorithm for parameter estimation and inference under Approximate Bayesian Computation (ABC) is proposed. Simulation studies show that new models perform well. Empirical analysis using several data sets from literature also provides evidence of the validity and utility of the new models. The relevant data sets and R scripts developed for this project can be accessed through the link. 1},
  archive      = {J_CSDA},
  author       = {Dwueng-Chwuan Jhwueng},
  doi          = {10.1016/j.csda.2020.106924},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106924},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Modeling rate of adaptive trait evolution using Cox–Ingersoll–Ross process: An approximate bayesian computation approach},
  volume       = {145},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian empirical likelihood for ridge and lasso
regressions. <em>CSDA</em>, <em>145</em>, 106917. (<a
href="https://doi.org/10.1016/j.csda.2020.106917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ridge and lasso regression models, which are also known as regularization methods, are widely used methods in machine learning and inverse problems that introduce additional information to solve ill-posed problems and/or perform feature selection. The ridge and lasso estimates for linear regression parameters can be interpreted as Bayesian posterior estimates when the regression parameters have Normal and independent Laplace (i.e., double-exponential) priors, respectively. A significant challenge in regularization problems is that these approaches assume that data are normally distributed, which makes them not robust to model misspecification . A Bayesian approach for ridge and lasso models based on empirical likelihood is proposed. This method is semiparametric because it combines a nonparametric model and a parametric model . Hence, problems with model misspecification are avoided. Under the Bayesian empirical likelihood approach, the resulting posterior distribution lacks a closed form and has a nonconvex support, which makes the implementation of traditional Markov chain Monte Carlo (MCMC) methods such as Gibbs sampling and Metropolis–Hastings very challenging. To solve the nonconvex optimization and nonconvergence problems, the tailored Metropolis–Hastings approach is implemented. The asymptotic Bayesian credible intervals are derived.},
  archive      = {J_CSDA},
  author       = {Adel Bedoui and Nicole A. Lazar},
  doi          = {10.1016/j.csda.2020.106917},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106917},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayesian empirical likelihood for ridge and lasso regressions},
  volume       = {145},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Vertex nomination: The canonical sampling and the extended
spectral nomination schemes. <em>CSDA</em>, <em>145</em>, 106916. (<a
href="https://doi.org/10.1016/j.csda.2020.106916">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Suppose that one particular block in a stochastic block model is of interest, but block labels are only observed for a few of the vertices in the network. Utilizing a graph realized from the model and the observed block labels, the vertex nomination task is to order the vertices with unobserved block labels into a ranked nomination list with the goal of having an abundance of interesting vertices near the top of the list. There are vertex nomination schemes in the literature, including the optimally precise canonical nomination scheme L C LC and the consistent spectral partitioning nomination scheme L P LP . While the canonical nomination scheme L C LC is provably optimally precise, it is computationally intractable, being impractical to implement even on modestly sized graphs. With this in mind, an approximation of the canonical scheme – denoted the canonical sampling nomination scheme L C S LCS – is introduced; L C S LCS relies on a scalable, Markov chain Monte Carlo-based approximation of L C LC , and converges to L C LC as the amount of sampling goes to infinity. The spectral partitioning nomination scheme is also extended to the extended spectral partitioning nomination scheme , L E P LEP , which introduces a novel semisupervised clustering framework to improve upon the precision of L P LP . Real-data and simulation experiments are employed to illustrate the precision of these vertex nomination schemes, as well as their empirical computational complexity.},
  archive      = {J_CSDA},
  author       = {Jordan Yoder and Li Chen and Henry Pao and Eric Bridgeford and Keith Levin and Donniell E. Fishkind and Carey Priebe and Vince Lyzinski},
  doi          = {10.1016/j.csda.2020.106916},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106916},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Vertex nomination: The canonical sampling and the extended spectral nomination schemes},
  volume       = {145},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Weighted quantile regression in varying-coefficient model
with longitudinal data. <em>CSDA</em>, <em>145</em>, 106915. (<a
href="https://doi.org/10.1016/j.csda.2020.106915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A weighted approach is developed to improve estimation efficiency in varying-coefficient quantile regression model, with longitudinal data . The weights are obtained from empirical likelihood of varying-coefficient mean model, where the nonparametric functions are approximated by basis splines, and the matrix expansion idea in quadratic inference function method is used, to model the inverse of conditional correlation matrix within subject. Theoretical results show that, the weighted estimators of the varying coefficients in quantile regression, can achieve higher efficiency than conventional estimators without weighting scheme. Simulation studies are used to assess the finite sample performance and a real data analysis is also conducted.},
  archive      = {J_CSDA},
  author       = {Fangzheng Lin and Yanlin Tang and Zhongyi Zhu},
  doi          = {10.1016/j.csda.2020.106915},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106915},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Weighted quantile regression in varying-coefficient model with longitudinal data},
  volume       = {145},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bias reduction in the population size estimation of large
data sets. <em>CSDA</em>, <em>145</em>, 106914. (<a
href="https://doi.org/10.1016/j.csda.2020.106914">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimation of the population size of large data sets and hard to reach populations can be a significant problem. For example, in the military, manpower is limited and the manual processing of large data sets can be time consuming. In addition, accessing the full population of data may be restricted by factors such as cost, time, and safety. Four new population size estimators are proposed, as extensions of existing methods, and their performances are compared in terms of bias with two existing methods in the big data literature. These would be particularly beneficial in the context of time-critical decisions or actions. The comparison is based on a simulation study and the application to five real network data sets (Twitter, LiveJournal, Pokec, Youtube, Wikipedia Talk). Whilst no single estimator (out of the four proposed) generates the most accurate estimates overall, the proposed estimators are shown to produce more accurate population size estimates for small sample sizes, but in some cases show more variability than existing estimators in the literature.},
  archive      = {J_CSDA},
  author       = {Jeffrey Chu and Yuanyuan Zhang and Stephen Chan and Saralees Nadarajah},
  doi          = {10.1016/j.csda.2020.106914},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106914},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bias reduction in the population size estimation of large data sets},
  volume       = {145},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal spatial aggregation of space–time models and
applications. <em>CSDA</em>, <em>145</em>, 106913. (<a
href="https://doi.org/10.1016/j.csda.2020.106913">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cancers are serious health concerns for every country. In the U.S. various cancer data are collected, monitored, and studied by the American Cancer Society (ACS). Since the data involves both spatial and temporal components, space–time models are useful for their analyses. Often these data (such as cancer rates) from varying geographical or political areas will be aggregated spatially to correspond to larger regions for analysis at that spatial scale . Methods to compare spatial aggregation schemes and to identify the optimal spatial aggregation are introduced. Specifically, some useful theorems and algorithms to determine the aggregation scheme that results in the minimum aggregate model error will be given, and they are demonstrated using the U.S. ovarian cancer incidence .},
  archive      = {J_CSDA},
  author       = {Andrew Gehman and William W.S. Wei},
  doi          = {10.1016/j.csda.2020.106913},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106913},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Optimal spatial aggregation of space–time models and applications},
  volume       = {145},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new approach to varying-coefficient additive models with
longitudinal covariates. <em>CSDA</em>, <em>145</em>, 106912. (<a
href="https://doi.org/10.1016/j.csda.2020.106912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The varying-coefficient additive model is a novel tool for analyzing functional data. The model generalizes both the varying-coefficient model and the additive model, and retains their merits as an effective dimension reduction model that is flexible yet easily interpretable. However, the original method only works for densely recorded functional response processes with time-invariant covariates . To broaden its applicability, the model is extended to allow for time-dependent covariates and a new fitting approach is proposed that can handle sparsely recorded functional response processes. Consistency and L 2 L2 rate of convergence are developed for the proposed estimators of the unknown functions. A simple algorithm is developed that overcomes the computational difficulty caused by the non-convexity of the objective function. The proposed approach is illustrated through a simulation study and a real data application.},
  archive      = {J_CSDA},
  author       = {Xiaoke Zhang and Qixian Zhong and Jane-Ling Wang},
  doi          = {10.1016/j.csda.2020.106912},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106912},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A new approach to varying-coefficient additive models with longitudinal covariates},
  volume       = {145},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Active learning in multiple-class classification problems
via individualized binary models. <em>CSDA</em>, <em>145</em>, 106911.
(<a href="https://doi.org/10.1016/j.csda.2020.106911">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a unified algorithm for both categorical and ordinal labeled data in multiclass classification problems, where each subject belongs to one class only. In training an effective classification rule, it is critical that one have and rely on a sufficient amount of reliably labeled data. As information on the training sample sizes needed to obtain satisfactory performance is lacking, we adopt an adaptive subject recruiting scheme with an experimental design criterion to shorten the training process. Because this kind of active learning method is naturally conducted in a sequential manner, we adopt sequential analysis to control the required sample size and ensure the performance of the final classifier. Additionally, we report its statistical properties and numerical results from using synthesized and real data.},
  archive      = {J_CSDA},
  author       = {Jingjing Li and Zimu Chen and Zhanfeng Wang and Yuan-chin Ivan Chang},
  doi          = {10.1016/j.csda.2020.106911},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106911},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Active learning in multiple-class classification problems via individualized binary models},
  volume       = {145},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Smooth backfitting for errors-in-variables varying
coefficient regression models. <em>CSDA</em>, <em>145</em>, 106909. (<a
href="https://doi.org/10.1016/j.csda.2019.106909">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Varying coefficient models inherit the simplicity and easy interpretation of classical linear models while enjoying the flexibility of nonparametric models . They are very useful in analyzing the relation between a response and a set of predictors. There has been no study, however, on the estimation of varying coefficients when the predictors, on which the varying coefficients depend, are contaminated by measurement errors. A new kernel smoothing technique that is tailored to the structure of an underlying varying coefficient model as well as corrects for the bias due to the measurement errors is developed here. The estimators of the varying coefficients are given implicitly by solving a system of integral equations, whose implementation requires an iterative backfitting procedure. The existence of a unique solution and the convergence of the associated backfitting algorithm are established theoretically. Some numerical evidences that support the theory and demonstrate the success of the proposed methodology are presented.},
  archive      = {J_CSDA},
  author       = {Kyunghee Han and Young K. Lee and Byeong U. Park},
  doi          = {10.1016/j.csda.2019.106909},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106909},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Smooth backfitting for errors-in-variables varying coefficient regression models},
  volume       = {145},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Specification tests in semiparametric transformation models
— a multiplier bootstrap approach. <em>CSDA</em>, <em>145</em>, 106908.
(<a href="https://doi.org/10.1016/j.csda.2019.106908">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semiparametric transformation models are considered, where after pre-estimation of a parametric transformation of the response the data are modeled by means of nonparametric regression. Subsequent procedures for testing lack-of-fit of the regression function and for significance of covariates are suggested. In contrast to existing procedures, the tests are asymptotically not influenced by the pre-estimation of the transformation in the sense that they have the same asymptotic distribution as in regression models without transformation. Validity of a multiplier bootstrap procedure is shown which is easier to implement and much less computationally demanding than bootstrap procedures based on the transformation model. In a simulation study the superior performance of the procedure in comparison with its existing competitors is demonstrated.},
  archive      = {J_CSDA},
  author       = {Nick Kloodt and Natalie Neumeyer},
  doi          = {10.1016/j.csda.2019.106908},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106908},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Specification tests in semiparametric transformation models — a multiplier bootstrap approach},
  volume       = {145},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Logistic regression with missing covariates—parameter
estimation, model selection and prediction within a joint-modeling
framework. <em>CSDA</em>, <em>145</em>, 106907. (<a
href="https://doi.org/10.1016/j.csda.2019.106907">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Logistic regression is a common classification method in supervised learning. Surprisingly, there are very few solutions for performing logistic regression with missing values in the covariates . A complete approach based on a stochastic approximation version of the EM algorithm is proposed in order to perform statistical inference with missing values, including the estimation of the parameters and their variance, derivation of confidence intervals, and also a model selection procedure . The problem of prediction for new observations on a test set with missing covariate data is also tackled. Supported by a simulation study in which the method is compared to previous ones, it has proved to be computationally efficient, and has good coverage and variable selection properties. The approach is then illustrated on a dataset of severely traumatized patients from Paris hospitals by predicting the occurrence of hemorrhagic shock, a leading cause of early preventable death in severe trauma cases. The aim is to improve the current red flag procedure, a binary alert identifying patients with a high risk of severe hemorrhage. The method is implemented in the R package misaem .},
  archive      = {J_CSDA},
  author       = {Wei Jiang and Julie Josse and Marc Lavielle and TraumaBase Group},
  doi          = {10.1016/j.csda.2019.106907},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106907},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Logistic regression with missing covariates—Parameter estimation, model selection and prediction within a joint-modeling framework},
  volume       = {145},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modelling multilevel data under complex sampling designs: An
empirical likelihood approach. <em>CSDA</em>, <em>145</em>, 106906. (<a
href="https://doi.org/10.1016/j.csda.2019.106906">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data used in social, behavioural, health or biological sciences may have a hierarchical structure due to the population of interest or the sampling design. Multilevel or marginal models are often used to analyse such hierarchical data . These data are often selected with unequal probabilities from a clustered and stratified population. An empirical likelihood approach for the regression parameters of a multilevel model is proposed. It has the advantage of taking into account of the sampling design. This approach can be used for point estimation, hypothesis testing and confidence intervals for the sub-vector of parameters. It provides asymptotically valid inference for small and large sampling fractions. The simulation study shows the advantages of the empirical likelihood approach over alternative parametric approaches. The approach proposed is illustrated using the Programme for International Student Assessment (PISA) survey data.},
  archive      = {J_CSDA},
  author       = {Melike Oǧuz-Alper and Yves G. Berger},
  doi          = {10.1016/j.csda.2019.106906},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106906},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Modelling multilevel data under complex sampling designs: An empirical likelihood approach},
  volume       = {145},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adjacency-based regularization for partially ranked data
with non-ignorable missing. <em>CSDA</em>, <em>145</em>, 106905. (<a
href="https://doi.org/10.1016/j.csda.2019.106905">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In analyzing ranked data, we often encounter situations in which data are partially ranked. Regarding partially ranked data as missing data, this paper addresses parameter estimation for partially ranked data under a (possibly) non-ignorable missing mechanism. We propose estimators for both complete rankings and missing mechanisms together with a simple estimation procedure. The proposed procedure leverages the structured regularization based on an adjacency structure behind partially ranked data as well as the Expectation–Maximization algorithm. The experimental results demonstrate that the proposed estimator works well under non-ignorable missing mechanisms.},
  archive      = {J_CSDA},
  author       = {Kento Nakamura and Keisuke Yano and Fumiyasu Komaki},
  doi          = {10.1016/j.csda.2019.106905},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106905},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Adjacency-based regularization for partially ranked data with non-ignorable missing},
  volume       = {145},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation and inference for area-wise spatial income
distributions from grouped data. <em>CSDA</em>, <em>145</em>, 106904.
(<a href="https://doi.org/10.1016/j.csda.2019.106904">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating income distributions plays an important role in the measurement of inequality and poverty over space. The existing literature on income distributions predominantly focuses on estimating an income distribution for a country or a region separately and the simultaneous estimation of multiple income distributions has not been discussed in spite of its practical importance. To overcome the difficulty, effective methods are proposed for the simultaneous estimation and inference for area-wise spatial income distributions taking account of geographical information from grouped data. An efficient Bayesian approach to estimation and inference for area-wise latent parameters are developed, which gives area-wise summary measures of income distributions such as mean incomes and Gini indices , not only for sampled areas but also for areas without any samples thanks to the latent spatial state–space structure. The proposed method is demonstrated using the Japanese municipality-wise grouped income data. The simulation studies show the superiority of the proposed method to a crude conventional approach which estimates the income distributions separately. R code implementing the proposed methods is available at https://github.com/sshonosuke/SPID .},
  archive      = {J_CSDA},
  author       = {Shonosuke Sugasawa and Genya Kobayashi and Yuki Kawakubo},
  doi          = {10.1016/j.csda.2019.106904},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106904},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Estimation and inference for area-wise spatial income distributions from grouped data},
  volume       = {145},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust bayesian small area estimation based on quantile
regression. <em>CSDA</em>, <em>145</em>, 106900. (<a
href="https://doi.org/10.1016/j.csda.2019.106900">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantile and M-quantile regression have been applied successfully to small area estimation within the frequentist approach . Quantile regression is applied in the same context but from a Bayesian perspective . Joint modelling of the quantile function is considered, adopting a non parametric assumption on the data generating process that nonetheless explicitly includes the normal distribution as a special case. A specification of the random part of the model that is simple and consistent with the predictive aim of small area estimation is proposed. Although the main output of the method is the estimation of the whole quantile function, estimators of the small area means based on the integration of the quantile function are proposed and discussed. A simulation exercise is used to assess the frequentist properties of these proposed predictors, that result at least as efficient as frequentist small area estimators based on quantile regression in scenarios characterized by the presence of outliers. The proposed method is illustrated using data from the European survey on Income and Living Conditions (EU-SILC).},
  archive      = {J_CSDA},
  author       = {Enrico Fabrizi and Nicola Salvati and Carlo Trivisano},
  doi          = {10.1016/j.csda.2019.106900},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106900},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Robust bayesian small area estimation based on quantile regression},
  volume       = {145},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparametric procedures for partially paired data in two
groups. <em>CSDA</em>, <em>144</em>, 106903. (<a
href="https://doi.org/10.1016/j.csda.2019.106903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fully nonparametric method is developed for comparing samples with partially paired data. Partially-paired (correlated) data naturally arise, for example, as a result of missing values, in incomplete block designs or meta analysis. In the nonparametric setup, treatment effects are characterized in terms of functionals of distribution functions and the only assumption needed is that the marginal distributions are non-degenerate. The setup accommodates binary, ordered categorical, discrete and continuous data in a seamless fashion. The use of nonparametric effects addresses the Behrens–Fisher problem from the nonparametric point of view and allows construction of confidence intervals. Although the nonparametric methods are mainly asymptotic, methods for small sample approximations are also proposed. Size and power simulation results show numerical evidence of favorable performance of the nonparametric methods. The new nonparametric method has overwhelming power advantage when treatment effects are in the shape of the distributions and they perform comparably well with parametric methods for location-type alternatives. Data from randomized trials in public health are used to illustrate the application of the method.},
  archive      = {J_CSDA},
  author       = {Solomon W. Harrar and Merga B. Feyasa and Eshetu Wencheko},
  doi          = {10.1016/j.csda.2019.106903},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106903},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Nonparametric procedures for partially paired data in two groups},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Corrected mallows criterion for model averaging.
<em>CSDA</em>, <em>144</em>, 106902. (<a
href="https://doi.org/10.1016/j.csda.2019.106902">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important problem with model averaging approach is the choice of weights. The Mallows criterion for choosing weights suggested by Hansen (2007) is the first asymptotically optimal criterion , which has been used widely. In the current paper, the authors propose a corrected Mallows model averaging (MMAc) method based on F F distribution in small sample sizes. MMAc exhibits the same asymptotic optimality as Mallows model averaging (MMA) in the sense of minimizing the squared errors. The consistency of the MMAc based weights tending to the optimal weights minimizing MSE is also studied. The authors derive the convergence rate of the new empirical weights. Similar property for MMA and Jackknife model averaging (JMA) by Hansen and Racine (2012) is established as well. An extensive simulation study shows that MMAc often performs better than MMA and other commonly used model averaging methods, especially for small and moderate sample size cases. The results from the real data analysis also support the proposed method.},
  archive      = {J_CSDA},
  author       = {Jun Liao and Guohua Zou},
  doi          = {10.1016/j.csda.2019.106902},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106902},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Corrected mallows criterion for model averaging},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Borrowing strength and borrowing index for bayesian
hierarchical models. <em>CSDA</em>, <em>144</em>, 106901. (<a
href="https://doi.org/10.1016/j.csda.2019.106901">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel borrowing strength measure and an overall borrowing index to characterize the strength of borrowing behaviors among subgroups are proposed for a given Bayesian hierarchical model. The constructions of the proposed indexes are based on the Mallow’s distance and can be easily computed using MCMC samples for univariate or multivariate posterior distributions . Consequently, the proposed indexes can serve as meaningful and useful exploratory tools to better understand the roles played by the priors in a hierarchical model, including their influences on the posteriors that are used to make statistical inferences . These relationships are otherwise ambiguous. The proposed methods can be applied to both the continuous and binary outcome variables. Furthermore, the proposed approach can be easily adapted to various settings of clinical trials, where Bayesian hierarchical models are deem appropriate. The effectiveness of the proposed method is illustrated using extensive simulation studies and a real data example.},
  archive      = {J_CSDA},
  author       = {Ganggang Xu and Huirong Zhu and J. Jack Lee},
  doi          = {10.1016/j.csda.2019.106901},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106901},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Borrowing strength and borrowing index for bayesian hierarchical models},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient calculation of the joint distribution of order
statistics. <em>CSDA</em>, <em>144</em>, 106899. (<a
href="https://doi.org/10.1016/j.csda.2019.106899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of computing the joint distribution of order statistics of stochastically independent random variables in one- and two-group models is considered. While recursive formulae for evaluating the joint cumulative distribution function of such order statistics exist, their numerical implementation remains a challenging task. This task is tackled by presenting novel generalizations of known recursions. They are utilized to obtain exact results (calculated in rational arithmetic) as well as faithfully rounded results. Finally, some applications in goodness-of-fit testing, step-wise multiple hypothesis testing , and sample size calculation for studies with multiple endpoints are discussed.},
  archive      = {J_CSDA},
  author       = {Jonathan von Schroeder and Thorsten Dickhaus},
  doi          = {10.1016/j.csda.2019.106899},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106899},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Efficient calculation of the joint distribution of order statistics},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). More powerful goodness-of-fit tests for uniform stochastic
ordering. <em>CSDA</em>, <em>144</em>, 106898. (<a
href="https://doi.org/10.1016/j.csda.2019.106898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ordinal dominance curve (ODC) is a useful graphical tool to compare two population distributions. These distributions are said to satisfy uniform stochastic ordering (USO) if the ODC for them is star-shaped. A goodness-of-fit test for USO was recently proposed when both distributions are unknown. This test involves calculating the L p Lp distance between an empirical estimator of the ODC and its least star-shaped majorant. The least favorable configuration of the two distributions was established so that proper critical values could be determined; i.e., to control the probability of type I error for all star-shaped ODCs. However, the use of these critical values can lead to a conservative test and minimal power to detect certain non-star-shaped alternatives. Two new methods for determining data-dependent critical values are proposed. Simulation is used to show both methods can provide substantial increases in power while still controlling the size of the distance-based test. The methods are also applied to a data set involving premature infants. An R package that implements all tests is provided.},
  archive      = {J_CSDA},
  author       = {Dewei Wang and Chuan-Fa Tang and Joshua M. Tebbs},
  doi          = {10.1016/j.csda.2019.106898},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106898},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {More powerful goodness-of-fit tests for uniform stochastic ordering},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model checks for functional linear regression models based
on projected empirical processes. <em>CSDA</em>, <em>144</em>, 106897.
(<a href="https://doi.org/10.1016/j.csda.2019.106897">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goodness-of-fit testing for functional linear regression models with functional responses is studied. A residual-marked empirical process-based test is proposed. The test is projection-based, which can well circumvent the curse of dimensionality . The test is omnibus against any global alternative hypothesis as it integrates over all projection directions in the unit ball. The weak convergence of the test statistic under the null hypothesis is derived and it is shown that the proposed test can detect the local alternative hypotheses distinct from the null hypothesis at the fastest possible rate of order O ( n − 1 ∕ 2 ) O(n−1∕2) . To reduce computational burden for critical value determination, a nonparametric Monte Carlo method is used, and simulation studies show the good performance of the proposed method in various scenarios. An ergonomics data set is analyzed for illustration.},
  archive      = {J_CSDA},
  author       = {Feifei Chen and Qing Jiang and Zhenghui Feng and Lixing Zhu},
  doi          = {10.1016/j.csda.2019.106897},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106897},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Model checks for functional linear regression models based on projected empirical processes},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model-free two-sample test for network-valued data.
<em>CSDA</em>, <em>144</em>, 106896. (<a
href="https://doi.org/10.1016/j.csda.2019.106896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the framework of Object Oriented Data Analysis, a permutation approach to the two-sample testing problem for network-valued data is proposed. In detail, the present framework proceeds in four steps: (i) matrix representation of the networks, (ii) computation of the matrix of pairwise (inter-point) distances, (iii) computation of test statistics based on inter-point distances and (iv) embedding of the test statistics within a permutation test . The proposed testing procedures are proven to be exact for every finite sample size and consistent. Two new test statistics based on inter-point distances (i.e., IP-Student and IP-Fisher) are defined and a method to combine them to get a further inferential tool (i.e., IP-StudentFisher) is introduced. Simulated data shows that tests with our statistic exhibit a statistical power that is either the best or second-best but very close to the best on a variety of possible alternatives hypotheses and other statistics. A second simulation study that aims at better understanding which features are captured by specific combinations of matrix representations and distances is presented. Finally, a case study on mobility networks in the city of Milan is carried out. The proposed framework is fully implemented in the R package nevada (NEtwork-VAlued Data Analysis).},
  archive      = {J_CSDA},
  author       = {Ilenia Lovato and Alessia Pini and Aymeric Stamm and Simone Vantini},
  doi          = {10.1016/j.csda.2019.106896},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106896},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Model-free two-sample test for network-valued data},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tests for validity of the semiparametric heteroskedastic
transformation model. <em>CSDA</em>, <em>144</em>, 106895. (<a
href="https://doi.org/10.1016/j.csda.2019.106895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There exist a number of tests for assessing the nonparametric heteroskedastic location-scale assumption. The goodness-of-fit tests considered are for the more general hypothesis of the validity of this model under a parametric functional transformation on the response variable, specifically testing for independence between the regressors and the errors in a model where the transformed response is just a location/scale shift of the error is considered. The proposed criteria use the familiar factorization property of the joint characteristic function under independence. The difficulty is that the errors are unobserved and hence one needs to employ properly estimated residuals in their place. The limit distribution of the test statistics is studied under the null hypothesis as well as under alternatives, and also a resampling procedure is suggested in order to approximate the critical values of the tests. This resampling is subsequently employed in a series of Monte Carlo experiments that illustrate the finite-sample properties of the new test. The performance of related test statistics for normality and symmetry of errors is also investigated, and application of our methods on real data sets is provided.},
  archive      = {J_CSDA},
  author       = {Marie Hušková and Simos G. Meintanis and Charl Pretorius},
  doi          = {10.1016/j.csda.2019.106895},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106895},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Tests for validity of the semiparametric heteroskedastic transformation model},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Grouped variable screening for ultra-high dimensional data
for linear model. <em>CSDA</em>, <em>144</em>, 106894. (<a
href="https://doi.org/10.1016/j.csda.2019.106894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ultra-high dimensional data sets often need a screening step that removes irrelevant variables prior to the main analysis. In high-dimensional linear regression, screening relevant predictors before the model estimation often yields a better prediction accuracy and much faster computation. However, most existing screening approaches target on individual predictors, thus are not able to incorporate structured predictors, such as dummy variables and grouped variables. New screening methods for naturally grouped predictors for high dimensional linear regression are presented. Two popular variable screening methods are generalized to the grouped predictors case, and also a novel screening procedure is proposed. Asymptotic sure screening properties for all three methods are established. Also empirical benefits of the screening approaches via simulation and a real data analysis are demonstrated. Specifically, a two-step analysis that does a screening followed by a sparse estimation improves the prediction accuracy as well as computing time, compared to one-stage sparse regression.},
  archive      = {J_CSDA},
  author       = {Debin Qiu and Jeongyoun Ahn},
  doi          = {10.1016/j.csda.2019.106894},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106894},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Grouped variable screening for ultra-high dimensional data for linear model},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bias and covariance of the least squares estimate in a
structured errors-in-variables problem. <em>CSDA</em>, <em>144</em>,
106893. (<a href="https://doi.org/10.1016/j.csda.2019.106893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A structured errors-in-variables (EIV) problem arising in metrology is studied. The observations of a sensor response are subject to perturbation. The input estimation from the transient response leads to a structured EIV problem. Total least squares (TLS) is a typical estimation method to solve EIV problems. The TLS estimator of an EIV problem is consistent, and can be computed efficiently when the perturbations have zero mean, and are independently and identically distributed (i.i.d). If the perturbation is additionally Gaussian, the TLS solution coincides with maximum-likelihood (ML). However, the computational complexity of structured TLS and total ML prevents their real-time implementation. The least-squares (LS) estimator offers a suboptimal but simple recursive solution to structured EIV problems with correlation, but the statistical properties of the LS estimator are unknown. To know the LS estimate uncertainty in EIV problems, either structured or not, to provide confidence bounds for the estimation uncertainty , and to find the difference from the optimal solutions, the bias and variance of the LS estimates should be quantified. Expressions to predict the bias and variance of LS estimators applied to unstructured and structured EIV problems are derived. The predicted bias and variance quantify the statistical properties of the LS estimate and give an approximation of the uncertainty and the mean squared error for comparison to the Cramér–Rao lower bound of the structured EIV problem.},
  archive      = {J_CSDA},
  author       = {Gustavo Quintana Carapia and Ivan Markovsky and Rik Pintelon and Péter Zoltán Csurcsia and Dieter Verbeke},
  doi          = {10.1016/j.csda.2019.106893},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106893},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bias and covariance of the least squares estimate in a structured errors-in-variables problem},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quantile regression in big data: A divide and conquer based
strategy. <em>CSDA</em>, <em>144</em>, 106892. (<a
href="https://doi.org/10.1016/j.csda.2019.106892">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantile regression, which analyzes the conditional distribution of outcomes given a set of covariates , has been widely used in many fields. However, the volume and velocity of big data make the estimation of quantile regression model extremely difficult due to the intensive computation and the limited storage. Based on divide and conquer strategy, a simple and efficient method is proposed to address this problem. The proposed approach only keeps summary statistics of each data block and then can use them to reconstruct the estimator of the entire data with asymptotically negligible approximation error. This property makes the proposed method particularly appealing when data blocks are retained in multiple servers or come in the form of data stream. Furthermore, the proposed estimator is shown to be consistent and asymptotically as efficient as the estimating equation estimator calculated using the entire data together when certain conditions hold. The merits of the proposed method are illustrated using both simulation studies and real data analysis.},
  archive      = {J_CSDA},
  author       = {Lanjue Chen and Yong Zhou},
  doi          = {10.1016/j.csda.2019.106892},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106892},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Quantile regression in big data: A divide and conquer based strategy},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation of the additive hazards model with case k
interval-censored failure time data in the presence of informative
censoring. <em>CSDA</em>, <em>144</em>, 106891. (<a
href="https://doi.org/10.1016/j.csda.2019.106891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The additive hazards model is one of the most commonly used model in regression analysis of failure time data and many estimation procedures have been developed for its inference under various situations (Kalbfleisch and Prentice (2002); Lin and Ying (1994); Sun (2006)). In this paper, we consider a situation, case K K interval-censored data with informative interval censoring, that often occurs in practice such as medical follow-up studies but has not been discussed much in the literature due to the difficulties involved. For the problem, a joint model is proposed to describe the correlation between the failure time of interest and the underlying censoring or observation process and a sieve maximum likelihood approach is developed. In particular, an EM algorithm is presented for the implementation of the proposed estimation procedure and the asymptotic properties of the resulting estimators are established. A simulation study is conducted to assess the finite sample performance of the proposed method and suggests that it works well for practical situations. Also the method is applied to an AIDS study that motivated this study.},
  archive      = {J_CSDA},
  author       = {Shuying Wang and Chunjie Wang and Peijie Wang and Jianguo Sun},
  doi          = {10.1016/j.csda.2019.106891},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106891},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Estimation of the additive hazards model with case k interval-censored failure time data in the presence of informative censoring},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interval estimation of the ruin probability in the classical
compound poisson risk model. <em>CSDA</em>, <em>144</em>, 106890. (<a
href="https://doi.org/10.1016/j.csda.2019.106890">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating ruin probability is an important problem in insurance. Zhang et al. (2014) proposed a novel nonparametric estimation method for the ruin probability in the classical risk model with unknown claim size distribution, based on the Fourier transform and the kernel density estimation . However, asymptotic distributions of their estimators are unknown, which hampers statistical inference for the ruin probability. The authors establish asymptotic normal distributions of the estimators with known and with unknown intensity. Since the standard deviations of estimators are hard to estimate, a bootstrap method is advanced to estimate them. This allows one to construct a confidence interval estimate of the ruin probability. Furthermore, a new method is proposed to fast calculate the estimates, and the numerical results are stable and free of the “curse of large initial surplus” problem. Simulations are conducted to demonstrate nice finite sample performance of the estimators. A real dataset from a car insurance company is analyzed for illustrating the use of the proposed methodology.},
  archive      = {J_CSDA},
  author       = {Honglong You and Junyi Guo and Jiancheng Jiang},
  doi          = {10.1016/j.csda.2019.106890},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106890},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Interval estimation of the ruin probability in the classical compound poisson risk model},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A high-dimensional spatial rank test for two-sample location
problems. <em>CSDA</em>, <em>144</em>, 106889. (<a
href="https://doi.org/10.1016/j.csda.2019.106889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In high-dimensional situations, the traditional multivariate sign- or rank-based procedures for the two-sample location testing problems are ineffective, since in the construction of the test statistics, the scatter matrix to be inverted is singular. To solve this problem, many high-dimensional spatial sign or rank tests have been proposed, some of which are very efficient. However, most of these existing tests no longer work in very high dimensional situations, which only allows the dimension of variables to be the square of the sample sizes at most, hence are restrictive for practical applications. On this ground, a new high-dimensional spatial rank test is proposed in this paper, which is invariant under scalar transformations, maintains the efficiency advantage of spatial-rank-based testing methods, and could even allow the dimension to grow almost exponentially with the sample sizes. The theoretical results of the proposed test are established, followed by some convincing numerical results and two real data analyses .},
  archive      = {J_CSDA},
  author       = {Long Feng and Xiaoxu Zhang and Binghui Liu},
  doi          = {10.1016/j.csda.2019.106889},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106889},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A high-dimensional spatial rank test for two-sample location problems},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Smoothed empirical likelihood inference and variable
selection for quantile regression with nonignorable missing response.
<em>CSDA</em>, <em>144</em>, 106888. (<a
href="https://doi.org/10.1016/j.csda.2019.106888">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With nonignorable missing responses, an efficient estimator and a variable selection method for quantile regression coefficient are proposed based on smoothed weighted empirical likelihood (SWEL). To handle the identifiability issue, a part of covariates named as nonresponse instrument is used, which is related to response but unrelated to the propensity conditioned on response and other observed covariates . The generalized method of moments is applied to estimate the parameters in the nonresponse propensity. Based on inverse probability weighting and kernel smoothing approaches, consistency and asymptotic normality of the proposed estimator for quantile regression coefficient are established. The asymptotic properties of the resulting SWEL ratio function and the corresponding confidence regions are derived. Further, a penalized SWEL method and its algorithm for variable selection are investigated. The finite-sample performance of the proposed estimator is studied through simulation, and an application to HIV-CD4 data set is also presented.},
  archive      = {J_CSDA},
  author       = {Ting Zhang and Lei Wang},
  doi          = {10.1016/j.csda.2019.106888},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106888},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Smoothed empirical likelihood inference and variable selection for quantile regression with nonignorable missing response},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A goodness-of-fit test for zero-inflated poisson mixed
effects models in tree abundance studies. <em>CSDA</em>, <em>144</em>,
106887. (<a href="https://doi.org/10.1016/j.csda.2019.106887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Field studies in ecology often make use of data collected in a hierarchical fashion, and may combine studies that vary in sampling design. For example, studies of tree recruitment after disturbance may use counts of individual seedlings from plots that vary in spatial arrangement and sampling density. To account for the multi-level design and the fact that more than a few plots usually yield no individuals, a mixed effects zero inflated Poisson model is often adopted. Although it is a convenient modeling strategy, various aspects of the model could be misspecified. A comprehensive test procedure, based on the cumulative sum of the residuals, is proposed. The test is proven to be consistent, and its convergence properties are established as well. The application of the proposed test is illustrated by a real data example and simulation studies.},
  archive      = {J_CSDA},
  author       = {Juxin Liu and Yanyuan Ma and Jill Johnstone},
  doi          = {10.1016/j.csda.2019.106887},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106887},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A goodness-of-fit test for zero-inflated poisson mixed effects models in tree abundance studies},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation of local degree distributions via local weighted
averaging and monte carlo cross-validation. <em>CSDA</em>, <em>144</em>,
106886. (<a href="https://doi.org/10.1016/j.csda.2019.106886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Owing to their capability of summarising the interactions between the elements of a system, networks have become a common type of data across a broad range of scientific fields. As networks can be heterogeneous – in the sense that different regions of the network may exhibit different topologies – an important topic concerns the study of their local properties. A method to estimate the local degree distribution of a vertex in a heterogeneous network is developed. The contributions are twofold: firstly, the proposal of an estimator based on local weighted averaging and secondly, the set up of a Monte Carlo cross-validation procedure to pick the parameters of this estimator. The method is illustrated by several numerical experiments, showing in particular that the approach considerably improves upon the natural, empirical estimator.},
  archive      = {J_CSDA},
  author       = {Paulo Serra and Michel Mandjes},
  doi          = {10.1016/j.csda.2019.106886},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106886},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Estimation of local degree distributions via local weighted averaging and monte carlo cross-validation},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computing confidence intervals from massive data via
penalized quantile smoothing splines. <em>CSDA</em>, <em>144</em>,
106885. (<a href="https://doi.org/10.1016/j.csda.2019.106885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {New methodology is presented for the computation of pointwise confidence intervals from massive response data sets in one or two covariates using robust and flexible quantile regression splines. Novel aspects of the method include a new cross-validation procedure for selecting the penalization coefficient and a reformulation of the quantile smoothing problem based on a weighted data representation. These innovations permit for uncertainty quantification and fast parameter selection in very large data sets via a distributed “bag of little bootstraps”. Experiments with synthetic data demonstrate that the computed confidence intervals feature empirical coverage rates that are generally within 2\% of the nominal rates. The approach is broadly applicable to the analysis of large data sets in one or two dimensions . Comparative (or “A/B”) experiments conducted at Netflix aimed at optimizing the quality of streaming video originally motivated this work, but the proposed methods have general applicability. The methodology is illustrated using an open source application : the comparison of geo-spatial climate model scenarios from NASA’s Earth Exchange.},
  archive      = {J_CSDA},
  author       = {Likun Zhang and Enrique del Castillo and Andrew J. Berglund and Martin P. Tingley and Nirmal Govind},
  doi          = {10.1016/j.csda.2019.106885},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106885},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Computing confidence intervals from massive data via penalized quantile smoothing splines},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient computation for differential network analysis with
applications to quadratic discriminant analysis. <em>CSDA</em>,
<em>144</em>, 106884. (<a
href="https://doi.org/10.1016/j.csda.2019.106884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential network analysis is an important statistical problem with wide applications. Many statisticians focus on binary problems and propose to perform such analysis by obtaining sparse estimates of the difference between precision matrices . These methods are supported by excellent theoretical properties and practical performance. However, efficient computation for these methods remains a challenging problem. A novel algorithm referred to as the SMORE algorithm is proposed for differential network analysis . The SMORE algorithm has low storage cost and high computation speed, especially in the presence of strong sparsity . In the meantime, the SMORE algorithm provides a unified framework for binary and multiple network problems. In addition, the SMORE algorithm can be applied in high-dimensional quadratic discriminant analysis problems as well, leading to a new approach for multiclass high-dimensional quadratic discriminant analysis . Numerical studies confirm the stability and the efficiency of the proposed SMORE algorithm in both differential network analysis and quadratic discriminant analysis.},
  archive      = {J_CSDA},
  author       = {Yuqing Pan and Qing Mai},
  doi          = {10.1016/j.csda.2019.106884},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106884},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Efficient computation for differential network analysis with applications to quadratic discriminant analysis},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Structured analysis of the high-dimensional FMR model.
<em>CSDA</em>, <em>144</em>, 106883. (<a
href="https://doi.org/10.1016/j.csda.2019.106883">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The finite mixture of regression (FMR) model is a popular tool for accommodating data heterogeneity . In the analysis of FMR models with high-dimensional covariates , it is necessary to conduct regularized estimation and identify important covariates rather than noises. In the literature, there has been a lack of attention paid to the differences among important covariates, which can lead to the underlying structure of covariate effects. Specifically, important covariates can be classified into two types: those that behave the same in different subpopulations and those that behave differently. It is of interest to conduct structured analysis to identify such structures, which will enable researchers to better understand covariates and their associations with outcomes. Specifically, the FMR model with high-dimensional covariates is considered. A structured penalization approach is developed for regularized estimation, selection of important variables, and, equally importantly, identification of the underlying covariate effect structure. The proposed approach can be effectively realized, and its statistical properties are rigorously established. Simulation demonstrates its superiority over alternatives. In the analysis of cancer gene expression data , interesting models/structures missed by the existing analysis are identified.},
  archive      = {J_CSDA},
  author       = {Mengque Liu and Qingzhao Zhang and Kuangnan Fang and Shuangge Ma},
  doi          = {10.1016/j.csda.2019.106883},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106883},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Structured analysis of the high-dimensional FMR model},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A least squares-type density estimator using a polynomial
function. <em>CSDA</em>, <em>144</em>, 106882. (<a
href="https://doi.org/10.1016/j.csda.2019.106882">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Higher-order density approximation and estimation methods using orthogonal series expansion have been extensively discussed in statistical literature and its various fields of application. This study proposes least squares-type estimation for series expansion via minimizing the weighted square difference of series distribution expansion and a benchmarking distribution estimator. As the least squares-type estimator has an explicit expression, similar to the classical moment-matching technique, its asymptotic properties are easily obtained under certain regularity conditions . In addition, we resolve the non-negativity issue of the series expansion using quadratic programming . Numerical examples with various simulated and real datasets demonstrate the superiority of the proposed estimator.},
  archive      = {J_CSDA},
  author       = {Jongho Im and Kosuke Morikawa and Hyung-Tae Ha},
  doi          = {10.1016/j.csda.2019.106882},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106882},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A least squares-type density estimator using a polynomial function},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel bayesian approach for variable selection in linear
regression models. <em>CSDA</em>, <em>144</em>, 106881. (<a
href="https://doi.org/10.1016/j.csda.2019.106881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel Bayesian approach to the problem of variable selection in multiple linear regression models is proposed. In particular, a hierarchical setting which allows for direct specification of a priori beliefs about the number of nonzero regression coefficients as well as a specification of beliefs that given coefficients are nonzero is presented. This is done by introducing a new prior for a random set which holds the indices of the predictors with nonzero regression coefficients. To guarantee numerical stability, a g g -prior with an additional ridge parameter is adopted for the unknown regression coefficients. In order to simulate from the joint posterior distribution an intelligent random walk Metropolis–Hastings algorithm which is able to switch between different models is proposed. For the model transitions a novel proposal, which prefers to add a priori or empirically important predictors to the model and further tries to remove less important ones, is used. Testing the algorithm on real and simulated data illustrates that it performs at least on par and often even better than other well-established methods. Finally, it is proven that under some nominal assumptions, the presented approach is consistent in terms of model selection.},
  archive      = {J_CSDA},
  author       = {Konstantin Posch and Maximilian Arbeiter and Juergen Pilz},
  doi          = {10.1016/j.csda.2019.106881},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106881},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A novel bayesian approach for variable selection in linear regression models},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Structural learning of contemporaneous dependencies in
graphical VAR models. <em>CSDA</em>, <em>144</em>, 106880. (<a
href="https://doi.org/10.1016/j.csda.2019.106880">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An objective Bayes approach based on graphical modeling is proposed to learn the contemporaneous dependencies among multiple time series within the framework of Vector Autoregressive (VAR) models. Assuming that, at any time, the covariance matrix is Markov with respect to the same decomposable graph, it is shown that the likelihood of a graphical VAR can be factorized as an ordinary (decomposable) graphical model . Additionally, using a fractional Bayes factor approach, the marginal likelihood is obtained in closed form, and an MCMC algorithm for Bayesian graphical model determination with limited computational burden is presented. The method is validated through a simulation study and applied to a real data set concerning active users of the Earthquake Network application for smartphones.},
  archive      = {J_CSDA},
  author       = {Lucia Paci and Guido Consonni},
  doi          = {10.1016/j.csda.2019.106880},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106880},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Structural learning of contemporaneous dependencies in graphical VAR models},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multivariate normal regression model for survival data
subject to different types of dependent censoring. <em>CSDA</em>,
<em>144</em>, 106879. (<a
href="https://doi.org/10.1016/j.csda.2019.106879">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In survival analysis observations are often right censored and this complicates considerably the analysis of these data. Right censoring can have several underlying causes: administrative censoring, loss to follow up, competing risks, etc. The (latent) censoring times corresponding to the latter two types of censoring are possibly related to the survival time of interest, and in that case this should be taken into account in the model. A unifying model is presented that allows these censoring mechanisms in one single model, and that is also able to incorporate the effect of covariates on these times. Each time variable is modeled by means of a transformed linear model, with the particularity that the error terms of the transformed times follow a multivariate normal distribution allowing for non-zero correlations. It is shown that the model is identified and the model parameters are estimated through a maximum likelihood approach. The performance of the proposed method is compared with methods that assume independent censoring using finite sample simulations. The results show that the proposed method exhibits major advantages in terms of reducing the bias of the parameter estimates. However, a strong deviation from normality and/or a strong violation of the homogeneous variance assumption may lead to biased estimates. Finally, the model and the estimation method are illustrated using the analysis of data coming from a prostate cancer clinical trial.},
  archive      = {J_CSDA},
  author       = {Negera Wakgari Deresa and Ingrid Van Keilegom},
  doi          = {10.1016/j.csda.2019.106879},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106879},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A multivariate normal regression model for survival data subject to different types of dependent censoring},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Some goodness-of-fit tests for the poisson distribution with
applications in biodosimetry. <em>CSDA</em>, <em>144</em>, 106878. (<a
href="https://doi.org/10.1016/j.csda.2019.106878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {New characterizations of the Poisson distribution based on an identity involving the Binomial thinning operator are presented. These characterizations allow the construction of statistics for testing the Poisson distribution against alternatives belonging to a large family called the LC-class, and against general alternatives. The usefulness and the power of the tests are illustrated with several examples of applications in Biodosimetry.},
  archive      = {J_CSDA},
  author       = {Pedro Puig and Christian H. Weiß},
  doi          = {10.1016/j.csda.2019.106878},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106878},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Some goodness-of-fit tests for the poisson distribution with applications in biodosimetry},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Empirical likelihood for partially linear single-index
models with missing observations. <em>CSDA</em>, <em>144</em>, 106877.
(<a href="https://doi.org/10.1016/j.csda.2019.106877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the empirical likelihood for a partially linear single-index model with a subset of covariates and response missing at random . By using the bias-correction and the imputation method , two empirical log-likelihood ratios are proposed such that any of two ratios is asymptotically chi-squared. Two maximum empirical likelihood estimates of the index coefficients and the estimator of link function are constructed, their asymptotic distributions and optimal convergence rate are obtained. It is proved that our methods yield asymptotically equivalent estimators for the index coefficients. An important feature of our methods is their ability to handle missing response and/or partially missing covariates . In addition, we study the estimation and empirical likelihood for two special cases—the single-index model and partially linear model with observations are missing at random . A simulation study indicates that the proposed methods are comparable for bias and standard deviation, as well as in terms of coverage probabilities and average areas (lengths) of confidence regions (intervals). The proposed methods are illustrated by an example of real data.},
  archive      = {J_CSDA},
  author       = {Liugen Xue and Jinghua Zhang},
  doi          = {10.1016/j.csda.2019.106877},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106877},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Empirical likelihood for partially linear single-index models with missing observations},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian bridge-randomized penalized quantile regression.
<em>CSDA</em>, <em>144</em>, 106876. (<a
href="https://doi.org/10.1016/j.csda.2019.106876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantile regression (QR) is an ideal alternative for depicting the conditional quantile functions of a response variable when the conditions of linear regression are unavailable. One advantage of QR in relation to the traditional mean regression is that the QR estimates are more robust against outliers and a large class of error distributions. Regularization methods have been verified to be effective in QR literature for simultaneously conducting parameter estimation and variable selection. This study considers a bridge-randomized penalty of regression coefficients by incorporating uncertainty penalty into Bayesian bridge QR. The asymmetric Laplace distribution (ALD) and the generalized Gaussian distribution (GGD) priors are imposed on model errors and regression coefficients , respectively, to establish a Bayesian bridge-randomized QR model. In addition, bridge penalty exponent is deemed as a parameter, and a Beta-distributed prior is forced on. By utilizing the normal-exponential and uniform-Gamma mixture representations of the ALD and the GGD, a Bayesian hierarchical model is constructed to conduct the fully Bayesian posterior inference. Gibbs sampler and Metropolis–Hastings algorithms are utilized to draw Markov chain Monte Carlo samples from the full conditional posterior distributions of all unknown parameters. Finally, the proposed procedures are illustrated by simulation studies and applied to a real-data analysis.},
  archive      = {J_CSDA},
  author       = {Yuzhu Tian and Xinyuan Song},
  doi          = {10.1016/j.csda.2019.106876},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106876},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayesian bridge-randomized penalized quantile regression},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bootstrapping kernel intensity estimation for inhomogeneous
point processes with spatial covariates. <em>CSDA</em>, <em>144</em>,
106875. (<a href="https://doi.org/10.1016/j.csda.2019.106875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The bias–variance trade-off for inhomogeneous point processes with covariates is theoretically and empirically addressed. A consistent kernel estimator for the first-order intensity function based on covariates is constructed, which uses a convenient relationship between the intensity and the density of events location. The asymptotic bias and variance of the estimator are derived and hence the expression of its infeasible optimal bandwidth. Three data-driven bandwidth selectors are proposed to estimate the optimal bandwidth. One of them is based on a new smooth bootstrap proposal which is proved to be consistent under a Poisson assumption. The other two are a rule-of-thumb method based on assuming normality, and a simple non-model-based approach. An extensive simulation study is accomplished considering Poisson and non-Poisson scenarios, and including a comparison with other competitors. The practicality of the new proposals is shown through an application to real data about wildfires in Canada, using meteorological covariates.},
  archive      = {J_CSDA},
  author       = {M.I. Borrajo and W. González-Manteiga and M.D. Martínez-Miranda},
  doi          = {10.1016/j.csda.2019.106875},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106875},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bootstrapping kernel intensity estimation for inhomogeneous point processes with spatial covariates},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semiparametric estimation for the non-mixture cure model in
case-cohort and nested case-control studies. <em>CSDA</em>,
<em>144</em>, 106874. (<a
href="https://doi.org/10.1016/j.csda.2019.106874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Case-cohort and nested case-control designs are widely used strategies to reduce costs of covariate measurements in epidemiological cohort studies. A unified likelihood framework for two cohort designs is constructed and two statistical procedures are presented for making inference about the effects of incomplete covariates on the cumulative incidence of clinical event time. A pseudo-maximum likelihood estimation based on the sieve method is developed for the semiparametric non-mixture cure model, which can handle missing covariates and a cure fraction occurring in censored survival data. The resulting estimators are shown to be consistent and asymptotically normal in both case-cohort and nested case-control studies. In addition, for two cohort designs, an expectation–maximization (EM) algorithm is developed to simplify the maximization of the likelihood function with the Bernstein-based smoothing technique. Such a procedure would allow one to estimate the nonparametric component of the semiparametric model in closed form and relieve the computational burden. Simulation studies demonstrate that the proposed estimators have good properties in practical situations, and a motivating application to real data is provided to illustrate the methodology.},
  archive      = {J_CSDA},
  author       = {Bo Han and Xiaoguang Wang},
  doi          = {10.1016/j.csda.2019.106874},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106874},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Semiparametric estimation for the non-mixture cure model in case-cohort and nested case-control studies},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Single-index modal regression via outer product gradients.
<em>CSDA</em>, <em>144</em>, 106867. (<a
href="https://doi.org/10.1016/j.csda.2019.106867">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing methods for single-index models (SIM) were focused on either mean regression or quantile regression, while the former is sensitive to outliers or heavy tailed distributions and the latter may lose efficiency for normally distributed data. Then a robust, efficient and easily implemented estimation procedure for index coefficient in SIM is developed by integrating the ideas of local modal regression and outer product gradients. Under some mild conditions, we establish the asymptotic normality of the proposed estimators. We further discuss the optimal choices of tuning parameters including one common bandwidth for nonparametric polynomial smoothing and another key bandwidth that controls the robustness and efficiency of the estimator, based on the derived theories. A practical modified EM algorithm is also presented for implementation. Finally, some simulation studies and two real data analysis are conducted to confirm the merits and theoretical findings of the novel method.},
  archive      = {J_CSDA},
  author       = {Jing Yang and Guoliang Tian and Fang Lu and Xuewen Lu},
  doi          = {10.1016/j.csda.2019.106867},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106867},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Single-index modal regression via outer product gradients},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model-based co-clustering for mixed type data.
<em>CSDA</em>, <em>144</em>, 106866. (<a
href="https://doi.org/10.1016/j.csda.2019.106866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The importance of clustering for creating groups of observations is well known. The emergence of high-dimensional data sets with a huge number of features leads to co-clustering techniques, and several methods have been developed for simultaneously producing groups of observations and features. By grouping the data set into blocks (the crossing of a row-cluster and a column-cluster), these techniques can sometimes better summarize the data set and its inherent structure. The Latent Block Model (LBM) is a well-known method for performing co-clustering. However, recently, contexts with features of different types (here called mixed type data sets) are becoming more common. The LBM is not directly applicable to this kind of data set. Here a natural extension of the usual LBM to the “Multiple Latent Block Model” (MLBM) is proposed in order to handle mixed type data sets. Inference is performed using a Stochastic EM-algorithm that embeds a Gibbs sampler , and allows for missing data situations. A model selection criterion is defined to choose the number of row and column clusters. The method is then applied to both simulated and real data sets .},
  archive      = {J_CSDA},
  author       = {Margot Selosse and Julien Jacques and Christophe Biernacki},
  doi          = {10.1016/j.csda.2019.106866},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106866},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Model-based co-clustering for mixed type data},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A two-piece normal measurement error model. <em>CSDA</em>,
<em>144</em>, 106863. (<a
href="https://doi.org/10.1016/j.csda.2019.106863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the context of measurement error models , the true unobservable covariates are commonly assumed to have a normal distribution . This assumption is replaced here by a more flexible two-piece normal distribution , which allows for asymmetry. After setting-up a general formulation for two-piece distributions, we focus on the case of the normal two-piece construction. It turns out that the joint distribution of the actual observations (the multivariate observed covariates and the response) is a two-component mixture of multivariate skew-normal distributions. This connection facilitates the construction of an EM-type algorithm for performing maximum likelihood estimation. Some numerical experimentation with two real datasets indicates a substantial improvement of the present formulation with respect to the classical normal-theory construction, which greatly compensates the introduction of a single parameter for regulation of skewness.},
  archive      = {J_CSDA},
  author       = {Reinaldo B. Arellano-Valle and Adelchi Azzalini and Clécio S. Ferreira and Karol Santoro},
  doi          = {10.1016/j.csda.2019.106863},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106863},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A two-piece normal measurement error model},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Conditional maximum likelihood estimation for semiparametric
transformation models with doubly truncated data. <em>CSDA</em>,
<em>144</em>, 106862. (<a
href="https://doi.org/10.1016/j.csda.2019.106862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Doubly truncated data arise when a failure time T T is observed only if it falls within a subject-specific, possibly random, interval [ U , V ] [U,V] , where U U and V V are referred to as left- and right-truncation times, respectively. In this article, we consider the problem of fitting semiparametric transformation regression models to doubly truncated data . Most of the existing approaches in literature, which adjust for double truncation in regression models, require independence between failure times and truncation times, which may not hold in practice. To relax the independence assumption to conditional independence given covariates , we consider a conditional likelihood approach and develop the conditional maximum likelihood estimators (cMLE) for the regression parameters and cumulative hazard function of models. Based on score equations for the regression parameter and the infinite-dimensional function, we propose an iterative algorithm for obtaining the cMLE. The cMLE is shown to be consistent and asymptotically normal. Simulation studies indicate that the cMLE performs well and outperforms the existing estimators when an independence assumption holds. Applications to an AIDS dataset is given to illustrate the proposed method.},
  archive      = {J_CSDA},
  author       = {Pao-sheng Shen and Huichen Hsu},
  doi          = {10.1016/j.csda.2019.106862},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106862},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Conditional maximum likelihood estimation for semiparametric transformation models with doubly truncated data},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian analysis of spatial generalized linear mixed models
with laplace moving average random fields. <em>CSDA</em>, <em>144</em>,
106861. (<a href="https://doi.org/10.1016/j.csda.2019.106861">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian random field (GRF) models are widely used in spatial statistics to capture spatially correlated error . Gaussian processes can easily be replaced by the less commonly used Laplace moving averages (LMAs) in spatial generalized linear mixed models (SGLMMs). LMAs are shown to offer improved predictive power when the data exhibits localized spikes in the response. Further, SGLMMs with LMAs are shown to maintain analogous parameter inference and similar computing to Gaussian SGLMMs. A novel discrete space LMA model for irregular lattices is proposed, along with conjugate samplers for LMAs with georeferenced and areal support. A Bayesian analysis of SGLMMs with LMAs and GRFs is conducted over multiple data support and response types.},
  archive      = {J_CSDA},
  author       = {Adam Walder and Ephraim M. Hanks},
  doi          = {10.1016/j.csda.2019.106861},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106861},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayesian analysis of spatial generalized linear mixed models with laplace moving average random fields},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Approximate filtering of conditional intensity process for
poisson count data: Application to urban crime. <em>CSDA</em>,
<em>144</em>, 106850. (<a
href="https://doi.org/10.1016/j.csda.2019.106850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The primary focus is a sequential data assimilation method for count data modelled by an inhomogeneous Poisson process . In particular, a quadratic approximation technique similar to the extended Kalman filter is applied to develop a sub-optimal, discrete-time, filtering algorithm , called the extended Poisson–Kalman filter (ExPKF), where only the mean and covariance are sequentially updated using count data via the Poisson likelihood function. The performance of ExPKF is investigated in several synthetic experiments where the true solution is known. In numerical examples, ExPKF provides a good estimate of the “true” posterior mean , which can be well-approximated by the particle filter (PF) algorithm in the very large sample size limit. In addition, the experiments demonstrate that the ExPKF algorithm can be conveniently used to track parameter changes; on the other hand, a non-filtering framework such as a maximum likelihood estimation (MLE) would require a statistical test for change points or implement time-varying parameters. Finally, to demonstrate the model on real-world data, the ExPKF is used to approximate the uncertainty of urban crime intensity and parameters for self-exciting crime models. The Chicago Police Department’s CLEAR (Citizen Law Enforcement Analysis and Reporting) system data is used as a case study for both univariate and multivariate Hawkes models. An improved goodness of fit measured by the Kolmogorov–Smirnov (KS) statistics is achieved by the filtered intensity. The potential of using filtered intensity to improve police patrolling prioritisation is also tested. By comparing with the prioritisation based on MLE-derived intensity and historical frequency, the result suggests an insignificant difference between them. While the filter is developed and tested in the context of urban crime, it has the potential to make a contribution to data assimilation in other application areas.},
  archive      = {J_CSDA},
  author       = {Naratip Santitissadeekorn and David J.B. Lloyd and Martin B. Short and Sylvain Delahaies},
  doi          = {10.1016/j.csda.2019.106850},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106850},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Approximate filtering of conditional intensity process for poisson count data: Application to urban crime},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ensemble quantile classifier. <em>CSDA</em>, <em>144</em>,
106849. (<a href="https://doi.org/10.1016/j.csda.2019.106849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Both the median-based classifier and the quantile-based classifier are useful for discriminating high-dimensional data with heavy-tailed or skewed inputs. But these methods are restricted as they assign equal weight to each variable in an unregularized way. The ensemble quantile classifier is a more flexible regularized classifier that provides better performance with high-dimensional data, asymmetric data or when there are many irrelevant extraneous inputs. The improved performance is demonstrated by a simulation study as well as an application to text categorization. It is proven that the estimated parameters of the ensemble quantile classifier consistently estimate the minimal population loss under suitable general model assumptions. It is also shown that the ensemble quantile classifier is Bayes optimal under suitable assumptions with asymmetric Laplace distribution inputs.},
  archive      = {J_CSDA},
  author       = {Yuanhao Lai and Ian McLeod},
  doi          = {10.1016/j.csda.2019.106849},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106849},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Ensemble quantile classifier},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Regression analysis of interval-censored failure time data
with time-dependent covariates. <em>CSDA</em>, <em>144</em>, 106848. (<a
href="https://doi.org/10.1016/j.csda.2019.106848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interval-censored failure time data often occur in many areas and their analysis has recently attracted a great deal of attention. On the other hand, most of the existing literature for them can only deal with time-independent covariates . Sometimes one may face time dependent covariates and furthermore the covariates could also suffer measurement errors. For the situation, one approach is to conduct a joint analysis for which many methods have been developed in the literature under various framework. One drawback of these methods is that they usually assume that there are no more measurements on the covariates after the failure time and it is apparent that this may not be true. In this paper, a new joint analysis approach is proposed that can take into account the extra observations. In particular, for estimation, a MCEM algorithm is developed that is much more stable and converges much faster than the existing algorithms. To assess the finite sample performance of the proposed method, an extensive simulation study is conducted and suggests that it works well for practical situations. Also the method is applied to an AIDS study that motivated this investigation.},
  archive      = {J_CSDA},
  author       = {Fengting Yi and Niansheng Tang and Jianguo Sun},
  doi          = {10.1016/j.csda.2019.106848},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106848},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Regression analysis of interval-censored failure time data with time-dependent covariates},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian inference of a directional brain network model for
intracranial EEG data. <em>CSDA</em>, <em>144</em>, 106847. (<a
href="https://doi.org/10.1016/j.csda.2019.106847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human brain is a network system in which brain regions, as network nodes, constantly interact with each other. The directional effect exerted by one brain component on another is referred to as directional connectivity. Since the brain is also a continuous time dynamic system, it is natural to use ordinary differential equations (ODEs) to model directional connections among brain regions. The authors propose a high-dimensional ODE model to explore directional connectivity among many small brain regions recorded by intracranial EEG (iEEG). The new ODE model, motivated by the physical mechanism of the damped harmonic oscillator , is effective for approximating neural oscillation, a rhythmic or repetitive neural activity involved in many important brain functions. To produce scientifically meaningful network results, a cluster structure is assumed for the ODE model parameters that quantify directional connectivity among regions. The cluster structure is in line with the functional specialization of the human brain; the brain areas specialized in the same function tend to be in the same cluster. Two Bayesian methods are developed to estimate the model parameters of the proposed ODE model and to identify clusters of strongly connected brain regions. The proposed ODE model and Bayesian method are applied to iEEG data collected from a patient with medically intractable epilepsy and used to examine the patient’s brain networks before the seizure onset.},
  archive      = {J_CSDA},
  author       = {Tingting Zhang and Yinge Sun and Huazhang Li and Guofen Yan and Seiji Tanabe and Ruizhong Miao and Yaotian Wang and Brian S. Caffo and Mark S. Quigg},
  doi          = {10.1016/j.csda.2019.106847},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106847},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayesian inference of a directional brain network model for intracranial EEG data},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Motor unit number estimation via sequential monte carlo.
<em>CSDA</em>, <em>144</em>, 106845. (<a
href="https://doi.org/10.1016/j.csda.2019.106845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A change in the number of motor units that operate a particular muscle is an important indicator for the progress of a neuromuscular disease and the efficacy of a therapy. Inference for realistic statistical models of the typical data produced when testing muscle function is difficult, and estimating the number of motor units is an ongoing statistical challenge. We consider a set of models for the data, each with a different number of working motor units, and present a novel method for Bayesian inference based on sequential Monte Carlo . This provides estimates of the marginal likelihood and, hence, a posterior probability for each model. Implementing this approach in practice requires a sequential Monte Carlo method that has excellent computational and Monte Carlo properties. We achieve this by benefiting from the model’s conditional independence structure, where, given knowledge of which motor units fired as a result of a particular stimulus, parameters that specify the size of each unit’s response are independent of the parameters defining the probability that a unit will respond at all. The scalability of our methodology relies on the natural conjugacy structure that we create for the former and an enforced, approximate, conjugate structure for the latter. A simulation study demonstrates the accuracy of our method, and inferences are consistent across two different datasets arising from the same rat tibial muscle.},
  archive      = {J_CSDA},
  author       = {Simon A.C. Taylor and Chris Sherlock and Gareth Ridall and Paul Fearnhead},
  doi          = {10.1016/j.csda.2019.106845},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106845},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Motor unit number estimation via sequential monte carlo},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A comparison of general-purpose optimization algorithms for
finding optimal approximate experimental designs. <em>CSDA</em>,
<em>144</em>, 106844. (<a
href="https://doi.org/10.1016/j.csda.2019.106844">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several common general purpose optimization algorithms are compared for finding A A - and D D -optimal designs for different types of statistical models of varying complexity, including high dimensional models with five and more factors. The algorithms of interest include exact methods, such as the interior point method , the Nelder–Mead method, the active set method, the sequential quadratic programming , and metaheuristic algorithms , such as particle swarm optimization , simulated annealing and genetic algorithms . Several simulations are performed, which provide general recommendations on the utility and performance of each method, including hybridized versions of metaheuristic algorithms for finding optimal experimental designs . A key result is that general-purpose optimization algorithms , both exact methods and metaheuristic algorithms, perform well for finding optimal approximate experimental designs .},
  archive      = {J_CSDA},
  author       = {Ricardo García-Ródenas and José Carlos García-García and Jesús López-Fidalgo and José Ángel Martín-Baos and Weng Kee Wong},
  doi          = {10.1016/j.csda.2019.106844},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106844},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A comparison of general-purpose optimization algorithms for finding optimal approximate experimental designs},
  volume       = {144},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian modeling and computation for analyte quantification
in complex mixtures using raman spectroscopy. <em>CSDA</em>,
<em>143</em>, 106846. (<a
href="https://doi.org/10.1016/j.csda.2019.106846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A two-stage algorithm based on Bayesian modeling and computation for quantifying analyte concentration in complex mixtures with Raman spectroscopy is proposed. A hierarchical Bayesian model is constructed for spectral signal analysis, and reversible-jump Markov chain Monte Carlo (RJMCMC) computation is carried out for model selection and spectral variable estimation. Processing is performed in two stages. In the first stage, the peak representation for a target analyte spectrum is learned. In the second, the peak variables learned from the first stage are used to estimate the concentration of the target analyte in a mixture. Numerical experiments validated the performance over a wide range of simulation conditions and established the algorithm accuracy over conventional multivariate regression algorithms for analyte quantification (when constrained to a small training sample size). In addition, the algorithm was applied to analyze experimental spontaneous Raman spectroscopy data collected for glucose concentration estimation in a biopharmaceutical process monitoring application. The results show that this algorithm can be a promising complementary tool alongside conventional multivariate regression algorithms in Raman spectroscopy-based mixture quantification studies, especially when collection of a large training dataset is challenging or resource-intensive.},
  archive      = {J_CSDA},
  author       = {Ningren Han and Rajeev J. Ram},
  doi          = {10.1016/j.csda.2019.106846},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106846},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayesian modeling and computation for analyte quantification in complex mixtures using raman spectroscopy},
  volume       = {143},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Determining the number of effective parameters in kernel
density estimation. <em>CSDA</em>, <em>143</em>, 106843. (<a
href="https://doi.org/10.1016/j.csda.2019.106843">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The hat matrix maps the vector of response values in a regression to its predicted counterpart. The trace of this hat matrix is the workhorse for calculating the effective number of parameters in both parametric and nonparametric regression settings. Drawing on the regression literature, the standard kernel density estimate is transformed to mimic a regression estimate thus allowing extraction of a usable hat matrix for calculating the effective number of parameters of the kernel density estimate. Asymptotic expressions for the trace of this hat matrix are derived under standard regularity conditions for mixed, continuous, and discrete densities. Simulations validate the theoretical contributions. Several empirical examples demonstrate the usefulness of the method suggesting that calculating the effective number of parameters of a kernel density estimator maybe useful in interpreting differences across estimators.},
  archive      = {J_CSDA},
  author       = {Nadine McCloud and Christopher F. Parmeter},
  doi          = {10.1016/j.csda.2019.106843},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106843},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Determining the number of effective parameters in kernel density estimation},
  volume       = {143},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bayesian mixture model for clustering circular data.
<em>CSDA</em>, <em>143</em>, 106842. (<a
href="https://doi.org/10.1016/j.csda.2019.106842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering complex circular phenomena is a common problem in different scientific disciplines. Examples include the clustering of directions of animal movement in the wild to identify migration patterns, and the classification of angular positions of meteorological events to investigate seasonality fluctuations. The main goal is to develop a novel methodology for clustering and classification of circular data, under a Bayesian mixture modeling framework. The mixture model is defined assuming that the number of components is finite, but unknown, and that each component follows a projected normal distribution . Model selection is performed by jointly making inferences about the parameters of the mixture model and the number of components, choosing the model with the highest posterior probability . A deterministic relabeling strategy is used to recover identifiability for the components in the chosen model. Estimates of both the posterior classification probabilities and the scaled densities are approximated via the relabeled MCMC output. The proposed methods are illustrated using both simulated and real datasets, and performance comparisons with existing strategies are also given. The results suggest that the new approach is an appealing alternative for the clustering and classification of circular data.},
  archive      = {J_CSDA},
  author       = {Carlos E. Rodríguez and Gabriel Núñez-Antonio and Gabriel Escarela},
  doi          = {10.1016/j.csda.2019.106842},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106842},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A bayesian mixture model for clustering circular data},
  volume       = {143},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data-cloning SMC2: A global optimizer for maximum likelihood
estimation of latent variable models. <em>CSDA</em>, <em>143</em>,
106841. (<a href="https://doi.org/10.1016/j.csda.2019.106841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A data-cloning SMC 2 algorithm is proposed as a general-purpose, global optimization routine for the maximum likelihood estimation of models with latent variables. In the SMC 2 phase, the method first marginalizes out the latent variable(s) by applying one layer of SMC at a fixed parameter value and then searches for the optimal parameters through another layer of SMC. The data-cloning phase is deployed to ensure global convergence by dampening multi-modality and to reduce the Monte Carlo error associated with SMC. This new method has broad applicability and is massively parallelizable through leveraging modern multi-core CPU or GPU computing.},
  archive      = {J_CSDA},
  author       = {Jin-Chuan Duan and Andras Fulop and Yu-Wei Hsieh},
  doi          = {10.1016/j.csda.2019.106841},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106841},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Data-cloning SMC2: A global optimizer for maximum likelihood estimation of latent variable models},
  volume       = {143},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian variable selection in non-homogeneous hidden markov
models through an evolutionary monte carlo method. <em>CSDA</em>,
<em>143</em>, 106840. (<a
href="https://doi.org/10.1016/j.csda.2019.106840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hidden Markov models (HMMs) are dynamic mixture models applied to time series in order to classify the observations into a small number of homogeneous groups , to understand when change points occur, and to model data heterogeneity through the switching between subseries with different state-dependent parameters. In the most general case, HMMs have an unobserved Markov chain whose transition probabilities are time-varying and dependent on exogenous variables through multinomial logit functions. When many covariates are available it is worthwhile selecting the subsets of variables which might affect most each row of the transition matrices . A Bayesian method for the stochastic selection of subsets of covariates is proposed by developing a novel evolutionary Monte Carlo algorithm . The methodology is illustrated and shown to be effective by performing experiments and comparisons on both synthetic data sets and a real multivariate time series with covariates.},
  archive      = {J_CSDA},
  author       = {Luigi Spezia},
  doi          = {10.1016/j.csda.2019.106840},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106840},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Bayesian variable selection in non-homogeneous hidden markov models through an evolutionary monte carlo method},
  volume       = {143},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Benchmark for filter methods for feature selection in
high-dimensional classification data. <em>CSDA</em>, <em>143</em>,
106839. (<a href="https://doi.org/10.1016/j.csda.2019.106839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection is one of the most fundamental problems in machine learning and has drawn increasing attention due to high-dimensional data sets emerging from different fields like bioinformatics. For feature selection, filter methods play an important role, since they can be combined with any machine learning model and can heavily reduce run time of machine learning algorithms . The aim of the analyses is to review how different filter methods work, to compare their performance with respect to both run time and predictive accuracy , and to provide guidance for applications. Based on 16 high-dimensional classification data sets, 22 filter methods are analyzed with respect to run time and accuracy when combined with a classification method. It is concluded that there is no group of filter methods that always outperforms all other methods, but recommendations on filter methods that perform well on many of the data sets are made. Also, groups of filters that are similar with respect to the order in which they rank the features are found. For the analyses, the R machine learning package mlr is used. It provides a uniform programming API and therefore is a convenient tool to conduct feature selection using filter methods.},
  archive      = {J_CSDA},
  author       = {Andrea Bommert and Xudong Sun and Bernd Bischl and Jörg Rahnenführer and Michel Lang},
  doi          = {10.1016/j.csda.2019.106839},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106839},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Benchmark for filter methods for feature selection in high-dimensional classification data},
  volume       = {143},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The zipf–poisson-stopped-sum distribution with an
application for modeling the degree sequence of social networks.
<em>CSDA</em>, <em>143</em>, 106838. (<a
href="https://doi.org/10.1016/j.csda.2019.106838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Under the Zipf Distribution , the frequency of a value is a power function of its size. Thus, when plotting frequencies versus size in log–log scale of data following that distribution, one obtains a straight line. The Zipf has been assumed to be appropriate for modeling highly skewed data from many different areas. Nevertheless, for many real data sets , the linearity is observed only in the tail; thus, the Zipf is fitted only for values larger than a given threshold and, consequently, there is a loss of information. The Zipf–Poisson-stopped-sum distribution is introduced as a more flexible alternative. It is proven that in log–log scale allows for top-concavity, maintaining the linearity in the tail. Consequently, the distribution fits properly many data sets in their entire range. To prove the suitability of our model 16 network degree sequences describing the interaction between members of a given platform have been fitted. The results have been compared with the fits obtained through other bi-parametric distributions.},
  archive      = {J_CSDA},
  author       = {Ariel Duarte-López and Marta Pérez-Casany and Jordi Valero},
  doi          = {10.1016/j.csda.2019.106838},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106838},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {The Zipf–Poisson-stopped-sum distribution with an application for modeling the degree sequence of social networks},
  volume       = {143},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variations of power-expected-posterior priors in normal
regression models. <em>CSDA</em>, <em>143</em>, 106836. (<a
href="https://doi.org/10.1016/j.csda.2019.106836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The power-expected-posterior (PEP) prior is an objective prior for Gaussian linear models, which leads to consistent model selection inference, under the M-closed scenario, and tends to favour parsimonious models. Recently, two new forms of the PEP prior were proposed which generalize its applicability to a wider range of models. The properties of these two PEP variants within the context of the normal linear model are examined thoroughly, focusing on the prior dispersion and on the consistency of the induced model selection procedure . Results show that both PEP variants have larger variances than the unit-information g -prior and that they are M-closed consistent as the limiting behaviour of the corresponding marginal likelihoods matches that of the BIC. The consistency under the M-open case, using three different model misspecification scenarios is further investigated.},
  archive      = {J_CSDA},
  author       = {Dimitris Fouskakis and Ioannis Ntzoufras and Konstantinos Perrakis},
  doi          = {10.1016/j.csda.2019.106836},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106836},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Variations of power-expected-posterior priors in normal regression models},
  volume       = {143},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Approximate least squares estimation for spatial
autoregressive models with covariates. <em>CSDA</em>, <em>143</em>,
106833. (<a href="https://doi.org/10.1016/j.csda.2019.106833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the rapid development of social network sites , the spatial autoregressive model with covariates has been popularly applied in real practice. However, traditional estimation methods such as the quasi-maximum likelihood estimator are computationally infeasible if the network size n is huge. To circumvent this infeasibility, a novel method named approximate least square estimator (ALSE) is proposed by optimizing an approximate least squares objective function. It can reduce the computational complexity from O ( n 3 ) to O ( n ) . Under certain appropriate conditions, the ALSE is consistent and asymptotically normal. In addition, a novel test statistic is proposed to test the identifiability of the parameters in covariates. Extensive simulation studies and a Sina Weibo dataset are analyzed to assess the finite-sample performance of the ALSE.},
  archive      = {J_CSDA},
  author       = {Yingying Ma and Wei Lan and Fanying Zhou and Hansheng Wang},
  doi          = {10.1016/j.csda.2019.106833},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106833},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Approximate least squares estimation for spatial autoregressive models with covariates},
  volume       = {143},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Introducing article numbering to computational statistics
and data analysis. <em>CSDA</em>, <em>142</em>, 106857. (<a
href="https://doi.org/10.1016/S0167-9473(19)30212-9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_CSDA},
  author       = {Darren Sugrue ( Publisher )},
  doi          = {10.1016/S0167-9473(19)30212-9},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106857},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Introducing article numbering to computational statistics and data analysis},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rothman–woodroofe symmetry test statistic revisited.
<em>CSDA</em>, <em>142</em>, 106837. (<a
href="https://doi.org/10.1016/j.csda.2019.106837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Rothman–Woodroofe symmetry test statistic is revisited on the basis of independent but not necessarily identically distributed random variables . The distribution-freeness if the underlying distributions are all symmetric and continuous is obtained. The results are applied for testing symmetry in a meta-analysis random effects model. The consistency of the procedure is discussed in this situation as well. A comparison with an alternative proposal from the literature is conducted via simulations. Real data are analyzed to demonstrate how the new approach works in practice.},
  archive      = {J_CSDA},
  author       = {Daniel Gaigall},
  doi          = {10.1016/j.csda.2019.106837},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106837},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Rothman–Woodroofe symmetry test statistic revisited},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sparse principal component based high-dimensional mediation
analysis. <em>CSDA</em>, <em>142</em>, 106835. (<a
href="https://doi.org/10.1016/j.csda.2019.106835">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal mediation analysis aims to quantify the intermediate effect of a mediator on the causal pathway from treatment to outcome. When dealing with multiple mediators, which are potentially causally dependent, the possible decomposition of pathway effects grows exponentially with the number of mediators. An existing approach incorporated the principal component analysis (PCA) to address this challenge based on the fact that the transformed mediators are conditionally independent given the orthogonality of the principal components (PCs). However, the transformed mediator PCs, which are linear combinations of original mediators, can be difficult to interpret. A sparse high-dimensional mediation analysis approach is proposed which adopts the sparse PCA method to the mediation setting. The proposed approach is applied to a task-based functional magnetic resonance imaging study, illustrating its ability to detect biologically meaningful results related to an identified mediator.},
  archive      = {J_CSDA},
  author       = {Yi Zhao and Martin A. Lindquist and Brian S. Caffo},
  doi          = {10.1016/j.csda.2019.106835},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106835},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Sparse principal component based high-dimensional mediation analysis},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generating random correlation matrices with fixed values: An
application to the evaluation of multivariate surrogate endpoints.
<em>CSDA</em>, <em>142</em>, 106834. (<a
href="https://doi.org/10.1016/j.csda.2019.106834">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When assessing surrogate endpoints in clinical studies under a causal-inference framework, a simulation-based sensitivity analysis is required, so as to sample the unidentifiable parameters across plausible values. To be precise, correlation matrices need to be sampled with only some of their entries identified from the data, known as the matrix completion problem. The positive-definiteness constraints are cumbersome functions involving all matrix entries, making this a challenging task. Some existing algorithms rely on sampling and then rejecting invalid solutions. A very efficient algorithm is built on previous work to generate large correlation matrices with some a prior fixed elements. The proposed methodology is applied to tackle a difficult problem in the surrogate marker field, namely, the evaluation of multivariate, potentially high-dimensional, surrogate endpoints. Whereas existing methods are limited to very low-dimensional surrogates, the new proposal is stable, fast, shows good properties, and is implemented in a user-friendly and freely available R package.},
  archive      = {J_CSDA},
  author       = {Alvaro Jóse Flórez and Ariel Alonso Abad and Geert Molenberghs and Wim Van Der Elst},
  doi          = {10.1016/j.csda.2019.106834},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106834},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Generating random correlation matrices with fixed values: An application to the evaluation of multivariate surrogate endpoints},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust test for dispersion parameter change in discretely
observed diffusion processes. <em>CSDA</em>, <em>142</em>, 106832. (<a
href="https://doi.org/10.1016/j.csda.2019.106832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the problem of testing for dispersion parameter change in discretely observed diffusion processes when the observations are contaminated by outliers. To lessen the impact of outliers, we first calculate residuals using a robust estimate and then propose a trimmed-residual based CUSUM test. The proposed test is shown to converge weakly to a function of the Brownian bridge under the null hypothesis of no parameter change. We conduct simulations to evaluate performances of the proposed test in the presence of outliers. Numerical results confirm that the proposed test possesses a strong robust property against outliers. In real data analysis, we fit the Ornstein–Uhlenbeck process to KOSPI200 volatility index data and locate some change points that are not detected by a naive CUSUM test.},
  archive      = {J_CSDA},
  author       = {Junmo Song},
  doi          = {10.1016/j.csda.2019.106832},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106832},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Robust test for dispersion parameter change in discretely observed diffusion processes},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A nonparametric feature screening method for
ultrahigh-dimensional missing response. <em>CSDA</em>, <em>142</em>,
106828. (<a href="https://doi.org/10.1016/j.csda.2019.106828">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the feature screening issue for ultrahigh-dimensional data with responses missing at random . A novel nonparametric feature screening procedure is developed to identify the important features via the conditionally imputing marginal Spearman rank correlation . The proposed nonparametric screening approach has several desirable merits. First, it is nonparametric without assuming any regression form of predictors on response variable. Second, it is robust to outliers and heavy-tailed data. Third, under some regularity conditions , it is shown that the proposed feature screening procedure has the sure screening and ranking consistency properties. Simulation studies evidence that the proposed screening procedure outperforms several existing model-free screening procedures. An example taken from the microarray diffuse large-B-cell lymphoma study is used to illustrate the proposed methodologies.},
  archive      = {J_CSDA},
  author       = {Xiaoxia Li and Niansheng Tang and Jinhan Xie and Xiaodong Yan},
  doi          = {10.1016/j.csda.2019.106828},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106828},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A nonparametric feature screening method for ultrahigh-dimensional missing response},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust wald-type methods for testing equality between two
populations regression parameters: A comparative study under the
logistic model. <em>CSDA</em>, <em>142</em>, 106827. (<a
href="https://doi.org/10.1016/j.csda.2019.106827">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Comparing the regression parameters between two populations is useful to understand the homogeneity of the process underlying the data. The problem of logistic regression is considered when the practitioner handles data from two populations and when the goal is to test the hypothesis that some regression parameters are equal in both populations. A classical testing procedure is to construct a Wald-type test from the maximum likelihood estimators obtained from each data set. However, as in the one-population setting, the presence of outliers in any of the two samples may distort both the level and/or the power of this procedure. Instead of the maximum likelihood procedure, reliable statistics are built using a class of robust estimators which bound large values of the deviance as well as the effect of high leverage points . The asymptotic behaviour of this family of test statistics is derived under the null and contiguous alternatives. Besides, the robustness of the tests is investigated through the influence function. A simulation study allows to compare, under different contamination schemes, the behaviour of the tests based on the maximum likelihood estimators and on their robust counterparts. The numerical study shows that the Wald tests based on the maximum likelihood estimators or on the unweighted robust ones break down when atypical data arise in the samples, while both the level and power of the Wald-type tests based on redescending weighted M − M− estimators are stable against the considered contaminations. The analysis of a real data set enables to investigate the p − p− value sensitivity to the presence of outliers.},
  archive      = {J_CSDA},
  author       = {Ana M. Bianco and Graciela Boente and Isabel M. Rodrigues},
  doi          = {10.1016/j.csda.2019.106827},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106827},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Robust wald-type methods for testing equality between two populations regression parameters: A comparative study under the logistic model},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semiparametric model of mean residual life with biased
sampling data. <em>CSDA</em>, <em>142</em>, 106826. (<a
href="https://doi.org/10.1016/j.csda.2019.106826">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The mean residual life function is an important and attractive alternative to the survival function or the hazard function of survival time in practice. It describes the remaining life expectancy of a subject surviving up to time t. To study the relationship between the mean residual life and its associated covariates , a broad class of mean residual life models under general biased sampling data is proposed in this paper, thereby extending the reach of this flexible and powerful tool. The unknown parameters are estimated by using inverse probability weighting method. An easily used model diagnostic method is also presented to assess the adequacy of the model. Both asymptotic properties and finite sample performances of the proposed estimators are established. Finally the practical appeal of the estimator is shown via two real applications using the Channing House data and the Canadian Study of Health and Aging (CSHA) dementia data.},
  archive      = {J_CSDA},
  author       = {Huijuan Ma and Wei Zhao and Yong Zhou},
  doi          = {10.1016/j.csda.2019.106826},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106826},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Semiparametric model of mean residual life with biased sampling data},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multilayer exponential random graph modelling approach for
weighted networks. <em>CSDA</em>, <em>142</em>, 106825. (<a
href="https://doi.org/10.1016/j.csda.2019.106825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new modelling approach for the analysis of weighted networks with ordinal/ polytomous dyadic values is introduced. Specifically, it is proposed to model the weighted network connectivity structure using a hierarchical multilayer exponential random graph model (ERGM) generative process where each network layer represents a different ordinal dyadic category. The network layers are assumed to be generated by an ERGM process conditional on their closest lower network layers. A crucial advantage of the proposed method is the possibility of adopting the binary network statistics specification to describe both the between-layer and across-layer network processes and thus facilitating the interpretation of the parameter estimates associated to the network effects included in the model. The Bayesian approach provides a natural way to quantify the uncertainty associated to the model parameters. From a computational point of view, an extension of the approximate exchange algorithm is proposed to sample from the doubly-intractable parameter posterior distribution . A simulation study is carried out on artificial data and applications of the methodology are illustrated on well-known datasets. Finally, a goodness-of-fit diagnostic procedure for model assessment is proposed.},
  archive      = {J_CSDA},
  author       = {Alberto Caimo and Isabella Gollini},
  doi          = {10.1016/j.csda.2019.106825},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106825},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A multilayer exponential random graph modelling approach for weighted networks},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feature screening for ultrahigh dimensional categorical data
with covariates missing at random. <em>CSDA</em>, <em>142</em>, 106824.
(<a href="https://doi.org/10.1016/j.csda.2019.106824">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing feature screening methods assume that data are fully observed. It is quite a challenge to develop screening methods for incomplete data since the traditional missing data analysis techniques cannot be directly applied to ultrahigh dimensional case. A two-step model-free feature screening procedure for ultrahigh dimensional categorical data when some covariate values are missing at random is developed. For each covariate with missing data, the first step screens out the variables in the unspecified propensity function. In the second step, screening statistics such as the adjusted Pearson Chi-Square statistics can be calculated by leveraging the variables obtained in the first step and the special structure of categorical data. Sure screening properties are established for the proposed method. Finite sample performance is investigated by simulation studies and a real data example.},
  archive      = {J_CSDA},
  author       = {Lyu Ni and Fang Fang and Jun Shao},
  doi          = {10.1016/j.csda.2019.106824},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106824},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Feature screening for ultrahigh dimensional categorical data with covariates missing at random},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Approximate maximum likelihood estimation of a threshold
diffusion process. <em>CSDA</em>, <em>142</em>, 106823. (<a
href="https://doi.org/10.1016/j.csda.2019.106823">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In order to estimate the parameters of a two-regime threshold diffusion process with discretely sampled data, an approximate maximum likelihood method (AMLE) based on approximating the log-likelihood function of the observations is proposed. Both the drift and the diffusion terms are allowed to be either linear or non-linear. In order to choose the most appropriate among these four possibilities, three information criteria are employed. Further, a likelihood ratio test can help to determine whether threshold effects are present. Via simulations, the finite sample performance of the proposed AMLE is compared to an alternative quasi-likelihood estimator and the finite sample performance of the information criteria as well as the likelihood ratio test are studied. Finally, the efficacy of our approach is demonstrated with two financial time series.},
  archive      = {J_CSDA},
  author       = {Ting-Hung Yu and Henghsiu Tsai and Heiko Rachinger},
  doi          = {10.1016/j.csda.2019.106823},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106823},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Approximate maximum likelihood estimation of a threshold diffusion process},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On parsimonious models for modeling matrix data.
<em>CSDA</em>, <em>142</em>, 106822. (<a
href="https://doi.org/10.1016/j.csda.2019.106822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finite mixture modeling is a popular technique for capturing heterogeneity in data. Although the vast majority of the theory developed in this area up to date deals with vector-valued data, some recent advancements have been made to expand the concept to matrix-valued data, for example, by means of matrix Gaussian mixture models . Unfortunately, matrix mixtures tend to suffer from the overparameterization issue due to a high number of parameters involved in the model. As a result, this may lead to problems such as overfitting and mixture order underestimation. One possible approach of addressing the overparameterization issue that has proven to be effective in the vector-valued framework is to consider various parsimonious models. One of the most popular classes of parsimonious models is based on the spectral decomposition of covariance matrices . An attempt to generalize this class and make it applicable in the matrix setting is made. Estimation procedures are thoroughly discussed for all models considered. The application of the proposed methodology is studied on synthetic and real-life data sets.},
  archive      = {J_CSDA},
  author       = {Shuchismita Sarkar and Xuwen Zhu and Volodymyr Melnykov and Salvatore Ingrassia},
  doi          = {10.1016/j.csda.2019.106822},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106822},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {On parsimonious models for modeling matrix data},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Solution paths for the generalized lasso with applications
to spatially varying coefficients regression. <em>CSDA</em>,
<em>142</em>, 106821. (<a
href="https://doi.org/10.1016/j.csda.2019.106821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Penalized regression can improve prediction accuracy and reduce dimension. The generalized lasso problem is used in many applications in various fields. The generalized lasso penalizes a linear transformation of the coefficients rather than the coefficients themselves. The proposed algorithm solves the generalized lasso problem and provides the full solution path. A confidence set can then be constructed on the generalized lasso parameters based on the modified residual bootstrap lasso. The approach is demonstrated using spatially varying coefficients regression , and it is shown to be both accurate and efficient compared to previous work.},
  archive      = {J_CSDA},
  author       = {Yaqing Zhao and Howard Bondell},
  doi          = {10.1016/j.csda.2019.106821},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106821},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Solution paths for the generalized lasso with applications to spatially varying coefficients regression},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation of the ROC curve from the lehmann family.
<em>CSDA</em>, <em>142</em>, 106820. (<a
href="https://doi.org/10.1016/j.csda.2019.106820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A semiparametric model of the ROC curve based on the Lehmann family of distributions is an alternative to the popular binormal model. A special case of this model is the Bi-Weibull model. New estimators of the unknown model parameter and consequently the ROC curve from the Lehmann family are presented, and their properties are proved. The accuracy of the proposed estimators is compared with the accuracy of a known estimator based on the partial likelihood method. The conclusion that some of the new estimators perform generally better than their competitor is made.},
  archive      = {J_CSDA},
  author       = {Alicja Jokiel-Rokita and Rafał Topolnicki},
  doi          = {10.1016/j.csda.2019.106820},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106820},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Estimation of the ROC curve from the lehmann family},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generalized ℓ1-penalized quantile regression with linear
constraints. <em>CSDA</em>, <em>142</em>, 106819. (<a
href="https://doi.org/10.1016/j.csda.2019.106819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many application areas, prior subject matter knowledge can be formulated as constraints on parameters in order to get a more accurate fit. A generalized ℓ 1 ℓ1 -penalized quantile regression with linear constraints on parameters is considered, including either linear inequality or equality constraints or both. It allows a general form of penalization, including the usual lasso, the fused lasso and the adaptive lasso as special cases. The KKT conditions of the optimization problem are derived and the whole solution path is computed as a function of the tuning parameter. A formula for the number of degrees of freedom is derived, which is used to construct model selection criteria for selecting optimal tuning parameters. Finally, several simulation studies and two real data examples are presented to illustrate the proposed method.},
  archive      = {J_CSDA},
  author       = {Yongxin Liu and Peng Zeng and Lu Lin},
  doi          = {10.1016/j.csda.2019.106819},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106819},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Generalized ℓ1-penalized quantile regression with linear constraints},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quantile based dimension reduction in censored regression.
<em>CSDA</em>, <em>142</em>, 106818. (<a
href="https://doi.org/10.1016/j.csda.2019.106818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers regression models with censored data where the dependent variable T T and the censoring variable C C are both assumed to follow a multi-index structure with the covariates . An iterative and structure-adaptive procedure is proposed to estimate the sufficient dimension reduction (SDR) spaces for T T and C C , as well as their joint SDR space, simultaneously. A cross-validation procedure is used to determine the structural dimensions of the individual SDR spaces. Simulation study shows that in terms of estimation efficiency, the proposed method is comparable to parametric models such as the Cox proportional hazards model when the latter is supposed to benefit from correct model specification, and outperforms the latter otherwise. When applied to the popular primary biliary cirrhosis data, the new approach is able to identify an important predictor for the patients’ survival time, which has long been noted by clinicians as a critical indicator but has so far not been picked up by existing statistical analysis.},
  archive      = {J_CSDA},
  author       = {Mei Yan and Efang Kong and Yingcun Xia},
  doi          = {10.1016/j.csda.2019.106818},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106818},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Quantile based dimension reduction in censored regression},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variational nonparametric discriminant analysis.
<em>CSDA</em>, <em>142</em>, 106817. (<a
href="https://doi.org/10.1016/j.csda.2019.106817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variable selection and classification are common objectives in the analysis of high-dimensional data. Most such methods make distributional assumptions that may not be compatible with the diverse families of distributions data can take. A novel Bayesian nonparametric discriminant analysis model that performs both variable selection and classification within a seamless framework is proposed. Pólya tree priors are assigned to the unknown group-conditional distributions to account for their uncertainty, and allow prior beliefs about the distributions to be incorporated simply as hyperparameters. The adoption of collapsed variational Bayes inference in combination with a chain of functional approximations led to an algorithm with low computational cost. The resultant decision rules carry heuristic interpretations and are related to an existing two-sample Bayesian nonparametric hypothesis test . By an application to some simulated and publicly available real datasets, the proposed method exhibits good performance when compared to current state-of-the-art approaches.},
  archive      = {J_CSDA},
  author       = {Weichang Yu and Lamiae Azizi and John T. Ormerod},
  doi          = {10.1016/j.csda.2019.106817},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106817},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Variational nonparametric discriminant analysis},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Uncertainty quantification using bayesian neural networks in
classification: Application to biomedical image segmentation.
<em>CSDA</em>, <em>142</em>, 106816. (<a
href="https://doi.org/10.1016/j.csda.2019.106816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most recent research of deep neural networks in the field of computer vision has focused on improving performances of point predictions by developing network architectures or learning algorithms. Reliable uncertainty quantification accompanied by point estimation can lead to a more informed decision, and the quality of prediction can be improved. In this paper, we invoke a Bayesian neural network and propose a natural way of quantifying uncertainties in classification problems by decomposing the moment-based predictive uncertainty into two parts: aleatoric and epistemic uncertainty . The proposed method takes into account the discrete nature of the outcome, yielding the correct interpretation of each uncertainty. We demonstrate that the proposed uncertainty quantification method provides additional insights into the point prediction using two Ischemic Stroke Lesion Segmentation Challenge datasets and the Digital Retinal Images for Vessel Extraction dataset.},
  archive      = {J_CSDA},
  author       = {Yongchan Kwon and Joong-Ho Won and Beom Joon Kim and Myunghee Cho Paik},
  doi          = {10.1016/j.csda.2019.106816},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106816},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Uncertainty quantification using bayesian neural networks in classification: Application to biomedical image segmentation},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semiparametric model for covariance regression analysis.
<em>CSDA</em>, <em>142</em>, 106815. (<a
href="https://doi.org/10.1016/j.csda.2019.106815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating covariance matrices is an important research topic in statistics and finance. A semiparametric model for covariance matrix estimation is proposed. Specifically, the covariance matrix is modeled as a polynomial function of the symmetric adjacency matrix with time varying parameters . The asymptotic properties for the time varying coefficient and the associated semiparametric covariance estimators are established. A Bayesian information criterion to select the order of the polynomial function is also investigated. Simulation studies and an empirical example are presented to illustrate the usefulness of the proposed method.},
  archive      = {J_CSDA},
  author       = {Jin Liu and Yingying Ma and Hansheng Wang},
  doi          = {10.1016/j.csda.2019.106815},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106815},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Semiparametric model for covariance regression analysis},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Function-on-function quadratic regression models.
<em>CSDA</em>, <em>142</em>, 106814. (<a
href="https://doi.org/10.1016/j.csda.2019.106814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A quadratic regression model where the covariate and the response are both functional is considered, which is a reasonable extension of common function-on-function linear regression models. Methods to estimate the coefficient functions , predict unknown response and test significance of the quadratic term are developed in functional principal component regression paradigm. Asymptotic theories for these approaches are also established. A simulation study is presented to demonstrate the finite sample performances of the proposed methods and an application to real data is used to illustrate the improvement that can be gained by comparing to the function-on-function linear models.},
  archive      = {J_CSDA},
  author       = {Yifan Sun and Qihua Wang},
  doi          = {10.1016/j.csda.2019.106814},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106814},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Function-on-function quadratic regression models},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Time-dependent poisson reduced rank models for political
text data analysis. <em>CSDA</em>, <em>142</em>, 106813. (<a
href="https://doi.org/10.1016/j.csda.2019.106813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider Poisson reduced rank models where parameters depend on time. Our main motivation comes from studies in comparative politics where one wants to locate party positions in a certain political space. For this purpose, several empirical methods have been proposed using text as data sources. As the data structure of texts is quite complex, its analysis to extract information is generally a difficult task. Furthermore, political texts usually contain a large number of words such that a simultaneous analysis of word counts becomes challenging. In this paper, we consider Poisson models for each word count simultaneously and provide a statistical method suitable to analyze political text data. We consider a novel model which allows the political lexicon to change over time and develop an estimation procedure based on LASSO and fused LASSO penalization techniques to address high-dimensionality via significant dimension reduction. This model gives insights into the potentially changing use of words by left and right-wing parties over time. The procedure allows to identify automatically those words having a discriminating effect between party positions. To address the dependence structure of word counts over time, we propose integer-valued time series processes to implement a suitable bootstrap method for constructing confidence intervals for the model parameters. We apply our approach to party manifesto data from German parties over seven federal elections after German reunification. The approach does not require any a priori information nor expert knowledge to process the data.},
  archive      = {J_CSDA},
  author       = {Carsten Jentsch and Eun Ryung Lee and Enno Mammen},
  doi          = {10.1016/j.csda.2019.106813},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106813},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Time-dependent poisson reduced rank models for political text data analysis},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient ADMM algorithm for high dimensional precision
matrix estimation via penalized quadratic loss. <em>CSDA</em>,
<em>142</em>, 106812. (<a
href="https://doi.org/10.1016/j.csda.2019.106812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The estimation of high dimensional precision matrices has been a central topic in statistical learning. However, as the number of parameters scales quadratically with the dimension p p , many state-of-the-art methods do not scale well to solve problems with a very large p p . In this paper, we propose a very efficient algorithm for precision matrix estimation via penalized quadratic loss functions . Under the high dimension low sample size setting, the computation complexity of our algorithm is linear in both the sample size and the number of parameters. Such a computation complexity is in some sense optimal, as it is the same as the complexity needed for computing the sample covariance matrix . Numerical studies show that our algorithm is much more efficient than other state-of-the-art methods when the dimension p p is very large.},
  archive      = {J_CSDA},
  author       = {Cheng Wang and Binyan Jiang},
  doi          = {10.1016/j.csda.2019.106812},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106812},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {An efficient ADMM algorithm for high dimensional precision matrix estimation via penalized quadratic loss},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Common sampling orders of regular vines with application to
model selection. <em>CSDA</em>, <em>142</em>, 106811. (<a
href="https://doi.org/10.1016/j.csda.2019.106811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The selection of vine structure to represent dependencies in a data set with a regular vine copula model is still an open question. Up to date, the most popular heuristic to choose the vine structure is to construct consecutive trees by capturing largest correlations in lower trees. However, this might not lead to the optimal vine structure. A new heuristic based on sampling orders implied by regular vines is investigated. The idea is to start with an initial vine structure, that can be chosen with any existing procedure and search for a regular vine copula representing the data better within vines having 2 common sampling orders with this structure. Several algorithms are proposed to support the new heuristic. Both in the simulation study and real data analysis, the potential of the new heuristic to find a structure fitting the data better than the initial vine copula model, is shown.},
  archive      = {J_CSDA},
  author       = {Kailun Zhu and Dorota Kurowicka and Gabriela F. Nane},
  doi          = {10.1016/j.csda.2019.106811},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106811},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Common sampling orders of regular vines with application to model selection},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A partial correlation vine based approach for modeling and
forecasting multivariate volatility time-series. <em>CSDA</em>,
<em>142</em>, 106810. (<a
href="https://doi.org/10.1016/j.csda.2019.106810">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel approach for dynamic modeling and forecasting of realized covariance matrices is proposed. Realized variances and realized correlation matrices are jointly estimated. The one-to-one relationship between a positive definite correlation matrix and its associated set of partial correlations corresponding to any vine specification is used for data transformation. The model components therefore are realized variances as well as realized standard and partial correlations corresponding to a daily log-return series. As such, they have a clear practical interpretation. A method to select a regular vine structure, which allows for parsimonious time-series and dependence modeling of the model components, is introduced. Being algebraically independent the latter do not underlie any algebraic constraint . The proposed model approach is outlined in detail and motivated along with a real data example on six highly liquid stocks. The forecasting performance is evaluated both with respect to statistical precision and in the context of portfolio optimization. Comparisons with Cholesky decomposition based benchmark models support the excellent prediction ability of the proposed model approach.},
  archive      = {J_CSDA},
  author       = {Nicole Barthel and Claudia Czado and Yarema Okhrin},
  doi          = {10.1016/j.csda.2019.106810},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {106810},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {A partial correlation vine based approach for modeling and forecasting multivariate volatility time-series},
  volume       = {142},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). F-SAEM: A fast stochastic approximation of the EM algorithm
for nonlinear mixed effects models. <em>CSDA</em>, <em>141</em>,
123–138. (<a href="https://doi.org/10.1016/j.csda.2019.07.001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to generate samples of the random effects from their conditional distributions is fundamental for inference in mixed effects models . Random walk Metropolis is widely used to perform such sampling, but this method is known to converge slowly for medium dimensional problems, or when the joint structure of the distributions to sample is spatially heterogeneous. The main contribution consists of an independent Metropolis–Hastings (MH) algorithm based on a multidimensional Gaussian proposal that takes into account the joint conditional distribution of the random effects and does not require any tuning. Indeed, this distribution is automatically obtained thanks to a Laplace approximation of the incomplete data model. Such approximation is shown to be equivalent to linearizing the structural model in the case of continuous data. Numerical experiments based on simulated and real data illustrate the performance of the proposed methods. For fitting nonlinear mixed effects models , the suggested MH algorithm is efficiently combined with a stochastic approximation version of the EM algorithm for maximum likelihood estimation of the global parameters.},
  archive      = {J_CSDA},
  author       = {Belhal Karimi and Marc Lavielle and Eric Moulines},
  doi          = {10.1016/j.csda.2019.07.001},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {123-138},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {F-SAEM: A fast stochastic approximation of the EM algorithm for nonlinear mixed effects models},
  volume       = {141},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Detecting and testing altered brain connectivity networks
with k-partite network topology. <em>CSDA</em>, <em>141</em>, 109–122.
(<a href="https://doi.org/10.1016/j.csda.2019.06.007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emerging brain connectivity network studies suggest that interactions betweenvarious distributed neuronal populations may be characterized by an organized complex topological structure . Many neuropsychiatric disorders are associated with altered topological patterns of brain connectivity. Therefore, a key inquiry of connectivity analysis is to detect group-level differentially expressed connectome patterns from the massive neuroimaging data . Recently, statistical methods have been developed to detect differentially expressed connectivity features at a subnetwork level, extending more commonly applied edge level analysis. However, the graph topological structures in these methods are limited to community/cliques which may not effectively uncover the underlying complex and disease-related brain circuits/subnetworks. Building on these previous subnetwork detection methods, a new statistical approach is developed to automatically identify the latent differentially expressed brain connectivity subnetworks with k-partite graph topological structures from large brain connectivity matrices . In addition, statistical inferential techniques are provided to test the detected topological structure . The new methods are evaluated via extensive simulation studies and then applied to resting state fMRI data (24 cases and 18 controls) for Parkinson’s disease research. A differentially expressed connectivity network with the k-partite graph topological structure is detected which reveals underlying neural features distinguishing Parkinson’s disease patients from healthy control subjects.},
  archive      = {J_CSDA},
  author       = {Shuo Chen and F. DuBois Bowman and Yishi Xing},
  doi          = {10.1016/j.csda.2019.06.007},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {109-122},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Detecting and testing altered brain connectivity networks with k-partite network topology},
  volume       = {141},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fused variable screening for massive imbalanced data.
<em>CSDA</em>, <em>141</em>, 94–108. (<a
href="https://doi.org/10.1016/j.csda.2019.06.013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imbalanced data , in which the data exhibit an unequal or highly-skewed distribution between its classes/categories, are pervasive in many scientific fields, with application range from bioinformatics, text classification , face recognition, fraud detection, etc. Imbalanced data in modern science are often of massive size and high dimensionality , for example, gene expression data for diagnosing rare diseases. To address this issue, a fused screening procedure is proposed for dimension reduction with large-scale high dimensional imbalanced data under repeated case-control samplings. There are several advantages of the proposed method: it is model-free without any model specification for the underlying distribution; it is relatively inexpensive in computational cost by using the subsampling technique; it is robust to outliers in the predictors. The theoretical properties are established under regularity conditions . Numerical studies including extensive simulations and a real data example confirm that the proposed method performs well in practical settings.},
  archive      = {J_CSDA},
  author       = {Jinhan Xie and Meiling Hao and Wenxin Liu and Yuanyuan Lin},
  doi          = {10.1016/j.csda.2019.06.013},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {94-108},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Fused variable screening for massive imbalanced data},
  volume       = {141},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Conditional absolute mean calibration for partial linear
multiplicative distortion measurement errors models. <em>CSDA</em>,
<em>141</em>, 77–93. (<a
href="https://doi.org/10.1016/j.csda.2019.06.009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we consider partial linear regression models when all the variables are measured with multiplicative distortion measurement errors. To eliminate the effect caused by the distortion, we propose the conditional absolute mean calibration, which avoids to use the nonzero expectation conditions imposed on the variables. With these calibrated variables, a profile least squares estimator is obtained, associated with its normal approximation based and empirical likelihood based confidence intervals. For the hypothesis testing on parameters, a restricted estimator under the null hypothesis and a test statistic are proposed. A smoothly clipped absolute deviation penalty is employed to select the relevant variables. The resulting penalized estimators are shown to be asymptotically normal and have the oracle property. Lastly, a score-type test statistic is then proposed for checking the validity of partial linear models. We derive asymptotic distribution of the proposed test statistic. The quadratic form of the scaled test statistic has an asymptotic chi-squared distribution under the null hypothesis and follows a noncentral chi-squared distribution under local alternatives, which converge to the null hypothesis at a parametric rate. Simulation studies demonstrate the performance of our proposed procedure and a real example is analyzed as illustrate its practical usage.},
  archive      = {J_CSDA},
  author       = {Jun Zhang and Bingqing Lin and Zhenghui Feng},
  doi          = {10.1016/j.csda.2019.06.009},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {77-93},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Conditional absolute mean calibration for partial linear multiplicative distortion measurement errors models},
  volume       = {141},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-dimensional structure learning of binary pairwise
markov networks: A comparative numerical study. <em>CSDA</em>,
<em>141</em>, 62–76. (<a
href="https://doi.org/10.1016/j.csda.2019.06.012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning the undirected graph structure of a Markov network from data is a problem that has received a lot of attention during the last few decades. As a result of the general applicability of the model class, a myriad of methods have been developed in parallel in several research fields. Recently, as the size of the considered systems has increased, the focus of new methods has been shifted towards the high-dimensional domain. In particular, introduction of the pseudo-likelihood function has pushed the limits of score-based methods which were originally based on the likelihood function. At the same time, methods based on simple pairwise tests have been developed to meet the challenges arising from increasingly large data sets in computational biology. Apart from being applicable to high-dimensional problems, methods based on the pseudo-likelihood and pairwise tests are fundamentally very different. To compare the accuracy of the different types of methods, an extensive numerical study is performed on data generated by binary pairwise Markov networks. A parallelizable Gibbs sampler , based on restricted Boltzmann machines , is proposed as a tool to efficiently sample from sparse high-dimensional networks. The results of the study show that pairwise methods can be more accurate than pseudo-likelihood methods in settings often encountered in high-dimensional structure learning applications.},
  archive      = {J_CSDA},
  author       = {Johan Pensar and Yingying Xu and Santeri Puranen and Maiju Pesonen and Yoshiyuki Kabashima and Jukka Corander},
  doi          = {10.1016/j.csda.2019.06.012},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {62-76},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {High-dimensional structure learning of binary pairwise markov networks: A comparative numerical study},
  volume       = {141},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiplicative bias correction for asymmetric kernel density
estimators revisited. <em>CSDA</em>, <em>141</em>, 40–61. (<a
href="https://doi.org/10.1016/j.csda.2019.06.010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiplicative bias correction technique is revisited for asymmetric kernel density estimators (KDEs) when the data is nonnegative or bounded. It is crucial to classify the recently developed asymmetric KDEs into two types. The multiplicative bias correction applied to the non two-regime type is shown to effectively reduce the order of the bias, at the expense of a constant-factor inflation of the variance. However, it is revealed that, in common with other bias corrections, the multiplicative bias correction applied to the two-regime type fails in reducing the bias near the boundary, unless the density to be estimated satisfies the shoulder condition.},
  archive      = {J_CSDA},
  author       = {Gaku Igarashi and Yoshihide Kakizawa},
  doi          = {10.1016/j.csda.2019.06.010},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {40-61},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Multiplicative bias correction for asymmetric kernel density estimators revisited},
  volume       = {141},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On bootstrap consistency of MAVE for single index models.
<em>CSDA</em>, <em>141</em>, 28–39. (<a
href="https://doi.org/10.1016/j.csda.2019.06.002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper concerns the bootstrap consistency of the minimum average variance estimation (MAVE) method for the single index model. This paper shows that the conditional wild bootstrap estimator of the parameter index shares the same asymptotic covariance of the original MAVE estimator. Thus, the asymptotic distribution can be accurately estimated by the proposed wild bootstrap method . As an application of this method, this paper proposes a conditional Wald type test for the parameter index. It will be shown by simulations that the conditional bootstrap based test is more powerful than the test based on the traditional plug-in covariance estimator. A real data analysis is also provided to demonstrate the effectiveness of the bootstrap method .},
  archive      = {J_CSDA},
  author       = {Hong-Fan Zhang and Lei Huang and Lian-Lian Liu},
  doi          = {10.1016/j.csda.2019.06.002},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {28-39},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {On bootstrap consistency of MAVE for single index models},
  volume       = {141},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data-driven model checking for errors-in-variables
varying-coefficient models with replicate measurements. <em>CSDA</em>,
<em>141</em>, 12–27. (<a
href="https://doi.org/10.1016/j.csda.2019.06.003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, the adequacy check of errors-in-variables varying-coefficient models is investigated when replicate measurements are available. Estimation using the naive method that ignores measurement errors is biased. After the calibration of the estimators of the regression coefficient functions , we construct an empirical-process-based test statistic by the attenuation of corrected residuals. The asymptotic properties of the test statistic under the null hypothesis, global and various local alternatives are established. Simulation studies and real data analyses reveal that the proposed test performs satisfactorily.},
  archive      = {J_CSDA},
  author       = {Miaomiao Wang and Chunling Liu and Tianfa Xie and Zhihua Sun},
  doi          = {10.1016/j.csda.2019.06.003},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {12-27},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {Data-driven model checking for errors-in-variables varying-coefficient models with replicate measurements},
  volume       = {141},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MIXANDMIX: Numerical techniques for the computation of
empirical spectral distributions of population mixtures. <em>CSDA</em>,
<em>141</em>, 1–11. (<a
href="https://doi.org/10.1016/j.csda.2019.06.011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The MIXANDMIX (mixtures by Anderson mixing) tool for the computation of the empirical spectral distribution of random matrices generated by mixtures of populations is described. Within the population mixture model the mapping between the population distributions and the limiting spectral distribution can be obtained by solving a set of systems of non-linear equations, for which an efficient implementation is provided. The contributions include a method for accelerated fixed point convergence, a homotopy continuation strategy to prevent convergence to non-admissible solutions, a blind non-uniform grid construction for effective distribution support detection and approximation, and a parallel computing architecture. Comparisons are performed with available packages for the single population case and with results obtained by simulation for the more general model implemented here. Results show competitive performance and improved flexibility.},
  archive      = {J_CSDA},
  author       = {Lucilio Cordero-Grande},
  doi          = {10.1016/j.csda.2019.06.011},
  journal      = {Computational Statistics &amp; Data Analysis},
  pages        = {1-11},
  shortjournal = {Comput. Stat. Data Anal.},
  title        = {MIXANDMIX: Numerical techniques for the computation of empirical spectral distributions of population mixtures},
  volume       = {141},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
