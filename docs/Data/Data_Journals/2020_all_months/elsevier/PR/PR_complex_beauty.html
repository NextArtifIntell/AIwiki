<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>PR_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="pr---415">PR - 415</h2>
<ul>
<li><details>
<summary>
(2020). Geometric rectification of document images using adversarial
gated unwarping network. <em>PR</em>, <em>108</em>, 107576. (<a
href="https://doi.org/10.1016/j.patcog.2020.107576">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Document images captured in natural scenes with a hand-held camera often suffer from geometric distortions and cluttered backgrounds. In this paper, we propose a simple yet efficient deep model named Adversarial Gated Unwarping Network (AGUN) to rectify these images. In this model, the rectification task is recast as a dense grid prediction problem. We thereby develop a pyramid encoder-decoder architecture to predict the unwarping grid at multiple resolutions in a coarse-to-fine fashion. Based on the observation that the structural visual cues, e.g., text-lines, text blocks, lines in tables, which are critical for the estimation of unwarping mapping, are non-uniformly distributed in the images, three gated modules are introduced to guide the network focusing on these informative cues rather than other interferences such as blank areas and complex backgrounds. To generate more visually pleasing rectification results, we further adopt adversarial training mechanism to implicitly constrain the unwarping grid estimation. Our model can rectify arbitrarily distorted document images with complicated page layouts and cluttered backgrounds. Experiments on the public benchmark dataset and the synthetic dataset demonstrate that our approach outperforms the state-of-the-art methods in terms of OCR accuracy and several widely used quantitative evaluation metrics .},
  archive      = {J_PR},
  author       = {Xiyan Liu and Gaofeng Meng and Bin Fan and Shiming Xiang and Chunhong Pan},
  doi          = {10.1016/j.patcog.2020.107576},
  journal      = {Pattern Recognition},
  pages        = {107576},
  shortjournal = {Pattern Recognition},
  title        = {Geometric rectification of document images using adversarial gated unwarping network},
  volume       = {108},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Transductive semi-supervised metric learning for person
re-identification. <em>PR</em>, <em>108</em>, 107569. (<a
href="https://doi.org/10.1016/j.patcog.2020.107569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning is important and has become more widespread because obtaining labeled data is expensive and labor-intensive. In this paper, we focus on the challenging semi-supervised person Re-identification (ReID) task, which is a metric learning problem based on the assumption that unlabeled data is open-set. To address this problem, we propose the Transductive Semi-Supervised Metric Learning (TSSML) framework. In TSSML, we propose a graph-based transductive hard mining method for deeply mining hard triplets in unlabeled data and a degree-based relationship confidence scoring method for further reducing incorrect triplets. Moreover, we investigate the feature consistency loss (FCL) and adopt the curriculum learning strategy to improve the representation learning for semi-supervised ReID . Extensive experiments have been conducted on three large-scale ReID datasets and demonstrate the effectiveness of our TSSML framework.},
  archive      = {J_PR},
  author       = {Xinyuan Chang and Zhiheng Ma and Xing Wei and Xiaopeng Hong and Yihong Gong},
  doi          = {10.1016/j.patcog.2020.107569},
  journal      = {Pattern Recognition},
  pages        = {107569},
  shortjournal = {Pattern Recognition},
  title        = {Transductive semi-supervised metric learning for person re-identification},
  volume       = {108},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). View-specific subspace learning and re-ranking for
semi-supervised person re-identification. <em>PR</em>, <em>108</em>,
107568. (<a href="https://doi.org/10.1016/j.patcog.2020.107568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (re-ID) focuses on matching the same person across non-overlapping camera views. Most existing methods require tedious manual annotation and can only learn a unitary transformation for images across views, which severely lack of scalability and suffer from view-specific biases. To address these issues, we put forward a View-Specific Semi-supervised Subspace Learning (VS-SSL) approach that can learn specific projections for each view, utilizing limited labeled data to guide the training while leveraging abundant unlabeled data simultaneously. Moreover, a novel re-ranking strategy is proposed to boost the performance further, which re-estimates the similarity between probe and galleries according to the overlap ratio between their expanded neighbors and their position in each other’s ranking list. The effectiveness of the proposed framework is evaluated on several widely-used datasets (VIPeR, PRID450S, PRID2011, CUHK01 and Market-1501), yielding superior performance for both semi-supervised and supervised re-ID.},
  archive      = {J_PR},
  author       = {Jieru Jia and Qiuqi Ruan and Yi Jin and Gaoyun An and Shiming Ge},
  doi          = {10.1016/j.patcog.2020.107568},
  journal      = {Pattern Recognition},
  pages        = {107568},
  shortjournal = {Pattern Recognition},
  title        = {View-specific subspace learning and re-ranking for semi-supervised person re-identification},
  volume       = {108},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online kernel classification with adjustable bandwidth using
control-based learning approach. <em>PR</em>, <em>108</em>, 107566. (<a
href="https://doi.org/10.1016/j.patcog.2020.107566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel control-based kernel learning approach is proposed for inferring online binary classification tasks . Following a carefully designed alternating optimization scheme, the learning problems are transformed into two optimal feedback control problems for a series of linear, controllable systems . Model parameters including weights and kernel bandwidth can be efficiently updated by solving the control problems. These consequently lead to our control-based adaptive online kernel classification algorithm (CAOKC). The bandwidth, although nonlinear in our model, can still be updated accurately after linearization. Thus, compared with the existing benchmark algorithms with fixed kernels, the CAOKC algorithm is able to achieve a more adaptive, robust classification performance with better prediction accuracy by regarding the bandwidth as an adjustable parameter. The results presented in this paper also demonstrate how optimal control can provide novel insights and be an effective approach for addressing various learning tasks. Numerical results on benchmark synthetic and realistic datasets are provided to illustrate our method.},
  archive      = {J_PR},
  author       = {Jiaming Zhang and Hanwen Ning},
  doi          = {10.1016/j.patcog.2020.107566},
  journal      = {Pattern Recognition},
  pages        = {107566},
  shortjournal = {Pattern Recognition},
  title        = {Online kernel classification with adjustable bandwidth using control-based learning approach},
  volume       = {108},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A mathematical programming approach for integrated multiple
linear regression subset selection and validation. <em>PR</em>,
<em>108</em>, 107565. (<a
href="https://doi.org/10.1016/j.patcog.2020.107565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subset selection for multiple linear regression aims to construct a regression model that minimizes errors by selecting a small number of explanatory variables . Once a model is built, various statistical tests and diagnostics are conducted to validate the model and to determine whether the regression assumptions are met. Most traditional approaches require human decisions at this step. For example, the user may repeat adding or removing a variable until a satisfactory model is obtained. However, this trial-and-error strategy cannot guarantee that a subset that minimizes the errors while satisfying all regression assumptions will be found. In this paper, we propose a fully automated model building procedure for multiple linear regression subset selection that integrates model building and validation based on mathematical programming . The proposed model minimizes mean squared errors while ensuring that the majority of the important regression assumptions are met. We also propose an efficient constraint to approximate the constraint for the coefficient t -test. When no subset satisfies all of the considered regression assumptions, our model provides an alternative subset that satisfies most of these assumptions. Computational results show that our model yields better solutions (i.e., satisfying more regression assumptions) compared to the state-of-the-art benchmark models while maintaining similar explanatory power .},
  archive      = {J_PR},
  author       = {Seokhyun Chung and Young Woong Park and Taesu Cheong},
  doi          = {10.1016/j.patcog.2020.107565},
  journal      = {Pattern Recognition},
  pages        = {107565},
  shortjournal = {Pattern Recognition},
  title        = {A mathematical programming approach for integrated multiple linear regression subset selection and validation},
  volume       = {108},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rich heterogeneous information preserving network
representation learning. <em>PR</em>, <em>108</em>, 107564. (<a
href="https://doi.org/10.1016/j.patcog.2020.107564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network representation learning has attracted increasing attention recently due to its applicability in network analysis . However, most existing network representation learning models only focus on preserving fragmentary aspects of network information, either node proximities or fixed semantic information. In this paper, we propose a novel algorithm named Rich Heterogeneous Information Preserving Network Representation Learning (HIRL), which integrates the high-order proximity among nodes and semantic information into a generic framework by exploiting a flexible autoencoder network. Based on the proposed algorithm, we can explore the hidden information in heterogeneous information networks through any custom form of path schema, and represents different types of nodes in a continuous and common vector space. Moreover, the proposed HIRL is applicable to homogeneous information networks. Extensive experimental results demonstrate that our approach can effectively preserve the information in networks under various path schemas, and performs better on real-world applications such as network reconstruction, link prediction, and node classification compared with the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Bin Yu and Jinzhi Hu and Yu Xie and Chen Zhang and Zhouhua Tang},
  doi          = {10.1016/j.patcog.2020.107564},
  journal      = {Pattern Recognition},
  pages        = {107564},
  shortjournal = {Pattern Recognition},
  title        = {Rich heterogeneous information preserving network representation learning},
  volume       = {108},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cross-modal knowledge reasoning for knowledge-based visual
question answering. <em>PR</em>, <em>108</em>, 107563. (<a
href="https://doi.org/10.1016/j.patcog.2020.107563">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge-based Visual Question Answering (KVQA) requires external knowledge beyond the visible content to answer questions about an image. This ability is challenging but indispensable to achieve general VQA. One limitation of existing KVQA solutions is that they jointly embed all kinds of information without fine-grained selection, which introduces unexpected noises for reasoning the correct answer. How to capture the question-oriented and information-complementary evidence remains a key challenge to solve the problem. Inspired by the human cognition theory, in this paper, we depict an image by multiple knowledge graphs from the visual, semantic and factual views. Thereinto, the visual graph and semantic graph are regarded as image-conditioned instantiation of the factual graph. On top of these new representations, we re-formulate Knowledge-based Visual Question Answering as a recurrent reasoning process for obtaining complementary evidence from multimodal information . To this end, we decompose the model into a series of memory-based reasoning steps, each performed by a G raph-based R ead, U pdate, and C ontrol ( GRUC ) module that conducts parallel reasoning over both visual and semantic information. By stacking the modules multiple times, our model performs transitive reasoning and obtains question-oriented concept representations under the constrain of different modalities. Finally, we perform graph neural networks to infer the global-optimal answer by jointly considering all the concepts. We achieve a new state-of-the-art performance on three popular benchmark datasets, including FVQA, Visual7W-KB and OK-VQA, and demonstrate the effectiveness and interpretability of our model with extensive experiments. The source code is available at: https://github.com/astro-zihao/gruc},
  archive      = {J_PR},
  author       = {Jing Yu and Zihao Zhu and Yujing Wang and Weifeng Zhang and Yue Hu and Jianlong Tan},
  doi          = {10.1016/j.patcog.2020.107563},
  journal      = {Pattern Recognition},
  pages        = {107563},
  shortjournal = {Pattern Recognition},
  title        = {Cross-modal knowledge reasoning for knowledge-based visual question answering},
  volume       = {108},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sensor-based and vision-based human activity recognition: A
comprehensive survey. <em>PR</em>, <em>108</em>, 107561. (<a
href="https://doi.org/10.1016/j.patcog.2020.107561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human activity recognition (HAR) technology that analyzes data acquired from various types of sensing devices, including vision sensors and embedded sensors, has motivated the development of various context-aware applications in emerging domains, e.g., the Internet of Things (IoT) and healthcare. Even though a considerable number of HAR surveys and review articles have been conducted previously, the major/overall HAR subject has been ignored, and these studies only focus on particular HAR topics. Therefore, a comprehensive review paper that covers major subjects in HAR is imperative. This survey analyzes the latest state-of-the-art research in HAR in recent years, introduces a classification of HAR methodologies, and shows advantages and weaknesses for methods in each category. Specifically, HAR methods are classified into two main groups, which are sensor-based HAR and vision-based HAR, based on the generated data type . After that, each group is divided into subgroups that perform different procedures, including the data collection, pre-processing methods, feature engineering, and the training process. Moreover, an extensive review regarding the utilization of deep learning in HAR is also conducted. Finally, this paper discusses various challenges in the current HAR topic and offers suggestions for future research.},
  archive      = {J_PR},
  author       = {L. Minh Dang and Kyungbok Min and Hanxiang Wang and Md. Jalil Piran and Cheol Hee Lee and Hyeonjoon Moon},
  doi          = {10.1016/j.patcog.2020.107561},
  journal      = {Pattern Recognition},
  pages        = {107561},
  shortjournal = {Pattern Recognition},
  title        = {Sensor-based and vision-based human activity recognition: A comprehensive survey},
  volume       = {108},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SCUT-HCCDoc: A new benchmark dataset of handwritten chinese
text in unconstrained camera-captured documents. <em>PR</em>,
<em>108</em>, 107559. (<a
href="https://doi.org/10.1016/j.patcog.2020.107559">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a large-scale dataset, called SCUT-HCCDoc, to address challenging detection and recognition problems of handwritten Chinese text (HCT) in the camera-captured documents. Despite extensive studies of optical character recognition (OCR) and offline handwriting recognition for document images , text detection and recognition in the camera-captured documents remains an unsolved problem that is worth for extensive study and investigation. With recent advances in deep learning , researchers have proposed useful architectures for feature learning , detection, and recognition for the scene text. However, the performance of deep learning methods highly depends on the amount and diversity of training data. Previous OCR and offline HCT datasets were built under specific constraints, and most of the recent scene text datasets are for non-handwritten text. Hence, there is a lack of a comprehensive scene handwritten text benchmark. This study focuses on scenes with handwritten Chinese text. We introduce the SCUT-HCCDoc database for HCT detection, recognition and spotting. SCUT-HCCDoc contains 12,253 camera-captured document images with 116,629 text lines and 1,155,801 characters. The diversity of SCUT-HCCDoc can be described at three levels: (1) image-level diversity : image appearance and geometric variances caused by camera-captured settings (such as perspective, background, and resolution) and different applications (such as note-taking, test papers, and homework); (2) text-level diversity : variances of text line length, rotation, etc.; (3) character-level diversity : variances of character categories (up to 6109 classes with additional English letters, and digits), character size, individual writing style, etc. Three kinds of baseline experiments were conducted, where we used several popular text detection methods for text line detection, CTC-based/attention-based methods for text line recognition, and combine text detectors with CTC-based recognizer to achieve end-to-end text spotting. The results indicate the diversity of SCUT-HCCDoc and the challenges of HCT understanding in document images. The dataset is available at https://github.com/HCIILAB/SCUT-HCCDoc_Dataset_Release .},
  archive      = {J_PR},
  author       = {Hesuo Zhang and Lingyu Liang and Lianwen Jin},
  doi          = {10.1016/j.patcog.2020.107559},
  journal      = {Pattern Recognition},
  pages        = {107559},
  shortjournal = {Pattern Recognition},
  title        = {SCUT-HCCDoc: A new benchmark dataset of handwritten chinese text in unconstrained camera-captured documents},
  volume       = {108},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SEMEDA: Enhancing segmentation precision with semantic edge
aware loss. <em>PR</em>, <em>108</em>, 107557. (<a
href="https://doi.org/10.1016/j.patcog.2020.107557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Per-Pixel Cross entropy (PPCE) is a commonly used loss on semantic segmentation tasks. However, it suffers from a number of drawbacks. Firstly, PPCE only depends on the probability of the ground truth class since the latter is usually one-hot encoded. Secondly, PPCE treats all pixels independently and does not take the local structure into account. While perceptual losses (e.g. matching prediction and ground truth in the embedding space of a pre-trained VGG network) would theoretically address these concerns, it does not constitute a practical solution as segmentation masks follow a distribution that differs largely from natural images. In this paper, we introduce a SEMantic EDge-Aware strategy (SEMEDA) to solve these issues. Inspired by perceptual losses, we propose to match the ’probability texture’ of predicted segmentation mask and ground truth through a proxy network trained for semantic edge detection on the ground truth masks. Through thorough experimental validation on several datasets, we show that SEMEDA steadily improves the segmentation accuracy with negligible computational overhead and can be added with any popular segmentation networks in an end-to-end training framework.},
  archive      = {J_PR},
  author       = {Yifu Chen and Arnaud Dapogny and Matthieu Cord},
  doi          = {10.1016/j.patcog.2020.107557},
  journal      = {Pattern Recognition},
  pages        = {107557},
  shortjournal = {Pattern Recognition},
  title        = {SEMEDA: Enhancing segmentation precision with semantic edge aware loss},
  volume       = {108},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Freely typed keystroke dynamics-based user authentication
for mobile devices based on heterogeneous features. <em>PR</em>,
<em>108</em>, 107556. (<a
href="https://doi.org/10.1016/j.patcog.2020.107556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Keystroke dynamics-based authentication (KDA) is one of the human behavioral biometric-based user authentication methods based on the unique typing pattern of a person. Previous KDA studies on mobile devices primarily focused on fixed-length text-based KDA, such as passwords and personal identification numbers . This can strengthen the login system and prevent abnormal usage of impostors based on certain attack methods, such as shoulder surfing and smudge attacks. However, this method possesses a limitation that continuous monitoring is not possible after login. To solve this problem, KDA based on freely typed text was studied; however, there are only a few studies on this technique. Further, the performance authentication based on these studies is insufficient for a real-world implementation. In this paper, we propose a novel freely typed text-based KDA method for mobile devices named FACT, i.e., user authentication on mobile devices based on free text, accelerator, coordinate, and time. We collected data from three different smartphone sensors while typing in two languages (English and Korean), and 17 variables were extracted for a set of keystroke data. A total of six authentication methods were employed and the proposed FACT yielded an equal error rate lower than 1\% with only one reference keystroke set; moreover, it demonstrated a perfect protection capability while using Korean when more than four reference keystroke sets were used. To contribute to the research and industrial community, we have publicized our collected keystroke dataset so that anyone who conducts a KDA study or develops a KDA-related mobile service can use the dataset without any restrictions.},
  archive      = {J_PR},
  author       = {Junhong Kim and Pilsung Kang},
  doi          = {10.1016/j.patcog.2020.107556},
  journal      = {Pattern Recognition},
  pages        = {107556},
  shortjournal = {Pattern Recognition},
  title        = {Freely typed keystroke dynamics-based user authentication for mobile devices based on heterogeneous features},
  volume       = {108},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MuLTReNets: Multilingual text recognition networks for
simultaneous script identification and handwriting recognition.
<em>PR</em>, <em>108</em>, 107555. (<a
href="https://doi.org/10.1016/j.patcog.2020.107555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multilingual handwritten text recognition is often accomplished in two cascaded steps: script identification and handwriting recognition. However, this scheme is not optimal due to error accumulation. To perform simultaneous script identification and handwriting recognition, in this paper, we propose a new framework named multilingual text recognition networks (MuLTReNets). Specifically, the system has four major modules: feature extractor, script identifier, handwriting recognizer and auto-weighter . The feature extractor integrates both spatial and temporal knowledge to encode text images into features shared by the script identifier and recognizer. The script identifier predicts script category from a variable-length sequence incorporating an auto-weighter for balancing different scripts, while the handwriting recognizer adopts long-short term memory (LSTM) and Connectionist Temporal Classification (CTC) to accomplish sequence decoding. Via multi-task learning, the proposed framework can benefit both two multilingual recognition schemes: unified recognition with merged alphabet (MuLTReNetV1) and cascaded script identification-single script recognition with joint training (MuLTReNetV2). We evaluated the performance of the proposed method on handwritten text databases of five languages, which are English, French, Kannada, Urdu, and Bangla. Experimental results demonstrate that our method performs superiorly for both script identification and handwriting recognition. The accuracy of script identification reaches 99.9\%. While in handwriting recognition, the proposed system not only outperforms cascade systems but also surpasses systems particularly designed for specific scripts.},
  archive      = {J_PR},
  author       = {Zhuo Chen and Fei Yin and Xu-Yao Zhang and Qing Yang and Cheng-Lin Liu},
  doi          = {10.1016/j.patcog.2020.107555},
  journal      = {Pattern Recognition},
  pages        = {107555},
  shortjournal = {Pattern Recognition},
  title        = {MuLTReNets: Multilingual text recognition networks for simultaneous script identification and handwriting recognition},
  volume       = {108},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Density peak clustering based on relative density
relationship. <em>PR</em>, <em>108</em>, 107554. (<a
href="https://doi.org/10.1016/j.patcog.2020.107554">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The density peak clustering algorithm treats local density peaks as cluster centers, and groups non-center data points by assuming that one data point and its nearest higher-density neighbor are in the same cluster. While this algorithm is shown to be promising in some applications, its clustering results are found to be sensitive to density kernels, and large density differences across clusters tend to result in wrong cluster centers. In this paper we attribute these problems to the inconsistency between the assumption and implementation adopted in this algorithm. While the assumption is based totally on relative density relationship, this algorithm adopts absolute density as one criterion to identify cluster centers. This observation prompts us to present a cluster center identification criterion based only on relative density relationship. Specifically, we define the concept of subordinate to describe the relative density relationship, and use the number of subordinates as a criterion to identify cluster centers. Our approach makes use of only relative density relationship and is less influenced by density kernels and density differences across clusters. In addition, we discuss the problems of two existing density kernels, and present an average-distance based kernel. In data clustering experiments we validate the new criterion and density kernel respectively, and then test the whole algorithm and compare with some other clustering algorithms.},
  archive      = {J_PR},
  author       = {Jian Hou and Aihua Zhang and Naiming Qi},
  doi          = {10.1016/j.patcog.2020.107554},
  journal      = {Pattern Recognition},
  pages        = {107554},
  shortjournal = {Pattern Recognition},
  title        = {Density peak clustering based on relative density relationship},
  volume       = {108},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generalized low-rank approximation of matrices based on
multiple transformation pairs. <em>PR</em>, <em>108</em>, 107545. (<a
href="https://doi.org/10.1016/j.patcog.2020.107545">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dimensionality reduction is a critical step in the learning process that plays an essential role in various applications. The most popular methods for dimensionality reduction, SVD and PCA, for instance, only work on one-dimensional data. This means that for higher-order data like matrices or more generally tensors, data should be fold to the vector format. Thus, this approach ignores the spatial relationships of features and increases the probability of overfitting as well. Due to the mentioned issues, several methods like Generalized Low-Rank Approximation of Matrices (GLRAM) and Multilinear PCA (MPCA) proposed to deal with multi-dimensional data in their original format. Consequently, the spatial relationships of features preserved and the probability of overfitting diminished. Besides, the time and space complexity in such methods are less than vector-based ones. However, since the multilinear approach needs fewer parameters, its search space is much smaller than that of the vector-based one. To solve the previous problems of multilinear methods like GLRAM, we proposed a novel extension of GLRAM in which instead one transformation pair use multiple left and right transformation pairs on the projected data. Consequently, this provides the problem with a larger feasible region and smaller reconstruction error. This article provides several analytical discussions and experimental results that confirm the quality of the proposed method.},
  archive      = {J_PR},
  author       = {Soheil Ahmadi and Mansoor Rezghi},
  doi          = {10.1016/j.patcog.2020.107545},
  journal      = {Pattern Recognition},
  pages        = {107545},
  shortjournal = {Pattern Recognition},
  title        = {Generalized low-rank approximation of matrices based on multiple transformation pairs},
  volume       = {108},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Relational graph neural network for situation recognition.
<em>PR</em>, <em>108</em>, 107544. (<a
href="https://doi.org/10.1016/j.patcog.2020.107544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, situation recognition as a new challenging task for image understanding has gained great attention, which needs to simultaneously predict the main activity (verb) and its associated objects (noun entities) in a structured and detailed way. Several methods have been proposed to handle this task, but usually they cannot effectively model the relationships between the activity and the objects. In this paper, we propose a Relational Graph Neural Network (RGNN) for situation recognition, which builds a neural graph on the activity and the objects, and models the triplet relationships between the activity and pairs of objects through message passing between graph nodes . Moreover, we propose a two-stage training strategy to optimize the model. A progressive supervised learning is first adopted to obtain an initial prediction for the activity and the objects. Then, the initial predictions are refined by using a policy-gradient method to directly optimize the non-differentiable value-all metric. To verify the effectiveness of our method, we perform extensive experiments on the Imsitu dataset which is currently the only available dataset for situation recognition. Experimental results show that our approach outperforms the state-of-the-art methods on verb and value metrics, and demonstrates better relationships between the activity and the objects.},
  archive      = {J_PR},
  author       = {Ya Jing and Junbo Wang and Wei Wang and Liang Wang and Tieniu Tan},
  doi          = {10.1016/j.patcog.2020.107544},
  journal      = {Pattern Recognition},
  pages        = {107544},
  shortjournal = {Pattern Recognition},
  title        = {Relational graph neural network for situation recognition},
  volume       = {108},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Diverse training dataset generation based on a
multi-objective optimization for semi-supervised classification.
<em>PR</em>, <em>108</em>, 107543. (<a
href="https://doi.org/10.1016/j.patcog.2020.107543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The self-labeled technique is a type of semi-supervised classification that can be used when labeled data are lacking. Although existing self-labeled techniques show promise in many areas of classification and pattern recognition, they commonly incorrectly label data. The reasons for this problem are the shortage of labeled data and the inappropriate distribution of data in problem space. To deal with this problem, we propose in this paper a synthetic, labeled data generation method based on accuracy and density. Positions of generated data are improved through a multi-objective evolutionary algorithm with two objectives – accuracy and density. The density function generates data with an appropriate distribution and diversity in feature space, whereas the accuracy function eliminates outlier data. The advantage of the proposed method over existing ones is that it simultaneously considers accuracy and distribution of generated data in feature space. We have applied the new proposed method on four self-labeled techniques with different features, i.e., Democratic-co, Tri-training, Co-forest, and Co-bagging. The results show that the proposed method is superior to existing methods in terms of classification accuracy . Also, the superiority of the proposed method is confirmed over other data generation methods such as SMOTE, Borderline SMOTE, Safe-level SMOTE and SMOTE-RSB.},
  archive      = {J_PR},
  author       = {Zahra Donyavi and Shahrokh Asadi},
  doi          = {10.1016/j.patcog.2020.107543},
  journal      = {Pattern Recognition},
  pages        = {107543},
  shortjournal = {Pattern Recognition},
  title        = {Diverse training dataset generation based on a multi-objective optimization for semi-supervised classification},
  volume       = {108},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bringing semantics into word image representation.
<em>PR</em>, <em>108</em>, 107542. (<a
href="https://doi.org/10.1016/j.patcog.2020.107542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The shift from one-hot to distributed representation, popularly referred to as word embedding has changed the landscape of natural language processing ( nlp ) and information retrieval ( ir ) communities. In the domain of document images , we have always appreciated the need for learning a holistic word image representation which is popularly used for the task of word spotting. The representations proposed for word spotting is different from word embedding in text since the later captures the semantic aspects of the word which is a crucial ingredient to numerous nlp and ir tasks. In this work, we attempt to encode the notion of semantics into word image representation by bringing the advancements from the textual domain. We propose two novel forms of representations where the first form is designed to be inflection invariant by focusing on the approximate linguistic root of the word, while the second form is built along the lines of recent textual word embedding techniques such as Word2Vec. We observe that such representations are useful for both traditional word spotting and also enrich the search results by accounting the semantic nature of the task. We conduct our experiments on the challenging document images taken from historical-modern collections, handwritten-printed domains, and Latin-Indic scripts. For the purpose of semantic evaluation , we have prepared a large synthetic word image dataset and report interesting results for the standard semantic evaluation metrics such as word analogy and word similarity.},
  archive      = {J_PR},
  author       = {Praveen Krishnan and C.V. Jawahar},
  doi          = {10.1016/j.patcog.2020.107542},
  journal      = {Pattern Recognition},
  pages        = {107542},
  shortjournal = {Pattern Recognition},
  title        = {Bringing semantics into word image representation},
  volume       = {108},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discovering and incorporating latent target-domains for
domain adaptation. <em>PR</em>, <em>108</em>, 107536. (<a
href="https://doi.org/10.1016/j.patcog.2020.107536">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we aim to address the unsupervised domain adaptation problem where the data in the target domain are much more diverse compared with the data in the source domain. In particular, this problem is formulated as discovering and incorporating latent domains underlying target data of interest for unsupervised domain adaptation. More specifically, the discovery of the latent target domains is based on three criteria, including the maximization of compactness and distinctiveness of the data in the individual latent target-domain, as well as the minimization of total divergence from the latent target-domains to the source domain. For each pair formed by a latent target domain and the source domain, we learn a feature space where the discrepancy between the source domain and the specific latent target domain is shrunk. Finally, we consider the projected source domain data on the learned latent feature spaces as different views of the source domain, and propose an extended multiple kernel learning algorithm to train a more robust and precise classifier for predicting the unlabeled target data. The effectiveness of our proposed method is demonstrated on various benchmark datasets for object recognition and human activity recognition . Moreover, we also show that our proposed method can be treated as an effective complement to the deep learning based unsupervised domain adaptation.},
  archive      = {J_PR},
  author       = {Haoliang Li and Wen Li and Shiqi Wang},
  doi          = {10.1016/j.patcog.2020.107536},
  journal      = {Pattern Recognition},
  pages        = {107536},
  shortjournal = {Pattern Recognition},
  title        = {Discovering and incorporating latent target-domains for domain adaptation},
  volume       = {108},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image segmentation using dense and sparse hierarchies of
superpixels. <em>PR</em>, <em>108</em>, 107532. (<a
href="https://doi.org/10.1016/j.patcog.2020.107532">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the intersection between hierarchical and superpixel image segmentation . Two strategies are considered: (i) the classical region merging, that creates a dense hierarchy with a higher number of levels, and (ii) the recursive execution of some superpixel algorithm, which generates a sparse hierarchy with fewer levels. We show that, while dense methods can capture more intermediate or higher-level object information, sparse methods are considerably faster and usually with higher boundary adherence at finer levels. We first formalize the two strategies and present a sparse method, which is faster than its superpixel algorithm and with similar boundary adherence. We then propose a new dense method to be used as post-processing from the intermediate level, as obtained by our sparse method, upwards. This combination results in a unique strategy and the most effective hierarchical segmentation method among the compared state-of-the-art approaches, with efficiency comparable to the fastest superpixel algorithms.},
  archive      = {J_PR},
  author       = {Felipe Lemes Galvão and Silvio Jamil Ferzoli Guimarães and Alexandre Xavier Falcão},
  doi          = {10.1016/j.patcog.2020.107532},
  journal      = {Pattern Recognition},
  pages        = {107532},
  shortjournal = {Pattern Recognition},
  title        = {Image segmentation using dense and sparse hierarchies of superpixels},
  volume       = {108},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). HCNN-PSI: A hybrid CNN with partial semantic information for
space target recognition. <em>PR</em>, <em>108</em>, 107531. (<a
href="https://doi.org/10.1016/j.patcog.2020.107531">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Space target recognition is the basic task of space situational awareness and has developed significantly in the last decade. This paper proposes a hybrid convolutional neural network with partial semantic information for space target recognition, which joints the global features and partial semantic information. Firstly, we propose a two-stage target detection network based on the characteristics of deep space targets. Secondly, we use the Mask R-CNN to segment the main components of the detected satellite. Thirdly, the recognized target and the segmented components are sent to the hybrid extractor to train the hybrid network. What we have done is to find the proper weights of the partial semantic information that plays different importance. The loss function of the hybrid network integrates the global-based and component-based loss with different weights. In comparison with several sets of comparative experiments , the proposed method has achieved a satisfactory result. Besides, we have simulated some real space target images by data processing and achieved a competitive performance in both the simulated dataset and the public dataset.},
  archive      = {J_PR},
  author       = {Xi Yang and Tan Wu and Nannan Wang and Yan Huang and Bin Song and Xinbo Gao},
  doi          = {10.1016/j.patcog.2020.107531},
  journal      = {Pattern Recognition},
  pages        = {107531},
  shortjournal = {Pattern Recognition},
  title        = {HCNN-PSI: A hybrid CNN with partial semantic information for space target recognition},
  volume       = {108},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast and incremental algorithms for exponential
semi-supervised discriminant embedding. <em>PR</em>, <em>108</em>,
107530. (<a href="https://doi.org/10.1016/j.patcog.2020.107530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In various pattern classification problems, semi-supervised learning methods have shown its effectiveness in utilizing unlabeled data to yield better performance than some supervised and unsupervised learning methods. Semi-supervised discriminant embedding (SDE) is a semi-supervised extension of local discriminant embedding (LDE). However, when dealing with high dimensional data , SDE often suffers from the small-sample-size (SSS) problem. In order to settle this problem, an exponential semi-supervised discriminant embedding (ESDE) method was proposed in [ F. Dornaika, Y. EI Traboulsi . Matrix exponential based semi-supervised discriminant embedding for image classification , Pattern Recognition, 61 (2017): 92–103], which makes use of the tool of matrix exponential. Despite its high discriminative ability, the computational overhead of ESDE is very large for high dimensional data . In order to cure this drawback, the first contribution of this paper is to propose a fast implementation on the ESDE method. The key is to equivalently transform the large matrix problem of size d into a much smaller one of size n , where d is the data dimension and n is the number of training samples, with d ≫ n in practice. On the other hand, in many real world applications , it is likely that whole labeled training set is unavailable beforehand, and the training data is obtained incrementally. Many incremental semi-supervised learning methods have been proposed to deal with this problem, to the best of our knowledge, however, there are no incremental algorithms for matrix exponential discriminant methods till now. To fill in this gap, the second contribution of this paper is to propose incremental ESDE algorithms for incremental learning problems. Numerical experiments on some real-world data sets show the numerical behavior of the proposed algorithms.},
  archive      = {J_PR},
  author       = {Yingdi Lu and Gang Wu},
  doi          = {10.1016/j.patcog.2020.107530},
  journal      = {Pattern Recognition},
  pages        = {107530},
  shortjournal = {Pattern Recognition},
  title        = {Fast and incremental algorithms for exponential semi-supervised discriminant embedding},
  volume       = {108},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). One-vs-one classification for deep neural networks.
<em>PR</em>, <em>108</em>, 107528. (<a
href="https://doi.org/10.1016/j.patcog.2020.107528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For performing multi-class classification, deep neural networks almost always employ a One-vs-All (OvA) classification scheme with as many output units as there are classes in a dataset. The problem of this approach is that each output unit requires a complex decision boundary to separate examples from one class from all other examples. In this paper, we propose a novel One-vs-One (OvO) classification scheme for deep neural networks that trains each output unit to distinguish between a specific pair of classes. This method increases the number of output units compared to the One-vs-All classification scheme but makes learning correct decision boundaries much easier. In addition to changing the neural network architecture , we changed the loss function, created a code matrix to transform the one-hot encoding to a new label encoding, and changed the method for classifying examples. To analyze the advantages of the proposed method, we compared the One-vs-One and One-vs-All classification methods on three plant recognition datasets (including a novel dataset that we created) and a dataset with images of different monkey species using two deep architectures. The two deep convolutional neural network (CNN) architectures, Inception-V3 and ResNet-50, are trained from scratch or pre-trained weights. The results show that the One-vs-One classification method outperforms the One-vs-All method on all four datasets when training the CNNs from scratch. However, when using the two classification schemes for fine-tuning pre-trained CNNs, the One-vs-All method leads to the best performances, which is presumably because the CNNs had been pre-trained using the One-vs-All scheme.},
  archive      = {J_PR},
  author       = {Pornntiwa Pawara and Emmanuel Okafor and Marc Groefsema and Sheng He and Lambert R.B. Schomaker and Marco A. Wiering},
  doi          = {10.1016/j.patcog.2020.107528},
  journal      = {Pattern Recognition},
  pages        = {107528},
  shortjournal = {Pattern Recognition},
  title        = {One-vs-one classification for deep neural networks},
  volume       = {108},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mutual information based feature subset selection in
multivariate time series classification. <em>PR</em>, <em>108</em>,
107525. (<a href="https://doi.org/10.1016/j.patcog.2020.107525">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with supervised classification of multivariate time series . In particular, the goal is to propose a filter method to select a subset of time series. Consequently, we adopt the framework proposed by Brown al. [1]. The key point in this framework is the computation of the mutual information between the features, which allows us to measure the relevance of each feature subset. In our case, where the features are a time series, we use an adaptation of existing nonparametric mutual information estimators based on the k-nearest neighbor. Specifically, for the purpose of bringing these methods to the time series scenario, we rely on the use of dynamic time warping dissimilarity. Our experimental results show that our method is able to strongly reduce the number of time series while keeping or increasing the classification accuracy .},
  archive      = {J_PR},
  author       = {Josu Ircio and Aizea Lojo and Usue Mori and Jose A. Lozano},
  doi          = {10.1016/j.patcog.2020.107525},
  journal      = {Pattern Recognition},
  pages        = {107525},
  shortjournal = {Pattern Recognition},
  title        = {Mutual information based feature subset selection in multivariate time series classification},
  volume       = {108},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-view subspace learning via bidirectional sparsity.
<em>PR</em>, <em>108</em>, 107524. (<a
href="https://doi.org/10.1016/j.patcog.2020.107524">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the improvement of multi-view data collection technology, multi-view learning has become a hot research area. How to deal with diverse and complex data is one of the challenging problems in multi-view learning. However, it is hard for traditional multi-view subspace learning methods to find an effective subspace dimension and deal with outliers simultaneously. In this paper, we propose a novel method, named as Multi-view Subspace Learning via Bidirectional Sparsity(SLBS), which is effective to overcome the above difficulties and learn a better representation. Specifically, we divide the shared subspace into two parts. One is a row sparse matrix to do a secondary extraction of features and the other is a column sparse matrix to reduce the influence of outliers. The proposed model is a non-convex problem which is difficult to be solved. To address this problem, we develop an efficient algorithm and analyze its convergence and computational complexity . Finally, compared with other multi-view subspace learning methods, the extensive experimental results on real-world datasets present the effectiveness of our SLBS.},
  archive      = {J_PR},
  author       = {Ruidong Fan and Tingjin Luo and Wenzhang Zhuge and Sheng Qiang and Chenping Hou},
  doi          = {10.1016/j.patcog.2020.107524},
  journal      = {Pattern Recognition},
  pages        = {107524},
  shortjournal = {Pattern Recognition},
  title        = {Multi-view subspace learning via bidirectional sparsity},
  volume       = {108},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spectral clustering via ensemble deep autoencoder learning
(SC-EDAE). <em>PR</em>, <em>108</em>, 107522. (<a
href="https://doi.org/10.1016/j.patcog.2020.107522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several works have studied clustering strategies that combine classical clustering algorithms and deep learning methods. These strategies generally improve clustering performance, however deep autoencoder setting issues impede the robustness of these approaches. To alleviate the impact of hyperparameters setting, we propose a model which combines spectral clustering and deep autoencoder strengths in an ensemble framework. Our proposal does not require any pretraining and includes the three following steps: generating various deep embeddings from the original data, constructing a sparse and low-dimensional ensemble affinity matrix based on anchors strategy and applying spectral clustering to obtain the common space shared by multiple deep representations. While the anchors strategy ensures an efficient merging of the encodings, the fusion of various deep representations enables to mitigate the deep networks setting issues. Experiments on various benchmark datasets demonstrate the potential and robustness of our approach compared to state-of-the-art deep clustering methods .},
  archive      = {J_PR},
  author       = {Séverine Affeldt and Lazhar Labiod and Mohamed Nadif},
  doi          = {10.1016/j.patcog.2020.107522},
  journal      = {Pattern Recognition},
  pages        = {107522},
  shortjournal = {Pattern Recognition},
  title        = {Spectral clustering via ensemble deep autoencoder learning (SC-EDAE)},
  volume       = {108},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On unsupervised simultaneous kernel learning and data
clustering. <em>PR</em>, <em>108</em>, 107518. (<a
href="https://doi.org/10.1016/j.patcog.2020.107518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel optimization framework for joint unsupervised clustering and kernel learning is derived. Sparse nonnegative matrix factorization of kernel covariance matrices is utilized to categorize data according to their information content. It is demonstrated that a pertinent kernel covariance matrix for clustering can be constructed such that it is block diagonal within arbitrary row and column permutations, while each diagonal block has rank one. To achieve this, a linear combination of a dictionary of kernels is sought such that it has rank equal to the number of clusters while a certain kernel eigenvalue is maximized by a novel difference of convex functions formulation. We establish that the proposed algorithm converges to a stationary solution. Numerical tests with different datasets demonstrate the effectiveness of the proposed scheme whose performance is very close to supervised methods, and performs better than unsupervised alternatives without the need of painstaking parameter tuning.},
  archive      = {J_PR},
  author       = {Akshay Malhotra and Ioannis D. Schizas},
  doi          = {10.1016/j.patcog.2020.107518},
  journal      = {Pattern Recognition},
  pages        = {107518},
  shortjournal = {Pattern Recognition},
  title        = {On unsupervised simultaneous kernel learning and data clustering},
  volume       = {108},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accurate, data-efficient, unconstrained text recognition
with convolutional neural networks. <em>PR</em>, <em>108</em>, 107482.
(<a href="https://doi.org/10.1016/j.patcog.2020.107482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unconstrained text recognition is an important computer vision task, featuring a wide variety of different sub-tasks, each with its own set of challenges. One of the biggest promises of deep neural networks has been the convergence and automation of feature extractors from input raw signals, allowing for the highest possible performance with minimum required domain knowledge. To this end, we propose a data-efficient, end-to-end neural network model for generic, unconstrained text recognition. In our proposed architecture we strive for simplicity and efficiency without sacrificing recognition accuracy. Our proposed architecture is a fully convolutional network without any recurrent connections trained with the CTC loss function. Thus it operates on arbitrary input sizes and produces strings of arbitrary length in a very efficient and parallelizable manner. We show the generality and superiority of our proposed text recognition architecture by achieving state-of-the-art results on seven public benchmark datasets, covering a wide spectrum of text recognition tasks, namely: Handwriting Recognition, CAPTCHA recognition, OCR , License Plate Recognition , and Scene Text Recognition. Our proposed architecture has won the ICFHR2018 Competition on Automated Text Recognition on a READ Dataset.},
  archive      = {J_PR},
  author       = {Mohamed Yousef and Khaled F. Hussain and Usama S. Mohammed},
  doi          = {10.1016/j.patcog.2020.107482},
  journal      = {Pattern Recognition},
  pages        = {107482},
  shortjournal = {Pattern Recognition},
  title        = {Accurate, data-efficient, unconstrained text recognition with convolutional neural networks},
  volume       = {108},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Abnormal event detection in surveillance videos based on
low-rank and compact coefficient dictionary learning. <em>PR</em>,
<em>108</em>, 107355. (<a
href="https://doi.org/10.1016/j.patcog.2020.107355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a novel approach to abnormal event detection in crowded scenes is presented based on a new low-rank and compact coefficient dictionary learning (LRCCDL) algorithm. First, based on the background subtraction and binarization of surveillance videos , we construct a feature space by extracting the histogram of maximal optical flow projection (HMOFP) feature of the foreground from a normal training frame set. Second, in the training stage, a new joint optimization of the nuclear-norm and l 2, 1 -norm is applied to obtain a compact coefficient low-rank dictionary. Third, in the detection stage, l 2, 1 -norm optimization is utilized to obtain the reconstruction coefficient vectors of the testing samples. Note that the l 2, 1 -norm forces the reconstruction vectors of all the testing samples to compactly surround the same center in the training stage, such that the reconstruction errors of abnormal testing samples are different from those of normal ones. Finally, a reconstruction cost (RC) is introduced to detect abnormal frames. Experimental results on both global and local abnormal event detection show the effectiveness of our algorithm. Based on comparisons with state-of-the-art methods employing various criteria, the proposed algorithm achieves comparable detection results.},
  archive      = {J_PR},
  author       = {Ang Li and Zhenjiang Miao and Yigang Cen and Xiao-Ping Zhang and Linna Zhang and Shiming Chen},
  doi          = {10.1016/j.patcog.2020.107355},
  journal      = {Pattern Recognition},
  pages        = {107355},
  shortjournal = {Pattern Recognition},
  title        = {Abnormal event detection in surveillance videos based on low-rank and compact coefficient dictionary learning},
  volume       = {108},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Few-shot activity recognition with cross-modal memory
network. <em>PR</em>, <em>108</em>, 107348. (<a
href="https://doi.org/10.1016/j.patcog.2020.107348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning based action recognition methods require large amount of labelled training data. However, labelling large-scale video data is time consuming and tedious. In this paper, we consider a more challenging few-shot action recognition problem where the training samples are few and rare. To solve this problem, memory network has been designed to use an external memory to remember the experience learned in training and then apply it to few-shot prediction during testing. However, existing memory-based methods just update the visual information with fixed label embeddings in the memory, which cannot adapt well to novel activities during testing. To alleviate the issue, we propose a novel end-to-end cross-modal memory network for few-shot activity recognition. Specifically, the proposed memory architecture stores the dynamic visual and textual semantics for some high-level attributes related to human activities . And the learned memory can provide effective multi-modal information for new activity recognition in the testing stage. Extensive experimental results on two video datasets, including HMDB51 and UCF101, indicate that our method could achieve significant improvements over other previous methods.},
  archive      = {J_PR},
  author       = {Lingling Zhang and Xiaojun Chang and Jun Liu and Minnan Luo and Mahesh Prakash and Alexander G. Hauptmann},
  doi          = {10.1016/j.patcog.2020.107348},
  journal      = {Pattern Recognition},
  pages        = {107348},
  shortjournal = {Pattern Recognition},
  title        = {Few-shot activity recognition with cross-modal memory network},
  volume       = {108},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graph-based parallel large scale structure from motion.
<em>PR</em>, <em>107</em>, 107537. (<a
href="https://doi.org/10.1016/j.patcog.2020.107537">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While Structure from Motion achieves great success in 3D reconstruction , it still meets challenges on large scale scenes. Incremental SfM approaches are robust to outliers, but are limited by low efficiency and easy suffer from drift problem. Though Global SfM methods are more efficient than incremental approaches , they are sensitive to outliers, and would also meet memory limitation and time bottleneck. In this work, large scale SfM is deemed as a graph problem, where graph are respectively constructed in image clustering step and local reconstructions merging step. By leveraging the graph structure, we are able to handle large scale dataset in divide-and-conquer manner. Firstly, images are modelled as graph nodes , with edges are retrieved from geometric information after feature matching. Then images are divided into independent clusters by a image clustering algorithm , and followed by a subgraph expansion step , the connection and completeness of scenes are enhanced by walking along a maximum spanning tree, which is utilized to construct overlapping images between clusters. Secondly, Image clusters are distributed into servers to execute SfM in parallel mode. Thirdly, after local reconstructions complete, we construct a minimum spanning tree to find accurate similarity transformations. Then the minimum spanning tree is transformed into a Minimum Height Tree to find a proper anchor node , and is further utilized to prevent error accumulation. We evaluate our approach on various kinds of datasets and our approach shows superiority over the state-of-the-art in accuracy and efficiency. Our algorithm is open-sourced in https://github.com/AIBluefisher/GraphSfM .},
  archive      = {J_PR},
  author       = {Yu Chen and Shuhan Shen and Yisong Chen and Guoping Wang},
  doi          = {10.1016/j.patcog.2020.107537},
  journal      = {Pattern Recognition},
  pages        = {107537},
  shortjournal = {Pattern Recognition},
  title        = {Graph-based parallel large scale structure from motion},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-supervised deep reconstruction of mixed strip-shredded
text documents. <em>PR</em>, <em>107</em>, 107535. (<a
href="https://doi.org/10.1016/j.patcog.2020.107535">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The reconstruction of shredded documents consists of coherently arranging fragments of paper (shreds) to recover the original document(s). A great challenge in computational reconstruction is to properly evaluate the compatibility between the shreds. While traditional pixel-based approaches are not robust to real shredding, more sophisticated solutions compromise significantly time performance. The solution presented in this work extends our previous deep learning method for single-page reconstruction to a more realistic/complex scenario: the reconstruction of several mixed shredded documents at once. In our approach, the compatibility evaluation is modeled as a two-class (valid or invalid) pattern recognition problem. The model is trained in a self-supervised manner on samples extracted from simulated-shredded documents, which obviates manual annotation. Experimental results on three datasets – including a new collection of 100 strip-shredded documents produced for this work – have shown that the proposed method outperforms the competing ones on complex scenarios, achieving accuracy superior to 90\%.},
  archive      = {J_PR},
  author       = {Thiago M. Paixão and Rodrigo F. Berriel and Maria C.S. Boeres and Alessandro L. Koerich and Claudine Badue and Alberto F. De Souza and Thiago Oliveira-Santos},
  doi          = {10.1016/j.patcog.2020.107535},
  journal      = {Pattern Recognition},
  pages        = {107535},
  shortjournal = {Pattern Recognition},
  title        = {Self-supervised deep reconstruction of mixed strip-shredded text documents},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modality adversarial neural network for visible-thermal
person re-identification. <em>PR</em>, <em>107</em>, 107533. (<a
href="https://doi.org/10.1016/j.patcog.2020.107533">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing Visible-Thermal Person Re-identification (VT-REID) methods usually adopt two-stream networks for cross-modality images. The two streams are trained to extract features from different modality images respectively. In contrast, we design a Modality Adversarial Neural Network (MANN) to solve VT-REID problem. Our proposed MANN includes a one-stream feature extractor and a modality discriminator . The heterogeneous images are processed by the feature extractor to generate modality-invariant features. And the designed modality discriminator aims to distinguish whether the extracted features are from visible or thermal modality. Moreover, our advanced dual-constrained triplet loss is introduced for better cross-modality matching performance. The experiments on two cross-modality person re-identification datasets show that MANN can effectively learn modality-invariant features and outperform state-of-the-art methods by a large margin.},
  archive      = {J_PR},
  author       = {Yi Hao and Jie Li and Nannan Wang and Xinbo Gao},
  doi          = {10.1016/j.patcog.2020.107533},
  journal      = {Pattern Recognition},
  pages        = {107533},
  shortjournal = {Pattern Recognition},
  title        = {Modality adversarial neural network for visible-thermal person re-identification},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two-stage knowledge transfer framework for image
classification. <em>PR</em>, <em>107</em>, 107529. (<a
href="https://doi.org/10.1016/j.patcog.2020.107529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The two-stage strategy has been widely used in image classification . However, these methods barely take the classification criteria of the first stage into consideration in the second prediction stage. In this paper, we propose a novel Two-Stage Representation method (TSR), and convert it to a Single-Teacher Single-Student (STSS) problem in our two-stage knowledge transfer framework for image classification. Specifically, the first stage classifier is formulated as the teacher, which holds the ‘gate value’ to supervise the student classifier in the second stage. To transfer knowledge from the teacher classifier, we seek the nearest neighbours of the test sample to generate a set of candidate target classes in the first stage. Then, a student classifier learns from the samples belonging to these candidate classes in the second stage. Under the supervision of the teacher classifier, the teacher approves the student only if it obtains a higher score than the ‘gate value’. In actuality, the proposed framework generates a stronger classifier by staging two weaker classifiers in a novel way. The experiments on several databases show that our proposed framework is effective, which outperforms multiple popular classification methods.},
  archive      = {J_PR},
  author       = {Jianhang Zhou and Shaoning Zeng and Bob Zhang},
  doi          = {10.1016/j.patcog.2020.107529},
  journal      = {Pattern Recognition},
  pages        = {107529},
  shortjournal = {Pattern Recognition},
  title        = {Two-stage knowledge transfer framework for image classification},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Counter-examples generation from a positive unlabeled image
dataset. <em>PR</em>, <em>107</em>, 107527. (<a
href="https://doi.org/10.1016/j.patcog.2020.107527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the problem of positive unlabeled (PU) learning. In this context, we propose a two-stage GAN-based model. More specifically, the main contribution is to incorporate a biased PU risk within the standard GAN discriminator loss function. In this manner, the discriminator is constrained to steer the generator to converge towards the unlabeled samples distribution while diverging from the positive samples distribution. Consequently, the proposed model, referred to as D-GAN, exclusively learns the counter-examples distribution without prior knowledge. Experimental results on simple and complex image datasets demonstrate that our approach outperforms state-of-the-art PU methods without prior by overcoming issues such as sensitivity to prior knowledge or first-stage overfitting.},
  archive      = {J_PR},
  author       = {Florent Chiaroni and Ghazaleh Khodabandelou and Mohamed-Cherif Rahal and Nicolas Hueber and Frederic Dufaux},
  doi          = {10.1016/j.patcog.2020.107527},
  journal      = {Pattern Recognition},
  pages        = {107527},
  shortjournal = {Pattern Recognition},
  title        = {Counter-examples generation from a positive unlabeled image dataset},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel strategy to balance the results of cross-modal
hashing. <em>PR</em>, <em>107</em>, 107523. (<a
href="https://doi.org/10.1016/j.patcog.2020.107523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing methods for cross-modal retrieval has drawn increasing research interests and has been widely studied in recent years due to the explosive growth of multimedia big data. However, a significant phenomenon which has been ignored is that there is a large gap between the results of cross-modal hashing in most cases. For example, the results of Text-to-Image frequently outperform that of Image-to-Text with a large margin. In this paper, we propose a strategy named semantic augmentation to improve and balance the results of cross-modal hashing. An intermediate semantic space is constructed to re-align the feature representations that embedded with weak semantic information. By using the intermediate semantic space, the semantic information of visual features can be further augmented before being sent to cross-modal hashing algorithms . Extensive experiments are carried out on four datasets via seven state-of-the-art cross-modal hashing methods. Compared against the results without semantic augmentation, the Image-to-Text results of these methods with semantic augmentation are improved considerably, which demonstrates the effectiveness of the proposed semantic augmentation strategy in bridging the gap between the results of cross-modal retrieval. Additional experiments are conducted on the real-valued, semi-supervised, semi-paired, partial-paired, and unpaired cross-modal retrieval methods , the results further indicates the effectiveness of our strategy in improving performance of cross-modal retrieval.},
  archive      = {J_PR},
  author       = {Fangming Zhong and Zhikui Chen and Geyong Min and Feng Xia},
  doi          = {10.1016/j.patcog.2020.107523},
  journal      = {Pattern Recognition},
  pages        = {107523},
  shortjournal = {Pattern Recognition},
  title        = {A novel strategy to balance the results of cross-modal hashing},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A linear multivariate binary decision tree classifier based
on k-means splitting. <em>PR</em>, <em>107</em>, 107521. (<a
href="https://doi.org/10.1016/j.patcog.2020.107521">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel linear multivariate decision tree classifier , Binary Decision Tree based on K-means Splitting (BDTKS), is presented in this paper. The unsupervised K-means clustering is recursively integrated into the binary tree, building a hierarchical classifier. The introduction of the unsupervised K-means clustering provides the powerful generalization ability for the resulting BDTKS model. Then, the good generalization ability of BDTKS ensures the classification performance. A novel non-split condition with an easy-setting hyperparameter which focuses more on minority classes of the current node is proposed and applied in the BDTKS model, avoiding ignoring the minority classes in the class imbalance cases. Furthermore, the K-means centroid based BDTKS model is converted into the hyperplane based decision tree, speeding up the process of classification. Extensive experiments on the publicly available data sets have demonstrated that the proposed BDTKS matches or outperforms the previous decision trees.},
  archive      = {J_PR},
  author       = {Fei Wang and Quan Wang and Feiping Nie and Zhongheng Li and Weizhong Yu and Fuji Ren},
  doi          = {10.1016/j.patcog.2020.107521},
  journal      = {Pattern Recognition},
  pages        = {107521},
  shortjournal = {Pattern Recognition},
  title        = {A linear multivariate binary decision tree classifier based on K-means splitting},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Active contour model for inhomogenous image segmentation
based on jeffreys divergence. <em>PR</em>, <em>107</em>, 107520. (<a
href="https://doi.org/10.1016/j.patcog.2020.107520">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inhomogenous image segmentation has been a research challenge in recent years. To deal with this difficulty, we propose a new local and global active contour model based on Jeffreys divergence. First, unlike the local data fitting energy of the region-scalable fitting model, a new local data fitting energy based on Jeffreys divergence is proposed instead of Euclidean distance , which achieves relatively better segmentation. Second, to improve the versatility of the model, a new global data fitting energy based on Jeffreys divergence is proposed. Finally, the adaptive weights of the local and global data fitting energies are developed to increase the robustness to the initial curve. Experiments on real-world and medical images with inhomogeneities indicate that the proposed model can obtain accurate segmentation results efficiently and is not strictly dependent on setting up initial curves.},
  archive      = {J_PR},
  author       = {Bin Han and Yiquan Wu},
  doi          = {10.1016/j.patcog.2020.107520},
  journal      = {Pattern Recognition},
  pages        = {107520},
  shortjournal = {Pattern Recognition},
  title        = {Active contour model for inhomogenous image segmentation based on jeffreys divergence},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Complex heterogeneity learning: A theoretical and empirical
study. <em>PR</em>, <em>107</em>, 107519. (<a
href="https://doi.org/10.1016/j.patcog.2020.107519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data heterogeneity such as task heterogeneity, view heterogeneity, and instance heterogeneity often co-exist in many real-world applications including insider threat detection, traffic prediction , brain image analysis, quality control in manufacturing processes , etc. However, most of the existing techniques might not take fully advantage of the rich heterogeneity. To address this problem, we propose a novel graph-based approach named M 3 to simultaneously model triple heterogeneity in a principled framework. The main idea is to employ the hybrid graphs to jointly model the task relatedness, view consistency, and bag-instance correlation by enhancing the labeling consistency between nearby nodes on the graphs. Furthermore, we analyze the generalization performance of the proposed method based on Rademacher complexity, which sheds light on the benefits of jointly modeling multiple types of heterogeneity. The resulting optimization problem is challenging since the objective function is non-smooth and non-convex. We propose an iterative algorithm based on block coordinate descent and bundle method to solve the problem. Experimental results on various datasets demonstrate the effectiveness of the proposed method.},
  archive      = {J_PR},
  author       = {Pei Yang and Qi Tan and Jingrui He},
  doi          = {10.1016/j.patcog.2020.107519},
  journal      = {Pattern Recognition},
  pages        = {107519},
  shortjournal = {Pattern Recognition},
  title        = {Complex heterogeneity learning: A theoretical and empirical study},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accelerating information entropy-based feature selection
using rough set theory with classified nested equivalence classes.
<em>PR</em>, <em>107</em>, 107517. (<a
href="https://doi.org/10.1016/j.patcog.2020.107517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature selection effectively reduces the dimensionality of data. For feature selection, rough set theory offers a systematic theoretical framework based on consistency measures, of which information entropy is one of the most important significance measures of attributes. However, an information-entropy-based significance measure is computationally expensive and requires repeated calculations. Although many accelerating strategies have been proposed thus far, there remains a bottleneck when using an information-entropy-based feature selection algorithm to handle large-scale datasets with high dimensions. In this study, we introduce a c lassified n ested e quivalence c lass (CNEC)-based approach to calculate the information-entropy-based significance for feature selection using rough set theory. The proposed method extracts knowledge of the reducts of a decision table to reduce the universe and construct CNECs. By exploring the properties of different types of CNECs, we can not only accelerate both outer and inner significance calculation by discarding useless CNECs but also effectively decrease the number of inner significance calculations by using one type of CNECs. The use of CNECs is shown to significantly enhance three representative entropy-based feature selection algorithms that use rough set theory. The feature subset selected by the CNEC-based algorithms is the same as that selected by algorithms using the original definition of information entropies. Experiments conducted using 31 datasets from multiple sources, such as the UCI repository and KDD Cup competition, including large-scale and high-dimensional datasets, confirm the efficiency and effectiveness of the proposed method.},
  archive      = {J_PR},
  author       = {Jie Zhao and Jia-ming Liang and Zhen-ning Dong and De-yu Tang and Zhen Liu},
  doi          = {10.1016/j.patcog.2020.107517},
  journal      = {Pattern Recognition},
  pages        = {107517},
  shortjournal = {Pattern Recognition},
  title        = {Accelerating information entropy-based feature selection using rough set theory with classified nested equivalence classes},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sparse regularized low-rank tensor regression with
applications in genomic data analysis. <em>PR</em>, <em>107</em>,
107516. (<a href="https://doi.org/10.1016/j.patcog.2020.107516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many applications in biomedical informatics deal with data in the tensor form. Traditional regression methods which take vectors as covariates may encounter difficulties in handling tensors due to their ultrahigh dimensionality and complex structure. In this paper, we introduce a novel sparse regularized Tucker tensor regression model to exploit the structure of tensor covariates and perform feature selection on tensor data. Based on Tucker decomposition of the regression coefficient tensor, we reduce the ultrahigh dimensionality to a manageable level. To make our model identifiable, we impose the orthonormality constraint on the factor matrices. Unlike previous tensor regression models that impose sparse penalty on the factor matrices of the coefficient tensor, our model directly imposes sparse penalty on the coefficient tensor to select the relevant features on tensor data. An efficient optimization algorithm based on alternating direction method of multiplier (ADMM) algorithm is designed to solve our proposed model. The performance of our model is evaluated on both synthetic and real genomic data. Experiment results on synthetic data demonstrate that our model could identify the true related signals more accurately than other state-of-the-art regression models. The analysis on genomic data of melanoma demonstrates that our model can achieve better prediction performance and identify markers with important implications. Our model and the associated studies can provide useful insights to disease or pathogenesis mechanisms, and will benefit further studies in variable selection.},
  archive      = {J_PR},
  author       = {Le Ou-Yang and Xiao-Fei Zhang and Hong Yan},
  doi          = {10.1016/j.patcog.2020.107516},
  journal      = {Pattern Recognition},
  pages        = {107516},
  shortjournal = {Pattern Recognition},
  title        = {Sparse regularized low-rank tensor regression with applications in genomic data analysis},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast sparse coding networks for anomaly detection in videos.
<em>PR</em>, <em>107</em>, 107515. (<a
href="https://doi.org/10.1016/j.patcog.2020.107515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The semi-supervised video anomaly detection assumes that only normal video clips are available for training. Therefore, the intuitive idea is either to learn a dictionary by sparse coding or to train encoding-decoding neural networks by minimizing the reconstruction errors. For the former, the optimization of sparse coefficients is extremely time-consuming. For the latter, this manner cannot guarantee that an abnormal data corresponds to a larger reconstruction error due to the strong generalization of neural networks. To remedy their weaknesses and leverage their strengths, we propose a Fast Sparse Coding Network (FSCN) based on High-level Features. First, we propose a two-stream neural network to extract Spatial-Temporal Fusion Features (STFF) in hidden layers. With the STFF at hand, we use a Fast Sparse Coding Network to build a normal dictionary. By leveraging the predictor to produce approximate sparse coefficients, our FSCN generates sparse coefficients within a forward pass, which is simple and computationally efficient. Compared with traditional sparse coding based methods, FSCN is hundreds of or even thousands of times faster at the test stage. Extensive experiments on benchmark datasets demonstrate that our method reaches the state-of-the-art level. 1},
  archive      = {J_PR},
  author       = {Peng Wu and Jing Liu and Mingming Li and Yujia Sun and Fang Shen},
  doi          = {10.1016/j.patcog.2020.107515},
  journal      = {Pattern Recognition},
  pages        = {107515},
  shortjournal = {Pattern Recognition},
  title        = {Fast sparse coding networks for anomaly detection in videos},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dirichlet variational autoencoder. <em>PR</em>,
<em>107</em>, 107514. (<a
href="https://doi.org/10.1016/j.patcog.2020.107514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes Dirichlet Variational Autoencoder (DirVAE) using a Dirichlet prior. To infer the parameters of DirVAE, we utilize the stochastic gradient method by approximating the inverse cumulative distribution function of the Gamma distribution , which is a component of the Dirichlet distribution. This approximation on a new prior led an investigation on the component collapsing, and DirVAE revealed that the component collapsing originates from two problem sources: decoder weight collapsing and latent value collapsing. The experimental results show that 1) DirVAE generates the result with the best log-likelihood compared to the baselines; 2) DirVAE produces more interpretable latent values with no collapsing issues which the baselines suffer from; 3) the latent representation from DirVAE achieves the best classification accuracy in the (semi-)supervised classification tasks on MNIST, OMNIGLOT, COIL-20, SVHN, and CIFAR-10 compared to the baseline VAEs; and 4) the DirVAE augmented topic models show better performances in most cases.},
  archive      = {J_PR},
  author       = {Weonyoung Joo and Wonsung Lee and Sungrae Park and Il-Chul Moon},
  doi          = {10.1016/j.patcog.2020.107514},
  journal      = {Pattern Recognition},
  pages        = {107514},
  shortjournal = {Pattern Recognition},
  title        = {Dirichlet variational autoencoder},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Depth image super-resolution using correlation-controlled
color guidance and multi-scale symmetric network. <em>PR</em>,
<em>107</em>, 107513. (<a
href="https://doi.org/10.1016/j.patcog.2020.107513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth image super-resolution (DISR) is an effective solution to improve the quality of depth images captured by real world low-cost cameras. In this paper, we propose a multi-scale symmetric network with the correlation-controlled color guidance block (CCGB) for DISR. The proposed network consists of two multi-scale sub-networks to respectively provide guidance and estimate depth. A symmetric unit (SU), which is a mini-encoder-decoder structure with residual learning, is designed and used as a basic network atom. The encoder part in SU aims to extract essential features, while the decoder part works to restore edge details. The way the SU processes information matches well with the textureless and sharp-edge characteristics of depth images. The two sub-networks present a high-level symmetric structure connected by dense guidance links in between. Based on the correlation analyses between the two sub-networks, each guidance link will transfer information trough a CCGB designed to implement channel-wise re-weighting mechanism. The accurate color guidance from CCGB helps avoiding artifacts introduced by non-co-occurrence of depth discontinuities and color edges. Experimental results demonstrate the superiority of the proposed method over several state-of-the-art DISR works.},
  archive      = {J_PR},
  author       = {Tao Li and Hongwei Lin and Xiucheng Dong and Xiaohua Zhang},
  doi          = {10.1016/j.patcog.2020.107513},
  journal      = {Pattern Recognition},
  pages        = {107513},
  shortjournal = {Pattern Recognition},
  title        = {Depth image super-resolution using correlation-controlled color guidance and multi-scale symmetric network},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distractor-aware discrimination learning for online multiple
object tracking. <em>PR</em>, <em>107</em>, 107512. (<a
href="https://doi.org/10.1016/j.patcog.2020.107512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online multi-object tracking needs to overcome the intrinsic detector deficiencies, e.g., missing detections, false alarms, and inaccurate detection responses, to grow multiple object trajectories without using future information. Various distractions exist during this growing process like background clutters, similar targets, and occlusions, which present a great challenge. We in this work propose a method for learning a distractor-aware discriminative model that can handle continuous missed and inaccurate detection problems due to the occlusion or the motion blur . To deal with target appearance variations, a relational attention learning mechanism is proposed to capture the distinctive target appearances by selectively aggregating features from history states with weights extracted from their appearance topological relationship . Based on the discrimination model, a multi-stage tracking pipeline is designed for automatic trajectory initialization,propagation, and termination. Extensive experimental analyses and comparisons demonstrate its state-of-the-art performance on widely used challenging MOT16 and MOT17 benchmarks. The source code of this work is released to facilitate further studies on the multi-object tracking problem. 1},
  archive      = {J_PR},
  author       = {Zongwei Zhou and Wenhan Luo and Qiang Wang and Junliang Xing and Weiming Hu},
  doi          = {10.1016/j.patcog.2020.107512},
  journal      = {Pattern Recognition},
  pages        = {107512},
  shortjournal = {Pattern Recognition},
  title        = {Distractor-aware discrimination learning for online multiple object tracking},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Skeleton-based action recognition with hierarchical spatial
reasoning and temporal stack learning network. <em>PR</em>,
<em>107</em>, 107511. (<a
href="https://doi.org/10.1016/j.patcog.2020.107511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based action recognition aims to recognize human actions by exploring the inherent characteristics from the given skeleton sequences and has attracted far more attention due to its great important potentials in practical applications. Previous methods have illustrated that learning discriminative spatial and temporal features from the skeleton sequences is a crucial factor to recognize human actions. Nevertheless, how to model spatio-temporal evolutions is still a challenging problem. In this work, we propose a novel model with hierarchical spatial reasoning and temporal stack learning network (HSR-TSL) to explore the discriminative spatial and temporal features for human action recognition , which consists of a hierarchical spatial reasoning network (HSRN) and a temporal stack learning network (TSLN). Specifically, the HSRN employs a hierarchical residual graph neural network to capture two-level spatial features : intra spatial information of each part and body-level structural information between each part. The TSLN models the detailed temporal dynamics of skeleton sequences by a composition of multiple skip-clip LSTMs . During training, we develop a clip-based incremental loss to effectively optimize the model. We perform extensive experiments on five challenging benchmarks to verify the effectiveness of each component of our model. The comparison results illustrate that our approach significantly boosts the performances for skeleton-based action recognition.},
  archive      = {J_PR},
  author       = {Chenyang Si and Ya Jing and Wei Wang and Liang Wang and Tieniu Tan},
  doi          = {10.1016/j.patcog.2020.107511},
  journal      = {Pattern Recognition},
  pages        = {107511},
  shortjournal = {Pattern Recognition},
  title        = {Skeleton-based action recognition with hierarchical spatial reasoning and temporal stack learning network},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient sampling-based energy function evaluation for
ensemble optimization using simulated annealing. <em>PR</em>,
<em>107</em>, 107510. (<a
href="https://doi.org/10.1016/j.patcog.2020.107510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we attempted to develop a method for accelerating parameter optimization of an object detector ensemble over large image datasets by using simulated annealing. We propose a novel sampling-based evaluation method that considers the minimum portion of the dataset required in each iteration to maintain solution quality. This approach can be considered a noisy evaluation of the energy. The sample sizes required during the search process are theoretically determined by adapting the convergence results for noisy evaluation. To determine applicability, we prepared and optimized two ensembles for diabetic retinopathy pre-screening based on microaneurysm detection with convolutional neural network-based and traditional object detectors. Our experimental results indicate that the proposed sampling-based evaluation method substantially reduced the computational time required for optimizing the parameters of the ensembles while preserving solution quality.},
  archive      = {J_PR},
  author       = {János Tóth and Henrietta Tomán and András Hajdu},
  doi          = {10.1016/j.patcog.2020.107510},
  journal      = {Pattern Recognition},
  pages        = {107510},
  shortjournal = {Pattern Recognition},
  title        = {Efficient sampling-based energy function evaluation for ensemble optimization using simulated annealing},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the stability of persistent entropy and new summary
functions for topological data analysis. <em>PR</em>, <em>107</em>,
107509. (<a href="https://doi.org/10.1016/j.patcog.2020.107509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Persistent homology and persistent entropy have recently become useful tools for patter recognition. In this paper, we find requirements under which persistent entropy is stable to small perturbations in the input data and scale invariant . In addition, we describe two new stable summary functions combining persistent entropy and the Betti curve. Finally, we use the previously defined summary functions in a material classification task to show their usefulness in machine learning and pattern recognition.},
  archive      = {J_PR},
  author       = {Nieves Atienza and Rocio Gonzalez-Díaz and Manuel Soriano-Trigueros},
  doi          = {10.1016/j.patcog.2020.107509},
  journal      = {Pattern Recognition},
  pages        = {107509},
  shortjournal = {Pattern Recognition},
  title        = {On the stability of persistent entropy and new summary functions for topological data analysis},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonlinear dimensionality reduction for clustering.
<em>PR</em>, <em>107</em>, 107508. (<a
href="https://doi.org/10.1016/j.patcog.2020.107508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce an approach to divisive hierarchical clustering that is capable of identifying clusters in nonlinear manifolds. This approach uses the isometric mapping (Isomap) to recursively embed (subsets of) the data in one dimension, and then performs a binary partition designed to avoid the splitting of clusters. We provide a theoretical analysis of the conditions under which contiguous and high-density clusters in the original space are guaranteed to be separable in the one-dimensional embedding. To the best of our knowledge there is little prior work that studies this problem. Extensive experiments on simulated and real data sets show that hierarchical divisive clustering algorithms derived from this approach are effective.},
  archive      = {J_PR},
  author       = {Sotiris Tasoulis and Nicos G. Pavlidis and Teemu Roos},
  doi          = {10.1016/j.patcog.2020.107508},
  journal      = {Pattern Recognition},
  pages        = {107508},
  shortjournal = {Pattern Recognition},
  title        = {Nonlinear dimensionality reduction for clustering},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The benefits of target relations: A comparison of multitask
extensions and classifier chains. <em>PR</em>, <em>107</em>, 107507. (<a
href="https://doi.org/10.1016/j.patcog.2020.107507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multitask (multi-target or multi-output) learning (MTL) deals with simultaneous prediction of several outputs. MTL approaches rely on the optimization of a joint score function over the targets. However, defining a joint score in global models is problematic when the target scales are different. To address such problems, single target (i.e. local) learning strategies are commonly employed. Here we propose alternative tree-based learning strategies to handle the issue with target scaling in global models, and to identify the learning order for chaining operations in local models. In the first proposal, the problems with target scaling are resolved using alternative splitting strategies which consider the learning tasks in a multi-objective optimization framework. The second proposal deals with the problem of ordering in the chaining strategies. We introduce an alternative estimation strategy, minimum error chain policy, that gradually expands the input space using the estimations that approximate to true characteristics of outputs, namely out-of-bag estimations in tree-based ensemble framework. Our experiments on benchmark datasets illustrate the success of the proposed multitask extension of trees compared to the decision trees with de facto design especially for datasets with large number of targets. In line with that, minimum error chain policy improves the performance of the state-of-the-art chaining policies.},
  archive      = {J_PR},
  author       = {Esra Adıyeke and Mustafa Gökçe Baydoğan},
  doi          = {10.1016/j.patcog.2020.107507},
  journal      = {Pattern Recognition},
  pages        = {107507},
  shortjournal = {Pattern Recognition},
  title        = {The benefits of target relations: A comparison of multitask extensions and classifier chains},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A sparsity-promoting image decomposition model for depth
recovery. <em>PR</em>, <em>107</em>, 107506. (<a
href="https://doi.org/10.1016/j.patcog.2020.107506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel image decomposition model for scene depth recovery from low-quality depth measurements and its corresponding high resolution color image. Through our observation, the depth map mainly contains smooth regions separated by additive step discontinuities, and can be simultaneously decomposed into a local smooth surface and an approximately piecewise constant component. Therefore, the proposed unified model combines the least square polynomial approximation (for smooth surface) and a sparsity-promoting prior (for piecewise constant) to better portray the 2D depth signal intrinsically. As we know, the representation of the piecewise constant signal in gradient domain is extremely sparse. Previous researches using total variation filter based on L 1 -norm or L p -norm (0 &lt; p &lt; 1) are both sub-optimal when addressing the tradeoff between enhancing the sparsity and keeping the model convex . We propose a novel non-convex penalty based on Moreau envelope, which promotes the prior sparsity and simultaneously maintains the convexity of the whole model for each variable. We prove the convexity of the proposed model and give the convergence analysis of the algorithm. We also introduce an iterative reweighted strategy applied on the sparsity prior to deal with the depth-color inconsistent problem and to locate the depth boundaries. Moreover, we provide an accelerated algorithm to deal with the problem of non-uniform down-sampling when transforming the depth observation matrix into the Fourier domain for fast processing. Experimental results demonstrate that the proposed method can handle various types of depth degradation and achieve promising performance in terms of recovery accuracy and running time.},
  archive      = {J_PR},
  author       = {Xinchen Ye and Mingliang Zhang and Jingyu Yang and Xin Fan and Fangfang Guo},
  doi          = {10.1016/j.patcog.2020.107506},
  journal      = {Pattern Recognition},
  pages        = {107506},
  shortjournal = {Pattern Recognition},
  title        = {A sparsity-promoting image decomposition model for depth recovery},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Low-rank quaternion tensor completion for recovering color
videos and images. <em>PR</em>, <em>107</em>, 107505. (<a
href="https://doi.org/10.1016/j.patcog.2020.107505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-rank quaternion tensor completion method, a novel approach to recovery color videos and images, is proposed in this paper. We respectively reconstruct a color image and a color video as a quaternion matrix (second-order tensor) and a third-order quaternion tensor by encoding the red, green, and blue channel pixel values on the three imaginary parts of a quaternion. Different from some traditional models which treat color pixel as a scalar and represent color channels separately, whereas, during the quaternion-based reconstruction, it is significant that the inherent color structures of color images and color videos can be completely preserved. Under the definition of Tucker rank, the global low-rank prior to quaternion tensor is encoded as the nuclear norm of unfolding quaternion matrices. Then, by applying the ADMM framework, we provide the tensor completion algorithm for any order ( ≥ 2) quaternion tensors, which theoretically can be well used to recover missing entries of any multidimensional data with color structures. Simulation results for color videos and color images recovery show the superior performance and efficiency of the proposed method over some state-of-the-art existing ones.},
  archive      = {J_PR},
  author       = {Jifei Miao and Kit Ian Kou and Wankai Liu},
  doi          = {10.1016/j.patcog.2020.107505},
  journal      = {Pattern Recognition},
  pages        = {107505},
  shortjournal = {Pattern Recognition},
  title        = {Low-rank quaternion tensor completion for recovering color videos and images},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A divide-and-conquer strategy for facial landmark detection
using dual-task CNN architecture. <em>PR</em>, <em>107</em>, 107504. (<a
href="https://doi.org/10.1016/j.patcog.2020.107504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel deep learning-based framework for facial landmark detection. This framework takes as input face image returned by a face detector (Faster R-CNN) and generates as output a set of landmarks positions. Prior CNN-based methods often select randomly small local patches to predict an initial guess of landmarks locations. One issue with these local patches is that the adjacent landmarks might share the same regions due to the overlapping, thus, they might not convey precise information of each individual landmark . By contrast, our approach formulates this problem as a divide-conquer search for facial patches using CNN architecture in a hierarchy, where the input face image is recursively split into two cohesive non-overlapped subparts until each one contains only the region around the expected landmark. To attain better division of face topology, the search is carried out in a structured coarse-to-fine manner, where a learned hierarchical model of the face defining the granularity of each division level is introduced. We also propose a cascaded regressor to detect and refine the position of the individual landmark in each predicted non-overlapped patch. We adopt a carefully designed shallow CNN architecture so that to improve real-time performance. In addition, unlike previous cascaded methods, our regressor does not require auxiliary input such as initial landmarks locations. Extensive experiments on several challenging datasets (including MTFL, AFW, AFLW, COFW, 300W, and 300VW) show that our approach is particularly impressive in the unconstrained scenarios where it outperforms prior arts in both accuracy and efficiency.},
  archive      = {J_PR},
  author       = {Rachida Hannane and Abdessamad Elboushaki and Karim Afdel},
  doi          = {10.1016/j.patcog.2020.107504},
  journal      = {Pattern Recognition},
  pages        = {107504},
  shortjournal = {Pattern Recognition},
  title        = {A divide-and-conquer strategy for facial landmark detection using dual-task CNN architecture},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Precise detection of chinese characters in historical
documents with deep reinforcement learning. <em>PR</em>, <em>107</em>,
107503. (<a href="https://doi.org/10.1016/j.patcog.2020.107503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The decision-making ability of deep reinforcement learning has been proved successfully in a variety of fields. Here, we use this method for precise character detection by making tight bounding boxes around the Chinese characters in historical documents. An agent is trained to learn the control policy of fine-tuning a bounding box step-by-step through a Markov Decision Process . We introduce a novel f ully c onvolutional n etwork with p osition-sensitive Region-of-Interest (RoI) pooling (FCPN). The network receives character patches as input without fixed size, and it can fuse position information into the features of actions. Besides, we propose a d ense r eward f unction (DRF) that provides excellent rewards according to different actions and environment states, improving the decision-making ability of the agent. Our approach is designed as a universal method that can be applied to the output of all character-level or word-level text detectors to obtain more precise detection results. Application to the Tripitaka Koreana in Han (TKH) and Multiple Tripitaka in Han (MTH) datasets confirm the very promising performance of this method. In particular, our approach yields a significant improvement under a large Intersection over Union (IoU) of 0.8. The robustness and generality are also proved by experiments on the scene text datasets ICDAR2013 and ICDAR2015.},
  archive      = {J_PR},
  author       = {Wu Sihang and Wang Jiapeng and Ma Weihong and Jin Lianwen},
  doi          = {10.1016/j.patcog.2020.107503},
  journal      = {Pattern Recognition},
  pages        = {107503},
  shortjournal = {Pattern Recognition},
  title        = {Precise detection of chinese characters in historical documents with deep reinforcement learning},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-label classification of multi-modality skin lesion via
hyper-connected convolutional neural network. <em>PR</em>, <em>107</em>,
107502. (<a href="https://doi.org/10.1016/j.patcog.2020.107502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical and dermoscopy images (multi-modality image pairs) are routinely used sequentially in the assessment of skin lesions. Clinical images characterize a lesion&#39;s geometry and color; dermoscopy depicts vascularity, dots and globules from the sub-surface of the lesion. Together these modalities provide labels to characterize a skin lesion. Recently, convolutional neural networks (CNNs), due to the ability to learn low-level features and high-level semantic information in an end-to-end architecture, have been shown to be the state-of-the-art in skin lesion classification. Most of the CNN methods have relied on dermoscopy alone. In the few published papers that support multi-modalities, the methods are based on ‘late-fusion’ to integrate extracted clinical and dermoscopy image features separately. These late-fusion methods tend to ignore the accessible complementary image features between the paired images at the early stage of the CNN architecture. We propose a hyper-connected CNN (HcCNN) to classify skin lesions. Compared to existing multi-modality CNNs, our HcCNN has an additional hyper-branch that integrates intermediary image features in a hierarchical manner. The hyper-branch enables the network to learn more complex combinations between the images at all, early and late, stages of the network. We also coupled the HcCNN with a multi-scale attention block (MsA) to prioritize semantically important subtle regions in the two modalities across various image scales. Our HcCNN achieved an average accuracy of 74.9\% for multi-label classification on the 7-point Checklist dataset, which is a well-benchmarked public dataset. Conclusions: Our method is more accurate than the state-of-the-art methods and, in particular, our method achieved consistent and the best results in datasets with imbalanced label distributions.},
  archive      = {J_PR},
  author       = {Lei Bi and David Dagan Feng and Michael Fulham and Jinman Kim},
  doi          = {10.1016/j.patcog.2020.107502},
  journal      = {Pattern Recognition},
  pages        = {107502},
  shortjournal = {Pattern Recognition},
  title        = {Multi-label classification of multi-modality skin lesion via hyper-connected convolutional neural network},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Handling incomplete heterogeneous data using VAEs.
<em>PR</em>, <em>107</em>, 107501. (<a
href="https://doi.org/10.1016/j.patcog.2020.107501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variational autoencoders (VAEs), as well as other generative models , have been shown to be efficient and accurate for capturing the latent structure of vast amounts of complex high-dimensional data. However, existing VAEs can still not directly handle data that are heterogenous (mixed continuous and discrete) or incomplete (with missing data at random), which is indeed common in real-world applications. In this paper, we propose a general framework to design VAEs suitable for fitting incomplete heterogenous data. The proposed HI-VAE includes likelihood models for real-valued, positive real valued, interval, categorical, ordinal and count data, and allows accurate estimation (and potentially imputation) of missing data. Furthermore, HI-VAE presents competitive predictive performance in supervised tasks, outperforming supervised models when trained on incomplete data.},
  archive      = {J_PR},
  author       = {Alfredo Nazábal and Pablo M. Olmos and Zoubin Ghahramani and Isabel Valera},
  doi          = {10.1016/j.patcog.2020.107501},
  journal      = {Pattern Recognition},
  pages        = {107501},
  shortjournal = {Pattern Recognition},
  title        = {Handling incomplete heterogeneous data using VAEs},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semi-supervised learning framework based on statistical
analysis for image set classification. <em>PR</em>, <em>107</em>,
107500. (<a href="https://doi.org/10.1016/j.patcog.2020.107500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical models have been widely adopted for image set classification owing to their capacity in characterizing the data distribution more flexibly and faithfully. However, these methods typically suffer from the problem that the query image set has weak statistical correlations with the training sets, which leads to larger fluctuations in performance. To address this problem, we propose a semi-supervised fuzzy discriminative learning framework based on Log-Euclidean multivariate Gaussians descriptor to facilitate more robust image set classification. Specifically, by using the semi-supervised setting which definitely has access to the labeled training data and the available unlabeled testing data, we adopt manifold distance metric to construct a “fully trusted” graph and derive two new data dependent probabilistic kernels to strongly reflect the underlying connection relationships between the training and query Gaussian manifold components. The resulted kernel representations are eventually integrated into a kernel fuzzy discriminant framework to enhance the compactness of intra-class Gaussian components and enlarge the margin for inter-class Gaussian components . Thus, more discriminating power of our learning machine is obtained for the classification of the query image set. Extensive experiments on several datasets well demonstrate the effectiveness of the proposed method compared with other image set algorithms.},
  archive      = {J_PR},
  author       = {Wenzhu Yan and Quansen Sun and Huaijiang Sun and Yanmeng Li},
  doi          = {10.1016/j.patcog.2020.107500},
  journal      = {Pattern Recognition},
  pages        = {107500},
  shortjournal = {Pattern Recognition},
  title        = {Semi-supervised learning framework based on statistical analysis for image set classification},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cost-sensitive deep forest for price prediction.
<em>PR</em>, <em>107</em>, 107499. (<a
href="https://doi.org/10.1016/j.patcog.2020.107499">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For many real-world applications, predicting a price range is more practical and desirable than predicting a concrete value. In this case, price prediction can be regarded as a classification problem. Although deep forest is recognized as the best solution to many classification problems, a crucial issue limits its direct application to price prediction, i.e., it treated all the misclassifications equally no matter how far away they are from the real classes, since their impacts on the accuracy are the same. This is unreasonable to price prediction as the misclassification should be as close to the real price range as possible even if they have to be wrongly classified. To address this issue, we propose a cost-sensitive deep forest for price prediction, which maintains the high accuracy of deep forest, and propels the misclassifications to be closer to the real price range to reduce the cost of misclassifications. To make the classification more meaningful, we develop a discretization method to pre-define the classes of price, by modifying the conventional K-means method. The experimental results based on multiple real-world datasets (i.e., car sharing, house renting and real estate selling) show that, the cost-sensitive deep forest can significantly reduce the cost in comparison with the conventional deep forest and other baselines, while keeping satisfactory accuracy.},
  archive      = {J_PR},
  author       = {Chao Ma and Zhenbing Liu and Zhiguang Cao and Wen Song and Jie Zhang and Weiliang Zeng},
  doi          = {10.1016/j.patcog.2020.107499},
  journal      = {Pattern Recognition},
  pages        = {107499},
  shortjournal = {Pattern Recognition},
  title        = {Cost-sensitive deep forest for price prediction},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semantic segmentation using stride spatial pyramid pooling
and dual attention decoder. <em>PR</em>, <em>107</em>, 107498. (<a
href="https://doi.org/10.1016/j.patcog.2020.107498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is an end-to-end task that requires both semantic and spatial accuracy. It is important for deep learning-based segmentation methods to effectively utilize the high-level feature map whose semantic information is abundant and the low-level feature map whose spatial information is accurate. However, existing segmentation networks typically cannot take full advantage of these two kinds of feature maps, leading to inferior performance. This paper attempts to overcome this challenge by introducing two novel structures. On the one hand, we propose a structure called stride spatial pyramid pooling (SSPP) to capture multiscale semantic information from the high-level feature map. Compared with existing pyramid pooling methods based on the atrous convolution, the SSPP structure is able to gather more information from the high-level feature map with faster inference speed, which improves the utilization efficiency of the high-level feature map significantly. On the other hand, we propose a dual attention decoder consisting of a channel attention branch and a spatial attention branch to make full use of the high- and low-level feature maps simultaneously. The dual attention decoder can result in a more “semantic” low-level feature map and a high-level feature map with more accurate spatial information, which bridges the gap between these two kinds of feature maps and benefits their fusion. We evaluate the proposed model on several publicly available semantic image segmentation benchmarks including PASCAL VOC 2012, Cityscapes and COCO-Stuff. The qualitative and quantitative results demonstrate that our method can achieve the state-of-the-art performance.},
  archive      = {J_PR},
  author       = {Chengli Peng and Jiayi Ma},
  doi          = {10.1016/j.patcog.2020.107498},
  journal      = {Pattern Recognition},
  pages        = {107498},
  shortjournal = {Pattern Recognition},
  title        = {Semantic segmentation using stride spatial pyramid pooling and dual attention decoder},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Zero-shot handwritten chinese character recognition with
hierarchical decomposition embedding. <em>PR</em>, <em>107</em>, 107488.
(<a href="https://doi.org/10.1016/j.patcog.2020.107488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handwritten Chinese Character Recognition (HCCR) is a challenging topic in the field of pattern recognition due to large-scale character vocabulary, complex hierarchical structure, various writing styles, and scarce training samples. In this paper, we explored the hierarchical knowledge of Chinese characters and presented a novel zero-shot HCCR method. First, we handled the relations between the characters and their primitives, such as radicals and structures, to obtain a tree layout of primitives. Then, we presented a novel zero-shot hierarchical decomposition embedding method to encode the tree layout into a semantic vector . Next, we devised a Convolutional Neural Network (CNN) based framework to learn both radicals and structures of characters via the semantic vector . As different Chinese characters share some common radicals and structures, our method is able to recognize new categories without any labeled samples from them. Moreover, our method is effective in both traditional HCCR and zero-shot HCCR tasks. It achieves competitive performance on the traditional experiment setting and significantly surpasses the state-of-the-art methods on the zero-shot experiment setting.},
  archive      = {J_PR},
  author       = {Zhong Cao and Jiang Lu and Sen Cui and Changshui Zhang},
  doi          = {10.1016/j.patcog.2020.107488},
  journal      = {Pattern Recognition},
  pages        = {107488},
  shortjournal = {Pattern Recognition},
  title        = {Zero-shot handwritten chinese character recognition with hierarchical decomposition embedding},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-adaptive manifold discriminant analysis for feature
extraction from hyperspectral imagery. <em>PR</em>, <em>107</em>,
107487. (<a href="https://doi.org/10.1016/j.patcog.2020.107487">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional manifold learning methods generally include a single projection stage that maps high-dimensional data into lower-dimensional space. However, these methods cannot guarantee that the projection matrix is optimal for classification, which limits their practical application. To address this issue, we propose a two-stage projection matrix optimization model termed self-adaptive manifold discriminant analysis (SAMDA). In pre-training projection stage, SAMDA obtains an initial projection matrix by constructing an interclass graph and an intraclass graph under the graph embedding (GE) framework. In weight optimization stage , a maximal manifold margin criterion is developed to further optimize the weights of projection matrix by feature similarity. A self-adaptive optimization process is introduced to increase the margins among different manifolds in low-dimensional space and extract discriminant features that are beneficial to classification. Experimental results on PaviaU, Indian Pines and Heihe data sets demonstrate that the proposed SAMDA method can achieve better classification results than some state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Hong Huang and Zhengying Li and Haibo He and Yule Duan and Song Yang},
  doi          = {10.1016/j.patcog.2020.107487},
  journal      = {Pattern Recognition},
  pages        = {107487},
  shortjournal = {Pattern Recognition},
  title        = {Self-adaptive manifold discriminant analysis for feature extraction from hyperspectral imagery},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-head enhanced self-attention network for novelty
detection. <em>PR</em>, <em>107</em>, 107486. (<a
href="https://doi.org/10.1016/j.patcog.2020.107486">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One-class classification (OCC) is a classical problem in computer vision that can be described as the task of classifying outlier class samples (OC samples) from the OCC model trained on inlier class samples (IC samples) when datasets are highly biased toward one class due to the insufficient sample size of the other class. Currently, the adversarial learning OCC (ALOCC) method has been proven to significantly improve OCC performance. However, its drawbacks include instability issues and non-evident reconstruction between the IC and OC samples. Therefore, we propose multihead enhanced self-attention in the ALOCC network, thereby increasing the difference between the IC and OC samples and significantly increasing OCC accuracy compared with ALOCC accuracy. For training, we propose a new loss, called adversarial-balance loss, that effectively solves the training instability problem, further increasing OCC accuracy. The experiments show the effectiveness of the proposed method compared with state-of-art methods.},
  archive      = {J_PR},
  author       = {Yingying Zhang and Yuxin Gong and Haogang Zhu and Xiao Bai and Wenzhong Tang},
  doi          = {10.1016/j.patcog.2020.107486},
  journal      = {Pattern Recognition},
  pages        = {107486},
  shortjournal = {Pattern Recognition},
  title        = {Multi-head enhanced self-attention network for novelty detection},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Defocus map estimation from a single image using improved
likelihood feature and edge-based basis. <em>PR</em>, <em>107</em>,
107485. (<a href="https://doi.org/10.1016/j.patcog.2020.107485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Defocus map estimation (DME) is very useful in many computer vision applications and has drawn much attention in recent years. Edge-based DME methods can generate sharp defocus discontinuities but usually suffer from textures of the input image. Region-based methods are free of textures but cannot catch the defocus discontinuities very well. In this paper, we propose a DME method combining edge-based and region-based methods together to keep their respective advantages while eliminating the shortcomings. The combination is achieved via regression tree fields (RTF). In an RTF, the input feature and the linear basis are of vital importance. For our RTF, they are obtained as follows. (i) Two orthogonal gradient operators with the corresponding subsets of Gabor filters are employed in localized 2D frequency analysis to generate accurate likelihood, and the first K highest local maximums of likelihood are sent to an RTF as input feature. (ii) At the same time, the input image is processed by three edge-based methods and the results serve as the linear basis of RTF. The experiments demonstrate that the proposed method outperforms state-of-the-art DME methods. Moreover, the proposed method can be readily applied to defocused image deblurring and defocus blur detection.},
  archive      = {J_PR},
  author       = {Shaojun Liu and Qingmin Liao and Jing-Hao Xue and Fei Zhou},
  doi          = {10.1016/j.patcog.2020.107485},
  journal      = {Pattern Recognition},
  pages        = {107485},
  shortjournal = {Pattern Recognition},
  title        = {Defocus map estimation from a single image using improved likelihood feature and edge-based basis},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Attention and boundary guided salient object detection.
<em>PR</em>, <em>107</em>, 107484. (<a
href="https://doi.org/10.1016/j.patcog.2020.107484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, fully convolutional neural network (FCN) has broken all records in various vision task. It also achieves great performance in salient object detection. However, most of the state-of-the-art methods have suffered from the challenge of precisely segmenting the entire salient object with uniform region and explicit boundary and effectively suppressing the backgrounds on complex images. There is still a large room for improvement over the FCN-based saliency detection approaches. In this paper, we propose an attention and boundary guided deep neural network for salient object detection to better locate and segment the salient objects with uniform interior and explicit boundary. A channel-wise attention module is utilized to emphasize the important regions, which selects the important feature channels and assigns large weights to them. A boundary information localization module is proposed for suppressing the irrelevant boundary information to better locate and explore the useful structure of objects. The proposed approach achieves state-of-the-art performance on four well-known benchmark datasets.},
  archive      = {J_PR},
  author       = {Qing Zhang and Yanjiao Shi and Xueqin Zhang},
  doi          = {10.1016/j.patcog.2020.107484},
  journal      = {Pattern Recognition},
  pages        = {107484},
  shortjournal = {Pattern Recognition},
  title        = {Attention and boundary guided salient object detection},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). 3DSymm: Robust and accurate 3D reflection symmetry
detection. <em>PR</em>, <em>107</em>, 107483. (<a
href="https://doi.org/10.1016/j.patcog.2020.107483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reflection symmetry is a very commonly occurring feature in both natural and man-made objects, which helps in understanding objects better and makes them visually pleasing. Detection of reflection symmetry is a fundamental problem in the field of computer vision and computer graphics which aids in understanding and representing reflective symmetric objects. In this work, we attempt the problem of detecting the 3D global reflection symmetry of a 3D object represented as a point cloud. The main challenge is to handle outliers, missing parts, and perturbations from the perfect reflection symmetry. We propose a descriptor-free approach, in which, we pose the problem of reflection symmetry detection as an optimization problem and provide a closed-form solution. We show that the proposed method achieves state-of-the-art performance on the standard dataset.},
  archive      = {J_PR},
  author       = {Rajendra Nagar and Shanmuganathan Raman},
  doi          = {10.1016/j.patcog.2020.107483},
  journal      = {Pattern Recognition},
  pages        = {107483},
  shortjournal = {Pattern Recognition},
  title        = {3DSymm: Robust and accurate 3D reflection symmetry detection},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural network with multiple connection weights.
<em>PR</em>, <em>107</em>, 107481. (<a
href="https://doi.org/10.1016/j.patcog.2020.107481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biological studies have shown that the interaction between neurons are based on neurotransmitters, which transmit signals between neurons, and that one neuron sends information to another neuron by releasing a number of different neurotransmitters, which play different roles. Motivated by this biological discovery, a novel neural networks model is proposed by extending the dimension of connection weights from one to multiple, i.e. there are multiple not only one connections between each two units. The number of dimensions of connection weight represents the number of categories of neurotransmitters and different components of the weight correspond to different neurotransmitters. In order to make these neurotransmitters collaborate and compete appropriately, the input and output for each unit in our proposed model have been heuristically defined. From the biological perspective, the proposed neural network is much closer to biological neural network. From the viewpoint of new model structure, the characteristic that the activation of each hidden unit is based on several filters, can improve the interpretability of features learned by the proposed neural network. Experimental results on MNIST, NORB and several other data sets have demonstrated that the performances of traditional neural networks can be improved by extending the dimension of connection weight between units, and the idea of multiple connection weights provides a new paradigm for the design of neural networks.},
  archive      = {J_PR},
  author       = {Jiangshe Zhang and Junying Hu and Junmin Liu},
  doi          = {10.1016/j.patcog.2020.107481},
  journal      = {Pattern Recognition},
  pages        = {107481},
  shortjournal = {Pattern Recognition},
  title        = {Neural network with multiple connection weights},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). TPM: Multiple object tracking with tracklet-plane matching.
<em>PR</em>, <em>107</em>, 107480. (<a
href="https://doi.org/10.1016/j.patcog.2020.107480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple object tracking (MOT) aims to model the temporal relationship among detected objects and associate them into trajectories. Thus, one major challenge of MOT lies in the confusion from noisy object detection results. In this paper, we propose Tracklet-Plane Matching (TPM), a new approach which improves the performance of MOT by modeling and reducing the interferences from noisy or confusing object detections. TPM first constructs good temporally-related object detections into short tracklets. Then, a tracklet-plane matching process is introduced to organize related tracklets into planes and associate them into long trajectories. The tracklet-plane matching process assigns visually confusing tracklets into different tracklet planes according to their contextual information, thus properly reducing the confusion among similar tracklets. At the same time, it also allows association among temporally non-neighboring or overlapping tracklets, which provides good flexibility to handle confusion from noisy detections. Under this process, a tracklet-importance evaluation scheme and a representative-based similarity modeling scheme are introduced. These two schemes can properly evaluate the reliability of detection results and identify reliable ones during association so that the impact of noisy or confusing detections can be well-mitigated. Experimental results on benchmark datasets demonstrate that the proposed approach outperforms the state-of-the-art MOT methods.},
  archive      = {J_PR},
  author       = {Jinlong Peng and Tao Wang and Weiyao Lin and Jian Wang and John See and Shilei Wen and Erui Ding},
  doi          = {10.1016/j.patcog.2020.107480},
  journal      = {Pattern Recognition},
  pages        = {107480},
  shortjournal = {Pattern Recognition},
  title        = {TPM: Multiple object tracking with tracklet-plane matching},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint and individual matrix factorization hashing for
large-scale cross-modal retrieval. <em>PR</em>, <em>107</em>, 107479.
(<a href="https://doi.org/10.1016/j.patcog.2020.107479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal hashing methods have gained considerable attention in recent years due to their effectiveness and efficiency for cross-modal similarity searches. Existing multimodal hashing methods either learn unified hash codes for different modalities or learn individual hash codes for each modality and then explore cross-correlations between them. Generally, learning unified hash codes tends to preserve the shared properties of multimodal data and learning individual hash codes tends to preserve the specific properties of each modality. There remains a crucial bottleneck regarding how to learn hash codes that simultaneously preserve the shared properties and specific properties of multimodal data. Therefore, we present a joint and individual matrix factorization hashing (JIMFH) method, which not only learns unified hash codes for multimodal data to preserve their common properties but also learns individual hash codes for each modality to retain its specific properties. The proposed JIMFH learns unified hash codes by joint matrix factorization, which jointly factorizes all modalities into a shared latent semantic space. In addition, JIMFH learns individual hash codes by individual matrix factorization, which separately factorizes each modality into a modal-specific latent semantic space. Finally, unified hash codes and individual hash codes are combined to obtain the final hash codes. In this way, hash codes learned by JIMFH can preserve both the shared properties and specific properties of multimodal data, and therefore the retrieval performance is enhanced. Comprehensive experiments show that the proposed JIMFH performs much better than many state-of-the-art methods on cross-modal retrieval applications.},
  archive      = {J_PR},
  author       = {Di Wang and Quan Wang and Lihuo He and Xinbo Gao and Yumin Tian},
  doi          = {10.1016/j.patcog.2020.107479},
  journal      = {Pattern Recognition},
  pages        = {107479},
  shortjournal = {Pattern Recognition},
  title        = {Joint and individual matrix factorization hashing for large-scale cross-modal retrieval},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient volume repairing method by using a modified
allen-cahn equation. <em>PR</em>, <em>107</em>, 107478. (<a
href="https://doi.org/10.1016/j.patcog.2020.107478">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classifying and rendering volumes of the structure are two essential goals of the visualization process. However, loss of some voxels can cause poor visualization results, such as small holes or non-smooth patches in visualized volumes. Beginning with the classified volumes, we propose a modified Allen-Cahn equation, which has the motion of mean curvature, to recover lost voxels and to fill holes. Consequently, a probability function can be obtained, which indicates the probability of each voxel being a volume voxel. Usually, the obtained probability function is smooth due to the motion of the mean curvature flow. Therefore visualization quality of volumes can be significantly improved. The equation is numerically computed by the unconditional stable operator splitting method with a large time step size. Thus the numerical scheme is fast and can be straightforwardly applied to GPU-accelerated DCT implementation that performs up to many times faster than CPU-only alternatives. Many experimental results have been performed to demonstrate the efficiency of the proposed method.},
  archive      = {J_PR},
  author       = {Yibao Li and Shouren Lan and Xin Liu and Bingheng Lu and Lisheng Wang},
  doi          = {10.1016/j.patcog.2020.107478},
  journal      = {Pattern Recognition},
  pages        = {107478},
  shortjournal = {Pattern Recognition},
  title        = {An efficient volume repairing method by using a modified allen-cahn equation},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Play and rewind: Context-aware video temporal action
proposals. <em>PR</em>, <em>107</em>, 107477. (<a
href="https://doi.org/10.1016/j.patcog.2020.107477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate the problem of Temporal Action Proposal (TAP) generation, which plays a fundamental role in large-scale untrimmed video analysis but remains largely unsolved. Most of the prior works proposed the temporal actions by predicting the temporal boundaries or actionness scores of video units. Nevertheless, context information among surrounding video units has not been adequately explored, which may result in severe loss of information. In this work, we propose a context-aware temporal action proposal network which makes full use of the contextual information in two aspects: 1) To generate initial proposals, we design a Bi-directional Parallel LSTMs to extract the visual features of a video unit by considering its contextual information. Therefore, the prediction of temporal boundaries and actionness scores will be more accurate because it knows what happened in the past and what will happen in the future; and 2) To refine the initial proposals, we design an action-attention based re-ranking network which considers both surrounding proposal and initial actionness scores to assign true action proposals with high confidence scores. Extensive experiments are conducted on two challenging datasets for both temporal action proposal generation and detection tasks, demonstrating the effectiveness of the proposed approach. In particular, on THUMOS’14 dataset, our method significantly surpasses state-of-the-art methods by 7.73\% on AR@50. Our code is released at: https://github.com/Rheelt/TAPG .},
  archive      = {J_PR},
  author       = {Lianli Gao and Tao Li and Jingkuan Song and Zhou Zhao and Heng Tao Shen},
  doi          = {10.1016/j.patcog.2020.107477},
  journal      = {Pattern Recognition},
  pages        = {107477},
  shortjournal = {Pattern Recognition},
  title        = {Play and rewind: Context-aware video temporal action proposals},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new unified method for detecting text from marathon
runners and sports players in video (PR-d-19-01078R2). <em>PR</em>,
<em>107</em>, 107476. (<a
href="https://doi.org/10.1016/j.patcog.2020.107476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting text located on the torsos of marathon runners and sports players in video is a challenging issue due to poor quality and adverse effects caused by flexible/colorful clothing, and different structures of human bodies or actions. This paper presents a new unified method for tackling the above challenges. The proposed method fuses gradient magnitude and direction coherence of text pixels in a new way for detecting candidate regions. Candidate regions are used for determining the number of temporal frame clusters obtained by K-means clustering on frame differences. This process in turn detects key frames. The proposed method explores Bayesian probability for skin portions using color values at both pixel and component levels of temporal frames, which provides fused images with skin components. Based on skin information, the proposed method then detects faces and torsos by finding structural and spatial coherences between them. We further propose adaptive pixels linking a deep learning model for text detection from torso regions. The proposed method is tested on our own dataset collected from marathon/sports video and three standard datasets, namely, RBNR, MMM and R-ID of marathon images, to evaluate the performance. In addition, the proposed method is also tested on the standard natural scene datasets, namely, CTW1500 and MS-COCO text datasets, to show the objectiveness of the proposed method. A comparative study with the state-of-the-art methods on bib number/text detection of different datasets shows that the proposed method outperforms the existing methods.},
  archive      = {J_PR},
  author       = {Sauradip Nag and Palaiahnakote Shivakumara and Umapada Pal and Tong Lu and Michael Blumenstein},
  doi          = {10.1016/j.patcog.2020.107476},
  journal      = {Pattern Recognition},
  pages        = {107476},
  shortjournal = {Pattern Recognition},
  title        = {A new unified method for detecting text from marathon runners and sports players in video (PR-D-19-01078R2)},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hierarchical dense recursive network for image
super-resolution. <em>PR</em>, <em>107</em>, 107475. (<a
href="https://doi.org/10.1016/j.patcog.2020.107475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image super-resolution (SR) techniques with deep convolutional network (CNN) have achieved significant improvements compared to previous shallow-learning-based methods. Especially for dense connection based networks, these methods have yielded unprecedented achievements but bring the higher complexity and more parameters. To this end, this paper considers both reconstruction performance and efficiency, and advocates a novel hierarchical dense connection network (HDN) for image SR. First of all, we construct a hierarchical dense residual block (HDB) to promote the feature representation while saving the memory footprint with a hierarchical matrix structure design. In this way, it can provide additional interleaved pathways for information fusion and gradient optimization but with a shallower depth compare to the previous networks. In particular, a group of convolutional layers with small size (1 × 1) are embedded in HDB, releasing the computational burden and parameters by rescaling the feature dimensions. Furthermore, HDBs are connected to each other in a sharing manner, thereby allowing the network to fuse the features in different stages. At the final, the multi-scale features from these HDBs are integrated into global fusion module (GFM) for a global fusion and representation, and then the final profile-enriched residual map is obtained by realigning and sub-pixel upsampling the fusion maps. Extensive experimental results on benchmark datasets and really degraded images show that our model outperforms the state-of-the-art methods in terms of quantitative indicators and realistic visual effects, as well as enjoys a fast and accurate reconstruction.},
  archive      = {J_PR},
  author       = {Kui Jiang and Zhongyuan Wang and Peng Yi and Junjun Jiang},
  doi          = {10.1016/j.patcog.2020.107475},
  journal      = {Pattern Recognition},
  pages        = {107475},
  shortjournal = {Pattern Recognition},
  title        = {Hierarchical dense recursive network for image super-resolution},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel hybrid approach for crack detection. <em>PR</em>,
<em>107</em>, 107474. (<a
href="https://doi.org/10.1016/j.patcog.2020.107474">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vision-based crack detection is of crucial importance in various industries, and it is very challenging due to weak signals in noisy backgrounds. In this paper, we propose a novel hybrid approach for crack detection in raw images, which combines deep learning models and Bayesian probabilistic analysis for robust crack detection. First, we re-train a state-of-the-art object detector ( e.g. a Faster R-CNN) to detect crack patches of suitable SNR (signal-noise-ratio). We design a semi-automatic method to generate ground truths of crack patches along crack lines for training. To further improve the accuracy of crack detections over the whole image, we propose a Bayesian integration algorithm to suppress false detections. Specifically, we use a deep CNN to recognize the orientation of the crack segment in each detected patch. Then, a Bayesian probability is computed on the accumulated evidence from detected adjacent patches within a neighborhood based on spatial proximity, orientation consistency and alignment consistency. The patch which lacks local supports is suppressed as false detection. An algorithm to learn the parameters of Bayesian integration is also derived. Extensive experiments and evaluations are performed on a new comprehensive dataset of crack images. The results show that our approach outperforms the state-of-the-art baseline approach on deep CNN classifier. Ablation experiments are also conducted to show the effectiveness of proposed techniques.},
  archive      = {J_PR},
  author       = {Fen Fang and Liyuan Li and Ying Gu and Hongyuan Zhu and Joo-Hwee Lim},
  doi          = {10.1016/j.patcog.2020.107474},
  journal      = {Pattern Recognition},
  pages        = {107474},
  shortjournal = {Pattern Recognition},
  title        = {A novel hybrid approach for crack detection},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards more discriminative features for texture
recognition. <em>PR</em>, <em>107</em>, 107473. (<a
href="https://doi.org/10.1016/j.patcog.2020.107473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local binary patterns (LBP) are considered to be one of the most computationally efficient descriptor that can also be combined jointly among different variants to increase accuracy. In this study, we propose a method to obtain more discriminative 2D LBP features by optimizing projections of a joint LBP distribution onto the marginal histograms. To find a more efficient representation of the feature vector, we seek the least redundant marginal histograms of a joint LBP distribution via optimizing several constraints. In this way, we aim to have a more compact yet accurate feature vector in contrast to the methods that flatten the joint distribution. Experiments we perform on five popular texture datasets show that the feature vectors optimized with the proposed method provide higher recognition rates with the same size vectors and comparable results even with lower dimensional vectors . We also compare the proposed algorithm to more recent texture recognition methods based on convolutional neural networks and show that it can still provide comparable results even though the resulting feature vectors are smaller by orders of magnitude.},
  archive      = {J_PR},
  author       = {Llukman Cerkezi and Cihan Topal},
  doi          = {10.1016/j.patcog.2020.107473},
  journal      = {Pattern Recognition},
  pages        = {107473},
  shortjournal = {Pattern Recognition},
  title        = {Towards more discriminative features for texture recognition},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep matching network for handwritten chinese character
recognition. <em>PR</em>, <em>107</em>, 107471. (<a
href="https://doi.org/10.1016/j.patcog.2020.107471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Just like its remarkable achievements in many computer vision tasks, the convolutional neural networks (CNN) provide an end-to-end solution in handwritten Chinese character recognition (HCCR) with great success. However, the process of learning discriminative features for image recognition is difficult in cases where little data is available. In this paper, we propose a matching network which builds a connection between template characters and handwritten characters inspired by the human learning process of writing Chinese characters. The matching network replaces the parameters in the softmax regression layer with the features extracted from the template character images. After the training process has been finished, the powerful discriminative features help us to generalize the predictive power not just to new data, but to entire new Chinese characters that never appear in the training set before. Experiments performed on the ICDAR-2013 offline HCCR datasets have shown that the proposed method achieves a comparable performance to current CNN-based classifiers. Besides, the matching network has a very promising generalization ability to new Chinese characters that never appear in the existing training set.},
  archive      = {J_PR},
  author       = {Zhiyuan Li and Qi Wu and Yi Xiao and Min Jin and Huaxiang Lu},
  doi          = {10.1016/j.patcog.2020.107471},
  journal      = {Pattern Recognition},
  pages        = {107471},
  shortjournal = {Pattern Recognition},
  title        = {Deep matching network for handwritten chinese character recognition},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Binary coyote optimization algorithm for feature selection.
<em>PR</em>, <em>107</em>, 107470. (<a
href="https://doi.org/10.1016/j.patcog.2020.107470">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Coyote Optimization Algorithm (COA) is a bio-inspired optimization algorithm based on the intelligent behavior of coyotes. COA was proposed recently and it considers the social organization of the coyotes and its adaptation to the environment in order to solve continuous optimization problems. In addition, it is a population-based algorithm and it can be classified as both, swarm intelligence and evolutionary heuristics, because contributes with a different algorithmic structure. This paper proposes a binary version of the COA, named Binary COA (BCOA) applying to select the optimal feature subset for classification, based on the hyperbolic transfer function in a wrapper model. By this way, the features are selected based on the performance evaluation of a classification algorithm. We tested the effectiveness of the BCOA wrapper with the Naïve Bayes classifier and were used seven public domain benchmark datasets to compare the proposed approach in terms of classification accuracy, number of selected features and computational cost with other state-of-art algorithms of the literature. The results shown that BCOA was able to find subsets with few features while it still performs well in terms of classification accuracy.},
  archive      = {J_PR},
  author       = {Rodrigo Clemente Thom de Souza and Camila Andrade de Macedo and Leandro dos Santos Coelho and Juliano Pierezan and Viviana Cocco Mariani},
  doi          = {10.1016/j.patcog.2020.107470},
  journal      = {Pattern Recognition},
  pages        = {107470},
  shortjournal = {Pattern Recognition},
  title        = {Binary coyote optimization algorithm for feature selection},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Covariance descriptors on a gaussian manifold and their
application to image set classification. <em>PR</em>, <em>107</em>,
107463. (<a href="https://doi.org/10.1016/j.patcog.2020.107463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Covariance descriptors (CovDs) for image set classification have been widely studied recently. Different from the conventional CovDs, which describe similarities between pixels at different locations, we focus more on similarities between regions that convey more comprehensive information. In this paper, we extract pixel-wise features of image regions and represent them by Gaussian models . We extend the conventional covariance computation onto a special type of Riemannian manifold, namely a Gaussian manifold, so that it is applicable to our image set data representation provided in terms of Gaussian models. We present two methods to calculate a Riemannian local difference vector on the Gaussian manifold (RieLDV-G) and generate our proposed Riemannian covariance descriptors (RieCovDs) using the resulting RieLDV-G. By measuring the recognition accuracy achieved on benchmarking datasets , we demonstrate experimentally the superior performance of our proposed RieCovDs descriptors, as compared with state-of-the-art methods. ( The code is available at: https://github.com/Kai-Xuan/RiemannianCovDs )},
  archive      = {J_PR},
  author       = {Kai-Xuan Chen and Jie-Yi Ren and Xiao-Jun Wu and Josef Kittler},
  doi          = {10.1016/j.patcog.2020.107463},
  journal      = {Pattern Recognition},
  pages        = {107463},
  shortjournal = {Pattern Recognition},
  title        = {Covariance descriptors on a gaussian manifold and their application to image set classification},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). AutoPruner: An end-to-end trainable filter pruning method
for efficient deep model inference. <em>PR</em>, <em>107</em>, 107461.
(<a href="https://doi.org/10.1016/j.patcog.2020.107461">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Channel pruning is an important method to speed up CNN model’s inference. Previous filter pruning algorithms regard importance evaluation and model fine-tuning as two independent steps. This paper argues that combining them into a single end-to-end trainable system will lead to better results. We propose an efficient channel selection layer, namely AutoPruner, to find less important filters automatically in a joint training manner. Our AutoPruner takes previous activation responses as an input and generates a true binary index code for pruning. Hence, all the filters corresponding to zero index values can be removed safely after training. By gradually erasing several unimportant filters, we can prevent an excessive drop in model accuracy. Compared with previous state-of-the-art pruning algorithms (including training from scratch), AutoPruner achieves significantly better performance. Furthermore, ablation experiments show that the proposed novel mini-batch pooling and binarization operations are vital for the success of model pruning.},
  archive      = {J_PR},
  author       = {Jian-Hao Luo and Jianxin Wu},
  doi          = {10.1016/j.patcog.2020.107461},
  journal      = {Pattern Recognition},
  pages        = {107461},
  shortjournal = {Pattern Recognition},
  title        = {AutoPruner: An end-to-end trainable filter pruning method for efficient deep model inference},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generative adversarial classifier for handwriting characters
super-resolution. <em>PR</em>, <em>107</em>, 107453. (<a
href="https://doi.org/10.1016/j.patcog.2020.107453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative Adversarial Networks (GAN) receive great attention recently due to its excellent performance in image generation , transformation, and super-resolution. However, less emphasis or study has been put on GAN for classification with super-resolution. Moreover, though GANs may fabricate images which perceptually looks realistic, they usually fabricate some fake details especially in character data; this would impose further difficulties when they are input for classification. In this paper, we propose a novel Generative Adversarial Classifier (GAC) for low-resolution handwriting character recognition. Specifically, we design an additional classifier component in GAC, leading to a novel three-player GAN model which is not only able to generate high-quality super-resolved images, but also favorable for classification. Experimental results show that our proposed method can obtain remarkable performance in handwriting characters with 8 ×  super-resolution, achieving new state-of-the-art on benchmark dataset CASIA-HWDB1.1, and MNIST.},
  archive      = {J_PR},
  author       = {Zhuang Qian and Kaizhu Huang and Qiu-Feng Wang and Jimin Xiao and Rui Zhang},
  doi          = {10.1016/j.patcog.2020.107453},
  journal      = {Pattern Recognition},
  pages        = {107453},
  shortjournal = {Pattern Recognition},
  title        = {Generative adversarial classifier for handwriting characters super-resolution},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive core fusion-based density peak clustering for
complex data with arbitrary shapes and densities. <em>PR</em>,
<em>107</em>, 107452. (<a
href="https://doi.org/10.1016/j.patcog.2020.107452">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A challenging issue of clustering in real-word application is to detect clusters with arbitrary shapes and densities in complex data. Many conventional clustering algorithms are capable of detecting non-spherical clusters, but their performance is limited when processing data with complex shapes and multiple density peaks in a cluster without knowing the number of clusters. This paper proposes an adaptive core fusion-based density peak clustering (CFDPC) for detecting clusters in any shape and density adaptively. An initial clustering based on automatic finding of density peaks is proposed first. An adaptive searching approach is then proposed to find core points, and a within-cluster similarity-based core fusion strategy is proposed to obtain the final clustering results . The CFDPC where the number of clusters arises intuitively is simple and efficient. The performance of CFDPC is successfully verified in clustering several benchmark complex datasets with diverse shapes and densities.},
  archive      = {J_PR},
  author       = {Fang Fang and Lei Qiu and Shenfang Yuan},
  doi          = {10.1016/j.patcog.2020.107452},
  journal      = {Pattern Recognition},
  pages        = {107452},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive core fusion-based density peak clustering for complex data with arbitrary shapes and densities},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Density peaks clustering based on density backbone and fuzzy
neighborhood. <em>PR</em>, <em>107</em>, 107449. (<a
href="https://doi.org/10.1016/j.patcog.2020.107449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Density peaks clustering (DPC) is as an efficient clustering algorithm due for using a non-iterative process. However, DPC and most of its improvements suffer from the following shortcomings: (1) highly sensitive to its cutoff distance parameter, (2) ignoring the local structure of data in computing local densities, (3) using a crisp kernel to calculate local densities, and (4) suffering from the cause of chain reaction. To address these issues, in this paper a new method called DPC-DBFN is proposed. The proposed method uses a fuzzy kernel for improving separability of clusters and reducing the impact of outliers. DPC-DBFN uses a density-based kNN graph for labeling backbones. This strategy prevents the chain reaction and effectively assigns true labels to those instances located on the border regions to effectively cluster data with various shapes and densities. The DPC-DBFN is evaluated on some real-world and synthetic datasets. The experimental results show the effectiveness and robustness of the proposed algorithm.},
  archive      = {J_PR},
  author       = {Abdulrahman Lotfi and Parham Moradi and Hamid Beigy},
  doi          = {10.1016/j.patcog.2020.107449},
  journal      = {Pattern Recognition},
  pages        = {107449},
  shortjournal = {Pattern Recognition},
  title        = {Density peaks clustering based on density backbone and fuzzy neighborhood},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Point attention network for semantic segmentation of 3D
point clouds. <em>PR</em>, <em>107</em>, 107446. (<a
href="https://doi.org/10.1016/j.patcog.2020.107446">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks (CNNs) have performed extremely well on data represented by regularly arranged grids such as images. However, directly leveraging the classic convolution kernels or parameter sharing mechanisms on sparse 3D point clouds is inefficient due to their irregular and unordered nature. We propose a point attention network that learns rich local shape features and their contextual correlations for 3D point cloud semantic segmentation . Since the geometric distribution of the neighboring points is invariant to the point ordering, we propose a Local Attention-Edge Convolution (LAE-Conv) to construct a local graph based on the neighborhood points searched in multi-directions. We assign attention coefficients to each edge and then aggregate the point features as a weighted sum of its neighbors. The learned LAE-Conv layer features are then given to a point-wise spatial attention module to generate an interdependency matrix of all points regardless of their distances, which captures long-range spatial contextual features contributing to more precise semantic information. The proposed point attention network consists of an encoder and decoder which, together with the LAE-Conv layers and the point-wise spatial attention modules, make it an end-to-end trainable network for predicting dense labels for 3D point cloud segmentation . Experiments on challenging benchmarks of 3D point clouds show that our algorithm can perform at par or better than the existing state of the art methods .},
  archive      = {J_PR},
  author       = {Mingtao Feng and Liang Zhang and Xuefei Lin and Syed Zulqarnain Gilani and Ajmal Mian},
  doi          = {10.1016/j.patcog.2020.107446},
  journal      = {Pattern Recognition},
  pages        = {107446},
  shortjournal = {Pattern Recognition},
  title        = {Point attention network for semantic segmentation of 3D point clouds},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cylinders extraction in non-oriented point clouds as a
clustering problem. <em>PR</em>, <em>107</em>, 107443. (<a
href="https://doi.org/10.1016/j.patcog.2020.107443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding geometric primitives in 3D point clouds is a fundamental task in many engineering applications such as robotics, autonomous-vehicles and automated industrial inspection. Among all solid shapes, cylinders are frequently found in a variety of scenes, comprising natural or man-made objects. Despite their ubiquitous presence, automated extraction and fitting can become challenging if performed ”in-the-wild”, when the number of primitives is unknown or the point cloud is noisy and not oriented. In this paper we pose the problem of extracting multiple cylinders in a scene by means of a Game-Theoretic inlier selection process exploiting the geometrical relations between pairs of axis candidates. First, we formulate the similarity between two possible cylinders considering the rigid motion aligning the two axes to the same line. This motion is represented with a unitary dual-quaternion so that the distance between two cylinders is induced by the length of the shortest geodesic path in SE(3). Then, a Game-Theoretical process exploits such similarity function to extract sets of primitives maximizing their inner mutual consensus. The outcome of the evolutionary process consists in a probability distribution over the sets of candidates (ie axes), which in turn is used to directly estimate the final cylinder parameters. An extensive experimental section shows that the proposed algorithm offers a high resilience to noise, since the process inherently discards inconsistent data. Compared to other methods, it does not need point normals and does not require a fine tuning of multiple parameters.},
  archive      = {J_PR},
  author       = {Filippo Bergamasco and Mara Pistellato and Andrea Albarelli and Andrea Torsello},
  doi          = {10.1016/j.patcog.2020.107443},
  journal      = {Pattern Recognition},
  pages        = {107443},
  shortjournal = {Pattern Recognition},
  title        = {Cylinders extraction in non-oriented point clouds as a clustering problem},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enhanced automatic twin support vector machine for
imbalanced data classification. <em>PR</em>, <em>107</em>, 107442. (<a
href="https://doi.org/10.1016/j.patcog.2020.107442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most of the classification approaches assume that the sample distribution among classes is balanced. Still, such an assumption leads to biased performance over the majority class. This paper proposes an enhanced automatic twin support vector machine – (EATWSVM) to deal with imbalanced data, which incorporates a kernel representation within a TWSVM-based optimization. To learn the kernel function, we impose a Gaussian similarity, ruled by a Mahalanobis distance, and couple a centered kernel alignment-based approach to improving the data separability . Besides, we suggest a suitable range to fix the regularization parameters concerning both the dataset’ imbalance ratio and overlap. Lastly, we adopt One-vs-One and One-vs-Rest frameworks to extend our EATWSVM formulation for multi-class tasks. Obtained results on synthetic and real-world datasets show that our approach outperforms state-of-the-art methods concerning classification performance and training time.},
  archive      = {J_PR},
  author       = {C. Jimenez-Castaño and A. Alvarez-Meza and A. Orozco-Gutierrez},
  doi          = {10.1016/j.patcog.2020.107442},
  journal      = {Pattern Recognition},
  pages        = {107442},
  shortjournal = {Pattern Recognition},
  title        = {Enhanced automatic twin support vector machine for imbalanced data classification},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generative attention adversarial classification network for
unsupervised domain adaptation. <em>PR</em>, <em>107</em>, 107440. (<a
href="https://doi.org/10.1016/j.patcog.2020.107440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation is a significant and popular issue of solving distribution discrepancy among different domains in computer vision . Generally, previous works proposed are mainly devoted to reducing domain shift between source domain with labeled data and target domain without labels. Adversarial learning in deep networks has already been widely applied to learn disentangled and transferable features between two different domains to minimize domains distribution discrepancy. However, these methods rarely consider class distributions among source data during adversarial learning, and they pay little attention to these transferable regions among source and target domains images. In this paper, we propose a Generative Attention Adversarial Classification Network (GAACN) model for unsupervised domain adaptation . To learn a joint feature distribution between source and target domains, we present an improved generative adversarial network (GAN) following the feature extractor. Firstly, the discriminator of GAN discriminates the distribution of domains and the classes distribution among source data during adversarial learning, so that our feature extractor can learn a joint feature distribution between source and target domains and maintain the classes consistent simultaneously. Secondly, we present an attention module embedded in GAN, which allows the discriminator to discriminate the transferable regions among the images of source and target domains. Lastly, we propose a simple and efficient method which allocates pseudo-labels for unlabeled target data, and it can improve the performance of our model GAACN while mitigating negative transfer. Extensive experiments demonstrate that our proposed model achieves perfect results on several standard domain adaptation datasets.},
  archive      = {J_PR},
  author       = {Wendong Chen and Haifeng Hu},
  doi          = {10.1016/j.patcog.2020.107440},
  journal      = {Pattern Recognition},
  pages        = {107440},
  shortjournal = {Pattern Recognition},
  title        = {Generative attention adversarial classification network for unsupervised domain adaptation},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning to complete partial observations from unpaired
prior knowledge. <em>PR</em>, <em>107</em>, 107426. (<a
href="https://doi.org/10.1016/j.patcog.2020.107426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel training strategy that allows convolutional encoder-decoder networks, to complete partially observed data by means of hallucination. As input, it takes data from a partially observed domain, for which no complete ground truth is available, and data from an unpaired prior knowledge domain and trains the network in an end-to-end manner. This strategy is demonstrated for the task of completing 2-D road layouts as well as 3-D vehicle shapes. In contrast to alternative approaches, our strategy is compatible with networks that use skip connections, to improve detail in the completed output, while not requiring adversarial supervision. To demonstrate its benefits, our training strategy is benchmarked against two state-of-the-art baselines, one using a two-step auto-encoder training strategy and one using an adversarial strategy . Our novel strategy achieves an improvement up to +12\% F-measure on the Cityscapes dataset. The learned network intrinsically generalizes better than the baselines on unseen datasets, which is demonstrated by an improvement up to +24\% F-measure on the unseen KITTI dataset. Moreover, our approach outperforms the baselines using the same backbone network on the 3-D shape completion benchmark by reducing the Hamming distance with 15\%.},
  archive      = {J_PR},
  author       = {Chenyang Lu and Gijs Dubbelman},
  doi          = {10.1016/j.patcog.2020.107426},
  journal      = {Pattern Recognition},
  pages        = {107426},
  shortjournal = {Pattern Recognition},
  title        = {Learning to complete partial observations from unpaired prior knowledge},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semi-supervised elastic manifold embedding with deep
learning architecture. <em>PR</em>, <em>107</em>, 107425. (<a
href="https://doi.org/10.1016/j.patcog.2020.107425">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based embedding aims to reduce the dimension of high dimensional data and to extract relevant features for learning tasks. In this letter, we propose an Elastic graph-based embedding with deep architecture which deeply explores the structural information of the data. We introduce a flexible deep learning that can overcome the limitations and weaknesses of single-layer learning models. The proposed deep architecture incorporates the geometrical manifold structure of the data. The resulting framework can be used for semi-supervised and supervised settings. Besides, the resulting optimization problems can be solved efficiently. We apply the algorithm on five public image datasets including scene, face and object datasets . These experiments demonstrate the effectiveness of the proposed embedding method, and also show that the proposed method compares favorably with many competing state-of-the-art graph-based methods.},
  archive      = {J_PR},
  author       = {R. Zhu and F. Dornaika and Y. Ruichek},
  doi          = {10.1016/j.patcog.2020.107425},
  journal      = {Pattern Recognition},
  pages        = {107425},
  shortjournal = {Pattern Recognition},
  title        = {Semi-supervised elastic manifold embedding with deep learning architecture},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Face re-identification challenge: Are face recognition
models good enough? <em>PR</em>, <em>107</em>, 107422. (<a
href="https://doi.org/10.1016/j.patcog.2020.107422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face re-identification (Re-ID) aims to track the same individuals over space and time with subtle identity class information in automatically detected face images captured by unconstrained surveillance camera views. Despite significant advances of face recognition systems for constrained social media facial images , face Re-ID is more challenging due to poor-quality surveillance face imagery data and remains under-studied. However, solving this problem enables a wide range of practical applications, ranging from law enforcement and information security to business, entertainment and e-commerce. To facilitate more studies on face Re-ID towards practical and robust solutions, a true large scale Surveillance Face Re-ID benchmark ( SurvFace ) is introduced, characterised by natively low-resolution, motion blur , uncontrolled poses, varying occlusion, poor illumination, and background clutters. This new benchmark is the largest and more importantly the only true surveillance face Re-ID dataset to our best knowledge, where facial images are captured and detected under realistic surveillance scenarios. We show that the current state-of-the-art FR methods are surprisingly poor for face Re-ID. Besides, face Re-ID is generally more difficult in an open-set setting as naturally required in surveillance scenarios, owing to a large number of non-target people (distractors) appearing in open ended scenes. Moreover, the low-resolution problem inherent to surveillance facial imagery is investigated. Finally, we discuss open research problems that need to be solved in order to overcome the under-studied face Re-ID problem.},
  archive      = {J_PR},
  author       = {Zhiyi Cheng and Xiatian Zhu and Shaogang Gong},
  doi          = {10.1016/j.patcog.2020.107422},
  journal      = {Pattern Recognition},
  pages        = {107422},
  shortjournal = {Pattern Recognition},
  title        = {Face re-identification challenge: Are face recognition models good enough?},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Detection of rotational symmetry in curves represented by
the slope chain code. <em>PR</em>, <em>107</em>, 107421. (<a
href="https://doi.org/10.1016/j.patcog.2020.107421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new approach based on the Slope Chain Code to determine whether a curve is rotational symmetrical and its order of symmetry. The proposed approach works for open and closed perfectly symmetrical or quasi-symmetrical 2D curves. Simple operations on the SCC and its invariant properties are central to our methodology. To evaluate the proposed methodology, we use 1400 curves from a public database. For the symmetrical/asymmetrical classification task , a recall (R) of 0.86, a balanced accuracy (BA) of 0.92, and a precision (P) of 0.87 were obtained. For the quasi-symmetrical/quasi-asymmetrical classification task , R=0.77, BA=0.83, and P=0.70 were obtained. For the order of rotational symmetry detection task, the following performance was achieved: R=0.97, BA=0.98, and P=0.95 for a symmetrical set of curves, and R=0.98, BA=0.98, and P=0.90 for a quasi-symmetrical set of curves. We conclude our presentation demonstrating the usefulness of our methodology with three practical applications},
  archive      = {J_PR},
  author       = {Wendy Aguilar and Montserrat Alvarado-Gonzalez and Edgar Garduño and Carlos Velarde and Ernesto Bribiesca},
  doi          = {10.1016/j.patcog.2020.107421},
  journal      = {Pattern Recognition},
  pages        = {107421},
  shortjournal = {Pattern Recognition},
  title        = {Detection of rotational symmetry in curves represented by the slope chain code},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiple strong and balanced cluster-based ensemble of deep
learners. <em>PR</em>, <em>107</em>, 107420. (<a
href="https://doi.org/10.1016/j.patcog.2020.107420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolutional Neural Networks (CNNs), also known as deep learners have seen much success in the last few years due to the availability of large amounts of data and high-performance computational resources. A CNN can be trained effectively if large amounts of data are available as it enables a CNN to find the optimal set of features and weights that can achieve the highest generalization performance. However, due to the requirement of large data size, CNNs require a lot of resources for example running time and computational resources to achieve a reasonable performance. Additionally, unbalanced data makes it difficult to train a CNN effectively that can achieve good generalization performance. In order to alleviate these limitations, in this paper, we propose a novel ensemble of deep learners that learns by combining multiple deep learners trained on small strongly class associated input data effectively. We propose a novel methodology of generating random subspace through clustering input data and propose a measure which can classify each cluster as a strong data cluster and a balanced data cluster. A methodology is also proposed that balances all strong data clusters in the pool so that an architecturally simple CNN can be trained on all balanced data clusters simultaneously. Classification decisions on all trained CNNs are then fused through majority voting to generate class decisions of the ensemble. The performance of the proposed ensemble approach is evaluated on UCI benchmark datasets, and results are compared with existing state-of-the-art ensemble approaches. Significance testing was conducted to further validate the efficacy of the results and a significance test analysis is presented.},
  archive      = {J_PR},
  author       = {Zohaib Jan and Brijesh Verma},
  doi          = {10.1016/j.patcog.2020.107420},
  journal      = {Pattern Recognition},
  pages        = {107420},
  shortjournal = {Pattern Recognition},
  title        = {Multiple strong and balanced cluster-based ensemble of deep learners},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visual tracking by dynamic matching-classification network
switching. <em>PR</em>, <em>107</em>, 107419. (<a
href="https://doi.org/10.1016/j.patcog.2020.107419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deep trackers can be roughly divided into either matching-based or classification-based methods. The formers are fast but not very robust; while the latter ones introduce more discriminative information but often very slow. In this work, we present a novel real-time robust tracking method to take full use of the benefits from both kinds of networks. First, we propose a matching-classification network switching (MCS) framework to integrate the matching, classification, verification networks and conduct dynamic switching among them. Second, to speed up online update, we devlop a meta learning method as a critical component in our classification network. The meta classifier is trained offline to obtain general discriminative ability and updated online to the current frame just through one iteration. Extensive experiments are conducted on two popular benchmark datasets. Both qualitative and quantitative evaluations show that our tracker performs favorably against other state-of-the-art trackers with real-time performance.},
  archive      = {J_PR},
  author       = {Peixia Li and Boyu Chen and Dong Wang and Huchuan Lu},
  doi          = {10.1016/j.patcog.2020.107419},
  journal      = {Pattern Recognition},
  pages        = {107419},
  shortjournal = {Pattern Recognition},
  title        = {Visual tracking by dynamic matching-classification network switching},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Gesture recognition based on deep deformable 3D
convolutional neural networks. <em>PR</em>, <em>107</em>, 107416. (<a
href="https://doi.org/10.1016/j.patcog.2020.107416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic gesture recognition, which plays an essential role in human-computer interaction, has been widely investigated but not yet fully addressed. The challenge mainly lies in three folders: 1) to model both of the spatial appearance and the temporal evolution simultaneously; 2) to address the interference from the varied and complex background; 3) the requirement of real-time processing. In this paper, we address the above challenges by proposing a novel deep deformable 3D convolutional neural network for end-to-end learning, which not only gains impressive accuracy in challenging datasets but also can meet the requirement of the real-time processing. We propose three types of very deep 3D CNNs for gesture recognition, which can directly model the spatiotemporal information with their inherent hierarchical structure. To eliminate the background interference, a light-weight spatiotemporal deformable convolutional module is specially designed to augment the spatiotemporal sampling locations of the 3D convolution by learning additional offsets according to the preceding feature map. It can not only diversify the shape of the convolution kernel to better fit the appearance of the hands and arms, but also help the models pay more attention to the discriminative frames in the video sequence. The proposed method is evaluated on three challenging datasets, EgoGesture, Jester and Chalearn-IsoGD, and achieves the state-of-the-art performance on all of them. Our model ranked first on Jester’s official leader-board until the submission time. The code and the trained models are released for better communication and future works 1 .},
  archive      = {J_PR},
  author       = {Yifan Zhang and Lei Shi and Yi Wu and Ke Cheng and Jian Cheng and Hanqing Lu},
  doi          = {10.1016/j.patcog.2020.107416},
  journal      = {Pattern Recognition},
  pages        = {107416},
  shortjournal = {Pattern Recognition},
  title        = {Gesture recognition based on deep deformable 3D convolutional neural networks},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A robust matching pursuit algorithm using information
theoretic learning. <em>PR</em>, <em>107</em>, 107415. (<a
href="https://doi.org/10.1016/j.patcog.2020.107415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current orthogonal matching pursuit (OMP) algorithms calculate the correlation between two vectors using the inner product operation and minimize the mean square error, which are both suboptimal when there are non-Gaussian noises or outliers in the observation data. To overcome these problems, a new OMP algorithm is developed based on information theoretic learning (ITL), which is built on the following new techniques: (1) an ITL-based correlation (ITL-Correlation) is developed as a new similarity measure which can better exploit higher-order statistics of the data, and is robust against many different types of noise and outliers in a sparse representation framework; (2) a non-second order statistic measurement and minimization method is developed to improve the robustness of OMP by overcoming the limitation of Gaussianity inherent in a cost function based on second-order moments. The experimental results on both simulated and real-world data consistently demonstrate the superiority of the proposed OMP algorithm in data recovery, image reconstruction, and classification.},
  archive      = {J_PR},
  author       = {Miaohua Zhang and Yongsheng Gao and Changming Sun and Michael Blumenstein},
  doi          = {10.1016/j.patcog.2020.107415},
  journal      = {Pattern Recognition},
  pages        = {107415},
  shortjournal = {Pattern Recognition},
  title        = {A robust matching pursuit algorithm using information theoretic learning},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Learning discriminative features via weights-biased softmax
loss. <em>PR</em>, <em>107</em>, 107405. (<a
href="https://doi.org/10.1016/j.patcog.2020.107405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Loss functions play a key role in training superior deep neural networks . In convolutional neural networks (CNNs), the popular cross entropy loss together with softmax does not explicitly guarantee minimization of intra-class variance or maximization of inter-class variance. In the early studies, there is no theoretical analysis and experiments explicitly indicating how to choose the number of units in fully connected layer. To help CNNs learn features more fast and discriminative, there are two contributions in this paper. First, we determine the minimum number of units in FC layer by rigorous theoretical analysis and extensive experiment, which reduces CNNs’ parameter memory and training time. Second, we propose a negative-focused weights-biased softmax (W-Softmax) loss to help CNNs learn more discriminative features . The proposed W-Softmax loss not only theoretically formulates the intra-class compactness and inter-class separability, but also can avoid overfitting by enlarging decision margins. Moreover, the size of decision margins can be flexibly controlled by adjusting a hyperparameter α . Extensive experimental results on several benchmark datasets show the superiority of W-Softmax in image classification tasks .},
  archive      = {J_PR},
  author       = {Xiaobin Li and Weiqiang Wang},
  doi          = {10.1016/j.patcog.2020.107405},
  journal      = {Pattern Recognition},
  pages        = {107405},
  shortjournal = {Pattern Recognition},
  title        = {Learning discriminative features via weights-biased softmax loss},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic graph convolutional network for multi-video
summarization. <em>PR</em>, <em>107</em>, 107382. (<a
href="https://doi.org/10.1016/j.patcog.2020.107382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-video summarization is an effective tool for users to browse multiple videos. In this paper, multi-video summarization is formulated as a graph analysis problem and a dynamic graph convolutional network is proposed to measure the importance and relevance of each video shot in its own video as well as in the whole video collection. Two strategies are proposed to solve the inherent class imbalance problem of video summarization task. Moreover, we propose a diversity regularization to encourage the model to generate a diverse summary. Extensive experiments are conducted, and the comparisons are carried out with the state-of-the-art video summarization methods, the traditional and novel graph models. Our method achieves state-of-the-art performances on two standard video summarization datasets. The results demonstrate the effectiveness of our proposed model in generating a representative summary for multiple videos with good diversity.},
  archive      = {J_PR},
  author       = {Jiaxin Wu and Sheng-hua Zhong and Yan Liu},
  doi          = {10.1016/j.patcog.2020.107382},
  journal      = {Pattern Recognition},
  pages        = {107382},
  shortjournal = {Pattern Recognition},
  title        = {Dynamic graph convolutional network for multi-video summarization},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning spatial-temporal deformable networks for
unconstrained face alignment and tracking in videos. <em>PR</em>,
<em>107</em>, 107354. (<a
href="https://doi.org/10.1016/j.patcog.2020.107354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a spatial-temporal deformable networks approach to investigate both problems of face alignment in static images and face tracking in videos under unconstrained environments. Unlike conventional feature extractions which cannot explicitly exploit augmented spatial geometry for various facial shapes, in our approach, we propose a deformable hourglass networks (DHGN) method, which aims to learn a deformable mask to reduce the variances of facial deformation and extract attentional facial regions for robust feature representation. However, our DHGN is limited to extract only spatial appearance features from static facial images , which cannot explicitly exploit the temporal consistency information across consecutive frames in videos. For efficient temporal modeling , we further extend our DHGN to a temporal DHGN (T-DHGN) paradigm particularly for video-based face alignment. To this end, our T-DHGN principally incorporates with a temporal relational reasoning module, so that the temporal order relationship among frames is encoded in the relational feature. By doing this, our T-DHGN reasons about the temporal offsets to select a subset of discriminative frames over time steps, thus allowing temporal consistency information memorized to flow across frames for stable landmark tracking in videos. Compared with most state-of-the-art methods, our approach achieves superior performance on folds of widely-evaluated benchmarking datasets . Code will be made publicly available upon publication.},
  archive      = {J_PR},
  author       = {Hongyu Zhu and Hao Liu and Congcong Zhu and Zongyong Deng and Xuehong Sun},
  doi          = {10.1016/j.patcog.2020.107354},
  journal      = {Pattern Recognition},
  pages        = {107354},
  shortjournal = {Pattern Recognition},
  title        = {Learning spatial-temporal deformable networks for unconstrained face alignment and tracking in videos},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep co-training for semi-supervised image segmentation.
<em>PR</em>, <em>107</em>, 107269. (<a
href="https://doi.org/10.1016/j.patcog.2020.107269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we aim to improve the performance of semantic image segmentation in a semi-supervised setting where training is performed with a reduced set of annotated images and additional non-annotated images. We present a method based on an ensemble of deep segmentation models . Models are trained on subsets of the annotated data and use non-annotated images to exchange information with each other, similar to co-training. Diversity across models is enforced with the use of adversarial samples. We demonstrate the potential of our method on three challenging image segmentation problems, and illustrate its ability to share information between simultaneously trained models, while preserving their diversity. Results indicate clear advantages in terms of performance compared to recently proposed semi-supervised methods for segmentation.},
  archive      = {J_PR},
  author       = {Jizong Peng and Guillermo Estrada and Marco Pedersoli and Christian Desrosiers},
  doi          = {10.1016/j.patcog.2020.107269},
  journal      = {Pattern Recognition},
  pages        = {107269},
  shortjournal = {Pattern Recognition},
  title        = {Deep co-training for semi-supervised image segmentation},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SMPLR: Deep learning based SMPL reverse for 3D human pose
and shape recovery. <em>PR</em>, <em>106</em>, 107472. (<a
href="https://doi.org/10.1016/j.patcog.2020.107472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose to embed SMPL within a deep-based model to accurately estimate 3D pose and shape from a still RGB image . We use CNN-based 3D joint predictions as an intermediate representation to regress SMPL pose and shape parameters. Later, 3D joints are reconstructed again in the SMPL output. This module can be seen as an autoencoder where the encoder is a deep neural network and the decoder is SMPL model. We refer to this as SMPL reverse (SMPLR). By implementing SMPLR as an encoder-decoder we avoid the need of complex constraints on pose and shape. Furthermore, given that in-the-wild datasets usually lack accurate 3D annotations, it is desirable to lift 2D joints to 3D without pairing 3D annotations with RGB images. Therefore, we also propose a denoising autoencoder (DAE) module between CNN and SMPLR, able to lift 2D joints to 3D and partially recover from structured error. We evaluate our method on SURREAL and Human3.6M datasets, showing improvement over SMPL-based state-of-the-art alternatives by about 4 and 12 mm, respectively.},
  archive      = {J_PR},
  author       = {Meysam Madadi and Hugo Bertiche and Sergio Escalera},
  doi          = {10.1016/j.patcog.2020.107472},
  journal      = {Pattern Recognition},
  pages        = {107472},
  shortjournal = {Pattern Recognition},
  title        = {SMPLR: Deep learning based SMPL reverse for 3D human pose and shape recovery},
  volume       = {106},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient attribute-space connected filter on graphs to
reconstruct paths in point-clouds. <em>PR</em>, <em>106</em>, 107467.
(<a href="https://doi.org/10.1016/j.patcog.2020.107467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measurements by many multi-sensor systems can be considered as point-clouds. One such system is the tracker for the PANDA experiment. Charged particles passing through the tracker produce patterns representing their paths. We present a new, graph-based, attribute-space morphological connected filter for reconstructing particle paths through such a detector. We introduce the concept of attribute-spaces and attribute-space connected filters on graphs, rather than binary images and show a new processing scheme to reduce the size of the memory required to store the attribute-space representations of binary images and graphs. The result is an O ( N log ( N )) algorithm with a total recognition error of approximately 0.10, a significant improvement compared to our previous state-of-the-art O ( N 2 ) algorithm with a total error of 0.17.},
  archive      = {J_PR},
  author       = {M. Babai and N. Kalantar-Nayestanaki and J.G. Messchendorp and M.H.F. Wilkinson},
  doi          = {10.1016/j.patcog.2020.107467},
  journal      = {Pattern Recognition},
  pages        = {107467},
  shortjournal = {Pattern Recognition},
  title        = {An efficient attribute-space connected filter on graphs to reconstruct paths in point-clouds},
  volume       = {106},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive ROI generation for video object segmentation using
reinforcement learning. <em>PR</em>, <em>106</em>, 107465. (<a
href="https://doi.org/10.1016/j.patcog.2020.107465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of the proposed method is semi-supervised video object segmentation where only the ground-truth segmentation of the first frame is provided. The existing approaches rely on selecting the region of interest for model update; however it is rough and inflexible, leading to performance degradation . To overcome this limitation, a novel approach is proposed which utilizes reinforcement learning to select optimal adaptation areas for each frame, based on the historical segmentation information . The RL model learns to take optimal actions to adjust the region of interest inferred from the previous frame for online model updating. To speed up the model adaption, a novel multi-branch tree based exploration method is designed to quickly select the best state action pairs. The proposed method is evaluated on three common video object segmentation datasets including DAVIS 2016, SegTrack V2 and Youtube-Object. The results show that the proposed work improves the state-of-the-art of the mean region similarity to 87.1\% on the DAVIS 2016 dataset, and to 79.5\% on the Youtube-Object dataset. Meanwhile, competitive performance is obtained on the SegTrack V2 dataset. Code is at https://github.com/insomnia94/ARG .},
  archive      = {J_PR},
  author       = {Mingjie Sun and Jimin Xiao and Eng Gee Lim and Yanchun Xie and Jiashi Feng},
  doi          = {10.1016/j.patcog.2020.107465},
  journal      = {Pattern Recognition},
  pages        = {107465},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive ROI generation for video object segmentation using reinforcement learning},
  volume       = {106},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data-augmented matched subspace detector for hyperspectral
subpixel target detection. <em>PR</em>, <em>106</em>, 107464. (<a
href="https://doi.org/10.1016/j.patcog.2020.107464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of subspace-based methods such as matched subspace detector (MSD) and MSD with interaction effects (MSDinter) heavily depends on the background subspace and the target subspace. Nonetheless, constructing a representative target subspace is challenging due to the limited availability of target spectra in a collected hyperspectral image . In this paper, we propose two new hyperspectral target detection methods termed data-augmented MSD (DAMSD) and data-augmented MSDinter (DAMSDI) that can effectively solve the scarcity problem of target spectra and from which a representative target-background mixed subspace can be learned. We first synthesise target-background mixed spectra based on classical hyperspectral mixing models and then learn a target-background mixed subspace via principal component analysis. Compared with MSD and MSDinter, the learned mixed subspace is more representative as spectral variability of target spectra is explained to the largest extent and it leads to an improvement in computational speed and numerical stability. We demonstrate the efficacy of DAMSD and DAMSDI for subpixel target detection on two public hyperspectral image datasets.},
  archive      = {J_PR},
  author       = {Xiaochen Yang and Mingzhi Dong and Ziyu Wang and Lianru Gao and Lefei Zhang and Jing-Hao Xue},
  doi          = {10.1016/j.patcog.2020.107464},
  journal      = {Pattern Recognition},
  pages        = {107464},
  shortjournal = {Pattern Recognition},
  title        = {Data-augmented matched subspace detector for hyperspectral subpixel target detection},
  volume       = {106},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A CNN-based 3D human pose estimation based on projection of
depth and ridge data. <em>PR</em>, <em>106</em>, 107462. (<a
href="https://doi.org/10.1016/j.patcog.2020.107462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a method that use a convolutional neural network (CNN) to estimate human pose by analyzing the projection of the depth and ridge data, which represent local maxima in a distance transform map. To fully utilize the 3D information of depth points, we propose a method to project the depth and ridge data on various directions. The proposed projection method can reduce the 3D information loss, the ridge data can avoid joint drift, and the CNN increases localization accuracy . The proposed method proceeds as follows. (1) We use depth data to segment the human from the background and extract ridge data from human silhouettes. (2) We project the depth and ridge data onto XY, XZ, and ZY planes. (3) ResNet-101 accepts six projected images and use 1  ×  1 convolution layers to generate 2D heatmaps and offsets. (4) We generate 2D keypoints per plane by using the soft-argmax operation. (5) We obtain 3D joint positions by using the fully-connected layers. In experiments on the SMMC-10, EVAL, and ITOP datasets, the proposed method achieved the state-of-the-art pose estimation accuracies. The proposed method can eliminate the 3D information loss and drift of joint positions that can occur during estimation of human pose.},
  archive      = {J_PR},
  author       = {Yeonho Kim and Daijin Kim},
  doi          = {10.1016/j.patcog.2020.107462},
  journal      = {Pattern Recognition},
  pages        = {107462},
  shortjournal = {Pattern Recognition},
  title        = {A CNN-based 3D human pose estimation based on projection of depth and ridge data},
  volume       = {106},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Every node counts: Self-ensembling graph convolutional
networks for semi-supervised learning. <em>PR</em>, <em>106</em>,
107451. (<a href="https://doi.org/10.1016/j.patcog.2020.107451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph convolutional network (GCN) provides a powerful means for graph-based semi-supervised tasks. However, as a localized first-order approximation of spectral graph convolution, the classic GCN can not take full advantage of unlabeled data , especially when the unlabeled node is far from labeled ones. To capitalize on the information from unlabeled nodes to boost the training for GCN, we propose a novel framework named Self-Ensembling GCN (SEGCN), which marries GCN with Mean Teacher – a powerful self-ensemble learning mechanism for semi-supervised task. SEGCN contains a student model and a teacher model. As a student, it not only learns to correctly classify the labeled nodes, but also tries to be consistent with the teacher on unlabeled nodes in more challenging situations, such as a high dropout rate and graph corrosion. As a teacher, it averages the student model weights and generates more accurate predictions to lead the student. In such a mutual-promoting process, both labeled and unlabeled samples can be fully utilized for backpropagating effective gradients to train GCN. In a variety of semi-supervised classification benchmarks, i.e. Citeseer, Cora, Pubmed and NELL, we validate that the proposed method matches the state of the arts in the classification accuracy . The code is publicly available at https://github.com/RoyalVane/SEGCN .},
  archive      = {J_PR},
  author       = {Yawei Luo and Rongrong Ji and Tao Guan and Junqing Yu and Ping Liu and Yi Yang},
  doi          = {10.1016/j.patcog.2020.107451},
  journal      = {Pattern Recognition},
  pages        = {107451},
  shortjournal = {Pattern Recognition},
  title        = {Every node counts: Self-ensembling graph convolutional networks for semi-supervised learning},
  volume       = {106},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Orthogonal neighborhood preserving discriminant analysis
with patch embedding for face recognition. <em>PR</em>, <em>106</em>,
107450. (<a href="https://doi.org/10.1016/j.patcog.2020.107450">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intuitively, all facial images of a person are located on or near a manifold in the high-dimensional image space, and the process of face recognition can be regarded as the recovery process of multiple low-dimensional manifolds. To preserve the manifold structure information of intra-class samples after dimensionality reduction, we proposed a patch-based multi-manifold orthogonal neighborhood-preserving discriminant analysis algorithm, namely ONPDA. From the perspective of path alignment, we consider the intra-class compactness, intra-class structure and inter-class separability simultaneously. Moreover, we infuse intra-class structure information described by the sample reconstruction into intra-class compactness loss, considering the compactness of two reconstruction groups instead of sample pairs in the same class. By analyzing the relationship between the projection direction and the maximum inter-class margin, we select the samples that should participate in the inter-class separability on the patch. Meanwhile, a fast orthogonalization method is performed to obtain the orthogonal projection matrix . Besides, we perform ONPDA in reproducing kernel Hilbert space which gives rise to nonlinear maps, resulting in the kernel ONPDA (KONPDA). Experimental results compared with some state-of-the-art methods on a toy dataset and several benchmark face image databases demonstrate the effectiveness of ONPDA and KONPDA.},
  archive      = {J_PR},
  author       = {Liangchen Hu and Wensheng Zhang},
  doi          = {10.1016/j.patcog.2020.107450},
  journal      = {Pattern Recognition},
  pages        = {107450},
  shortjournal = {Pattern Recognition},
  title        = {Orthogonal neighborhood preserving discriminant analysis with patch embedding for face recognition},
  volume       = {106},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multistage attention network for image inpainting.
<em>PR</em>, <em>106</em>, 107448. (<a
href="https://doi.org/10.1016/j.patcog.2020.107448">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image inpainting refers to the process of restoring the mask regions of damaged images. Existing inpainting algorithms have exhibited outstanding performance on certain inpainting tasks that are focused on recovering small masks or square masks. Tasks that attempt to reconstruct large proportion of damaged images can still be improved. Although many attention-related algorithms have been proposed to solve image inpainting tasks, most of them ignore the requirements to balancing the detail and style level. In this paper, we propose a novel image inpainting method for large-scale irregular masks. We introduce a special multistage attention module that considers structure consistency and detail fineness. The proposed multistage attention module operates in a coarse to-fine manner, where the early stage performs large feature patch swapping and ensures the global consistency in images, and the next stage swaps small patches to refine the texture. Then, we adopt a partial convolution strategy to avoid the misuse of invalid data during convolution. Several losses are combined as the training objective function to generate excellent results with global consistency and exquisite detail. Qualitative and quantitative experiments on the Paris StreetView, CelebA, and Places2 datasets demonstrate the superior performance of the proposed approach compared with state-of-the-art models.},
  archive      = {J_PR},
  author       = {Ning Wang and Sihan Ma and Jingyuan Li and Yipeng Zhang and Lefei Zhang},
  doi          = {10.1016/j.patcog.2020.107448},
  journal      = {Pattern Recognition},
  pages        = {107448},
  shortjournal = {Pattern Recognition},
  title        = {Multistage attention network for image inpainting},
  volume       = {106},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Building outline extraction from ALS point clouds using
medial axis transform descriptors. <em>PR</em>, <em>106</em>, 107447.
(<a href="https://doi.org/10.1016/j.patcog.2020.107447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automatic building extraction and delineation from airborne LiDAR point cloud data of urban environments is still a challenging task due to the variety and complexity at which buildings appear. The Medial Axis Transform (MAT) is able to describe the geometric shape and topology of an object, but has never been applied for building roof outline extraction. It represents the shape of an object by its centerline, or skeleton structure instead of its boundary. Notably, end points of the MAT in principle coincide with corner points of building outlines. However, the MAT is sensitive to small boundary irregularities, which makes shape detection in airborne point clouds challenging. We propose a robust MAT-based method for detecting building corner points, which are then connected to form a building boundary polygon. First, we approximate the 2D MAT of a set of building edge points acquired by the alpha-shape algorithm to derive a so-called building roof skeleton. We then propose a hierarchical corner-aware segmentation to cluster skeleton points based on their properties which are the so-called separation angle, radius of the maximally inscribe circle, and defining edge point indices. From each segment, a corner point is then estimated by extrapolating the position of the zero radius inscribed circle based on the skeleton point positions within the segment. Our experiment uses point cloud datasets of Makassar, Indonesia and EYE-Amsterdam, The Netherlands. The average positional accuracy of the building outline results for Makassar and EYE-Amsterdam is 65 cm and 70 cm, respectively, which meet one-meter base map accuracy criteria. The results imply that skeletonization is a promising tool to extract relevant geometric information on e.g. building outlines even from far from perfect geographical point cloud data.},
  archive      = {J_PR},
  author       = {Elyta Widyaningrum and Ravi Y. Peters and Roderik C. Lindenbergh},
  doi          = {10.1016/j.patcog.2020.107447},
  journal      = {Pattern Recognition},
  pages        = {107447},
  shortjournal = {Pattern Recognition},
  title        = {Building outline extraction from ALS point clouds using medial axis transform descriptors},
  volume       = {106},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-view subspace clustering via simultaneously learning
the representation tensor and affinity matrix. <em>PR</em>,
<em>106</em>, 107441. (<a
href="https://doi.org/10.1016/j.patcog.2020.107441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view subspace clustering aims at separating data points into multiple underlying subspaces according to their multi-view features. Existing low-rank tensor representation-based multi-view subspace clustering algorithms are robust to noise and can preserve the high-order correlations of multi-view features. However, they may suffer from two common problems: (1) the local structures and different importance of each view feature are often neglected; (2) the low-rank representation tensor and affinity matrix are learned separately. To address these issues, we propose a unified framework to learn the Graph regularized Low-rank representation Tensor and Affinity matrix (GLTA) for multi-view subspace clustering. In the proposed GLTA framework, the tensor singular value decomposition-based tensor nuclear norm is adopted to explore the high-order cross-view correlations. The manifold regularization is exploited to preserve the local structures embedded in high-dimensional space. The importance of different features is automatically measured when constructing the final affinity matrix . An iterative algorithm is developed to solve GLTA using the alternating direction method of multipliers . Extensive experiments on seven challenging datasets demonstrate the superiority of GLTA over the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Yongyong Chen and Xiaolin Xiao and Yicong Zhou},
  doi          = {10.1016/j.patcog.2020.107441},
  journal      = {Pattern Recognition},
  pages        = {107441},
  shortjournal = {Pattern Recognition},
  title        = {Multi-view subspace clustering via simultaneously learning the representation tensor and affinity matrix},
  volume       = {106},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-dimensional classification via kNN feature
augmentation. <em>PR</em>, <em>106</em>, 107423. (<a
href="https://doi.org/10.1016/j.patcog.2020.107423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multi-dimensional classification (MDC), each training example is represented by a single instance (feature vector) while associated with multiple class variables, each of which specifies its class membership w.r.t. one specific class space. Most existing MDC approaches try to model dependencies among class variables in output space when inducing predictive functions, while the potential usefulness of manipulating feature space hasn’t been investigated. As a first attempt towards feature manipulation in input space for MDC, a simple yet effective approach named Kram is proposed which enriches the original feature space with augmented features based on k NN techniques. Specifically, simple counting statistics on the class membership of neighboring MDC examples as well as distance information between MDC examples and their k nearest neighbors are used to generate augmented feature vector. In this way, discriminative information from class space is expected to be brought into the feature space which would be helpful to the following MDC predictive model induction. To validate the effectiveness of the proposed feature augmentation techniques, comprehensive comparative studies are conducted over fifteen benchmark data sets. Compared to the original feature space, it is clearly shown that the k NN-augmented features generated by the proposed Kram approach can significantly improve generalization abilities of existing MDC approaches.},
  archive      = {J_PR},
  author       = {Bin-Bin Jia and Min-Ling Zhang},
  doi          = {10.1016/j.patcog.2020.107423},
  journal      = {Pattern Recognition},
  pages        = {107423},
  shortjournal = {Pattern Recognition},
  title        = {Multi-dimensional classification via kNN feature augmentation},
  volume       = {106},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast matching via ergodic markov chain for super-large
graphs. <em>PR</em>, <em>106</em>, 107418. (<a
href="https://doi.org/10.1016/j.patcog.2020.107418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In theory, graph matching is a combinatorial problem . One state-of-the-art technique in graph matching, called spectral matching, relaxes the matching problem for consistent correspondence into spectral decomposition of the affinity matrix of graphs, but the most variations of spectral based algorithms suffer from their O ( n 4 ) memory requirement. In this paper we propose a probabilistic spectral matching approach, in which the graph matching problem is formulated as an ergodic Markov chain, and the process of matching is addressed to reach the steady-state of the Markov chain. The approach decomposes the probability transition matrix , and solves the matching problem in O ( n 2 ) space complexity using limited computing resource and RAM . This property makes the approach suitable for super-large graphs matching (for example, graphs with the number of points over 1000). We evaluate our algorithm on both the synthetic and the real datasets, and demonstrate that the proposed approach is significantly faster, and consumes smaller memory than SM, RRWM and FaSM with no loss of accuracy.},
  archive      = {J_PR},
  author       = {Yali Zheng and Lili Pan and Jiye Qian and Hongliang Guo},
  doi          = {10.1016/j.patcog.2020.107418},
  journal      = {Pattern Recognition},
  pages        = {107418},
  shortjournal = {Pattern Recognition},
  title        = {Fast matching via ergodic markov chain for super-large graphs},
  volume       = {106},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graph-based boosting algorithm to learn labeled and
unlabeled data. <em>PR</em>, <em>106</em>, 107417. (<a
href="https://doi.org/10.1016/j.patcog.2020.107417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble learning is an effective technique to learn the information of data by combining multiple models. But usually the combined models are supervised learning algorithms which need a lot of labeled data to tune their parameters. Some ensemble learning algorithms were proposed to exploit the information of unlabeled data. These methods had to learn the samples with pseudo-labels due to the scarcity of labeled data. But it’s inevitable for the samples with pseudo-labels to bring wrong information during training process. In this paper, we will propose a novel graph-based boosting (GBB) algorithm to learn labeled and unlabeled data. GBB is a framework combining many models linearly. And pseudo-labels will not occur during training process. GBB will assign a new weighting vector for the labeled samples and a transformed similarity matrix for all samples to train the combined model at each iteration. We also extend GBB, termed as weighted GBB (WGBB), to learn imbalanced data by adding a weighting vector for the labeled data. Finally, 14 relatively balanced datasets and 22 imbalanced datasets are used to validate the performances of GBB and WGBB respectively. Experimental results illustrate that GBB can achieve a competitive performance and WGBB has an obvious advantage to handle classification problem of imbalanced data, comparing with other related algorithms.},
  archive      = {J_PR},
  author       = {Zheng Liu and Wei Jin and Ying Mu},
  doi          = {10.1016/j.patcog.2020.107417},
  journal      = {Pattern Recognition},
  pages        = {107417},
  shortjournal = {Pattern Recognition},
  title        = {Graph-based boosting algorithm to learn labeled and unlabeled data},
  volume       = {106},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Structure alignment of attributes and visual features for
cross-dataset person re-identification. <em>PR</em>, <em>106</em>,
107414. (<a href="https://doi.org/10.1016/j.patcog.2020.107414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cross-dataset person re-identification, it is challenging to address the problem of domain shift between training and test data. Although unsupervised domain adaptation methods have been developed, the performance is still much weaker compared with that of supervised methods because these models cannot follow a supervised optimization in unlabeled target domains. To address this problem, a transductive structure alignment-based self-reconstruction dictionary learning approach is proposed in this paper for cross-dataset person re-identification (PRID). Specifically, visual-attribute embedding is first learned to achieve knowledge transfer from the source domain to the target domain. In this process, visual-attribute structures are aligned via class prototype dictionaries to promote the discrimination of predicted semantic attributes by exploiting structure information between the visual feature and class prototype. Moreover, to mitigate domain shift, domain-invariant visual-attribute self-reconstruction is integrated into our dictionary learning framework. An identifier is then constructed by integrating the discriminativeness of attribute and compatibility matrix shared both source domain and target domain. Finally, the pre-learned model is tuned by selecting samples from the target domain which are not labeled but assigned pseudo-labels. Extensive experimental results on benchmark datasets show that our approach outperforms several state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Huafeng Li and Zhenyu Kuang and Zhengtao Yu and Jiebo Luo},
  doi          = {10.1016/j.patcog.2020.107414},
  journal      = {Pattern Recognition},
  pages        = {107414},
  shortjournal = {Pattern Recognition},
  title        = {Structure alignment of attributes and visual features for cross-dataset person re-identification},
  volume       = {106},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Can we automate diagrammatic reasoning? <em>PR</em>,
<em>106</em>, 107412. (<a
href="https://doi.org/10.1016/j.patcog.2020.107412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagrammatic reasoning (DR) problems are well known. However, solving DR problems represented in 4 × 1 Raven’s Progressive Matrix (RPM) form using computer vision and pattern recognition has not yet been tried. Emergence of deep learning techniques aided by advanced computing can be exploited to solve such DR problems. In this paper, we propose a new learning framework by combining LSTM and Convolutional LSTM to solve 4 × 1 DR problems. Initially, the elementary geometrical shapes in such problems are detected using a typical CNN-based detector. Next, relations of various shapes are analyzed and a high-level feature set is produced and processed in the LSTM framework. A new 4 × 1 DR dataset has been prepared and made available to the research community. We believe, it will be helpful in advancing this research further. We have compared our method with some of the existing frameworks that can be used for solving RPM-guided DR problems. We have recorded 18–20\% increase in the average prediction accuracy as compared to the prior frameworks when applied to RPM-guided DR problems. We believe the CV research community will be interested to carry out similar research, particularly to investigate the feasibility of solving other types of known DR problems.},
  archive      = {J_PR},
  author       = {Arif Ahmed Sekh and Debi Prosad Dogra and Samarjit Kar and Partha Pratim Roy and Dilip K. Prasad},
  doi          = {10.1016/j.patcog.2020.107412},
  journal      = {Pattern Recognition},
  pages        = {107412},
  shortjournal = {Pattern Recognition},
  title        = {Can we automate diagrammatic reasoning?},
  volume       = {106},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New label propagation algorithm with pairwise constraints.
<em>PR</em>, <em>106</em>, 107411. (<a
href="https://doi.org/10.1016/j.patcog.2020.107411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The label propagation algorithm is a well-known semi-supervised clustering method , which uses pre-given partial labels as constraints to predict the labels of unlabeled data . However, the algorithm has the following limitations: (1) it does not fully consider the misalignment between the pre-given labels and clustering labels, and (2) it only uses label information as clustering constraints. Real applications not only contain partial label information but pairwise constraints on a dataset. To overcome these deficiencies, a new version of the label propagation algorithm is proposed, which makes use of pairwise relations of labels as constraints to construct an optimization model for spreading labels. Experimental analysis was used to compare the proposed algorithm with 8 other semi-supervised clustering algorithms on 11 benchmark datasets. The experimental results demonstrated that the proposed algorithm is more effective than other algorithms.},
  archive      = {J_PR},
  author       = {Liang Bai and Junbin Wang and Jiye Liang and Hangyuan Du},
  doi          = {10.1016/j.patcog.2020.107411},
  journal      = {Pattern Recognition},
  pages        = {107411},
  shortjournal = {Pattern Recognition},
  title        = {New label propagation algorithm with pairwise constraints},
  volume       = {106},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Structure-aware human pose estimation with graph
convolutional networks. <em>PR</em>, <em>106</em>, 107410. (<a
href="https://doi.org/10.1016/j.patcog.2020.107410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human pose estimation is the task of localizing body key points from still images. As body key points are inter-connected, it is desirable to model the structural relationships between body key points to further improve the localization performance . In this paper, based on original graph convolutional networks , we propose a novel model, termed Pose Graph Convolutional Network (PGCN), to exploit these important relationships for pose estimation. Specifically, our model builds a directed graph between body key points according to the natural compositional model of a human body. Each node (key point) is represented by a 3-D tensor consisting of multiple feature maps, initially generated by our backbone network , to retain accurate spatial information. Furthermore, attention mechanism is presented to focus on crucial edges (structured information) between key points. PGCN is then learned to map the graph into a set of structure-aware key point representations which encode both structure of human body and appearance information of specific key points. Additionally, we propose two modules for PGCN, i.e., the Local PGCN (L-PGCN) module and Non-Local PGCN (NL-PGCN) module. The former utilizes spatial attention to capture the correlations between the local areas of adjacent key points to refine the location of key points. While the latter captures long-range relationships via non-local operation to associate the challenging key points. By equipping with these two modules, our PGCN can further improve localization performance . Experiments both on single- and multi-person estimation benchmark datasets show that our method consistently outperforms competing state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Yanrui Bin and Zhao-Min Chen and Xiu-Shen Wei and Xinya Chen and Changxin Gao and Nong Sang},
  doi          = {10.1016/j.patcog.2020.107410},
  journal      = {Pattern Recognition},
  pages        = {107410},
  shortjournal = {Pattern Recognition},
  title        = {Structure-aware human pose estimation with graph convolutional networks},
  volume       = {106},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scale balance for prototype-based binary quantization.
<em>PR</em>, <em>106</em>, 107409. (<a
href="https://doi.org/10.1016/j.patcog.2020.107409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, prototype-based binary quantization (PBQ) is a promising solution for the approximate nearest neighbor search problem, which simultaneously preserves the affinity structures of prototypes in both Euclidean space as well as those of their codes in binary space. To learn longer binary codes , space decomposition based on product quantization is usually adopted. In practice, we find that the scale between Euclidean distance and Hamming distance usually varies across these decomposed subspaces, which degenerates the performance of PBQ based methods. We make an attempt to balance the scale of these subspaces via a joint optimization problem in the classic PBQ model, and present both an iterative and alternate algorithm for optimization. We conducted experiments on 6 public databases, and demonstrated that our scale balancing based methods SKMH and SABQ outperform state-of-the-art hashing methods including popular prototype-based binary quantization methods, with up to 81.62\% relative performance gains when learning 256-bit binary codes .},
  archive      = {J_PR},
  author       = {Zhiyang Li and Wenyu Qu and Yuan Cao and Heng Qi and Milos Stojmenovic and Jia Hu},
  doi          = {10.1016/j.patcog.2020.107409},
  journal      = {Pattern Recognition},
  pages        = {107409},
  shortjournal = {Pattern Recognition},
  title        = {Scale balance for prototype-based binary quantization},
  volume       = {106},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Manifold learning for user profiling and identity
verification using motion sensors. <em>PR</em>, <em>106</em>, 107408.
(<a href="https://doi.org/10.1016/j.patcog.2020.107408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile devices are becoming ubiquitous and being increasingly used for data-sensitive activities such as communication, personal media storage, and banking. The protection of such data commonly relies on passwords and biometric traits such as fingerprints. These methods perform the user authentication sporadically and often require action from the user, which may make them susceptible to spoofing attacks . This scenario can be mitigated if we bring to bear motion-sensing based methods for authentication , which operate continuously and without requiring user action, hence are harder to attack. Such methods could be used allied with traditional authentication methods or on their own. This paper explores this idea in a novel user-agnostic approach for identity verification based on motion traits acquired by mobile sensors. The proposed approach does not require user-specific training before deployment in mobile devices nor does it require any extra sensor in the device. This solution is capable of learning a user profiling manifold from a small user subset and extend it to unknown users. We validated the proposal on two public datasets. The reported experiments demonstrate remarkable results under a cross-dataset protocol and an open-set setup. Moreover, we performed several analyses aiming at answering critical questions of a biometric method and the presented solution.},
  archive      = {J_PR},
  author       = {Geise Santos and Paulo Henrique Pisani and Roberto Leyva and Chang-Tsun Li and Tiago Tavares and Anderson Rocha},
  doi          = {10.1016/j.patcog.2020.107408},
  journal      = {Pattern Recognition},
  pages        = {107408},
  shortjournal = {Pattern Recognition},
  title        = {Manifold learning for user profiling and identity verification using motion sensors},
  volume       = {106},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving the performance of lightweight CNNs for binary
classification using quadratic mutual information regularization.
<em>PR</em>, <em>106</em>, 107407. (<a
href="https://doi.org/10.1016/j.patcog.2020.107407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose regularized lightweight deep convolutional neural network models, capable of effectively operating in real-time on-drone for high-resolution video input. Furthermore, we study the impact of hinge loss against the cross entropy loss on the classification performance, mainly in binary classification problems. Finally, we propose a novel regularization method motivated by the Quadratic Mutual Information, in order to improve the generalization ability of the utilized models. Extensive experiments on various binary classification problems involved in autonomous systems are performed, indicating the effectiveness of the proposed models. The experimental evaluation on four datasets indicates that hinge loss is the optimal choice for binary classification problems, considering lightweight deep models. Finally, the effectiveness of the proposed regularizer in enhancing the generalization ability of the proposed models is also validated.},
  archive      = {J_PR},
  author       = {Maria Tzelepi and Anastasios Tefas},
  doi          = {10.1016/j.patcog.2020.107407},
  journal      = {Pattern Recognition},
  pages        = {107407},
  shortjournal = {Pattern Recognition},
  title        = {Improving the performance of lightweight CNNs for binary classification using quadratic mutual information regularization},
  volume       = {106},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ellipse fitting by spatial averaging of random ensembles.
<em>PR</em>, <em>106</em>, 107406. (<a
href="https://doi.org/10.1016/j.patcog.2020.107406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Earlier ellipse fitting methods often consider the algebraic and geometric forms of the ellipse. The work presented here makes use of an ensemble to provide better results. The method proposes a new ellipse parametrization based on the coordinates of both foci, and the distance between them and each point of the ellipse where the Euclidean norm is applied. Besides, a certain number of subsets are uniformly drawn without replacement from the overall training set which allows estimating the center of the distribution robustly by employing the L1 median of each estimated focus. An additional postprocessing stage is proposed to filter out the effect of bad fits. In order to evaluate the performance of this method, four different error measures were considered. Results show that our proposal outperforms all its competitors, especially when higher levels of outliers are presented. Several synthetic and real data tests were developed and confirmed such finding.},
  archive      = {J_PR},
  author       = {Karl Thurnhofer-Hemsi and Ezequiel López-Rubio and Elidia Beatriz Blázquez-Parra and M. Carmen Ladrón-de-Guevara-Muñoz and Óscar David de-Cózar-Macias},
  doi          = {10.1016/j.patcog.2020.107406},
  journal      = {Pattern Recognition},
  pages        = {107406},
  shortjournal = {Pattern Recognition},
  title        = {Ellipse fitting by spatial averaging of random ensembles},
  volume       = {106},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). U2-net: Going deeper with nested u-structure for salient
object detection. <em>PR</em>, <em>106</em>, 107404. (<a
href="https://doi.org/10.1016/j.patcog.2020.107404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we design a simple yet powerful deep network architecture , U 2 -Net, for salient object detection (SOD). The architecture of our U 2 -Net is a two-level nested U-structure. The design has the following advantages: (1) it is able to capture more contextual information from different scales thanks to the mixture of receptive fields of different sizes in our proposed ReSidual U-blocks (RSU), (2) it increases the depth of the whole architecture without significantly increasing the computational cost because of the pooling operations used in these RSU blocks. This architecture enables us to train a deep network from scratch without using backbones from image classification tasks. We instantiate two models of the proposed architecture, U 2 -Net (176.3 MB, 30 FPS on GTX 1080Ti GPU) and U 2 -Net† (4.7 MB, 40 FPS), to facilitate the usage in different environments. Both models achieve competitive performance on six SOD datasets. The code is available: https://github.com/NathanUA/U-2-Net .},
  archive      = {J_PR},
  author       = {Xuebin Qin and Zichen Zhang and Chenyang Huang and Masood Dehghan and Osmar R. Zaiane and Martin Jagersand},
  doi          = {10.1016/j.patcog.2020.107404},
  journal      = {Pattern Recognition},
  pages        = {107404},
  shortjournal = {Pattern Recognition},
  title        = {U2-net: Going deeper with nested U-structure for salient object detection},
  volume       = {106},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel online action detection framework from untrimmed
video streams. <em>PR</em>, <em>106</em>, 107396. (<a
href="https://doi.org/10.1016/j.patcog.2020.107396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online temporal action localization from an untrimmed video stream is a challenging problem in computer vision . It is challenging because of i) in an untrimmed video stream, more than one action instance may appear, including background scenes, and ii) in online settings, only past and current information is available. Therefore, temporal priors, such as the average action duration of training data, which have been exploited by previous action detection methods, are not suitable for this task because of the high intra-class variation in human actions. We propose a novel online action detection framework that considers actions as a set of temporally ordered subclasses and leverages a future frame generation network to cope with the limited information issue associated with the problem outlined above. Additionally, we augment our data by varying the lengths of videos to allow the proposed method to learn about the high intra-class variation in human actions. We evaluate our method using two benchmark datasets, THUMOS’14 and ActivityNet, for an online temporal action localization scenario and demonstrate that the performance is comparable to state-of-the-art methods that have been proposed for offline settings.},
  archive      = {J_PR},
  author       = {Da-Hye Yoon and Nam-Gyu Cho and Seong-Whan Lee},
  doi          = {10.1016/j.patcog.2020.107396},
  journal      = {Pattern Recognition},
  pages        = {107396},
  shortjournal = {Pattern Recognition},
  title        = {A novel online action detection framework from untrimmed video streams},
  volume       = {106},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Recurrent bag-of-features for visual information analysis.
<em>PR</em>, <em>106</em>, 107380. (<a
href="https://doi.org/10.1016/j.patcog.2020.107380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Learning (DL) has provided powerful tools for visual information analysis. For example, Convolutional Neural Networks (CNNs) are excelling in complex and challenging image analysis tasks by extracting meaningful feature vectors with high discriminative power . However, these powerful feature vectors are crushed through the pooling layers of the network, that usually implement the pooling operation in a less sophisticated manner. This can lead to significant information loss, especially in cases where the informative content of the data is sequentially distributed over the spatial or temporal dimension, e.g., videos, which often require extracting fine-grained temporal information. A novel stateful recurrent pooling approach, that can overcome the aforementioned limitations, is proposed in this paper. The proposed method is inspired by the well-known Bag-of-Features (BoF) model, but employs a stateful trainable recurrent quantizer, instead of plain static quantization, allowing for efficiently processing sequential data and encoding both their temporal, as well as their spatial aspects. The effectiveness of the proposed Recurrent BoF model to enclose spatio-temporal information compared to other competitive methods is demonstrated using six different datasets and two different tasks.},
  archive      = {J_PR},
  author       = {Marios Krestenitis and Nikolaos Passalis and Alexandros Iosifidis and Moncef Gabbouj and Anastasios Tefas},
  doi          = {10.1016/j.patcog.2020.107380},
  journal      = {Pattern Recognition},
  pages        = {107380},
  shortjournal = {Pattern Recognition},
  title        = {Recurrent bag-of-features for visual information analysis},
  volume       = {106},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A survey on deep learning methods for scene flow estimation.
<em>PR</em>, <em>106</em>, 107378. (<a
href="https://doi.org/10.1016/j.patcog.2020.107378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, computer vision has achieved remarkable accomplishments in many domains under the thriving of deep learning . Scene flow estimation turns from the classical manual feature construction to the deep convolutional neural network (DCNN) approaches. In this paper, we review recent works about scene flow, mainly focusing on DCNN methods. We present some milestones of scene flow in recent years, and categorize these methods into supervised and unsupervised based methods. Meanwhile, we also review some multi-task methods related to scene flow. At last, we present a performance comparison among different methods.},
  archive      = {J_PR},
  author       = {Jiajie Liu and Han Li and Ruihong Wu and Qingyun Zhao and Yiyou Guo and Long Chen},
  doi          = {10.1016/j.patcog.2020.107378},
  journal      = {Pattern Recognition},
  pages        = {107378},
  shortjournal = {Pattern Recognition},
  title        = {A survey on deep learning methods for scene flow estimation},
  volume       = {106},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Non-rigid infrared and visible image registration by
enhanced affine transformation. <em>PR</em>, <em>106</em>, 107377. (<a
href="https://doi.org/10.1016/j.patcog.2020.107377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image registration is a prerequisite for infrared (IR) and visible (VIS) image fusion. In practical application, most scenes are not planar and there is significant distinctness between IR and VIS cameras. Therefore, for non-rigid IR and VIS image registration, non-linear transformation is more applicable than affine transformation. Typically, non-linear transformation is modeled with point feature. However, this can degrade the generalization ability of transformation model and increase computational complexity . Aim at this problem, we propose an enhanced affine transformation (EAT) for non-rigid IR and VIS image registration. In this paper, image registration is transformed into point set registration and then the optimal EAT model constructed by global deformation is estimated from local feature . At first, a Gaussian-fields-based objective function is established and simplified by using the potential correspondence between an image pair. With the combination of affine and polynomial transformation , the EAT model is then proposed to describe the regular pattern of non-rigid and global deformation between an image pair. Finally, a coarse-to-fine strategy based on quasi-Newton method is designed and applied to determine the optimal transformation coefficients from edge point feature of IR and VIS images, in order to accomplish non-rigid image registration. The qualitative and quantitative comparisons on synthesized point sets and real images demonstrate that the proposed method is superior over the state-of-the-art methods in the accuracy and efficiency of image registration.},
  archive      = {J_PR},
  author       = {Chaobo Min and Yan Gu and Yingjie Li and Feng Yang},
  doi          = {10.1016/j.patcog.2020.107377},
  journal      = {Pattern Recognition},
  pages        = {107377},
  shortjournal = {Pattern Recognition},
  title        = {Non-rigid infrared and visible image registration by enhanced affine transformation},
  volume       = {106},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparallel support vector machine with large margin
distribution for pattern classification. <em>PR</em>, <em>106</em>,
107374. (<a href="https://doi.org/10.1016/j.patcog.2020.107374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The large margin distribution machine (LDM) combines the working principle of support vector machine (SVM) and the margin distribution to directly improve the algorithm&#39;s generalization. The margin distribution can be expressed with the margin mean and margin variance. It has been proved to be an efficient algorithm for binary classification. Inspired by the LDM, a novel classifier termed as LMD-NPSVM is proposed to improve the generalization performance of the nonparallel support vector machine (NPSVM) in this paper. Firstly, to meet the structure of NPSVM, the large margin distribution is reconstructed. Then, the linear LMD-NPSVM is built by introducing the reconstructed margin distribution into NPSVM. In addition, the linear case is extended to the nonlinear case with a kernel trick. All experiments show that our LMD-NPSVM is superior to the state-of-the-art algorithms in generalization performance.},
  archive      = {J_PR},
  author       = {Liming Liu and Maoxiang Chu and Rongfen Gong and Yongcheng Peng},
  doi          = {10.1016/j.patcog.2020.107374},
  journal      = {Pattern Recognition},
  pages        = {107374},
  shortjournal = {Pattern Recognition},
  title        = {Nonparallel support vector machine with large margin distribution for pattern classification},
  volume       = {106},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Probabilistic SVM classifier ensemble selection based on
GMDH-type neural network. <em>PR</em>, <em>106</em>, 107373. (<a
href="https://doi.org/10.1016/j.patcog.2020.107373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support vector machine (SVM) provides a good classification and regression ability, especially, for small sample learning. However, in practice, the learning ability of implemented SVM is occasionally far from the expected level. Group method of data handling neural network (GMDH-NN) has been applied in various fields for pattern recognition and data mining. It makes it possible to automatically find interrelations in data, to select an optimal structure of network or model and to improve the accuracy of existing algorithms. In this work we propose to take the advantages of GMDH-NN for further increasing the classification performance of SVM. One weakness of the symmetric regularity criterion of GMDH-NN is that if one of the input attributes has a relatively big range, then it may overcome the other attributes. Thus, we first define a standardized symmetric regularity criterion (SSRC) to evaluate and select the candidate models, and optimize a classifier ensemble selection approach. Secondly, we define a novel structure of initial model of GMDH-NN which is from the posterior probability outputs of SVMs. These probabilistic outputs are generated from the improved Platt’s probabilistic outputs. Thirdly, in real classification tasks , different classifiers usually have different classification advantages. So we use probabilistic SVM as base learner and integrate the probabilistic SVMs with GMDH-NN, and then propose a special classifier ensemble selection approach for probabilistic SVM classifiers based on GMDH-NN called GMDH-PSVM. Moreover, we use the Borda sorting and Random weighted Borda sorting to discuss the results of our experiments. Experiments on standard UCI datasets demonstrate the effectiveness of our method.},
  archive      = {J_PR},
  author       = {Lixiang Xu and Xiaofeng Wang and Lu Bai and Jin Xiao and Qi Liu and Enhong Chen and Xiaoyi Jiang and Bin Luo},
  doi          = {10.1016/j.patcog.2020.107373},
  journal      = {Pattern Recognition},
  pages        = {107373},
  shortjournal = {Pattern Recognition},
  title        = {Probabilistic SVM classifier ensemble selection based on GMDH-type neural network},
  volume       = {106},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust twin support vector regression based on rescaled
hinge loss. <em>PR</em>, <em>105</em>, 107395. (<a
href="https://doi.org/10.1016/j.patcog.2020.107395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, with the help of the rescaled Hinge loss, we propose a twin support vector regression (TSVR) model that is robust to noise. The corresponding optimization problem turns out to be non-convex with smooth l 2 regularizer. To solve the problem efficiently, we convert it to its dual form, thereby transforming it into a convex optimization problem. An algorithm, named Res-TSVR, is provided to solve the formulated dual problem. The proof of the convergence of the algorithm is given. It is shown that the maximum number of iterations to achieve an ε-precision solution to the dual problem is O ( log ( 1 ε ) ) O(log(1ε)) . We conduct a set of numerical experiments to compare the proposed method with the recently proposed robust approaches of TSVR and the standard SVR. Experimental results reveal that the proposed approach outperforms other robust methods of TSVR in terms of generalization performance and robustness to noise with comparable training time. This claim is based on the experiments performed using seven real-world data sets and three synthetic data sets.},
  archive      = {J_PR},
  author       = {Manisha Singla and Debdas Ghosh and K.K. Shukla and Witold Pedrycz},
  doi          = {10.1016/j.patcog.2020.107395},
  journal      = {Pattern Recognition},
  pages        = {107395},
  shortjournal = {Pattern Recognition},
  title        = {Robust twin support vector regression based on rescaled hinge loss},
  volume       = {105},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Video anomaly detection and localization using motion-field
shape description and homogeneity testing. <em>PR</em>, <em>105</em>,
107394. (<a href="https://doi.org/10.1016/j.patcog.2020.107394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detection and localization of abnormal behaviors in surveillance videos of crowded scenes is challenging, where high-density people and various objects performing highly unpredictable motions lead to severe occlusions, making object segmentation and tracking extremely difficult. We associate the optical flows between multiple frames to capture short-term trajectories and introduce the histogram-based shape descriptor to describe such short-term trajectories, which reflects faithfully the motion trend and details in local patches. Furthermore, we propose a method to detect anomalies over time and space by judging whether the similarities between the testing sample and the retrieved K -NN samples follow the pattern distribution of homogeneous intra-class similarities, which is unsupervised one-class learning requiring no clustering nor prior assumption. Such a scheme can adapt to the whole scene, since the probability is used to judge and the calculation of probability is not affected by motion distortions arising from perspective distortion, which gains advantage over the existing solutions. We conduct experiments on real-world surveillance videos , and the results demonstrate that the proposed method can reliably detect and locate the abnormal events in video sequences, outperforming the state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Xinfeng Zhang and Su Yang and Jiulong Zhang and Weishan Zhang},
  doi          = {10.1016/j.patcog.2020.107394},
  journal      = {Pattern Recognition},
  pages        = {107394},
  shortjournal = {Pattern Recognition},
  title        = {Video anomaly detection and localization using motion-field shape description and homogeneity testing},
  volume       = {105},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Person identification using EEG channel selection with
hybrid flower pollination algorithm. <em>PR</em>, <em>105</em>, 107393.
(<a href="https://doi.org/10.1016/j.patcog.2020.107393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, electroencephalogram (EEG) signal presents a great potential for a new biometric system to deal with a cognitive task. Several studies defined the EEG with uniqueness features, universality, and natural robustness that can be used as a new track to prevent spoofing attacks . The EEG signals are the graphical recording of the brain electrical activities which can be measured by placing electrodes (channels) in various positions of the scalp. With a large number of channels, some channels have very important information for biometric system while others not. The channel selection problem has been recently formulated as an optimisation problem and solved by optimisation techniques. This paper proposes hybrid optimisation techniques based on binary flower pollination algorithm (FPA) and β -hill climbing (called FPA β -hc) for selecting the most relative EEG channels (i.e., features) that come up with efficient accuracy rate of personal identification. Each EEG signals with three different groups of EEG channels have been utilized (i.e., time domain, frequency domain, and time-frequency domain). The FPA β -hc is measured using a standard EEG signal dataset, namely, EEG motor movement/imagery dataset with a real world data taken from 109 persons each with 14 different cognitive tasks using 64 channels. To evaluate the performance of the FPA β -hc, five measurement criteria are considered:accuracy (Acc), (ii) sensitivity (Sen), (iii) F-score (F_s), (v) specificity (Spe), and (iv) number of channels selected (No. Ch). The proposed method is able to identify the personals with high Acc, Sen., F_s, Spe, and less number of channels selected. Interestingly, the experimental results suggest that FPA β -hc is able to reduce the number of channels with accuracy rate up to 96\% using time-frequency domain features. For comparative evaluation , the proposed method is able to achieve results better than those produced by binary-FPA-OPF method using the same EEG motor movement/imagery datasets. In a nutshell, the proposed method can be very beneficial for effective use of EEG signals in biometric applications.},
  archive      = {J_PR},
  author       = {Zaid Abdi Alkareem Alyasseri and Ahamad Tajudin Khader and Mohammed Azmi Al-Betar and Osama Ahmad Alomari},
  doi          = {10.1016/j.patcog.2020.107393},
  journal      = {Pattern Recognition},
  pages        = {107393},
  shortjournal = {Pattern Recognition},
  title        = {Person identification using EEG channel selection with hybrid flower pollination algorithm},
  volume       = {105},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Reinterpreting CTC training as iterative fitting.
<em>PR</em>, <em>105</em>, 107392. (<a
href="https://doi.org/10.1016/j.patcog.2020.107392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The connectionist temporal classification (CTC) enables end-to-end sequence learning by maximizing the probability of correctly recognizing sequences during training. The outputs of a CTC-trained model tend to form a series of spikes separated by strongly predicted blanks, know as the spiky problem. To figure out the reason for it, we reinterpret the CTC training process as an iterative fitting task that is based on frame-wise cross-entropy loss. It offers us an intuitive way to compare target probabilities with model outputs for each iteration, and explain how the model outputs gradually turns spiky. Inspired by it, we put forward two ways to modify the CTC training. The experiments demonstrate that our method can well solve the spiky problem and moreover, lead to faster convergence over various training settings. Beside this, the reinterpretation of CTC, as a brand new perspective, may be potentially useful in other situations. The code is publicly available at https://github.com/hzli-ucas/caffe/tree/ctc.},
  archive      = {J_PR},
  author       = {Hongzhu Li and Weiqiang Wang},
  doi          = {10.1016/j.patcog.2020.107392},
  journal      = {Pattern Recognition},
  pages        = {107392},
  shortjournal = {Pattern Recognition},
  title        = {Reinterpreting CTC training as iterative fitting},
  volume       = {105},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optical flow-based structure-from-motion for the
reconstruction of epithelial surfaces. <em>PR</em>, <em>105</em>,
107391. (<a href="https://doi.org/10.1016/j.patcog.2020.107391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper details a novel optical flow-based structure from motion (SfM) approach for the reconstruction of surfaces with few textures using video sequences acquired under strong illumination changes. An original image search and grouping strategy allows to reconstruct each 3D scene point using a large set of 2D homologous points extracted from a reference image and its superimposed images acquired from different viewpoints. A variational optical flow scheme with a descriptor-based data term leads to a robust, accurate and dense homologous point determination between the image pairs. Thus, contrary to classical SfM usable for textured scenes, the proposed dense point cloud reconstruction algorithm requires neither a feature point tracking method nor any multi-view stereo technique. The performance of the proposed SfM approach is assessed on phantoms with known ground truth and on very complex patient data of various medical examinations and image modalities.},
  archive      = {J_PR},
  author       = {Tan-Binh Phan and Dinh-Hoan Trinh and Didier Wolf and Christian Daul},
  doi          = {10.1016/j.patcog.2020.107391},
  journal      = {Pattern Recognition},
  pages        = {107391},
  shortjournal = {Pattern Recognition},
  title        = {Optical flow-based structure-from-motion for the reconstruction of epithelial surfaces},
  volume       = {105},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning EEG topographical representation for classification
via convolutional neural network. <em>PR</em>, <em>105</em>, 107390. (<a
href="https://doi.org/10.1016/j.patcog.2020.107390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalography (EEG) topographical representation (ETR) can monitor regional brain activities and is emerging as a successful technique for causally exploring cortical mechanisms and connections. However, it is a challenge to find a robust method supporting high-dimensional EEG data with low signal-to-noise ratios from multiple objects and multiple channels. To address this issue, a new ETR energy calculation method for learning the EEG patterns of brain activities using a convolutional neural network is reported. It is able to customize temporal ETR training and recognize multiple objects within a common learning model. Specifically, an open-access dataset from the 2008 Brain-Computer Interface (BCI) Competition IV-2a is used for classification of five classes containing four Motor Imagery actions and one relax action. The proposed classification framework outperforms the best state-of-the-art classification method by 10.11\% in average subject accuracy. Furthermore, by studying the ETR parameter optimization, a user interface for BCI applications is obtained and a real-time method implemented.},
  archive      = {J_PR},
  author       = {Meiyan Xu and Junfeng Yao and Zhihong Zhang and Rui Li and Baorong Yang and Chunyan Li and Jun Li and Junsong Zhang},
  doi          = {10.1016/j.patcog.2020.107390},
  journal      = {Pattern Recognition},
  pages        = {107390},
  shortjournal = {Pattern Recognition},
  title        = {Learning EEG topographical representation for classification via convolutional neural network},
  volume       = {105},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scoring disease-microRNA associations by integrating disease
hierarchy into graph convolutional networks. <em>PR</em>, <em>105</em>,
107385. (<a href="https://doi.org/10.1016/j.patcog.2020.107385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we present an updated predictor DimiG 2.0, which uses a semi-supervised multi-label graph convolutional network (GCN) to infer disease-associated microRNAs (miRNAs) on an interaction network between protein coding genes (PCGs) and miRNAs using disease-PCG associations. DimiG 2.0 benefits from integrating the hierarchy of diseases into the GCN. DimiG 2.0 has the following updates: 1) It incorporates the hierarchy of diseases to regularize the GCN, encouraging diseases in the hierarchy to share similar miRNAs. 2) It integrates the PCGs with interacting partners but without associated diseases into model training, these unlabeled PCGs increase the size of the constructed interaction network. 3) It is able to predict associated miRNAs for 1017 diseases (updated from 248). 4) It updates expression data across tissues from the latest GTEx v7, and the expression values are quantified in Transcripts Per Million (TPM). Our results show that DimiG 2.0 outperforms state-of-the-art semi-supervised and supervised methods on the constructed benchmarked sets.},
  archive      = {J_PR},
  author       = {Xiaoyong Pan and Hong-Bin Shen},
  doi          = {10.1016/j.patcog.2020.107385},
  journal      = {Pattern Recognition},
  pages        = {107385},
  shortjournal = {Pattern Recognition},
  title        = {Scoring disease-microRNA associations by integrating disease hierarchy into graph convolutional networks},
  volume       = {105},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). BrainPrint: EEG biometric identification based on analyzing
brain connectivity graphs. <em>PR</em>, <em>105</em>, 107381. (<a
href="https://doi.org/10.1016/j.patcog.2020.107381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Research on brain biometrics using electroencephalographic (EEG) signals has received increasing attentions in recent years. In particular, it has been recognized that the brain functional connectivity reflects individual variability. However, many questions need to be answered before we can properly use distinctive characteristics of brain connectivity for biometric applications. This paper proposes a graph-based method for EEG biometric identification. It consists of a network estimation module to generate brain connectivity networks and a graph analysis module to generate topological features based on brain networks. Specifically, we investigate seven different connectivity metrics for the network estimation module, each of which is characterized by a certain signal interaction mechanism, defining a peculiar subjective brain network. A new connectivity metric is proposed based on the algorithmic complexity of EEG signals from a information-theoretic perspective. Meanwhile, six nodal features and six global features are proposed and studied for the graph analysis module. A comprehensive evaluation is carried out to assess the impact of different connectivity metrics, graph features, and EEG frequency bands on biometric identification performance. The results demonstrate that the graph-based method proposed in this study is effective in improving the recognition rate and inter-state stability of EEG-based biometric identification systems. Our findings about the network patterns and graph features bring a further understanding of distinctiveness of humans’ EEG functional connectivity and provide useful guidance for the design of graph-based EEG biometric systems.},
  archive      = {J_PR},
  author       = {Min Wang and Jiankun Hu and Hussein A. Abbass},
  doi          = {10.1016/j.patcog.2020.107381},
  journal      = {Pattern Recognition},
  pages        = {107381},
  shortjournal = {Pattern Recognition},
  title        = {BrainPrint: EEG biometric identification based on analyzing brain connectivity graphs},
  volume       = {105},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Supervised dimensionality reduction of proportional data
using mixture estimation. <em>PR</em>, <em>105</em>, 107379. (<a
href="https://doi.org/10.1016/j.patcog.2020.107379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, an effective novel approach for dimensionality reduction of labeled proportional data is proposed. By avoiding formulating an eigenvalue problem and constructing a neighborhood graph , the introduced method mitigates some of the major problems from which the well-known algorithms in this category suffer. These disadvantages include problem handling multi-modal or sparse data as well as curse of dimensionality. The devised method transfers the data from high-dimensional space into low-dimensional space using a linear transform which is optimized using an information theoretic measure . To find this projection, a novel approach has been adopted in which projected data are transfered into the low-dimensional space first, and a mixture of distributions is estimated using the projected data for each class separately. In the next step, the distance between the estimated distributions is used as a measure of separation for data classes, and a heuristic search is carried on to find the optimal projection. The effectiveness of the proposed algorithm is demonstrated using different datasets in different scenarios in comparison with other well-known algorithms in the literature.},
  archive      = {J_PR},
  author       = {Walid Masoudimansour and Nizar Bouguila},
  doi          = {10.1016/j.patcog.2020.107379},
  journal      = {Pattern Recognition},
  pages        = {107379},
  shortjournal = {Pattern Recognition},
  title        = {Supervised dimensionality reduction of proportional data using mixture estimation},
  volume       = {105},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Gait recognition invariant to carried objects using alpha
blending generative adversarial networks. <em>PR</em>, <em>105</em>,
107376. (<a href="https://doi.org/10.1016/j.patcog.2020.107376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait recognition invariant to carried objects (COs) is very difficult in a real-life scene because the COs can have various shapes and sizes, in addition to unpredictable carrying locations (e.g., front, back, and side, or multiple locations). Therefore, in this paper, we propose a robust method for gait recognition against various COs by reconstructing a gait template without COs. A straightforward approach is to directly generate a gait template without COs given a gait template with COs as the input using a conventional generative adversarial network. There is, however, a potential risk of unnecessarily altering parts that were originally unaffected by COs (e.g., leg parts for a person carrying a backpack). Because we do not want to touch such unaffected parts in the original template, we first estimate a gait template without COs, and then blend it with the original template by an estimated alpha matte that indicates the blending parameters. We then create an alpha-blended template from the original template and the generated template without COs based on the estimated alpha matte. We use two independent generators to estimate the alpha matte and the generated template without COs. Finally, we feed the alpha-blended gait template into a state-of-the-art discrimination network for gait recognition. The experimental results on three publicly available gait databases with real-life COs demonstrate the state-of-the-art performance of the proposed method.},
  archive      = {J_PR},
  author       = {Xiang Li and Yasushi Makihara and Chi Xu and Yasushi Yagi and Mingwu Ren},
  doi          = {10.1016/j.patcog.2020.107376},
  journal      = {Pattern Recognition},
  pages        = {107376},
  shortjournal = {Pattern Recognition},
  title        = {Gait recognition invariant to carried objects using alpha blending generative adversarial networks},
  volume       = {105},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised feature selection with adaptive multiple graph
learning. <em>PR</em>, <em>105</em>, 107375. (<a
href="https://doi.org/10.1016/j.patcog.2020.107375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised feature selection methods try to select features which can well preserve the intrinsic structure of data. To represent such structure, conventional methods construct various graphs from data. In most cases, those different graphs often contain some consensus and complementary information. To make full use of such information, we construct multiple base graphs and learn an adaptive consensus graph from these base graphs for feature selection. In our method, we integrate the multiple graph learning and the feature selection into a unified framework, which can jointly characterize the structure of the data and select the features to preserve such structure. The underlying optimization problem is hard to solve, and we solve it via a block coordinate descent schema, whose convergence is guaranteed. The extensive experiments well demonstrate the effectiveness of our proposed framework.},
  archive      = {J_PR},
  author       = {Peng Zhou and Liang Du and Xuejun Li and Yi-Dong Shen and Yuhua Qian},
  doi          = {10.1016/j.patcog.2020.107375},
  journal      = {Pattern Recognition},
  pages        = {107375},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised feature selection with adaptive multiple graph learning},
  volume       = {105},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning residual refinement network with semantic context
representation for real-time saliency object detection. <em>PR</em>,
<em>105</em>, 107372. (<a
href="https://doi.org/10.1016/j.patcog.2020.107372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Salient object detection (SOD) aims to precisely segment out the most attractive areas in a single image. With the rapid development of deep learning , much effort has been paid to learn an effective representation for SOD from bottom-up or top-down pathways. However, they fail to precisely separate out the whole salient object with fine boundaries due to the repeated subsampling operations such as pooling and striding leading to the loss of fine structures and spatial details . To address these issues, in this paper, we propose a residual refinement network with semantic context features for SOD. First, we design an encoder-decoder structure with side-connections to capture the sharper object boundaries, which can not only gradually recover the spatial details in each feature map from top to down, but also enhance the features at all scales with high-level semantic context information. The semantic context enhanced features are further strengthen by using a set of atrous convolutional filters with multiple atrous rates to encode multi-scale context information. Finally, using the side-output features as input, we develop a recurrent residual module to gradually learn to recover the missing boundary details in the previous coarsely predicted saliency map in a coarse-to-fine manner. Extensive evaluations on six popular SOD benchmark datasets demonstrate leading performance of the proposed approach compared with state-of-the-art methods. Especially, our approach runs in real-time at a speed of 29 fps.},
  archive      = {J_PR},
  author       = {Tengpeng Li and Huihui Song and Kaihua Zhang and Qingshan Liu},
  doi          = {10.1016/j.patcog.2020.107372},
  journal      = {Pattern Recognition},
  pages        = {107372},
  shortjournal = {Pattern Recognition},
  title        = {Learning residual refinement network with semantic context representation for real-time saliency object detection},
  volume       = {105},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Revisiting spectral clustering for near-convex decomposition
of 2D shape. <em>PR</em>, <em>105</em>, 107371. (<a
href="https://doi.org/10.1016/j.patcog.2020.107371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel 2D shape decomposition algorithm via a recursive partitioning process. Starting with the contour points of a shape, we repeatedly separate the points into two parts by spectral clustering , until the stopping condition is met. Motivated by the fact that the points in a convex part are mutually visible, we regard the visibility matrix of points as the affinity matrix of spectral clustering to obtain a near-convex decomposition. Additionally, we present an efficient stopping rule to avoid over-segmentation on the shape branches. The stopping criterion is based on a novel shape signature called visible protrusion strength which can be used to measure the segmentability of a sub-shape. Finally, we demonstrate the efficiency of our algorithm on a variety of publicly available shapes, and provide qualitative and quantitative comparisons with state-of-art approaches.},
  archive      = {J_PR},
  author       = {Zhiyang Li and Jia Hu and Milos Stojmenovic and Zhaobin Liu and Weijiang Liu},
  doi          = {10.1016/j.patcog.2020.107371},
  journal      = {Pattern Recognition},
  pages        = {107371},
  shortjournal = {Pattern Recognition},
  title        = {Revisiting spectral clustering for near-convex decomposition of 2D shape},
  volume       = {105},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep transductive network for generalized zero shot
learning. <em>PR</em>, <em>105</em>, 107370. (<a
href="https://doi.org/10.1016/j.patcog.2020.107370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero Shot Learning (ZSL) aims to learn projective functions on labeled seen data and transfer the learned functions to unseen classes by discovering their relationship with semantic embeddings . However, the mapping process often suffers from the domain shift problem caused by only using the labeled seen data. In this paper, we propose a novel explainable Deep Transductive Network (DTN) for the task of Generalized ZSL (GZSL) by training on both labeled seen data and unlabeled unseen data, with subsequent testing on both seen classes and unseen classes. The proposed network exploits a KL Divergence constraint to iteratively refine the probability of classifying unlabeled instances by learning from their high confidence assignments with the assistance of an auxiliary target distribution. Besides, to avoid the meaningless ascription assumption of unseen data on GZSL, we also propose an experimental paradigm by splitting the unseen data into two equivalent parts for training and testing respectively. Extensive experiments and detailed analysis demonstrate that our DTN can efficiently handle the problems and achieve the state-of-the-art performance on four popular datasets.},
  archive      = {J_PR},
  author       = {Haofeng Zhang and Li Liu and Yang Long and Zheng Zhang and Ling Shao},
  doi          = {10.1016/j.patcog.2020.107370},
  journal      = {Pattern Recognition},
  pages        = {107370},
  shortjournal = {Pattern Recognition},
  title        = {Deep transductive network for generalized zero shot learning},
  volume       = {105},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning direct optimization for scene understanding.
<em>PR</em>, <em>105</em>, 107369. (<a
href="https://doi.org/10.1016/j.patcog.2020.107369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a Learning Direct Optimization (LiDO) method for the refinement of a latent variable model that describes input image x . Our goal is to explain a single image x with an interpretable 3D computer graphics model having scene graph latent variables z (such as object appearance, camera position). Given a current estimate of z we can render a prediction of the image g ( z ), which can be compared to the image x . The standard way to proceed is then to measure the error E ( x, g ( z )) between the two, and use an optimizer to minimize the error. However, it is unknown which error measure E would be most effective for simultaneously addressing issues such as misaligned objects, occlusions, textures, etc. In contrast, the LiDO approach trains a Prediction Network to predict an update directly to correct z , rather than minimizing the error with respect to z . Experiments show that LiDO converges rapidly as it does not need to perform a search on the error landscape, produces better solutions than error-based competitors, and is able to handle the mismatch between the data and the fitted scene model. We apply LiDO to a realistic synthetic dataset , and show that the method also transfers to work well with real images.},
  archive      = {J_PR},
  author       = {Lukasz Romaszko and Christopher K.I. Williams and John Winn},
  doi          = {10.1016/j.patcog.2020.107369},
  journal      = {Pattern Recognition},
  pages        = {107369},
  shortjournal = {Pattern Recognition},
  title        = {Learning direct optimization for scene understanding},
  volume       = {105},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Supervised deep hashing with a joint deep network.
<em>PR</em>, <em>105</em>, 107368. (<a
href="https://doi.org/10.1016/j.patcog.2020.107368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing has gained great attention in large-scale image retrieval due to efficient storage and fast search. Recently, many deep hashing approaches have achieved good results since deep neural network owns powerful learning capability. However, these deep hashing approaches can perform deep features learning and binary-like codes learning synchronously, the information loss between binary-like codes and binary codes will increase due to the binarization operation. A further deficiency is that binary-like codes learning based on deep feature representations is a shallow learning procedure, which cannot fully exploit deep feature representations to generate hash codes. To solve the above problems, we propose a Deep Learning Supervised Hashing (DLSH) method which adopts deep structure to learn binary codes based on deep feature representations for large-scale image retrieval . Specifically, we integrate deep features learning module, deep mapping module and binary codes learning module in one unified architecture. The network is trained in an end-to-end way. In addition, a new objective function is designed to preserve the balancing property and semantic similarity of binary codes by incorporating the semantic similarity term and the balanceable property term. Experimental results on four benchmarks demonstrate that the proposed approach outperforms several state-of-the-art hashing methods.},
  archive      = {J_PR},
  author       = {Yaxiong Chen and Xiaoqiang Lu and Xuelong Li},
  doi          = {10.1016/j.patcog.2020.107368},
  journal      = {Pattern Recognition},
  pages        = {107368},
  shortjournal = {Pattern Recognition},
  title        = {Supervised deep hashing with a joint deep network},
  volume       = {105},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graph-based neural networks for explainable image privacy
inference. <em>PR</em>, <em>105</em>, 107360. (<a
href="https://doi.org/10.1016/j.patcog.2020.107360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of social media and smartphones, people share their daily lives via a large number of images, but the convince also raises a problem of privacy leakage . Therefore, effective methods are needed to infer the privacy risk of images and identify images that may disclose privacy. Several works have tried to solve this problem with deep learning models. However, we know little about how the models infer the privacy label of an image, thus it is not easy to understand why the image may disclose privacy. Inspired by recent research on graph neural networks , we introduce prior knowledge to the deep models to make the inference more explainable. We propose the Graph-based neural networks for Image Privacy (GIP) to infer the privacy risk of images. The GIP mainly focuses on objects in an image, and the knowledge graph is extracted from the objects in the dataset without reliance on extra knowledge. Experimental results show that the GIP achieves higher performance compared with the object-based methods and comparable performance even compared with the multi-modal fusion method. The results show that the introduction of the knowledge graph not only makes the deep model more explainable but also makes better use of the information of objects provided by the images. Combing the knowledge graph with deep learning is a promising way to help protect image privacy that is worth exploring.},
  archive      = {J_PR},
  author       = {Guang Yang and Juan Cao and Zhineng Chen and Junbo Guo and Jintao Li},
  doi          = {10.1016/j.patcog.2020.107360},
  journal      = {Pattern Recognition},
  pages        = {107360},
  shortjournal = {Pattern Recognition},
  title        = {Graph-based neural networks for explainable image privacy inference},
  volume       = {105},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stacked squeeze-and-excitation recurrent residual network
for visual-semantic matching. <em>PR</em>, <em>105</em>, 107359. (<a
href="https://doi.org/10.1016/j.patcog.2020.107359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, visual-textual matching has been widely studied in the intersection of computer vision and natural language processing communities. A feasible scheme for learning discriminative representations is leveraging hierarchical features to align both modalities at multiple semantic levels . However, most existing approaches rely on pre-trained object detectors or semantic parsers to generate multi-level representations, whose performance is overly dependent on the extra supervision and thereby leads to its vulnerability. In this paper, we introduce a Stacked Squeeze-and-Excitation Recurrent Residual Network (SER 2 -Net) for visual-textual matching. Firstly, an efficient multi-level representation module is presented to produce a series of semantically discriminative features without the aid of extra supervision, which is built by stacking the squeeze-and-excitation recurrent residual (SER 2 ) learning components. Specifically, SER 2 incorporates the residual learning and inverse recurrent connection into the squeeze-and-excitation learning block, which allows for utilizing complementary current information and residual information to improve the modality-specific representation ability. Besides, to capture the implicit correlations contained among multi-level features, we propose a novel objective namely Cross-modal Semantic Discrepancy (CMSD) loss, which is characterized by exploiting the interdependency among different semantic levels to narrow the cross-modal distribution discrepancy. Extensive experiments on two benchmark datasets validate the superiority of our model, which compares favorably with the state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Haoran Wang and Zhong Ji and Zhigang Lin and Yanwei Pang and Xuelong Li},
  doi          = {10.1016/j.patcog.2020.107359},
  journal      = {Pattern Recognition},
  pages        = {107359},
  shortjournal = {Pattern Recognition},
  title        = {Stacked squeeze-and-excitation recurrent residual network for visual-semantic matching},
  volume       = {105},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Context from within: Hierarchical context modeling for
semantic segmentation. <em>PR</em>, <em>105</em>, 107358. (<a
href="https://doi.org/10.1016/j.patcog.2020.107358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conditional Random Fields (CRFs) have been widely adopted in conjunction with Fully Convolutional Networks (FCNs) to model and integrate contextual information in the semantic segmentation procedure. In contrast to existing approaches applying CRFs in parallel or in cascade with FCNs, we propose a new paradigm to incorporate CRFs deeper inside the architecture of FCNs to model the context exhibited within the middle layers of an FCN. We approximate the mean-field inference process of a dense CRF as a multi-dimensional Gated Recurrent Unit (GRU) layer, termed CRF-GRU layer, effectively extracting intermediate context within an FCN. More importantly, multiple CRF-GRU layers can be injected into an FCN to model hierarchical contexts presented in multiple middle layers, showing competitive results on the PASCAL VOC 2012 and PASCAL-Context datasets. Secondly, we contribute a new approach to automatically learn, from the training data, the optimal segmentation architecture of the FCN with multiple CRF-GRU layers injected. The proposed approach relies on Genetic Evolution Strategies to allow the existing architecture to iteratively evolve towards higher accuracy instances. The discovered network not only outperforms state-of-the-art segmentation techniques , but also provides exciting new insights into the design of the segmentation networks .},
  archive      = {J_PR},
  author       = {Kien Nguyen and Clinton Fookes and Sridha Sridharan},
  doi          = {10.1016/j.patcog.2020.107358},
  journal      = {Pattern Recognition},
  pages        = {107358},
  shortjournal = {Pattern Recognition},
  title        = {Context from within: Hierarchical context modeling for semantic segmentation},
  volume       = {105},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient adaptive inference for deep convolutional neural
networks using hierarchical early exits. <em>PR</em>, <em>105</em>,
107346. (<a href="https://doi.org/10.1016/j.patcog.2020.107346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early exits are capable of providing deep learning models with adaptive computational graphs that can readily adapt on-the-fly to the available resources. Despite their advantages, existing early exit methods suffer from many limitations which limit their performance, e.g., they ignore the information extracted from previous exit layers, they are unable to efficiently handle feature maps with large sizes, etc. To overcome these limitations we propose a Bag-of-Features (BoF)-based method that is capable of constructing efficient hierarchical early exit layers with minimal computational overhead, while also providing an adaptive inference method that allows for early stopping the inference process when the network is confident enough for its output, leading to significant performance benefits. To this end, the BoF model is extended and adapted to the needs of early exits by constructing additive shared histogram spaces that gradually refine the information extracted from the various layers of a network, in a hierarchical manner, while also employing a classification layer reuse strategy to further reduce the number of parameters needed per exit layer. Note that the proposed method is generic and can be readily combined with any neural network architecture . The effectiveness of the proposed method is demonstrated using five different image datasets, proving that early exits can be readily transformed into a practical tool, which can be effectively used in various real-world embedded applications.},
  archive      = {J_PR},
  author       = {Nikolaos Passalis and Jenni Raitoharju and Anastasios Tefas and Moncef Gabbouj},
  doi          = {10.1016/j.patcog.2020.107346},
  journal      = {Pattern Recognition},
  pages        = {107346},
  shortjournal = {Pattern Recognition},
  title        = {Efficient adaptive inference for deep convolutional neural networks using hierarchical early exits},
  volume       = {105},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simplified unsupervised image translation for semantic
segmentation adaptation. <em>PR</em>, <em>105</em>, 107343. (<a
href="https://doi.org/10.1016/j.patcog.2020.107343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image to image translation achieves superior performance with the advent of generative adversarial networks . In this paper, we propose a Simplified Unsupervised Image Translation (SUIT) model for domain adaptation on semantic segmentation . We adopt adversarial training for superior image generation , and design a novel semantic-content loss to enhance visual appearance preservation. Thus, the high-fidelity generated images with target-style can help the model generalize to the target domain. Besides, the semantic-content loss contains two components, which focus on label- and content-consistency, respectively. Both of them can be derived from existing modules of SUIT, which makes it simple yet suitable for domain adaptation on semantic segmentation tasks. Meanwhile, since the transformation network (generator) is decoupled from the segmentation network , the former can be easily transplanted to other semantic segmentation models. Extensive experimental results demonstrate that these translated images within SUIT can significantly improve performance of the model on the target domain, and our model with FCN8s-VGG16 architecture achieves around 13 percentage points improvement in terms of mIoU on multiple semantic segmentation adaptation benchmarks.},
  archive      = {J_PR},
  author       = {Rui Li and Wenming Cao and Qianfen Jiao and Si Wu and Hau-San Wong},
  doi          = {10.1016/j.patcog.2020.107343},
  journal      = {Pattern Recognition},
  pages        = {107343},
  shortjournal = {Pattern Recognition},
  title        = {Simplified unsupervised image translation for semantic segmentation adaptation},
  volume       = {105},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep multi-person kinship matching and recognition for
family photos. <em>PR</em>, <em>105</em>, 107342. (<a
href="https://doi.org/10.1016/j.patcog.2020.107342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel D eep K inship M atching and R ecognition ( DKMR ) framework for multi-person kinship matching and recognition, which is a complicated and challenging task with little previous literature. Compared with most existing kinship understanding methods that mainly work on matching kinship in pairwise face images, we target at recognizing the exact kinship in nuclear family photos consisting of multiple persons. The proposed DKMR framework contains three modules. Firstly, we design a deep kinship matching model (termed DKM-TRL) to predict kin-or-not scores by integrating the triple ranking loss into a Siamese CNN model. Secondly, we develop a deep kinship recognition model (named DKR-GA) to predict the exact kinship categories, in which gender and relative age attributes are utilized to learn more discriminative representations. Thirdly, based on the outputs of DKM-TRL and DKR-GA, we propose a reasoning conditional random field (R-CRF) model to infer the corresponding optimal family tree by exploiting the common kinship knowledge of a nuclear family. To evaluate the effectiveness of our DKMR framework, we conduct extensive experiments and the results show that it can gain superior performance on Group-Face dataset, TSKinFace dataset and FIW dataset over state-of-the-arts.},
  archive      = {J_PR},
  author       = {Mengyin Wang and Xiangbo Shu and Jiashi Feng and Xun Wang and Jinhui Tang},
  doi          = {10.1016/j.patcog.2020.107342},
  journal      = {Pattern Recognition},
  pages        = {107342},
  shortjournal = {Pattern Recognition},
  title        = {Deep multi-person kinship matching and recognition for family photos},
  volume       = {105},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep quantization generative networks. <em>PR</em>,
<em>105</em>, 107338. (<a
href="https://doi.org/10.1016/j.patcog.2020.107338">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Equipped with powerful convolutional neural networks (CNNs), generative models have achieved tremendous success in various vision applications. However, deep generative networks suffer from high computational and memory costs in both model training and deployment. While many efforts have been devoted to accelerate discriminative models by quantization, effectively reducing the costs for deep generative models is more challenging and remains unexplored. In this work, we investigate applying quantization technology to deep generative models. We find that keeping as much information as possible for quantized activations is key to obtain high-quality generative models. With this in mind, we propose Deep Quantization Generative Networks (DQGNs) to effectively accelerate and compress deep generative networks. By expanding the dimensions of the quantization basis space, DQGNs can achieve lower quantization error and are highly adaptive to complex data distributions. Various experiments on two powerful frameworks ( i.e ., variational auto-encoders, and generative adversarial networks) and two practical applications ( i.e ., style transfer, and super-resolution) demonstrate our findings and the effectiveness of our proposed approach.},
  archive      = {J_PR},
  author       = {Diwen Wan and Fumin Shen and Li Liu and Fan Zhu and Lei Huang and Mengyang Yu and Heng Tao Shen and Ling Shao},
  doi          = {10.1016/j.patcog.2020.107338},
  journal      = {Pattern Recognition},
  pages        = {107338},
  shortjournal = {Pattern Recognition},
  title        = {Deep quantization generative networks},
  volume       = {105},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust one-stage object detection with location-aware
classifiers. <em>PR</em>, <em>105</em>, 107334. (<a
href="https://doi.org/10.1016/j.patcog.2020.107334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent progress on one-stage detectors focuses on improving the quality of bounding boxes , while they pay less attention to the classification head. In this work, we focus on investigating the influence of the classification head. To understand the behavior of the classifier in one-stage detectors, we resort to the methods of the Explainable deep learning area. We visualize its learned representations via activation maps and analyze its robustness to image scene context. Based on the analysis, we observe that the classifier limits the performance of the detector due to its limited receptive field and the lack of object locations. Then, we design a simple but efficient location-aware multi-dilation module (LAMD) to enhance the weak classifier . We conduct extensive experiments on the COCO benchmark to validate the effectiveness of LAMD. The results suggest that our LAMD can achieve consistent improvements and leads to robust detection across various one-stage detectors with different backbones.},
  archive      = {J_PR},
  author       = {Qiang Chen and Peisong Wang and Anda Cheng and Wanguo Wang and Yifan Zhang and Jian Cheng},
  doi          = {10.1016/j.patcog.2020.107334},
  journal      = {Pattern Recognition},
  pages        = {107334},
  shortjournal = {Pattern Recognition},
  title        = {Robust one-stage object detection with location-aware classifiers},
  volume       = {105},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-attention driven adversarial similarity learning
network. <em>PR</em>, <em>105</em>, 107331. (<a
href="https://doi.org/10.1016/j.patcog.2020.107331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Similarity learning is a kind of machine learning algorithm that aims to measure the relevance between given objects. However, conventional similarity learning algorithms usually measure the distance between the entire given objects in the latent feature space. Consequently, the obtained similarity scores only represent how close are the entire given objects, but are incapable of demonstrating which part of them are similar to each other and how semantically similar are they. To address the above problems, in this paper, we propose a self-attention driven adversarial similarity learning network. Discriminative self-attention weights are firstly assigned to different regions of the given objects. The similarity learning step measures the relevance between these self-attention weighted feature maps of given objects under various topic vectors. The topic vectors are conditioned to capture and preserve hidden semantic information within data distribution by a generator-discriminator model with adversarial loss. This model aims to generate objects from topic vectors and propagates the difference between the generated and the real objects back to the similarity learning step, which forces the topic vectors to not only assign discriminative similarity scores to different object pairs but also further mine the hidden semantic information within data distribution. The final similarity scores represent how tight the given objects are connected to the topics. In addition, the regions with higher self-attention weights make more contribution to the discriminative similarity scores. The effectiveness of the proposed method is demonstrated through evaluations based on image retrieval task and document retrieval task and compared against various state-of-the-art algorithms in the field. The visualization results of topic vectors and self-attention weighted feature maps are demonstrated to make our proposed method explainable.},
  archive      = {J_PR},
  author       = {Xinjian Gao and Zhao Zhang and Tingting Mu and Xudong Zhang and Chaoran Cui and Meng Wang},
  doi          = {10.1016/j.patcog.2020.107331},
  journal      = {Pattern Recognition},
  pages        = {107331},
  shortjournal = {Pattern Recognition},
  title        = {Self-attention driven adversarial similarity learning network},
  volume       = {105},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Projection based weight normalization: Efficient method for
optimization on oblique manifold in DNNs. <em>PR</em>, <em>105</em>,
107317. (<a href="https://doi.org/10.1016/j.patcog.2020.107317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimizing deep neural networks (DNNs) often suffers from the ill-conditioned problem. We observe that the scaling based weight space symmetry (SBWSS) in rectified nonlinear network will cause this negative effect. Therefore, we propose to constrain the incoming weights of each neuron to be unit-norm, which is formulated as an optimization problem over the Oblique manifold. A simple yet efficient method referred to as projection based weight normalization (PBWN) is also developed to solve this problem. This proposed method has the property of regularization and collaborates well with the commonly used batch normalization technique. We conduct comprehensive experiments on several widely-used image datasets including CIFAR-10, CIFAR-100, SVHN and ImageNet for supervised learning over the state-of-the-art neural networks . The experimental results show that our method is able to improve the performance of different architectures consistently. We also apply our method to Ladder network for semi-supervised learning on permutation invariant MNIST dataset, and our method achievers the state-of-the-art methods: we obtain test errors as 2.52\%, 1.06\%, and 0.91\% with only 20, 50, and 100 labeled samples, respectively.},
  archive      = {J_PR},
  author       = {Lei Huang and Xianglong Liu and Jie Qin and Fan Zhu and Li Liu and Ling Shao},
  doi          = {10.1016/j.patcog.2020.107317},
  journal      = {Pattern Recognition},
  pages        = {107317},
  shortjournal = {Pattern Recognition},
  title        = {Projection based weight normalization: Efficient method for optimization on oblique manifold in DNNs},
  volume       = {105},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Heterogenous output regression network for direct face
alignment. <em>PR</em>, <em>105</em>, 107311. (<a
href="https://doi.org/10.1016/j.patcog.2020.107311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face alignment has gained great popularity in computer vision due to its wide-spread applications. In this paper, we propose a novel learning architecture, i.e. , heterogenous output regression network (HORNet), for face alignment, which directly predicts facial landmarks from images. HORNet is based on kernel approximations and establishes a new compact multi-layer architecture. A nonlinear layer with cosine activations disentangles nonlinear relationships between representations of images and shapes of facial landmarks. A linear layer with identity activations explicitly encodes landmark correlations by low-rank learning via matrix elastic nets. HORNet is highly flexible and can work either with pre-built feature representations or with convolutional architectures for end-to-end learning. HORNet leverages the strengths of both kernel methods in modeling nonlinearities and of neural networks in structural prediction. This combination renders it effective and efficient for direct face alignment. Extensive experiments on five in-the-wild datasets show that HORNet delivers high performance and consistently exceeds state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Xiantong Zhen and Mengyang Yu and Zehao Xiao and Lei Zhang and Ling Shao},
  doi          = {10.1016/j.patcog.2020.107311},
  journal      = {Pattern Recognition},
  pages        = {107311},
  shortjournal = {Pattern Recognition},
  title        = {Heterogenous output regression network for direct face alignment},
  volume       = {105},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive iterative attack towards explainable adversarial
robustness. <em>PR</em>, <em>105</em>, 107309. (<a
href="https://doi.org/10.1016/j.patcog.2020.107309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image classifiers based on deep neural networks show severe vulnerability when facing adversarial examples crafted on purpose. Designing more effective and efficient adversarial attacks is attracting considerable interest due to its potential contribution to interpretability of deep learning and validation of neural networks’ robustness. However, current iterative attacks use a fixed step size for each noise-adding step, making further investigation into the effect of variable step size on model robustness ripe for exploration. We prove that if the upper bound of noise added to the original image is fixed, the attack effect can be improved if the step size is positively correlated with the gradient obtained at each step by querying the target model. In this paper, we propose Ada-FGSM (Adaptive FGSM), a new iterative attack that adaptively allocates step size of noises according to gradient information at each step. Improvement of success rate and accuracy decrease measured on ImageNet with multiple models emphasizes the validity of our method. We analyze the process of iterative attack by visualizing their trajectory and gradient contour, and further explain the vulnerability of deep neural networks to variable step size adversarial examples .},
  archive      = {J_PR},
  author       = {Yucheng Shi and Yahong Han and Quanxin Zhang and Xiaohui Kuang},
  doi          = {10.1016/j.patcog.2020.107309},
  journal      = {Pattern Recognition},
  pages        = {107309},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive iterative attack towards explainable adversarial robustness},
  volume       = {105},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Binary neural networks: A survey. <em>PR</em>, <em>105</em>,
107281. (<a href="https://doi.org/10.1016/j.patcog.2020.107281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The binary neural network , largely saving the storage and computation, serves as a promising technique for deploying deep models on resource-limited devices. However, the binarization inevitably causes severe information loss, and even worse, its discontinuity brings difficulty to the optimization of the deep network. To address these issues, a variety of algorithms have been proposed, and achieved satisfying progress in recent years. In this paper, we present a comprehensive survey of these algorithms, mainly categorized into the native solutions directly conducting binarization, and the optimized ones using techniques like minimizing the quantization error , improving the network loss function, and reducing the gradient error . We also investigate other practical aspects of binary neural networks such as the hardware-friendly design and the training tricks. Then, we give the evaluation and discussions on different tasks, including image classification , object detection and semantic segmentation . Finally, the challenges that may be faced in future research are prospected.},
  archive      = {J_PR},
  author       = {Haotong Qin and Ruihao Gong and Xianglong Liu and Xiao Bai and Jingkuan Song and Nicu Sebe},
  doi          = {10.1016/j.patcog.2020.107281},
  journal      = {Pattern Recognition},
  pages        = {107281},
  shortjournal = {Pattern Recognition},
  title        = {Binary neural networks: A survey},
  volume       = {105},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards interpretable and robust hand detection via
pixel-wise prediction. <em>PR</em>, <em>105</em>, 107202. (<a
href="https://doi.org/10.1016/j.patcog.2020.107202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The lack of interpretability of existing CNN-based hand detection methods makes it difficult to understand the rationale behind their predictions. In this paper, we propose a novel neural network model , which introduces interpretability into hand detection for the first time. The main improvements include: (1) Detect hands at pixel level to explain what pixels are the basis for its decision and improve transparency of the model. (2) The explainable Highlight Feature Fusion block highlights distinctive features among multiple layers and learns discriminative ones to gain robust performance. (3) We introduce a transparent representation, the rotation map, to learn rotation features instead of complex and non-transparent rotation and derotation layers. (4) Auxiliary supervision accelerates the training process, which saves more than 10 h in our experiments. Experimental results on the VIVA and Oxford hand detection and tracking datasets show competitive accuracy of our method compared with state-of-the-art methods with higher speed. Models and code are available: https://isrc.iscas.ac.cn/gitlab/research/pr2020-phdn .},
  archive      = {J_PR},
  author       = {Dan Liu and Libo Zhang and Tiejian Luo and Lili Tao and Yanjun Wu},
  doi          = {10.1016/j.patcog.2020.107202},
  journal      = {Pattern Recognition},
  pages        = {107202},
  shortjournal = {Pattern Recognition},
  title        = {Towards interpretable and robust hand detection via pixel-wise prediction},
  volume       = {105},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spectral bounding: Strictly satisfying the 1-lipschitz
property for generative adversarial networks. <em>PR</em>, <em>105</em>,
107179. (<a href="https://doi.org/10.1016/j.patcog.2019.107179">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imposing the 1-Lipschitz constraint is a problem of key importance in the training of Generative Adversarial Networks (GANs), which has been proved to productively improve stability of GAN training. Although some interesting alternative methods have been proposed to enforce the 1-Lipschitz property, these existing approaches (e.g., weight clipping, gradient penalty (GP), and spectral normalization (SN)) are only partially successful. In this paper, we propose a novel method, which we refer to as spectral bounding (SB) to strictly enforce the 1-Lipschitz constraint. Our method adopts very cost-effective terms of both 1-norm and ∞-norm, and yet allows us to efficiently approximate the upper bound of spectral norms. In this way, our method provide important insights to the relationship between an alternative of strictly satisfying the Lipschitz property and explainable training stability improvements of GAN. Our proposed method thus significantly enhances the stability of GAN training and the quality of generated images. Extensive experiments are conducted, showing that the proposed method outperforms GP and SN on both CIFAR-10 and ILSVRC2015 (ImagetNet) dataset in terms of the standard inception score.},
  archive      = {J_PR},
  author       = {Zhihong Zhang and Yangbin Zeng and Lu Bai and Yiqun Hu and Meihong Wu and Shuai Wang and Edwin R. Hancock},
  doi          = {10.1016/j.patcog.2019.107179},
  journal      = {Pattern Recognition},
  pages        = {107179},
  shortjournal = {Pattern Recognition},
  title        = {Spectral bounding: Strictly satisfying the 1-lipschitz property for generative adversarial networks},
  volume       = {105},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spectral rotation for deep one-step clustering. <em>PR</em>,
<em>105</em>, 107175. (<a
href="https://doi.org/10.1016/j.patcog.2019.107175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Previous spectral clustering methods sequentially conduct three steps, i.e., similarity matrix learning from original data, spectral representation learning, and K-means clustering on spectral representation, respectively, to difficultly output robust clustering result even though each of three steps achieves individual optimization. The reason is that each goal of former two steps is not focused on achieving optimal clustering result . Moreover, original data usually contains noise to affect the clustering result, as well as has high-dimensional representation to easily result in the curse of dimensionality. In this paper, we propose a deep spectral clustering method which embeds four parts ( i.e., similarity matrix learning, spectral representation learning, optimized K-means clustering, and transformation matrix learning) in a unified framework with the following advantages: 1) similarity matrix is obtained from the low-dimensional feature space of original data where the influence of both noise and high-dimensional data are considered; 2) optimized K-means clustering rotates original result of K-means clustering to search optimized clustering hyperplane which partitions data points into clusters; and 3) each of four parts is iteratively updated so that the clustering result is obtained based on the feedback of other three parts. As a result, our proposed framework develops a two-task deep clustering model with linear activation functions to output effective clustering result. Experimental results on real data sets show the effectiveness of our method in terms of four clustering evaluation metrics , compared to state-of-the-art clustering methods .},
  archive      = {J_PR},
  author       = {Xiaofeng Zhu and Yonghua Zhu and Wei Zheng},
  doi          = {10.1016/j.patcog.2019.107175},
  journal      = {Pattern Recognition},
  pages        = {107175},
  shortjournal = {Pattern Recognition},
  title        = {Spectral rotation for deep one-step clustering},
  volume       = {105},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Gated CNN: Integrating multi-scale feature layers for object
detection. <em>PR</em>, <em>105</em>, 107131. (<a
href="https://doi.org/10.1016/j.patcog.2019.107131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Different convolutional layers in an explainable CNN usually encode different kinds of semantic information for an image, thus the feature fusion approaches like SSD, DSSD, and FPN are widely employed to enhance the detection performance by integrating different results based on multiple convolutional layers . However, the typical fusion approaches first need to independently detect objects based on one convolutional layer before fusion, and this single layer may exist noises or be irrelevant to objects, resulting in detection failure. To tackle the above problem, this paper proposes “Gated CNN” (short for “G-CNN”) to introduce a “gate” structure to integrate multiple convolutional layers for object detection. Injected by multi-scale feature layers, a gate employs several filters to extract useful information and block noises by executing one more convolutional or deconvolutional operation simultaneously, thus a gate-based feature layer is more effective and efficient as compared to the convolutional one. Besides, G-CNN employs a detector with two branches to predict the locations and categories of objects, respectively, as well as an inter-class loss to help detectors learn discrepant information among categories. Therefore, the learned detectors could better differentiate similar objects of different categories. Extensive experiments are conducted on two image datasets (PASCAL VOC and COCO), and the results demonstrate that G-CNN outperforms the state-of-the-art approaches, with a mAP of 40.9\% at 10.6 FPS.},
  archive      = {J_PR},
  author       = {Jin Yuan and Heng-Chang Xiong and Yi Xiao and Weili Guan and Meng Wang and Richang Hong and Zhi-Yong Li},
  doi          = {10.1016/j.patcog.2019.107131},
  journal      = {Pattern Recognition},
  pages        = {107131},
  shortjournal = {Pattern Recognition},
  title        = {Gated CNN: Integrating multi-scale feature layers for object detection},
  volume       = {105},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exponential sparsity preserving projection with applications
to image recognition. <em>PR</em>, <em>104</em>, 107357. (<a
href="https://doi.org/10.1016/j.patcog.2020.107357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparsity preserving projection (SPP), as a widely used linear unsupervised dimensionality reduction (DR) method, is designed to preserve the sparse reconstructive relationship of the raw data. SPP constructs an affinity weight matrix by solving a sparse representation model which does not need any parameters. Moreover, the obtained projection may contain some discriminating information even if no prior knowledge is provided. Although SPP may be more conveniently used in practice due to these advantages, it still suffers from the so-called small-sample-size problem as may other DR methods do. To solve this problem, we propose an exponential sparsity preserving projection (ESPP) by using matrix exponential , and present two efficiently numerical methods for solving the corresponding large-scale matrix exponential eigenvalue problem. ESPP avoids the singularity of the coefficient matrices , and obtains more valuable information for the SPP. Image recognition experiments are conducted on several real-world image databases and the experimental results illustrate the outperformances of ESPP.},
  archive      = {J_PR},
  author       = {Wei Wei and Hua Dai and Wei-tai Liang},
  doi          = {10.1016/j.patcog.2020.107357},
  journal      = {Pattern Recognition},
  pages        = {107357},
  shortjournal = {Pattern Recognition},
  title        = {Exponential sparsity preserving projection with applications to image recognition},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SGM-net: Skeleton-guided multimodal network for action
recognition. <em>PR</em>, <em>104</em>, 107356. (<a
href="https://doi.org/10.1016/j.patcog.2020.107356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-modality human action recognition on RGB or skeleton has been extensively studied. Each of these two modalities has its own advantages as well as limitations, because they depict action from different perspectives. The feature of different modalities can complement each other for describing actions. Therefore, it is meaningful to fuse these two modalities using their complementarity for action recognition. However, existing multimodal methods fail to fully exploit the complementarity of RGB and skeleton modalities. In this paper, we propose a Skeleton-Guided Multimodal Network (SGM-Net) for human action recognition . The proposed method takes full use of the complementarity of these two modalities at semantic feature level. From the technical perspective, we introduce a guided block, the key component of SGM-Net. It enables skeleton feature to guide on RGB feature, so that the important RGB information strongly related to the action is enhanced. Moreover, in the guided block, two schemes of correlation operation are explored. We perform a series of ablation experiments to verify the effectiveness of the guided block. The experimental results show that our approach achieves state-of-the-art performance over the existing methods on NTU and Sub-JHMDB datasets.},
  archive      = {J_PR},
  author       = {Jianan Li and Xuemei Xie and Qingzhe Pan and Yuhan Cao and Zhifu Zhao and Guangming Shi},
  doi          = {10.1016/j.patcog.2020.107356},
  journal      = {Pattern Recognition},
  pages        = {107356},
  shortjournal = {Pattern Recognition},
  title        = {SGM-net: Skeleton-guided multimodal network for action recognition},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Timed-image based deep learning for action recognition in
video sequences. <em>PR</em>, <em>104</em>, 107353. (<a
href="https://doi.org/10.1016/j.patcog.2020.107353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper addresses two issues relative to machine learning on 2D + X data volumes, where 2D refers to image observation and X denotes a variable that can be associated with time, depth, wavelength, etc. The first issue addressed is conditioning these structured volumes for compatibility with respect to convolutional neural networks operating on 2D image file formats . The second issue is associated with sensitive action detection in the “2D + Time” case (video clips and image time series). For the data conditioning issue, the paper first highlights that referring 2D spatial convolution to its 1D Hilbert based instance is highly accurate for information compressibility upon tight frames of convolutional networks . As a consequence of this compressibility, the paper proposes converting the 2D + X data volume into a single meta-image file format, prior to machine learning frameworks. This conversion is such that any 2D frame of the 2D + X data is reshaped as a 1D array indexed by a Hilbert space-filling curve and the third variable X of the initial file format becomes the second variable in the meta-image format. For the sensitive action recognition issue, the paper provides: ( i ) a 3 category video database involving non-violent, moderate and extreme violence actions; ( ii ) the conversion of this database into a timed meta-image database from the 2D + Time to 2D conditioning stage described above and ( iii ) outstanding 2-level and 3-level violence classification results from deep convolutional neural networks operating on meta-image databases.},
  archive      = {J_PR},
  author       = {Abdourrahmane Mahamane Atto and Alexandre Benoit and Patrick Lambert},
  doi          = {10.1016/j.patcog.2020.107353},
  journal      = {Pattern Recognition},
  pages        = {107353},
  shortjournal = {Pattern Recognition},
  title        = {Timed-image based deep learning for action recognition in video sequences},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint graph regularized dictionary learning and sparse
ranking for multi-modal multi-shot person re-identification.
<em>PR</em>, <em>104</em>, 107352. (<a
href="https://doi.org/10.1016/j.patcog.2020.107352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The promising achievement of sparse ranking in image-based recognition gives rise to a number of development on person re-identification (Re-ID) which aims to reconstruct the probe as a linear combination of few atoms/images from an over-complete dictionary/gallery. However, most of the existing sparse ranking based Re-ID methods lack considering the geometric relationships between probe, gallery, and cross-modal images of the same person in multi-shot Re-ID. In this paper, we propose a novel joint graph regularized dictionary learning and sparse ranking method for multi-modal multi-shot person Re-ID. First, we explore the probe-based geometrical structure by enforcing the smoothness between the codings/coefficients, which refers to the multi-shot images from the same person in probe. Second, we explore the gallery-based geometrical structure among gallery images, which encourages the multi-shot images from the same person in the gallery making similar contributions while reconstructing a certain probe image. Third, we explore the cross-modal geometrical structure by enforcing the smoothness between the cross-modal images and thus extend our model for the multi-modal case. Finally, we design an APG based optimization to solve the problem. Comprehensive experiments on benchmark datasets demonstrate the superior performance of the proposed model. The code is available at https://github.com/ttaalle/Lhc .},
  archive      = {J_PR},
  author       = {Aihua Zheng and Hongchao Li and Bo Jiang and Wei-Shi Zheng and Bin Luo},
  doi          = {10.1016/j.patcog.2020.107352},
  journal      = {Pattern Recognition},
  pages        = {107352},
  shortjournal = {Pattern Recognition},
  title        = {Joint graph regularized dictionary learning and sparse ranking for multi-modal multi-shot person re-identification},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Re-ranking image-text matching by adaptive metric fusion.
<em>PR</em>, <em>104</em>, 107351. (<a
href="https://doi.org/10.1016/j.patcog.2020.107351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-text matching has drawn much attention recently with the rapid growth of multi-modal data. Many effective approaches have been proposed to solve this challenging problem, but limited effort has been devoted to re-ranking methods. Compared with the uni-modal re-ranking methods, modality heterogeneity is the major difficulty when designing a re-ranking method in the cross-modal field, which mainly lies in two aspects of different visual and textual feature spaces and different distributions in inverse directions. In this paper, we propose a heuristic re-ranking method called Adaptive Metric Fusion (AMF) for image-text matching. The method can obtain a better metric by adaptively fusing metrics based on two modules: 1) Cross-modal Reciprocal Encoding , which considers ranks in inverse directions to comprehensively evaluate a metric. The sentence retrieval and image retrieval have different distribution characteristics and galleries in different modalities, thus it is necessary to exploit them simultaneously for appropriate metric fusion. 2) Query Replacement Gap , which quantifies the gap between cross-modal and uni-modal similarities to alleviate the influence of different visual and textual feature spaces on the fused metric. The proposed re-ranking method can be implemented in an unsupervised way without requiring any human interaction or annotated data, and can be easily applied to any initial ranking result. Extensive experiments and analysis validate the effectiveness of our method on the large-scale MS-COCO and Flickr30K datasets.},
  archive      = {J_PR},
  author       = {Kai Niu and Yan Huang and Liang Wang},
  doi          = {10.1016/j.patcog.2020.107351},
  journal      = {Pattern Recognition},
  pages        = {107351},
  shortjournal = {Pattern Recognition},
  title        = {Re-ranking image-text matching by adaptive metric fusion},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GAN-based person search via deep complementary classifier
with center-constrained triplet loss. <em>PR</em>, <em>104</em>, 107350.
(<a href="https://doi.org/10.1016/j.patcog.2020.107350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the person search task, which is a computer vision technology that finds the location of a pedestrian and retrieves it in a video taken by a single camera or multiple cameras. This task is much more challenging than the conventional settings for person re-identification or pedestrian detection since the search is susceptible to factors such as different resolutions, similar pedestrians, lighting, viewing angles and occlusion. Moreover, the person search task is a typical big data-small sample problem because each pedestrian only has a few images. It is difficult for the model to learn the discriminant features of pedestrians with a small quantity of pedestrian data. This paper proposes a framework for person search that uses the original training set without collecting extra data by implementing a generative adversarial network (GAN) to generate unlabeled samples . We propose a deep complementary classifier for pedestrian detection to leverage complementary object regions for pedestrian/non-pedestrian classification. In the re-identification section, we propose a center-constrained triplet loss that avoids the complicated triplet selection of the triplet loss and simultaneously pushes away all the distances of rather similar negative centers and the positive center. Experiments show that the GAN-generated data can effectively help to improve the discriminating ability of the CNN model. On the two large-scale datasets, CUHK-SYSU and PRW, we achieve a performance improvement over the baseline CNN . We additionally apply the proposed center-constrained triplet loss and complementary classifiers in the training model, and we achieve mAP improvements over the original method of +1.9\% on CUHK-SYSU and +2.5\% on PRW.},
  archive      = {J_PR},
  author       = {Rui Yao and Cunyuan Gao and Shixiong Xia and Jiaqi Zhao and Yong Zhou and Fuyuan Hu},
  doi          = {10.1016/j.patcog.2020.107350},
  journal      = {Pattern Recognition},
  pages        = {107350},
  shortjournal = {Pattern Recognition},
  title        = {GAN-based person search via deep complementary classifier with center-constrained triplet loss},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Anisotropic tubular minimal path model with fast marching
front freezing scheme. <em>PR</em>, <em>104</em>, 107349. (<a
href="https://doi.org/10.1016/j.patcog.2020.107349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we introduce an anisotropic minimal path model based on a new Riemannian tensor integrating the crossing-adaptive anisotropic radius-lifted tensor field and the front freezing indicator by appearance and path features. The non-local path feature only can be obtained during the geodesic distance computation process by the fast marching method . The predefined criterion derived from path feature is able to steer the front evolution by freezing the point causing high bending of the geodesic to solve the shortcut problem. We performed qualitative and quantitative experiments on synthetic and real images (including retinal vessels, rivers and roads) and compare with the minimal path models with classical anisotropic Riemannian metric and dynamic isotropic metric, which demonstrated the proposed method can detect desired targets from complex tubular tree structures.},
  archive      = {J_PR},
  author       = {Li Liu and Da Chen and Laurent D. Cohen and Jiasong Wu and Michel Paques and Huazhong Shu},
  doi          = {10.1016/j.patcog.2020.107349},
  journal      = {Pattern Recognition},
  pages        = {107349},
  shortjournal = {Pattern Recognition},
  title        = {Anisotropic tubular minimal path model with fast marching front freezing scheme},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semi-supervised network embedding with text information.
<em>PR</em>, <em>104</em>, 107347. (<a
href="https://doi.org/10.1016/j.patcog.2020.107347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network embedding plays a pivotal role in network analysis , due to the capability of encoding each node to a low-dimensional dense feature vector. However, most existing network embedding approaches only focus on preserving structural information in the network. The text features and category attributes of nodes are ignored, which are important to network analysis . In this paper, we propose an innovative semi-supervised network embedding (SNE) model integrating structural information, text features and category attributes into embedding vectors simultaneously. Specifically, we design a structure preserving module and a text representation module to capture the global structural information and the text features separately. Meanwhile, a label indicator matrix and a supervised loss are proposed for preserving category information and mapping nodes in the same class closer. We utilize stacked auto-encoders to explore the highly nonlinear characteristics of the network. By optimizing the reconstruction loss and the designed supervised loss jointly in the proposed semi-supervised model, the embedding vectors are finally learned. Extensive experiments on real-world datasets demonstrate that our method is superior to the state-of-the-art baselines in a variety of tasks, including visualization, node classification and clustering.},
  archive      = {J_PR},
  author       = {Maoguo Gong and Chuanyu Yao and Yu Xie and Mingliang Xu},
  doi          = {10.1016/j.patcog.2020.107347},
  journal      = {Pattern Recognition},
  pages        = {107347},
  shortjournal = {Pattern Recognition},
  title        = {Semi-supervised network embedding with text information},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Point set registration with mixture framework and
variational inference. <em>PR</em>, <em>104</em>, 107345. (<a
href="https://doi.org/10.1016/j.patcog.2020.107345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new point set registration method based on mixture framework and variational inference. A three-phase registration strategy (TRS) is proposed to automatically process point set registration problem in different cases. A Gaussian variational mixture model (GVMM) with isotropic and anisotropic components under the variational inference framework is designed to weaken the effect of outliers. The Dirichlet distribution is applied to govern the mixture proportion of Gaussian components and then distinguishes missing points. We test the performance of our method in contour registration, Graffiti images, retinal images, remote sensing images and 3D human motion , and compare with six state-of-the-art methods. Our method shows favorable performances in most scenarios.},
  archive      = {J_PR},
  author       = {Xinke Ma and Shijin Xu and Jie Zhou and Qinglu Yang and Yang Yang and Kun Yang and Sim Heng Ong},
  doi          = {10.1016/j.patcog.2020.107345},
  journal      = {Pattern Recognition},
  pages        = {107345},
  shortjournal = {Pattern Recognition},
  title        = {Point set registration with mixture framework and variational inference},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-label feature selection with shared common mode.
<em>PR</em>, <em>104</em>, 107344. (<a
href="https://doi.org/10.1016/j.patcog.2020.107344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-label feature selection plays an indispensable role in multi-label learning, which eliminates irrelevant and redundant features while retaining relevant features. Most of existing multi-label feature selection methods employ two strategies to construct feature selection models: extracting label correlations to guide feature selection process and maintaining the consistency between the feature matrix and the reduced low-dimensional feature matrix. However, the data information is described by two data matrices: the feature matrix and the label matrix. Previous methods devote attention to either of the two data matrices. To address this issue, we propose a novel feature selection method named Feature Selection considering Shared Common Mode between features and labels (SCMFS). First, we utilize Coupled Matrix Factorization (CMF) to extract the shared common mode between the feature matrix and the label matrix, considering the comprehensive data information in the two matrices. Additionally, Non-negative Matrix Factorization (NMF) is adopted to enhance the interpretability for feature selection. Extensive experiments are implemented on fifteen real-world benchmark data sets for multiple evaluation metrics , the experimental results demonstrate the classification superiority of the proposed method.},
  archive      = {J_PR},
  author       = {Liang Hu and Yonghao Li and Wanfu Gao and Ping Zhang and Juncheng Hu},
  doi          = {10.1016/j.patcog.2020.107344},
  journal      = {Pattern Recognition},
  pages        = {107344},
  shortjournal = {Pattern Recognition},
  title        = {Multi-label feature selection with shared common mode},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Recognizing actions in images by fusing multiple body
structure cues. <em>PR</em>, <em>104</em>, 107341. (<a
href="https://doi.org/10.1016/j.patcog.2020.107341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although Convolutional Neural Networks (CNNs) have made substantial improvements in many computer vision tasks, there remains room for improvements in image-based action recognition due to the limited capability to exploit the body structure information.In this work, we propose a unified deep model to explicitly explore body structure information and fuse multiple body structure cues for robust action recognition in images.In order to fully explore the body structure information, we design the Body Structure Exploration sub-network.It generates two novel body structure cues, Structural Body Parts and Limb Angle Descriptor, which capture structure information of human bodies from the global and local perspectives respectively. And then, we design the Action Classification sub-network to fuse the predictions from multiple body structure cues to obtain precise results. Moreover, we integrate the two sub-networks into a unified model by sharing the bottom convolutional layers , which improves the computational efficiency in both training and testing stages. We comprehensively evaluate our network on the challenging image-based human action datasets, Pascal VOC 2012 Action and Stanford40. Our approach achieves 93.5\% and 93.8\% mAP respectively, which outperforms all recent approaches in this field.},
  archive      = {J_PR},
  author       = {Yang Li and Kan Li and Xinxin Wang},
  doi          = {10.1016/j.patcog.2020.107341},
  journal      = {Pattern Recognition},
  pages        = {107341},
  shortjournal = {Pattern Recognition},
  title        = {Recognizing actions in images by fusing multiple body structure cues},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Overview of deep-learning based methods for salient object
detection in videos. <em>PR</em>, <em>104</em>, 107340. (<a
href="https://doi.org/10.1016/j.patcog.2020.107340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video salient object detection is a challenging and important problem in computer vision domain. In recent years, deep-learning based methods have contributed to significant improvements in this domain. This paper provides an overview of recent developments in this domain and compares the corresponding methods up to date, including 1) Classification of the state-of-the-art methods and their frameworks; 2) summary of the benchmark datasets and commonly used evaluation metrics ; 3) experimental comparison of the performances of the state-of-the-art methods; 4) suggestions of some promising future works for unsolved challenges.},
  archive      = {J_PR},
  author       = {Qiong Wang and Lu Zhang and Yan Li and Kidiyo Kpalma},
  doi          = {10.1016/j.patcog.2020.107340},
  journal      = {Pattern Recognition},
  pages        = {107340},
  shortjournal = {Pattern Recognition},
  title        = {Overview of deep-learning based methods for salient object detection in videos},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust visual tracking by embedding combination and
weighted-gradient optimization. <em>PR</em>, <em>104</em>, 107339. (<a
href="https://doi.org/10.1016/j.patcog.2020.107339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing tracking-by-detection approaches build trackers on binary classifiers . Despite achieving state-of-the-art performance on tracking benchmarks, these trackers pay limited attention to data imbalance issue, e.g , positive and negative, easy and hard. In this paper, we demonstrate that separately learning feature embeddings corresponding to negative samples with different semantic characteristics is effective in reducing the background diversity to handle the imbalance between positive and negative samples, which facilitates background awareness of classifiers. Specifically, we propose a negative sample embedding combination network, which helps to learn several sub-embeddings and combine them to build a robust classifier. In addition, we propose a weighted-gradient loss to handle the imbalance between easy and hard samples. The gradient contribution of each sample to model training is dynamically weighted according to the gradient distribution, which prevents easy samples from overwhelming model training. Extensive experiments on benchmarks demonstrate that our tracker performs favorably against state-of-the-art algorithms.},
  archive      = {J_PR},
  author       = {Jin Feng and Peng Xu and Shi Pu and Kaili Zhao and Honggang Zhang},
  doi          = {10.1016/j.patcog.2020.107339},
  journal      = {Pattern Recognition},
  pages        = {107339},
  shortjournal = {Pattern Recognition},
  title        = {Robust visual tracking by embedding combination and weighted-gradient optimization},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Explaining away results in accurate and tolerant template
matching. <em>PR</em>, <em>104</em>, 107337. (<a
href="https://doi.org/10.1016/j.patcog.2020.107337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recognising and locating image patches or sets of image features is an important task underlying much work in computer vision . Traditionally this has been accomplished using template matching . However, template matching is notoriously brittle in the face of changes in appearance caused by, for example, variations in viewpoint, partial occlusion , and non-rigid deformations. This article tests a method of template matching that is more tolerant to such changes in appearance and that can, therefore, more accurately identify image patches. In traditional template matching the comparison between a template and the image is independent of the other templates. In contrast, the method advocated here takes into account the evidence provided by the image for the template at each location and the full range of alternative explanations represented by the same template at other locations and by other templates. Specifically, the proposed method of template matching is performed using a form of probabilistic inference known as “explaining away”. The algorithm used to implement explaining away has previously been used to simulate several neurobiological mechanisms, and been applied to image contour detection and pattern recognition tasks. Here it is applied for the first time to image patch matching, and is shown to produce superior results in comparison to the current state-of-the-art methods.},
  archive      = {J_PR},
  author       = {M.W. Spratling},
  doi          = {10.1016/j.patcog.2020.107337},
  journal      = {Pattern Recognition},
  pages        = {107337},
  shortjournal = {Pattern Recognition},
  title        = {Explaining away results in accurate and tolerant template matching},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modality-specific and shared generative adversarial network
for cross-modal retrieval. <em>PR</em>, <em>104</em>, 107335. (<a
href="https://doi.org/10.1016/j.patcog.2020.107335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal retrieval aims to realize accurate and flexible retrieval across different modalities of data, e.g., image and text, which has achieved significant progress in recent years, especially since generative adversarial networks (GAN) were used. However, there still exists much room for improvement. How to jointly extract and utilize both the modality-specific (complementarity) and modality-shared (correlation) features effectively has not been well studied. In this paper, we propose an approach named Modality-Specific and Shared Generative Adversarial Network (MS 2 GAN) for cross-modal retrieval. The network architecture consists of two sub-networks that aim to learn modality-specific features for each modality, followed by a common sub-network that aims to learn the modality-shared features for each modality. Network training is guided by the adversarial scheme between the generative and discriminative models. The generative model learns to predict the semantic labels of features, model the inter- and intra-modal similarity with label information, and ensure the difference between the modality-specific and modality-shared features, while the discriminative model learns to classify the modality of features. The learned modality-specific and shared feature representations are jointly used for retrieval. Experiments on three widely used benchmark multi-modal datasets demonstrate that MS 2 GAN can outperform state-of-the-art related works.},
  archive      = {J_PR},
  author       = {Fei Wu and Xiao-Yuan Jing and Zhiyong Wu and Yimu Ji and Xiwei Dong and Xiaokai Luo and Qinghua Huang and Ruchuan Wang},
  doi          = {10.1016/j.patcog.2020.107335},
  journal      = {Pattern Recognition},
  pages        = {107335},
  shortjournal = {Pattern Recognition},
  title        = {Modality-specific and shared generative adversarial network for cross-modal retrieval},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Remote sensing image segmentation using geodesic-kernel
functions and multi-feature spaces. <em>PR</em>, <em>104</em>, 107333.
(<a href="https://doi.org/10.1016/j.patcog.2020.107333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image representation is the key factor influencing the accuracy of remote sensing image segmentation . Traditional algorithms rely on the pixel-wise characteristics exhibited in the feature space. They introduce spatial information by establishing the connections between neighboring pixels in the neighborhood system. But the spectral-spatial features cannot be well expressed. In this paper, a Riemannian manifold space is introduced to express the contextual information by jointly mapping the spectral features of a pixel and its neighboring ones on to it. To benefit from the expression ability and geometric properties of the Riemannian manifold, a data submanifold and a parameter submanifold are established to depict the characteristics of the detected image and all possible segmentation results. On the parameter submanifold, only points representing objects of the current segmentation are active. Then distance between a point on the data submanifold and an active point on the parameter submanifold is measured by a geodesic-kernel function. Consequently, four geodesic-kernel function-based manifold projection criteria are proposed to explore the complementation between features expressed in different feature spaces. Experiments on synthetic and real remote sensing images demonstrated that the proposed geodesic-kernel function-based manifold projection algorithm outperforms traditional ones and features expressed in different feature spaces did contain some complementary information.},
  archive      = {J_PR},
  author       = {Xuemei Zhao and Haijian Wang and Jun Wu and Yu Li and Shijie Zhao},
  doi          = {10.1016/j.patcog.2020.107333},
  journal      = {Pattern Recognition},
  pages        = {107333},
  shortjournal = {Pattern Recognition},
  title        = {Remote sensing image segmentation using geodesic-kernel functions and multi-feature spaces},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Topic modelling for routine discovery from egocentric
photo-streams. <em>PR</em>, <em>104</em>, 107330. (<a
href="https://doi.org/10.1016/j.patcog.2020.107330">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Developing tools to understand and visualize lifestyle is of high interest when addressing the improvement of habits and well-being of people. Routine, defined as the usual things that a person does daily, helps describe the individuals’ lifestyle. With this paper, we are the first ones to address the development of novel tools for automatic discovery of routine days of an individual from his/her egocentric images. In the proposed model, sequences of images are firstly characterized by semantic labels detected by pre-trained CNNs. Then, these features are organized in temporal-semantic documents to later be embedded into a topic models space. Finally, Dynamic-Time-Warping and Spectral-Clustering methods are used for final day routine/non-routine discrimination. Moreover, we introduce a new EgoRoutine -dataset, a collection of 104 egocentric days with more than 100.000 images recorded by 7 users. Results show that routine can be discovered and behavioural patterns can be observed.},
  archive      = {J_PR},
  author       = {Estefania Talavera and Carolin Wuerich and Nicolai Petkov and Petia Radeva},
  doi          = {10.1016/j.patcog.2020.107330},
  journal      = {Pattern Recognition},
  pages        = {107330},
  shortjournal = {Pattern Recognition},
  title        = {Topic modelling for routine discovery from egocentric photo-streams},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CoCNN: RGB-d deep fusion for stereoscopic salient object
detection. <em>PR</em>, <em>104</em>, 107329. (<a
href="https://doi.org/10.1016/j.patcog.2020.107329">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many convolutional neural network (CNN)-based approaches for stereoscopic salient object detection involve fusing either low-level or high-level features from the color and disparity channels. The former method generally produces incomplete objects, whereas the latter tends to blur object boundaries. In this paper, a coupled CNN (CoCNN) is proposed to fuse color and disparity features from low to high layers in a unified deep model. It consists of three parts: two parallel multilinear span networks, a cascaded span network and a conditional random field module. We first apply the multilinear span network to compute multiscale saliency predictions based on RGB and disparity individually. Each prediction, learned under separate supervision, utilizes the multilevel features extracted by the multilinear span network. Second, a proposed cascaded span network, under deep supervision, is designed as a coupling unit to fuse the two feature streams at each scale and integrate all fused features in a supervised manner to construct a saliency map . Finally, we formulate a constraint in the form of a conditional random field model to refine the saliency map based on the a priori assumption that objects with similar saliency values have similar colors and disparities. Experiments conducted on two commonly used datasets demonstrate that the proposed method outperforms previous state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Fangfang Liang and Lijuan Duan and Wei Ma and Yuanhua Qiao and Zhi Cai and Jun Miao and Qixiang Ye},
  doi          = {10.1016/j.patcog.2020.107329},
  journal      = {Pattern Recognition},
  pages        = {107329},
  shortjournal = {Pattern Recognition},
  title        = {CoCNN: RGB-D deep fusion for stereoscopic salient object detection},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Clustering quality metrics for subspace clustering.
<em>PR</em>, <em>104</em>, 107328. (<a
href="https://doi.org/10.1016/j.patcog.2020.107328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of clustering validation, i.e., clustering evaluation without knowledge of ground-truth labels, for the increasingly-popular framework known as subspace clustering. Existing clustering quality metrics (CQMs) rely heavily on a notion of distance between points, but common metrics fail to capture the geometry of subspace clustering. We propose a novel point-to-point pseudometric for points lying on a union of subspaces and show how this allows for the application of existing CQMs to the subspace clustering problem . We provide theoretical and empirical justification for the proposed point-to-point distance, and then demonstrate on a number of common benchmark datasets that our proposed methods generally outperform existing graph-based CQMs in terms of choosing the best clustering and the number of clusters.},
  archive      = {J_PR},
  author       = {John Lipor and Laura Balzano},
  doi          = {10.1016/j.patcog.2020.107328},
  journal      = {Pattern Recognition},
  pages        = {107328},
  shortjournal = {Pattern Recognition},
  title        = {Clustering quality metrics for subspace clustering},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Key protected classification for collaborative learning.
<em>PR</em>, <em>104</em>, 107327. (<a
href="https://doi.org/10.1016/j.patcog.2020.107327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale datasets play a fundamental role in training deep learning models. However, dataset collection is difficult in domains that involve sensitive information . Collaborative learning techniques provide a privacy-preserving solution, by enabling training over a number of private datasets that are not shared by their owners. However, recently, it has been shown that the existing collaborative learning frameworks are vulnerable to an active adversary that runs a generative adversarial network (GAN) attack. In this work, we propose a novel classification model that is resilient against such attacks by design. More specifically, we introduce a key-based classification model and a principled training scheme that protects class scores by using class-specific private keys, which effectively hide the information necessary for a GAN attack. We additionally show how to utilize high dimensional keys to improve the robustness against attacks without increasing the model complexity. Our detailed experiments demonstrate the effectiveness of the proposed technique. Source code will be made available at https://github.com/mbsariyildiz/key-protected-classification .},
  archive      = {J_PR},
  author       = {Mert Bulent Sariyildiz and Ramazan Gokberk Cinbis and Erman Ayday},
  doi          = {10.1016/j.patcog.2020.107327},
  journal      = {Pattern Recognition},
  pages        = {107327},
  shortjournal = {Pattern Recognition},
  title        = {Key protected classification for collaborative learning},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-focus image fusion based on non-negative sparse
representation and patch-level consistency rectification. <em>PR</em>,
<em>104</em>, 107325. (<a
href="https://doi.org/10.1016/j.patcog.2020.107325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most existing sparse representation-based (SR) fusion methods consider the local information of each image patch independently during fusion. Some spatial artifacts are easily introduced to the fused image. A sliding window technology is often employed by these methods to overcome this issue. However, this comes at the cost of high computational complexity. Alternatively, we come up with a novel multi-focus image fusion method that takes full consideration of the strong correlations among spatially adjacent image patches with NO need for a sliding window. To this end, a non-negative SR model with local consistency constraint (CNNSR) on the representation coefficients is first constructed to encode each image patch. Then a patch-level consistency rectification strategy is presented to merge the input image patches, by which the spatial artifacts in the fused images are greatly reduced. As well, a compact non-negative dictionary is constructed for the CNNSR model. Experimental results demonstrate that the proposed fusion method outperforms some state-of-the art methods. Moreover, the proposed method is computationally efficient, thereby facilitating real-world applications.},
  archive      = {J_PR},
  author       = {Qiang Zhang and Guanghe Li and Yunfeng Cao and Jungong Han},
  doi          = {10.1016/j.patcog.2020.107325},
  journal      = {Pattern Recognition},
  pages        = {107325},
  shortjournal = {Pattern Recognition},
  title        = {Multi-focus image fusion based on non-negative sparse representation and patch-level consistency rectification},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). End-to-end training of CNN ensembles for person
re-identification. <em>PR</em>, <em>104</em>, 107319. (<a
href="https://doi.org/10.1016/j.patcog.2020.107319">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an end-to-end ensemble method for person re-identification (ReID) to address the problem of overfitting in discriminative models. These models are known to converge easily, but they are biased to the training data in general and may produce a high model variance, which is known as overfitting. The ReID task is more prone to this problem due to the large discrepancy between training and test distributions. To address this problem, our proposed ensemble learning framework produces several diverse and accurate base learners in a single DenseNet. Since most of the costly dense blocks are shared, our method is computationally efficient, which makes it favorable compared to the conventional ensemble models. Experiments on several benchmark datasets demonstrate that our method achieves state-of-the-art results. Noticeable performance improvements, especially on relatively small datasets, indicate that the proposed method deals with the overfitting problem effectively.},
  archive      = {J_PR},
  author       = {Ayse Serbetci and Yusuf Sinan Akgul},
  doi          = {10.1016/j.patcog.2020.107319},
  journal      = {Pattern Recognition},
  pages        = {107319},
  shortjournal = {Pattern Recognition},
  title        = {End-to-end training of CNN ensembles for person re-identification},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Biclustering with dominant sets. <em>PR</em>, <em>104</em>,
107318. (<a href="https://doi.org/10.1016/j.patcog.2020.107318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biclustering can be defined as the simultaneous clustering of rows and columns in a data matrix and it has been recently applied to many scientific scenarios such as bioinformatics, text analysis and computer vision to name a few. In this paper we propose a novel biclustering approach, that is based on the concept of dominant-set clustering and extends such algorithm to the biclustering problem. In more detail, we propose a novel encoding of the biclustering problem as a graph so to use the dominant set concept to analyse rows and columns simultaneously. Moreover, we extend the Dominant Set Biclustering approach to facilitate the insertion of prior knowledge that may be available on the domain. We evaluated the proposed approach on a synthetic benchmark and on two computer vision tasks: multiple structure recovery and region-based correspondence . The empirical evaluation shows that the method achieves promising results that are comparable to the state-of-the-art and that outperforms competitors in various cases.},
  archive      = {J_PR},
  author       = {M. Denitto and M. Bicego and A. Farinelli and S. Vascon and M. Pelillo},
  doi          = {10.1016/j.patcog.2020.107318},
  journal      = {Pattern Recognition},
  pages        = {107318},
  shortjournal = {Pattern Recognition},
  title        = {Biclustering with dominant sets},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Video semantic segmentation via feature propagation with
holistic attention. <em>PR</em>, <em>104</em>, 107268. (<a
href="https://doi.org/10.1016/j.patcog.2020.107268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the frames of a video are inherently contiguous, information redundancy is ubiquitous. Unlike previous works densely process each frame of a video, in this paper we present a novel method to focus on efficient feature propagation across frames to tackle the challenging video semantic segmentation task. Firstly, we propose a Light, Efficient and Real-time network (denoted as LERNet) as a strong backbone network for per-frame processing. Then we mine rich features within a key frame and propagate the across-frame consistency information by calculating a temporal holistic attention with the following non-key frame. Each element of the attention matrix represents the global correlation between pixels of a non-key frame and the previous key frame. Concretely, we propose a brand-new attention module to capture the spatial consistency on low-level features along temporal dimension. Then we employ the attention weights as a spatial transition guidance for directly generating high-level features of the current non-key frame from the weighted corresponding key frame. Finally, we efficiently fuse the hierarchical features of the non-key frame and obtain the final segmentation result. Extensive experiments on two popular datasets, i.e. the CityScapes and the CamVid, demonstrate that the proposed approach achieves a remarkable balance between inference speed and accuracy.},
  archive      = {J_PR},
  author       = {Junrong Wu and Zongzheng Wen and Sanyuan Zhao and Kele Huang},
  doi          = {10.1016/j.patcog.2020.107268},
  journal      = {Pattern Recognition},
  pages        = {107268},
  shortjournal = {Pattern Recognition},
  title        = {Video semantic segmentation via feature propagation with holistic attention},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visual domain adaptation based on modified a−distance and
sparse filtering. <em>PR</em>, <em>104</em>, 107254. (<a
href="https://doi.org/10.1016/j.patcog.2020.107254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation studies how to build a robust model to solve pattern recognition problems when training in a source domain while testing in a related but different target domain. The existing methods focus on how to shorten the distance between the two domains, however, they have limited considerations on the preservation of data structures . In this paper, we propose a novel model for unsupervised domain adaptation . For the reduction of domain discrepancy, we propose modified A − A− distance, which is computationally fast and can be optimized using gradient information . Moreover, in order to capture the internal structures of target samples, within-domain normalization based sparse filtering is raised, which proved to be more powerful for domain adaptation. Extensive experiments compared to both shallow and deep methods demonstrate the effectiveness of our approach.},
  archive      = {J_PR},
  author       = {Chao Han and Yu Lei and Yu Xie and Deyun Zhou and Maoguo Gong},
  doi          = {10.1016/j.patcog.2020.107254},
  journal      = {Pattern Recognition},
  pages        = {107254},
  shortjournal = {Pattern Recognition},
  title        = {Visual domain adaptation based on modified a−distance and sparse filtering},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep imitator: Handwriting calligraphy imitation via deep
attention networks. <em>PR</em>, <em>104</em>, 107080. (<a
href="https://doi.org/10.1016/j.patcog.2019.107080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Calligraphy imitation (CI) from a handful of target handwriting samples is such a challenging task that most of the existing writing style analysis or handwriting generation methods do not exhibit satisfactory performance. In this paper, we propose a novel multi-module framework to address the problem of CI. Firstly, we utilized a deep convolution neural network (CNN) to extract personalized calligraphical features. Then we built a calligraphy-clustering attention module and a mata-style matrix (msM) to compute an embedding of calligraphy. The structure of conditional gated recurrent unit (cGRU) is then improved to predict the probabilistic density of pen tip movement displacement by dual condition inputs. Finally, we generated personalized handwriting stroke sequences through iterative sampling with Gaussian mixture model (GMM). Experiments on public online handwriting databases verify that the proposed method could achieve satisfactory performance; the generated samples achieved high similarities with original handwriting examples.},
  archive      = {J_PR},
  author       = {Bocheng Zhao and Jianhua Tao and Minghao Yang and Zhengkun Tian and Cunhang Fan and Ye Bai},
  doi          = {10.1016/j.patcog.2019.107080},
  journal      = {Pattern Recognition},
  pages        = {107080},
  shortjournal = {Pattern Recognition},
  title        = {Deep imitator: Handwriting calligraphy imitation via deep attention networks},
  volume       = {104},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A parallel fuzzy rule-base based decision tree in the
framework of map-reduce. <em>PR</em>, <em>103</em>, 107326. (<a
href="https://doi.org/10.1016/j.patcog.2020.107326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision trees are commonly used for learning and extracting classification rules from data. The fuzzy rule based decision tree (FRDT) is very representative owing to its better robustness and generalization. However, FRDT cannot work well on the analysis of large-scale data sets. One solution for this problem is parallel computing . A proved effective parallel computing model is Map-Reduce. Ensemble learning is an effective strategy which can significantly improve the generalization ability of machine learning systems . The objective of this paper is to develop a fuzzy rule-base based decision tree on the strategies of parallel computing and ensemble learning. First, we implement a parallel fusing fuzzy rule based classification system via Map-Reduce (MR-FFRCS) to display how to extract fuzzy rules from data in parallel and how to evaluate the fuzzy rules in an ensemble learning way. Then, taking MR-FFRCS as a fundamental module, we propose a parallel fuzzy rule-base based decision tree (MR-FRBDT) to improve the original FRDT algorithm . The experimental studies mainly focus on feasibility and parallelism . Compared with FRDT on 23 UCI benchmark data sets, the proposed MR-FRBDT algorithm with fewer parameters is effective and has the ability to handle large-scale data sets. Furthermore, some numerical experiments conducted on several large-scale data sets verify the parallel performance on reducing computing time and avoiding memory restrictions.},
  archive      = {J_PR},
  author       = {Yashuang Mu and Xiaodong Liu and Lidong Wang and Juxiang Zhou},
  doi          = {10.1016/j.patcog.2020.107326},
  journal      = {Pattern Recognition},
  pages        = {107326},
  shortjournal = {Pattern Recognition},
  title        = {A parallel fuzzy rule-base based decision tree in the framework of map-reduce},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New fractional-order legendre-fourier moments for pattern
recognition applications. <em>PR</em>, <em>103</em>, 107324. (<a
href="https://doi.org/10.1016/j.patcog.2020.107324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Orthogonal moments enable computer-based systems to discriminate between similar objects. Mathematicians proved that the orthogonal polynomials of fractional-orders outperformed their corresponding counterparts in representing the fine details of a given function. In this work, novel orthogonal fractional-order Legendre-Fourier moments are proposed for pattern recognition applications. The basis functions of these moments are defined and the essential mathematical equations for the recurrence relations, orthogonality and the similarity transformations (rotation and scaling) are derived. The proposed new fractional-order moments are tested where their performance is compared with the existing orthogonal quaternion, multi-channel and fractional moments. New descriptors were found to be superior to the existing ones in terms of accuracy, stability, noise resistance, invariance to similarity transformations, recognition rates and computational times.},
  archive      = {J_PR},
  author       = {Khalid M Hosny and Mohamed M Darwish and Tarek Aboelenen},
  doi          = {10.1016/j.patcog.2020.107324},
  journal      = {Pattern Recognition},
  pages        = {107324},
  shortjournal = {Pattern Recognition},
  title        = {New fractional-order legendre-fourier moments for pattern recognition applications},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bit-string representation of a fingerprint image by
normalized local structures. <em>PR</em>, <em>103</em>, 107323. (<a
href="https://doi.org/10.1016/j.patcog.2020.107323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional minutia-based fingerprint recognition requires a complicated geometric matching and hard to be adopted in the bit-string based cancellable biometrics or bio-encryption, as the minutia data representing a fingerprint image is geometrical, unordered and variable in size. In this paper, we propose a new method to represent a fingerprint image by an ordered and fixed-length bit-string to cope with those difficulties with providing a faster matching, compressibility and improved accuracy performance as well. Firstly, we devised a novel minutia-based local structure modeled by a mixture of 2D elliptical Gaussian functions to represent a minutia in the image pixel space. Then, each local structure was mapped to a point in a Euclidean space by normalizing the local structure by the number of minutiae in it. This simple yet crucial computation for converting the image space to the Euclidean-space enabled the fast dissimilarity computation of two local structures and all followed processes in our proposed method. A complementary texture-based local structure to the minutia-based local structure was also introduced, whereby both were compressed via principal component analysis and fused in the compressed Euclidean space. The fused local structures were then converted to a K-bit ordered string using the K-means clustering algorithm. This chain of computations with the sole use of Euclidean distance was vital for speedy and discriminative bit-string conversion. The accuracy was further improved by the finger-specific bit-training algorithm, in which two criteria were leveraged to select the useful bit positions for matching. Experiments were performed on Fingerprint Verification Competition (FVC) databases for comparisons with the existing techniques to show the superiority of the proposed method.},
  archive      = {J_PR},
  author       = {Jun Beom Kho and Andrew B.J. Teoh and Wonjune Lee and Jaihie Kim},
  doi          = {10.1016/j.patcog.2020.107323},
  journal      = {Pattern Recognition},
  pages        = {107323},
  shortjournal = {Pattern Recognition},
  title        = {Bit-string representation of a fingerprint image by normalized local structures},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A concave optimization algorithm for matching partially
overlapping point sets. <em>PR</em>, <em>103</em>, 107322. (<a
href="https://doi.org/10.1016/j.patcog.2020.107322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matching partially overlapping point sets is a challenging problem in computer vision . To achieve this goal, we model point matching as a mixed linear assignment - least square problem . By eliminating the transformation variable, we reduce the minimization problem to a concave optimization problem with the property that the objective function can be converted into a form with few nonlinear terms . We then use a heuristic variant of the branch-and-bound algorithm for optimization where convergence of the upper bound is used as the stopping criterion. We also propose a new lower bounding scheme which involves solving a k-cardinality linear assignment problem. Two cases of transformations, transformation output being linear with respect to parameters and 2D/3D similarity transformations, are discussed, resulting in ability to handle unknown arbitrary translation and similarity, respectively. Experimental results demonstrate better robustness of the algorithm over state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Wei Lian and Lei Zhang},
  doi          = {10.1016/j.patcog.2020.107322},
  journal      = {Pattern Recognition},
  pages        = {107322},
  shortjournal = {Pattern Recognition},
  title        = {A concave optimization algorithm for matching partially overlapping point sets},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graph convolutional network with structure pooling and
joint-wise channel attention for action recognition. <em>PR</em>,
<em>103</em>, 107321. (<a
href="https://doi.org/10.1016/j.patcog.2020.107321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, graph convolutional networks (GCNs) have achieved state-of-the-art results for skeleton based action recognition by expanding convolutional neural networks (CNNs) to graphs. However, due to the lack of effective feature aggregation method, e.g. max pooling in CNN, existing GCN-based methods only learn local information among adjacent joints and are hard to obtain high-level interaction features , such as interactions between five parts of human body. Moreover, subtle differences of confusing actions often hide in specific channels of key joints’ features, this kind of discriminative information is rarely exploited in previous methods. In this paper, we propose a novel graph convolutional network with structure based graph pooling (SGP) scheme and joint-wise channel attention (JCA) modules. The SGP scheme pools the human skeleton graph according to the prior knowledge of human body’s typology. This pooling scheme not only leads to more global representations but also reduces the amount of parameters and computation cost. The JCA module learns to selectively focus on discriminative joints of skeleton and pays different levels of attention to different channels. This novel attention mechanism enhance the model’s ability to classify confusing actions. We evaluate our SGP scheme and JCA module on three most challenging skeleton based action recognition datasets: NTU-RGB+D, Kinetics-M, and SYSU-3D. Our method outperforms the state-of-art methods on three benchmarks.},
  archive      = {J_PR},
  author       = {Yuxin Chen and Gaoqun Ma and Chunfeng Yuan and Bing Li and Hui Zhang and Fangshi Wang and Weiming Hu},
  doi          = {10.1016/j.patcog.2020.107321},
  journal      = {Pattern Recognition},
  pages        = {107321},
  shortjournal = {Pattern Recognition},
  title        = {Graph convolutional network with structure pooling and joint-wise channel attention for action recognition},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Training bidirectional generative adversarial networks with
hints. <em>PR</em>, <em>103</em>, 107320. (<a
href="https://doi.org/10.1016/j.patcog.2020.107320">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generative adversarial network (GAN) is composed of a generator and a discriminator where the generator is trained to transform random latent vectors to valid samples from a distribution and the discriminator is trained to separate such “fake” examples from true examples of the distribution, which in turn forces the generator to generate better fakes. The bidirectional GAN (BiGAN) also has an encoder working in the inverse direction of the generator to produce the latent space vector for a given example. This added encoder allows defining auxiliary reconstruction losses as hints for a better generator. On five widely-used data sets, we showed that BiGANs trained with the Wasserstein loss and augmented with hints learn better generators in terms of image generation quality and diversity, as measured numerically by the 1-nearest neighbor test, Fréchet inception distance, and reconstruction error, and qualitatively by visually analyzing the generated samples.},
  archive      = {J_PR},
  author       = {Uras Mutlu and Ethem Alpaydın},
  doi          = {10.1016/j.patcog.2020.107320},
  journal      = {Pattern Recognition},
  pages        = {107320},
  shortjournal = {Pattern Recognition},
  title        = {Training bidirectional generative adversarial networks with hints},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Single image-based head pose estimation with spherical
parametrization and 3D morphing. <em>PR</em>, <em>103</em>, 107316. (<a
href="https://doi.org/10.1016/j.patcog.2020.107316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Head pose estimation plays a vital role in various applications, e.g., driver-assistance systems, human-computer interaction, virtual reality technology , and so on. We propose a novel geometry-based method for accurately estimating the head pose from a single 2D face image at a very low computational cost. Specifically, the rectangular coordinates of only four non-coplanar feature points from a predefined 3D facial model as well as the corresponding ones automatically/manually extracted from a 2D face image are first normalized to exclude the effect of external factors (i.e., scale factor and translation parameters). Then, the four normalized 3D feature points are represented in spherical coordinates with reference to the uniquely determined sphere by themselves. Due to the spherical parametrization , the coordinates of feature points can then be morphed along all the three directions in the rectangular coordinates effectively. Finally, the rotation matrix indicating the head pose is obtained by minimizing the Euclidean distance between the normalized 2D feature points and the 2D re-projections of the morphed 3D feature points. Comprehensive experimental results over two popular datasets, i.e., Pointing’04 and Biwi Kinect , demonstrate that the proposed method can estimate head poses with higher accuracy and lower run time than state-of-the-art geometry-based methods. Even compared with start-of-the-art learning-based methods or geometry-based methods with additional depth information, our method still produces comparable performance.},
  archive      = {J_PR},
  author       = {Hui Yuan and Mengyu Li and Junhui Hou and Jimin Xiao},
  doi          = {10.1016/j.patcog.2020.107316},
  journal      = {Pattern Recognition},
  pages        = {107316},
  shortjournal = {Pattern Recognition},
  title        = {Single image-based head pose estimation with spherical parametrization and 3D morphing},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Textual data summarization using the self-organized
co-clustering model. <em>PR</em>, <em>103</em>, 107315. (<a
href="https://doi.org/10.1016/j.patcog.2020.107315">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, different studies have demonstrated the use of co-clustering, a data mining technique which simultaneously produces row-clusters of observations and column-clusters of features. The present work introduces a novel co-clustering model to easily summarize textual data in a document-term format. In addition to highlighting homogeneous co-clusters as other existing algorithms do we also distinguish noisy co-clusters from significant co-clusters, which is particularly useful for sparse document-term matrices. Furthermore, our model proposes a structure among the significant co-clusters, thus providing improved interpretability to users. The approach proposed contends with state-of-the-art methods for document and term clustering and offers user-friendly results. The model relies on the Poisson distribution and on a constrained version of the Latent Block Model, which is a probabilistic approach for co-clustering. A Stochastic Expectation-Maximization algorithm is proposed to run the model’s inference as well as a model selection criterion to choose the number of co-clusters. Both simulated and real data sets illustrate the efficiency of this model by its ability to easily identify relevant co-clusters.},
  archive      = {J_PR},
  author       = {Margot Selosse and Julien Jacques and Christophe Biernacki},
  doi          = {10.1016/j.patcog.2020.107315},
  journal      = {Pattern Recognition},
  pages        = {107315},
  shortjournal = {Pattern Recognition},
  title        = {Textual data summarization using the self-organized co-clustering model},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning to infer human attention in daily activities.
<em>PR</em>, <em>103</em>, 107314. (<a
href="https://doi.org/10.1016/j.patcog.2020.107314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The first attention model in the computer science community is proposed in 1998. In the following years, human attention has been intensively studied. However, these studies mainly refer human attention as the image regions that draw the attention of a human (outside the image) who is looking at the image. In this paper, we infer the attention of a human inside a third-person view video where the human is doing a task, and define human attention as attentional objects that coincide with the task the human is doing. To infer human attention, we propose a deep neural network model that fuses both low-level human pose cue and high-level task encoding cue. Due to the lack of appropriate public datasets for studying this problem, we newly collect a video dataset in complex Virtual-Reality (VR) scenes. In the experiments, we widely compare our method with three other methods on this VR dataset. In addition, we re-annotate a public real dataset and conduct the extensional experiments on this real dataset. The experiment results validate the effectiveness of our method.},
  archive      = {J_PR},
  author       = {Zhixiong Nan and Tianmin Shu and Ran Gong and Shu Wang and Ping Wei and Song-Chun Zhu and Nanning Zheng},
  doi          = {10.1016/j.patcog.2020.107314},
  journal      = {Pattern Recognition},
  pages        = {107314},
  shortjournal = {Pattern Recognition},
  title        = {Learning to infer human attention in daily activities},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning motion representation for real-time spatio-temporal
action localization. <em>PR</em>, <em>103</em>, 107312. (<a
href="https://doi.org/10.1016/j.patcog.2020.107312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current deep learning based spatio-temporal action localization methods that using motion information (predominated is optical flow) obtain the state-of-the-art performance. However, since the optical flow is pre-computed, leading to these methods face two problems – the computational efficiency is low and the whole network is not end-to-end trainable. We propose a novel spatio-temporal action localization approach with an integrated optical flow sub-network to address these two issues. Specifically, our designed flow subnet can estimate optical flow efficiently and accurately by using multiple consecutive RGB frames rather than two adjacent frames in a deep network, simultaneously, action localization is implemented in the same network interactive with flow computation end-to-end. To faster the speed, we exploit a neural network based feature fusion method in a pyramid hierarchical manner. It fuses spatial and temporal features at different granularities via combination function ( i.e. concatenation) and point-wise convolution to obtain multiscale spatio-temporal action features. Experimental results on three publicly available datasets, e.g. UCF101-24, JHMDB and AVA show that with both RGB appearance and optical flow cues, the proposed method gets the state-of-the-art performance in both efficiency and accuracy. Noticeably, it gets a significant improvement on efficiency. Compared to the currently most efficient method, it is 1.9 times faster in the running speed and 1.3\% video-mAP more accurate on the UCF101-24. Our proposed method reaches real-time computation for the first time (up to 38 FPS).},
  archive      = {J_PR},
  author       = {Dejun Zhang and Linchao He and Zhigang Tu and Shifu Zhang and Fei Han and Boxiong Yang},
  doi          = {10.1016/j.patcog.2020.107312},
  journal      = {Pattern Recognition},
  pages        = {107312},
  shortjournal = {Pattern Recognition},
  title        = {Learning motion representation for real-time spatio-temporal action localization},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive quantile low-rank matrix factorization.
<em>PR</em>, <em>103</em>, 107310. (<a
href="https://doi.org/10.1016/j.patcog.2020.107310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-rank matrix factorization (LRMF) has received much popularity owing to its successful applications in both computer vision and data mining. By assuming noise to come from a Gaussian, Laplace or mixture of Gaussian distributions, significant efforts have been made on optimizing the (weighted) L 1 or L 2 -norm loss between an observed matrix and its bilinear factorization. However, the type of noise distribution is generally unknown in real applications and inappropriate assumptions will inevitably deteriorate the behavior of LRMF. On the other hand, real data are often corrupted by skew rather than symmetric noise. To tackle this problem, this paper presents a novel LRMF model called AQ-LRMF by modeling noise with a mixture of asymmetric Laplace distributions . An efficient algorithm based on the expectation-maximization (EM) algorithm is also offered to estimate the parameters involved in AQ-LRMF. The AQ-LRMF model possesses the advantage that it can approximate noise well no matter whether the real noise is symmetric or skew. The core idea of AQ-LRMF lies in solving a weighted L 1 problem with weights being learned from data. The experiments conducted on synthetic and real data sets show that AQ-LRMF outperforms several state-of-the-art techniques. Furthermore, AQ-LRMF also has the superiority over the other algorithms in terms of capturing local structural information contained in real images.},
  archive      = {J_PR},
  author       = {Shuang Xu and Chunxia Zhang and Jiangshe Zhang},
  doi          = {10.1016/j.patcog.2020.107310},
  journal      = {Pattern Recognition},
  pages        = {107310},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive quantile low-rank matrix factorization},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Webly-supervised learning for salient object detection.
<em>PR</em>, <em>103</em>, 107308. (<a
href="https://doi.org/10.1016/j.patcog.2020.107308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {End-to-end training of a deep CNN-Based model for salient object detection usually requires a huge number of training samples with pixel-level annotations, which are costly and time-consuming to obtain. In this paper, we propose an approach that can utilize large amounts of web data for learning a deep salient object detection model. With thousands of images collected from the Web, we first employ several bottom-up saliency detection techniques to generate salient object masks for all images, and then use a novel quality evaluation method to pick out a subset of images with reliable masks for training. After that, we develop a self-training approach to boost the performance of our initial network, which iterates between the network training process and the training set updating process. Importantly, different from existing webly-supervised or weakly-supervised methods, our approach is able to automatically select reliable images for network training without requiring any human intervention (e.g., dividing images into different difficulty levels). Results of extensive experiments on several widely-used benchmarks demonstrate that our method has achieved state-of-the-art performance. It significantly outperforms existing unsupervised and weakly-supervised salient object detection methods, and achieves competitive or even better performance than fully supervised approaches.},
  archive      = {J_PR},
  author       = {Ao Luo and Xin Li and Fan Yang and Zhicheng Jiao and Hong Cheng},
  doi          = {10.1016/j.patcog.2020.107308},
  journal      = {Pattern Recognition},
  pages        = {107308},
  shortjournal = {Pattern Recognition},
  title        = {Webly-supervised learning for salient object detection},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fundamental sampling patterns for low-rank multi-view data
completion. <em>PR</em>, <em>103</em>, 107307. (<a
href="https://doi.org/10.1016/j.patcog.2020.107307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the multi-view data completion problem, i.e., to complete a matrix U = [ U 1 | U 2 ] U=[U1|U2] where the ranks of U, U 1 , and U 2 are given. In particular, we investigate the fundamental conditions on the sampling pattern, i.e., locations of the sampled entries for finite completability of such a multi-view data given the corresponding rank constraints. We provide a geometric analysis on the manifold structure for multi-view data to incorporate more than one rank constraint. We derive a probabilistic condition in terms of the number of samples per column that guarantees finite completability with high probability. Finally, we derive the guarantees for unique completability. Numerical results demonstrate reduced sampling complexity when the multi-view structure is taken into account as compared to when only low-rank structure of individual views is taken into account. Then, we propose an apporach using Newton’s method to almost achieve these information-theoretic bounds for mulit-view data retrieval by taking advantage of the rank decomposition and the analysis in this work.},
  archive      = {J_PR},
  author       = {Morteza Ashraphijuo and Xiaodong Wang and Vaneet Aggarwal},
  doi          = {10.1016/j.patcog.2020.107307},
  journal      = {Pattern Recognition},
  pages        = {107307},
  shortjournal = {Pattern Recognition},
  title        = {Fundamental sampling patterns for low-rank multi-view data completion},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning variable-length representation of words.
<em>PR</em>, <em>103</em>, 107306. (<a
href="https://doi.org/10.1016/j.patcog.2020.107306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A standard word embedding algorithm, such as ‘word2vec’, embeds each word as a dense vector of a preset dimensionality, the components of which are learned by maximizing the likelihood of predicting the context around it. However, as an inherent linguistic phenomenon, it is evident that there is a varying degree of difficulty in identifying words from their contexts. This suggests that a variable granularity in word vector representation may be useful to obtain sparser and more compressed word representations, requiring less storage space. To that end, in this paper, we propose a word vector training algorithm that uses a variable number of components to represent words. Given a text collection of documents, our algorithm, similar to the skip-gram approach of word2vec, learns to predict the context of a word given the current instance of a word. However, in contrast to skip-gram, which uses a static number of dimensions for each word vector, we propose to dynamically increase the dimensionality as a stochastic function of the prediction error. Our experiments with standard test collections demonstrate that our word representation method is able to achieve comparable (and sometimes even better) effectiveness than skip-gram word2vec, using a significantly smaller number of parameters (achieving compression ratio of around 65\%).},
  archive      = {J_PR},
  author       = {Debasis Ganguly},
  doi          = {10.1016/j.patcog.2020.107306},
  journal      = {Pattern Recognition},
  pages        = {107306},
  shortjournal = {Pattern Recognition},
  title        = {Learning variable-length representation of words},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Radical analysis network for learning hierarchies of chinese
characters. <em>PR</em>, <em>103</em>, 107305. (<a
href="https://doi.org/10.1016/j.patcog.2020.107305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chinese characters have a valuable property, this is, numerous Chinese characters are composed of a compact set of fundamental and structural radicals. This paper introduces a radical analysis network (RAN) that makes full use of this valuable property to implement radical-based Chinese character recognition. The proposed RAN employs an attention mechanism to extract radicals from Chinese characters and to detect spatial structures among the radicals. Then, the decoder in RAN generates a hierarchical composition of Chinese characters based on the knowledge of the extracted radicals and their internal structures. The method of treating a Chinese character as a composition of radicals rather than as a single character category is a human-like method that can reduce the size of the vocabulary, ignore redundant information among similar characters and enable the system to recognize unseen Chinese character categories, i.e., zero-shot learning. Through experiments, we assess the practicality of RAN for recognizing Chinese characters in natural scenes. Furthermore, a RAN framework can be proposed for scene text recognition with the extension of a dense recurrent neural network (denseRNN) encoder, a multihead coverage attention model and HSV representations. The proposed approach achieved the best performance in the ICPR MTWI 2018 competition.},
  archive      = {J_PR},
  author       = {Jianshu Zhang and Jun Du and Lirong Dai},
  doi          = {10.1016/j.patcog.2020.107305},
  journal      = {Pattern Recognition},
  pages        = {107305},
  shortjournal = {Pattern Recognition},
  title        = {Radical analysis network for learning hierarchies of chinese characters},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). IOS-net: An inside-to-outside supervision network for scale
robust text detection in the wild. <em>PR</em>, <em>103</em>, 107304.
(<a href="https://doi.org/10.1016/j.patcog.2020.107304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately detecting scene text is a challenging task due to perspective distortion, scale variance, varied orientations, uneven illumination. Among them, scale variance has always been a core issue and generally involves two types: various size and diverse aspect ratios of the text regions. In contrast to most existing approaches focusing on addressing one type of scale variance, this paper presents a novel inside-to-outside supervision network (IOS-Net) that can well tackle both two. Specifically, we design a hierarchical supervision module (HSM), which consists of a new inception unit with parallel asymmetric convolution and a skip-layer fusion structure . Inside the HSM, we introduce hierarchical supervision into the new inception unit to effectively capture the texts with diverse aspect ratios. Outside the HSM, we adopt multiple-scale supervision on the stacked HSMs to accurately detect the texts with various sizes. Moreover, a position-sensitive segmentation is used to enhance the representation of difficult text objects and the discrimination of adjacent ones. The proposed method achieves state-of-the-art performance on representative public benchmarks, reaching 86\% F-score and 11.5 frames per second (FPS) on the ICDAR 2015 incidental text dataset, 47\% F-score and 16.1 FPS on the COCO-Text dataset, 69\% F-score and 11.7 FPS on the ICDAR 2013 video text dataset.},
  archive      = {J_PR},
  author       = {Yuanqiang Cai and Weiqiang Wang and Yuting Chen and Qixiang Ye},
  doi          = {10.1016/j.patcog.2020.107304},
  journal      = {Pattern Recognition},
  pages        = {107304},
  shortjournal = {Pattern Recognition},
  title        = {IOS-net: An inside-to-outside supervision network for scale robust text detection in the wild},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CAGNet: Content-aware guidance for salient object detection.
<em>PR</em>, <em>103</em>, 107303. (<a
href="https://doi.org/10.1016/j.patcog.2020.107303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Beneficial from Fully Convolutional Neural Networks (FCNs), saliency detection methods have achieved promising results. However, it is still challenging to learn effective features for detecting salient objects in complicated scenarios, in which i) non-salient regions may have “salient-like” appearance; ii) the salient objects may have different-looking regions. To handle these complex scenarios, we propose a Feature Guide Network which exploits the nature of low-level and high-level features to i) make foreground and background regions more distinct and suppress the non-salient regions which have “salient-like” appearance; ii) assign foreground label to different-looking salient regions . Furthermore, we utilize a Multi-scale Feature Extraction Module (MFEM) for each level of abstraction to obtain multi-scale contextual information. Finally, we design a loss function which outperforms the widely used Cross-entropy loss. By adopting four different pre-trained models as the backbone, we prove that our method is very general with respect to the choice of the backbone model. Experiments on six challenging datasets demonstrate that our method achieves the state-of-the-art performance in terms of different evaluation metrics . Additionally, our approach contains fewer parameters than the existing ones, does not need any post-processing, and runs fast at a real-time speed of 28 FPS when processing a 480 × 480 image.},
  archive      = {J_PR},
  author       = {Sina Mohammadi and Mehrdad Noori and Ali Bahri and Sina Ghofrani Majelan and Mohammad Havaei},
  doi          = {10.1016/j.patcog.2020.107303},
  journal      = {Pattern Recognition},
  pages        = {107303},
  shortjournal = {Pattern Recognition},
  title        = {CAGNet: Content-aware guidance for salient object detection},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Shape-from-focus reconstruction using nonlocal matting
laplacian prior followed by MRF-based refinement. <em>PR</em>,
<em>103</em>, 107302. (<a
href="https://doi.org/10.1016/j.patcog.2020.107302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the problem of depth recovery from a sequence of multi-focus images, known as shape-from-focus (SFF). The conventional SFF techniques typically exhibit poor performance over textureless regions, and it is difficult to preserve depth edges and fine details while maintaining spatial consistency. Therefore, we propose an SFF depth recovery framework composed of depth reconstruction and refinement processes. We first formulate the depth reconstruction as a maximum a posterior (MAP) estimation problem with the inclusion of matting Laplacian prior. The nonlocal principle is adopted in matting Laplacian matrix construction to preserve depth edges and fine details. As the nonlocal principle breaks the spatial consistency, the reconstructed depth image is spatially inconsistent and suffers from the texture-copy artifacts. To smooth the noise and suppress the texture-copy artifacts, a closed-form edge-preserving depth refinement is proposed, which is formulated as a MAP estimation problem using Markov random fields (MRFs). Experimental results over synthetic and real scene datasets demonstrate the superiority of our algorithm in terms of robustness, and the ability to preserve edges and fine details while maintaining spatial consistency compared to existing approaches.},
  archive      = {J_PR},
  author       = {Zhiqiang Ma and Dongjoon Kim and Yeong-Gil Shin},
  doi          = {10.1016/j.patcog.2020.107302},
  journal      = {Pattern Recognition},
  pages        = {107302},
  shortjournal = {Pattern Recognition},
  title        = {Shape-from-focus reconstruction using nonlocal matting laplacian prior followed by MRF-based refinement},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Frobenius correlation based u-shapelets discovery for time
series clustering. <em>PR</em>, <em>103</em>, 107301. (<a
href="https://doi.org/10.1016/j.patcog.2020.107301">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An u-shapelet is a sub-sequence of a time series used for the clustering of time series datasets. The purpose of this paper is to discover u-shapelets on uncertain time series. To achieve this goal, we propose a dissimilarity score called FOTS whose computation is based on the eigenvector decomposition and the comparison of the autocorrelation matrices of the time series. This score is robust to the presence of uncertainty; it is not very sensitive to transient changes; it allows capturing complex relationships between time series such as oscillations and trends, and it is also well adapted to the comparison of short time series. The FOTS score is used with the Scalable Unsupervised Shapelet Discovery algorithm for the clustering of 63 datasets, and it has shown a substantial improvement in the quality of the clustering with respect to the Rand Index. This work defines a novel framework for the clustering of uncertain time series.},
  archive      = {J_PR},
  author       = {Vanel Steve Siyou Fotso and Engelbert Mephu Nguifo and Philippe Vaslin},
  doi          = {10.1016/j.patcog.2020.107301},
  journal      = {Pattern Recognition},
  pages        = {107301},
  shortjournal = {Pattern Recognition},
  title        = {Frobenius correlation based u-shapelets discovery for time series clustering},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint image deblurring and matching with feature-based
sparse representation prior. <em>PR</em>, <em>103</em>, 107300. (<a
href="https://doi.org/10.1016/j.patcog.2020.107300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image matching aims to find a similar area of the small image in the large image, which is one of the key steps in image fusion and vision-based navigation; however, most matching methods perform poorly when the images to be matched are blurred. Traditional approaches for blurred image matching usually follow a two-stage framework - first resorting to image deblurring and then performing image matching with the recovered image. However, the matching accuracy of these methods often suffers greatly from the deficiency of image deblurring. Recently, a joint image deblurring and matching method that utilizes the sparse representation prior to exploit the correlation between deblurring and matching was proposed to address this problem and found to obtain a higher matching accuracy. Yet, that technique is not efficient when the image is seriously blurred, and the method’s time complexity is excessive. In this paper, we propose a joint image deblurring and matching approach with a feature-based sparse representation prior. Our approach utilizes two-directional two-dimensional (2 D ) 2 PCA to extract feature vectors from images and obtains a sparse representation prior in a robust feature space rather than the original pixel space, thus mitigating the influence of image blur. Moreover, the reduction in the feature dimension can also increase the computational efficiency. Extensive experiments show that our approach significantly outperforms state-of-the-art approaches in terms of both accuracy and speed.},
  archive      = {J_PR},
  author       = {Juncai Peng and Yuanjie Shao and Nong Sang and Changxin Gao},
  doi          = {10.1016/j.patcog.2020.107300},
  journal      = {Pattern Recognition},
  pages        = {107300},
  shortjournal = {Pattern Recognition},
  title        = {Joint image deblurring and matching with feature-based sparse representation prior},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Corner detection based on shearlet transform and
multi-directional structure tensor. <em>PR</em>, <em>103</em>, 107299.
(<a href="https://doi.org/10.1016/j.patcog.2020.107299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image corners have been widely used in various computer vision tasks. Current multi-scale analysis based corner detectors do not make full use of the multi-scale and multi-directional structural information. This degrades their detection accuracy and capability of refining corners. In this work, an improved shearlet transform with a flexible number of directions and a reasonable support is proposed to extract accurate multi-scale and multi-directional structural information from images. To make full use of the structural information from the improved shearlets, a novel multi-directional structure tensor is constructed for corner detection, and a multi-scale corner measurement function is proposed to remove false candidate corners. Experimental results demonstrate that the proposed corner detector performs better than existing corner and interest point detectors in terms of detection accuracy, localization accuracy , and robustness to affine transformations, illumination changes, noise, viewpoint changes, etc. It has a great potential for extension as a descriptor and for applications in computer vision tasks.},
  archive      = {J_PR},
  author       = {Mingzhe Wang and Weichuan Zhang and Changming Sun and Arcot Sowmya},
  doi          = {10.1016/j.patcog.2020.107299},
  journal      = {Pattern Recognition},
  pages        = {107299},
  shortjournal = {Pattern Recognition},
  title        = {Corner detection based on shearlet transform and multi-directional structure tensor},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep support vector machine for hyperspectral image
classification. <em>PR</em>, <em>103</em>, 107298. (<a
href="https://doi.org/10.1016/j.patcog.2020.107298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To improve on the robustness of traditional machine learning approaches, emphasis has recently shifted to the integration of such methods with Deep Learning techniques. However, the classification problems, complexity and inconsistency in several spectral classifiers developed for hyperspectral images are some reasons warranting further research. This study investigates the application of Deep Support Vector Machine (DSVM) for hyperspectral image classification. Two hyperspectral images, Indian Pines and University of Pavia are used as tentative test beds for the experiment. The DSVM is implemented with four kernel functions: Exponential Radial Basis Function (ERBF), Gaussian Radial Basis Function (GRBF), neural and polynomial. Stand-alone Support Vector Machines form the interconnecting weights of the entire network. The network is trained with one hundred input datasets, and the interconnecting weights of the network are initialised using the regularisation parameter of the model. Numerical results show that the classification accuracies of the DSVM for Indian Pines and University of Pavia based on each DSVM kernel functions are: ERBF (98.87\%, 98.16\%), GRBF (98.90\%, 98.47\%), neural (98.41\%, 97.27\%), and polynomial (99.24\%, 98.79\%). By comparing the DSVM algorithm against well-known classifiers, Support Vector Machine (SVM), Deep Neural Network (DNN), Gaussian Mixture Model (GMM), K Nearest Neighbour (KNN), and K Means (KM) classifiers, the mean classification accuracies for Indian Pines and University of Pavia are: DSVM (98.86\%, 98.17\%), SVM (76.03\%, 73.52\%), DNN (94.45\%, 93.79\%), GMM (76.82\%, 78.35\%), KNN (76.87\%, 78.80\%), and KM (21.65\%, 18.18\%). These results indicate that the DSVM outperformed the other classification algorithms . The high accuracy obtained with the DSVM validates its efficacy as state-of-the-art algorithm for hyperspectral image classification.},
  archive      = {J_PR},
  author       = {Onuwa Okwuashi and Christopher E. Ndehedehe},
  doi          = {10.1016/j.patcog.2020.107298},
  journal      = {Pattern Recognition},
  pages        = {107298},
  shortjournal = {Pattern Recognition},
  title        = {Deep support vector machine for hyperspectral image classification},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A survey on image and video cosegmentation: Methods,
challenges and analyses. <em>PR</em>, <em>103</em>, 107297. (<a
href="https://doi.org/10.1016/j.patcog.2020.107297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image and video cosegmentation is a newly emerging and rapidly progressing area, which aims at delineating common objects at pixel-level from a group of images or a set of videos. Plenty of related works have been published and implemented in varied applications, but there lacks a systematic survey on both image and video cosegmentation. This paper provides a comprehensive overview including the existing methods, applications, and challenges. Specifically, different cosegmentation problem settings are described, the formulation details of the methods are summarized and their potential applications are listed. Moreover, the benchmark datasets and standard evaluation metrics are also given; and the future directions and unsolved challenges are discussed.},
  archive      = {J_PR},
  author       = {Yan Ren and Adams Wai Kin Kong and Licheng Jiao},
  doi          = {10.1016/j.patcog.2020.107297},
  journal      = {Pattern Recognition},
  pages        = {107297},
  shortjournal = {Pattern Recognition},
  title        = {A survey on image and video cosegmentation: Methods, challenges and analyses},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DevsNet: Deep video saliency network using short-term and
long-term cues. <em>PR</em>, <em>103</em>, 107294. (<a
href="https://doi.org/10.1016/j.patcog.2020.107294">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, there have been various saliency detection methods proposed for still images based on deep learning techniques. However, the research on saliency detection for video sequences is still limited. In this study, we introduce a novel deep learning framework of saliency detection for video sequences, namely Deep Video Saliency Network (DevsNet). DevsNet mainly consists of two components: 3D Convolutional Network (3D-ConvNet) and Bidirectional Convolutional Long-Short Term Memory Network (B-ConvLSTM). 3D-ConvNet is constructed to learn short-term spatiotemporal information and the long-term spatiotemporal features are learned by B-ConvLSTM. The designed B-ConvLSTM can extract the temporal information not just from the previous video frames but also from the next frames, which demonstrates that the proposed model considers both the forward and backward temporal information. By combining the short-term and long-term spatiotemporal cues, the proposed DevsNet can extract saliency information for video sequences effectively and efficiently. Extensive experiments have been conducted to show that the proposed model can obtain better performance in spatiotemporal saliency prediction than the state-of-the-art models.},
  archive      = {J_PR},
  author       = {Yuming Fang and Chi Zhang and Xiongkuo Min and Hanqin Huang and Yugen Yi and Guangtao Zhai and Chia-Wen Lin},
  doi          = {10.1016/j.patcog.2020.107294},
  journal      = {Pattern Recognition},
  pages        = {107294},
  shortjournal = {Pattern Recognition},
  title        = {DevsNet: Deep video saliency network using short-term and long-term cues},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning shape and motion representations for view invariant
skeleton-based action recognition. <em>PR</em>, <em>103</em>, 107293.
(<a href="https://doi.org/10.1016/j.patcog.2020.107293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skeleton-based action recognition is an increasing attentioned task that analyses spatial configuration and temporal dynamics of a human action from skeleton data, which has been widely applied in intelligent video surveillance and human-computer interaction. How to design an effective framework to learn discriminative spatial and temporal characteristics for skeleton-based action recognition is still a challenging problem. The shape and motion representations of skeleton sequences are the direct embodiment of spatial and temporal characteristics respectively, which can well address for human action description. In this work, we propose an original unified framework to learn comprehensive shape and motion representations from skeleton sequences by using Geometric Algebra. We firstly construct skeleton sequence space as a subset of Geometric Algebra to represent each skeleton sequence along both the spatial and temporal dimensions. Then rotor-based view transformation method is proposed to eliminate the effect of viewpoint variation, which remains the relative spatio-temporal relations among skeleton frames in a sequence. We also construct spatio-temporal view invariant model (STVIM) to collectively integrate spatial configuration and temporal dynamics of skeleton joints and bones. In STVIM, skeleton sequence shape and motion representations which mutually compensate are jointly learned to describe skeleton-based actions comprehensively. Furthermore, a selected multi-stream Convolutional Neural Network is employed to extract and fuse deep features from mapping images of the learned representations for skeleton-based action recognition. Experimental results on NTU RGB+D, Northwestern-UCLA and UTD-MHAD datasets consistently verify the effectiveness of our proposed method and the superior performance over state-of-the-art competitors.},
  archive      = {J_PR},
  author       = {Yanshan Li and Rongjie Xia and Xing Liu},
  doi          = {10.1016/j.patcog.2020.107293},
  journal      = {Pattern Recognition},
  pages        = {107293},
  shortjournal = {Pattern Recognition},
  title        = {Learning shape and motion representations for view invariant skeleton-based action recognition},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An improved GrabCut on multiscale features. <em>PR</em>,
<em>103</em>, 107292. (<a
href="https://doi.org/10.1016/j.patcog.2020.107292">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {the GrabCut can effectively extract the foreground according to features in a cartoon image; however, the performance is not so effective for a real image, because the feature extraction is independent of segmentation. To improve segmentation performance , this paper proposes an improved GrabCut which combines the segmentation and multiscale feature extraction into a unified model. In this model, the segmentation relies on multiscale features, and the multiscale features depend on multiscale decomposition. A novel total variation regularization is proposed in multiscale decomposition to preserve edges and remove the region inhomogeneity, by which the generalization of features for segmentation is improved. The features obtained by the multiscale decomposition are integrated into the segmentation process , and the foreground can be easily extracted from a proper scale. Experimental results indicate that, compared to the existing GrabCut and improved techniques, this method provides competitive performance in terms of the segmentation accuracy , while being insensitive to inhomogeneity.},
  archive      = {J_PR},
  author       = {Kun He and Dan Wang and Miao Tong and Zhijuan Zhu},
  doi          = {10.1016/j.patcog.2020.107292},
  journal      = {Pattern Recognition},
  pages        = {107292},
  shortjournal = {Pattern Recognition},
  title        = {An improved GrabCut on multiscale features},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Novel dimensionality reduction approach for unsupervised
learning on small datasets. <em>PR</em>, <em>103</em>, 107291. (<a
href="https://doi.org/10.1016/j.patcog.2020.107291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We focus on an image classification task in which only several unlabeled images per class are available for learning and low computational complexity is required. We recall the state-of-the-art methods that are used to solve the task: autoencoder-based approaches and manifold-decomposition-based approaches. Next, we introduce our proposed method, which is based on a combination of the F-transform and (kernel) principal component analysis. F-transform significantly reduces the computation time of PCA and increases the robustness of PCA to translation, while PCA proposes more descriptive features. This combination performs 3D reduction: the F-transform reduces dimensionality over a single 2D image, while PCA reduces dimensionality through the whole set of processed images. Based on the benchmark results, our method may outperform deep-learning-based methods in limited settings. For completeness, we also address other image resampling algorithms that can be used instead of the F-transform, and we find that the F-transform is the most suitable.},
  archive      = {J_PR},
  author       = {Petr Hurtik and Vojtech Molek and Irina Perfilieva},
  doi          = {10.1016/j.patcog.2020.107291},
  journal      = {Pattern Recognition},
  pages        = {107291},
  shortjournal = {Pattern Recognition},
  title        = {Novel dimensionality reduction approach for unsupervised learning on small datasets},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Three-stream fusion network for first-person interaction
recognition. <em>PR</em>, <em>103</em>, 107279. (<a
href="https://doi.org/10.1016/j.patcog.2020.107279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {First-person interaction recognition is a challenging task because of unstable video conditions resulting from the camera wearer’s movement. For human interaction recognition from a first-person viewpoint, this paper proposes a three-stream fusion network with two main parts: three-stream architecture and three-stream correlation fusion. The three-stream architecture captures the characteristics of the target appearance, target motion, and camera ego-motion. Meanwhile the three-stream correlation fusion combines the feature map of each of the three streams to consider the correlations among the target appearance, target motion, and camera ego-motion. The fused feature vector is robust to the camera movement and compensates for the noise of the camera ego-motion. Short-term intervals are modeled using the fused feature vector, and a long short-term memory (LSTM) model considers the temporal dynamics of the video. We evaluated the proposed method on two public benchmark datasets to validate the effectiveness of our approach. The experimental results show that the proposed fusion method successfully generated a discriminative feature vector, and our network outperformed all competing activity recognition methods in first-person videos where considerable camera ego-motion occurs.},
  archive      = {J_PR},
  author       = {Ye-Ji Kim and Dong-Gyu Lee and Seong-Whan Lee},
  doi          = {10.1016/j.patcog.2020.107279},
  journal      = {Pattern Recognition},
  pages        = {107279},
  shortjournal = {Pattern Recognition},
  title        = {Three-stream fusion network for first-person interaction recognition},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic soccer field of play registration. <em>PR</em>,
<em>103</em>, 107278. (<a
href="https://doi.org/10.1016/j.patcog.2020.107278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a strategy for the automatic registration of soccer images on a model of the field of play. First, a robust and efficient preprocessing is applied to discard the areas of the image that do not belong to the field of play and eliminate most edge points that are not part of the line marks. Then, a novel probabilistic decision tree is used to identify the most probable classification for the set of all the straight lines in the image. Additionally, the line surrounding the center circle is also modeled for providing results when only few straight lines are visible. Finally, a three-step analysis stage is applied to ensure the validity of the results. To assess its quality, the strategy has been tested on several sequences corresponding to three stadiums with different characteristics. The results obtained have shown that the registration is successful in most images (94\%).},
  archive      = {J_PR},
  author       = {Carlos Cuevas and Daniel Quilón and Narciso García},
  doi          = {10.1016/j.patcog.2020.107278},
  journal      = {Pattern Recognition},
  pages        = {107278},
  shortjournal = {Pattern Recognition},
  title        = {Automatic soccer field of play registration},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Open-set face identification with index-of-max hashing by
learning. <em>PR</em>, <em>103</em>, 107277. (<a
href="https://doi.org/10.1016/j.patcog.2020.107277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale face identification or 1-to-N matching where N is huge, plays a vital role in biometrics and surveillance. The system demands accurate and speedy matching where compact facial feature representation and a simple matcher are favored. On the other hand, most research considers closed-set identification that assumes that all identities of probe samples are enclosed in the gallery. On the contrary, open-set identification expects that some probe identities are not known to the system. This setup poses an additional challenge, where the system should be able to reject those probes that correspond to unknown identities. In this paper, we address the large-scale open-set face identification problem with a compact facial representation that is based on the index-of-maximum (IoM) hashing, which was designed for biometric template protection. To be specific, the existing random IoM hashing is advanced to a data-driven based hashing technique, where the hashed face code can be made compact and matching can be easily performed by the Hamming distance, which can offer highly efficient matching. Furthermore, since IoM hashing transforms the original facial features non-invertibly, the privacy of users can also be preserved. Along with IoM hashed face code, we explore several fusion strategies to address the open-set face identification problem. The comprehensive evaluations are carried out with three large-scale unconstrained face datasets, namely LFW, VGG2 and IJB-C.},
  archive      = {J_PR},
  author       = {Xingbo Dong and Soohyung Kim and Zhe Jin and Jung Yeon Hwang and Sangrae Cho and Andrew Beng Jin Teoh},
  doi          = {10.1016/j.patcog.2020.107277},
  journal      = {Pattern Recognition},
  pages        = {107277},
  shortjournal = {Pattern Recognition},
  title        = {Open-set face identification with index-of-max hashing by learning},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Driving maneuver early detection via sequence learning from
vehicle signals and video images. <em>PR</em>, <em>103</em>, 107276. (<a
href="https://doi.org/10.1016/j.patcog.2020.107276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driving Maneuver Early Detection (DMED) is particularly useful for many applications of intelligent vehicle systems, including driver warning and collision avoidance systems. In this paper, we introduce a robust DMED model, denoted as University of Michigan Dearborn (UMD)-DMED, developed using innovative features and deep learning techniques. The UMD-DMED model contains three major computational components, distance based representation of driving context, combined vehicle trajectory features and visual features, and a Long Short-Term Memory (LSTM)-based neural network that captures temporal dependencies of driving maneuvers. To properly evaluate the performances of UMD-DMED, we developed two DMED systems based on the UMD-DMED model, one system is based on partially observed evidence of maneuver events, and another on features observed ahead of the time that driving maneuvers take place. We conducted the extensive experiments using a data set containing 1078 maneuver events extracted from 37 hours of real world driving trips. The results demonstrate that the UMD-DMED model is capable of learning the latent features of five different classes of driving maneuvers, i.e. left turn, right turn, left lane change, right lane change, driving straight . Comparing to four different state-of-the-art DMED systems, the UMD-DMED achieved better detection performances in both, the detection based on partial observations of driver maneuvering, and based on driving context observed ahead-of-time.},
  archive      = {J_PR},
  author       = {Xishuai Peng and Yi Lu Murphey and Ruirui Liu and Yuanxiang Li},
  doi          = {10.1016/j.patcog.2020.107276},
  journal      = {Pattern Recognition},
  pages        = {107276},
  shortjournal = {Pattern Recognition},
  title        = {Driving maneuver early detection via sequence learning from vehicle signals and video images},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Global and local sensitivity guided key salient object
re-augmentation for video saliency detection. <em>PR</em>, <em>103</em>,
107275. (<a href="https://doi.org/10.1016/j.patcog.2020.107275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image saliency is determined by spatial semantic features , while video saliency is affected by multiple factors such as spatial and temporal information. Since human eyes stay extremely short on each frame, the dynamic salient area is more focused and concentrated on one salient object. In order to better simulate the human visual attention mechanism in dynamic scenes, we propose a key salient object re-augmentation method (KSORA) based on the guidance of both bottom-up weighted features and top-down semantic knowledge . The bottom-up feature weighting strategy effectively eliminates noisy and redundancy, and provide accurate local spatiotemporal features for saliency inference. The top-down key object enhancement strategy ranks salient candidates based on global statistical knowledge, so as to explicitly enhance the saliency proportion of the key object. The fusion of the local weighted spatiotemporal features and the global key object augmentation features not only ensures spatiotemporal consistency, but also facilitates obtaining more concentrated salient prediction. Results on three large datasets validate that our proposed method has the capability of improving the detection accuracy in complex scenes.},
  archive      = {J_PR},
  author       = {Zheng Wang and Ziqi Zhou and Huchuan Lu and Jianmin Jiang},
  doi          = {10.1016/j.patcog.2020.107275},
  journal      = {Pattern Recognition},
  pages        = {107275},
  shortjournal = {Pattern Recognition},
  title        = {Global and local sensitivity guided key salient object re-augmentation for video saliency detection},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Depth upsampling based on deep edge-aware learning.
<em>PR</em>, <em>103</em>, 107274. (<a
href="https://doi.org/10.1016/j.patcog.2020.107274">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth map upsampling will unavoidably smoothen the edges leading to blurry results on the depth boundaries, especially at large upscaling factors. Given that edges represent the most important cue in addressing the task of depth upsampling, we propose a novel depth upsampling framework based on deep edge-aware learning. Unlike existing CNN-based approaches that directly predict depth values from low resolution (LR) depth input, our framework firstly learns edge information of depth boundaries from the known LR depth map and its corresponding high resolution (HR) color image as reconstruction cues. Then, two depth restoration modules, i.e., a fast depth filling strategy and a cascaded restoration network, are proposed to recover HR depth map by leveraging the predicted edge map and the HR color image. Extensive comparisons on both edge inference and depth upsampling under noisy and noiseless cases demonstrate the superiority of the proposed approaches.},
  archive      = {J_PR},
  author       = {Zhihui Wang and Xinchen Ye and Baoli Sun and Jingyu Yang and Rui Xu and Haojie Li},
  doi          = {10.1016/j.patcog.2020.107274},
  journal      = {Pattern Recognition},
  pages        = {107274},
  shortjournal = {Pattern Recognition},
  title        = {Depth upsampling based on deep edge-aware learning},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast minutiae extractor using neural network. <em>PR</em>,
<em>103</em>, 107273. (<a
href="https://doi.org/10.1016/j.patcog.2020.107273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a fast and reliable neural network-based algorithm for fingerprint minutiae extraction. In particular, our algorithm involves a two-stage process: in the first stage, a network generates candidate patches in which minutiae may exist; in the second stage, another network extracts minutiae from every patch.These two networks share a common part to reduce the running time. Moreover, we analyze the properties of fingerprint images and propose a principle for designing efficient networks for minutiae extraction. For efficiency, our algorithm extracts minutiae directly from raw fingerprint images , without traditional pre-processes. Another benefit of this design is that the networks only require datasets with minutiae labels for training. On the public fingerprint datasets (FVC 2002 and 2004), our algorithm requires 26 ms on average to extract minutiae from one fingerprint on a single GPU . Compared with other neural network-based algorithms, our algorithm runs approximately 10 times faster and does not lose substantial accuracy.},
  archive      = {J_PR},
  author       = {Baicun Zhou and Congying Han and Yonghong Liu and Tiande Guo and Jin Qin},
  doi          = {10.1016/j.patcog.2020.107273},
  journal      = {Pattern Recognition},
  pages        = {107273},
  shortjournal = {Pattern Recognition},
  title        = {Fast minutiae extractor using neural network},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). HoPPF: A novel local surface descriptor for 3D object
recognition. <em>PR</em>, <em>103</em>, 107272. (<a
href="https://doi.org/10.1016/j.patcog.2020.107272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Three-dimensional feature descriptors play an important role in 3D computer vision because they are widely employed in many 3D perception applications to extract point correspondences between two point clouds. However, most existing description methods suffer from either weak robustness, low descriptiveness, or costly computation. Thus, a 3D local feature descriptor named Histograms of Point Pair Features (HoPPF) is proposed in this paper, and it is aimed at robust representation, high descriptiveness, and efficient computation. First, we propose a novel method to redirect surface normals and use the Poisson-disk sampling strategy to solve the problem of data redundancy in data pre-processing. Second, a new technique is applied to divide the local point pair set of each keypoint into eight regions. Then, the distribution of local point pairs of each region is used to construct the corresponding sub-features. Finally, the proposed HoPPF is generated by concatenating all sub-features into a vector. The performance of the HoPPF method is rigorously evaluated on several standard datasets. The results of the experiments and comparisons with other state-of-the-art methods validate the superiority of the HoPPF descriptor in term of robustness, descriptiveness, and efficiency. Moreover, the proposed technique for division of point pair sets is used to modify the other typical point-pair-based descriptor (i.e., PFH) to show its generalization ability . The proposed HoPPF is also applied to object recognition on real datasets captured by different devices (e.g., Kinect and LiDAR) to verify the feasibility of this method for 3D vision applications.},
  archive      = {J_PR},
  author       = {Huan Zhao and Minjie Tang and Han Ding},
  doi          = {10.1016/j.patcog.2020.107272},
  journal      = {Pattern Recognition},
  pages        = {107272},
  shortjournal = {Pattern Recognition},
  title        = {HoPPF: A novel local surface descriptor for 3D object recognition},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Shallow2Deep: Indoor scene modeling by single image
understanding. <em>PR</em>, <em>103</em>, 107271. (<a
href="https://doi.org/10.1016/j.patcog.2020.107271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dense indoor scene modeling from 2D images has been bottlenecked due to the absence of depth information and cluttered occlusions. We present an automatic indoor scene modeling approach using deep features from neural networks . Given a single RGB image , our method simultaneously recovers semantic contents, 3D geometry and object relationship by reasoning indoor environment context. Particularly, we design a shallow-to-deep architecture on the basis of convolutional networks for semantic scene understanding and modeling. It involves multi-level convolutional networks to parse indoor semantics/geometry into non-relational and relational knowledge. Non-relational knowledge extracted from shallow-end networks (e.g. room layout, object geometry) is fed forward into deeper levels to parse relational semantics (e.g. support relationship). A Relation Network is proposed to infer the support relationship between objects. All the structured semantics and geometry above are assembled to guide a global optimization for 3D scene modeling. Qualitative and quantitative analysis demonstrates the feasibility of our method in understanding and modeling semantics-enriched indoor scenes by evaluating the performance of reconstruction accuracy , computation performance and scene complexity.},
  archive      = {J_PR},
  author       = {Yinyu Nie and Shihui Guo and Jian Chang and Xiaoguang Han and Jiahui Huang and Shi-Min Hu and Jian Jun Zhang},
  doi          = {10.1016/j.patcog.2020.107271},
  journal      = {Pattern Recognition},
  pages        = {107271},
  shortjournal = {Pattern Recognition},
  title        = {Shallow2Deep: Indoor scene modeling by single image understanding},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cross-view hashing via supervised deep discrete matrix
factorization. <em>PR</em>, <em>103</em>, 107270. (<a
href="https://doi.org/10.1016/j.patcog.2020.107270">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matrix factorization has been utilized for the task of cross-view hashing, where basis functions are learned to map data from different views to the same hamming embedding. It is possible that the basis functions between the hamming embedding and the original data matrix contain rather complex hierarchical information, which existing work can not capture. In addition, previous work employs relaxation technique in the matrix factorization based hashing which may lead to large quantization error . To address these issues, this paper presents a novel Supervised Discrete Deep Matrix Factorization (SDDMF) for cross-view hashing. We introduce deep matrix factorization so that SDDMF is able to learn a set of hierarchical basis functions and unified binary codes from different views. In addition, a classification error term is incorporated into the objective to learn discriminative binary codes . We then employ a linearization technique to directly optimize the discrete constraints which can significantly reduce the quantization error. Experimental results on three standard datasets with image-text modalities verify that SDDMF significantly outperforms several state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Yingjun Xiong and Yan Xu and Xin Shu},
  doi          = {10.1016/j.patcog.2020.107270},
  journal      = {Pattern Recognition},
  pages        = {107270},
  shortjournal = {Pattern Recognition},
  title        = {Cross-view hashing via supervised deep discrete matrix factorization},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). TVENet: Temporal variance embedding network for fine-grained
action representation. <em>PR</em>, <em>103</em>, 107267. (<a
href="https://doi.org/10.1016/j.patcog.2020.107267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the breakthroughs in general action understanding, it has become an inevitable trend to analyze the actions in finer granularity . However, related researches have been largely hindered by the lack of fine-grained datasets and the difficulty of capturing subtle differences between fine-grained actions that are highly similar overall. In this paper, we address the above challenges by constructing a fine-grained action dataset, i.e., Figure Skating, which can be used for end-to-end network training and presenting a framework for the joint optimization of classification and similarity constraints. We propose to incorporate the triplet loss into the training of Convolutional Neural Network , which learns a mapping from fine-grained actions to a compact Euclidean space where distances directly correspond to a measure of action similarity. Triplet loss compels actions of distinct classes to have larger distances than actions of the same class. Besides, to boost the discrimination of the fine-grained actions, we further propose a temporal variance embedding network (TVENet) embedding temporal context variances into the feature embeddings during the joint network training. The experimental results on Figure Skating dataset, HMDB51 dataset as well as UCF101 dataset demonstrate the effectiveness of TVENet representation for fine-grained action search.},
  archive      = {J_PR},
  author       = {Tingting Han and Hongxun Yao and Wenlong Xie and Xiaoshuai Sun and Sicheng Zhao and Jun Yu},
  doi          = {10.1016/j.patcog.2020.107267},
  journal      = {Pattern Recognition},
  pages        = {107267},
  shortjournal = {Pattern Recognition},
  title        = {TVENet: Temporal variance embedding network for fine-grained action representation},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Saliency detection using a deep conditional random field
network. <em>PR</em>, <em>103</em>, 107266. (<a
href="https://doi.org/10.1016/j.patcog.2020.107266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Saliency detection has made remarkable progress along with the development of deep learning . While how to integrate the low-level intrinsic context with high-level semantic information to keep the object boundary sharp and restrain the background noise is still a challenging problem. Many attempts on network structures and refinement strategies have been explored, such as using Conditional Random Field (CRF) to improve the accuracy of saliency map , but it is independent from the deep network and cannot be trained end-to-end. To tackle this issue, we propose a novel Deep Conditional Random Field network (DCRF) to take both deep feature and neighbor information into consideration. First, Multi-scale Feature Extraction Module (MFEM) is adopted to capture the low level texture and high level semantic features, multi-stacks of deconvolution layers are employed to improve the spatial resolution of deep layers. Then we employ Backward Optimization Module (BOM) to guide shallower layers by high-level location and shape information derived from deeper layers, which intrinsically enhance the representational capacity of low-level features. Finally, a Deep Conditional Random Field Module (DCRFM) with unary and pairwise potentials is designed to concentrate on spatial neighbor relations to obtain a compact and uniformed saliency map . Extensive experimental results on 5 datasets in terms of 6 evaluation metrics demonstrate that the proposed method achieves state-of-the-art performance.},
  archive      = {J_PR},
  author       = {Wenliang Qiu and Xinbo Gao and Bing Han},
  doi          = {10.1016/j.patcog.2020.107266},
  journal      = {Pattern Recognition},
  pages        = {107266},
  shortjournal = {Pattern Recognition},
  title        = {Saliency detection using a deep conditional random field network},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LG: A clustering framework supported by point proximity
relations. <em>PR</em>, <em>103</em>, 107265. (<a
href="https://doi.org/10.1016/j.patcog.2020.107265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a research problem based on the data&#39;s proximity relationship which is not made full use of by all the existing algorithms. In this paper, we present a novel two-stage LG framework consisting of the proposed Local Energy Gradient Oppression (LEGO) and the Guide Point Assignation (GPA) strategies which are closely related to the data points’ proximity relations. In the LG framework, it is crucial to locate the appropriate centers for the subsequent data label assignment, and therefore we introduce the nuclear model viewing the dataset as a collection of charged particles, which is the basis of LEGO, and the points with local maximum potential energy are ascertained as the cluster centers. Besides, the GPA strategy innovatively adopts the idea that the cluster center actively selects data points as the same cluster, enabling the LG framework still to be effective when dealing with datasets of arbitrary shape distribution. Superiorities of the proposed framework and the two strategies are demonstrated on four synthetic datasets and three real-world faces image datasets in terms of two clustering performance metrics.},
  archive      = {J_PR},
  author       = {Qv Hui and Yin Jihao and Luo Xiaoyan},
  doi          = {10.1016/j.patcog.2020.107265},
  journal      = {Pattern Recognition},
  pages        = {107265},
  shortjournal = {Pattern Recognition},
  title        = {LG: A clustering framework supported by point proximity relations},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Handling gaussian blur without deconvolution. <em>PR</em>,
<em>103</em>, 107264. (<a
href="https://doi.org/10.1016/j.patcog.2020.107264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper presents a new theory of invariants to Gaussian blur. Unlike earlier methods, the blur kernel may be arbitrary oriented, scaled and elongated. Such blurring is a semi-group action in the image space, where the orbits are classes of blur-equivalent images. We propose a non-linear projection operator which extracts blur-insensitive component of the image. The invariants are then formally defined as moments of this component but can be computed directly from the blurred image without an explicit construction of the projections. Image description by the new invariants does not require any prior knowledge of the blur kernel parameters and does not include any deconvolution . The invariance property could be extended also to linear transformation of the image coordinates and combined affine-blur invariants can be constructed. Experimental comparison to three other blur-invariant methods is given. Potential applications of the new invariants are in blur/position invariant image recognition and in robust template matching .},
  archive      = {J_PR},
  author       = {Jitka Kostková and Jan Flusser and Matěj Lébl and Matteo Pedone},
  doi          = {10.1016/j.patcog.2020.107264},
  journal      = {Pattern Recognition},
  pages        = {107264},
  shortjournal = {Pattern Recognition},
  title        = {Handling gaussian blur without deconvolution},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised hashing based on the recovery of subspace
structures. <em>PR</em>, <em>103</em>, 107261. (<a
href="https://doi.org/10.1016/j.patcog.2020.107261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised semantic hashing should in principle keep the semantics among samples consistent with the intrinsic geometric structures of the dataset. In this paper, we propose a novel multiple stage unsupervised hashing method , named “ Unsupervised Hashing based on the Recovery of Subspace Structures ” (RSSH) for image retrieval . Specifically, we firstly adapt the Low-rank Representation (LRR) model into a new variant which treats the real-world data as samples drawn from a union of several low-rank subspaces. Then, the pairwise similarities are represented in a space-and-time saving manner based on the learned low-rank correlation matrix of the modified LRR. Next, the challenging discrete graph hashing is employed for binary hashing codes. Notably, we convert the original graph hashing model into an optimization-friendly formalization, which is addressed with efficient closed-form solutions for its subproblems . Finally, the devised linear hash functions are fast achieved for out-of-samples. Retrieval experiments on four image datasets testify the superiority of RSSH to several state-of-the-art hashing models. Besides, it’s worth mentioning that RSSH, a shallow model, significantly outperforms two recently proposed unsupervised deep hashing methods, which further confirms its effectiveness.},
  archive      = {J_PR},
  author       = {Zhibao Tian and Hui Zhang and Yong Chen and Dell Zhang},
  doi          = {10.1016/j.patcog.2020.107261},
  journal      = {Pattern Recognition},
  pages        = {107261},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised hashing based on the recovery of subspace structures},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modelling visual impressions for chinese and pakistani
ethnic groups. <em>PR</em>, <em>103</em>, 107259. (<a
href="https://doi.org/10.1016/j.patcog.2020.107259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work describes an investigation into the relationship between the visual impressions of a number of facial images each described by a set of parameters related to the position and size of discrete components: the eyes, the nose and the lips. Observations were made by members of two ethnic groups, Chinese and Pakistani, and the images comprised female faces from the same two groups. The observers used 16 impression scales to assess each image and a forced-choice scaling technique. From the factor analysis, the results showed that three visual impressions, attractive, feminine and mature , can well represent all the 16 scales. In the second experiment, the observers assessed visual impressions of more images using only these three impressions. Data are presented relating to the differences between the observations for the various facial locations, as well as between observers from different ethnic groups. Models have been developed that describe the data and their predictions outperformed traditional models, i.e.symmetry, golden ratio and neoclassical canons. The differences between the results of the two ethnic groups were found to be small; there were however, some significant differences in the responses to different facial features.},
  archive      = {J_PR},
  author       = {Muhammad Farhan Mughal and Ming Ronnier Luo and Michael Pointer},
  doi          = {10.1016/j.patcog.2020.107259},
  journal      = {Pattern Recognition},
  pages        = {107259},
  shortjournal = {Pattern Recognition},
  title        = {Modelling visual impressions for chinese and pakistani ethnic groups},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploring temporal consistency for human pose estimation in
videos. <em>PR</em>, <em>103</em>, 107258. (<a
href="https://doi.org/10.1016/j.patcog.2020.107258">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a method of exploring temporal information for estimating human poses in videos. The current state-of-the-art methods utilizing temporal information can be categorized into two major branches. The first category is a model-based method that captures the temporal information entirely by using a learnable function such as RNN or 3D convolution. However, these methods are limited in exploring temporal consistency, which is essential for estimating human joint positions in videos. The second category is the posterior enhancement method, where an independent post-processing step (e.g., using optical flow) is applied to enhance the prediction. However, operations such as optical flow estimation can be susceptible to the occlusion and motion blur problems, which will adversely affect the final performance. We propose a novel Temporal Consistency Exploration (TCE) module to address both shortcomings. Compared to previous approaches, the TCE module is more efficient as it captures the temporal consistency at the feature level without having to post-process and calculate extra optical flow. Further, to capture the rich spatial context in video data, we design a multi-scale TCE to explore the time consistency information at multi-scale spatial levels. Finally, a video-based pose estimation network is designed, which is based on the encoder-decoder architecture and extended with the powerful multi-scale TCE module. We comprehensively evaluate the proposed model on two video datasets, Sub-JHMDB and Penn, and our model achieves state-of-the-art performance on both datasets.},
  archive      = {J_PR},
  author       = {Yang Li and Kan Li and Xinxin Wang and Richard Yi Da Xu},
  doi          = {10.1016/j.patcog.2020.107258},
  journal      = {Pattern Recognition},
  pages        = {107258},
  shortjournal = {Pattern Recognition},
  title        = {Exploring temporal consistency for human pose estimation in videos},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simplifying TugGraph using zipping algorithms. <em>PR</em>,
<em>103</em>, 107257. (<a
href="https://doi.org/10.1016/j.patcog.2020.107257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphs are an invaluable modelling tool in many domains, but visualising large graphs in their entirety can be difficult. Hierarchical graph visualisation – recursively clustering a graph’s nodes to view it at a higher level of abstraction – has thus become popular. However, this can hide important information that a user needs to understand a graph’s topology, e.g. nodes’ neighbourhoods. TugGraph addressed this by ‘separating out’ a given node’s neighbours from their hierarchy ancestors to visualise them independently. Its original implementation was straightforward, but copied parts of the hierarchy, making it slow and memory-hungry. An optimised later version, which we refer to as FastTug , avoided this, but at a cost in clarity. Optimising TugGraph without sacrificing clarity is difficult because of the need to keep every hierarchy node connected, a common challenge for graph hierarchy editing algorithms. Recently, this problem has been addressed by ‘zipping’ algorithms, multi-level split/merge algorithms that preserve hierarchy node connectedness and can be built upon for higher-level editing. In this paper, we generalise the original unzipping algorithms to implement SimpleTug , a simple, modular version of TugGraph that is easy to understand and implement, and even faster and more memory-efficient than FastTug . We formally prove its equivalence to FastTug , and show how both can be parallelised. Using our millipede hierarchical image segmentation system, we show experimentally that both the serial and parallel versions of SimpleTug are around 25\% faster than their FastTug counterparts, whilst using considerably less memory. Finally, we discuss the interesting theoretical connections between TugGraph and zipping, and suggest ideas for further work.},
  archive      = {J_PR},
  author       = {S. Golodetz and A. Arnab and I.D. Voiculescu and S.A. Cameron},
  doi          = {10.1016/j.patcog.2020.107257},
  journal      = {Pattern Recognition},
  pages        = {107257},
  shortjournal = {Pattern Recognition},
  title        = {Simplifying TugGraph using zipping algorithms},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hyper-parameter optimization in classification: To-do or
not-to-do. <em>PR</em>, <em>103</em>, 107245. (<a
href="https://doi.org/10.1016/j.patcog.2020.107245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyper-parameter optimization is a process to find suitable hyper-parameters for predictive models. It typically incurs highly demanding computational costs due to the need of the time-consuming model training process to determine the effectiveness of each set of candidate hyper-parameter values. A priori, there is no guarantee that hyper-parameter optimization leads to improved performance. In this work, we propose a framework to address the problem of whether one should apply hyper-parameter optimization or use the default hyper-parameter settings for traditional classification algorithms . We implemented a prototype of the framework, which we use a basis for a three-fold evaluation with 486 datasets and 4 algorithms. The results indicate that our framework is effective at supporting modeling tasks in avoiding adverse effects of using ineffective optimizations. The results also demonstrate that incrementally adding training datasets improves the predictive performance of framework instantiations and hence enables “life-long learning.”},
  archive      = {J_PR},
  author       = {Ngoc Tran and Jean-Guy Schneider and Ingo Weber and A.K. Qin},
  doi          = {10.1016/j.patcog.2020.107245},
  journal      = {Pattern Recognition},
  pages        = {107245},
  shortjournal = {Pattern Recognition},
  title        = {Hyper-parameter optimization in classification: To-do or not-to-do},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online tracking of ants based on deep association metrics:
Method, dataset and evaluation. <em>PR</em>, <em>103</em>, 107233. (<a
href="https://doi.org/10.1016/j.patcog.2020.107233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tracking movement of insects in a social group (such as ants) is challenging, because the individuals are not only similar in appearance but also likely to perform intensive body contact and sudden movement adjustment (start/stop, direction changes). To address this challenge, we introduce an online multi-object tracking framework that combines both the motion and appearance information of ants. We obtain the appearance descriptors by using the ResNet model for offline training on a small (N=50) sample dataset . For online association, a cosine similarity metric computes the matching degree between historical appearance sequences of the trajectory and the current detection. We validate our method in both indoor (lab setup) and outdoor video sequences. The results show that our model obtains 99.3\%  ±  0.5\% MOTA and 91.9\%  ±  2.1\% MOTP across 24,050 testing samples in five indoor sequences, with real-time tracking performance. In an outdoor sequence, we achieve 99.3\% MOTA and 92.9\% MOTP across 22,041 testing samples. The datasets and code are made publicly available for future research in relevant domains.},
  archive      = {J_PR},
  author       = {Xiaoyan Cao and Shihui Guo and Juncong Lin and Wenshu Zhang and Minghong Liao},
  doi          = {10.1016/j.patcog.2020.107233},
  journal      = {Pattern Recognition},
  pages        = {107233},
  shortjournal = {Pattern Recognition},
  title        = {Online tracking of ants based on deep association metrics: Method, dataset and evaluation},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Pedestrian detection in underground mines via parallel
feature transfer network. <em>PR</em>, <em>103</em>, 107195. (<a
href="https://doi.org/10.1016/j.patcog.2020.107195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian detection has been one of the key technologies in computer vision for autonomous driving in underground mines. However, such pedestrian detection is easily affected by complex environmental factors, such as uneven light, dense dust and cable interference. Recently, the problem of pedestrian detection is solved as an object detection task, which has achieved significant advances with the framework of deep neural networks . In this paper, we propose a novel parallel feature transfer network based detector called PftNet that achieves better efficiency than one-stage methods and maintains comparable accuracy of two-stage methods. PftNet consists of two interconnected modules, i.e., the pedestrian identification module and the pedestrian location module. The former aims to roughly adjust the location and size of the anchor box , filter out the negative anchor box , and provide better initialization for the regression. The latter enables PftNet to adapt to different scales and aspect ratios of objects and further improves the regression accuracy. Meanwhile, a feature transfer block compromising gated units is well designed to transmit the pedestrian characteristics between two modules. Extensive experiments on self-annotated underground dataset as well as INRIA and ETH datasets show that PftNet achieves state-of-the-art detection efficiency with high accuracy, which is significant to realizing unmanned driving systems in mines.},
  archive      = {J_PR},
  author       = {Xing Wei and Haitao Zhang and Shaofan Liu and Yang Lu},
  doi          = {10.1016/j.patcog.2020.107195},
  journal      = {Pattern Recognition},
  pages        = {107195},
  shortjournal = {Pattern Recognition},
  title        = {Pedestrian detection in underground mines via parallel feature transfer network},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Circular object arrangement using spherical embeddings.
<em>PR</em>, <em>103</em>, 107192. (<a
href="https://doi.org/10.1016/j.patcog.2019.107192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of recovering a circular arrangement of data instances with respect to some proximity measure , such that nearby instances are more similar. Applications of this problem, also referred to as circular seriation, can be found in various disciplines such as genome sequencing, data visualization and exploratory data analysis. Circular seriation can be expressed as a quadratic assignment problem , which is in general an intractable problem. Spectral-based approaches can be used to find approximate solutions, but are shown to perform well only for a specific class of data matrices. We propose a bilevel optimization framework where we employ a spherical embedding approach together with a spectral method for circular ordering in order to recover circular arrangements of the embedded data. Experiments on real and synthetic datasets demonstrate the competitive performance of the proposed method.},
  archive      = {J_PR},
  author       = {Xenophon Evangelopoulos and Austin J. Brockmeier and Tingting Mu and John Y. Goulermas},
  doi          = {10.1016/j.patcog.2019.107192},
  journal      = {Pattern Recognition},
  pages        = {107192},
  shortjournal = {Pattern Recognition},
  title        = {Circular object arrangement using spherical embeddings},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised learning of optical flow with patch consistency
and occlusion estimation. <em>PR</em>, <em>103</em>, 107191. (<a
href="https://doi.org/10.1016/j.patcog.2019.107191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent works have shown that deep networks can be trained for optical flow estimation without supervision. Based on the photometric constancy assumption , most of these methods adopt the reconstruction loss as the supervision by point-based backward warping. Inspired by the traditional patch matching based approaches, we propose a patch-based consistency to improve the vanilla unsupervised learning method Ren et al. [1]. Instead of only comparing the corresponding pixel intensity , we locate the correspondence by using the image patches with census transform, which is more robust for the illumination variation and occlusion. Moreover, a novel parallel branch is devised to estimate a soft occlusion mask jointly in an unsupervised way. The mask is adopted to weight our patch-based consistency loss to alleviate the influence of the occlusion. The plenty of experiments have been implemented on Flying Chairs, KITTI and MPI-Sintel benchmarks. The results show that our method is efficient and outperforms the peer unsupervised learning methods that are using the FlowNet-liked network.},
  archive      = {J_PR},
  author       = {Zhe Ren and Junchi Yan and Xiaokang Yang and Alan Yuille and Hongyuan Zha},
  doi          = {10.1016/j.patcog.2019.107191},
  journal      = {Pattern Recognition},
  pages        = {107191},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised learning of optical flow with patch consistency and occlusion estimation},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fusion of complex networks and randomized neural networks
for texture analysis. <em>PR</em>, <em>103</em>, 107189. (<a
href="https://doi.org/10.1016/j.patcog.2019.107189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a high discriminative texture analysis method based on the fusion of complex networks and randomized neural networks . In this approach, the input image is modeled as a complex network and its topological properties as well as the image pixels are used to train randomized neural networks to create a signature that represents the deep characteristics of the texture. The results obtained surpassed the accuracy of many methods available in the literature. This performance demonstrates that our proposed approach opens a promising source of research, which consists of exploring the synergy of neural networks and complex networks in the texture analysis field.},
  archive      = {J_PR},
  author       = {Lucas C. Ribas and Jarbas Joaci de Mesquita Sá Junior and Leonardo F. S. Scabini and Odemir M. Bruno},
  doi          = {10.1016/j.patcog.2019.107189},
  journal      = {Pattern Recognition},
  pages        = {107189},
  shortjournal = {Pattern Recognition},
  title        = {Fusion of complex networks and randomized neural networks for texture analysis},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic characteristic-calibrated registration (ACC-REG):
Hippocampal surface registration using eigen-graphs. <em>PR</em>,
<em>103</em>, 107142. (<a
href="https://doi.org/10.1016/j.patcog.2019.107142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an efficient algorithm, the ACC-REG, to automatically extract intrinsic key characteristics on hippocampal mesh surfaces and hence compute an accurate registration mapping between them. Given a pair of hippocampal surface mesh, the proposed algorithm constructs the eigen-graphs, an intrinsic feature on the surface, on each surface as its representative. The eigen-graphs are then calibrated along the longitudinal direction of the hippocampal surfaces. Accurately corresponded intrinsic characteristics on each hippocampus can thus be extracted. As a result, the two surfaces can be registered with improved accuracy and low computation cost. Experiments on ADNI data demonstrate the effectiveness of the proposed ACC-REG model over existing methods.},
  archive      = {J_PR},
  author       = {Hei Long CHAN and Tsz Chun YAM and Lok Ming LUI},
  doi          = {10.1016/j.patcog.2019.107142},
  journal      = {Pattern Recognition},
  pages        = {107142},
  shortjournal = {Pattern Recognition},
  title        = {Automatic characteristic-calibrated registration (ACC-REG): Hippocampal surface registration using eigen-graphs},
  volume       = {103},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Guided CNN for generalized zero-shot and open-set
recognition using visual and semantic prototypes. <em>PR</em>,
<em>102</em>, 107263. (<a
href="https://doi.org/10.1016/j.patcog.2020.107263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the process of exploring the world, the curiosity constantly drives humans to cognize new things. Supposing you are a zoologist, for a presented animal image, you can recognize it immediately if you know its class. Otherwise, you would more likely attempt to cognize it by exploiting the side-information (e.g., semantic information, etc.) you have accumulated. Inspired by this, this paper decomposes the generalized zero-shot learning (G-ZSL) task into an open set recognition (OSR) task and a zero-shot learning (ZSL) task, where OSR recognizes seen classes (if we have seen (or known) them) and rejects unseen classes (if we have never seen (or known) them before), while ZSL identifies the unseen classes rejected by the former. Simultaneously, without violating OSR’s assumptions (only known class knowledge is available in training), we also first attempt to explore a new generalized open set recognition (G-OSR) by introducing the accumulated side-information from known classes to OSR. For G-ZSL, such a decomposition effectively solves the class overfitting problem with easily misclassifying unseen classes as seen classes. The problem is ubiquitous in most existing G-ZSL methods. On the other hand, for G-OSR, introducing such semantic information of known classes not only improves the recognition performance but also endows OSR with the cognitive ability of unknown classes. Specifically, a visual and semantic prototypes-jointly guided convolutional neural network (VSG-CNN) is proposed to fulfill these two tasks (G-ZSL and G-OSR) in a unified end-to-end learning framework. Extensive experiments on benchmark datasets demonstrate the advantages of our learning framework.},
  archive      = {J_PR},
  author       = {Chuanxing Geng and Lue Tao and Songcan Chen},
  doi          = {10.1016/j.patcog.2020.107263},
  journal      = {Pattern Recognition},
  pages        = {107263},
  shortjournal = {Pattern Recognition},
  title        = {Guided CNN for generalized zero-shot and open-set recognition using visual and semantic prototypes},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Radial-based undersampling for imbalanced data
classification. <em>PR</em>, <em>102</em>, 107262. (<a
href="https://doi.org/10.1016/j.patcog.2020.107262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data imbalance remains one of the most widespread problems affecting contemporary machine learning . The negative effect data imbalance can have on the traditional learning algorithms is most severe in combination with other dataset difficulty factors, such as small disjuncts, presence of outliers and insufficient number of training observations. Aforementioned difficulty factors can also limit the applicability of some of the methods of dealing with data imbalance, in particular the neighborhood-based oversampling algorithms based on SMOTE. Radial-Based Oversampling (RBO) was previously proposed to mitigate some of the limitations of the neighborhood-based methods. In this paper we examine the possibility of utilizing the concept of mutual class potential, used to guide the oversampling process in RBO, in the undersampling procedure. Conducted computational complexity analysis indicates a significantly reduced time complexity of the proposed Radial-Based Undersampling algorithm, and the results of the performed experimental study indicate its usefulness, especially on difficult datasets.},
  archive      = {J_PR},
  author       = {Michał Koziarski},
  doi          = {10.1016/j.patcog.2020.107262},
  journal      = {Pattern Recognition},
  pages        = {107262},
  shortjournal = {Pattern Recognition},
  title        = {Radial-based undersampling for imbalanced data classification},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-camera multi-player tracking with deep player
identification in sports video. <em>PR</em>, <em>102</em>, 107260. (<a
href="https://doi.org/10.1016/j.patcog.2020.107260">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identity switches caused by inter-object interactions remain a critical problem for multi-player tracking in real-world sports video analysis. Existing approaches utilizing the appearance model is difficult to associate detections and preserve identities due to the similar appearance of players in the same team. Instead of the appearance model, we propose a distinguishable deep representation for player identity in this paper. A robust multi-player tracker incorporating with deep player identification is further developed to produce identity-coherent trajectories. The framework consists of three parts: (1) the core component, a Deep Player Identification (DeepPlayer) model that provides an adequate discriminative feature through the coarse-to-fine jersey number recognition and the pose-guided partial feature embedding; (2) an Individual Probability Occupancy Map (IPOM) model for players 3D localization with ID; and (3) a K-Shortest Path with ID (KSP-ID) model that links nodes in the flow graph by a proposed player ID correlation coefficient. With the distinguishable identity, the performance of tracking is improved. Experiment results illustrate that our framework handles the identity switches effectively, and outperforms state-of-the-art trackers on the sports video benchmarks.},
  archive      = {J_PR},
  author       = {Ruiheng Zhang and Lingxiang Wu and Yukun Yang and Wanneng Wu and Yueqiang Chen and Min Xu},
  doi          = {10.1016/j.patcog.2020.107260},
  journal      = {Pattern Recognition},
  pages        = {107260},
  shortjournal = {Pattern Recognition},
  title        = {Multi-camera multi-player tracking with deep player identification in sports video},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semantic-aware scene recognition. <em>PR</em>, <em>102</em>,
107256. (<a href="https://doi.org/10.1016/j.patcog.2020.107256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene recognition is currently one of the top-challenging research fields in computer vision . This may be due to the ambiguity between classes: images of several scene classes may share similar objects, which causes confusion among them. The problem is aggravated when images of a particular scene class are notably different. Convolutional Neural Networks (CNNs) have significantly boosted performance in scene recognition, albeit it is still far below from other recognition tasks (e.g., object or image recognition). In this paper, we describe a novel approach for scene recognition based on an end-to-end multi-modal CNN that combines image and context information by means of an attention module. Context information, in the shape of a semantic segmentation , is used to gate features extracted from the RGB image by leveraging on information encoded in the semantic representation : the set of scene objects and stuff, and their relative locations. This gating process reinforces the learning of indicative scene content and enhances scene disambiguation by refocusing the receptive fields of the CNN towards them. Experimental results on three publicly available datasets show that the proposed approach outperforms every other state-of-the-art method while significantly reducing the number of network parameters. All the code and data used along this paper is available at: https://github.com/vpulab/Semantic-Aware-Scene-Recognition},
  archive      = {J_PR},
  author       = {Alejandro López-Cifuentes and Marcos Escudero-Viñolo and Jesús Bescós and Álvaro García-Martín},
  doi          = {10.1016/j.patcog.2020.107256},
  journal      = {Pattern Recognition},
  pages        = {107256},
  shortjournal = {Pattern Recognition},
  title        = {Semantic-aware scene recognition},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel image-dehazing network with a parallel attention
block. <em>PR</em>, <em>102</em>, 107255. (<a
href="https://doi.org/10.1016/j.patcog.2020.107255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image dehazing is a very important pre-processing step to many computer vision tasks such as object recognition and tracking. However, it is a challenging problem because the physical parameters of imaging, e.g. the depth information of scene pixels and the attenuation model, are usually unknown. Based on a physical model, different methods have been proposed to recover these parameters. Existing convolutional neural networks (CNNs) based methods try to solve the image dehazing problem using an end-to-end network to learn a direct mapping between a hazy image and its corresponding clear image. But the representational ability, spatial variant ability and dehazing capability of these network models are hindered by treating all the spatial and channel-wise features indiscriminately. Hence, we propose an end-to-end dehazing network with a parallel spatial/channel-wise attention block for capturing more informative spatial and channel-wise features respectively. Specifically, based on the encoder-decoder framework with a pyramid pooling operation, a novel parallel spatial/channel-wise attention block is proposed and applied to the end of the encoder for guiding the decoder to reconstruct better clear images. In the spatial/channel-wise attention block, the spatial attention module and the channel-wise attention module are connected in parallel, where the spatial attention module highlights important spatial positions of features. Meanwhile, the channel-wise module exploits inter-dependencies among the channel-wise features. Extensive experiments demonstrate that our network with a parallel spatial /channel-wise attention block can achieve better accuracy and visual results over state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Shibai Yin and Yibin Wang and Yee-Hong Yang},
  doi          = {10.1016/j.patcog.2020.107255},
  journal      = {Pattern Recognition},
  pages        = {107255},
  shortjournal = {Pattern Recognition},
  title        = {A novel image-dehazing network with a parallel attention block},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sparse filtered SIRT for electron tomography. <em>PR</em>,
<em>102</em>, 107253. (<a
href="https://doi.org/10.1016/j.patcog.2020.107253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electron tomographic reconstruction is a method for obtaining a three-dimensional image of a specimen with a series of two dimensional microscope images taken from different viewing angles. Filtered backprojection, one of the most popular tomographic reconstruction methods, does not work well under the existence of image noises and missing wedges. This paper presents a new approach to largely mitigate the effect of noises and missing wedges. We propose a novel filtered backprojection that optimizes the filter of the backprojection operator in terms of a reconstruction error. This data-dependent filter adaptively chooses the spectral domains of signals and noises, suppressing the noise frequency bands, so it is very effective in denoising . We also propose the new filtered backprojection embedded within the simultaneous iterative reconstruction iteration for mitigating the effect of missing wedges. Our numerical study is presented to show the performance gain of the proposed approach over the state-of-the-art.},
  archive      = {J_PR},
  author       = {Chen Mu and Chiwoo Park},
  doi          = {10.1016/j.patcog.2020.107253},
  journal      = {Pattern Recognition},
  pages        = {107253},
  shortjournal = {Pattern Recognition},
  title        = {Sparse filtered SIRT for electron tomography},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Smooth robust tensor principal component analysis for
compressed sensing of dynamic MRI. <em>PR</em>, <em>102</em>, 107252.
(<a href="https://doi.org/10.1016/j.patcog.2020.107252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic magnetic resonance imaging (DMRI) often requires a long time for measurement acquisition, and it is a crucial problem about the enhancement of reconstruction quality from a limited set of under-samples. The low-rank plus sparse decomposition model , which is also called robust principal component analysis (RPCA), is widely used for reconstruction of DMRI data in the model-based way. In this paper, considering that DMRI data are naturally in tensor form with block-wise smoothness, we propose a smooth robust tensor principal component analysis (SRTPCA) method for DMRI reconstruction. Compared with classical RPCA approaches, the low rank and sparsity terms are extended to tensor versions to fully exploit the spatial and temporal data structures . Moreover, a tensor total variation regularization term is used to encourage the multi-dimensional block-wise smoothness for the reconstructed DMRI data. The relaxed convex optimization model can be divided into several sub-problems by the alternating direction method of multipliers . Numerical experiments on cardiac perfusion and cine datasets demonstrate that the proposed SRTPCA method outperforms the state-of-the-art ones in terms of recovery accuracy.},
  archive      = {J_PR},
  author       = {Yipeng Liu and Tengteng Liu and Jiani Liu and Ce Zhu},
  doi          = {10.1016/j.patcog.2020.107252},
  journal      = {Pattern Recognition},
  pages        = {107252},
  shortjournal = {Pattern Recognition},
  title        = {Smooth robust tensor principal component analysis for compressed sensing of dynamic MRI},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Threshold optimization for f measure of macro-averaged
precision and recall. <em>PR</em>, <em>102</em>, 107250. (<a
href="https://doi.org/10.1016/j.patcog.2020.107250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are two different approaches to macro-averaging F measure for multi-label classification. The first encloses averaging F measure over all classes, which makes it easy to optimize. The second, extensively investigated in this paper, comprises the F measure of macro precision and recall calculation. We examine and compare these two measures when applied to different multi-label datasets. To optimize the performance measure , we adopt a widely known and proven modular approach. Classifiers sort the instances in descending order, according to a real-valued score of belonging to a corresponding class. After that, thresholds are selected so as to optimize the performance measure . If the number of classes is sufficiently large and the second alternative of macro-averaging F measure is employed, then it becomes non-trivial to define the optimal number of instances to assign to each class. Cyclic optimization procedure is widely used for threshold optimization although it results in a maximum in a special coordinate-wise sense. For a micro averaged F measure, such a coordinate-wise optimum is a maximum in the conventional sense of this term but it is not true for the F measure of macro precision and recall, which is shown by a counterexample . We reduce the problem of selecting the optimal threshold for each class to the problem of obtaining a fixed point of a specifically introduced transformation of a unit square. The suggested algorithm lets us localize all possible coordinate-wise maximums and detect the optimal among them. The approach is applied to datasets from diverse application domains.},
  archive      = {J_PR},
  author       = {Anna Berger and Sergey Guda},
  doi          = {10.1016/j.patcog.2020.107250},
  journal      = {Pattern Recognition},
  pages        = {107250},
  shortjournal = {Pattern Recognition},
  title        = {Threshold optimization for f measure of macro-averaged precision and recall},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Identity-aware CycleGAN for face photo-sketch synthesis and
recognition. <em>PR</em>, <em>102</em>, 107249. (<a
href="https://doi.org/10.1016/j.patcog.2020.107249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face photo-sketch synthesis and recognition has many applications in digital entertainment and law enforcement. Recently, generative adversarial networks (GANs) based methods have significantly improved the quality of image synthesis , but they have not explicitly considered the purpose of recognition. In this paper, we first propose an Identity-Aware CycleGAN (IACycleGAN) model that applies a new perceptual loss to supervise the image generation network. It improves CycleGAN on photo-sketch synthesis by paying more attention to the synthesis of key facial regions, such as eyes and nose, which are important for identity recognition. Furthermore, we develop a mutual optimization procedure between the synthesis model and the recognition model, which iteratively synthesizes better images by IACycleGAN and enhances the recognition model by the triplet loss of the generated and real samples. Extensive experiments are performed on both photo-to-sketch and sketch-to-photo tasks using the widely used CUFS and CUFSF databases. The results show that the proposed method performs better than several state-of-the-art methods in terms of both synthetic image quality and photo-sketch recognition accuracy.},
  archive      = {J_PR},
  author       = {Yuke Fang and Weihong Deng and Junping Du and Jiani Hu},
  doi          = {10.1016/j.patcog.2020.107249},
  journal      = {Pattern Recognition},
  pages        = {107249},
  shortjournal = {Pattern Recognition},
  title        = {Identity-aware CycleGAN for face photo-sketch synthesis and recognition},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Long video question answering: A matching-guided attention
model. <em>PR</em>, <em>102</em>, 107248. (<a
href="https://doi.org/10.1016/j.patcog.2020.107248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing video question answering methods answer given questions based on short video snippets. The underlying assumption is that the visual content indicating the ground truth answer ubiquitously exists in the snippet. It might be problematic for long video applications, since involving large numbers of answer-irrelevant snippets will dramatically degenerate the performance. To deal with this issue, we focus on a rarely investigated but practically important problem, namely long video QA, by predicting answers directly from long videos rather than manually pre-extracted short video snippets. We accordingly propose a Matching-guided Attention Model (MAM) which jointly extracts question-related video snippets and predicts answers in a unified framework. To localize questions accurately and efficiently, we calculate corresponding matching scores and boundary regression results for candidate video snippet proposals generated by sliding windows of limited granularity . Guided by the matching scores, the model pays different attention to the extracted video snippet proposals for each question. Finally, we use the attended visual features along with the question to predict the answer in a classification manner. A key obstacle to training our model is that publicly available video QA datasets only contain short videos especially designed for short video QA. Thus, we generate two new datasets for this task on the top of TACoS Multi-level dataset and MSR-VTT dataset by generating QA pairs from the video captions, called TACoS-QA and MSR-VTT-QA . Experimental results show the effectiveness of our proposed method on both datasets by comparing with two short video QA methods and a baseline method .},
  archive      = {J_PR},
  author       = {Weining Wang and Yan Huang and Liang Wang},
  doi          = {10.1016/j.patcog.2020.107248},
  journal      = {Pattern Recognition},
  pages        = {107248},
  shortjournal = {Pattern Recognition},
  title        = {Long video question answering: A matching-guided attention model},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep and joint learning of longitudinal data for alzheimer’s
disease prediction. <em>PR</em>, <em>102</em>, 107247. (<a
href="https://doi.org/10.1016/j.patcog.2020.107247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer&#39;s disease (AD) is an irreversible and progressive neurodegenerative disease. The close AD monitoring of this disease is essential for the patient treatment plan adjustment. For AD monitoring, clinical score prediction via neuroimaging data is highly desirable since it is able to reveal the disease status, adequately. For this task, most previous studies are focused on a single time point without considering relationship between neuroimaging data (e.g., Magnetic Resonance Imaging (MRI)) and clinical scores at multiple time points. Differing from these studies, we propose to build a framework based on longitudinal multiple time points data to predict clinical scores. Specifically, the proposed framework consists of three parts, feature selection based on correntropy regularized joint learning, feature encoding based on deep polynomial network, and ensemble learning for regression via the support vector regression method. Two scenarios are designed for scores prediction. Namely, scenario 1 uses the baseline data to achieve the longitudinal scores prediction, while scenario 2 utilizes all the previous time points data to obtain the predicted scores at the next time point, which can improve the score prediction&#39;s accuracy. Meanwhile, the missing clinical scores at longitudinal multiple time points are imputated to solve the incompleteness of the data. Extensive experiments on the public database of Alzheimer&#39;s Disease Neuroimaging Initiative (ADNI) demonstrate that our proposed framework can effectively reveal the relationship between clinical score and MRI data and outperforms the state-of-the-art methods in scores prediction.},
  archive      = {J_PR},
  author       = {Baiying Lei and Mengya Yang and Peng Yang and Feng Zhou and Wen Hou and Wenbin Zou and Xia Li and Tianfu Wang and Xiaohua Xiao and Shuqiang Wang},
  doi          = {10.1016/j.patcog.2020.107247},
  journal      = {Pattern Recognition},
  pages        = {107247},
  shortjournal = {Pattern Recognition},
  title        = {Deep and joint learning of longitudinal data for alzheimer&#39;s disease prediction},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep morphological networks. <em>PR</em>, <em>102</em>,
107246. (<a href="https://doi.org/10.1016/j.patcog.2020.107246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mathematical morphology provides powerful nonlinear operators for a variety of image processing tasks such as filtering, segmentation, and edge detection. In this paper, we propose a way to use these nonlinear operators in an end-to-end deep learning framework and illustrate them on different applications. We demonstrate on various examples that new layers making use of the morphological non-linearities are complementary to convolution layers . These new layers can be used to integrate the non-linear operations and pooling into a joint operation. We finally enhance results obtained in boundary detection using this new family of layers with just 0.01\% of the parameters of competing state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Gianni Franchi and Amin Fehri and Angela Yao},
  doi          = {10.1016/j.patcog.2020.107246},
  journal      = {Pattern Recognition},
  pages        = {107246},
  shortjournal = {Pattern Recognition},
  title        = {Deep morphological networks},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DeepCT: A novel deep complex-valued network with learnable
transform for video saliency prediction. <em>PR</em>, <em>102</em>,
107234. (<a href="https://doi.org/10.1016/j.patcog.2020.107234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The past decade has witnessed the success of transformed domain methods for image saliency prediction. However, it is intractable to develop a transformed domain method for video saliency prediction, due to the limited choices on spatio-temporal transforms. In this paper, we propose learning the transform from training data, rather than the predefined transform in the existing methods. Specifically, we develop a novel deep Complex-valued network with learnable Transform (DeepCT) for video saliency prediction. The architecture of DeepCT includes the Complex-valued Transform Module (CTM), inverse CTM (iCTM) and Complex-valued Stacked Convolutional Long Short-Term Memory network (CS-ConvLSTM). In the CTM and iCTM, multi-scale pyramid structures are introduced, as we find that transforms at multiple receptive scales can improve the accuracy of saliency prediction. To make the CTM and iCTM “invertible”, we further propose the cycle consistency loss in training DeepCT, which is composed of frame reconstruction loss and complex feature reconstruction loss. Additionally, the CS-ConvLSTM is developed to learn the temporal saliency transition across video frames. Finally, the experimental results show that our DeepCT method outperforms other 13 state-of-the-art methods for video saliency prediction.},
  archive      = {J_PR},
  author       = {Lai Jiang and Mai Xu and Shanyi Zhang and Leonid Sigal},
  doi          = {10.1016/j.patcog.2020.107234},
  journal      = {Pattern Recognition},
  pages        = {107234},
  shortjournal = {Pattern Recognition},
  title        = {DeepCT: A novel deep complex-valued network with learnable transform for video saliency prediction},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Uncertain motion tracking based on convolutional net with
semantics estimation and region proposals. <em>PR</em>, <em>102</em>,
107232. (<a href="https://doi.org/10.1016/j.patcog.2020.107232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In real world, most of the tracking methods suffer from uncertain motion, which may make the tracker failure because of a local search window with the motion smooth assumption. To address this problem, a novel tracking framework based on convolutional net with semantics estimation and region proposals is proposed. Firstly, we present a semantics object proposals generation strategy, including category-level semantics proposals, one-object-level semantics estimation and semantics-contextual proposals generation, to obtain a few of high-quality object-oriented proposals covering uncertain motion. Secondly, combining the globally sparse semantics region proposals prediction and correlation filter prediction, a hybrid semantics tracking algorithm is proposed, which obtains a coarse object location by the decision of multiple response maps. Finally, we learn and train independent correlation filter to estimate the scale of target for a higher tracking accuracy. Extensive experiments on two visual tracking benchmarks and results demonstrate our method achieves state-of-the-art performance.},
  archive      = {J_PR},
  author       = {Huanlong Zhang and Jian Chen and Guohao Nie and Shiqiang Hu},
  doi          = {10.1016/j.patcog.2020.107232},
  journal      = {Pattern Recognition},
  pages        = {107232},
  shortjournal = {Pattern Recognition},
  title        = {Uncertain motion tracking based on convolutional net with semantics estimation and region proposals},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Synthesizing talking faces from text and audio: An
autoencoder and sequence-to-sequence convolutional neural network.
<em>PR</em>, <em>102</em>, 107231. (<a
href="https://doi.org/10.1016/j.patcog.2020.107231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthesizing talking face from text and audio is increasingly becoming a direction in human-machine and face-to-face interactions. Although progress has been made, several existing methods either have unsatisfactory co-articulation modeling effects or ignore relations between adjacent inputs. Moreover, some of these methods often train models on shaky head videos or utilize linear-based face parameterization strategies, which further decrease synthesized quality. To address the above issues, this study proposes a sequence-to-sequence convolutional neural network to automatically synthesize talking face video with accurate lip sync. First, an advanced landmark location pipeline is used to accurately locate the facial landmarks, which can effectively reduce landmark shake. Then, a part-based autoencoder is presented to encode face images into a low-dimensional space and obtain compact representations . A sequence-to-sequence network is also presented to encode the relation of neighboring frames with multiple loss functions, and talking faces are synthesized through a reconstruction strategy with a decoder. Experiments on two public audio-visual datasets and a new dataset called CCTV news demonstrate the effectiveness of the proposed method against other state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Na Liu and Tao Zhou and Yunfeng Ji and Ziyi Zhao and Lihong Wan},
  doi          = {10.1016/j.patcog.2020.107231},
  journal      = {Pattern Recognition},
  pages        = {107231},
  shortjournal = {Pattern Recognition},
  title        = {Synthesizing talking faces from text and audio: An autoencoder and sequence-to-sequence convolutional neural network},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A quadrilateral scene text detector with two-stage network
architecture. <em>PR</em>, <em>102</em>, 107230. (<a
href="https://doi.org/10.1016/j.patcog.2020.107230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many of the state-of-the-art methods can only localize scene texts with rotated rectangle boundaries, which may result in incorrect rectification of the detected scene texts and erroneous elimination of proposals or detections during non-maximum suppression (NMS). A few existing methods that can detect scene texts with quadrilateral boundaries, are just based on one-stage architectures or sliding windows scanning and thus have sub-optimal performance. To address these problems, we propose an end-to-end two-stage network architecture for scene text detection, which can accurately localize scene texts with quadrilateral boundaries. At the first stage, we propose a quadrilateral region proposal network (QRPN) for generating quadrilateral proposals, based on a newly proposed quadrilateral regression algorithm. At the second stage, we introduce a novel weighted RoI pooling module with learned weight masks to pool the features, and then classify the proposals and refine their shapes with the proposed quadrilateral regression algorithm again. Specially, during training, we adopt a dual-branch structure of detection heads, that is, jointly train the quadrilateral detection head and an additional rotated rectangle detection head. Furthermore, we develop an accelerated NMS algorithm with O( nlogn ) complexity, for redundant quadrilateral text proposals and detections eliminating during the first and the second stage, respectively. Experiments on several challenging benchmarks demonstrate the superior performance of the proposed method, which achieves state-of-the-art results on widely used benchmarks ICDAR 2017 MLT, RCTW, and ICDAR 2015 Incidental Scene Text benchmark.},
  archive      = {J_PR},
  author       = {Siwei Wang and Yudong Liu and Zheqi He and Yongtao Wang and Zhi Tang},
  doi          = {10.1016/j.patcog.2020.107230},
  journal      = {Pattern Recognition},
  pages        = {107230},
  shortjournal = {Pattern Recognition},
  title        = {A quadrilateral scene text detector with two-stage network architecture},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online signature verification using single-template matching
with time-series averaging and gradient boosting. <em>PR</em>,
<em>102</em>, 107227. (<a
href="https://doi.org/10.1016/j.patcog.2020.107227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In keeping with recent developments in artificial intelligence in the era of big data, there is a demand for online signature verification systems that operate at high speeds, provide a high level of security, and allow high tolerances while achieving sufficient performance. In response to these needs, the present study proposes a novel, single-template strategy using a mean template set and weighted multiple dynamic time warping (DTW) distances for a function-based approach to online signature verification. Specifically, to obtain an effective mean template for each feature while reflecting intra-user variability between all the reference samples, we adopt a novel time-series averaging method based on Euclidean barycenter-based DTW barycenter averaging. Then, by using the mean template set, we calculate multiple DTW distances from multivariate time series based on dependent and independent warping. Finally, to boost the discriminative power , we apply a weighting scheme using a gradient boosting model to efficiently combine the multiple DTW distances. Experimental results using the common SVC2004 Task1/Task2 and MCYT-100 signature datasets confirm that the proposed method is effective for online signature verification.},
  archive      = {J_PR},
  author       = {Manabu Okawa},
  doi          = {10.1016/j.patcog.2020.107227},
  journal      = {Pattern Recognition},
  pages        = {107227},
  shortjournal = {Pattern Recognition},
  title        = {Online signature verification using single-template matching with time-series averaging and gradient boosting},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MIMN-DPP: Maximum-information and minimum-noise
determinantal point processes for unsupervised hyperspectral band
selection. <em>PR</em>, <em>102</em>, 107213. (<a
href="https://doi.org/10.1016/j.patcog.2020.107213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Band selection plays an important role in hyperspectral imaging for reducing the data and improving the efficiency of data acquisition and analysis whilst significantly lowering the cost of the imaging system. Without the category labels, it is challenging to select an effective and low-redundancy band subset. In this paper, a new unsupervised band selection algorithm is proposed based on a new band search criterion and an improved Determinantal Point Processes (DPP). First, to preserve the original information of hyperspectral image , a novel band search criterion is designed for searching the bands with high information entropy and low noise. Unfortunately, finding the optimal solution based on the search criteria to select a low-redundancy band subset is a NP-hard problem. To solve this problem, we consider the correlation of bands from both original hyperspectral image and its spatial information to construct a double-graph model to describe the relationship between spectral bands. Besides, an improved DPP algorithm is proposed for the approximate search of a low-redundancy band subset from the double-graph model. Experiment results on several well-known datasets show that the proposed optical band selection algorithm achieves better performance than many other state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Chen Weizhao and Yang Zhijing and Jinchang Ren and Cao Jiangzhong and Cai Nian and Huimin Zhao and Yuen Peter},
  doi          = {10.1016/j.patcog.2020.107213},
  journal      = {Pattern Recognition},
  pages        = {107213},
  shortjournal = {Pattern Recognition},
  title        = {MIMN-DPP: Maximum-information and minimum-noise determinantal point processes for unsupervised hyperspectral band selection},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-scale differential feature for ECG biometrics with
collective matrix factorization. <em>PR</em>, <em>102</em>, 107211. (<a
href="https://doi.org/10.1016/j.patcog.2020.107211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electrocardiogram (ECG) biometrics has recently received considerable attention and is considered to be a promising biometric trait . Although some promising results on ECG biometrics have been reported, it is still challenging to perform this technique robustly and precisely. To address these issues, this paper presents a novel ECG biometrics framework: Multi-Scale Differential Feature for ECG biometrics with Collective Matrix Factorization (CMF). First, we extract the Multi-Scale Differential Feature (MSDF) from the one-dimensional ECG signal and then fuse MSDF with 1DMRLBP to generate the MSDF-1DMRLBP, which acts as the base feature of the ECG signal. Second, to extract discriminative information from the intermediate base features, we leverage the CMF technique to generate the final robust ECG representations by simultaneously embedding MSDF-1DMRLBP and label information. Consequently, the final robust features could preserve the intra-subject and inter-subject similarities. Extensive experiments are conducted on four ECG databases, and the results demonstrate that the proposed method can outperform the state-of-the-art in terms of both accuracy and efficiency.},
  archive      = {J_PR},
  author       = {Kuikui Wang and Gongping Yang and Yuwen Huang and Yilong Yin},
  doi          = {10.1016/j.patcog.2020.107211},
  journal      = {Pattern Recognition},
  pages        = {107211},
  shortjournal = {Pattern Recognition},
  title        = {Multi-scale differential feature for ECG biometrics with collective matrix factorization},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Invariant subspace learning for time series data based on
dynamic time warping distance. <em>PR</em>, <em>102</em>, 107210. (<a
href="https://doi.org/10.1016/j.patcog.2020.107210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-dimensional and compact representation of time series data is of importance for mining and storage. In practice, time series data are vulnerable to various temporal transformations, such as shift and temporal scaling, however, which are unavoidable in the process of data collection. If a learning algorithm directly calculates the difference between such transformed data based on Euclidean distance , the measurement cannot faithfully reflect the similarity and hence could not learn the underlying discriminative features . In order to solve this problem, we develop a novel subspace learning algorithm based on dynamic time warping (DTW) distance which is an elastic distance defined in a DTW space. The algorithm aims to minimize the reconstruction error in the DTW space. However, since DTW space is a semi-pseudo metric space, it is difficult to generalize common subspace learning algorithms for such semi-pseudo metric spaces. In this work, we introduce warp operators with which DTW reconstruction error can be approximated by reconstruction error between transformed series and their reconstructions in a subspace. The warp operators align time series data with their linear representations in the DTW space, which is in particular important for misaligned time series, so that the subspace can be learned to obtain an intrinsic basis (dictionary) for the representation of the data. The warp operators and the subspace are optimized alternatively until reaching equilibrium. Experiments results show that the proposed algorithm outperforms traditional subspace learning algorithms and temporal transform-invariance based methods (including SIDL, Kernel PCA, and SPMC et. al), and obtains competitive results with the state-of-the-art algorithms, such as BOSS algorithm.},
  archive      = {J_PR},
  author       = {Huiqi Deng and Weifu Chen and Qi Shen and Andy J. Ma and Pong C. Yuen and Guocan Feng},
  doi          = {10.1016/j.patcog.2020.107210},
  journal      = {Pattern Recognition},
  pages        = {107210},
  shortjournal = {Pattern Recognition},
  title        = {Invariant subspace learning for time series data based on dynamic time warping distance},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Abnormality detection in retinal image by individualized
background learning. <em>PR</em>, <em>102</em>, 107209. (<a
href="https://doi.org/10.1016/j.patcog.2020.107209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computer-aided lesion detection (CAD) techniques, which provide potential for automatic early screening of retinal pathologies, are widely studied in retinal image analysis. While many CAD approaches based on lesion samples or lesion features can well detect pre-defined lesion types, it remains challenging to detect various abnormal regions (namely abnormalities) from retinal images. In this paper, we try to identify diverse abnormalities from a retinal test image by finely learning its individualized retinal background (IRB) on which retinal lesions superimpose. 3150 normal retinal images are collected as the priors for IRB learning. A preprocessing step is applied to all retinal images for spatial, scale and color normalization. Retinal blood vessels, which have individual variations in different images, are particularly suppressed from all images. A multi-scale sparse coding based learning (MSSCL) algorithm and a repeated learning strategy are proposed for finely learning the IRB. By the MSSCL algorithm, a background space is constructed by sparsely encoding the test image in a multi-scale manner using the dictionary learned from normal retinal images, which will contain more complete IRB information than any single-scale coding result. From the background space, the IRB can be well learned by low-rank approximation and thus different salient lesions can be separated and detected. The MSSCL algorithm will be iteratively repeated on the modified test image in which the detected salient lesions are suppressed, so as to further improve the accuracy of the IRB and suppress lesions in the IRB. Consequently, a high-accuracy IRB can be learned and thus both salient lesions and weak lesions that have low contrasts with the background can be clearly separated. The effectiveness and contributions of the proposed method are validated by experiments over different clinical data-sets and comparisons with the state-of-the-art CAD methods.},
  archive      = {J_PR},
  author       = {Benzhi Chen and Lisheng Wang and Xiuying Wang and Jian Sun and Yijie Huang and Dagan Feng and Zongben Xu},
  doi          = {10.1016/j.patcog.2020.107209},
  journal      = {Pattern Recognition},
  pages        = {107209},
  shortjournal = {Pattern Recognition},
  title        = {Abnormality detection in retinal image by individualized background learning},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fingerprint pore matching using deep features. <em>PR</em>,
<em>102</em>, 107208. (<a
href="https://doi.org/10.1016/j.patcog.2020.107208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a popular living fingerprint feature, sweat pore has been adopted to build robust high resolution automated fingerprint recognition systems (AFRSs). Pore matching is an important step in high resolution fingerprint recognition. This paper proposes a novel pore matching method with high recognition accuracy. The method mainly solves the pore representation problem in the state-of-the-art direct pore matching method. By making full use of the diversity and large quantities of sweat pores on fingerprints, deep convolutional networks are carefully designed to learn a deep feature (denoted as DeepPoreID) for each pore. The inter-class difference and intra-class similarity of pore patch pairs can be well solved using deep learning. The DeepPoreID is then used to describe the local feature for each pore and finally integrated into the classical direct pore matching method. More specifically, pore patches, which are cropped from both Query and Template fingerprint images, are imported into the well-trained networks to generate DeepPoreID for pore representation. The similarity between those DeepPoreIDs are then obtained by calculating the Euclidian Distance between them. Subsequently, one-to-many coarse pore correspondences are established via comparing their similarity. Finally, classical Weighted RANdom SAmple Consensus (WRANSAC) is employed to pick true pore correspondences from coarse ones. The experiments carried on the two public high resolution fingerprint database have shown the effectiveness of the proposed DeepPoreID, especially for fingerprint matching with small image size. Meanwhile, better recognition accuracy is achieved by the proposed method when compared with the existing state-of-the-art methods. About 35\% rise in equal error rate (EER) and about 30\% rise in FMR1000 when compared with the best result evaluated on the database with image size of 320 × 240 pixels.},
  archive      = {J_PR},
  author       = {Feng Liu and Yuanhao Zhao and Guojie Liu and Linlin Shen},
  doi          = {10.1016/j.patcog.2020.107208},
  journal      = {Pattern Recognition},
  pages        = {107208},
  shortjournal = {Pattern Recognition},
  title        = {Fingerprint pore matching using deep features},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Auto-weighted multi-view co-clustering via fast matrix
factorization. <em>PR</em>, <em>102</em>, 107207. (<a
href="https://doi.org/10.1016/j.patcog.2020.107207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering is a hot research topic in machine learning and pattern recognition, however, it remains high computational complexity when clustering multi-view data sets. Although a number of approaches have been proposed to accelerate the computational efficiency, most of them do not consider the data duality between features and samples. In this paper, we propose a novel co-clustering approach termed as Fast Multi-view Bilateral K-means (FMVBKM), which can implement clustering task on row and column of the input data matrix, simultaneously. Specifically, FMVBKM applies the relaxed K-means clustering technique to multi-view data clustering . In addition, to decrease information loss in matrix factorization , we further introduce a new co-clustering method named as Fast Multi-view Matrix Tri-Factorization (FMVMTF). Extensive experimental results on six benchmark data sets show that the proposed two approaches not only have comparable clustering performance but also present the high computational efficiency, in comparison with state-of-the-art multi-view clustering methods.},
  archive      = {J_PR},
  author       = {Feiping Nie and Shaojun Shi and Xuelong Li},
  doi          = {10.1016/j.patcog.2020.107207},
  journal      = {Pattern Recognition},
  pages        = {107207},
  shortjournal = {Pattern Recognition},
  title        = {Auto-weighted multi-view co-clustering via fast matrix factorization},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). A novel density-based clustering algorithm using nearest
neighbor graph. <em>PR</em>, <em>102</em>, 107206. (<a
href="https://doi.org/10.1016/j.patcog.2020.107206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Density-based clustering has several desirable properties, such as the abilities to handle and identify noise samples, discover clusters of arbitrary shapes, and automatically discover of the number of clusters. Identifying the core samples within the dense regions of a dataset is a significant step of the density-based clustering algorithm . Unlike many other algorithms that estimate the density of each samples using different kinds of density estimators and then choose core samples based on a threshold, in this paper, we present a novel approach for identifying local high-density samples utilizing the inherent properties of the nearest neighbor graph (NNG). After using the density estimator to filter noise samples, the proposed algorithm ADBSCAN in which “A” stands for “Adaptive” performs a DBSCAN-like clustering process. The experimental results on artificial and real-world datasets have demonstrated the significant performance improvement over existing density-based clustering algorithms.},
  archive      = {J_PR},
  author       = {Hao Li and Xiaojie Liu and Tao Li and Rundong Gan},
  doi          = {10.1016/j.patcog.2020.107206},
  journal      = {Pattern Recognition},
  pages        = {107206},
  shortjournal = {Pattern Recognition},
  title        = {A novel density-based clustering algorithm using nearest neighbor graph},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scene recognition: A comprehensive survey. <em>PR</em>,
<em>102</em>, 107205. (<a
href="https://doi.org/10.1016/j.patcog.2020.107205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the success of deep learning in the field of computer vision, object recognition has made important breakthroughs, and its recognition accuracy has been drastically improved. However, the performance of scene recognition is still not sufficient to some extent because of complex configurations. Over the past several years, scene recognition algorithms have undergone important evolution as a result of the development of machine learning and Deep Convolutional Neural Networks (DCNN). This paper reviews many of the most popular and effective approaches to scene recognition, which is expected to create benefits for future research and practical applications. We seek to establish relationships among different algorithms and determine the critical components that lead to remarkable performance. Through the analysis of some representative schemes, motivation and insights are identified, which will help to facilitate the design of better recognition architectures. In addition, current available scene datasets and benchmarks are presented for evaluation and comparison. Finally, potential problems and promising directions are highlighted.},
  archive      = {J_PR},
  author       = {Lin Xie and Feifei Lee and Li Liu and Koji Kotani and Qiu Chen},
  doi          = {10.1016/j.patcog.2020.107205},
  journal      = {Pattern Recognition},
  pages        = {107205},
  shortjournal = {Pattern Recognition},
  title        = {Scene recognition: A comprehensive survey},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image decomposition based matrix regression with
applications to robust face recognition. <em>PR</em>, <em>102</em>,
107204. (<a href="https://doi.org/10.1016/j.patcog.2020.107204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The previous matrix regression based methods mainly focus on designing a robust error term to characterize the occlusion and illumination changes. In actually, it is very challenging to give a strong model for solving the original images directly since the images contains rich and complex structure information. To address this problem, we aim to simplify the complex images and propose a simple and robust matrix regression based classification model. In our method, we firstly employ the local gradient distribution to decompose the image into a series of gradient images (LID for short). Each gradient image reveals the local structure information in different gradient orientations. Subsequently, we consider each gradient image as the diagonal block element and construct the diagonal block matrix for image representation. Nuclear norm based matrix regression model (NMR) is then applied to complete the classification tasks. The proposed model can be called ID-NMR for short. We further design a fast ADMM optimization algorithm to solve the proposed ID-NMR due to the fact that the big diagonal block matrix will increase the computational load. Experimental results show that the proposed method performs favorably compared with state-of-the-art regression based classification methods.},
  archive      = {J_PR},
  author       = {Jianjun Qian and Jian Yang and Yong Xu and Jin Xie and Zhihui Lai and Bob Zhang},
  doi          = {10.1016/j.patcog.2020.107204},
  journal      = {Pattern Recognition},
  pages        = {107204},
  shortjournal = {Pattern Recognition},
  title        = {Image decomposition based matrix regression with applications to robust face recognition},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). HscoreNet: A deep network for estrogen and progesterone
scoring using breast IHC images. <em>PR</em>, <em>102</em>, 107200. (<a
href="https://doi.org/10.1016/j.patcog.2020.107200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estrogen and progesterone receptors serve as an important predictive and prognostic biomarkers for breast cancer immunohistological analysis. For breast cancer prognosis, pathologists manually compute the score based on the visual expression and the number of immunopositive and immunonegative nuclei. This manual scoring technique is time-consuming, cumbersome, expensive, error-prone, and susceptible to intra- and interobserver ambiguities. To solve these issues, we proposed a deep neural network (i.e., HscoreNet ), which consists of three parts, i.e., encoder, decoder, and scoring layer. A total of 600 (300 ER and 300 PR) regions of interest at 40 ×  magnification from 100 histologically confirmed slides were used in this study. The size of each region of interest was 2048  ×  1536 pixels (width  ×  height). The encoder layer has been used to transform input pixels into a lower-dimensional representation, whereas the decoder reconstructs the output of the encoder through minimization of a cost function. The decoder generates an image that only contains immunopositive and immunonegative nuclei. The output of the decoder is fed to the input of the scoring layer. This layer computes the Histo-score or H-score based on the staining intensity, the color expression, and the number of immunopositive and immunonegative nuclei. Pathologists compute this score to subcategorize the cancer grades and to decide proper treatment procedures. Our proposed approach is affordable, accurate, and fast. We achieved excellent performance, with 95.87\% precision and 94.53\% classification accuracy . Our proposed approach streamlines the human error-prone and time-consuming process. This methodology can also be used for other types of histology and immunohistology image segmentation and scoring.},
  archive      = {J_PR},
  author       = {Monjoy Saha and Indu Arun and Rosina Ahmed and Sanjoy Chatterjee and Chandan Chakraborty},
  doi          = {10.1016/j.patcog.2020.107200},
  journal      = {Pattern Recognition},
  pages        = {107200},
  shortjournal = {Pattern Recognition},
  title        = {HscoreNet: A deep network for estrogen and progesterone scoring using breast IHC images},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Object recognition based on convex hull alignment.
<em>PR</em>, <em>102</em>, 107199. (<a
href="https://doi.org/10.1016/j.patcog.2020.107199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common approach to recognition of objects in cluttered scenes is to generate hypotheses about objects present in the scene by matching local descriptors of point features. These hypotheses are then evaluated by measuring how well they explain a particular part of the scene. In this paper, we investigate an alternative approach, which is based on alignment of convex hulls of segments detected in a depth image with convex hulls of target 3D object models or their parts. This alignment is performed using the Convex Template Instance descriptor. This descriptor was originally proposed for fruit recognition and classification of segmented objects. We have adapted this approach to recognize objects in complex scenes. Furthermore, we propose a novel three-level hypothesis evaluation strategy which can be used to achieve highly efficient object recognition. The proposed approach is evaluated by comparison with nine state-of-the-art approaches using three challenging benchmark datasets.},
  archive      = {J_PR},
  author       = {Robert Cupec and Ivan Vidović and Damir Filko and Petra Đurović},
  doi          = {10.1016/j.patcog.2020.107199},
  journal      = {Pattern Recognition},
  pages        = {107199},
  shortjournal = {Pattern Recognition},
  title        = {Object recognition based on convex hull alignment},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Appropriateness of performance indices for imbalanced data
classification: An analysis. <em>PR</em>, <em>102</em>, 107197. (<a
href="https://doi.org/10.1016/j.patcog.2020.107197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Indices quantifying the performance of classifiers under class-imbalance, often suffer from distortions depending on the constitution of the test set or the class-specific classification accuracy , creating difficulties in assessing the merit of the classifier. We identify two fundamental conditions that a performance index must satisfy to be respectively resilient to altering number of testing instances from each class and the number of classes in the test set. In light of these conditions, under the effect of class imbalance, we theoretically analyze four indices commonly used for evaluating binary classifiers and five popular indices for multi-class classifiers. For indices violating any of the conditions, we also suggest remedial modification and normalization. We further investigate the capability of the indices to retain information about the classification performance over all the classes, even when the classifier exhibits extreme performance on some classes. Simulation studies are performed on high dimensional deep representations of subset of the ImageNet dataset using four state-of-the-art classifiers tailored for handling class imbalance. Finally, based on our theoretical findings and empirical evidence, we recommend the appropriate indices that should be used to evaluate the performance of classifiers in presence of class-imbalance.},
  archive      = {J_PR},
  author       = {Sankha Subhra Mullick and Shounak Datta and Sourish Gunesh Dhekane and Swagatam Das},
  doi          = {10.1016/j.patcog.2020.107197},
  journal      = {Pattern Recognition},
  pages        = {107197},
  shortjournal = {Pattern Recognition},
  title        = {Appropriateness of performance indices for imbalanced data classification: An analysis},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PoseConvGRU: A monocular approach for visual ego-motion
estimation by learning. <em>PR</em>, <em>102</em>, 107187. (<a
href="https://doi.org/10.1016/j.patcog.2019.107187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual ego-motion estimation is one of the longstanding problems which estimates the movement of cameras from images. Learning based ego-motion estimation methods have seen an increasing attention since its desirable properties of robustness to image noise and camera calibration independence. In this work, we propose a data-driven approach of learning based visual ego-motion estimation for a monocular camera. We use an end-to-end learning approach in allowing the model to learn a map from input image pairs to the corresponding ego-motion, which is parameterized as 6-DoF transformation matrix . We introduce a two-module Long-term Recurrent Convolutional Neural Networks called PoseConvGRU. The feature-encoding module encodes the short-term motion feature in an image pair, while the memory-propagating module captures the long-term motion feature in the consecutive image pairs. The visual memory is implemented with convolutional gated recurrent units, which allows propagating information over time. At each time step, two consecutive RGB images are stacked together to form a 6-channel tensor for feature-encoding module to learn how to extract motion information and estimate poses. The sequence of output maps is then passed through the memory-propagating module to generate the relative transformation pose of each image pair. In addition, we have designed a series of data augmentation methods to avoid the overfitting problem and improve the performance of the model when facing challengeable scenarios such as high-speed or reverse driving. We evaluate the performance of our proposed approach on the KITTI Visual Odometry benchmark and Malaga 2013 Dataset. The experiments show a competitive performance of the proposed method to the state-of-the-art monocular geometric and learning methods and encourage further exploration of learning-based methods for the purpose of estimating camera ego-motion even though geometrical methods demonstrate promising results.},
  archive      = {J_PR},
  author       = {Guangyao Zhai and Liang Liu and Linjian Zhang and Yong Liu and Yunliang Jiang},
  doi          = {10.1016/j.patcog.2019.107187},
  journal      = {Pattern Recognition},
  pages        = {107187},
  shortjournal = {Pattern Recognition},
  title        = {PoseConvGRU: A monocular approach for visual ego-motion estimation by learning},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scale robust deep oriented-text detection network.
<em>PR</em>, <em>102</em>, 107180. (<a
href="https://doi.org/10.1016/j.patcog.2019.107180">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text detection is a prerequisite of text recognition, and multi-oriented text detection is a hot topic recently. The existing multi-oriented text detection methods fall short when facing two issues: 1) text scales change in a wide range, and 2) there exists the foreground-background class imbalance. In this paper, we propose a scale-robust deep multi-oriented text-detection model, which not only has the efficiency of the one-stage deep detection model, but also has the comparable accuracy of the two-stage deep text-detection model. We design the feature refining block to fuse multi-scale context features for the purpose of keeping text detection in a higher-resolution feature map. Moreover, in order to mitigate the foreground-background class imbalance, Focal Loss is adopted to up weight the hard-classified samples. Our method is implemented on four benchmark text datasets: ICDAR2013, ICDAR2015, COCO-Text and MSRA-TD500. The experimental results demonstrate that our method is superior to the existing one-stage deep text-detection models and comparable to the state-of-the-art text detection methods.},
  archive      = {J_PR},
  author       = {Yuqiang Zheng and Yuan Xie and Yanyun Qu and Xiaodong Yang and Cuihua Li and Yan Zhang},
  doi          = {10.1016/j.patcog.2019.107180},
  journal      = {Pattern Recognition},
  pages        = {107180},
  shortjournal = {Pattern Recognition},
  title        = {Scale robust deep oriented-text detection network},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised domain adaptive re-identification: Theory and
practice. <em>PR</em>, <em>102</em>, 107173. (<a
href="https://doi.org/10.1016/j.patcog.2019.107173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of unsupervised domain adaptive re-identification (re-ID) which is an active topic in computer vision but lacks a theoretical foundation. We first extend existing unsupervised domain adaptive classification theories to re-ID tasks. Concretely, we introduce some assumptions on the extracted feature space and then derive several loss functions guided by these assumptions. To optimize them, a novel self-training scheme for unsupervised domain adaptive re-ID tasks is proposed. It iteratively makes guesses for unlabeled target data based on an encoder and trains the encoder based on the guessed labels. Extensive experiments on unsupervised domain adaptive person re-ID and vehicle re-ID tasks with comparisons to the state-of-the-arts confirm the effectiveness of the proposed theories and self-training framework. Our code is available on https://github.com/LcDog/DomainAdaptiveReID .},
  archive      = {J_PR},
  author       = {Liangchen Song and Cheng Wang and Lefei Zhang and Bo Du and Qian Zhang and Chang Huang and Xinggang Wang},
  doi          = {10.1016/j.patcog.2019.107173},
  journal      = {Pattern Recognition},
  pages        = {107173},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised domain adaptive re-identification: Theory and practice},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An accelerated correlation filter tracker. <em>PR</em>,
<em>102</em>, 107172. (<a
href="https://doi.org/10.1016/j.patcog.2019.107172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent visual object tracking methods have witnessed a continuous improvement in the state-of-the-art with the development of efficient discriminative correlation filters (DCF) and robust deep neural network features. Despite the outstanding performance achieved by the above combination, existing advanced trackers suffer from the burden of high computational complexity of the deep feature extraction and online model learning. We propose an accelerated ADMM optimisation method obtained by adding a momentum to the optimisation sequence iterates, and by relaxing the impact of the error between DCF parameters and their norm. The proposed optimisation method is applied to an innovative formulation of the DCF design, which seeks the most discriminative spatially regularised feature channels. A further speed up is achieved by an adaptive initialisation of the filter optimisation process. The significantly increased convergence of the DCF filter is demonstrated by establishing the optimisation process equivalence with a continuous dynamical system for which the convergence properties can readily be derived. The experimental results obtained on several well-known benchmarking datasets demonstrate the efficiency and robustness of the proposed ACFT method, with a tracking accuracy comparable to the start-of-the-art trackers.},
  archive      = {J_PR},
  author       = {Tianyang Xu and Zhen-Hua Feng and Xiao-Jun Wu and Josef Kittler},
  doi          = {10.1016/j.patcog.2019.107172},
  journal      = {Pattern Recognition},
  pages        = {107172},
  shortjournal = {Pattern Recognition},
  title        = {An accelerated correlation filter tracker},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Blind single image super-resolution with a mixture of deep
networks. <em>PR</em>, <em>102</em>, 107169. (<a
href="https://doi.org/10.1016/j.patcog.2019.107169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing deep neural network based image super-resolution (SR) methods are mostly designed for non-blind cases, where the blur kernel used to generate the low-resolution (LR) images is assumed to be known and fixed. However, this assumption does not hold in many real scenarios. Motivated by the observation that SR of LR images generated by different blur kernels are essentially different but also correlated, we propose a mixture model of deep networks, which is capable of clustering SR tasks of different blur kernels into a set of groups. Each group is composed of correlated SR tasks with similar blur kernels and can be effectively handled by a combination of specific networks in the mixture model. To achieve automatic SR tasks clustering and network selection, we model the blur kernel with a latent variable, which is inferred from the input image by an encoder network. Since the ground-truth of the latent variable is unknown in the training stage, we initialize the encoder network by pre-training it on the blur kernel classification task to avoid trivial solutions . To jointly train the mixture model and the encoder network, we further derive a lower bound of the likelihood function, which circumvents the intractability in direct maximum likelihood estimation. Extensive evaluations are performed on benchmark data sets and validate the effectiveness of the proposed method.},
  archive      = {J_PR},
  author       = {Yifan Wang and Lijun Wang and Hongyu Wang and Peihua Li and Huchuan Lu},
  doi          = {10.1016/j.patcog.2019.107169},
  journal      = {Pattern Recognition},
  pages        = {107169},
  shortjournal = {Pattern Recognition},
  title        = {Blind single image super-resolution with a mixture of deep networks},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A reduced universum twin support vector machine for class
imbalance learning. <em>PR</em>, <em>102</em>, 107150. (<a
href="https://doi.org/10.1016/j.patcog.2019.107150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In most of the real world datasets, there is an imbalance in the number of samples belonging to different classes. Various pattern classification problems such as fault or disease detection involve class imbalanced data. The support vector machine (SVM) classifier becomes biased towards the majority class due to class imbalance. Moreover, in the existing SVM based techniques for class imbalance, there is no information about the distribution of data. Motivated by the idea of prior information about data distribution, a reduced universum twin support vector machine for class imbalance learning (RUTSVM-CIL) is proposed in this paper. For the first time, universum learning is incorporated with SVM to solve the problem of class imbalance. Oversampling and undersampling of data is performed to remove the imbalance in the classes. The universum data points are used to give prior information about the data. To reduce the computation time of our universum based algorithm, we use a small sized rectangular kernel matrix. The reduced kernel matrix needs less storage space, and thus applicable for large scale imbalanced datasets. Comprehensive experimentation is performed on various synthetic, real world and large scale imbalanced datasets. In comparison to the existing approaches for class imbalance, the proposed RUTSVM-CIL gives better generalization performance for most of the benchmark datasets. Also, the computation cost of RUTSVM-CIL is very less, making it suitable for real world applications.},
  archive      = {J_PR},
  author       = {B. Richhariya and M. Tanveer},
  doi          = {10.1016/j.patcog.2019.107150},
  journal      = {Pattern Recognition},
  pages        = {107150},
  shortjournal = {Pattern Recognition},
  title        = {A reduced universum twin support vector machine for class imbalance learning},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ensemble deep learning for automated visual classification
using EEG signals. <em>PR</em>, <em>102</em>, 107147. (<a
href="https://doi.org/10.1016/j.patcog.2019.107147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an automated visual classification framework in which a novel analysis method (LSTMS-B) of EEG signals guides the selection of multiple networks that leads to the improvement of classification performance. The method, called LSTMS-B, combines deep learning and ensemble learning to extract the category-dependent representations of EEG signals. Specifically, it introduces Swish activation function into traditional LSTM which reduces the effect of vanishing gradient and optimize the training process. Besides, the Bagging theory is applied to increase the generalization. The LSTMS-B method reaches the average precision of 97.13\% for learning EEG visual presentations, which greatly outperforms traditional LSTM network and other contrast models. Then, to verify its application value, a ResNet-based regression is trained using original images and relevant EEG representations learned before. We use the output of the regression as the features to classify the images, and finally obtain the average classification accuracy of 90.16\%.},
  archive      = {J_PR},
  author       = {Xiao Zheng and Wanzhong Chen and Yang You and Yun Jiang and Mingyang Li and Tao Zhang},
  doi          = {10.1016/j.patcog.2019.107147},
  journal      = {Pattern Recognition},
  pages        = {107147},
  shortjournal = {Pattern Recognition},
  title        = {Ensemble deep learning for automated visual classification using EEG signals},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Domain adaptive representation learning for facial action
unit recognition. <em>PR</em>, <em>102</em>, 107127. (<a
href="https://doi.org/10.1016/j.patcog.2019.107127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning robust representations for applications with multiple modalities of input can have a significant impact on improving performance. Traditional representation learning methods rely on projecting the input modalities to a common subspace to maximize agreement amongst the modalities for a particular task. We propose a novel approach to representation learning that uses a latent representation decoder to reconstruct the target modality and thereby employ the target modality purely as a supervision signal for discovering correlations between the modalities. Through cross modality supervision, we demonstrate that the learnt representation is able to improve upon the performance of the task of facial action unit (AU) recognition over modality specific representations and even their fused counterparts. As an extension, we explore a new transfer learning technique to adapt the learnt representation to the target domain. We also present a shared representation based feature fusion methodology to improve the performance of any multi-modal system. Our experiments on three AU recognition datasets - MMSE, BP4D and DISFA, show strong performance gains producing state-of-the-art results in spite of the absence of data from a modality.},
  archive      = {J_PR},
  author       = {Nishant Sankaran and Deen Dayal Mohan and Nagashri N. Lakshminarayana and Srirangaraj Setlur and Venu Govindaraju},
  doi          = {10.1016/j.patcog.2019.107127},
  journal      = {Pattern Recognition},
  pages        = {107127},
  shortjournal = {Pattern Recognition},
  title        = {Domain adaptive representation learning for facial action unit recognition},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised representation learning by discovering reliable
image relations. <em>PR</em>, <em>102</em>, 107107. (<a
href="https://doi.org/10.1016/j.patcog.2019.107107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning robust representations that allow to reliably establish relations between images is of paramount importance for virtually all of computer vision . Annotating the quadratic number of pairwise relations between training images is simply not feasible, while unsupervised inference is prone to noise, thus leaving the vast majority of these relations to be unreliable. To nevertheless find those relations which can be reliably utilized for learning, we follow a divide-and-conquer strategy: We find reliable similarities by extracting compact groups of images and reliable dissimilarities by partitioning these groups into subsets, converting the complicated overall problem into few reliable local subproblems . For each of the subsets we obtain a representation by learning a mapping to a target feature space so that their reliable relations are kept. Transitivity relations between the subsets are then exploited to consolidate the local solutions into a concerted global representation. While iterating between grouping, partitioning, and learning, we can successively use more and more reliable relations which, in turn, improves our image representation. In experiments, our approach shows state-of-the-art performance on unsupervised classification on ImageNet with 46.0\% and competes favorably on different transfer learning tasks on PASCAL VOC.},
  archive      = {J_PR},
  author       = {Timo Milbich and Omair Ghori and Ferran Diego and Björn Ommer},
  doi          = {10.1016/j.patcog.2019.107107},
  journal      = {Pattern Recognition},
  pages        = {107107},
  shortjournal = {Pattern Recognition},
  title        = {Unsupervised representation learning by discovering reliable image relations},
  volume       = {102},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-task CNN for restoring corrupted fingerprint images.
<em>PR</em>, <em>101</em>, 107203. (<a
href="https://doi.org/10.1016/j.patcog.2020.107203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fingerprint image enhancement is one of the fundamental modules in an automated fingerprint recognition system (AFRS). While the performance of AFRS advances with sophisticated fingerprint matching algorithms , poor fingerprint image quality remains a major issue to achieve accurate fingerprint recognition . In this paper, we present a multi-task convolutional neural network (CNN) based method to recover fingerprint ridge structures from corrupted fingerprint images. By learning from the noises and corruptions caused by various undesirable conditions of finger and sensor, the proposed CNN model consists of two streams that reconstruct the fingerprint image and orientation field simultaneously. The enhanced fingerprint is further refined using the orientation field information. Moreover, we create a deliberately corrupted fingerprint image dataset associated with ground truth images to facilitate the supervised learning of the proposed CNN model. Experimental results show significant improvement on both image quality and fingerprint matching accuracy after applying the proposed fingerprint image enhancement technique to several well-known fingerprint datasets.},
  archive      = {J_PR},
  author       = {Wei Jing Wong and Shang-Hong Lai},
  doi          = {10.1016/j.patcog.2020.107203},
  journal      = {Pattern Recognition},
  pages        = {107203},
  shortjournal = {Pattern Recognition},
  title        = {Multi-task CNN for restoring corrupted fingerprint images},
  volume       = {101},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards explaining anomalies: A deep taylor decomposition of
one-class models. <em>PR</em>, <em>101</em>, 107198. (<a
href="https://doi.org/10.1016/j.patcog.2020.107198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Detecting anomalies in the data is a common machine learning task, with numerous applications in the sciences and industry. In practice, it is not always sufficient to reach high detection accuracy, one would also like to be able to understand why a given data point has been predicted to be anomalous. We propose a principled approach for one-class SVMs (OC-SVM), that draws on the novel insight that these models can be rewritten as distance/pooling neural networks . This ‘neuralization’ step lets us apply deep Taylor decomposition (DTD), a methodology that leverages the model structure in order to quickly and reliably explain decisions in terms of input features. The proposed method (called ‘OC-DTD’) is applicable to a number of common distance-based kernel functions , and it outperforms baselines such as sensitivity analysis, distance to nearest neighbor, or edge detection.},
  archive      = {J_PR},
  author       = {Jacob Kauffmann and Klaus-Robert Müller and Grégoire Montavon},
  doi          = {10.1016/j.patcog.2020.107198},
  journal      = {Pattern Recognition},
  pages        = {107198},
  shortjournal = {Pattern Recognition},
  title        = {Towards explaining anomalies: A deep taylor decomposition of one-class models},
  volume       = {101},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enhancing deep neural networks via multiple kernel learning.
<em>PR</em>, <em>101</em>, 107194. (<a
href="https://doi.org/10.1016/j.patcog.2020.107194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks and Multiple Kernel Learning are representation learning methodologies of widespread use and increasing success. While the former aims at learning representations through a hierarchy of features of increasing complexity, the latter provides a principled approach for the combination of base representations. In this paper, we introduce a general framework in which the internal representations computed by a deep neural network are optimally combined by means of Multiple Kernel Learning . The resulting ensemble methodology is instantiated for Multi-layer Perceptrons architectures (both fully trained and with random-weights), and for Convolutional Neural Networks . Experimental results on several benchmark datasets concretely show the advantages and potentialities of the proposed approach.},
  archive      = {J_PR},
  author       = {Ivano Lauriola and Claudio Gallicchio and Fabio Aiolli},
  doi          = {10.1016/j.patcog.2020.107194},
  journal      = {Pattern Recognition},
  pages        = {107194},
  shortjournal = {Pattern Recognition},
  title        = {Enhancing deep neural networks via multiple kernel learning},
  volume       = {101},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). UcoSLAM: Simultaneous localization and mapping by fusion of
keypoints and squared planar markers. <em>PR</em>, <em>101</em>, 107193.
(<a href="https://doi.org/10.1016/j.patcog.2019.107193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simultaneous Localization and Mapping is the process of simultaneously creating a map of the environment while navigating in it. Most of the SLAM approaches use natural features (e.g. keypoints) that are unstable over time, repetitive in many cases or their number insufficient for a robust tracking (e.g. in indoor buildings). Other researchers, on the other hand, have proposed the use of artificial landmarks, such as squared fiducial markers , placed in the environment to help tracking and relocalization. This paper proposes a novel SLAM approach by fusing natural and artificial landmarks in order to achieve long-term robust tracking in many scenarios. Our method has been compared to the start-of-the-art methods ORB-SLAM2 [1], LDSO [2] and SPM-SLAM [3] in the public datasets Kitti [4], Euroc-MAV [5], TUM [6] and SPM [3], obtaining better precision, robustness and speed. Our tests also show that the combination of markers and keypoints achieves better accuracy than each one of them independently.},
  archive      = {J_PR},
  author       = {Rafael Muñoz-Salinas and R. Medina-Carnicer},
  doi          = {10.1016/j.patcog.2019.107193},
  journal      = {Pattern Recognition},
  pages        = {107193},
  shortjournal = {Pattern Recognition},
  title        = {UcoSLAM: Simultaneous localization and mapping by fusion of keypoints and squared planar markers},
  volume       = {101},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Three-step action search networks with deep q-learning for
real-time object tracking. <em>PR</em>, <em>101</em>, 107188. (<a
href="https://doi.org/10.1016/j.patcog.2019.107188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sliding window and candidate sampling are two widely used search strategies for visual object tracking, but they are far behind real-time. By treating the tracking problem as a three-step decision-making process, a novel tracking network, which explores only three small subsets of candidate regions, is developed to achieve faster (real-time) localization of the target object along the frames in a video. A convolutional neural network agent is formulated to interact with a video over time, and two action-value functions are exploited to learn a favorable policy off-line to determine the best action for visual object tracking. Our model is trained in a collaborative learning way by using action classification and cumulative reward approximation in reinforcement learning. We have evaluated our proposed tracker against a number of state-of-the-art ones over three popular tracking benchmarks including OTB-2013, OTB-2015, and VOT2017. The experimental results have demonstrated that our proposed method can achieve very competitive performance on real-time object tracking.},
  archive      = {J_PR},
  author       = {Zhu Teng and Baopeng Zhang and Jianping Fan},
  doi          = {10.1016/j.patcog.2019.107188},
  journal      = {Pattern Recognition},
  pages        = {107188},
  shortjournal = {Pattern Recognition},
  title        = {Three-step action search networks with deep Q-learning for real-time object tracking},
  volume       = {101},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fuzzy directional enlacement landscapes for the evaluation
of complex spatial relations. <em>PR</em>, <em>101</em>, 107185. (<a
href="https://doi.org/10.1016/j.patcog.2019.107185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural spatial relations between image components are fundamental in the human perception of image similarity, and constitute a challenging topic in the domain of image analysis. By definition, some specific relations are ambiguous and difficult to formalize precisely by humans. In this work, we deal with the issue of evaluating complex spatial configurations , where objects can surround each other, potentially with multiple levels of depth. Based on a recently introduced spatial relation called enlacement, which generalizes the idea of surrounding for arbitrary objects, we propose a fuzzy landscape model that allows both to visualize and evaluate this relation directly in the image space, following different directions. Experiments on several characteristic examples highlight the interest and the behavior of this approach, allowing for rich interpretations of these complex spatial configurations .},
  archive      = {J_PR},
  author       = {Michaël Clément and Camille Kurtz and Laurent Wendling},
  doi          = {10.1016/j.patcog.2019.107185},
  journal      = {Pattern Recognition},
  pages        = {107185},
  shortjournal = {Pattern Recognition},
  title        = {Fuzzy directional enlacement landscapes for the evaluation of complex spatial relations},
  volume       = {101},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ensemble adversarial black-box attacks against deep learning
systems. <em>PR</em>, <em>101</em>, 107184. (<a
href="https://doi.org/10.1016/j.patcog.2019.107184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning (DL) models, e.g., state-of-the-art convolutional neural networks (CNNs), have been widely applied into security sensitivity tasks, such as face payment, security monitoring, automated driving, etc. Then their vulnerability analysis is an emergent topic, especially for black-box attacks, where adversaries do not know the model internal architectures or training parameters. In this paper, two types of ensemble-based black-box attack strategies, selective cascade ensemble strategy (SCES) and stack parallel ensemble strategy (SPES), are proposed to explore the vulnerability of DL system and potential factors that contribute to the high-efficiency attacks are explored. SCES adopts a boosting structure of ensemble learning and SPES employs a bagging structure. Moreover, two pairwise and non-pairwise diversity measures are adopted to examine the relationship between the diversity in substitutes ensembles and transferability of generated adversarial examples. Experimental results show that proposed ensemble adversarial black-box attack strategies can successfully attack the DL system with some defense mechanism, such as adversarial training and ensemble adversarial training. The experimental results also show the greater the diversity in substitute ensembles enables stronger transferability.},
  archive      = {J_PR},
  author       = {Jie Hang and Keji Han and Hui Chen and Yun Li},
  doi          = {10.1016/j.patcog.2019.107184},
  journal      = {Pattern Recognition},
  pages        = {107184},
  shortjournal = {Pattern Recognition},
  title        = {Ensemble adversarial black-box attacks against deep learning systems},
  volume       = {101},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Identifying the best data-driven feature selection method
for boosting reproducibility in classification tasks. <em>PR</em>,
<em>101</em>, 107183. (<a
href="https://doi.org/10.1016/j.patcog.2019.107183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considering the proliferation of extremely high-dimensional data in many domains including computer vision and healthcare applications such as computer-aided diagnosis (CAD), advanced techniques for reducing data dimensionality and identifying the most relevant features for a given classification task such as distinguishing between healthy and disordered brain states are needed. Despite the existence of many works on boosting the classification accuracy using a particular feature selection (FS) method, choosing the best one from a large pool of existing FS techniques for boosting feature reproducibility within a dataset of interest remains a formidable challenge to tackle. Notably, a good performance of a particular FS method does not necessarily imply that the experiment is reproducible and that the features identified are optimal for the entirety of the samples. Essentially, this paper presents the first attempt to address the following challenge: “Given a set of different feature selection methods { F S 1 , ⋯ , F S K } , {FS1,⋯,FSK}, and a dataset of interest, how to identify the most reproducible and ‘trustworthy’ connectomic features that would produce reliable biomarkers capable of accurately differentiate between two specific conditions?” To this aim, we propose FS-Select framework which explores the relationships among the different FS methods using a multi-graph architecture based on feature reproducibility power, average accuracy , and feature stability of each FS method. By extracting the ‘central’ graph node , we identify the most reliable and reproducible FS method for the target brain state classification task along with the most discriminative features fingerprinting these brain states. To evaluate the reproducibility power of FS-Select, we perturbed the training set by using different cross-validation strategies on a multi-view small-scale connectomic dataset (late mild cognitive impairment vs Alzheimer’s disease) and large-scale dataset including autistic vs healthy subjects. Our experiments revealed reproducible connectional features fingerprinting disordered brain states.},
  archive      = {J_PR},
  author       = {Nicolas Georges and Islem Mhiri and Islem Rekik and the Alzheimer’s Disease Neuroimaging Initiative},
  doi          = {10.1016/j.patcog.2019.107183},
  journal      = {Pattern Recognition},
  pages        = {107183},
  shortjournal = {Pattern Recognition},
  title        = {Identifying the best data-driven feature selection method for boosting reproducibility in classification tasks},
  volume       = {101},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image analysis by log-polar exponent-fourier moments.
<em>PR</em>, <em>101</em>, 107177. (<a
href="https://doi.org/10.1016/j.patcog.2019.107177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Moments, as a popular class of the global invariant image descriptors, have been widely used in image analysis, pattern recognition and computer vision applications. Exponent-Fourier moments (EFMs) are a new set of orthogonal moments based on exponential functions, which are suitable for image analysis and rotation invariant pattern recognition. However, EFMs lack natively the scaling-invariant property. In addition, they always suffer from high time complexity, numerical instability, and reconstruction error, especially for higher order of moments. In this paper, we introduce a class of scaling and rotation-invariant orthogonal moments, named Log-Polar Exponent-Fourier moments (LPEFMs), by extending the classical EFMs to the log-polar coordinates. Firstly, we redefined the EFMs’ basis functions in log-polar domain instead of Cartesian/polar coordinate domain in order to obtain the scaling-invariant property. Then, we develop a new framework for computing the LPEFMs by using pseudo-polar Fourier transform and frequency domain interpolation, which result in better image representation capability, numerical stability, and computational speed. Compared with the classical EFMs, the proposed LPEFMs have four advantages, scaling invariance , speed , accuracy and stability . Theoretical analysis and simulation results are provided to validate the proposed image moment and to compare its performance with previous works.},
  archive      = {J_PR},
  author       = {Hong-ying YANG and Shu-ren QI and Chao WANG and Si-bo YANG and Xiang-yang WANG},
  doi          = {10.1016/j.patcog.2019.107177},
  journal      = {Pattern Recognition},
  pages        = {107177},
  shortjournal = {Pattern Recognition},
  title        = {Image analysis by log-polar exponent-fourier moments},
  volume       = {101},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discriminative distribution alignment: A unified framework
for heterogeneous domain adaptation. <em>PR</em>, <em>101</em>, 107165.
(<a href="https://doi.org/10.1016/j.patcog.2019.107165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous domain adaptation (HDA) aims to leverage knowledge from a source domain for helping learn an accurate model in a heterogeneous target domain. HDA is exceedingly challenging since the feature spaces of domains are distinct. To tackle this issue, we propose a unified learning framework called Discriminative Distribution Alignment (DDA) for deriving a domain-invariant subspace. The proposed DDA can simultaneously match the discriminative directions of domains, align the distributions across domains, and enhance the separability of data during adaptation. To achieve this, DDA trains an adaptive classifier by both reducing the distribution divergence and enlarging distances between class centroids . Based on the proposed DDA framework, we further develop two methods, by embedding the cross-entropy loss and squared loss into this framework, respectively. We conduct experiments on the tasks of categorization across domains and modalities. Experimental results clearly demonstrate that the proposed DDA outperforms several state-of-the-art models.},
  archive      = {J_PR},
  author       = {Yuan Yao and Yu Zhang and Xutao Li and Yunming Ye},
  doi          = {10.1016/j.patcog.2019.107165},
  journal      = {Pattern Recognition},
  pages        = {107165},
  shortjournal = {Pattern Recognition},
  title        = {Discriminative distribution alignment: A unified framework for heterogeneous domain adaptation},
  volume       = {101},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Contextual deconvolution network for semantic segmentation.
<em>PR</em>, <em>101</em>, 107152. (<a
href="https://doi.org/10.1016/j.patcog.2019.107152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a Contextual Deconvolution Network (CDN) and focus on context association in decoder network. Specifically, in upsampling path, we introduce two types of contextual modules to model the interdependencies of features in channel and spatial dimensions respectively. The channel contextual module captures image-level semantic information by aggregating the feature maps across spatial dimensions, and clarifies global ambiguity of features. Meanwhile, the spatial contextual module obtains patch-level semantic context by learning a spatial weight map, and enhance the feature discrimination. We embed the two contextual modules into individual components of the decoder network, thus improving the representation power and gaining more precise segment results. Thorough evaluations are performed on four challenging datasets, i.e., PASCAL VOC 2012, ADE20K, PASCAL-Context and Cityscapes dataset. Our approach achieves competitive performance with state-of-the-art models on PASCAL VOC 2012,ADE20K and Cityscapes dataset, and new state-of-the-art performance on PASCAL-Context dataset.},
  archive      = {J_PR},
  author       = {Jun Fu and Jing Liu and Yong Li and Yongjun Bao and Weipeng Yan and Zhiwei Fang and Hanqing Lu},
  doi          = {10.1016/j.patcog.2019.107152},
  journal      = {Pattern Recognition},
  pages        = {107152},
  shortjournal = {Pattern Recognition},
  title        = {Contextual deconvolution network for semantic segmentation},
  volume       = {101},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A repeatable and robust local reference frame for 3D surface
matching. <em>PR</em>, <em>100</em>, 107186. (<a
href="https://doi.org/10.1016/j.patcog.2019.107186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local reference frames (LRFs) have been widely used for 3D local surface description. In this work, we propose a repeatable LRF with strong robustness to different nuisances. Different from existing LRF methods, the proposed LRF uses a part of neighboring points within the support region to calculate the z -axis, and performs an effective feature transformation on the neighboring points to define the x -axis. Specifically, feature transformation is applied to the data on a projection plane based on three point distribution characteristics via weighted strategies. These characteristics include the z -height, the distance to the center and the average length to 1-ring neighbors, covariance analysis is then applied to the transformed points to obtain the eigenvector with the largest eigenvalue , which points towards the maximum variance direction. Using a sign disambiguation technique, the modified eigenvector is used to define the final x -axis. Furthermore, a scale strategy is proposed to improve the robustness of the LRF with respect to mesh decimation. The proposed LRF was rigorously tested on six public benchmark datasets consisting of three different application contexts, i.e., 3D shape retrieval, 3D object recognition and registration. Experiments show that our method achieves significantly higher repeatability and stronger robustness than the state-of-the-arts under Gaussian noise , shot noise and mesh resolution variation. Finally, the descriptor matching results on four typical datasets further demonstrate the effectiveness of our LRF.},
  archive      = {J_PR},
  author       = {Sheng Ao and Yulan Guo and Jindong Tian and Yong Tian and Dong Li},
  doi          = {10.1016/j.patcog.2019.107186},
  journal      = {Pattern Recognition},
  pages        = {107186},
  shortjournal = {Pattern Recognition},
  title        = {A repeatable and robust local reference frame for 3D surface matching},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Monocular pedestrian orientation estimation based on deep
2D-3D feedforward. <em>PR</em>, <em>100</em>, 107182. (<a
href="https://doi.org/10.1016/j.patcog.2019.107182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate pedestrian orientation estimation of autonomous driving helps the ego vehicle obtain the intentions of pedestrians in the related environment, which are the base of safety measures such as collision avoidance and prewarning. However, because of relatively small sizes and high-level deformation of pedestrians, common pedestrian orientation estimation models fail to extract sufficient and comprehensive information from them, thus having their performance restricted, especially monocular ones which fail to obtain depth information of objects and related environment. In this paper, a novel monocular pedestrian orientation estimation model, called FFNet, is proposed. Apart from camera captures, the model adds the 2D and 3D dimensions of pedestrians as two other inputs according to the logic relationship between orientation and them. The 2D and 3D dimensions of pedestrians are determined from the camera captures and further utilized through two feedforward links connected to the orientation estimator. The feedforward links strengthen the logicality and interpretability of the network structure of the proposed model. Experiments show that the proposed model has at least 1.72\% AOS increase than most state-of-the-art models after identical training processes. The model also has competitive results in orientation estimation evaluation on KITTI dataset.},
  archive      = {J_PR},
  author       = {Chenchen Zhao and Yeqiang Qian and Ming Yang},
  doi          = {10.1016/j.patcog.2019.107182},
  journal      = {Pattern Recognition},
  pages        = {107182},
  shortjournal = {Pattern Recognition},
  title        = {Monocular pedestrian orientation estimation based on deep 2D-3D feedforward},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Patched-tube unitary transform for robust tensor completion.
<em>PR</em>, <em>100</em>, 107181. (<a
href="https://doi.org/10.1016/j.patcog.2019.107181">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of the robust tensor completion problem for third-order tensors is to recover a low-rank tensor from incomplete and/or corrupted observations. In this paper, we develop a patched-tubes unitary transform method for robust tensor completion. The proposed method is to extract similar patched-tubes to form a third-order sub-tensor, and then a transformed tensor singular value decomposition is employed to recover such low-rank incomplete and/or corrupted sub-tensor. Here the unitary transform matrix for transformed tensor singular value decomposition is constructed by using left singular vectors of the unfolding matrix arising from such sub-tensor. Moreover, we establish the perturbation results of the transformed tensor singular value decomposition for patched-tubes tensor completion. Extensive numerical experiments on hyperspectral, video and face data sets are presented to demonstrate the superior performance of the proposed patched-tubes unitary transform method over testing state-of-the-art robust tensor completion methods.},
  archive      = {J_PR},
  author       = {Michael K. Ng and Xiongjun Zhang and Xi-Le Zhao},
  doi          = {10.1016/j.patcog.2019.107181},
  journal      = {Pattern Recognition},
  pages        = {107181},
  shortjournal = {Pattern Recognition},
  title        = {Patched-tube unitary transform for robust tensor completion},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep label refinement for age estimation. <em>PR</em>,
<em>100</em>, 107178. (<a
href="https://doi.org/10.1016/j.patcog.2019.107178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Age estimation of unknown persons is a challenging pattern analysis task due to the lack of training data and various ageing mechanisms for different individuals. Label distribution learning-based methods usually make distribution assumptions to simplify age estimation. However, since different genders, races and/or any other characteristics may influence facial ageing, age-label distributions are often complicated and difficult to model parametrically. In this paper, we propose a label refinery network (LRN) with two concurrent processes: label distribution refinement and slack regression refinement. The label refinery network aims to learn age-label distributions progressively in an iterative manner. In this way, we can adaptively obtain the specific age-label distributions for different facial images without making strong assumptions on the fixed distribution formulations. To further utilize the correlations among age labels, we propose a slack regression refinery to convert the age-label regression model into an age-interval regression model. Extensive experiments on three popular datasets, namely, MORPH Album2, ChaLearn15 and MegaAge-Asian, demonstrate the superiority of our method.},
  archive      = {J_PR},
  author       = {Peipei Li and Yibo Hu and Xiang Wu and Ran He and Zhenan Sun},
  doi          = {10.1016/j.patcog.2019.107178},
  journal      = {Pattern Recognition},
  pages        = {107178},
  shortjournal = {Pattern Recognition},
  title        = {Deep label refinement for age estimation},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep eigen-filters for face recognition: Feature
representation via unsupervised multi-structure filter learning.
<em>PR</em>, <em>100</em>, 107176. (<a
href="https://doi.org/10.1016/j.patcog.2019.107176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training deep convolutional neural networks (CNNs) often requires high computational cost and a large number of learnable parameters. To overcome this limitation, one solution is computing predefined convolution kernels from training data. In this paper, we propose a novel three-stage approach for filter learning alternatively. It learns filters in multiple structures including standard filters, channel-wise filters and point-wise filters which are inspired from variations of CNNs’ convolution operations. By analyzing the linear combination between learned filters and original convolution kernels in pre-trained CNNs, the reconstruction error is minimized to determine the most representative filters from the filter bank. These filters are used to build a network followed by HOG-based feature extraction for feature representation. The proposed approach shows competitive performance on color face recognition compared with other deep CNNs-based methods. Besides, it provides a perspective of interpreting CNNs by introducing the concepts of advanced convolutional layers to unsupervised filter learning.},
  archive      = {J_PR},
  author       = {Ming Zhang and Sheheryar Khan and Hong Yan},
  doi          = {10.1016/j.patcog.2019.107176},
  journal      = {Pattern Recognition},
  pages        = {107176},
  shortjournal = {Pattern Recognition},
  title        = {Deep eigen-filters for face recognition: Feature representation via unsupervised multi-structure filter learning},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). No-reference mesh visual quality assessment via ensemble of
convolutional neural networks and compact multi-linear pooling.
<em>PR</em>, <em>100</em>, 107174. (<a
href="https://doi.org/10.1016/j.patcog.2019.107174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blind or No reference quality evaluation is a challenging issue since it is done without access to the original content. In this work, we propose a method based on deep learning for the mesh visual quality assessment without reference. For a given 3D model, we first compute its mesh saliency. Then, we extract views from the 3D mesh and the corresponding mesh saliency. After that, the views are split into small patches that are filtered using a saliency threshold. Only the salient patches are selected and used as input data. After that, three pre-trained deep convolutional neural networks are employed for feature learning : VGG, AlexNet, and ResNet . Each network is fine-tuned and produces a feature vector. The Compact Multi-linear Pooling (CMP) is used afterward to fuse the retrieved vectors into a global feature representation. Finally, fully connected layers followed by a regression module are used to estimate the quality score. Extensive experiments are executed on four mesh quality datasets and comparisons with existing methods demonstrate the effectiveness of our method in terms of correlation with subjective scores.},
  archive      = {J_PR},
  author       = {Ilyass Abouelaziz and Aladine Chetouani and Mohammed El Hassouni and Longin Jan Latecki and Hocine Cherifi},
  doi          = {10.1016/j.patcog.2019.107174},
  journal      = {Pattern Recognition},
  pages        = {107174},
  shortjournal = {Pattern Recognition},
  title        = {No-reference mesh visual quality assessment via ensemble of convolutional neural networks and compact multi-linear pooling},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Deep morphological simplification network (MS-net) for
guided registration of brain magnetic resonance images. <em>PR</em>,
<em>100</em>, 107171. (<a
href="https://doi.org/10.1016/j.patcog.2019.107171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deformable brain MR image registration is challenging due to large inter-subject anatomical variation. For example, the highly complex cortical folding pattern makes it hard to accurately align corresponding cortical structures of individual images. In this paper, we propose a novel deep learning way to simplify the difficult registration problem of brain MR images. Specifically, we train a morphological simplification network (MS-Net), which can generate a simple image with less anatomical details based on the complex input. With MS-Net, the complexity of the fixed image or the moving image under registration can be reduced gradually, thus building an individual (simplification) trajectory represented by MS-Net outputs. Since the generated images at the ends of the two trajectories (of the fixed and moving images) are so simple and very similar in appearance, they are easy to register. Thus, the two trajectories can act as a bridge to link the fixed and the moving images, and guide their registration. Our experiments show that the proposed method can achieve highly accurate registration performance on different datasets ( i.e. , NIREP, LPBA, IBSR, CUMC, and MGH). Moreover, the method can be also easily transferred across diverse image datasets and obtain superior accuracy on surface alignment. We propose MS-Net as a powerful and flexible tool to simplify brain MR images and their registration. To our knowledge, this is the first work to simplify brain MR image registration by deep learning , instead of estimating deformation field directly.},
  archive      = {J_PR},
  author       = {Dongming Wei and Lichi Zhang and Zhengwang Wu and Xiaohuan Cao and Gang Li and Dinggang Shen and Qian Wang},
  doi          = {10.1016/j.patcog.2019.107171},
  journal      = {Pattern Recognition},
  pages        = {107171},
  shortjournal = {Pattern Recognition},
  title        = {Deep morphological simplification network (MS-net) for guided registration of brain magnetic resonance images},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MeMu: Metric correlation siamese network and multi-class
negative sampling for visual tracking. <em>PR</em>, <em>100</em>,
107170. (<a href="https://doi.org/10.1016/j.patcog.2019.107170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the great success in the computer vision field, visual tracking is still a challenging task. The main obstacle is that the target object often suffers from interference, such as occlusion. As most Siamese network-based trackers mainly sample image patches of target objects for training, the tracking algorithm lacks sufficient information about the surrounding environment. Besides, many Siamese network-based tracking algorithms build a regression only with the target object samples without considering the relationship between target and background, which may deteriorate the performance of trackers. In this paper, we propose a metric correlation Siamese network and multi-class negative sampling tracking method. For the first time, we explore a sampling approach that includes three different kinds of negative samples: virtual negative samples for pre-learning the potential occlusion situation, boundary negative samples to cope with potential tracking drift, and context negative samples to cope with potential incorrect positioning. With the three kinds of negative samples, we also propose a metric correlation method to train a correlation filter that contains metric information for better discrimination. Furthermore, we design a Siamese network-based architecture to embed the metric correlation filter module mentioned above in order to benefit from the powerful representation ability of deep learning . Extensive experiments on challenging OTB100 and VOT2017 datasets demonstrate the competitive performance of the proposed algorithm performs favorably compared with state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Yafu Xiao and Jing Li and Bo Du and Jia Wu and Jun Chang and Wenfan Zhang},
  doi          = {10.1016/j.patcog.2019.107170},
  journal      = {Pattern Recognition},
  pages        = {107170},
  shortjournal = {Pattern Recognition},
  title        = {MeMu: Metric correlation siamese network and multi-class negative sampling for visual tracking},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). No-reference stereoscopic image quality assessment using a
multi-task CNN and registered distortion representation. <em>PR</em>,
<em>100</em>, 107168. (<a
href="https://doi.org/10.1016/j.patcog.2019.107168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene discrepancy between the left and right views presents more challenges for image quality assessment (IQA) of stereoscopic images as opposed to monocular ones. Existing no-reference stereoscopic IQA (NR-SIQA) metrics cannot achieve a good performance on asymmetrically distorted stereoscopic images. In this paper, we propose an NR-SIQA index that first addresses scene discrepancy by means of image registration. It then uses a registered distortion representation based on the left and registered right views to represent the distortion in the stereoscopic image. Because different distortion types influence image quality differently, a multi-task convolutional neural network (CNN) is employed to learn image quality prediction and distortion-type identification simultaneously. We first design a one-column multi-task CNN model, that learns from the registered distortion representation. Then, we extend the one-column model to a three-column model, which also learns from the left and right views. Our experimental results validate the effectiveness of the proposed registered distortion representation and multi-task CNN architecture. The proposed one- and three-column models outperform the state-of-the-art NR-SIQA metrics, especially for asymmetrically distorted stereoscopic images.},
  archive      = {J_PR},
  author       = {Yiqing Shi and Wenzhong Guo and Yuzhen Niu and Jiamei Zhan},
  doi          = {10.1016/j.patcog.2019.107168},
  journal      = {Pattern Recognition},
  pages        = {107168},
  shortjournal = {Pattern Recognition},
  title        = {No-reference stereoscopic image quality assessment using a multi-task CNN and registered distortion representation},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Locality-constrained affine subspace coding for image
classification and retrieval. <em>PR</em>, <em>100</em>, 107167. (<a
href="https://doi.org/10.1016/j.patcog.2019.107167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature coding is a key component of the bag of visual words (BoVW) model, which is designed to improve image classification and retrieval performance . In the feature coding process, each feature of an image is nonlinearly mapped via a dictionary of visual words to form a high-dimensional sparse vector. Inspired by the well-known locality-constrained linear coding (LLC), we present a locality-constrained affine subspace coding (LASC) method to address the limitation whereby LLC fails to consider the local geometric structure around visual words. LASC is distinguished from all the other coding methods since it constructs a dictionary consisting of an ensemble of affine subspaces. As such, the local geometric structure of a manifold is explicitly modeled by such a dictionary. In the process of coding, each feature is linearly decomposed and weighted to form the first-order LASC vector with respect to its top-k neighboring subspaces. To further boost performance, we propose the second-order LASC vector based on information geometry. We use the proposed coding method to perform both image classification and image retrieval tasks and the experimental results show that the method achieves superior or competitive performance in comparison to state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Bingbing Zhang and Qilong Wang and Xiaoxiao Lu and Fasheng Wang and Peihua Li},
  doi          = {10.1016/j.patcog.2019.107167},
  journal      = {Pattern Recognition},
  pages        = {107167},
  shortjournal = {Pattern Recognition},
  title        = {Locality-constrained affine subspace coding for image classification and retrieval},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Discovering influential factors in variational
autoencoders. <em>PR</em>, <em>100</em>, 107166. (<a
href="https://doi.org/10.1016/j.patcog.2019.107166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of machine learning , it is still a critical issue to identify and supervise the learned representation without manually intervening or intuition assistance to extract useful knowledge or serve for the downstream tasks. In this work, we focus on supervising the influential factors extracted by the variational autoencoder (VAE). The VAE is proposed to learn independent low dimension representation while facing the problem that sometimes pre-set factors are ignored. We argue that the mutual information of the input and each learned factor of the representation plays a necessary indicator of discovering the influential factors. We find the VAE objective inclines to induce mutual information sparsity in factor dimension over the data intrinsic dimension and therefore result in some non-influential factors whose function on data reconstruction could be ignored. We show mutual information also influences the lower bound of VAE’s reconstruction error and downstream classification task . To make such indicator applicable, we design an algorithm for calculating the mutual information for VAE and prove its consistency. Experimental results on MNIST, CelebA and DEAP datasets show that mutual information can help determine influential factors, of which some are interpretable and can be used to further generation and classification tasks, and help discover the variant that connects with emotion on DEAP dataset.},
  archive      = {J_PR},
  author       = {Shiqi Liu and Jingxin Liu and Qian Zhao and Xiangyong Cao and Huibin Li and Deyu Meng and Hongying Meng and Sheng Liu},
  doi          = {10.1016/j.patcog.2019.107166},
  journal      = {Pattern Recognition},
  pages        = {107166},
  shortjournal = {Pattern Recognition},
  title        = {Discovering influential factors in variational autoencoders},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semi-supervised robust deep neural networks for multi-label
image classification. <em>PR</em>, <em>100</em>, 107164. (<a
href="https://doi.org/10.1016/j.patcog.2019.107164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a robust method for semi-supervised training of deep neural networks for multi-label image classification . To this end, a ramp loss is utilized since it is more robust against noisy and incomplete image labels compared to the classic hinge loss. The proposed method allows for learning from both labeled and unlabeled data in a semi-supervised setting. This is achieved by propagating labels from the labeled images to their unlabeled neighbors in the feature space. Using a robust loss function becomes crucial here, as the initial label propagations may include many errors, which degrades the performance of non-robust loss functions. In contrast, the proposed robust ramp loss restricts extreme penalties from the samples with incorrect labels, and the label assignment improves in each iteration and contributes to the learning process. The proposed method achieves state-of-the-art results in semi-supervised learning experiments on the CIFAR-10 and STL-10 datasets, and comparable results to the state-of the-art in supervised learning experiments on the NUS-WIDE and MS-COCO datasets. Experimental results also verify that our proposed method is more robust against noisy image labels as expected.},
  archive      = {J_PR},
  author       = {Hakan Cevikalp and Burak Benligiray and Omer Nezih Gerek},
  doi          = {10.1016/j.patcog.2019.107164},
  journal      = {Pattern Recognition},
  pages        = {107164},
  shortjournal = {Pattern Recognition},
  title        = {Semi-supervised robust deep neural networks for multi-label image classification},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A domain adaptation approach to solve inverse problems in
imaging via coupled deep dictionary learning. <em>PR</em>, <em>100</em>,
107163. (<a href="https://doi.org/10.1016/j.patcog.2019.107163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we focus on solving four standard inverse problems in imaging – denoising , deblurring, super-resolution, and reconstruction. All these problems are usually associated with image acquisition. Traditionally, signal processing techniques are used to solve such problems. However, such techniques are computationally expensive. In many applications today, when the requirement is to compute on the edge, such expensive inversion techniques are not viable solutions. Thus, one resorts to machine learning based solutions. In the past few years, variants of neural networks were developed to ‘learn’ the inversion operation. Although the learning was computationally expensive, the learnt model was light-weight and portable; suitable for deployment on leaner platforms. This work is based on the same concept, however, instead of using neural networks we treat inversion as a domain adaptation problem – a transfer from corrupted space to clean space. Our work is based on the developing field of coupled representation learning . We propose a deep version of the well known coupled dictionary learning technique; but instead of learning a single layer of dictionary, we learn multiple layers. Experimental results on standard datasets against state-of-the-art solutions for each problem, show that our method yields the best results both in terms of accuracy and speed.},
  archive      = {J_PR},
  author       = {Vanika Singhal and Angshul Majumdar},
  doi          = {10.1016/j.patcog.2019.107163},
  journal      = {Pattern Recognition},
  pages        = {107163},
  shortjournal = {Pattern Recognition},
  title        = {A domain adaptation approach to solve inverse problems in imaging via coupled deep dictionary learning},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A dual-cue network for multispectral photometric stereo.
<em>PR</em>, <em>100</em>, 107162. (<a
href="https://doi.org/10.1016/j.patcog.2019.107162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating pixel-wise surface normal from a single image is a challenging task but offers great values to computer vision and robotics applications . By using the spectrally and spatially variant illumination, multispectral photometric stereo can produce pixel-wise surface normal from just one image. But multispectral photometric stereo methods may encounter the tangle problem of illumination, surface reflectance and camera response, which lead to an under-determined system. Existing approaches rely on either extra depth information or material calibration strategies, assuming a Lambertian surface condition which limits their application in practical systems. Previous learning-based methods employ fully-connected or CNN architectures to estimate surface normal. Compared with fully-connected framework, CNN takes advantage of the information embedded in the neighborhood of a surface point, but losing high-frequency surface normal details. In this paper, we present a new method that addresses this task by designing two stacked deep network. We first apply a CNN-based structural cue network to approximate coarse surface normal on small patches. Then, we use a pixel level fully-connected photometric cue network to further refine surface normal details and correct errors from the first step. The fused network is robust to non-Lambertian surfaces and complex illumination environments, such as ambient light and variant light directions. Experimental results show that our dual-cue fused network outperforms existing methods.},
  archive      = {J_PR},
  author       = {Yakun Ju and Xinghui Dong and Yingyu Wang and Lin Qi and Junyu Dong},
  doi          = {10.1016/j.patcog.2019.107162},
  journal      = {Pattern Recognition},
  pages        = {107162},
  shortjournal = {Pattern Recognition},
  title        = {A dual-cue network for multispectral photometric stereo},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Connectivity-based cylinder detection in unorganized point
clouds. <em>PR</em>, <em>100</em>, 107161. (<a
href="https://doi.org/10.1016/j.patcog.2019.107161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cylinder detection is an important step in reverse engineering of industrial sites, as such environments often contain a large number of cylindrical pipes and tanks. However, existing techniques for cylinder detection require the specification of several parameters which are difficult to adjust because their values depend on the noise level of the input point cloud. Also, these solutions often expect the cylinders to be either parallel or perpendicular to the ground. We present a cylinder-detection technique that is robust to noise, contains parameters which require little to no fine-tuning, and can handle cylinders with arbitrary orientations. Our approach is based on a robust linear-time circle-detection algorithm that naturally discards outliers, allowing our technique to handle datasets with various density and noise levels while using a set of default parameter values. It works by projecting the point cloud onto a set of directions over the unit hemisphere and detecting circular projections formed by samples defining connected components in 3D. The extracted cylindrical surfaces are obtained by fitting a cylinder to each connected component. We compared our technique against the state-of-the-art methods on both synthetic and real datasets containing various densities and noise levels, and show that it outperforms existing techniques in terms of accuracy and robustness to noise, while still maintaining a competitive running time.},
  archive      = {J_PR},
  author       = {Abner M.C. Araújo and Manuel M. Oliveira},
  doi          = {10.1016/j.patcog.2019.107161},
  journal      = {Pattern Recognition},
  pages        = {107161},
  shortjournal = {Pattern Recognition},
  title        = {Connectivity-based cylinder detection in unorganized point clouds},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Few-shot traffic sign recognition with clustering inductive
bias and random neural network. <em>PR</em>, <em>100</em>, 107160. (<a
href="https://doi.org/10.1016/j.patcog.2019.107160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reliable and fast traffic sign recognition (TSR) that locates the traffic sign from an image and then estimates its category is a crucial perception function for Advanced Driver Assistance Systems (ADAS) of autonomous vehicles. Most of the popular deep convolutional neural networks (DCNNs) based TSR techniques advocate discriminative feature learning for traffic signs against their appearance variability. However, such feature learning scheme may suffer from the diversity of traffic signs categories, especially when samples within each category are limited for model training (i.e., few-shot learning). Here, we present a generative feature learning based TSR network with well generalization capacity and high computational efficiency. Instead of relying on large amounts of supervision to learn discriminative features , our method devotes to learn common but unique properties of class-specific traffic signs with few training samples. Specifically, we combine clustering inductive bias with a random neural network , and then exploit computational advantages offered by a fast random projection algorithm. Experiments on two TSR benchmarks illustrate that our method achieves comparable or higher recognition accuracy than state-of-the-art DCNN-based methods with less training data and inference time consumption.},
  archive      = {J_PR},
  author       = {Shichao Zhou and Chenwei Deng and Zhengquan Piao and Baojun Zhao},
  doi          = {10.1016/j.patcog.2019.107160},
  journal      = {Pattern Recognition},
  pages        = {107160},
  shortjournal = {Pattern Recognition},
  title        = {Few-shot traffic sign recognition with clustering inductive bias and random neural network},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Encoding features robust to unseen modes of variation with
attentive long short-term memory. <em>PR</em>, <em>100</em>, 107159. (<a
href="https://doi.org/10.1016/j.patcog.2019.107159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long short-term memory (LSTM) is a type of recurrent neural networks that is efficient for encoding spatio-temporal features in dynamic sequences. Recent work has shown that the LSTM retains information related to the mode of variation in the input dynamic sequence which reduces the discriminability of the encoded features. To encode features robust to unseen modes of variation, we devise an LSTM adaptation named attentive mode variational LSTM. The proposed attentive mode variational LSTM utilizes the concept of attention to separate the input dynamic sequence into two parts: (1) task-relevant dynamic sequence features and (2) task-irrelevant static sequence features. The task-relevant dynamic features are used to encode and emphasize the dynamics in the input sequence. The task-irrelevant static sequence features are utilized to encode the mode of variation in the input dynamic sequence. Finally, the attentive mode variational LSTM suppresses the effect of mode variation with a shared output gate and results in a spatio-temporal feature robust to unseen variations. The effectiveness of the proposed attentive mode variational LSTM has been verified using two tasks: facial expression recognition and human action recognition. Comprehensive and extensive experiments have verified that the proposed method encodes spatio-temporal features robust to variations unseen during the training.},
  archive      = {J_PR},
  author       = {Wissam J. Baddar and Yong Man Ro},
  doi          = {10.1016/j.patcog.2019.107159},
  journal      = {Pattern Recognition},
  pages        = {107159},
  shortjournal = {Pattern Recognition},
  title        = {Encoding features robust to unseen modes of variation with attentive long short-term memory},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint temporal context exploitation and active learning for
video segmentation. <em>PR</em>, <em>100</em>, 107158. (<a
href="https://doi.org/10.1016/j.patcog.2019.107158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The segmentation of video, or separating out objects in the foreground, is an important application of pattern recognition and computer vision . Segmentation errors in pattern recognition approaches mainly come from difficulties in selecting maximally informative frames for learning. In this paper, we develop an approach to video segmentation that relies on temporal features by modeling the uncertainty of the distribution of different feature mask forms. We use those uncertainty values for unsupervised active learning. We evaluate our approach on the DAVIS16 annotated video data set and Shining3D dental video data set, and the results show our approach to be more accurate than other video segmentation approaches .},
  archive      = {J_PR},
  author       = {Yan Tian and Guohua Cheng and Judith Gelernter and Shihao Yu and Chao Song and Bailin Yang},
  doi          = {10.1016/j.patcog.2019.107158},
  journal      = {Pattern Recognition},
  pages        = {107158},
  shortjournal = {Pattern Recognition},
  title        = {Joint temporal context exploitation and active learning for video segmentation},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Transfer learning-based discriminative correlation filter
for visual tracking. <em>PR</em>, <em>100</em>, 107157. (<a
href="https://doi.org/10.1016/j.patcog.2019.107157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most Correlation Filter (CF)-based tracking methods can hardly handle occlusion or severe deformation, due to the lack of effective utilization of previous target information. To overcome this, we propose a novel Transfer Learning-based Discriminative Correlation Filter (TLDCF), which extracts knowledge from multiple previous tracking tasks and applies the knowledge for a new tracking task through Instance-Transfer Learning (ITL) and Probability-Transfer Learning (PTL). ITL applies knowledge of Gaussian Mixture Modelling (GMM) target representations and multi-channel filters learned in previous frames to directly train a new correlation filter. This improves the robustness of tracker for heavy occlusion and large appearance variations. Meanwhile, PTL encodes the spatio-temporal relationship predicted by Kalman Filter (KF) into a shared Gaussian prior to suppress huge location drift caused by similar targets. For optimization, we develop an efficient Alternating Direction Method of Multipliers (ADMM) based algorithm to calculate CFs on each independent channel in real time. Extensive experiments on OTB-2013 and OTB-2015 datasets well demonstrate the effectiveness of the proposed method. In particular, our method improves AUC score of the two datasets by 5.5\% and 3.9\% respectively compared to baseline, and achieves competitive performance against recent state-of-the-art deep trackers.},
  archive      = {J_PR},
  author       = {Bo Huang and Tingfa Xu and Jianan Li and Ziyi Shen and Yiwen Chen},
  doi          = {10.1016/j.patcog.2019.107157},
  journal      = {Pattern Recognition},
  pages        = {107157},
  shortjournal = {Pattern Recognition},
  title        = {Transfer learning-based discriminative correlation filter for visual tracking},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Combining deep generative and discriminative models for
bayesian semi-supervised learning. <em>PR</em>, <em>100</em>, 107156.
(<a href="https://doi.org/10.1016/j.patcog.2019.107156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative models can be used for a wide range of tasks, and have the appealing ability to learn from both labelled and unlabelled data . In contrast, discriminative models cannot learn from unlabelled data, but tend to outperform their generative counterparts in supervised tasks. We develop a framework to jointly train deep generative and discriminative models, enjoying the benefits of both. The framework allows models to learn from labelled and unlabelled data, as well as naturally account for uncertainty in predictive distributions , providing the first Bayesian approach to semi-supervised learning with deep generative models. We demonstrate that our blended discriminative and generative models outperform purely generative models in both predictive performance and uncertainty calibration in a number of semi-supervised learning tasks.},
  archive      = {J_PR},
  author       = {Jonathan Gordon and José Miguel Hernández-Lobato},
  doi          = {10.1016/j.patcog.2019.107156},
  journal      = {Pattern Recognition},
  pages        = {107156},
  shortjournal = {Pattern Recognition},
  title        = {Combining deep generative and discriminative models for bayesian semi-supervised learning},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). BLAN: Bi-directional ladder attentive network for facial
attribute prediction. <em>PR</em>, <em>100</em>, 107155. (<a
href="https://doi.org/10.1016/j.patcog.2019.107155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep facial attribute prediction has received considerable attention with a wide range of real-world applications in the past few years. Existing works almost extract abstract global features at high levels of deep neural networks to make predictions. However, local features at low levels, which contain detailed local attribute information, are not well exploited. In this paper, we propose a novel Bi-directional Ladder Attentive Network (BLAN) to learn hierarchical representations, covering the correlations between feature hierarchies and attribute characteristics. BLAN adopts layer-wise bi-directional connections based on the autoencoder framework from low to high levels. In this way, hierarchical features with local and global attribute characteristics could be correspondingly interweaved at each level via multiple designed Residual Dual Attention Modules (RDAMs). Besides, we derive a Local Mutual Information Maximization (LMIM) loss to further incorporate the locality of facial attributes to high-level representations at each hierarchy. Multiple attribute classifiers receive hierarchical representations to produce local and global decisions, followed by a proposed adaptive score fusion module to merge these decisions for yielding the final prediction result. Extensive experiments on two facial attribute datasets, CelebA and LFWA, demonstrate that our BLAN outperforms state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Xin Zheng and Huaibo Huang and Yanqing Guo and Bo Wang and Ran He},
  doi          = {10.1016/j.patcog.2019.107155},
  journal      = {Pattern Recognition},
  pages        = {107155},
  shortjournal = {Pattern Recognition},
  title        = {BLAN: Bi-directional ladder attentive network for facial attribute prediction},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Statistical bootstrap-based principal mode component
analysis for dynamic background subtraction. <em>PR</em>, <em>100</em>,
107153. (<a href="https://doi.org/10.1016/j.patcog.2019.107153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Background subtraction is needed to extract foreground information from a video sequence for further processing in many applications, such as surveillance tracking. However, due to the presence of a dynamic background and noise, extracting foreground accurately from a video sequence remains challenging. A novel projection method, namely Principal Mode Component Analysis (PMCA), is proposed to capture the most repetitive patterns of a video sequence, which is one of the key characteristics of the video background. The patterns are captured by applying the bootstrapping method together with the statistic mode measure. The bootstrapping method can model the distribution of almost any statistic of the dynamic background and complicated noise. This is different from current methods, which restrict the distribution to a closed-form function. We introduce a mathematical relaxation that can formulate the statistical mode measure for a continuous video data. A fast exhaustive search method is proposed to find the global optimal solution for the PMCA. This fast method adopts a simplification procedure that makes the optimization procedure independent of the video size. The proposed method is computationally much more traceable than existing ones. We compare the proposed method with 10 different methods, including several state-of-the-art techniques, for 19 different real-world video sequences from two popular datasets. Experiment results show that the proposed method performs the best in 16 cases and second best in 2 cases.},
  archive      = {J_PR},
  author       = {Benson S.Y. Lam and Amanda M.Y. Chu and H. Yan},
  doi          = {10.1016/j.patcog.2019.107153},
  journal      = {Pattern Recognition},
  pages        = {107153},
  shortjournal = {Pattern Recognition},
  title        = {Statistical bootstrap-based principal mode component analysis for dynamic background subtraction},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Concatenation hashing: A relative position preserving method
for learning binary codes. <em>PR</em>, <em>100</em>, 107151. (<a
href="https://doi.org/10.1016/j.patcog.2019.107151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing methods perform the efficient nearest neighbor search by mapping high-dimensional data to binary codes . Compared to projection-based hashing methods, hashing methods that adopt the clustering technique can encode the complex relationship of the data into binary codes . However, their search performance is affected by the boundary of the cluster. Two similar data points may be assigned to two different clusters and then encoded into two much different binary codes. In this paper, we propose a new hashing method based on the clustering technique and it can alleviate the effect from the cluster boundary. It is from an observation that the relative positions of any two close data points to each cluster center are close. An alternating optimization is developed to simultaneously discover the cluster structures of the data and learn the hash functions to preserve the relative positions of the data to each cluster center. To integrate the information in each cluster, the corresponding binary code of each data point is obtained by concatenating the substrings learnt by the hash functions in each cluster. The experiments show that our method is competitive to or better than the state-of-the-art hashing methods.},
  archive      = {J_PR},
  author       = {Zhenyu Weng and Yuesheng Zhu},
  doi          = {10.1016/j.patcog.2019.107151},
  journal      = {Pattern Recognition},
  pages        = {107151},
  shortjournal = {Pattern Recognition},
  title        = {Concatenation hashing: A relative position preserving method for learning binary codes},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MDFN: Multi-scale deep feature learning network for object
detection. <em>PR</em>, <em>100</em>, 107149. (<a
href="https://doi.org/10.1016/j.patcog.2019.107149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an innovative object detector by leveraging deep features learned in high-level layers. Compared with features produced in earlier layers, the deep features are better at expressing semantic and contextual information. The proposed deep feature learning scheme shifts the focus from concrete features with details to abstract ones with semantic information. It considers not only individual objects and local contexts but also their relationships by building a multi-scale deep feature learning network (MDFN). MDFN efficiently detects the objects by introducing information square and cubic inception modules into the high-level layers, which employs parameter-sharing to enhance the computational efficiency. MDFN provides a multi-scale object detector by integrating multi-box, multi-scale and multi-level technologies. Although MDFN employs a simple framework with a relatively small base network (VGG-16), it achieves better or competitive detection results than those with a macro hierarchical structure that is either very deep or very wide for stronger ability of feature extraction. The proposed technique is evaluated extensively on KITTI, PASCAL VOC, and COCO datasets, which achieves the best results on KITTI and leading performance on PASCAL VOC and COCO. This study reveals that deep features provide prominent semantic information and a variety of contextual contents, which contribute to its superior performance in detecting small or occluded objects. In addition, the MDFN model is computationally efficient, making a good trade-off between the accuracy and speed.},
  archive      = {J_PR},
  author       = {Wenchi Ma and Yuanwei Wu and Feng Cen and Guanghui Wang},
  doi          = {10.1016/j.patcog.2019.107149},
  journal      = {Pattern Recognition},
  pages        = {107149},
  shortjournal = {Pattern Recognition},
  title        = {MDFN: Multi-scale deep feature learning network for object detection},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep cascaded cross-modal correlation learning for
fine-grained sketch-based image retrieval. <em>PR</em>, <em>100</em>,
107148. (<a href="https://doi.org/10.1016/j.patcog.2019.107148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained Sketch-based Image Retrieval (FG-SBIR), which utilizes hand-drawn sketches to search the target object images, has recently drawn much attention. It is a challenging task because sketches and images belong to different modalities and sketches are highly abstract and ambiguous. Existing solutions to this problem either focus on visual comparisons between sketches and images and ignore the multimodal characteristics of annotated images, or treat the retrieval as a one-time process. In this paper, we formulate FG-SBIR as a coarse-to-fine process, and propose a Deep Cascaded Cross-modal Ranking Model (DCCRM) that can exploit all the beneficial multimodal information in sketches and annotated images and improve both the retrieval efficiency and the top- K ranked effectiveness. Our goal concentrates on constructing deep representations for sketches, images, and descriptions, and learning the optimized deep correlations across such different domains. Thus for a given query sketch, its relevant images with fine-grained instance-level similarities in a specific category can be returned, and the strict requirement of the instance-level retrieval for FG-SBIR is satisfied. Very positive results have been obtained in our experiments by using a large quantity of public data.},
  archive      = {J_PR},
  author       = {Yanfei Wang and Fei Huang and Yuejie Zhang and Rui Feng and Tao Zhang and Weiguo Fan},
  doi          = {10.1016/j.patcog.2019.107148},
  journal      = {Pattern Recognition},
  pages        = {107148},
  shortjournal = {Pattern Recognition},
  title        = {Deep cascaded cross-modal correlation learning for fine-grained sketch-based image retrieval},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). F-measure curves: A tool to visualize classifier performance
under imbalance. <em>PR</em>, <em>100</em>, 107146. (<a
href="https://doi.org/10.1016/j.patcog.2019.107146">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from imbalanced data is a challenging problem in many real-world machine learning applications due in part to the bias of performance in most classification systems. This bias may exist due to three reasons: (1) Classification systems are often optimized and compared using performance measurements that are unsuitable for imbalance problems; (2) most learning algorithms are designed and tested on a fixed imbalance level of data, which may differ from operational scenarios; (3) the preference of correct classification of classes is different from one application to another. This paper investigates specialized performance evaluation metrics and tools for imbalance problem, including scalar metrics that assume a given operating condition (skew level and relative preference of classes), and global evaluation curves or metrics that consider a range of operating conditions. We focus on the case in which the scalar metric F-measure is preferred over other scalar metrics, and propose a new global evaluation space for the F-measure that is analogous to the cost curves for expected cost. In this space, a classifier is represented as a curve that shows its performance over all of its decision thresholds and a range of possible imbalance levels for the desired preference of true positive rate to precision. Curves obtained in the F-measure space are compared to those of existing spaces (ROC, precision-recall and cost) and analogously to cost curves. The proposed F-measure space allows to visualize and compare classifiers’ performance under different operating conditions more easily than in ROC and precision-recall spaces. This space allows us to set the optimal decision threshold of a soft classifier and to select the best classifier among a group. This space also allows to empirically improve the performance obtained with ensemble learning methods specialized for class imbalance, by selecting and combining the base classifiers for ensembles using a modified version of the iterative Boolean combination algorithm that is optimized using the F-measure instead of AUC. Experiments on a real-world dataset for video face recognition show the advantages of evaluating and comparing different classifiers in the F-measure space versus ROC, precision-recall, and cost spaces. In addition, it is shown that the performance evaluated using the F-measure of Bagging ensemble method can improve considerably by using the modified iterative Boolean combination algorithm.},
  archive      = {J_PR},
  author       = {Roghayeh Soleymani and Eric Granger and Giorgio Fumera},
  doi          = {10.1016/j.patcog.2019.107146},
  journal      = {Pattern Recognition},
  pages        = {107146},
  shortjournal = {Pattern Recognition},
  title        = {F-measure curves: A tool to visualize classifier performance under imbalance},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graph regularized low-rank representation for submodule
clustering. <em>PR</em>, <em>100</em>, 107145. (<a
href="https://doi.org/10.1016/j.patcog.2019.107145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new submodule clustering method for imaging (2-D) data is proposed. Unlike most existing clustering methods that first convert such data into vectors as preprocessing, the proposed method arranges the data samples as lateral slices of a third-order tensor. Our algorithm is based on the union-of-free-submodules model and the samples are represented using t-product in the third-order tensor space. First, we impose a low-rank constraint on the representation tensor to capture the principle information of data. By incorporating manifold regularization into the tensor factorization, the proposed method explicitly exploits the local manifold structure of data. Meanwhile, a segmentation dependent term is employed to integrate the two pipeline steps of affinity learning and spectral clustering into a unified optimization framework. The proposed method can be efficiently solved based on the alternating direction method of multipliers and spectral clustering . Finally, a nonlinear extension is proposed to handle data drawn from a mixture of nonlinear manifolds. Extensive experimental results on five real-world image datasets confirm the effectiveness of the proposed methods.},
  archive      = {J_PR},
  author       = {Tong Wu},
  doi          = {10.1016/j.patcog.2019.107145},
  journal      = {Pattern Recognition},
  pages        = {107145},
  shortjournal = {Pattern Recognition},
  title        = {Graph regularized low-rank representation for submodule clustering},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic processing of historical arabic documents: A
comprehensive survey. <em>PR</em>, <em>100</em>, 107144. (<a
href="https://doi.org/10.1016/j.patcog.2019.107144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, there is a huge amount of Historical Arabic Documents (HAD) in the national libraries and archives around the world. Analyzing this type of data manually is a difficult and costly task. Thus, an automatic process is required to exploit these documents more rapidly. Processing historical documents is a recent research subject that has seen a remarkable growth in the last years. Processing Historical Arabic Documents is a particularly challenging problem. First, due to complicated nature of Arabic script compared to other scripts and second because the documents are ancient. This paper focuses on this difficult problem and provides a comprehensive survey of existing research work. First, we describe in detail the challenges making the automatic processing of Historical Arabic Documents a difficult task. Second, we classify this task into four applications of automatic processing of HAD: i) Analyze the document to extract the main text ii) Identify the writer of the document iii) Recognize some words or parts of the document in a reference dataset andiv) Retrieve and extract specific data from the document. For each application, existing approaches are surveyed and qualitatively described. Finally, we focus on available datasets and describe how they can be used in each application.},
  archive      = {J_PR},
  author       = {Mohamed Ibn Khedher and Houda Jmila and Mounim A. El-Yacoubi},
  doi          = {10.1016/j.patcog.2019.107144},
  journal      = {Pattern Recognition},
  pages        = {107144},
  shortjournal = {Pattern Recognition},
  title        = {Automatic processing of historical arabic documents: A comprehensive survey},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). AI-GAN: Asynchronous interactive generative adversarial
network for single image rain removal. <em>PR</em>, <em>100</em>,
107143. (<a href="https://doi.org/10.1016/j.patcog.2019.107143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single image rain removal plays an important role in numerous multimedia applications . Existing algorithms usually tackle the deraining problem by the way of signal removal, which lead to over-smoothness and generate unexpected artifacts in de-rained images. This paper addresses the deraining problem from a completely different perspective of feature-wise disentanglement, and introduces the interactions and constraints between two disentangled latent spaces. Specifically, we propose an Asynchronous Interactive Generative Adversarial Network (AI-GAN) to progressively disentangle the rainy image into background and rain spaces in feature level through a two-branch structure. Each branch employs a two-stage synthesis strategy and interacts asynchronously by exchanging feed-forward information and sharing feedback gradients, achieving complementary adversarial optimization. This ‘adversarial’ is not only the ‘adversarial’ between the generator and the discriminator , but also means that the two generators are entangled, and interact with each other in the optimization process. Extensive experimental results demonstrate that AI-GAN outperforms state-of-the-art deraining methods and benefits various typical multimedia applications such as Image/Video Coding, Action Recognition, and Person Re-identification.},
  archive      = {J_PR},
  author       = {Xin Jin and Zhibo Chen and Weiping Li},
  doi          = {10.1016/j.patcog.2019.107143},
  journal      = {Pattern Recognition},
  pages        = {107143},
  shortjournal = {Pattern Recognition},
  title        = {AI-GAN: Asynchronous interactive generative adversarial network for single image rain removal},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A fusion network for road detection via spatial propagation
and spatial transformation. <em>PR</em>, <em>100</em>, 107141. (<a
href="https://doi.org/10.1016/j.patcog.2019.107141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the fusion of image and point cloud data for road detection. To take advantage of both deep network and multi-modal data fusion, we propose an end-to-end road segmentation network called SPSTFN (Spatial Propagation and Spatial Transformation Fusion Network). Our method considers the model-level fusion and dual-view fusion in the network simultaneously for the first time. Specifically, the proposed SPSTFN contains three parts: the point cloud branch, the image branch, and the fusion block. Firstly, we design a simple but efficient lightweight network to handle the unordered and sparse point cloud to obtain a coarse representation of the road area. Then, an equal-resolution convolutional block is adopted to capture the low-level features of the image which are used to produce the heat diffusion coefficients of the joint anisotropic diffusion based spatial propagation model . Thirdly, we conduct the diffusion process on the coarse representation under the guidance of the learned low-level image features , both in the perspective and bird views, via the spatial transformation in the network. Finally, the diffusion results of the two views are then integrated to generate the final refined representation of the road area. The proposed fusion method is totally data-driven and parameter-free, and the whole fusion network can be trained with the standard BP (Back Propagation) algorithm. Without any additional process steps and pre-training, the proposed method obtains competitive results on the KITTI Road Benchmark.},
  archive      = {J_PR},
  author       = {Fei Yang and Huan Wang and Zhong Jin},
  doi          = {10.1016/j.patcog.2019.107141},
  journal      = {Pattern Recognition},
  pages        = {107141},
  shortjournal = {Pattern Recognition},
  title        = {A fusion network for road detection via spatial propagation and spatial transformation},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Human activity recognition from UAV-captured video
sequences. <em>PR</em>, <em>100</em>, 107140. (<a
href="https://doi.org/10.1016/j.patcog.2019.107140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research paper introduces a new approach for human activity recognition from UAV-captured video sequences. The proposed approach involves two phases: an offline phase and an inference phase. A scene stabilization step is performed together with these two phases. The offline phase aims to generate the human/non-human model as well as a human activity model using a convolutional neural network . The inference phase makes use of the already generated models in order to detect humans and recognize their activities. Our main contribution lies in adapting the convolutional neural networks, normally dedicated to the classification task , to detect humans. In addition, the classification of human activities is carried out according to two scenarios: An instant classification of video frames and an entire classification of the video sequences. Relying on an experimental evaluation of the proposed methods for human detection and human activity classification on the UCF-ARG dataset, we validated not only these contributions but also the performance of our methods compared to the existing ones.},
  archive      = {J_PR},
  author       = {Hazar Mliki and Fatma Bouhlel and Mohamed Hammami},
  doi          = {10.1016/j.patcog.2019.107140},
  journal      = {Pattern Recognition},
  pages        = {107140},
  shortjournal = {Pattern Recognition},
  title        = {Human activity recognition from UAV-captured video sequences},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Video super-resolution via pre-frame constrained and
deep-feature enhanced sparse reconstruction. <em>PR</em>, <em>100</em>,
107139. (<a href="https://doi.org/10.1016/j.patcog.2019.107139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new video super-resolution (SR) method that can generate high-quality and temporally coherent high-resolution (HR) videos. Starting from the traditional sparse reconstruction framework that works well for image SR, we improve it significantly from the following aspects to obtain an effect video SR method. Firstly, to enhance the temporal coherence between adjacent HR frames, once a HR frame is estimated, we use it to guide the sparse reconstruction of the next low-resolution frame. Secondly, instead of using just hand-craft features, we further incorporate deep features generated by VGG16 into our sparse reconstruction based video SR method. Thirdly, we constantly update the dictionary, which is the core of the sparse reconstruction, by making use of the previously estimated HR frame. Finally, after the HR video is reconstructed, we use a joint bilateral filter to post-process it to remove artifacts and transfer image details. Experiments demonstrate that the proposed four strategies effectively improve our final results. In most of the experiments of this paper, our results are better than those produced by the latest deep learning based approaches.},
  archive      = {J_PR},
  author       = {Qiuxia Lai and Yongwei Nie and Hanqiu Sun and Qiang Xu and Zhensong Zhang and Mingyu Xiao},
  doi          = {10.1016/j.patcog.2019.107139},
  journal      = {Pattern Recognition},
  pages        = {107139},
  shortjournal = {Pattern Recognition},
  title        = {Video super-resolution via pre-frame constrained and deep-feature enhanced sparse reconstruction},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Non-rigid object tracking via deep multi-scale
spatial-temporal discriminative saliency maps. <em>PR</em>,
<em>100</em>, 107130. (<a
href="https://doi.org/10.1016/j.patcog.2019.107130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel effective non-rigid object tracking framework based on the spatial-temporal consistent saliency detection. In contrast to most existing trackers that utilize a bounding box to specify the tracked target, the proposed framework can extract accurate regions of the target as tracking outputs . It achieves a better description of the non-rigid objects and reduces the background pollution for the tracking model. Furthermore, our model has several unique characteristics. First, a tailored fully convolutional neural network (TFCN) is developed to model the local saliency prior for a given image region, which not only provides the pixel-wise outputs but also integrates the semantic information. Second, a novel multi-scale multi-region mechanism is proposed to generate local saliency maps that effectively consider visual perceptions with different spatial layouts and scale variations. Subsequently, the local saliency maps are fused via a weighted entropy method , resulting in a discriminative saliency map. Finally, we present a non-rigid object tracking algorithm based on the predicted saliency maps. By utilizing a spatial-temporal consistent saliency map (STCSM), we conduct the target-background classification and use an online fine-tuning scheme for model updating. Extensive experiments demonstrate that the proposed algorithm achieves competitive performance in both saliency detection and visual tracking, especially outperforming other related trackers on the non-rigid object tracking datasets. Source codes and compared results are released at https://github.com/Pchank/TFCNTracker .},
  archive      = {J_PR},
  author       = {Pingping Zhang and Wei Liu and Dong Wang and Yinjie Lei and Hongyu Wang and Huchuan Lu},
  doi          = {10.1016/j.patcog.2019.107130},
  journal      = {Pattern Recognition},
  pages        = {107130},
  shortjournal = {Pattern Recognition},
  title        = {Non-rigid object tracking via deep multi-scale spatial-temporal discriminative saliency maps},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A paired sparse representation model for robust face
recognition from a single sample. <em>PR</em>, <em>100</em>, 107129. (<a
href="https://doi.org/10.1016/j.patcog.2019.107129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse representation-based classification (SRC) has been shown to achieve a high level of accuracy in face recognition (FR). However, matching faces captured in unconstrained video against a gallery with a single reference facial still per individual typically yields low accuracy. For improved robustness to intra-class variations, SRC techniques for FR have recently been extended to incorporate variational information from an external generic set into an auxiliary dictionary. Despite their success in handling linear variations , non-linear variations (e.g., pose and expressions) between probe and reference facial images cannot be accurately reconstructed with a linear combination of images in the gallery and auxiliary dictionaries because they do not share the same type of variations. In order to account for non-linear variations due to pose, a paired sparse representation model is introduced allowing for joint use of variational information and synthetic face images. The proposed model, called synthetic plus variational model , reconstructs a probe image by jointly using (1) a variational dictionary and (2) a gallery dictionary augmented with a set of synthetic images generated over a wide diversity of pose angles. The augmented gallery dictionary is then encouraged to pair the same sparsity pattern with the variational dictionary for similar pose angles by solving a newly formulated simultaneous sparsity-based optimization problem . Experimental results obtained on Chokepoint and COX-S2V datasets, using different face representations, indicate that the proposed approach can outperform state-of-the-art SRC-based methods for still-to-video FR with a single sample per person.},
  archive      = {J_PR},
  author       = {Fania Mokhayeri and Eric Granger},
  doi          = {10.1016/j.patcog.2019.107129},
  journal      = {Pattern Recognition},
  pages        = {107129},
  shortjournal = {Pattern Recognition},
  title        = {A paired sparse representation model for robust face recognition from a single sample},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CARs-lands: An associative classifier for large-scale
datasets. <em>PR</em>, <em>100</em>, 107128. (<a
href="https://doi.org/10.1016/j.patcog.2019.107128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Associative classifiers are one of the most efficient classifiers for large datasets. However, they are unsuitable to be directly used in large-scale data problems. Associative classifiers discover frequent/rare rules or both in order to produce an efficient classifier. Discovery rules need to explore a large solution space in a well-organized manner; hence, learning of the associative classification methods of large datasets is not suitable on large-scale datasets because of memory and time-complexity constraints. The proposed method, CARs-Lands, presents an efficient distributed associative classifier. In CARs-Lands, first, a modified dataset is generated. This new dataset has sub-datasets that are completely appropriate to produce classification association rules (CARs) in a parallel manner. The produced dataset by CARs-Lands contains two types of instances: main instances and neighbor instances. Main instances can be either real instances of training dataset or meta-instances, which are not in the training dataset; each main instance has several neighbor instances from the training dataset, which together form a sub-dataset. These sub-datasets are used for parallel local association rule mining. In CARs-Lands, local association rules lead to more accurate prediction, because each test instance is classified by the association rules of their nearest neighbors in the training datasets. The proposed approach is evaluated in terms of accuracy on six real-world large-scale datasets against five recent and well-known methods. Experiment results show that the proposed classification method has high prediction accuracy and is highly competitive when compared to other classification methods.},
  archive      = {J_PR},
  author       = {Mehrdad Almasi and Mohammad Saniee Abadeh},
  doi          = {10.1016/j.patcog.2019.107128},
  journal      = {Pattern Recognition},
  pages        = {107128},
  shortjournal = {Pattern Recognition},
  title        = {CARs-lands: An associative classifier for large-scale datasets},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Clustering social audiences in business information
networks. <em>PR</em>, <em>100</em>, 107126. (<a
href="https://doi.org/10.1016/j.patcog.2019.107126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Business information networks involve diverse users and rich content and have emerged as important platforms for enabling business intelligence and business decision making. A key step in an organizations business intelligence process is to cluster users with similar interests into social audiences and discover the roles they play within a business network. In this article, we propose a novel machine-learning approach, called CBIN, that co-clusters business information networks to discover and understand these audiences. The CBIN framework is based on co-factorization. The audience clusters are discovered from a combination of network structures and rich contextual information, such as node interactions and node-content correlations. Since what defines an audience cluster is data-driven, plus they often overlap, pre-determining the number of clusters is usually very difficult. Therefore, we have based CBIN on an overlapping clustering paradigm with a hold-out strategy to discover the optimal number of clusters given the underlying data. Experiments validate an outstanding performance by CBIN compared to other state-of-the-art algorithms on 13 real-world enterprise datasets.},
  archive      = {J_PR},
  author       = {Yu Zheng and Ruiqi Hu and Sai-fu Fung and Celina Yu and Guodong Long and Ting Guo and Shirui Pan},
  doi          = {10.1016/j.patcog.2019.107126},
  journal      = {Pattern Recognition},
  pages        = {107126},
  shortjournal = {Pattern Recognition},
  title        = {Clustering social audiences in business information networks},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep ensemble network using distance maps and body part
features for skeleton based action recognition. <em>PR</em>,
<em>100</em>, 107125. (<a
href="https://doi.org/10.1016/j.patcog.2019.107125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Human action recognition is a hot research topic in the field of computer vision . The availability of low cost depth sensors in the market made the extraction of reliable skeleton maps of human objects easier. This paper proposes three subnets, referred to as SNet, TNet , and BodyNet to capture diverse spatio-temporal dynamics for action recognition task. Specifically, SNet is used to capture pose dynamics from the distance maps in the spatial domain. The second subnet ( TNet ) captures the temporal dynamics along the sequence. The third net ( BodyNet ) extracts distinct features from the fine-grained body parts in the temporal domain. With the motivation of ensemble learning , a hybrid network, referred to as HNet , is modeled using two subnets ( TNet and BodyNet ) to capture robust temporal dynamics. Finally, SNet and HNet are fused as one ensemble network for action classification task . Our method achieves competitive results on three widely used datasets: UTD MHAD, UT Kinect and NTU RGB+D.},
  archive      = {J_PR},
  author       = {Naveenkumar M. and Domnic S.},
  doi          = {10.1016/j.patcog.2019.107125},
  journal      = {Pattern Recognition},
  pages        = {107125},
  shortjournal = {Pattern Recognition},
  title        = {Deep ensemble network using distance maps and body part features for skeleton based action recognition},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Correlation-aware adversarial domain adaptation and
generalization. <em>PR</em>, <em>100</em>, 107124. (<a
href="https://doi.org/10.1016/j.patcog.2019.107124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptation (DA) and domain generalization (DG) have emerged as a solution to the domain shift problem where the distribution of the source and target data is different. The task of DG is more challenging than DA as the target data is totally unseen during the training phase in DG scenarios. The current state-of-the-art employs adversarial techniques, however, these are rarely considered for the DG problem. Furthermore, these approaches do not consider correlation alignment which has been proven highly beneficial for minimizing domain discrepancy. In this paper, we propose a correlation-aware adversarial DA and DG framework where the features of the source and target data are minimized using correlation alignment along with adversarial learning. Incorporating the correlation alignment module along with adversarial learning helps to achieve a more domain agnostic model due to the improved ability to reduce domain discrepancy with unlabeled target data more effectively. Experiments on benchmark datasets serve as evidence that our proposed method yields improved state-of-the-art performance.},
  archive      = {J_PR},
  author       = {Mohammad Mahfujur Rahman and Clinton Fookes and Mahsa Baktashmotlagh and Sridha Sridharan},
  doi          = {10.1016/j.patcog.2019.107124},
  journal      = {Pattern Recognition},
  pages        = {107124},
  shortjournal = {Pattern Recognition},
  title        = {Correlation-aware adversarial domain adaptation and generalization},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Prototype learning and collaborative representation using
grassmann manifolds for image set classification. <em>PR</em>,
<em>100</em>, 107123. (<a
href="https://doi.org/10.1016/j.patcog.2019.107123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image set classification using manifolds is becoming increasingly more attractive since it considers non-Euclidean geometry. However, with the success of dictionary learning for image set classification using manifolds, how to learn an over-complete dictionary is still challenging. This paper proposes a novel prototype subspace learning method, in which a set of images is represented by a linear subspace and then mapped onto a Grassmann manifold. With this subspace representation, class prototypes and intra-class differences can be represented as principal components and variation subspaces, respectively. Isometric mapping further maps the manifolds into the symmetrical space via collaborative representation , which permits a closed-term solution. The proposed method is evaluated for face recognition, object recognition and action recognition. Extensive experimental results on the Honda, Extended YaleB, ETH-80 and Cambridge-Gesture datasets verify the superiority of the proposed method over the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Dong Wei and Xiaobo Shen and Quansen Sun and Xizhan Gao and Wenzhu Yan},
  doi          = {10.1016/j.patcog.2019.107123},
  journal      = {Pattern Recognition},
  pages        = {107123},
  shortjournal = {Pattern Recognition},
  title        = {Prototype learning and collaborative representation using grassmann manifolds for image set classification},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dissimilarity-based representations for one-class
classification on time series. <em>PR</em>, <em>100</em>, 107122. (<a
href="https://doi.org/10.1016/j.patcog.2019.107122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In several real-world classification problems it can be impractical to collect samples from classes other than the one of interest, hence the need for classifiers trained on a single class. There is a rich literature concerning binary and multi-class time series classification but less concerning one-class learning. In this study, we investigate the little-explored one-class time series classification problem. We represent time series as vectors of dissimilarities from a set of time series referred to as prototypes. Based on this approach, we evaluate a Cartesian product of 12 dissimilarity measures , and 8 prototype methods (strategies to select prototypes). Finally, a one-class nearest neighbor classifier is used on the dissimilarity-based representations (DBR). Experimental results show that DBR are competitive overall when compared with a strong baseline on the data-sets of the UCR/UEA archive. Additionally, DBR enable dimensionality reduction, and visual exploration of data-sets.},
  archive      = {J_PR},
  author       = {Stefano Mauceri and James Sweeney and James McDermott},
  doi          = {10.1016/j.patcog.2019.107122},
  journal      = {Pattern Recognition},
  pages        = {107122},
  shortjournal = {Pattern Recognition},
  title        = {Dissimilarity-based representations for one-class classification on time series},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel classification-selection approach for the self
updating of template-based face recognition systems. <em>PR</em>,
<em>100</em>, 107121. (<a
href="https://doi.org/10.1016/j.patcog.2019.107121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The boosting on the need of security notably increased the amount of possible facial recognition applications, especially due to the success of the Internet of Things (IoT) paradigm. However, although handcrafted and deep learning-inspired facial features reached a significant level of compactness and expressive power , the facial recognition performance still suffers from intra-class variations such as ageing, facial expressions, lighting changes, and pose. These variations cannot be captured in a single acquisition and require multiple acquisitions of long duration, which are expensive and need a high level of collaboration from the users. Among others, self-update algorithms have been proposed in order to mitigate these problems. Self-updating aims to add novel templates to the users’ gallery among the inputs submitted during system operations. Consequently, computational complexity and storage space tend to be among the critical requirements of these algorithms. The present paper deals with the above problems by a novel template-based self-update algorithm, able to keep over time the expressive power of a limited set of templates stored in the system database. The rationale behind the proposed approach is in the working hypothesis that a dominating mode characterises the features’ distribution given the client. Therefore, the key point is to select the best templates around that mode. We propose two methods, which are tested on systems based on handcrafted features and deep-learning-inspired autoencoders at the state-of-the-art. Three benchmark data sets are used. Experimental results confirm that, by effective and compact feature sets which can support our working hypothesis, the proposed classification-selection approaches overcome the problem of manual updating and, in case, stringent computational requirements.},
  archive      = {J_PR},
  author       = {Giulia Orrù and Gian Luca Marcialis and Fabio Roli},
  doi          = {10.1016/j.patcog.2019.107121},
  journal      = {Pattern Recognition},
  pages        = {107121},
  shortjournal = {Pattern Recognition},
  title        = {A novel classification-selection approach for the self updating of template-based face recognition systems},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic imposter based online instance matching for person
search. <em>PR</em>, <em>100</em>, 107120. (<a
href="https://doi.org/10.1016/j.patcog.2019.107120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person search aims to locate the target person matching a given query from a list of unconstrained whole images. It is a challenging task due to the unavailable bounding boxes of pedestrians, limited samples for each labeled identity and large amount of unlabeled persons in existing datasets. To address these issues, we propose a novel end-to-end learning framework for person search. The proposed framework settles pedestrian detection and person re-identification concurrently. To achieve the goal of co-learning and utilize the information of unlabeled persons, a novel yet extremely efficient Dynamic Imposter based Online Instance Matching (DI-OIM) loss is formulated. The DI-OIM loss is inspired by the observation that pedestrians appearing in the same image obviously have different identities. Thus we assign the unlabeled persons with dynamic pseudo-labels. The pseudo-labeled persons along with the labeled persons can be used to learn powerful feature representations. Experiments on CUHK-SYSU and PRW datasets demonstrate that our method outperforms other state-of-the-art algorithms. Moreover, it is superior and efficient in terms of memory capacity comparing with existing methods.},
  archive      = {J_PR},
  author       = {Ju Dai and Pingping Zhang and Huchuan Lu and Hongyu Wang},
  doi          = {10.1016/j.patcog.2019.107120},
  journal      = {Pattern Recognition},
  pages        = {107120},
  shortjournal = {Pattern Recognition},
  title        = {Dynamic imposter based online instance matching for person search},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generalized support vector data description for anomaly
detection. <em>PR</em>, <em>100</em>, 107119. (<a
href="https://doi.org/10.1016/j.patcog.2019.107119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional anomaly detection procedures assume that normal observations are obtained from a single distribution. However, due to the complexities of modern industrial processes, the observations may belong to multiple operating modes with different distributions. In such cases, traditional anomaly detection procedures may trigger false alarms while the process is indeed in another normally operating mode. We propose a generalized support vector-based anomaly detection procedure called generalized support vector data description which can be used to determine the anomalies in multimodal processes. The proposed procedure constructs hyperspheres for each class in order to include as many observations as possible and keep other class observations as far apart as possible. In addition, we introduce a generalized Bayesian framework which does not only consider the prior information from each mode, but also highlights the relationships among the modes. The effectiveness of the proposed procedure is demonstrated through various simulation studies and real-life applications.},
  archive      = {J_PR},
  author       = {Mehmet Turkoz and Sangahn Kim and Youngdoo Son and Myong K. Jeong and Elsayed A. Elsayed},
  doi          = {10.1016/j.patcog.2019.107119},
  journal      = {Pattern Recognition},
  pages        = {107119},
  shortjournal = {Pattern Recognition},
  title        = {Generalized support vector data description for anomaly detection},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Noise-robust dictionary learning with slack block-diagonal
structure for face recognition. <em>PR</em>, <em>100</em>, 107118. (<a
href="https://doi.org/10.1016/j.patcog.2019.107118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Strict ‘0-1’ block-diagonal structure has been widely used for learning structured representation in face recognition problems. However, it is questionable and unreasonable to assume the within-class representations are the same. To circumvent this problem, in this paper, we propose a slack block-diagonal (SBD) structure for representation where the target structure matrix is dynamically updated, yet its blockdiagonal nature is preserved. Furthermore, in order to depict the noise in face images more precisely, we propose a robust dictionary learning algorithm based on mixed-noise model by utilizing the above SBD structure (SBD 2 L). SBD 2 L considers that there exists two forms of noise in data which are drawn from Laplacian and Gaussion distribution, respectively. Moreover, SBD2L introduces a low-rank constraint on the representation matrix to enhance the dictionary’s robustness to noise. Extensive experiments on four benchmark databases show that the proposed SBD 2 L can achieve better classification results than several state-of-the-art dictionary learning methods.},
  archive      = {J_PR},
  author       = {Zhe Chen and Xiao-Jun Wu and He-Feng Yin and Josef Kittler},
  doi          = {10.1016/j.patcog.2019.107118},
  journal      = {Pattern Recognition},
  pages        = {107118},
  shortjournal = {Pattern Recognition},
  title        = {Noise-robust dictionary learning with slack block-diagonal structure for face recognition},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). UNIC: A fast nonparametric clustering. <em>PR</em>,
<em>100</em>, 107117. (<a
href="https://doi.org/10.1016/j.patcog.2019.107117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is among the tools for exploring, analyzing, and deriving information from data. In the case of large data sets, the real burden to the application of clustering algorithms can be their complexity and demand of control parameters. We present a new fast nonparametric clustering algorithm, UNIC, to address these challenges. To identify clusters, the algorithm evaluates the distances between selected points and other points in the set. While assessing these distances, it employs methods of robust statistics to identify the cluster borders. The performance of the proposed algorithm is assessed in an experimental study and compared with several existing clustering methods over a variety of benchmark data sets.},
  archive      = {J_PR},
  author       = {Nadiia Leopold and Oliver Rose},
  doi          = {10.1016/j.patcog.2019.107117},
  journal      = {Pattern Recognition},
  pages        = {107117},
  shortjournal = {Pattern Recognition},
  title        = {UNIC: A fast nonparametric clustering},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep reinforcement hashing with redundancy elimination for
effective image retrieval. <em>PR</em>, <em>100</em>, 107116. (<a
href="https://doi.org/10.1016/j.patcog.2019.107116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hashing is one of the most promising techniques in approximate nearest neighbor search due to its time efficiency and low cost in memory. Recently, with the help of deep learning, deep supervised hashing can perform representation learning and compact hash code learning jointly in an end-to-end style, and obtains better retrieval accuracy compared to non-deep methods. However, most deep hashing methods are trained with a pair-wise loss or triplet loss in a mini-batch style, which makes them inefficient at data sampling and cannot preserve the global similarity information. Besides that, many existing methods generate hash codes with redundant or even harmful bits, which is a waste of space and may lower the retrieval accuracy. In this paper, we propose a novel deep reinforcement hashing model with redundancy elimination called Deep Reinforcement De-Redundancy Hashing (DRDH), which can fully exploit large-scale similarity information and eliminate redundant hash bits with deep reinforcement learning. DRDH conducts hash code inference in a block-wise style, and uses Deep Q Network (DQN) to eliminate redundant bits. Very promising results have been achieved on four public datasets, i.e., CIFAR -10, NUS - WIDE, MS - COCO , and Open - Images - V 4, which demonstrate that our method can generate highly compact hash codes and yield better retrieval performance than those of state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Juexu Yang and Yuejie Zhang and Rui Feng and Tao Zhang and Weiguo Fan},
  doi          = {10.1016/j.patcog.2019.107116},
  journal      = {Pattern Recognition},
  pages        = {107116},
  shortjournal = {Pattern Recognition},
  title        = {Deep reinforcement hashing with redundancy elimination for effective image retrieval},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A robust statistics approach for plane detection in
unorganized point clouds. <em>PR</em>, <em>100</em>, 107115. (<a
href="https://doi.org/10.1016/j.patcog.2019.107115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Plane detection is a key component for many applications, such as industrial reverse engineering and self-driving cars. However, existing plane-detection techniques are sensitive to noise and to user-defined parameters. We introduce a fast deterministic technique for plane detection in unorganized point clouds that is robust to noise and virtually independent of parameter tuning. It is based on a novel planarity test drawn from robust statistics and on a split and merge strategy. Its parameter values are automatically adjusted to fit the local distribution of samples in the input dataset, thus leading to good reconstruction of even small planar regions. We demonstrate the effectiveness of our solution on several real datasets, comparing its performance to state-of-art plane detection techniques, and showing that it achieves better accuracy, while still being one of the fastest.},
  archive      = {J_PR},
  author       = {Abner M. C. Araújo and Manuel M. Oliveira},
  doi          = {10.1016/j.patcog.2019.107115},
  journal      = {Pattern Recognition},
  pages        = {107115},
  shortjournal = {Pattern Recognition},
  title        = {A robust statistics approach for plane detection in unorganized point clouds},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MobileFAN: Transferring deep hidden representation for face
alignment. <em>PR</em>, <em>100</em>, 107114. (<a
href="https://doi.org/10.1016/j.patcog.2019.107114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Facial landmark detection is a crucial prerequisite for many face analysis applications. Deep learning-based methods currently dominate the approach of addressing the facial landmark detection. However, such works generally introduce a large number of parameters, resulting in high memory cost. In this paper, we aim for a lightweight as well as effective solution to facial landmark detection. To this end, we propose an effective lightweight model, namely Mobile Face Alignment Network (MobileFAN), using a simple backbone MobileNetV2 as the encoder and three deconvolutional layers as the decoder. The proposed MobileFAN, with only 8\% of the model size and lower computational cost, achieves superior or equivalent performance compared with state-of-the-art models. Moreover, by transferring the geometric structural information of a face graph from a large complex model to our proposed MobileFAN through feature-aligned distillation and feature-similarity distillation, the performance of MobileFAN is further improved in effectiveness and efficiency for face alignment. Extensive experiment results on three challenging facial landmark estimation benchmarks including COFW, 300W and WFLW show the superiority of our proposed MobileFAN against state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Yang Zhao and Yifan Liu and Chunhua Shen and Yongsheng Gao and Shengwu Xiong},
  doi          = {10.1016/j.patcog.2019.107114},
  journal      = {Pattern Recognition},
  pages        = {107114},
  shortjournal = {Pattern Recognition},
  title        = {MobileFAN: Transferring deep hidden representation for face alignment},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deformable face net for pose invariant face recognition.
<em>PR</em>, <em>100</em>, 107113. (<a
href="https://doi.org/10.1016/j.patcog.2019.107113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unconstrained face recognition still remains a challenging task due to various factors such as pose, expression, illumination, partial occlusion , etc. In particular, the most significant appearance variations are stemmed from poses which leads to severe performance degeneration. In this paper, we propose a novel Deformable Face Net (DFN) to handle the pose variations for face recognition. The deformable convolution module attempts to simultaneously learn face recognition oriented alignment and identity-preserving feature extraction. The displacement consistency loss (DCL) is proposed as a regularization term to enforce the learnt displacement fields for aligning faces to be locally consistent both in the orientation and amplitude since faces possess strong structure. Moreover, the identity consistency loss (ICL) and the pose-triplet loss (PTL) are designed to minimize the intra-class feature variation caused by different poses and maximize the inter-class feature distance under the same poses. The proposed DFN can effectively handle pose invariant face recognition (PIFR). Extensive experiments show that the proposed DFN outperforms the state-of-the-art methods, especially on the datasets with large poses.},
  archive      = {J_PR},
  author       = {Mingjie He and Jie Zhang and Shiguang Shan and Meina Kan and Xilin Chen},
  doi          = {10.1016/j.patcog.2019.107113},
  journal      = {Pattern Recognition},
  pages        = {107113},
  shortjournal = {Pattern Recognition},
  title        = {Deformable face net for pose invariant face recognition},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ExprADA: Adversarial domain adaptation for facial expression
analysis. <em>PR</em>, <em>100</em>, 107111. (<a
href="https://doi.org/10.1016/j.patcog.2019.107111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a deep neural network based image-to-image translation for domain adaptation, which aims at finding translations between image domains. Despite recent GAN based methods showing promising results in image-to-image translation, they are prone to fail at preserving semantic information and maintaining image details during translation, which reduces their practicality on tasks such as facial expression synthesis. In this paper, we learn a framework with two training objectives: first, we propose a multi-domain image synthesis model, yielding a better recognition performance compared to other GAN based methods, with a focus on the data augmentation process; second, we explore the use of domain adaptation to transform the visual appearance of the images from different domains, with the detail of face characteristics (e.g., identity) well preserved. Doing so, the expression recognition model learned from the source domain can be generalized to the translated images from target domain, without the need for re-training a model for new target domain. Extensive experiments demonstrate that ExprADA shows significant improvements in facial expression recognition accuracy compared to state-of-the-art domain adaptation methods.},
  archive      = {J_PR},
  author       = {Behzad Bozorgtabar and Dwarikanath Mahapatra and Jean-Philippe Thiran},
  doi          = {10.1016/j.patcog.2019.107111},
  journal      = {Pattern Recognition},
  pages        = {107111},
  shortjournal = {Pattern Recognition},
  title        = {ExprADA: Adversarial domain adaptation for facial expression analysis},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Complex contourlet-CNN for polarimetric SAR image
classification. <em>PR</em>, <em>100</em>, 107110. (<a
href="https://doi.org/10.1016/j.patcog.2019.107110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A complex multiscale network named complex Contourlet convolutional neural network (complex Contourlet-CNN) is proposed for polarimetric synthetic aperture radar (PolSAR) image classification in this paper. In order to make full use of the phase information of PolSAR image , we redefine the conventional operations of CNN in complex field, and the data sets and parameters are always expressed through the complex matrixes in complex CNN. Moreover, the multiscale deep Contourlet filter banks are constructed to extract more robust discriminative features with multidirection, multiscale, and multiresolution properties, which can improve the performance of complex CNN by replacing filters of the first complex convolutional layer with the multiscale deep Contourlet filter banks. The Contourlet transform is used for helping the complex CNN network to capture abstract features in a certain direction and frequency band. Furthermore, the proposed network based on the multiscale geometric properties of Contourlet transform can retrieve the information in the region and direction corresponding to the extracted features. Experiments on different spatial resolutions and land coverings of Flevoland, San Francisco Bay, and Germany PolSAR images show that less training data is required and the performance of the proposed explainable deep learning method is comparable to that of the existing state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Lingling Li and Liyuan Ma and Licheng Jiao and Fang Liu and Qigong Sun and Jin Zhao},
  doi          = {10.1016/j.patcog.2019.107110},
  journal      = {Pattern Recognition},
  pages        = {107110},
  shortjournal = {Pattern Recognition},
  title        = {Complex contourlet-CNN for polarimetric SAR image classification},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Training data independent image registration using
generative adversarial networks and domain adaptation. <em>PR</em>,
<em>100</em>, 107109. (<a
href="https://doi.org/10.1016/j.patcog.2019.107109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical image registration is an important task in automated analysis of multimodal images and temporal data involving multiple patient visits. Conventional approaches, although useful for different image types, are time consuming. Of late, deep learning (DL) based image registration methods have been proposed that outperform traditional methods in terms of accuracy and time. However, DL based methods are heavily dependent on training data and do not generalize well when presented with images of different scanners or anatomies. We present a DL based approach that can perform medical image registration of one image type despite being trained with images of a different type. This is achieved by unsupervised domain adaptation in the registration process and allows for easier application to different datasets without extensive retraining. To achieve our objective we train a network that transforms the given input image pair to a latent feature space vector using autoencoders . The resultant encoded feature space is used to generate the registered images with the help of generative adversarial networks (GANs). This feature transformation ensures greater invariance to the input image type. Experiments on chest X-ray, retinal and brain MR images show that our method, trained on one dataset gives better registration performance for other datasets, outperforming conventional methods that do not incorporate domain adaptation .},
  archive      = {J_PR},
  author       = {Dwarikanath Mahapatra and Zongyuan Ge},
  doi          = {10.1016/j.patcog.2019.107109},
  journal      = {Pattern Recognition},
  pages        = {107109},
  shortjournal = {Pattern Recognition},
  title        = {Training data independent image registration using generative adversarial networks and domain adaptation},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Systematic review of 3D facial expression recognition
methods. <em>PR</em>, <em>100</em>, 107108. (<a
href="https://doi.org/10.1016/j.patcog.2019.107108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The three-dimensional representation of the human face has emerged as a viable and effective way to characterize the facial surface for expression classification purposes. The rapid progress in the area continually demands its up-to-date characterization to guide and support research decisions, specially for newcomer researchers. This systematic literature review focus on investigating three major aspects of 3D facial expression recognition methods: face representation, preprocessing and classification experiments. The investigation of 49 specialized studies revealed the preferential types of data and regions of interest for face representation in recent years, as well as a trend towards keypoint-independent methods. In addition, it brings to light current weaknesses regarding the report of preprocessing techniques and identifies challenges concerning the current possibility of fair comparison among multiple methods. The presented findings outline essential research decisions whose the regardful report is of great value to this research community.},
  archive      = {J_PR},
  author       = {Gilderlane Ribeiro Alexandre and José Marques Soares and George André Pereira Thé},
  doi          = {10.1016/j.patcog.2019.107108},
  journal      = {Pattern Recognition},
  pages        = {107108},
  shortjournal = {Pattern Recognition},
  title        = {Systematic review of 3D facial expression recognition methods},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Correlation classifiers based on data perturbation: New
formulations and algorithms. <em>PR</em>, <em>100</em>, 107106. (<a
href="https://doi.org/10.1016/j.patcog.2019.107106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a novel framework for a family of correlation classifiers that are reconstructed from uncertain convex programs under data perturbation. Under this framework, correlation classifiers are exploited from the pessimistic viewpoint under possible perturbation of data, and the max-min optimization problem is formulated by simplifying the original model in terms of adaptive uncertainty regions. The proposed model can be formulated as a minimization problem under proper conditions. The proximal majorization-minimization optimization (PMMO) based on Bregman divergences is devised to solve the proposed model that may be nonconvex or nonsmooth. It is found that using PMMO to solve the proposed model can exploit the convergence rate of the solution sequence in the nonconvex case. In the case of specific functions we can use the accelerated versions of first-order methods to solve the proposed model with convexity in order to make them have fast convergence rates in terms of the objective function. Extensive experiments on some data sets are conducted to demonstrate the feasibility and validity of the proposed model.},
  archive      = {J_PR},
  author       = {Zhizheng Liang and Xuewen Chen and Lei Zhang and Jin Liu and Yong Zhou},
  doi          = {10.1016/j.patcog.2019.107106},
  journal      = {Pattern Recognition},
  pages        = {107106},
  shortjournal = {Pattern Recognition},
  title        = {Correlation classifiers based on data perturbation: New formulations and algorithms},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Transferable heterogeneous feature subspace learning for
JPEG mismatched steganalysis. <em>PR</em>, <em>100</em>, 107105. (<a
href="https://doi.org/10.1016/j.patcog.2019.107105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Steganalysis is a technique that detects the presence of secret information in multimedia data. Many steganalysis algorithms have been proposed with high detection accuracy; however, the difference in statistical distribution between training and testing sets can cause mismatch problems, which will degrade the performance of traditional steganalysis algorithms. To solve this problem, we propose a transferable heterogeneous feature subspace learning (THFSL) algorithm for JPEG mismatched steganalysis. Our approach considers the feature space in each domain as a combination of the domain-independent features and the domain-related features. We use the transformation matrix to transfer both the domain-independent and domain-related features from the source and target domains to a common feature subspace, where each target sample can be better represented by a combination of source samples. By imposing low-rank constraints on the domain-independent features, the structures of data can be preserved, which can capture the intrinsic structures for discriminating cover and stego images . Our method can avoid a potentially negative transfer by using a sparse matrix to model the domain-related features and, thus, is more robust to different domain changes in mismatched steganalysis. Extensive experiments on various mismatched steganalysis tasks show the superiority of the proposed method over the state-of-the art methods.},
  archive      = {J_PR},
  author       = {Ju Jia and Liming Zhai and Weixiang Ren and Lina Wang and Yanzhen Ren and Lefei Zhang},
  doi          = {10.1016/j.patcog.2019.107105},
  journal      = {Pattern Recognition},
  pages        = {107105},
  shortjournal = {Pattern Recognition},
  title        = {Transferable heterogeneous feature subspace learning for JPEG mismatched steganalysis},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ensemble selection based on classifier prediction
confidence. <em>PR</em>, <em>100</em>, 107104. (<a
href="https://doi.org/10.1016/j.patcog.2019.107104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble selection is one of the most studied topics in ensemble learning because a selected subset of base classifiers may perform better than the whole ensemble system. In recent years, a great many ensemble selection methods have been introduced. However, many of these lack flexibility: either a fixed subset of classifiers is pre-selected for all test samples (static approach), or the selection of classifiers depends upon the performance of techniques that define the region of competence (dynamic approach). In this paper, we propose an ensemble selection method that takes into account each base classifier&#39;s confidence during classification and the overall credibility of the base classifier in the ensemble. In other words, a base classifier is selected to predict for a test sample if the confidence in its prediction is higher than its credibility threshold. The credibility thresholds of the base classifiers are found by minimizing the empirical 0–1 loss on the entire training observations. In this way, our approach integrates both the static and dynamic aspects of ensemble selection. Experiments on 62 datasets demonstrate that the proposed method achieves much better performance in comparison to some ensemble methods.},
  archive      = {J_PR},
  author       = {Tien Thanh Nguyen and Anh Vu Luong and Manh Truong Dang and Alan Wee-Chung Liew and John McCall},
  doi          = {10.1016/j.patcog.2019.107104},
  journal      = {Pattern Recognition},
  pages        = {107104},
  shortjournal = {Pattern Recognition},
  title        = {Ensemble selection based on classifier prediction confidence},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Active emulation of computer codes with gaussian processes –
application to remote sensing. <em>PR</em>, <em>100</em>, 107103. (<a
href="https://doi.org/10.1016/j.patcog.2019.107103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many fields of science and engineering rely on running simulations with complex and computationally expensive models to understand the involved processes in the system of interest. Nevertheless, the high cost involved hamper reliable and exhaustive simulations. Very often such codes incorporate heuristics that ironically make them less tractable and transparent. This paper introduces an active learning methodology for adaptively constructing surrogate models , i.e. emulators , of such costly computer codes in a multi-output setting. The proposed technique is sequential and adaptive, and is based on the optimization of a suitable acquisition function. It aims to achieve accurate approximations , model tractability, as well as compact and expressive simulated datasets. In order to achieve this, the proposed Active Multi-Output Gaussian Process Emulator (AMOGAPE) combines the predictive capacity of Gaussian Processes (GPs) with the design of an acquisition function that favors sampling in low density and fluctuating regions of the approximation functions. Comparing different acquisition functions, we illustrate the promising performance of the method for the construction of emulators with toy examples, as well as for a widely used remote sensing transfer code.},
  archive      = {J_PR},
  author       = {Daniel Heestermans Svendsen and Luca Martino and Gustau Camps-Valls},
  doi          = {10.1016/j.patcog.2019.107103},
  journal      = {Pattern Recognition},
  pages        = {107103},
  shortjournal = {Pattern Recognition},
  title        = {Active emulation of computer codes with gaussian processes – application to remote sensing},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Writer-aware CNN for parsimonious HMM-based offline
handwritten chinese text recognition. <em>PR</em>, <em>100</em>, 107102.
(<a href="https://doi.org/10.1016/j.patcog.2019.107102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, the hybrid convolutional neural network hidden Markov model (CNN-HMM) has been introduced for offline handwritten Chinese text recognition (HCTR) and has achieved state-of-the-art performance. However, modeling each of the large vocabulary of Chinese characters with a uniform and fixed number of hidden states requires high memory and computational costs and makes the tens of thousands of HMM state classes confusing. Another key issue of CNN-HMM for HCTR is the diversified writing style, which leads to model strain and a significant performance decline for specific writers. To address these issues, we propose a writer-aware CNN based on parsimonious HMM (WCNN-PHMM). First, PHMM is designed using a data-driven state-tying algorithm to greatly reduce the total number of HMM states, which not only yields a compact CNN by state sharing of the same or similar radicals among different Chinese characters but also improves the recognition accuracy due to the more accurate modeling of tied states and the lower confusion among them. Second, WCNN integrates each convolutional layer with one adaptive layer fed by a writer-dependent vector, namely, the writer code, to extract the irrelevant variability in writer information to improve recognition performance. The parameters of writer-adaptive layers are jointly optimized with other network parameters in the training stage, while a multiple-pass decoding strategy is adopted to learn the writer code and generate recognition results. Validated on the ICDAR 2013 competition of CASIA-HWDB database, the more compact WCNN-PHMM of a 7360-class vocabulary can achieve a relative character error rate (CER) reduction of 16.6\% over the conventional CNN-HMM without considering language modeling . By adopting a powerful hybrid language model (N-gram language model and recurrent neural network language model), the CER of WCNN-PHMM is reduced to 3.17\%. Moreover, the state-tying results of PHMM explicitly show the information sharing among similar characters and the confusion reduction of tied state classes. Finally, we visualize the learned writer codes and demonstrate the strong relationship with the writing styles of different writers. To the best of our knowledge, WCNN-PHMM yields the best results on the ICDAR 2013 competition set, demonstrating its power when enlarging the size of the character vocabulary.},
  archive      = {J_PR},
  author       = {Zi-Rui Wang and Jun Du and Jia-Ming Wang},
  doi          = {10.1016/j.patcog.2019.107102},
  journal      = {Pattern Recognition},
  pages        = {107102},
  shortjournal = {Pattern Recognition},
  title        = {Writer-aware CNN for parsimonious HMM-based offline handwritten chinese text recognition},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Disentangled representation learning and residual GAN for
age-invariant face verification. <em>PR</em>, <em>100</em>, 107097. (<a
href="https://doi.org/10.1016/j.patcog.2019.107097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the challenges in cross-age face recognition and verification is to effectively model the facial aging process. Despite the rapid advances in face-related areas, it is still very difficult for existing methods to simultaneously achieve accurate facial feature preservation and reliable aging during aging modeling. Aiming to address this issue, we introduce a Disentangled Representation learning and Residual Generative Adversarial Network (DR-RGAN) that represents the facial features without age interference, which is achieved by explicitly disentangling the facial features and age variation. An encoder-decoder structured generator produces aging images from unstructured facial representations by using the age characteristics provided separately. It can thus take the disentangled representation to preserve personal identity for face verification. Considering that pixel-based errors may cause a loss of detail, a VGG based content loss is further equipped to preferably preserve facial features. The discriminator is trained to distinguish the real from generated faces, carry out identification prediction, and leverage an age estimator to boost the aging accuracy. It is beneficial for obtaining more photorealistic and desirable aging effects, as well as more consistent face verification results. Extensive experiments on the CACD and LFW datasets demonstrate that our DR-RGAN generates pleasing aging imageries and achieves a high accuracy of face verification.},
  archive      = {J_PR},
  author       = {Shuyang Zhao and Jianwu Li and Jiaxing Wang},
  doi          = {10.1016/j.patcog.2019.107097},
  journal      = {Pattern Recognition},
  pages        = {107097},
  shortjournal = {Pattern Recognition},
  title        = {Disentangled representation learning and residual GAN for age-invariant face verification},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Handwriting posture prediction based on unsupervised model.
<em>PR</em>, <em>100</em>, 107093. (<a
href="https://doi.org/10.1016/j.patcog.2019.107093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Writing is an important basic skill for humans. To acquire such a skill, pupils often have to practice writing for several hours each day. However, different pupils usually possess distinct writing postures. Bad postures not only affect the speed and quality of writing, but also severely harm the healthy development of pupils’ spine and eyesight. Therefore, it is of key importance to identify or predict pupils’ writing postures and accordingly correct bad ones. In this paper, we formulate the problem of handwriting posture prediction for the first time. Further, we propose a neural network constructed with small convolution kernels to extract features from handwriting, and incorporate unsupervised learning and handwriting data analysis to predict writing postures. Extensive experiments reveal that our approach achieves an accuracy rate of 93.3\%, which is significantly higher than the 76.67\% accuracy of human experts.},
  archive      = {J_PR},
  author       = {Bailin Yang and Yulong Zhang and Zhenguang Liu and Xiaoheng Jiang and Mingliang Xu},
  doi          = {10.1016/j.patcog.2019.107093},
  journal      = {Pattern Recognition},
  pages        = {107093},
  shortjournal = {Pattern Recognition},
  title        = {Handwriting posture prediction based on unsupervised model},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semi-supervised cross-modal image generation with generative
adversarial networks. <em>PR</em>, <em>100</em>, 107085. (<a
href="https://doi.org/10.1016/j.patcog.2019.107085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal image generation is an important aspect of the multi-modal learning. Existing methods usually use the semantic feature to reduce the modality gap. Although these methods have achieved notable progress, there are still some limitations: (1) they usually use single modality information to learn the semantic feature ; (2) they require the training data to be paired. To overcome these problems, we propose a novel semi-supervised cross-modal image generation method, which consists of two semantic networks and one image generation network. Specifically, in the semantic networks , we use image modality to assist non-image modality for semantic feature learning by using a deep mutual learning strategy. In the image generation network, we introduce an additional discriminator to reduce the image reconstruction loss. By leveraging large amounts of unpaired data, our method can be trained in a semi-supervised manner. Extensive experiments demonstrate the effectiveness of the proposed method.},
  archive      = {J_PR},
  author       = {Dan Li and Changde Du and Huiguang He},
  doi          = {10.1016/j.patcog.2019.107085},
  journal      = {Pattern Recognition},
  pages        = {107085},
  shortjournal = {Pattern Recognition},
  title        = {Semi-supervised cross-modal image generation with generative adversarial networks},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Structured self-attention architecture for graph-level
representation learning. <em>PR</em>, <em>100</em>, 107084. (<a
href="https://doi.org/10.1016/j.patcog.2019.107084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, graph neural networks (GNNs) have shown to be effective in learning representative graph features. However, current pooling-based strategies for graph classification lack efficient utilization of graph representation information in which each node and layer have the same contribution to the output of graph-level representation. In this paper, we develop a novel architecture for extracting an effective graph representation by introducing structured multi-head self-attention in which the attention mechanism consists of three different forms, i.e., node-focused, layer-focused and graph-focused. In order to make full use of the information of graphs, the node-focused self-attention firstly aggregates neighbor node features with a scaled dot-product manner, and then the layer-focused and graph-focused self-attention serve as readout module to measure the importance of different nodes and layers to the model’s output. Moreover, it is able to improve the performance on graph classification tasks by combining these two self-attention mechanisms with base node-level GNNs. The proposed Structured Self-attention Architecture is evaluated on two kinds of graph benchmarks: bioinformatics datasets and social network datasets. Extensive experiments have demonstrated superior performance improvement to existing methods on predictive accuracy .},
  archive      = {J_PR},
  author       = {Xiaolong Fan and Maoguo Gong and Yu Xie and Fenlong Jiang and Hao Li},
  doi          = {10.1016/j.patcog.2019.107084},
  journal      = {Pattern Recognition},
  pages        = {107084},
  shortjournal = {Pattern Recognition},
  title        = {Structured self-attention architecture for graph-level representation learning},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Sign consistency for the linear programming discriminant
rule. <em>PR</em>, <em>100</em>, 107083. (<a
href="https://doi.org/10.1016/j.patcog.2019.107083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear discriminant analysis (LDA) is an important conventional model for data classification . Classical theory shows that LDA is Bayes consistent for a fixed data dimensionality p and a large training sample size n . However, in high-dimensional settings when p ≫ n , LDA is difficult due to the inconsistent estimation of the covariance matrix and the mean vectors of populations. Recently, a linear programming discriminant (LPD) rule was proposed for high-dimensional linear discriminant analysis, based on the sparsity assumption over the discriminant function . It is shown that the LPD rule is Bayes consistent in high-dimensional settings. In this paper, we further show that the LPD rule is sign consistent under the sparsity assumption. Such sign consistency ensures the LPD rule to select the optimal discriminative features for high-dimensional data classification problems. Evaluations on both synthetic and real data validate our result on the sign consistency of the LPD rule.},
  archive      = {J_PR},
  author       = {Zhen Zhang and Shengzheng Wang and Wei Bian},
  doi          = {10.1016/j.patcog.2019.107083},
  journal      = {Pattern Recognition},
  pages        = {107083},
  shortjournal = {Pattern Recognition},
  title        = {Sign consistency for the linear programming discriminant rule},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Wavelet-based segmentation on the sphere. <em>PR</em>,
<em>100</em>, 107081. (<a
href="https://doi.org/10.1016/j.patcog.2019.107081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmentation, a useful/powerful technique in pattern recognition, is the process of identifying object outlines within images. There are a number of efficient algorithms for segmentation in Euclidean space that depend on the variational approach and partial differential equation modelling. Wavelets have been used successfully in various problems in image processing , including segmentation, inpainting, noise removal, super-resolution image restoration, and many others. Wavelets on the sphere have been developed to solve such problems for data defined on the sphere, which arise in numerous fields such as cosmology and geophysics. In this work, we propose a wavelet-based method to segment images on the sphere, accounting for the underlying geometry of spherical data. Our method is a direct extension of the tight-frame based segmentation method used to automatically identify tube-like structures such as blood vessels in medical imaging . It is compatible with any arbitrary type of wavelet frame defined on the sphere, such as axisymmetric wavelets, directional wavelets, curvelets, and hybrid wavelet constructions. Such an approach allows the desirable properties of wavelets to be naturally inherited in the segmentation process . In particular, directional wavelets and curvelets, which were designed to efficiently capture directional signal content, provide additional advantages in segmenting images containing prominent directional and curvilinear features. We present several numerical experiments, applying our wavelet-based segmentation method , as well as the common K-means method, on real-world spherical images , including an Earth topographic map , a light probe image, solar data-sets, and spherical retina images. These experiments demonstrate the superiority of our method and show that it is capable of segmenting different kinds of spherical images, including those with prominent directional features. Moreover, our algorithm is efficient with convergence usually within a few iterations.},
  archive      = {J_PR},
  author       = {Xiaohao Cai and Christopher G.R. Wallis and Jennifer Y.H. Chan and Jason D. McEwen},
  doi          = {10.1016/j.patcog.2019.107081},
  journal      = {Pattern Recognition},
  pages        = {107081},
  shortjournal = {Pattern Recognition},
  title        = {Wavelet-based segmentation on the sphere},
  volume       = {100},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Depth-map completion for large indoor scene reconstruction.
<em>PR</em>, <em>99</em>, 107112. (<a
href="https://doi.org/10.1016/j.patcog.2019.107112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional Multi View Stereo (MVS) algorithms are often difficult to deal with large-scale indoor scene reconstruction, due to the photo-consistency measurement errors in weak textured regions, which are commonly exist in indoor scenes. To solve this limitation, in this paper we proposed a point cloud completion strategy that combines learning-based depth-map completion and geometry-based consistency filtering to fill large-area missing in depth-maps. The proposed method takes nonuniform and noisy MVS depth-map as input, and completes each depth-map individually. In the completion process, we first complete depth-maps using learning based method, and then filter each depth-map using depth consistency validation with its neighboring depth-maps. This depth-map completion and geometric filtering steps are performed iteratively until the number of depth points is converged. Experiments on large-scale indoor scenes and benchmark MVS datasets demonstrate the effectiveness of the proposed methods.},
  archive      = {J_PR},
  author       = {Hongmin Liu and Xincheng Tang and Shuhan Shen},
  doi          = {10.1016/j.patcog.2019.107112},
  journal      = {Pattern Recognition},
  pages        = {107112},
  shortjournal = {Pattern Recognition},
  title        = {Depth-map completion for large indoor scene reconstruction},
  volume       = {99},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic auto-weighted multi-view co-clustering. <em>PR</em>,
<em>99</em>, 107101. (<a
href="https://doi.org/10.1016/j.patcog.2019.107101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To exploit the complementary information of multi-view data, many weighted multi-view clustering methods have been proposed and have demonstrated impressive performance. However, most of these methods learn the view weights by introducing additional parameters, which can not be easily obtained in practice. Moreover, they all simply apply the learned weights on the original feature representation of each view, which may deteriorate the clustering performance in the case of high-dimensional data with redundancy and noise. In this paper, we extend information bottleneck co-clustering into a multi-view framework and propose a novel dynamic auto-weighted multi-view co-clustering algorithm to learn a group of weights for views with no need for extra weight parameters. By defining the new concept of the discrimination-compression rate, we quantify the importance of each view by evaluating the discriminativeness of the compact features (i.e., feature-wise clusters) of the views. Unlike existing weighted methods that impose weights on the original feature representations of multiple views, we apply the learned weights on the discriminative ones, which can reduce the negative impact of noisy features in high-dimensional data. To solve the optimization problem , a new two-step sequential method is designed. Experimental results on several datasets show the advantages of the proposed algorithm. To our knowledge, this is the first work incorporating weighting scheme into multi-view co-clustering framework.},
  archive      = {J_PR},
  author       = {Shizhe Hu and Xiaoqiang Yan and Yangdong Ye},
  doi          = {10.1016/j.patcog.2019.107101},
  journal      = {Pattern Recognition},
  pages        = {107101},
  shortjournal = {Pattern Recognition},
  title        = {Dynamic auto-weighted multi-view co-clustering},
  volume       = {99},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Large-scale multi-label classification using unknown
streaming images. <em>PR</em>, <em>99</em>, 107100. (<a
href="https://doi.org/10.1016/j.patcog.2019.107100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate the large-scale multi-label image classification problem when images with unknown novel classes come in stream during the training stage. It coincides with the practical requirement that usually novel classes are detected and used to update an existing image recognition system. Most existing multi-label image classification methods cannot be directly applied in this scenario, where the training and testing stages must have the same label set. In this paper, we proposed to learn a multi-label classifier and a novel-class detector alternately to solve this problem. The multi-label classifier is learned using a convolutional neural network (CNN) from the images in the known classes. We proposed a recurrent novel-class detector which is learned in the supervised manner to detect the novel class by encoding image features with the multi-label information. In the experiment, our method is evaluated on several large-scale multi-label benchmarks including MS COCO. The results show the proposed method is comparable to most existing multi-label image classification methods, which validate its efficacy when encountering streaming images with unknown classes.},
  archive      = {J_PR},
  author       = {Yu Zhang and Yin Wang and Xu-Ying Liu and Siya Mi and Min-Ling Zhang},
  doi          = {10.1016/j.patcog.2019.107100},
  journal      = {Pattern Recognition},
  pages        = {107100},
  shortjournal = {Pattern Recognition},
  title        = {Large-scale multi-label classification using unknown streaming images},
  volume       = {99},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Video classification and retrieval through spatio-temporal
radon features. <em>PR</em>, <em>99</em>, 107099. (<a
href="https://doi.org/10.1016/j.patcog.2019.107099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise in the availability of video content for access via the Internet and the medium of television has resulted in the development of automatic search procedures to retrieve the desired video. Searches can be simplified and hastened by employing automatic classification of videos. This paper proposes a descriptor called the Spatio-Temporal Histogram of Radon Projections (STHRP) for representing the temporal pattern of the contents of a video and demonstrates its application to video classification and retrieval. The first step in STHRP pattern computation is to represent any video as Three Orthogonal Planes (TOPs), i.e., XY, XT and YT, signifying the spatial and temporal contents. Frames corresponding to each plane are partitioned into overlapping blocks. Radon projections are obtained over these blocks at different orientations, resulting in weighted transform coefficients that are normalized and grouped into bins. Linear Discriminant Analysis (LDA) is performed over these coefficients of the TOPs to arrive at a compact description of STHRP pattern. Compared to existing classification and retrieval approaches, the proposed descriptor is highly robust to translation, rotation and illumination variations in videos. To evaluate the capabilities of the invariant STHRP pattern, we analyse the performance by conducting experiments on the UCF-101, HMDB51, 10contexts and TRECVID data sets for classification and retrieval using a bagged tree model. Experimental evaluation of video classification reveals that STHRP pattern can achieve classification rates of 96.15\%, 71.7\%, 93.24\% and 97.3\% for the UCF-101, HMDB51,10contexts and TRECVID 2005 data sets respectively. We conducted retrieval experiments on the TRECVID 2005, JHMDB and 10contexts data sets and the results revealed that STHRP pattern is able to provide the videos relevant to the user&#39;s query in minimal time (0.05s) with a good precision rate.},
  archive      = {J_PR},
  author       = {A. Sasithradevi and S. Mohamed Mansoor Roomi},
  doi          = {10.1016/j.patcog.2019.107099},
  journal      = {Pattern Recognition},
  pages        = {107099},
  shortjournal = {Pattern Recognition},
  title        = {Video classification and retrieval through spatio-temporal radon features},
  volume       = {99},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-model ensemble with rich spatial information for
object detection. <em>PR</em>, <em>99</em>, 107098. (<a
href="https://doi.org/10.1016/j.patcog.2019.107098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the development of deep learning networks and big data dimensionality, research on ensemble deep learning is receiving an increasing amount of attention. This paper takes the object detection task as the research domain and proposes an object detection framework based on ensemble deep learning. To guarantee the accuracy as well as real-time detection, the detector uses a Single Shot MultiBox Detector (SSD) as the backbone and combines ensemble learning with context modeling and multi-scale feature representation. Two modes were designed in order to achieve ensemble learning: NMS Ensembling and Feature Ensembling. In addition, to obtain contextual information, we used dilated convolution to expand the receptive field of the network. Compared with state-of-the-art detectors, our detector achieves superior performance on the PASCAL VOC set and the MS COCO set.},
  archive      = {J_PR},
  author       = {Jie Xu and Wei Wang and Hanyuan Wang and Jinhong Guo},
  doi          = {10.1016/j.patcog.2019.107098},
  journal      = {Pattern Recognition},
  pages        = {107098},
  shortjournal = {Pattern Recognition},
  title        = {Multi-model ensemble with rich spatial information for object detection},
  volume       = {99},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation of ergodicity limits of bag-of-words modeling for
guaranteed stochastic convergence. <em>PR</em>, <em>99</em>, 107094. (<a
href="https://doi.org/10.1016/j.patcog.2019.107094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper suggests an efficient dual ergodicity limits-based bag-of-words (DEL-BoW) modeling technique. The suggested DEL-BoW technique estimates two limits of ergodicity of a discrete random variable (drv) that is formed from the BoW classification performance of multiple runs. The first limit of ergodicity is estimated with a relatively larger ball of convergence to keep the drv shorter. Hence both robustness against random initialization and estimation of the optimal model-order are realized with a reduced number of iterations. Once the optimal model-order is estimated, the radius of ball of convergence is reduced and a second limit of ergodicity is estimated. Reducing the ball of convergence enlarges the size of the considered performance drv that enhances the classification performance. Experiments conducted on Caltech-101, Caltech-256, 15-Scenes, and Flower-102 datasets resulted in classification accuracy of 86.91\%, 72.57\%, 90.57\%, and 90.86\%, respectively. Comparison with state-of-the-art techniques shows the excellent performance of the DEL-BoW modeling process.},
  archive      = {J_PR},
  author       = {Ibrahim F. Jasim Ghalyan},
  doi          = {10.1016/j.patcog.2019.107094},
  journal      = {Pattern Recognition},
  pages        = {107094},
  shortjournal = {Pattern Recognition},
  title        = {Estimation of ergodicity limits of bag-of-words modeling for guaranteed stochastic convergence},
  volume       = {99},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Metric learning-based kernel transformer with triplets and
label constraints for feature fusion. <em>PR</em>, <em>99</em>, 107086.
(<a href="https://doi.org/10.1016/j.patcog.2019.107086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature fusion is an important skill to improve the performance in computer vision , the difficult problem of feature fusion is how to learn the complementary properties of different features. We recognize that feature fusion can benefit from kernel metric learning. Thus, a metric learning-based kernel transformer method for feature fusion is proposed in this paper. First, we propose a kernel transformer to convert data from data space to kernel space, which makes feature fusion and metric learning can be performed in the transformed kernel space. Second, in order to realize supervised learning, both triplets and label constraints are embedded into our model. Third, in order to solve the unknown kernel matrices, LogDet divergence is also introduced into our model. Finally, a complete optimization objective function is formed. Based on an alternating direction method of multipliers (ADMM) solver and the Karush-Kuhn-Tucker (KKT) theorem, the proposed optimization problem is solved with the rigorous theoretical analysis. Experimental results on image retrieval demonstrate the effectiveness of the proposed methods.},
  archive      = {J_PR},
  author       = {Shichao Kan and Linna Zhang and Zhihai He and Yigang Cen and Shiming Chen and Jikun Zhou},
  doi          = {10.1016/j.patcog.2019.107086},
  journal      = {Pattern Recognition},
  pages        = {107086},
  shortjournal = {Pattern Recognition},
  title        = {Metric learning-based kernel transformer with triplets and label constraints for feature fusion},
  volume       = {99},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient nearest neighbor search in high dimensional
hamming space. <em>PR</em>, <em>99</em>, 107082. (<a
href="https://doi.org/10.1016/j.patcog.2019.107082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fast approximate nearest neighbor search has been well studied for real-valued vectors, however, the methods for binary descriptors are less developed. The paper addresses this problem by resorting to the well established techniques in Euclidean space. To this end, the binary descriptors are firstly mapped into low dimensional float vectors under the condition that the neighborhood information in the original Hamming space could be preserved in the mapped Euclidean space as much as possible. Then, KD-Tree is used to partitioning the mapped Euclidean space in order to quickly find approximate nearest neighbors for a given query point . This is identical to filter out a subset of nearest neighbor candidates in the original Hamming space due to the property of neighborhood preserving. Finally, Hamming ranking is applied to the small number of candidates to find out the approximate nearest neighbor in the original Hamming space, with only a fraction of running time compared to the bruteforce linear scan. Our experiments demonstrate that the proposed method significantly outperforms the state of the arts, obtaining improved search accuracy at various speed up factors, e.g., at least 16\% improvement of search accuracy over previous methods (from 67.7\% to 83.7\%) when the search speed is 200 times faster than the linear scan for a one million database.},
  archive      = {J_PR},
  author       = {Bin Fan and Qingqun Kong and Baoqian Zhang and Hongmin Liu and Chunhong Pan and Jiwen Lu},
  doi          = {10.1016/j.patcog.2019.107082},
  journal      = {Pattern Recognition},
  pages        = {107082},
  shortjournal = {Pattern Recognition},
  title        = {Efficient nearest neighbor search in high dimensional hamming space},
  volume       = {99},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Fault recognition using an ensemble classifier based on
dempster–shafer theory. <em>PR</em>, <em>99</em>, 107079. (<a
href="https://doi.org/10.1016/j.patcog.2019.107079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Aiming at the poor performance of individual classifier in the field of fault recognition, in this paper, a new ensemble classifier is constructed to improve the classification accuracy by combining multiple classifiers based on Dempster–Shafer Theory (DST). However, in some specific cases, especially when dealing with the combination of conflicting evidences, the DST may produce counter-intuitive results and loss its advantages in combining classifiers. To solve this problem, a new improved combination method is proposed to alleviate the conflicts between evidences and a new ensemble technique is developed for the combination of individual classifiers, which can be well used in the design of accurate classifier ensembles. The main advantage of the proposed combination method is that of making the combination process more efficient and accurate by defining the objective weights and subjective weights of member classifiers’ outputs. To verify the effectiveness of the proposed combination method, four individual classifiers are selected for constructing ensemble classifier and tested on Tennessee-Eastman Process (TEP) datasets and UCI machine learning datasets. The experimental results show that the ensemble classifier can significantly improve the classification accuracy and outperforms all the selected individual classifiers. By comparison with other combination methods based on DST and some state-of-the-art ensemble methods, the proposed combination method shows better abilities in dealing with the combination of individual classifiers and outperforms the others in multiple performance measurements. Finally, the proposed ensemble classifier is applied to the fault recognition in real chemical plant.},
  archive      = {J_PR},
  author       = {Zhen Wang and Rongxi Wang and Jianmin Gao and Zhiyong Gao and Yanjie Liang},
  doi          = {10.1016/j.patcog.2019.107079},
  journal      = {Pattern Recognition},
  pages        = {107079},
  shortjournal = {Pattern Recognition},
  title        = {Fault recognition using an ensemble classifier based on Dempster–Shafer theory},
  volume       = {99},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Heterogeneous oblique random forest. <em>PR</em>,
<em>99</em>, 107078. (<a
href="https://doi.org/10.1016/j.patcog.2019.107078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision trees in random forests use a single feature in non-leaf nodes to split the data. Such splitting results in axis-parallel decision boundaries which may fail to exploit the geometric structure in the data. In oblique decision trees, an oblique hyperplane is employed instead of an axis-parallel hyperplane. Trees with such hyperplanes can better exploit the geometric structure to increase the accuracy of the trees and reduce the depth. The present realizations of oblique decision trees do not evaluate many promising oblique splits to select the best. In this paper, we propose a random forest of heterogeneous oblique decision trees that employ several linear classifiers at each non-leaf node on some top ranked partitions which are obtained via one-vs-all and two-hyperclasses based approaches and ranked based on ideal Gini scores and cluster separability. The oblique hyperplane that optimizes the impurity criterion is then selected as the splitting hyperplane for that node. We benchmark 190 classifiers on 121 UCI datasets. The results show that the oblique random forests proposed in this paper are the top 3 ranked classifiers with the heterogeneous oblique random forest being statistically better than all 189 classifiers in the literature.},
  archive      = {J_PR},
  author       = {Rakesh Katuwal and P.N. Suganthan and Le Zhang},
  doi          = {10.1016/j.patcog.2019.107078},
  journal      = {Pattern Recognition},
  pages        = {107078},
  shortjournal = {Pattern Recognition},
  title        = {Heterogeneous oblique random forest},
  volume       = {99},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning disentangling and fusing networks for face
completion under structured occlusions. <em>PR</em>, <em>99</em>,
107073. (<a href="https://doi.org/10.1016/j.patcog.2019.107073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face completion aims to generate semantically new pixels for missing facial components. It is a challenging generative task due to large variations of face appearance. This paper studies generative face completion under structured occlusions. We treat the face completion and corruption as disentangling and fusing processes of clean faces and occlusions, and propose a jointly disentangling and fusing Generative Adversarial Network (DF-GAN). First, three domains are constructed, corresponding to the distributions of occluded faces, clean faces and structured occlusions. The disentangling and fusing processes are formulated as the transformations between the three domains. Then the disentangling and fusing networks are built to learn the transformations from unpaired data, where the encoder-decoder structure is adopted and allows DF-GAN to simulate structure occlusions by modifying the latent representations. Finally, the disentangling and fusing processes are unified into a dual learning framework along with an adversarial strategy . The proposed method is evaluated on Meshface verification problem. Experimental results on four Meshface databases demonstrate the effectiveness of our proposed method for the face completion under structured occlusions.},
  archive      = {J_PR},
  author       = {Zhihang Li and Yibo Hu and Ran He and Zhenan Sun},
  doi          = {10.1016/j.patcog.2019.107073},
  journal      = {Pattern Recognition},
  pages        = {107073},
  shortjournal = {Pattern Recognition},
  title        = {Learning disentangling and fusing networks for face completion under structured occlusions},
  volume       = {99},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tooth morphometry using quasi-conformal theory. <em>PR</em>,
<em>99</em>, 107064. (<a
href="https://doi.org/10.1016/j.patcog.2019.107064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shape analysis is important in anthropology, bioarchaeology and forensic science for interpreting useful information from human remains. In particular, teeth are morphologically stable and hence well-suited for shape analysis. In this work, we propose a framework for tooth morphometry using quasi-conformal theory. Landmark-matching Teichmüller maps are used for establishing a 1-1 correspondence between tooth surfaces with prescribed anatomical landmarks. Then, a quasi-conformal statistical shape analysis model based on the Teichmüller mapping results is proposed for building a tooth classification scheme . We deploy our framework on a dataset of human premolars to analyze the tooth shape variation among genders and ancestries. Experimental results show that our method achieves much higher classification accuracy with respect to both gender and ancestry when compared to the existing methods. Furthermore, our model reveals the underlying tooth shape difference between different genders and ancestries in terms of the local geometric distortion and curvatures. In particular, our experiment suggests that the shape difference between genders is mostly captured by the conformal distortion but not the curvatures, while that between ancestries is captured by both of them.},
  archive      = {J_PR},
  author       = {Gary P.T. Choi and Hei Long Chan and Robin Yong and Sarbin Ranjitkar and Alan Brook and Grant Townsend and Ke Chen and Lok Ming Lui},
  doi          = {10.1016/j.patcog.2019.107064},
  journal      = {Pattern Recognition},
  pages        = {107064},
  shortjournal = {Pattern Recognition},
  title        = {Tooth morphometry using quasi-conformal theory},
  volume       = {99},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Collaborative and geometric multi-kernel learning for
multi-class classification. <em>PR</em>, <em>99</em>, 107050. (<a
href="https://doi.org/10.1016/j.patcog.2019.107050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multi-class classification is the problem of classifying the sample into one of three or more classes. In this paper, we propose an algorithm named collaborative and geometric multi-kernel learning (CGMKL) to classify multi-class data into corresponding class directly. The CGMKL uses the Multiple Empirical Kernel Learning (MEKL) to map the sample into multiple kernel spaces, and then trains the softmax function in each kernel space. To realize the collaborative learning , one regularization term, which controls the consistent outputs of samples in different kernel spaces, provides the complementary information. Moreover, another regularization term exhibits the classification result with a geometric feature by reducing the within-class distance of the outputs of samples. Extensive Experiments on the multi-class data sets validate the effectiveness of the CGMKL.},
  archive      = {J_PR},
  author       = {Zhe Wang and Zonghai Zhu and Dongdong Li},
  doi          = {10.1016/j.patcog.2019.107050},
  journal      = {Pattern Recognition},
  pages        = {107050},
  shortjournal = {Pattern Recognition},
  title        = {Collaborative and geometric multi-kernel learning for multi-class classification},
  volume       = {99},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Informative variable identifier: Expanding interpretability
in feature selection. <em>PR</em>, <em>98</em>, 107077. (<a
href="https://doi.org/10.1016/j.patcog.2019.107077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is nowadays an increasing interest in discovering relationships among input variables (also called features) from data to provide better interpretability , which yield more confidence in the solution and provide novel insights about the nature of the problem at hand. We propose a novel feature selection method, called Informative Variable Identifier (IVI), capable of identifying the informative variables and their relationships. It transforms the input-variable space distribution into a coefficient-feature space using existing linear classifiers or a more efficient weight generator that we also propose, Covariance Multiplication Estimator (CME). Informative features and their relationships are determined analyzing the joint distribution of these coefficients with resampling techniques. IVI and CME select the informative variables and then pass them on to any linear or nonlinear classifier. Experiments show that the proposed approach can outperform state-of-art algorithms in terms of feature identification capabilities, and even in classification performance when subsequent classifiers are used.},
  archive      = {J_PR},
  author       = {Sergio Muñoz-Romero and Arantza Gorostiaga and Cristina Soguero-Ruiz and Inmaculada Mora-Jiménez and José Luis Rojo-Álvarez},
  doi          = {10.1016/j.patcog.2019.107077},
  journal      = {Pattern Recognition},
  pages        = {107077},
  shortjournal = {Pattern Recognition},
  title        = {Informative variable identifier: Expanding interpretability in feature selection},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Eye landmarks detection via weakly supervised learning.
<em>PR</em>, <em>98</em>, 107076. (<a
href="https://doi.org/10.1016/j.patcog.2019.107076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extensive eye researches provide good results when images are captured under constrained environment. However, the accuracy of eye landmarks detection depends on explicit bounding-box of eye regions and drops severely in non-ideal conditions. This paper has proposed a novel weakly supervised eye landmarks detection algorithm with object detection and recurrent learning modules. The former is combined with faster R-CNN and is competent to detect bounding-box of facial components and initial positions of the eye. The recurrent module is employed for eye landmarks refinement using the initial eye shape. The proposed algorithm can augment training data effectively and our specific format data consist of supervised and weakly supervised samples. Supervised samples have ground truth of bounding-boxes, corresponding classification labels and eye landmarks coordinates while weakly supervised data does not have eye landmarks information. Despite trained on facial images, the proposed method can detect eyes in severely occluded or local view of facial images without prerequisites of face alignment. Further experiments are performed on our supervised testing set and some public datasets. Their results demonstrate the robustness and effectiveness of the proposed method.},
  archive      = {J_PR},
  author       = {Bin Huang and Renwen Chen and Qinbang Zhou and Wang Xu},
  doi          = {10.1016/j.patcog.2019.107076},
  journal      = {Pattern Recognition},
  pages        = {107076},
  shortjournal = {Pattern Recognition},
  title        = {Eye landmarks detection via weakly supervised learning},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning visual relationship and context-aware attention for
image captioning. <em>PR</em>, <em>98</em>, 107075. (<a
href="https://doi.org/10.1016/j.patcog.2019.107075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image captioning which automatically generates natural language descriptions for images has attracted lots of research attentions and there have been substantial progresses with attention based captioning methods. However, most attention-based image captioning methods focus on extracting visual information in regions of interest for sentence generation and usually ignore the relational reasoning among those regions of interest in an image. Moreover, these methods do not take into account previously attended regions which can be used to guide the subsequent attention selection. In this paper, we propose a novel method to implicitly model the relationship among regions of interest in an image with a graph neural network , as well as a novel context-aware attention mechanism to guide attention selection by fully memorizing previously attended visual content. Compared with the existing attention-based image captioning methods, ours can not only learn relation-aware visual representations for image captioning, but also consider historical context information on previous attention. We perform extensive experiments on two public benchmark datasets: MS COCO and Flickr30K, and the experimental results indicate that our proposed method is able to outperform various state-of-the-art methods in terms of the widely used evaluation metrics .},
  archive      = {J_PR},
  author       = {Junbo Wang and Wei Wang and Liang Wang and Zhiyong Wang and David Dagan Feng and Tieniu Tan},
  doi          = {10.1016/j.patcog.2019.107075},
  journal      = {Pattern Recognition},
  pages        = {107075},
  shortjournal = {Pattern Recognition},
  title        = {Learning visual relationship and context-aware attention for image captioning},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cluster-wise learning network for multi-person pose
estimation. <em>PR</em>, <em>98</em>, 107074. (<a
href="https://doi.org/10.1016/j.patcog.2019.107074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a cluster-wise feature aggregation network that exploits multi-level contextual association for multi-person pose estimation. The recent popular approach for pose estimation is extracting the local maximum response from each detection heatmap that trained for a specific keypoint type. To exploit more contextual information, our network simultaneously learns complementary semantic information to encourage the detected keypoints subject to a certain contextual constraint. Specifically, our network uses dense and sparse branches to generate paired multi-peak detection heatmaps for clusters of keypoints. To enhance the feature passing through the network, we aggregate information from different branches. The in-branch aggregation enriches the detection features in each branch by absorbing the holistic human region attention. The cross-branch aggregation further strengthens the detection features by fusing global and local context information between dense and sparse branches. We demonstrate competitive performance of our network on the benchmark dataset for multi-person pose estimation.},
  archive      = {J_PR},
  author       = {Ying Zhao and Zhiwei Luo and Changqin Quan and Dianchao Liu and Gang Wang},
  doi          = {10.1016/j.patcog.2019.107074},
  journal      = {Pattern Recognition},
  pages        = {107074},
  shortjournal = {Pattern Recognition},
  title        = {Cluster-wise learning network for multi-person pose estimation},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep conditional adaptation networks and label correlation
transfer for unsupervised domain adaptation. <em>PR</em>, <em>98</em>,
107072. (<a href="https://doi.org/10.1016/j.patcog.2019.107072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation aims to improve the performance of an unknown target domain by utilizing the knowledge learned from a related source domain. Given that the target label information is unavailable in the unsupervised situation, it is challenging to match the domain distributions and to transfer the source model to target applications. In this paper, a Deep Conditional Adaptation Networks (DCAN) is proposed to address the unsupervised domain adaptation problem. DCAN is implemented based on a deep neural network and attempts to learn domain invariant features based on the Wasserstein distance. A conditional adaptation strategy is presented to reduce the domain distribution discrepancy and to address category mismatch and class prior bias, which are usually ignored in marginal adaptation approaches. Furthermore, we propose a label correlation transfer algorithm to address the unsupervised issues, by generating more effective pseudo target labels based on the underlying cross-domain relationship. A set of comparative experiments were performed on standard domain adaptation benchmarks and the results demonstrate that the proposed DCAN outperforms previous adaptation methods.},
  archive      = {J_PR},
  author       = {Chen Yu and Yang Chunling and Zhang Yan and Li Yuze},
  doi          = {10.1016/j.patcog.2019.107072},
  journal      = {Pattern Recognition},
  pages        = {107072},
  shortjournal = {Pattern Recognition},
  title        = {Deep conditional adaptation networks and label correlation transfer for unsupervised domain adaptation},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep discriminative representation for generic palmprint
recognition. <em>PR</em>, <em>98</em>, 107071. (<a
href="https://doi.org/10.1016/j.patcog.2019.107071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-of-the-art palmprint recognition methods have achieved significant performances. However, most of the existing methods are focused on particular scenarios such as a specific illumination or being captured using a contact-based or contactless device. Therefore, these algorithms cannot meet the ever-changing complex application requirements. To resolve this issue, this paper proposes a generic framework to represent high-level discriminative features for multiple scenarios in palmprint recognition with learned discriminative deep convolutional networks named deep discriminative representation (DDR). We propose to learn discriminative deep convolutional networks with limited palmprint training data, which is utilized to extract deep discriminative features. Then, the collaborative representation based classifier is implemented for palmprint recognition, which is flexible and practical in numerous scenarios. The experimental results demonstrate that DDR produces the best recognition performance in generic palmprint recognition compared to other state-of-the-art methods. For contact-based palmprint recognition under different lighting sources, DDR achieved the best performance on the PolyU Multi-spectral database with M_R, M_B, M_G and M_NIR, respectively. As for contactless palmprint recognition, DDR obtained the highest results on the IITD and CASIA databases.},
  archive      = {J_PR},
  author       = {Shuping Zhao and Bob Zhang},
  doi          = {10.1016/j.patcog.2019.107071},
  journal      = {Pattern Recognition},
  pages        = {107071},
  shortjournal = {Pattern Recognition},
  title        = {Deep discriminative representation for generic palmprint recognition},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Separating structure from noise in large graphs using the
regularity lemma. <em>PR</em>, <em>98</em>, 107070. (<a
href="https://doi.org/10.1016/j.patcog.2019.107070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How can we separate structural information from noise in large graphs? To address this fundamental question, we propose a graph summarization approach based on Szemerédi’s Regularity Lemma, a well-known result in graph theory, which roughly states that every graph can be approximated by the union of a small number of random-like bipartite graphs called “regular pairs”. Hence, the Regularity Lemma provides us with a principled way to describe the essential structure of large graphs using a small amount of data. Our paper has several contributions: (i) We present our summarization algorithm which is able to reveal the main structural patterns in large graphs. (ii) We discuss how to use our summarization framework to efficiently retrieve from a database the top- k graphs that are most similar to a query graph. (iii) Finally, we evaluate the noise robustness of our approach in terms of the reconstruction error and the usefulness of the summaries in addressing the graph search task.},
  archive      = {J_PR},
  author       = {Marco Fiorucci and Francesco Pelosin and Marcello Pelillo},
  doi          = {10.1016/j.patcog.2019.107070},
  journal      = {Pattern Recognition},
  pages        = {107070},
  shortjournal = {Pattern Recognition},
  title        = {Separating structure from noise in large graphs using the regularity lemma},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A model-based gait recognition method with body pose and
human prior knowledge. <em>PR</em>, <em>98</em>, 107069. (<a
href="https://doi.org/10.1016/j.patcog.2019.107069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose in this paper a novel model-based gait recognition method, PoseGait . Gait recognition is a challenging and attractive task in biometrics . Early approaches to gait recognition were mainly appearance-based. The appearance-based features are usually extracted from human body silhouettes, which are easy to compute and have shown to be efficient for recognition tasks. Nevertheless silhouettes shape is not invariant to changes in clothing, and can be subject to drastic variations, due to illumination changes or other external factors. An alternative to silhouette-based features are model-based features. However, they are very challenging to acquire especially for low image resolution . In contrast to previous approaches, our model PoseGait exploits human 3D pose estimated from images by Convolutional Neural Network as the input feature for gait recognition. The 3D pose, defined by the 3D coordinates of joints of the human body, is invariant to view changes and other external factors of variation. We design spatio-temporal features from the 3D pose to improve the recognition rate. Our method is evaluated on two large datasets, CASIA B and CASIA E. The experimental results show that the proposed method can achieve state-of-the-art performance and is robust to view and clothing variations.},
  archive      = {J_PR},
  author       = {Rijun Liao and Shiqi Yu and Weizhi An and Yongzhen Huang},
  doi          = {10.1016/j.patcog.2019.107069},
  journal      = {Pattern Recognition},
  pages        = {107069},
  shortjournal = {Pattern Recognition},
  title        = {A model-based gait recognition method with body pose and human prior knowledge},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving retinal vessel segmentation with joint local loss
by matting. <em>PR</em>, <em>98</em>, 107068. (<a
href="https://doi.org/10.1016/j.patcog.2019.107068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Besides the binary segmentation, many retinal image segmentation methods also produce a score map, where a nonnegative score is assigned for each pixel to indicate the likelihood of being a vessel. This observation inspires us to propose a new approach as a post-processing step to improve existing methods by formulating segmentation as a matting problem. A trimap is obtained via a bi-level thresholding of the score map from existing methods, which is instrumental in focusing the attention to pixels of these unknown areas. A dedicated end-to-end matting algorithm is further developed to retrieve those vessel pixels in the unknown areas, and to produce the final vessel segmentation by minimizing global pixel loss and local matting loss. Our approach is shown to be particularly effective in rescuing thin and tiny vessels that may lead to disconnections of vessel fragments. Moreover, it is observed that our approach is capable of improving the overall segmentation performance across a broad range of existing methods.},
  archive      = {J_PR},
  author       = {He Zhao and Huiqi Li and Li Cheng},
  doi          = {10.1016/j.patcog.2019.107068},
  journal      = {Pattern Recognition},
  pages        = {107068},
  shortjournal = {Pattern Recognition},
  title        = {Improving retinal vessel segmentation with joint local loss by matting},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Keyword-based approach for recognizing fraudulent messages
by keystroke dynamics. <em>PR</em>, <em>98</em>, 107067. (<a
href="https://doi.org/10.1016/j.patcog.2019.107067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, many approaches that use keystroke dynamics in free text authentication have been proposed. The major drawback of the proposed approaches is that training generally requires several months, thereby resulting in low practicality. In this study, a method to detect U.S. English fraudulent messages by analyzing keyboard users&#39; keystroke dynamics is proposed. To the best of our knowledge, this is the first study to apply keystroke dynamics to detect fraudulent instant messages. In the proposed system, each user requires only approximately 20 min of training in U.S. English keystroke dynamics. Furthermore, a voting-based statistical classifier is presented to improve the recognition accuracy of instant messages and prevent phishing messages. Experimental results indicate that the proposed approach outperforms other relevant published methods in terms of shorter training time, fewer false alarms, and comparable recognition accuracy.},
  archive      = {J_PR},
  author       = {Cheng-Jung Tsai and Po-Hao Huang},
  doi          = {10.1016/j.patcog.2019.107067},
  journal      = {Pattern Recognition},
  pages        = {107067},
  shortjournal = {Pattern Recognition},
  title        = {Keyword-based approach for recognizing fraudulent messages by keystroke dynamics},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). 3D shape reconstruction from multifocus image fusion using a
multidirectional modified laplacian operator. <em>PR</em>, <em>98</em>,
107065. (<a href="https://doi.org/10.1016/j.patcog.2019.107065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multifocus image fusion techniques primarily emphasize human vision and machine perception to evaluate an image, which often ignore depth information contained in the focus regions. In this paper, a novel 3D shape reconstruction algorithm based on nonsubsampled shearlet transform (NSST) microscopic multifocus image fusion method is proposed to mine 3D depth information from the fusion process. The shift-invariant property of NSST guarantees the spatial corresponding relationship between the image sequence and its high-frequency subbands. Since the high-frequency components of an image represent the focus level of the image, a new multidirectional modified Laplacian (MDML) as the focus measure maps the high-frequency subbands to images of various levels of depth. Next, the initial 3D reconstruction result is obtained by using an optimal level selection strategy based on the summation of the multiscale Laplace responses to exploit these depth maps. Finally, an iterative edge repair method is implemented to refine the reconstruction result. The experimental results show that the proposed method has better performance, especially when the source images have low-contrast regions.},
  archive      = {J_PR},
  author       = {Tao Yan and Zhiguo Hu and Yuhua Qian and Zhiwei Qiao and Linyuan Zhang},
  doi          = {10.1016/j.patcog.2019.107065},
  journal      = {Pattern Recognition},
  pages        = {107065},
  shortjournal = {Pattern Recognition},
  title        = {3D shape reconstruction from multifocus image fusion using a multidirectional modified laplacian operator},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Clustering by connection center evolution. <em>PR</em>,
<em>98</em>, 107063. (<a
href="https://doi.org/10.1016/j.patcog.2019.107063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The determination of clustering centers generally depends on the observation scale that we use to analyze the data to be clustered. An inappropriate scale usually leads to unreasonable clustering centers and thus unreasonable results. In this study, we first consider the similarity of elements in the data as the connectivity of vertices in an undirected graph, then present the concept of connection center and regard it as the clustering center of the data. Based on this definition, the determination of clustering centers and the assignment of class become very simple, natural and effective. One more crucial finding is that the clustering centers of different scales can be obtained easily by different powers of a similarity matrix, and the change of power from small to large leads to the dynamic evolution of clustering centers from local (microscopic) to global (macroscopic). Further, in this process of evolution, the number of clusters changes discontinuously, which means that the presented method can automatically skip the unreasonable number of clusters, suggest appropriate observation scales and provide corresponding clustering results.},
  archive      = {J_PR},
  author       = {Xiurui Geng and Hairong Tang},
  doi          = {10.1016/j.patcog.2019.107063},
  journal      = {Pattern Recognition},
  pages        = {107063},
  shortjournal = {Pattern Recognition},
  title        = {Clustering by connection center evolution},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Supervised discrete cross-modal hashing based on kernel
discriminant analysis. <em>PR</em>, <em>98</em>, 107062. (<a
href="https://doi.org/10.1016/j.patcog.2019.107062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-modal hashing methods have drawn considerable attention due to the rapid growth of multi-modal data. To obtain efficient binary codes in a low-dimensional Hamming space, most existing approaches relaxed the discrete constraint, which could cause quantization loss and even result in performance degradation . In order to avoid this bottleneck, some scholars employed iterative discrete cyclic coordinate descent (DCC) to learn hash codes bit by bit, but this was very time-consuming. To counter this problem, a simple yet novel supervised discrete cross-modal hashing framework is represented to directly learn the unified discrete binary codes with a close-form, rather than bit by bit. Furthermore, to preserve label separability , the kernel discriminant analysis is fused into the proposed framework to enrich the discrimination ability of the learned binary codes. The goal of the proposed method is to obtain the common discrete binary codes of different modalities in a shared latent Hamming space so that the different modalities of a sample can be effectively connected. Experimental study shows the encouraging results of the proposed algorithm in comparisons to the state-of-the-art baseline approaches on four real-world datasets. Especially on the LabelMe dataset, the superiority of the proposed method is obvious, with an average improvement of 9\% over the best available results.},
  archive      = {J_PR},
  author       = {Yixian Fang and Yuwei Ren},
  doi          = {10.1016/j.patcog.2019.107062},
  journal      = {Pattern Recognition},
  pages        = {107062},
  shortjournal = {Pattern Recognition},
  title        = {Supervised discrete cross-modal hashing based on kernel discriminant analysis},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive appearance modeling via hierarchical entropy
analysis over multi-type features. <em>PR</em>, <em>98</em>, 107059. (<a
href="https://doi.org/10.1016/j.patcog.2019.107059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The descriptiveness of visual models is crucial for many image processing applications, however, it is still challenging to adaptively formulate such models. This paper systematically advocates a generic and adaptive appearance modeling method. For object-specific instances in images, it can adaptively generate a descriptive codebook by exploring the maximum discriminability of multi-type features. The key idea is to define feature-independent information entropy as a unified criterion to measure different features in a common entropy space. Towards this goal, a hierarchical maximum entropy (HME) model is proposed to conduct multi-feature selection based on the random forest . Specifically, the improved random forest replaces space-specific expression “distance similarity” with the statistical concept “entropy”. Thus, the random forest could integrate the subspace clustering results from different feature spaces. Such integration can not only afford adaptive feature selection and cross-feature error control but also be robust to possible weak/inconsistent feature expressions. To effectively construct a class-specific appearance model, a sparse codebook model, consisting of a series of weak learners, is proposed to further explore the maximum discriminative subspaces of each object class. Finally, a maximum entropy model is proposed to formulate appearance model by optimizing the probabilistic distributions of all the codebook words’ response parameters. To verify the efficacy and effectiveness of the proposed model, it is applied to multi-class image classification . We conduct extensive experiments and make comprehensive evaluations w.r.t several state-of-the-art methods over PASCAL VOC 2007, VOC 2012, Caltech 101 and Caltech 256 datasets. All the results demonstrate the advantages of the our method in terms of precision, robustness, flexibility, and versatility.},
  archive      = {J_PR},
  author       = {Jizhou Ma and Shuai Li and Hong Qin and Aimin Hao},
  doi          = {10.1016/j.patcog.2019.107059},
  journal      = {Pattern Recognition},
  pages        = {107059},
  shortjournal = {Pattern Recognition},
  title        = {Adaptive appearance modeling via hierarchical entropy analysis over multi-type features},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust one-dimensional calibration and localisation of a
distributed camera sensor network. <em>PR</em>, <em>98</em>, 107058. (<a
href="https://doi.org/10.1016/j.patcog.2019.107058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Calibration and localisation of a camera sensor network is an essential requirement for higher-level computer vision tasks, such as mapping or tracking. Additionally, distributed algorithms are being increasingly used to create scalable networks robust to node failure. We propose a distributed calibration and localisation algorithm based on multi-view one-dimensional calibration, alternating direction method of multipliers , and Gaussian belief propagation. Our algorithm builds upon an existing calibration algorithm by improving the numerical conditioning and non-linear refinement. We adapt this to a distributed network, bringing local estimates at each camera node to global consensus. Simulation and experimental results show that our algorithm performs with high accuracy compared to other calibration techniques, in centralised and distributed networks, and is well suited for practical applications.},
  archive      = {J_PR},
  author       = {Brendan Halloran and Prashan Premaratne and Peter James Vial},
  doi          = {10.1016/j.patcog.2019.107058},
  journal      = {Pattern Recognition},
  pages        = {107058},
  shortjournal = {Pattern Recognition},
  title        = {Robust one-dimensional calibration and localisation of a distributed camera sensor network},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accumulated and aggregated shifting of intensity for defect
detection on micro 3D textured surfaces. <em>PR</em>, <em>98</em>,
107057. (<a href="https://doi.org/10.1016/j.patcog.2019.107057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Micro three-dimensional (3D) textured surfaces are being designed for a lot of electronic products to improve appearance and user experience . Defects are, however, inevitably caused during industrial manufacture. They are difficult to be detected due to low contrast and unclear boundary between defect and irregular textured defect-free region. To achieve robust defect detection on micro 3D textured surfaces of industrial products, this paper proposes a probabilistic saliency framework with a novel feature enhancement mechanism. Two saliency features, absolute intensity deviation and local intensity aggregation, are designed to represent the pixel-level initial saliency. Based on these two features, an iterative framework, named accumulated and aggregated shifting of intensity (AASI), is proposed to shift the intensity of each pixel according to its saliency. Finally, all the pixels are classified as defective or defect-free by fitting the AASI iteration results to two statistical models, an exponential model and a linear model. Importantly, AASI procedure is unsupervised and training-free, so it does not rely on huge training data with time-consuming manual labels. Experimental results on a large-scale image dataset taken from real-world industrial product surfaces demonstrate that the proposed approach achieves state-of-the-art accuracy in industrial applications.},
  archive      = {J_PR},
  author       = {Yaping Yan and Shun’ichi Kaneko and Hirokazu Asano},
  doi          = {10.1016/j.patcog.2019.107057},
  journal      = {Pattern Recognition},
  pages        = {107057},
  shortjournal = {Pattern Recognition},
  title        = {Accumulated and aggregated shifting of intensity for defect detection on micro 3D textured surfaces},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A möbius transformation based model for fingerprint minutiae
variations. <em>PR</em>, <em>98</em>, 107054. (<a
href="https://doi.org/10.1016/j.patcog.2019.107054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When an individual’s fingerprint is scanned, although the global fingerprint pattern is unchanged, at the local level, between different scans the minutiae pattern may vary. Minutiae translation and rotation are caused by changing finger orientation and position shift during fingerprint acquisition. Minutiae patterns may also suffer non-linear distortion due to finger skin elasticity. Despite a variety of approaches to detecting deformations in fingerprint images , there has been no method available for capturing minutiae variations between two impressions of the same finger in a unified model. In this paper we address this issue by proposing a unified model to represent minutiae variations between fingerprint scans and formulate the changes to minutiae feature patterns. We identify the Möbius transformation as a good candidate for modelling minutiae translation, rotation and non-linear distortion, that is, different types of minutiae variations are described in a single model. Not only do we mathematically prove that the Möbius transformation based model is a unified model for capturing minutiae variations, but we also experimentally verify the effectiveness of this model using a public database.},
  archive      = {J_PR},
  author       = {James Moorfield and Song Wang and Wencheng Yang and Aseel Bedari and Peter Van Der Kamp},
  doi          = {10.1016/j.patcog.2019.107054},
  journal      = {Pattern Recognition},
  pages        = {107054},
  shortjournal = {Pattern Recognition},
  title        = {A möbius transformation based model for fingerprint minutiae variations},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-stage adaptive regression for online activity
recognition. <em>PR</em>, <em>98</em>, 107053. (<a
href="https://doi.org/10.1016/j.patcog.2019.107053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online activity recognition which aims to detect and recognize activity instantly from a continuous video stream is a key technology in human-robot interaction. However, the partial activity observation problem, mainly due to the incomplete sequence acquisition, makes it greatly challenging. This paper proposes a novel approach, named Multi-stage Adaptive Regression (MAR), for online activity recognition with the main focus on addressing the partial observation problem. Specifically, the MAR framework delicately assembles overlapped activity observations to improve its robustness against arbitrary activity segments. Then multiple score functions corresponding to each specific performance stage are collaboratively learned via a adaptive label strategy to enhance its power of discriminating similar partial activities. Moreover, the Online Human Interaction (OHI) database is constructed to evaluate the online activity recognition in human interaction scenarios. Extensive experimental evaluations on the Multi-Modal Action Detection (MAD) database and the OHI database show that the MAR method achieves an outstanding performance over the state-of-the-art approaches.},
  archive      = {J_PR},
  author       = {Bangli Liu and Haibin Cai and Zhaojie Ju and Honghai Liu},
  doi          = {10.1016/j.patcog.2019.107053},
  journal      = {Pattern Recognition},
  pages        = {107053},
  shortjournal = {Pattern Recognition},
  title        = {Multi-stage adaptive regression for online activity recognition},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discriminant component analysis via distance correlation
maximization. <em>PR</em>, <em>98</em>, 107052. (<a
href="https://doi.org/10.1016/j.patcog.2019.107052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the following study, an innovative supervised dimensionality reduction technique is proposed. dCor-based Dimensionality Reduction or dDR technique is based on distance correlation; a powerful correlation measure which is applicable to arbitrary-dimensional random variables . By projecting the samples to a lower dimensional space, dDR maximizes the correlation between explanatory and response variables. The proposed dDR algorithm can be easily implemented and it is computationally efficient. Moreover, it has a closed-form and a simple solution which makes it significantly effective in many different applications. In order to apply the proposed technique on non-linear problems, the kernel version of the dDR is also derived. Extensive analyses and empirical experiments across various visualization, classification, and regression tasks indicate that our algorithm is the method of choice; as it offers statistically superior results in comparison with other state-of-the-art approaches in the literature.},
  archive      = {J_PR},
  author       = {Lida Abdi and Ali Ghodsi},
  doi          = {10.1016/j.patcog.2019.107052},
  journal      = {Pattern Recognition},
  pages        = {107052},
  shortjournal = {Pattern Recognition},
  title        = {Discriminant component analysis via distance correlation maximization},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning image compressed sensing with sub-pixel
convolutional generative adversarial network. <em>PR</em>, <em>98</em>,
107051. (<a href="https://doi.org/10.1016/j.patcog.2019.107051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compressed sensing (CS) is a new technology to reconstruct image from randomized measurements, but the reconstruction procedure involves a time-consuming iterative optimization. In addition, the reconstruction quality becomes poor in low sampling rate . In order to alleviate these issues of the conventional CS image reconstruction, we propose a novel sub-pixel convolutional generative adversarial network (GAN) to learn compressed sensing reconstruction of images. The generator constructs the sub-pixel convolutional network to learn the explicit mapping from the low-dimensional measurement vector to the high-dimensional reconstruction, in which a compound loss, including reconstruction loss, measurement loss and adversarial loss, is designed to guide the network learning. By means of the adversarial training with discriminator , the generator can learn the inherent image distribution and improve the reconstruction quality. Moreover, the test image can be fast reconstructed by simply passing the low-dimensional measurement vector through the generator network . The proposed algorithm is tested on MNIST, F-MNIST and CelebA datasets, and the experimental results show that it is superior to some state-of-the-art deep learning based and iterative optimization based algorithms, in terms of both time complexity and reconstruction quality.},
  archive      = {J_PR},
  author       = {Yubao Sun and Jiwei Chen and Qingshan Liu and Guangcan Liu},
  doi          = {10.1016/j.patcog.2019.107051},
  journal      = {Pattern Recognition},
  pages        = {107051},
  shortjournal = {Pattern Recognition},
  title        = {Learning image compressed sensing with sub-pixel convolutional generative adversarial network},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A baseline regularization scheme for transfer learning with
convolutional neural networks. <em>PR</em>, <em>98</em>, 107049. (<a
href="https://doi.org/10.1016/j.patcog.2019.107049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In inductive transfer learning , fine-tuning pre-trained convolutional networks substantially outperforms training from scratch. When using fine-tuning, the underlying assumption is that the pre-trained model extracts generic features, which are at least partially relevant for solving the target task, but would be difficult to extract from the limited amount of data available on the target task. However, besides the initialization with the pre-trained model and the early stopping, there is no mechanism in fine-tuning for retaining the features learned on the source task. In this paper, we investigate several regularization schemes that explicitly promote the similarity of the final solution with the initial model. We show the benefit of having an explicit inductive bias towards the initial model. We eventually recommend that the baseline protocol for transfer learning should rely on a simple L 2 penalty using the pre-trained model as a reference.},
  archive      = {J_PR},
  author       = {Xuhong Li and Yves Grandvalet and Franck Davoine},
  doi          = {10.1016/j.patcog.2019.107049},
  journal      = {Pattern Recognition},
  pages        = {107049},
  shortjournal = {Pattern Recognition},
  title        = {A baseline regularization scheme for transfer learning with convolutional neural networks},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning binary code for fast nearest subspace search.
<em>PR</em>, <em>98</em>, 107040. (<a
href="https://doi.org/10.1016/j.patcog.2019.107040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subspace is widely used to represent objects under different viewpoints, illuminations, identities, and more. Due to the growing amount and dimensionality of visual contents, fast search in a large-scale database with high-dimensional subspaces is an important task in many applications, such as image retrieval , clustering, video retrieval, and visual recognition. This can be facilitated by approximate nearest subspace (ANS) search which requires effective subspace representation. All existing methods for this problem represent a subspace by a point in the Euclidean or the Grassmannian space before applying the approximate nearest neighbor (ANN) search. However, the efficiency of these methods is not guaranteed because the subspace representation step can be very time consuming when coping with high-dimensional data. Moreover, the subspace to point transforming process may cause subspace structural information loss which influences the search accuracy. In this paper, we present a new approach for hashing-based ANS search which can directly binarize a subspace without transforming it into a vector. The proposed method learns the binary codes for subspaces following a similarity preserving criterion, and simultaneously leverages the learned binary codes to train matrix classifiers as hash functions . Experiments on face and action recognition and video retrieval applications show that our method outperforms several state-of-the-art methods in both efficiency and accuracy. Moreover, we also compare our method with vector-based hashing methods . Results also show the superiority of our subspace matrix based search scheme.},
  archive      = {J_PR},
  author       = {Lei Zhou and Xiao Bai and Xianglong Liu and Jun Zhou and Edwin R. Hancock},
  doi          = {10.1016/j.patcog.2019.107040},
  journal      = {Pattern Recognition},
  pages        = {107040},
  shortjournal = {Pattern Recognition},
  title        = {Learning binary code for fast nearest subspace search},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fine-grained action segmentation using the semi-supervised
action GAN. <em>PR</em>, <em>98</em>, 107039. (<a
href="https://doi.org/10.1016/j.patcog.2019.107039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we address the problem of continuous fine-grained action segmentation, in which multiple actions are present in an unsegmented video stream. The challenge for this task lies in the need to represent the hierarchical nature of the actions and to detect the transitions between actions, allowing us to localise the actions within the video effectively. We propose a novel recurrent semi-supervised Generative Adversarial Network (GAN) model for continuous fine-grained human action segmentation. Temporal context information is captured via a novel Gated Context Extractor (GCE) module, composed of gated attention units, that directs the queued context information through the generator model , for enhanced action segmentation. The GAN is made to learn features in a semi-supervised manner, enabling the model to perform action classification jointly with the standard, unsupervised, GAN learning procedure. We perform extensive evaluations on different architectural variants to demonstrate the importance of the proposed network architecture , and show that it is capable of outperforming current state-of-the-art on three challenging datasets: 50 Salads, MERL Shopping and Georgia Tech Egocentric Activities dataset.},
  archive      = {J_PR},
  author       = {Harshala Gammulle and Simon Denman and Sridha Sridharan and Clinton Fookes},
  doi          = {10.1016/j.patcog.2019.107039},
  journal      = {Pattern Recognition},
  pages        = {107039},
  shortjournal = {Pattern Recognition},
  title        = {Fine-grained action segmentation using the semi-supervised action GAN},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Underwater scene prior inspired deep underwater image and
video enhancement. <em>PR</em>, <em>98</em>, 107038. (<a
href="https://doi.org/10.1016/j.patcog.2019.107038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In underwater scenes, wavelength-dependent light absorption and scattering degrade the visibility of images and videos. The degraded underwater images and videos affect the accuracy of pattern recognition, visual understanding, and key feature extraction in underwater scenes. In this paper, we propose an underwater image enhancement convolutional neural network (CNN) model based on underwater scene prior, called UWCNN. Instead of estimating the parameters of underwater imaging model, the proposed UWCNN model directly reconstructs the clear latent underwater image, which benefits from the underwater scene prior which can be used to synthesize underwater image training data. Besides, based on the light-weight network structure and effective training data, our UWCNN model can be easily extended to underwater videos for frame-by-frame enhancement. Specifically, combining an underwater imaging physical model with optical properties of underwater scenes, we first synthesize underwater image degradation datasets which cover a diverse set of water types and degradation levels. Then, a light-weight CNN model is designed for enhancing each underwater scene type, which is trained by the corresponding training data. At last, this UWCNN model is directly extended to underwater video enhancement. Experiments on real-world and synthetic underwater images and videos demonstrate that our method generalizes well to different underwater scenes.},
  archive      = {J_PR},
  author       = {Chongyi Li and Saeed Anwar and Fatih Porikli},
  doi          = {10.1016/j.patcog.2019.107038},
  journal      = {Pattern Recognition},
  pages        = {107038},
  shortjournal = {Pattern Recognition},
  title        = {Underwater scene prior inspired deep underwater image and video enhancement},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Spatio-temporal deformable 3D ConvNets with attention for
action recognition. <em>PR</em>, <em>98</em>, 107037. (<a
href="https://doi.org/10.1016/j.patcog.2019.107037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The irregularity of human actions poses great challenges in video action recognition . Recently, 3D ConvNet methods have shown promising performance at modelling the motion and appearance information. However, the fixed geometric structure of 3D convolution filters largely limits the learning capacity for video action recognition . To address this problem, this paper proposes a spatio-temporal deformable ConvNet module with an attention mechanism, which takes into consideration the mutual correlations in both temporal and spatial domains, to effectively capture the long-range and long-distance dependencies in the video actions. Our attention based deformable module, as a generic module for 3D ConvNets, can adaptively learn more accurate spatio-temporal offsets to model the action irregularity. The experiments on two popular datasets (UCF-101 and HMDB-51) demonstrate that our module significantly outperforms the state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Jun Li and Xianglong Liu and Mingyuan Zhang and Deqing Wang},
  doi          = {10.1016/j.patcog.2019.107037},
  journal      = {Pattern Recognition},
  pages        = {107037},
  shortjournal = {Pattern Recognition},
  title        = {Spatio-temporal deformable 3D ConvNets with attention for action recognition},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep-person: Learning discriminative deep features for
person re-identification. <em>PR</em>, <em>98</em>, 107036. (<a
href="https://doi.org/10.1016/j.patcog.2019.107036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person re-identification (Re-ID) requires discriminative features focusing on the full person to cope with inaccurate person bounding box detection, background clutter, and occlusion. Many recent person Re-ID methods attempt to learn such features describing full person details via part-based feature representation. However, the spatial context between these parts is ignored for the independent extractor on each separate part. In this paper, we propose to apply Long Short-Term Memory (LSTM) in an end-to-end way to model the pedestrian, seen as a sequence of body parts from head to foot. Integrating the contextual information strengthens the discriminative ability of local feature aligning better to full person. We also leverage the complementary information between local and global feature. Furthermore, we integrate both identification task and ranking task in one network, where a discriminative embedding and a similarity measurement are learned concurrently. This results in a novel three-branch framework named Deep-Person, which learns highly discriminative features for person Re-ID. Experimental results demonstrate that Deep-Person outperforms the state-of-the-art methods by a large margin on three challenging datasets including Market-1501, CUHK03, and DukeMTMC-reID.},
  archive      = {J_PR},
  author       = {Xiang Bai and Mingkun Yang and Tengteng Huang and Zhiyong Dou and Rui Yu and Yongchao Xu},
  doi          = {10.1016/j.patcog.2019.107036},
  journal      = {Pattern Recognition},
  pages        = {107036},
  shortjournal = {Pattern Recognition},
  title        = {Deep-person: Learning discriminative deep features for person re-identification},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). LSDSAR, a markovian a contrario framework for line segment
detection in SAR images. <em>PR</em>, <em>98</em>, 107034. (<a
href="https://doi.org/10.1016/j.patcog.2019.107034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a generic method for the detection of line segments in SAR images. The approach relies on an a contrario framework and is inspired by the state-of-the art LSD detector. As with all a contrario approaches, false detections are controlled through the use of a background model, whose development is especially challenging in the framework of SAR images. Indeed, statistical characteristics of SAR images strongly differ from those of optical images, making the use of existing background models intrinsically inadequate. In order to circumvent this problem, we proceed in two steps. First, the building blocks of the detector, namely the local orientations, are computed carefully to avoid any spatial bias. Second, we propose a new background model, in which the spatial dependency between local orientations are modeled with a Markov chain . This is in strong contrast with most existing a contrario methods who heavily rely on independence assumptions. We provide a complete and detailed algorithm for our line segment detector, and perform experiments on synthetic and real images demonstrating its efficiency.},
  archive      = {J_PR},
  author       = {Chenguang Liu and Rémy Abergel and Yann Gousseau and Florence Tupin},
  doi          = {10.1016/j.patcog.2019.107034},
  journal      = {Pattern Recognition},
  pages        = {107034},
  shortjournal = {Pattern Recognition},
  title        = {LSDSAR, a markovian a contrario framework for line segment detection in SAR images},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A survey on 3D mask presentation attack detection and
countermeasures. <em>PR</em>, <em>98</em>, 107032. (<a
href="https://doi.org/10.1016/j.patcog.2019.107032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the impressive progress in face recognition, current systems are vulnerable to presentation attacks, which subvert the face recognition systems by presenting a face artifact. Several techniques have been developed to automatically detect different presentation attacks, mostly for 2D photo print and video replay attacks. However, with the development of 3D modeling and printing technologies, 3D mask has become a more effective way to attack the face recognition systems. Over the last decade, various detection methods for 3D mask attacks have been proposed, but there is no survey yet to summarize the advances. We present a comprehensive overview of the state-of-the-art approaches in 3D mask spoofing and anti-spoofing, including existing databases and countermeasures . In addition, we quantitatively compare the performance of different mask spoofing detection methods on a common ground (i.e., using the same database and evaluation metric). The effectiveness of several 2D presentation attack detection methods is also evaluated on two 3D mask spoofing databases to show whether they are applicable or not for 3D mask attacks. Finally, we present some insights and summarize open issues to address in the future.},
  archive      = {J_PR},
  author       = {Shan Jia and Guodong Guo and Zhengquan Xu},
  doi          = {10.1016/j.patcog.2019.107032},
  journal      = {Pattern Recognition},
  pages        = {107032},
  shortjournal = {Pattern Recognition},
  title        = {A survey on 3D mask presentation attack detection and countermeasures},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-dimensional unsupervised classification via
parsimonious contaminated mixtures. <em>PR</em>, <em>98</em>, 107031.
(<a href="https://doi.org/10.1016/j.patcog.2019.107031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The contaminated Gaussian distribution represents a simple heavy-tailed elliptical generalization of the Gaussian distribution; unlike the often-considered t -distribution, it also allows for automatic detection of mild outlying or “bad” points in the same way that observations are typically assigned to the groups in the finite mixture model context. Starting from this distribution, we propose the contaminated factor analysis model as a method for dimensionality reduction and detection of bad points in higher dimensions. A mixture of contaminated Gaussian factor analyzers (MCGFA) model follows therefrom, and extends the recently proposed mixture of contaminated Gaussian distributions to high-dimensional data. We introduce a family of 32 parsimonious models formed by introducing constraints on the covariance and contamination structures of the general MCGFA model. We outline a variant of the expectation-maximization algorithm for parameter estimation. Various implementation issues are discussed, and the novel family of models is compared to well-established approaches on both simulated and real data.},
  archive      = {J_PR},
  author       = {Antonio Punzo and Martin Blostein and Paul D. McNicholas},
  doi          = {10.1016/j.patcog.2019.107031},
  journal      = {Pattern Recognition},
  pages        = {107031},
  shortjournal = {Pattern Recognition},
  title        = {High-dimensional unsupervised classification via parsimonious contaminated mixtures},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast and robust template matching with majority neighbour
similarity and annulus projection transformation. <em>PR</em>,
<em>98</em>, 107029. (<a
href="https://doi.org/10.1016/j.patcog.2019.107029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the paper, a novel fast and robust template matching method named A-MNS based on Majority Neighbour Similarity (MNS) and the annulus projection transformation (APT) is proposed. Its essence is the MNS, a useful, rotation-invariant, low-computational-cost and robust similarity measurement. The proposed method is theoretically demonstrated and experimentally evaluated as being able to estimate the rotation angle of the target object, overcome challenges such as background clutter, occlusion, arbitrary rotation transformation, and non-rigid deformation, while performing fast matching. Empirical results evaluated on the up-to-date benchmark show that A-MNS is 4.419 times faster than DDIS (the state-of-the-art) and is also competitive in terms of its matching accuracy.},
  archive      = {J_PR},
  author       = {Jinxiang Lai and Liang Lei and Kaiyuan Deng and Runming Yan and Yang Ruan and Zhou Jinyun},
  doi          = {10.1016/j.patcog.2019.107029},
  journal      = {Pattern Recognition},
  pages        = {107029},
  shortjournal = {Pattern Recognition},
  title        = {Fast and robust template matching with majority neighbour similarity and annulus projection transformation},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Realtime multi-scale scene text detection with scale-based
region proposal network. <em>PR</em>, <em>98</em>, 107026. (<a
href="https://doi.org/10.1016/j.patcog.2019.107026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-scale approaches have been widely used for achieving high accuracy for scene text detection, but they usually slow down the speed of the whole system. In this paper, we propose a two-stage framework for realtime multi-scale scene text detection. The first stage employs a novel S cale-based R egion P roposal N etwork (SRPN) which can localize text of wide scale range and estimate text scale efficiently. Based on SRPN, non-text regions are filtered out, and text region proposals are generated. Moreover, based on text scale estimation by SRPN, small or big texts in region proposals are resized into a unified normal scale range. The second stage then adopts a Fully Convolutional Network based scene text detector to localize text words from proposals of the first stage. Text detector in the second stage detects texts of narrow scale range but accurately. Since most non-text regions are eliminated through SRPN efficiently, and texts in proposals are properly scaled to avoid multi-scale pyramid processing, the whole system is quite fast. We evaluate both performance and speed of the proposed method on datasets ICDAR2015, ICDAR2013, and MSRA-TD500. On ICDAR2015, our system can reach the state-of-the-art F -measure score of 85.40\% at 16.5 fps (frame per second), and competitive performance of 79.66\% at 35.1 fps, either of which is more than 5 times faster than previous best methods. On ICDAR2013 and MSRA-TD500, we also achieve remarkable speedup by keeping competitive performance. Ablation experiments are also provided to demonstrate the reasonableness of our method.},
  archive      = {J_PR},
  author       = {Wenhao He and Xu-Yao Zhang and Fei Yin and Zhenbo Luo and Jean-Marc Ogier and Cheng-Lin Liu},
  doi          = {10.1016/j.patcog.2019.107026},
  journal      = {Pattern Recognition},
  pages        = {107026},
  shortjournal = {Pattern Recognition},
  title        = {Realtime multi-scale scene text detection with scale-based region proposal network},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A generalized least-squares approach regularized with graph
embedding for dimensionality reduction. <em>PR</em>, <em>98</em>,
107023. (<a href="https://doi.org/10.1016/j.patcog.2019.107023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In current graph embedding methods, low dimensional projections are obtained by preserving either global geometrical structure of data or local geometrical structure of data. In this paper, the PCA (Principal Component Analysis) idea of minimizing least-squares reconstruction errors is regularized with graph embedding, to unify various local manifold embedding methods within a generalized framework to keep global and local low dimensional subspace . Different from the well-known PCA method, our proposed generalized least-squares approach considers data distributions together with an instance penalty in each data point. In this way, PCA is viewed as a special instance of our proposed generalized least squares framework for preserving global projections. Applying a regulation of graph embedding, we can obtain projection that preserves both intrinsic geometrical structure and global structure of data. From the experimental results on a variety of face and handwritten digit recognition , our proposed method has advantage of superior performances in keeping lower dimensional subspaces and higher classification results than state-of-the-art graph embedding methods.},
  archive      = {J_PR},
  author       = {Xiang-Jun Shen and Si-Xing Liu and Bing-Kun Bao and Chun-Hong Pan and Zheng-Jun Zha and Jianping Fan},
  doi          = {10.1016/j.patcog.2019.107023},
  journal      = {Pattern Recognition},
  pages        = {107023},
  shortjournal = {Pattern Recognition},
  title        = {A generalized least-squares approach regularized with graph embedding for dimensionality reduction},
  volume       = {98},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pattern recognition referees 2018–2019. <em>PR</em>,
<em>97</em>, 107041. (<a
href="https://doi.org/10.1016/j.patcog.2019.107041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_PR},
  doi          = {10.1016/j.patcog.2019.107041},
  journal      = {Pattern Recognition},
  pages        = {107041},
  shortjournal = {Pattern Recognition},
  title        = {Pattern recognition referees 2018–2019},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Population-guided large margin classifier for high-dimension
low-sample-size problems. <em>PR</em>, <em>97</em>, 107030. (<a
href="https://doi.org/10.1016/j.patcog.2019.107030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel linear binary classifier, denoted by population-guided large margin classifier (PGLMC), applicable to any sorts of data, including high-dimensional low-sample-size (HDLSS). PGLMC is conceived with a projecting direction w given by the comprehensive consideration of local structural information of the hyperplane and the statistics of the training samples. Our proposed model has several advantages compared to those widely used approaches. First, it isn&#39;t sensitive to the intercept term b. Second, it operates well with imbalanced data . Third, it is relatively simple to be implemented based on Quadratic Programming. Fourth, it is robust to the model specification for various real applications. The theoretical properties of PGLMC are proven. We conduct a series of evaluations on the simulated and five realworld benchmark data sets, including DNA classification, medical image analysis and face recognition. PGLMC outperforms the state-of-theart classification methods in most cases, or obtains comparable results.},
  archive      = {J_PR},
  author       = {Qingbo Yin and Ehsan Adeli and Liran Shen and Dinggang Shen},
  doi          = {10.1016/j.patcog.2019.107030},
  journal      = {Pattern Recognition},
  pages        = {107030},
  shortjournal = {Pattern Recognition},
  title        = {Population-guided large margin classifier for high-dimension low-sample-size problems},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enhanced grassmann discriminant analysis with randomized
time warping for motion recognition. <em>PR</em>, <em>97</em>, 107028.
(<a href="https://doi.org/10.1016/j.patcog.2019.107028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a framework for classifying motion sequences, by extending the framework of Grassmann discriminant analysis (GDA). A problem of GDA is that its discriminant space is not necessarily optimal. This limitation becomes even more prominent when utilizing the subspace representation of randomized time warping (RTW). RTW is a sequence representation that can effectively model a motion’s temporal information by a low-dimensional subspace, simplifying the problem of comparing two sequences to that of comparing two subspaces. The key idea of the proposed enhanced GDA is projecting class subspaces onto a generalized difference subspace before mapping them on a Grassmann manifold. The GDS projection can remove overlapping components of the subspaces in the vector space, nearly orthogonalizing them. Consequently, a dictionary of orthogonalized class subspaces produces a set of more discriminant data points in the Grassmann manifold, in comparison with the original set. This set of data points can further enhance the discriminant ability of GDA. We demonstrate the validity of the proposed framework, RTW+eGDA, through experiments on motion recognition using the publicly available Cambridge gesture, KTH action, and UCF sports datasets.},
  archive      = {J_PR},
  author       = {Lincon S. Souza and Bernardo B. Gatto and Jing-Hao Xue and Kazuhiro Fukui},
  doi          = {10.1016/j.patcog.2019.107028},
  journal      = {Pattern Recognition},
  pages        = {107028},
  shortjournal = {Pattern Recognition},
  title        = {Enhanced grassmann discriminant analysis with randomized time warping for motion recognition},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust visual tracking based on adversarial unlabeled
instance generation with label smoothing loss regularization.
<em>PR</em>, <em>97</em>, 107027. (<a
href="https://doi.org/10.1016/j.patcog.2019.107027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have shown that deep neural networks have pushed visual tracking accuracy to new heights, but finding more robust long-term tracking is still challenging because of the dynamic foreground and background changes. This phenomenon affects the overall performance via online training sample generation. The dense sampling strategy has been widely used for its convenience, the appearance variation is severely limited by its highly spatial overlapping mechanism. The sample candidate evaluation with a classification score metric is not always reliable throughout the entire process, therefore, tracking failure is inevitable. As an effective solution, this paper proposes a novel sample-level generative adversarial network (GAN) to enrich the training data by generating massive amounts of sample-level GAN samples. These samples are not only similar to the real-life scenarios, but also could carry more diversity of deformation and motion blur to a certain degree. For occlusion invariance, a feature-level GAN is incorporated to generate more challenging feature-level GAN data by creating random occlusion masks in deep feature space. To facilitate the online learning process, a label smoothing loss regularization is introduced to achieve model regularization and over-fitting reduction by integrating the unlabeled GAN-generated training data with the realistically labeled ones. In addition, a re-detection correlation filter conservatively trained with reliable training data is employed to integrate a classification score metric to perform reliable model updates and avoid heavy degradation. Furthermore, we also carry out the re-detection correlation filter on the candidate region proposals to handle the tracking failures. The proposed tracker has shown superior performance in comparison to the other state-of-the-art tracking approaches on the OTB-2013, OTB-100, UAV123, UAV20L, and VOT2016 benchmark datasets.},
  archive      = {J_PR},
  author       = {Yamin Han and Peng Zhang and Wei Huang and Yufei Zha and Garth Douglas Cooper and Yanning Zhang},
  doi          = {10.1016/j.patcog.2019.107027},
  journal      = {Pattern Recognition},
  pages        = {107027},
  shortjournal = {Pattern Recognition},
  title        = {Robust visual tracking based on adversarial unlabeled instance generation with label smoothing loss regularization},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). In-air handwritten chinese text recognition with temporal
convolutional recurrent network. <em>PR</em>, <em>97</em>, 107025. (<a
href="https://doi.org/10.1016/j.patcog.2019.107025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a new human-computer interaction way, in-air handwriting allows users to perform gesture-based writing in the midair. However, most existing in-air handwriting systems mainly focus on recognizing either isolated characters/words or only a small number of texts, making those systems far from practical applications. Instead, here we present a 3D in-air handwritten Chinese text recognition (IAHCTR) system for the first time, and construct the first public large-scale IAHCT dataset. Moreover, a novel architecture, named the temporal convolutional recurrent network (TCRN), is proposed for online HCTR. Specifically, the TCRN first applies the 1-dimensional convolution to extract local contextual features from low-level trajectories, and then it utilizes the recurrent network to capture long-term dependencies of high-level outputs. Compared with the state-of-the-art architecture, the TCRN not only avoids the domain-specific knowledge for feature image extraction, but also attains higher training efficiency with a more compact model. Empirically, this TCRN also outperforms the single recurrent network with faster prediction and higher accuracy. Experiments on CASIA-OLHWDB2 &amp; ICDAR-2013 demonstrate that the TCRN yields the best result in comparison to the state-of-the-art methods for online HCTR.},
  archive      = {J_PR},
  author       = {Ji Gan and Weiqiang Wang and Ke Lu},
  doi          = {10.1016/j.patcog.2019.107025},
  journal      = {Pattern Recognition},
  pages        = {107025},
  shortjournal = {Pattern Recognition},
  title        = {In-air handwritten chinese text recognition with temporal convolutional recurrent network},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Time series classification using local distance-based
features in multi-modal fusion networks. <em>PR</em>, <em>97</em>,
107024. (<a href="https://doi.org/10.1016/j.patcog.2019.107024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose the use of a novel feature, called local distance features, for time series classification. The local distance features are extracted using Dynamic Time Warping (DTW) and classified using Convolutional Neural Networks (CNN). DTW is classically as a robust distance measure for distance-based time series recognition methods. However, by using DTW strictly as a global distance measure, information about the matching is discarded. We show that this information can further be used as supplementary input information in temporal CNNs. This is done by using both the raw data and the features extracted from DTW in multi-modal fusion CNNs. Furthermore, we explore the effects of different prototype selection methods, prototype numbers, and data fusion schemes induce on the accuracy. We perform experiments on a wide range of time series datasets including three Unipen handwriting datasets, four UCI Machine Learning Repository datasets, and 85 UCR Time Series Classification Archive datasets.},
  archive      = {J_PR},
  author       = {Brian Kenji Iwana and Seiichi Uchida},
  doi          = {10.1016/j.patcog.2019.107024},
  journal      = {Pattern Recognition},
  pages        = {107024},
  shortjournal = {Pattern Recognition},
  title        = {Time series classification using local distance-based features in multi-modal fusion networks},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A sparse structure for fast circle detection. <em>PR</em>,
<em>97</em>, 107022. (<a
href="https://doi.org/10.1016/j.patcog.2019.107022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the paper, we present a circle detector that achieves the state-of-art performance in almost every type of image. The detector represents each circle instance by a set of equally distributed arcs and searches for the same number of edge points to cover these arcs. The new formulation leads to the voting in minimizing/maximizing way which is different from the typical accumulative way adopted by Hough transform . From the formulation, circle detection is then decomposed into radius-dependent and -independent part. The calculation of independent part is computationally expensive but shared by different radii. This decomposition gets rid of the redundant computation in handling multiple radii and therefore speeds up the detection process. Originated from the sparse nature of independent part, we design a sparse structure for its batch computation which is fulfilled in just one sweep of the edge points. Circle detector based on this sparse structure is then proposed which achieves the comparable time complexity as the algorithm based on Hough transform using 2D accumulator array. For testing, we created an information-rich dataset with images coming from multiple sources. It contains five categories and covers a wide spectrum of images, ranging from true color images to the binary ones. The experimental results demonstrate that the proposed approach outperforms the solutions based on accumulative voting.},
  archive      = {J_PR},
  author       = {Yuanqi Su and Xiaoning Zhang and Bonan Cuan and Yuehu Liu and Zehao Wang},
  doi          = {10.1016/j.patcog.2019.107022},
  journal      = {Pattern Recognition},
  pages        = {107022},
  shortjournal = {Pattern Recognition},
  title        = {A sparse structure for fast circle detection},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Decomposed slice sampling for factorized distributions.
<em>PR</em>, <em>97</em>, 107021. (<a
href="https://doi.org/10.1016/j.patcog.2019.107021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Slice sampling provides an automatical adjustment to match the characteristics of the distribution. Although this method has made great success in many situations, it becomes limited when the distribution is complex. Inspired by Higdon [1], in this paper, we present a decomposed sampling framework based on slice sampling called decomposed slice sampling (DSS). We suppose that the target distribution can be divided into two multipliers so that information in each term can be used, respectively. The first multiplier is used in the first step of DSS to obtain horizontal slices and the last term is used in the second step. Simulations on four simple distributions indicate the effectiveness of our method. Compared with slice sampling and Hamiltonian Monte Carlo on Gaussian distributions in different dimensions and ten real-world datasets, the proposed method achieves better performance.},
  archive      = {J_PR},
  author       = {Jiachun Wang and Shiliang Sun},
  doi          = {10.1016/j.patcog.2019.107021},
  journal      = {Pattern Recognition},
  pages        = {107021},
  shortjournal = {Pattern Recognition},
  title        = {Decomposed slice sampling for factorized distributions},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online learning using projections onto shrinkage closed
balls for adaptive brain-computer interface. <em>PR</em>, <em>97</em>,
107017. (<a href="https://doi.org/10.1016/j.patcog.2019.107017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wearable/portable brain-computer interfaces (BCIs) for the long-term end use are a focus of recent BCI research. A challenge is how to update the BCI to meet changes in electroencephalography (EEG) signals, since the resource are so limited that retraining of traditional well-performed models, such as a support vector machine , is nearly impossible. To cope with this challenge, less-demanding adaptive online learning can be considered. We investigated an adaptive projected sub-gradient method (APSM) that is originated from the set theoretic estimation formulation and the projections onto convex sets theory. APSM provides a unifying framework for both adaptive classification and regression tasks . Coefficients of APSM are adjusted online as data arrive sequentially, with a regularization constraint made by projections onto a fixed closed ball. We extended the general APSM to a shrinkage form, where shrinkage closed balls were used instead of the original fixed one, expecting a more controllable fading effect and better adaptability. The convergence of shrinkage APSM was proved. It was also demonstrated that as shrinkage factor approached to 1, the limit point of shrinkage APSM would approach to the optimal solution with the least norm, which could be especially beneficial for generalization of the classifier. The performance of the proposed method was evaluated, and compared with those of the general APSM, the incremental support vector machine , and the passive aggressive algorithm, through an event-related potential-based BCI experiment. Results showed the advantage of the proposed method over the others on both the online classification performance and the easiness of tuning. Our study revealed the effectiveness of the proposed method for adaptive EEG classification, making it a promising tool for on-device training and updating of wearable/portable BCIs, as well as for application in other related fields, such as EEG-based biometrics .},
  archive      = {J_PR},
  author       = {Zheng Ma and Jun Cheng and Dapeng Tao},
  doi          = {10.1016/j.patcog.2019.107017},
  journal      = {Pattern Recognition},
  pages        = {107017},
  shortjournal = {Pattern Recognition},
  title        = {Online learning using projections onto shrinkage closed balls for adaptive brain-computer interface},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Attributes-aided part detection and refinement for person
re-identification. <em>PR</em>, <em>97</em>, 107016. (<a
href="https://doi.org/10.1016/j.patcog.2019.107016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Person attributes are often exploited as mid-level human semantic information to help promote the performance of person re-identification task. In this paper, unlike most existing methods simply taking the attribute learning as a classification problem, we perform it in a different way with the motivation that attributes are related to specific local regions, which refers to the perceptual ability of attributes. We utilize the process of attribute detection to generate corresponding attribute-part detectors, whose invariance to many influences like poses and camera views can be guaranteed. With detected local part regions, our model extracts local part features to handle the body part misalignment problem , which is another major challenge for person re-identification. The local descriptors are further refined by fused attribute information to eliminate interferences caused by detection deviation. Finally, the refined local feature works together with a holistic-level feature to constitute our final feature representation. Extensive experiments on two popular benchmarks with attribute annotations demonstrate the effectiveness of our model and competitive performance compared with state-of-the-art algorithms.},
  archive      = {J_PR},
  author       = {Shuzhao Li and Huimin Yu and Roland Hu},
  doi          = {10.1016/j.patcog.2019.107016},
  journal      = {Pattern Recognition},
  pages        = {107016},
  shortjournal = {Pattern Recognition},
  title        = {Attributes-aided part detection and refinement for person re-identification},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Auto-weighted multi-view clustering via deep matrix
decomposition. <em>PR</em>, <em>97</em>, 107015. (<a
href="https://doi.org/10.1016/j.patcog.2019.107015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real data are often collected from multiple channels or comprised of different representations (i.e., views). Multi-view learning provides an elegant way to analyze the multi-view data for low-dimensional representation. In recent years, several multi-view learning methods have been designed and successfully applied in various tasks. However, existing multi-view learning methods usually work in a single layer formulation. Since the mapping between the obtained representation and the original data contains rather complex hierarchical information with implicit lower-level hidden attributes, it is desirable to fully explore the hidden structures hierarchically. In this paper, a novel deep multi-view clustering model is proposed by uncovering the hierarchical semantics of the input data in a layer-wise way. By utilizing a novel collaborative deep matrix decomposition framework, the hidden representations are learned with respect to different attributes. The proposed model is able to collaboratively learn the hierarchical semantics obtained by each layer. The instances from the same class are forced to be closer layer by layer in the low-dimensional space, which is beneficial for the subsequent clustering task . Furthermore, an idea weight is automatically assigned to each view without introducing extra hyperparameter as previous methods do. To solve the optimization problem of our model, an efficient iterative updating algorithm is proposed and its convergence is also guaranteed theoretically. Our empirical study on multi-view clustering task shows encouraging results of our model in comparison to the state-of-the-art algorithms.},
  archive      = {J_PR},
  author       = {Shudong Huang and Zhao Kang and Zenglin Xu},
  doi          = {10.1016/j.patcog.2019.107015},
  journal      = {Pattern Recognition},
  pages        = {107015},
  shortjournal = {Pattern Recognition},
  title        = {Auto-weighted multi-view clustering via deep matrix decomposition},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Similarity learning with joint transfer constraints for
person re-identification. <em>PR</em>, <em>97</em>, 107014. (<a
href="https://doi.org/10.1016/j.patcog.2019.107014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The inconsistency of data distributions among multiple views is one of the most crucial issues which hinder the accuracy of person re-identification. To solve the problem, this paper presents a novel similarity learning model by combining the optimization of feature representation via multi-view visual words reconstruction and the optimization of metric learning via joint discriminative transfer learning. The starting point of the proposed model is to capture multiple groups of multi-view visual words ( MvVW ) through an unsupervised clustering method (i.e. K-means ) from human parts (e.g. head, torso, legs). Then, we construct a joint feature matrix by combining multi-group feature matrices with different body parts. To solve the inconsistent distributions under different views, we propose a method of joint transfer constraint to learn the similarity function by combining multiple common subspaces, each in charge of a sub-region. In the common subspaces, the original samples can be reconstructed based on MvVW under low-rank and sparse representation constraints, which can enhance the structure robustness and noise resistance. During the process of objective function optimization, based on confinement fusion of multi-view and multiple sub-regions, a solution strategy is proposed to solve the objective function using joint matrix transform . Taking all of these into account, the issue of person re-identification under inconsistent data distributions can be transformed into a consistent iterative convex optimization problem , and solved via the inexact augmented Lagrange multiplier ( IALM ) algorithm. Extensive experiments are conducted on three challenging person re-identification datasets (i.e., VIPeR, CUHK01 and PRID450S), which shows that our model outperforms several state-of-the-art methods.},
  archive      = {J_PR},
  author       = {Cairong Zhao and Xuekuan Wang and Wangmeng Zuo and Fumin Shen and Ling Shao and Duoqian Miao},
  doi          = {10.1016/j.patcog.2019.107014},
  journal      = {Pattern Recognition},
  pages        = {107014},
  shortjournal = {Pattern Recognition},
  title        = {Similarity learning with joint transfer constraints for person re-identification},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Supervised fuzzy partitioning. <em>PR</em>, <em>97</em>,
107013. (<a href="https://doi.org/10.1016/j.patcog.2019.107013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Centroid-based methods including k-means and fuzzy c-means are known as effective and easy-to-implement approaches to clustering purposes in many applications. However, these algorithms cannot be directly applied to supervised tasks. This paper thus presents a generative model extending the centroid-based clustering approach to be applicable to classification and regression tasks . Given an arbitrary loss function, the proposed approach, termed Supervised Fuzzy Partitioning (SFP), incorporates labels information into its objective function through a surrogate term penalizing the empirical risk. Entropy-based regularization is also employed to fuzzify the partition and to weight features, enabling the method to capture more complex patterns, identify significant features, and yield better performance facing high-dimensional data. An iterative algorithm based on block coordinate descent scheme is formulated to efficiently find a local optimum. Extensive classification experiments on synthetic, real-world, and high-dimensional datasets demonstrate that the predictive performance of SFP is competitive with state-of-the-art algorithms such as SVM and random forest . SFP has a major advantage over such methods, in that it not only leads to a flexible, nonlinear model but also can exploit any convex loss function in the training phase without compromising computational efficiency.},
  archive      = {J_PR},
  author       = {Pooya Ashtari and Fateme Nateghi Haredasht and Hamid Beigy},
  doi          = {10.1016/j.patcog.2019.107013},
  journal      = {Pattern Recognition},
  pages        = {107013},
  shortjournal = {Pattern Recognition},
  title        = {Supervised fuzzy partitioning},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Minimum margin loss for deep face recognition. <em>PR</em>,
<em>97</em>, 107012. (<a
href="https://doi.org/10.1016/j.patcog.2019.107012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Face recognition has achieved great success owing to the fast development of deep neural networks in the past few years. Different loss functions can be used in a deep neural network resulting in different performance. Most recently some loss functions have been proposed, which have advanced the state of the art. However, they cannot solve the problem of margin bias which is present in class imbalanced datasets, having the so-called long-tailed distributions. In this paper, we propose to solve the margin bias problem by setting a minimum margin for all pairs of classes. We present a new loss function, Minimum Margin Loss (MML), which is aimed at enlarging the margin of those overclose class centre pairs so as to enhance the discriminative ability of the deep features. MML, together with Softmax Loss and Centre Loss, supervises the training process to balance the margins of all classes irrespective of their class distributions. We implemented MML in Inception-ResNet-v1 and conducted extensive experiments on seven face recognition benchmark datasets, MegaFace, FaceScrub, LFW, SLLFW, YTF, IJB-B and IJB-C. Experimental results show that the proposed MML loss function has led to new state of the art in face recognition, reducing the negative effect of margin bias.},
  archive      = {J_PR},
  author       = {Xin Wei and Hui Wang and Bryan Scotney and Huan Wan},
  doi          = {10.1016/j.patcog.2019.107012},
  journal      = {Pattern Recognition},
  pages        = {107012},
  shortjournal = {Pattern Recognition},
  title        = {Minimum margin loss for deep face recognition},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scalable logo detection by self co-learning. <em>PR</em>,
<em>97</em>, 107003. (<a
href="https://doi.org/10.1016/j.patcog.2019.107003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing logo detection methods usually consider a small number of logo classes, limited images per class and assume fine-gained object bounding box annotations. This limits their scalability to real-world dynamic applications. In this work, we tackle these challenges by exploring a web data learning principle without the need for exhaustive manual labelling. Specifically, we propose a novel incremental learning approach, called Scalable Logo Self-co-Learning (SL 2 ), capable of automatically self-discovering informative training images from noisy web data for progressively improving model capability in a cross-model co-learning manner. Moreover, we introduce a very large (2,190,757 images of 194 logo classes) logo dataset “WebLogo-2M” by designing an automatic data collection and processing method. Extensive comparative evaluations demonstrate the superiority of SL 2 over the state-of-the-art strongly and weakly supervised detection models and contemporary web data learning approaches.},
  archive      = {J_PR},
  author       = {Hang Su and Shaogang Gong and Xiatian Zhu},
  doi          = {10.1016/j.patcog.2019.107003},
  journal      = {Pattern Recognition},
  pages        = {107003},
  shortjournal = {Pattern Recognition},
  title        = {Scalable logo detection by self co-learning},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Voronoi tree models for distribution-preserving sampling and
generation. <em>PR</em>, <em>97</em>, 107002. (<a
href="https://doi.org/10.1016/j.patcog.2019.107002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a method based on recursive binary Voronoi trees to learn a nonparametric model of the distribution underlying a given dataset. The obtained model can be used as a general tool both to extract good samples from the original dataset (e.g., for batch selection, bagging, or sample size reduction) or to generate new synthetic ones, also in a conditional fashion (e.g., to deal with imbalanced sets or to reconstruct corrupted points). In order to ensure that the distribution of the new sets, either sampled or generated, follows closely that of the original dataset, we design all the procedures according to a specific measure of distance between distributions. The use of binary recursive Voronoi structures enables the proposed algorithms to be simple, efficient and able to adapt to the shape of the original dataset. Simulation tests showcase the good performance and flexibility of the approach in various learning contexts.},
  archive      = {J_PR},
  author       = {Cristiano Cervellera and Danilo Macciò},
  doi          = {10.1016/j.patcog.2019.107002},
  journal      = {Pattern Recognition},
  pages        = {107002},
  shortjournal = {Pattern Recognition},
  title        = {Voronoi tree models for distribution-preserving sampling and generation},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Outer-points shaver: Robust graph-based clustering via node
cutting. <em>PR</em>, <em>97</em>, 107001. (<a
href="https://doi.org/10.1016/j.patcog.2019.107001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based clustering is an efficient method for identifying clusters in local and nonlinear data patterns. Among the existing methods, spectral clustering is one of the most prominent algorithms. However, this method is vulnerable to noise and outliers. This study proposes a robust graph-based clustering method that removes the data nodes of relatively low density. The proposed method calculates the pseudo-density from a similarity matrix, and reconstructs it using a sparse regularization model. In this process, noise and the outer points are determined and removed. Unlike previous edge cutting-based methods, the proposed method is robust to noise while detecting clusters because it cuts out irrelevant nodes. We use a simulation and real-world data to demonstrate the usefulness of the proposed method by comparing it to existing methods in terms of clustering accuracy and robustness to noisy data. The comparison results confirm that the proposed method outperforms the alternatives.},
  archive      = {J_PR},
  author       = {Younghoon Kim and Hyungrok Do and Seoung Bum Kim},
  doi          = {10.1016/j.patcog.2019.107001},
  journal      = {Pattern Recognition},
  pages        = {107001},
  shortjournal = {Pattern Recognition},
  title        = {Outer-points shaver: Robust graph-based clustering via node cutting},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic graph convolutional networks. <em>PR</em>,
<em>97</em>, 107000. (<a
href="https://doi.org/10.1016/j.patcog.2019.107000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many different classification tasks it is required to manage structured data, which are usually modeled as graphs. Moreover, these graphs can be dynamic, meaning that the vertices/edges of each graph may change over time. The goal is to exploit existing neural network architectures to model datasets that are best represented with graph structures that change over time. To the best of the authors’ knowledge, this task has not been addressed using these kinds of architectures. Two novel approaches are proposed, which combine Long Short-Term Memory networks and Graph Convolutional Networks to learn long short-term dependencies together with graph structure. The advantage provided by the proposed methods is confirmed by the results achieved on four real world datasets: an increase of up to 12 percentage points in Accuracy and F1 scores for vertex-based semi-supervised classification and up to 2 percentage points in Accuracy and F1 scores for graph-based supervised classification .},
  archive      = {J_PR},
  author       = {Franco Manessi and Alessandro Rozza and Mario Manzo},
  doi          = {10.1016/j.patcog.2019.107000},
  journal      = {Pattern Recognition},
  pages        = {107000},
  shortjournal = {Pattern Recognition},
  title        = {Dynamic graph convolutional networks},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new approach for reduction of attributes based on stripped
quotient sets. <em>PR</em>, <em>97</em>, 106999. (<a
href="https://doi.org/10.1016/j.patcog.2019.106999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Attribute reduction is a key problem in many areas such as data mining, pattern recognition, machine learning . The problems of finding all reducts as well as finding a minimal reduct in a given data table have been proved to be NP-hard. Therefore, to overcome this difficulty, many heuristic attribute reduction methods have been developed in recent years. In the process of heuristic attribute reduction, accelerating calculation of attribute significance is very important, especially for big data cases. In this paper, we firstly propose attribute significance measures based on stripped quotient sets. Then, by using these measures, we design efficient algorithms for calculating core and reduct, in which the time complexity will be considered in detail. Additionally, we will also give properties directly related to efficiently computing the attribute significance and significantly reducing the data size in the process of calculation. By theoretical and experimental views, we will show that our method can perform efficiently for large-scale data sets.},
  archive      = {J_PR},
  author       = {Nguyen Ngoc Thuy and Sartra Wongthanavasu},
  doi          = {10.1016/j.patcog.2019.106999},
  journal      = {Pattern Recognition},
  pages        = {106999},
  shortjournal = {Pattern Recognition},
  title        = {A new approach for reduction of attributes based on stripped quotient sets},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Salient video object detection using a virtual border and
guided filter. <em>PR</em>, <em>97</em>, 106998. (<a
href="https://doi.org/10.1016/j.patcog.2019.106998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present a novel method for salient object detection in videos. Salient object detection methods based on background prior may miss salient region when the salient object touches the frame borders. To solve this problem, we propose to detect the whole salient object via the adjunction of virtual borders. A guided filter is then applied on the temporal output to integrate the spatial edge information for a better detection of the salient object edges. At last, a global spatio-temporal saliency map is obtained by combining the spatial saliency map and the temporal saliency map together according to the entropy. The proposed method is assessed on three popular datasets (Fukuchi, FBMS and VOS) and compared to several state-of-the-art methods. The experimental results show that the proposed approach outperforms the tested methods.},
  archive      = {J_PR},
  author       = {Qiong Wang and Lu Zhang and Wenbin Zou and Kidiyo Kpalma},
  doi          = {10.1016/j.patcog.2019.106998},
  journal      = {Pattern Recognition},
  pages        = {106998},
  shortjournal = {Pattern Recognition},
  title        = {Salient video object detection using a virtual border and guided filter},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rough segmentation of coherent local intensity for bias
induced 3-d MR brain images. <em>PR</em>, <em>97</em>, 106997. (<a
href="https://doi.org/10.1016/j.patcog.2019.106997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Segmentation of brain MR volumes into different meaningful tissue classes is an essential prerequisite for many clinical analyses. However, intensity inhomogeneity or bias field, present in MR volumes, considerably degrades the quality of segmentation. In this regard, the paper presents a new segmentation algorithm , termed as CoLoRS ( Co herent Lo cal Intensity R ough S egmentation), for brain MR volumes corrupted with bias field artifact. It judiciously integrates the merits of coherent local intensity clustering and the theory of rough sets for simultaneous segmentation and bias field correction of brain MR volumes. The proposed algorithm partitions the entire image space into a number of small overlapping neighborhood regions. The bias, in each neighborhood region, is assumed to be constant. For each individual region, an objective function is defined for coherent local intensity rough segmentation. The voxels near the center point have similar influences on local objective function. In addition, the smaller distance between center and neighboring voxels yields more contribution on the voxel of interest. The proposed algorithm uses the dual-region concept to represent the neighborhood structure more efficiently. It makes possible of separate modeling of the voxels within neighborhood, according to their locations. Each region is considered to have several tissue classes, where each tissue class consists of a core region and an overlapping region. The segmentation in fuzzy approximation spaces provides an effective mean for brain MR volume analysis, as it handles overlapping partitions and addresses vagueness in tissue class definition. The effectiveness of the proposed algorithm, along with a comparison with existing approaches, is demonstrated on several publicly available brain MR data.},
  archive      = {J_PR},
  author       = {Shaswati Roy and Pradipta Maji},
  doi          = {10.1016/j.patcog.2019.106997},
  journal      = {Pattern Recognition},
  pages        = {106997},
  shortjournal = {Pattern Recognition},
  title        = {Rough segmentation of coherent local intensity for bias induced 3-D MR brain images},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). L1-subspace tracking for streaming data. <em>PR</em>,
<em>97</em>, 106992. (<a
href="https://doi.org/10.1016/j.patcog.2019.106992">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional data usually exhibit intrinsic low-rank structures. With tremendous amount of streaming data generated by ubiquitous sensors in the world of Internet-of-Things, fast detection of such low-rank pattern is of utmost importance to a wide range of applications. In this work, we present an L 1 -subspace tracking method to capture the low-rank structure of streaming data. The method is based on the L 1 -norm principal-component analysis ( L 1 -PCA) theory that offers outlier resistance in subspace calculation. The proposed method updates the L 1 -subspace as new data are acquired by sensors. In each time slot, the conformity of each datum is measured by the L 1 -subspace calculated in the previous time slot and used to weigh the datum. Iterative weighted L 1 -PCA is then executed through a refining function. The superiority of the proposed L 1 -subspace tracking method compared to existing approaches is demonstrated through experimental studies in various application fields.},
  archive      = {J_PR},
  author       = {Ying Liu and Konstantinos Tountas and Dimitris A. Pados and Stella N. Batalama and Michael J. Medley},
  doi          = {10.1016/j.patcog.2019.106992},
  journal      = {Pattern Recognition},
  pages        = {106992},
  shortjournal = {Pattern Recognition},
  title        = {L1-subspace tracking for streaming data},
  volume       = {97},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
