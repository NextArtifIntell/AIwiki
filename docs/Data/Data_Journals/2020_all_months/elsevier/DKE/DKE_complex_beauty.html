<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>DKE_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="dke---40">DKE - 40</h2>
<ul>
<li><details>
<summary>
(2020). An analysis of the collaboration network of the
international conference on conceptual modeling at the age of 40.
<em>DKE</em>, <em>130</em>, 101866. (<a
href="https://doi.org/10.1016/j.datak.2020.101866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The International Conference on Conceptual Modeling celebrated 40 years of existence at its 38th edition held in Salvador, Brazil , on 4–7 November 2019. As one of the most traditional and well-known conferences in the database area, it has its origins on the Entity-Relationship Model proposed by Peter P. Chen in 1975. To celebrate such an accomplishment, this article goes over the ER history from distinct perspectives. Overall, we investigate the complete ER collaboration network built on bibliographic data collected from DBLP, comprising its 38 editions held from 1979 to 2019. We analyze several aspects regarding the evolution of its network metrics, such as degree, clustering coefficient and average shortest path, over the four decades. In particular, we analyze the role of the most engaged ER authors, the number of distinct authors, institutions and published papers, and the evolution of some of the most frequent terms presented in the titles of its papers, as well as the influence and impact of the prominent ER authors.},
  archive      = {J_DKE},
  author       = {Lucas Henrique C. Lima and Alberto H.F. Laender and Mirella M. Moro and José Palazzo M. de Oliveira},
  doi          = {10.1016/j.datak.2020.101866},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101866},
  shortjournal = {Data Knowl. Eng.},
  title        = {An analysis of the collaboration network of the international conference on conceptual modeling at the age of 40},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Handling data imperfection—false data inputs in applications
for alzheimer’s patients. <em>DKE</em>, <em>130</em>, 101864. (<a
href="https://doi.org/10.1016/j.datak.2020.101864">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Handling data imperfection is a crucial issue in many application domains. This is particularly true when handling imperfect data inputs in applications for Alzheimer’s patients. In this paper we first propose a typology of imperfection for data entered by Alzheimer’s patients or their caregivers in the context of these applications (mainly due to the memory discordance caused by the disease). This topology includes nine direct and three indirect imperfection types. The direct ones are deduced from the data inputs e.g. uncertainty and uselessness. The indirect imperfection types are deduced from the direct ones, e.g. the redundancy. We then propose an approach, called DBE_ALZ, that handles false data entry by estimating the believability of each data input. Based on the proposed typology, the falsity of these data is related to five imperfection types: uncertainty, confusion, typing error, wrong knowledge and inconsistency. DBE_ALZ includes a believability model that defines a set of dimensions and sub-dimensions allowing a qualitative estimation of the believability of a given data input. It is estimated based on its reasonableness and the reliability of its author. Compared to related work, the data input reasonableness is measured not only based on common-sense standard, but also based on a set of personalized assertions. The reliability of the patient is estimated based on the progression of the disease and the state of his memory at the moment of entry. However, the reliability of the caregiver is estimated based on his age and his knowledge about the data input’s field. Based on the believability model, we estimate quantitatively the believability of the data input by defining a set of metrics associated to the proposed dimensions and sub-dimensions. The measurement methods rely on probability and fuzzy set theories to reason about uncertain and imprecise knowledge (Bayesian networks and Mamdani fuzzy inference systems). Three languages are supported: English, French and Arabic. Based on the generated believability degrees, a set of decisive actions are proposed to guarantee the quality of the data inputs e.g., inferring or not based on a given data. We illustrate the usefulness of our approach in the context of the Captain Memo memory prosthesis. Finally, we discuss the encouraging results derived from the evaluation step.},
  archive      = {J_DKE},
  author       = {Fatma Ghorbel and Fayçal Hamdi and Nassira Achich and Elisabeth Metais},
  doi          = {10.1016/j.datak.2020.101864},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101864},
  shortjournal = {Data Knowl. Eng.},
  title        = {Handling data imperfection—False data inputs in applications for alzheimer’s patients},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DECIDE: An agile event-and-data driven design methodology
for decisional big data projects. <em>DKE</em>, <em>130</em>, 101862.
(<a href="https://doi.org/10.1016/j.datak.2020.101862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decision making is the lifeblood of the enterprise — from the mundane to the strategically critical. However, the increasing deluge of data makes it more important than ever to understand and use it effectively in every context. Being “data driven” is more aspiration than reality in most organizations due to the complexity, volume, variability and velocity of data streams from every customer and employee interaction. The purpose of this paper is to provide a flexible and adaptable methodology for governing, managing and applying data throughout the enterprise, called DECIDE.},
  archive      = {J_DKE},
  author       = {Lilia Sfaxi and Mohamed Mehdi Ben Aissa},
  doi          = {10.1016/j.datak.2020.101862},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101862},
  shortjournal = {Data Knowl. Eng.},
  title        = {DECIDE: An agile event-and-data driven design methodology for decisional big data projects},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scalable distributed reachability query processing in
multi-labeled networks. <em>DKE</em>, <em>130</em>, 101854. (<a
href="https://doi.org/10.1016/j.datak.2020.101854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing reachability in a graph gains substantial interest as an important operation in network analysis and graph mining. In its simplest form, a reachability query is defined by a pair of nodes ( u u , v v ) and a graph G G , and detects if there is a path from u u to v v . This paper addresses a specific case of reachability on multi-labeled distributed graphs, where the query is parameterized by a set of source nodes S S , a set of target nodes T T and a set of constraints C C on the edge labels. We conduct a performance evaluation on both synthetic and real-world datasets, using multiple instances of Neo4j servers (as workers) running simultaneously. The results show that the number of workers, the network density and the number of cross-edges have a significant impact on the overall performance. Moreover, we observe that the proposed approach is scalable and can be used to solve label-constrained distributed set reachability queries in multi-labeled networks.},
  archive      = {J_DKE},
  author       = {Amina Gacem and Apostolos N. Papadopoulos and Kamel Boukhalfa},
  doi          = {10.1016/j.datak.2020.101854},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101854},
  shortjournal = {Data Knowl. Eng.},
  title        = {Scalable distributed reachability query processing in multi-labeled networks},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interpretable anomaly prediction: Predicting anomalous
behavior in industry 4.0 settings via regularized logistic regression
tools. <em>DKE</em>, <em>130</em>, 101850. (<a
href="https://doi.org/10.1016/j.datak.2020.101850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prediction of anomalous behavior in industrial assets based on sensor reading represents a key focus in modern business practice. As a matter of fact, forecast of forthcoming faults is crucial to implement predictive maintenance , i.e. maintenance decision making based on real time information from components and systems, which allows, among other benefits, to reduce maintenance cost, minimize downtime, increase safety, enhance product quality and productivity. However, building a model able to predict the future occurrence of a failure is challenging for various reasons. First, data are usually highly imbalanced , meaning that patterns describing a faulty regime are much less numerous than normal behavior instances, which makes model design difficult. Second, model predictions should be not only accurate (to avoid false alarms and missed detections) but also explainable to operators responsible for scheduling maintenance or control actions. In this paper we introduce a method called Interpretable Anomaly Prediction (IAP) allowing to handle these issues by using regularized logistic regression as core prediction model. In particular, in contrast to anomaly detection algorithms which permit to identify if the current data are anomalous or not, the proposed technique is able to predict the probability that future data will be abnormal. Furthermore, feature extraction and selection mechanisms give insights on the possible root causes leading to failures. The proposed strategy is validated with a large imbalanced multivariate time-series dataset consisting of measurements of several process variables surrounding an high pressure plunger pump situated in a complex chemical plant.},
  archive      = {J_DKE},
  author       = {Rocco Langone and Alfredo Cuzzocrea and Nikolaos Skantzos},
  doi          = {10.1016/j.datak.2020.101850},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101850},
  shortjournal = {Data Knowl. Eng.},
  title        = {Interpretable anomaly prediction: Predicting anomalous behavior in industry 4.0 settings via regularized logistic regression tools},
  volume       = {130},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An analytical model for information gathering and
propagation in social networks using random graphs. <em>DKE</em>,
<em>129</em>, 101852. (<a
href="https://doi.org/10.1016/j.datak.2020.101852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an analytical model for information gathering and propagation in social networks using random sampling. We represent the social network using the Erdos–Renyi model of the random graph. When a given node is selected in the social network, information about itself and all of its neighbors are obtained and these nodes are considered to be discovered. We provide an analytical solution for the expected number of nodes that are discovered as a function of the number of nodes randomly sampled in the graph. We use the concepts of combinatorics, probability, and inclusion–exclusion principle for computing the number of discovered nodes. This is a computationally-intensive problem with combinatorial complexity. This model is useful when crawling and mining of the social network graph is prohibited. Our work finds application in several important real-world decision support scenarios such as survey sample selection, construction of public directory, and crowdsourced databases using social networks, targeted advertising, and recommendation systems. It can also be used for finding a randomized dominating set of a graph that finds applications in computer networks, document summarization, and biological networks . We have evaluated the performance both analytically as well as by means of simulation, and the results are comparable. The results have an accuracy of around 96\% for random graphs and above 87\% for the power-law graphs.},
  archive      = {J_DKE},
  author       = {Samant Saurabh and Sanjay Madria and Anirban Mondal and Ashok Singh Sairam and Saurabh Mishra},
  doi          = {10.1016/j.datak.2020.101852},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101852},
  shortjournal = {Data Knowl. Eng.},
  title        = {An analytical model for information gathering and propagation in social networks using random graphs},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Natural logic knowledge bases and their graph form.
<em>DKE</em>, <em>129</em>, 101848. (<a
href="https://doi.org/10.1016/j.datak.2020.101848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes how knowledge bases can be represented in and reasoned with in natural logic. Natural logic is a regimented fragment of natural language possessing a well-defined logical semantics. As such, natural logic may be considered an attractive alternative among the various knowledge representation logics such as description logics . Our version of natural logic expands formal ontologies with affirmative propositions expressing a variety of relationships between concepts. It comprises (nested) restrictive relative clauses and prepositional phrases and, as a new construct, adverbial prepositional phrases. The natural logic knowledge base is to be used for deductive query answering applying inference rules. This is facilitated by introduction of Datalog as an embedding meta-logic. The inference rules are stated in Datalog and act directly on the natural logic formulations. The knowledge base propositions are decomposed into a graph form enabling path finding between concepts. The examples in the paper are derived from text source life-science descriptions.},
  archive      = {J_DKE},
  author       = {Troels Andreasen and Henrik Bulskov and Per Anker Jensen and Jørgen Fischer Nilsson},
  doi          = {10.1016/j.datak.2020.101848},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101848},
  shortjournal = {Data Knowl. Eng.},
  title        = {Natural logic knowledge bases and their graph form},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mining arguments in scientific abstracts with
discourse-level embeddings. <em>DKE</em>, <em>129</em>, 101840. (<a
href="https://doi.org/10.1016/j.datak.2020.101840">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Argument mining consists in the automatic identification of argumentative structures in texts. In this work we leverage existing discourse-level annotations to facilitate the identification of argumentative components and relations in scientific texts, which has been recognized as a particularly challenging task. We propose a new annotation schema and use it to augment a corpus of computational linguistics abstracts that had previously been annotated with discourse units and relations. Our initial experiments with the enriched corpus confirm the potential value of incorporating discourse information in argument mining tasks. In order to tackle the limitations posed by the lack of corpora containing both discourse and argumentative annotations we explore two transfer learning approaches in which discourse parsing is used as an auxiliary task when training argument mining models. In this case, as no discourse information is used as input, the resulting models could be used to predict the argumentative structure of unannotated texts.},
  archive      = {J_DKE},
  author       = {Pablo Accuosto and Horacio Saggion},
  doi          = {10.1016/j.datak.2020.101840},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101840},
  shortjournal = {Data Knowl. Eng.},
  title        = {Mining arguments in scientific abstracts with discourse-level embeddings},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Guest editorial – DaWaK 2019 special issue – evolving big
data analytics towards data science. <em>DKE</em>, <em>129</em>, 101838.
(<a href="https://doi.org/10.1016/j.datak.2020.101838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_DKE},
  author       = {Carlos Ordonez and Il-Yeol Song},
  doi          = {10.1016/j.datak.2020.101838},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101838},
  shortjournal = {Data Knowl. Eng.},
  title        = {Guest editorial – DaWaK 2019 special issue – evolving big data analytics towards data science},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design and implementation of ETL processes using BPMN and
relational algebra. <em>DKE</em>, <em>129</em>, 101837. (<a
href="https://doi.org/10.1016/j.datak.2020.101837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extraction, transformation, and loading (ETL) processes are used to extract data from internal and external sources of an organization, transform these data, and load them into a data warehouse . The Business Process Modeling and Notation (BPMN) has been proposed for expressing ETL processes at a conceptual level. A different approach is studied in this paper, where relational algebra (RA), extended with update operations, is used for specifying ETL processes. In this approach, data tasks in an ETL workflow can be automatically translated into SQL queries to be executed over a DBMS . To illustrate this study, the paper addresses the problem of updating Slowly Changing Dimensions (SCDs) with dependencies, that is, the case when updating a SCD table impacts on associated SCD tables. Tackling this problem requires extending the classic RA with update operations. The paper also shows the implementation of a portion of the TPC-DI benchmark that results from both approaches. Thus, the paper presents three implementations: (a) An SQL implementation based on the extended RA-based specification of an ETL process expressed in BPMN4ETL; and (b) Two implementations of workflows that follow from BPMN4ETL, one that uses the Pentaho DI tool, and another one that uses Talend Open Studio for DI. Experiments over these implementations of the TPC-DI benchmark for different scale factors were carried out, and are described and discussed in the paper, showing that the extended RA approach results in more efficient processes than the ones produced by implementing the BPMN4ETL specification over the mentioned ETL tools. The reasons for this result are also discussed.},
  archive      = {J_DKE},
  author       = {Judith Awiti and Alejandro A. Vaisman and Esteban Zimányi},
  doi          = {10.1016/j.datak.2020.101837},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101837},
  shortjournal = {Data Knowl. Eng.},
  title        = {Design and implementation of ETL processes using BPMN and relational algebra},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mo.re.farming: A hybrid architecture for tactical and
strategic precision agriculture. <em>DKE</em>, <em>129</em>, 101836. (<a
href="https://doi.org/10.1016/j.datak.2020.101836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we propose an innovative architecture, called Mo.Re.Farming , for handling agricultural data in an integrated fashion and supporting decision making in the precision agriculture domain. This architecture is oriented to data analysis and is inspired by Business Intelligence 2.0 approaches. It is hybrid in that it couples traditional and big data technologies to integrate heterogeneous data, at different levels of detail, from several owned and open data sources; its goal is to demonstrate that such integration is feasible and beneficial in supporting situ-specific and large-scale analyses. The proposed architecture has been developed in the context of the Mo.Re.Farming project, aimed at providing a Decision Support System for agricultural technicians in the Emilia-Romagna region and to enable analyses related to the use of water and chemical resources. The architecture is fully deployed and serves as a hub for agricultural data in Emilia-Romagna; the integrated data are made available in open access mode and can be accessed through web interfaces and through a set of web services. The paper describes the architecture from the technological and functional points of view and discusses the Mo.Re.Farming project outcomes and lessons learnt .},
  archive      = {J_DKE},
  author       = {Enrico Gallinucci and Matteo Golfarelli and Stefano Rizzi},
  doi          = {10.1016/j.datak.2020.101836},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101836},
  shortjournal = {Data Knowl. Eng.},
  title        = {Mo.Re.Farming: A hybrid architecture for tactical and strategic precision agriculture},
  volume       = {129},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PRESS: A personalised approach for mining top-k groups of
objects with subspace similarity. <em>DKE</em>, <em>128</em>, 101833.
(<a href="https://doi.org/10.1016/j.datak.2020.101833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personalised analytics is a powerful technology that can be used to improve the career, lifestyle, and health of individuals by providing them with an in-depth analysis of their characteristics as compared to other people. Existing research has often focused on mining general patterns or clusters, but without the facility for customisation to an individual’s needs. It is challenging to adapt such approaches to the personalised case, due to the high computational overhead they require for discovering patterns that are good across an entire dataset, rather than with respect to an individual. In this paper, we tackle the challenge of personalised pattern mining and propose a query-driven approach to mine objects with subspace similarity. Given a query object in a categorical dataset, our proposed algorithm, PRESS ( P e r sonalis e d S ubspace S imilarity), determines the top-k groups of objects, where each group has high similarity to the query for some particular subspace. We evaluate the efficiency and effectiveness of our approach on both synthetic and real datasets.},
  archive      = {J_DKE},
  author       = {Tahrima Hashem and Lida Rashidi and Lars Kulik and James Bailey},
  doi          = {10.1016/j.datak.2020.101833},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101833},
  shortjournal = {Data Knowl. Eng.},
  title        = {PRESS: A personalised approach for mining top-k groups of objects with subspace similarity},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A linear programming-based framework for handling missing
data in multi-granular data warehouses. <em>DKE</em>, <em>128</em>,
101832. (<a href="https://doi.org/10.1016/j.datak.2020.101832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data Warehouse (DW) and OLAP systems are first citizens of Business Intelligence tools. They are widely used in the academic and industrial communities for numerous different fields of application. Despite the maturity of DW and OLAP systems, with the advent of Big Data, more and more sources of data are available, and warehousing this data can lead to important quality issues. In this work, we focus on missing numerical and categorical in presence of aggregated facts. Motivated by the lack of a formal approach for the imputation of this kind of data taking into account all type of aggregation functions (distributive, algebraic and holistic), we propose an new methodology based on linear programming. Our methodology allows dealing with the relaxed constraints over classical SQL aggregation functions. The proposed approach is tested on two well-known datasets. Experiments show the effectiveness of the proposed approach.},
  archive      = {J_DKE},
  author       = {Sandro Bimonte and Libo Ren and Nestor Koueya},
  doi          = {10.1016/j.datak.2020.101832},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101832},
  shortjournal = {Data Knowl. Eng.},
  title        = {A linear programming-based framework for handling missing data in multi-granular data warehouses},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Natural language processing-enhanced extraction of SBVR
business vocabularies and business rules from UML use case diagrams.
<em>DKE</em>, <em>128</em>, 101822. (<a
href="https://doi.org/10.1016/j.datak.2020.101822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discovery, specification and proper representation of various aspects of business knowledge plays crucial part in model-driven information systems engineering, especially when it comes to the early stages of systems development. Being among the most applicable and advanced features of model-driven development, model transformation could help improving one of the most time- and resource-consuming efforts in this process, namely, discovery and specification of business vocabularies and business rules within the problem domain. One of our latest developments in this area was the solution for the automatic extraction of SBVR business vocabularies and business rules from UML use case diagrams, which was arguably one of the most comprehensive developments of this kind currently available in public. In this paper, we present an enhancement to our previous development by introducing a novel natural language processing component to it. This enhancement provides more advanced extraction capabilities (such as recognition of entities, entire noun and verb phrases, multinary associations) and better quality of the extraction results compared to our previous solution. The main contributions presented in this paper are pre- and post-processing algorithms, and two extraction algorithms using custom-trained POS tagger. Based on the related work findings, it is safe to state that the presented solution is novel and original in its approach of combining together M2M transformation of UML and SBVR models with natural language processing techniques in the field of model-driven information systems engineering.},
  archive      = {J_DKE},
  author       = {Paulius Danenas and Tomas Skersys and Rimantas Butleris},
  doi          = {10.1016/j.datak.2020.101822},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101822},
  shortjournal = {Data Knowl. Eng.},
  title        = {Natural language processing-enhanced extraction of SBVR business vocabularies and business rules from UML use case diagrams},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computational model for generating interactions in
conversational recommender system based on product functional
requirements. <em>DKE</em>, <em>128</em>, 101813. (<a
href="https://doi.org/10.1016/j.datak.2020.101813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conversational recommender system is a tool to help customer in deciding products they are going to buy, by conversational mechanism. By this mechanism, the system is able to imitate natural conversation between customer and professional sales support, for eliciting customer preference. However, many customers are not familiar with the technical features of multi-function and multi-feature products. A more natural way to explore customer preferences is by asking what they want to use with the product they are looking for (product functional requirements). Therefore, this paper proposes a computational model incorporating product functional requirements for interaction. The proposed model covers ontology and its structure as well as algorithms for generating interaction that comprises asking question, recommending products and presenting explanation of why a product is recommended. Based on our user studies, both expert users (familiar with product technical features) and novice users (not familiar with product technical feature) prefer our proposed interaction model than that of the flat interaction model (interaction model based on technical features). Meanwhile, functional requirements-based explanation is able to improve user trust in recommended products by 30\% for novice users and 17\% for expert users.},
  archive      = {J_DKE},
  author       = {Z.K.A. Baizal and Dwi H. Widyantoro and Nur Ulfa Maulidevi},
  doi          = {10.1016/j.datak.2020.101813},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101813},
  shortjournal = {Data Knowl. Eng.},
  title        = {Computational model for generating interactions in conversational recommender system based on product functional requirements},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Search-by-example over SQL repositories using structural and
intent-driven similarity. <em>DKE</em>, <em>128</em>, 101811. (<a
href="https://doi.org/10.1016/j.datak.2020.101811">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Searching the query log of a database system has a variety of applications. In a complex database, relevant queries in the log can serve as an initial example for query formulation, or may elucidate how to query the data in an optimized manner. Searching for queries that may cause a security or a privacy breach could be used to detect leaks of sensitive data. In general, queries in the query log can provide valuable information about how data have been accessed and used. Finding relevant queries requires conducting search over a repository of SQL queries. However, expressing the information need, to specify which queries should be retrieved, is not easy. In this paper we study the approach of search-by-example , where, given an SQL query Q Q , the goal is to retrieve queries that are similar to Q Q . We distinguish between two types of search— structural search and intent-driven search . In structural search, queries are considered similar if their textual formulations are similar, i.e., a small number of edit operations transform one query into the other. In intent-driven search, two queries are deemed similar if they were written for the same task. We illustrate these two types of similarity and the differences between them. We present four heuristics for testing query similarity. Two of the methods are exhaustive and two are less accurate and efficient. We explain how to utilize the efficient methods to boost a search using the exhaustive methods. An experimental evaluation and a user study illustrate the effectiveness of the methods.},
  archive      = {J_DKE},
  author       = {Gregory Borodin and Yaron Kanza},
  doi          = {10.1016/j.datak.2020.101811},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101811},
  shortjournal = {Data Knowl. Eng.},
  title        = {Search-by-example over SQL repositories using structural and intent-driven similarity},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Incremental clustering techniques for multi-party
privacy-preserving record linkage. <em>DKE</em>, <em>128</em>, 101809.
(<a href="https://doi.org/10.1016/j.datak.2020.101809">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Privacy-Preserving Record Linkage (PPRL) supports the integration of sensitive information from multiple datasets, in particular the privacy-preserving matching of records referring to the same entity. PPRL has gained much attention in many application areas, with the most prominent ones in the healthcare domain. PPRL techniques tackle this problem by conducting linkage on masked (encoded) values. Employing PPRL on records from multiple (more than two) parties/sources (multi-party PPRL, MP-PPRL) is an increasingly important but challenging problem that so far has not been sufficiently solved. Existing MP-PPRL approaches are limited to finding only those entities that are present in all parties thereby missing entities that match only in a subset of parties. Furthermore, previous MP-PPRL approaches face substantial scalability limitations due to the need of a large number of comparisons between masked records. We thus propose and evaluate new MP-PPRL approaches that find matches in any subset of parties and still scale to many parties. Our approaches maintain all matches within clusters, where these clusters are incrementally extended or refined by considering records from one party after the other. An empirical evaluation using multiple real datasets ranging from 3 to 26 parties each containing up to 5 million records validates that our protocols are efficient, and significantly outperform existing MP-PPRL approaches in terms of linkage quality and scalability.},
  archive      = {J_DKE},
  author       = {Dinusha Vatsalan and Peter Christen and Erhard Rahm},
  doi          = {10.1016/j.datak.2020.101809},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101809},
  shortjournal = {Data Knowl. Eng.},
  title        = {Incremental clustering techniques for multi-party privacy-preserving record linkage},
  volume       = {128},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multi-view similarity measure framework for trouble ticket
mining. <em>DKE</em>, <em>127</em>, 101800. (<a
href="https://doi.org/10.1016/j.datak.2020.101800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text similarity measures play a very important role in several text mining applications. Although there is an extensive literature on measuring the similarity between long texts, there is less work related to the measurement of similarity between short texts. And most of these works on short text similarity are based on adaptations of long-text similarity methods. Unfortunately, the description of a trouble ticket is just a kind of short texts. Thus, ticket mining applications such as ticket classification, ticket clustering, and ticket resolution recommendation often suffer from poor performance because of tickets’ particular characteristics of unstructured, short free-text with large vocabulary size, large volume, non-English dictionary words, and so on. Therefore, the ability to accurately measure the similarity between two tickets is critical to the performance of ticket mining. To address this performance issue, this paper proposes a multi-view similarity measure framework that easily integrates several kinds of existing similarity measures including surface matching based measures, semantic similarity measures and syntax based measures. Further, in order to make full use of the strengths of different similarity measures, our framework adopts four different policies to combine them. In particular, we consider a machine learning based policy that can be applied to integrate various similarity measures in a more general way, which makes our framework flexible and extensible. To demonstrate the effectiveness of measures generated from our framework, we empirically validate them on a publicly available short text data set and apply them to a real-world ticket data set from a large enterprise IT infrastructure. Some important findings obtained via the result analysis will be helpful to further improve performance.},
  archive      = {J_DKE},
  author       = {Jian Xu and Jiapeng Mu and Gaorong Chen},
  doi          = {10.1016/j.datak.2020.101800},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101800},
  shortjournal = {Data Knowl. Eng.},
  title        = {A multi-view similarity measure framework for trouble ticket mining},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Top-k user-specified preferred answers in massive graph
databases. <em>DKE</em>, <em>127</em>, 101798. (<a
href="https://doi.org/10.1016/j.datak.2020.101798">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are numerous applications where users wish to identify subsets of vertices in a social network or graph database that are of interest to them. They may specify sets of patterns and vertex properties, and each of these confers a score to a subgraph. The users want to find the subgraphs with top- k k highest scores. Examples in the real world where such subgraphs involve custom scoring methods include: techniques to identify sets of coordinated influence bots on Twitter, methods to identify suspicious subgraphs of nodes involved in nuclear proliferation networks, and sets of sockpuppet accounts seeking to illicitly influence star ratings on e-commerce platforms. All of these types of applications have numerous custom scoring methods. This motivates the concept of Scoring Queries presented in this paper — unlike past work, an important aspect of scoring queries is that the users get to choose the scoring mechanism, not the system. We present the Advanced top- k k ( ATK ) algorithm and show that it intelligently leverages graph indexes from the past but also presents novel pruning opportunities. We present an implementation of ATK showing that it beats out a baseline algorithm that builds on advanced subgraph matching methods with multiple graph database backends including Jena and GraphDB . We show that ATK scales well on real world graph databases from YouTube , Flickr , IMDb , and CiteSeerX .},
  archive      = {J_DKE},
  author       = {Noseong Park and Andrea Pugliese and Edoardo Serra and V.S. Subrahmanian},
  doi          = {10.1016/j.datak.2020.101798},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101798},
  shortjournal = {Data Knowl. Eng.},
  title        = {Top-k user-specified preferred answers in massive graph databases},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semi-automated development of conceptual models from natural
language text. <em>DKE</em>, <em>127</em>, 101796. (<a
href="https://doi.org/10.1016/j.datak.2020.101796">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The process of converting natural language specifications into conceptual models requires detailed analysis of natural language text, and designers frequently make mistakes when undertaking this transformation manually. Although many approaches have been used to partly automate this process, one of the main limitations is the lack of a domain-independent ontology that can be used as a repository for entities and relationships, thus guiding the transformation process. In this paper, a semi-automated system for mapping natural language text into conceptual models is proposed. The system, called SACMES, combines a linguistic approach with an ontological approach and human intervention to achieve the task. SACMES learns from the natural language specifications that it processes and stores the information that is learnt in a conceptual model ontology and a user history knowledge database. It then uses the stored information to improve performance and reduce the need for human intervention. The evaluation conducted on SACMES demonstrates that: (1) by using the system, precision and recall for users identifying entities of conceptual models is increased by 6\% and 13\%, respectively, while for relationships, increases are even higher, 14\% for precision and 23\% for recall; (2) the performance of the system is improved by processing more natural language requirements, and thus, the need for human intervention is decreased.},
  archive      = {J_DKE},
  author       = {Mussa Omar and George Baryannis},
  doi          = {10.1016/j.datak.2020.101796},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101796},
  shortjournal = {Data Knowl. Eng.},
  title        = {Semi-automated development of conceptual models from natural language text},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Content-based Node2Vec for representation of papers in the
scientific literature. <em>DKE</em>, <em>127</em>, 101794. (<a
href="https://doi.org/10.1016/j.datak.2020.101794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lower-dimensional representation of scientific text has attracted much attention among researchers due to its impact on many data mining and recommendation tasks. This paper studies two main research streams in scientific literature representation. First, both local and distributed representation viewpoints are reviewed and their advantages and disadvantages in lower dimensional representation are discussed. The paper then proposes a novel hybrid distributed technique for text representation. Using scientific articles as the major source of textual information, both the article’s content and citation network are used to build a distributed and universal lower dimensional representation. The superiority of the new technique to the traditional methods is then justified in predicting the existence of links in large citation graphs.},
  archive      = {J_DKE},
  author       = {B. Kazemi and A. Abhari},
  doi          = {10.1016/j.datak.2020.101794},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101794},
  shortjournal = {Data Knowl. Eng.},
  title        = {Content-based Node2Vec for representation of papers in the scientific literature},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A framework for multidimensional skyline queries over
streaming data. <em>DKE</em>, <em>127</em>, 101792. (<a
href="https://doi.org/10.1016/j.datak.2020.101792">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Skyline query has attracted a great deal of interest during last years because of its ability to help decision makers when multi-criteria objectives are to be handled. Several authors have pointed the interest of multidimensional skylines, i.e., the set of criteria become a parameter of the query. In order to efficiently evaluate these queries, index structures have been proposed. In this paper, we address the problem of efficiently handling multidimensional skyline queries in the context of streaming data. The appended records have a validity time interval after which they become outdated and hence, can be discarded. To that end, we propose a framework that handles an index structure periodically updated. Then the queries consider just the indexed data. This is the price we pay to deal with the streaming nature of the data we consider. Through extensive experiments, we demonstrate our framework’s ability to handle multidimensional skyline queries with challenging streaming data. The main criteria we consider to assess the performance of our solution are query execution time and both index structure maintenance time and its memory consumption.},
  archive      = {J_DKE},
  author       = {Karim Alami and Sofian Maabout},
  doi          = {10.1016/j.datak.2020.101792},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101792},
  shortjournal = {Data Knowl. Eng.},
  title        = {A framework for multidimensional skyline queries over streaming data},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hierarchy construction and classification of heterogeneous
information networks based on RSDAEf. <em>DKE</em>, <em>127</em>,
101790. (<a href="https://doi.org/10.1016/j.datak.2020.101790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heterogeneous information networks (HINs) composed of multiple types of nodes and links, play increasingly important roles in real life applications. Classification of the related data is an essential work in network analysis . Existing methods can effectively solve these classification tasks when they are applied to homogeneous information networks and simple data, but not for the noisy and sparse data. To address the problem, we propose Stacked Denoising Auto Encoder (SDAE) with sparse factors to learn features of nodes in heterogeneous networks . In particular, sparse factors are added in each hidden layer of the proposed stacked denoising auto-encoder to efficiently extract features from noisy and sparse data. Moreover, a relax strategy is employed to construct class hierarchy with high-quality based. Finally, nodes of the heterogeneous information network can be classified. Our proposed framework Relax strategy on Stacked Denoising Auto Encoder with sparse factors (RSDAEf) comparison with several existing methods clearly indicates RSDAEf outperforms the existing methods and achieves a classification precision of 88.3\% on DBLP dataset.},
  archive      = {J_DKE},
  author       = {Jinli Zhang and Zongli Jiang and Yongping Du and Tong Li and Yida Wang and Xiaohua Hu},
  doi          = {10.1016/j.datak.2020.101790},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101790},
  shortjournal = {Data Knowl. Eng.},
  title        = {Hierarchy construction and classification of heterogeneous information networks based on RSDAEf},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Integrating cuckoo search-grey wolf optimization and
correlative naive bayes classifier with map reduce model for big data
classification. <em>DKE</em>, <em>127</em>, 101788. (<a
href="https://doi.org/10.1016/j.datak.2019.101788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big data is progressively being used in various areas, such as industry, financial dealing, medicine, and so on, as it can handle the challenges in processing large amounts of data. One of the data mining techniques used widely and effectively to classify big data is the MapReduce model. In this paper, an approach for the classification of big data is developed using Cuckoo–Grey wolf based Correlative Naive Bayes classifier and MapReduce Model (CGCNB-MRM). Accordingly, a novel classifier, named Cuckoo–Grey wolf based Correlative Naive Bayes classifier (CG-CNB), is designed by modifying CNB classifier with a newly developed optimization algorithm , Cuckoo–Grey Wolf based Optimization (CGWO). CGWO algorithm is designed by the effective integration of Cuckoo Search (CS) Algorithm into Grey Wolf Optimizer (GWO), to optimize the CNB model by the optimal selection of the model parameters. Finally, the proposed CGCNB-MRM approach performs the classification for each data samples based on the probability index table and the posterior probability of the data. Three metrics, such as accuracy, sensitivity, and specificity, are utilized for the performance evaluation of the proposed CGCNB-MRM approach, where it could achieve 80.7\% accuracy with 84.5\% sensitivity and 76.9\% specificity and thus, prove its effectiveness in big data classification.},
  archive      = {J_DKE},
  author       = {Chitrakant Banchhor and N. Srinivasu},
  doi          = {10.1016/j.datak.2019.101788},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101788},
  shortjournal = {Data Knowl. Eng.},
  title        = {Integrating cuckoo search-grey wolf optimization and correlative naive bayes classifier with map reduce model for big data classification},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Preface | data &amp; knowledge engineering - volume 127.
<em>DKE</em>, <em>127</em>, 101775. (<a
href="https://doi.org/10.1016/j.datak.2019.101775">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_DKE},
  author       = {Max Silberztein and Elisabeth Métais and Farid Meziane and Elena Kornyshova and Faten Atigui},
  doi          = {10.1016/j.datak.2019.101775},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101775},
  shortjournal = {Data Knowl. Eng.},
  title        = {Preface | data &amp; knowledge engineering - volume 127},
  volume       = {127},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Query processing on large graphs: Approaches to scalability
and response time trade offs. <em>DKE</em>, <em>126</em>, 101736. (<a
href="https://doi.org/10.1016/j.datak.2019.101736">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphs, being an expressive data structure , have become increasingly important for modeling real-world applications, such as collaboration, different kinds of transactions, social networks, to name a few. With the advent of social networks and the web, the graph sizes have grown too large to fit in main memory precipitating the need for alternative approaches for an efficient, scalable evaluation of queries on graphs of any size. In this paper, we use the time-tested “divide and conquer” approach by partitioning a graph into desired number of partitions (and possibly with appropriate characteristics) and process queries over those partitions to obtain all or specified number of answers. This entails correctly computing answers that span multiple partitions or even need the same partition more than once. Given a set of partitions, there are a number of approaches using which a query can be evaluated: (i) O ne P artition A t a T ime ( OPAT ) approach, (ii) Traditional use of M ultiple P rocessors ( TraditionalMP ), and (iii) using the Map / Reduce M ulti- P rocessor approach ( MapReduceMP ) approach. The first approach, detailed in this paper, has established scalability through independent processing of partitions. The other two approaches address response time in addition to scalability. For the OPAT query evaluation approach, necessary minimal book keeping has been identified and its correctness established in this paper. Query answering on partitioned graphs also requires analyzing partitioning schemes for their impact on query processing and determining the number as well as the sequence in which partitions need to be loaded to reduce the response time for processing queries. We correlate query properties and partition characteristics to reduce query processing time in terms of the resources available. We also identify a set of quantitative metrics and use them for formulating heuristics to determine the order of loading partitions for efficient query processing. For OPAT approach, extensive experiments on large graphs (synthetic and real-world) using different partitioning schemes analyze the proposed heuristics on a variety of query types. The other two approaches are fleshed out, analyzed, and contrasted with the OPAT approach. An existing graph querying system has been extended to evaluate queries on partitioned graphs. Finally all three approaches are compared for their strengths and weaknesses.},
  archive      = {J_DKE},
  author       = {Soumyava Das and Abhishek Santra and Jay Bodra and Sharma Chakravarthy},
  doi          = {10.1016/j.datak.2019.101736},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101736},
  shortjournal = {Data Knowl. Eng.},
  title        = {Query processing on large graphs: Approaches to scalability and response time trade offs},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A cryptographic ensemble for secure third party data
analysis: Collaborative data clustering without data owner
participation. <em>DKE</em>, <em>126</em>, 101734. (<a
href="https://doi.org/10.1016/j.datak.2019.101734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces the twin concepts Cryptographic Ensembles and Global Encrypted Distance Matrices (GEDMs), designed to provide a solution to outsourced secure collaborative data clustering . The cryptographic ensemble comprises: Homomorphic Encryption (HE) to preserve raw data privacy, while supporting data analytics ; and Multi-User Order Preserving Encryption (MUOPE) to preserve the privacy of the GEDM. Clustering can therefore be conducted over encrypted datasets without requiring decryption or the involvement of data owners once encryption has taken place, all with no loss of accuracy. The GEDM concept is applicable to large scale collaborative data mining applications that feature horizontal data partitioning. In the paper DBSCAN clustering is adopted for illustrative and evaluation purposes. The results demonstrate that the proposed solution is both efficient and accurate while maintaining data privacy.},
  archive      = {J_DKE},
  author       = {Nawal Almutairi and Frans Coenen and Keith Dures},
  doi          = {10.1016/j.datak.2019.101734},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101734},
  shortjournal = {Data Knowl. Eng.},
  title        = {A cryptographic ensemble for secure third party data analysis: Collaborative data clustering without data owner participation},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discovering rare correlated periodic patterns in multiple
sequences. <em>DKE</em>, <em>126</em>, 101733. (<a
href="https://doi.org/10.1016/j.datak.2019.101733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Periodic-Frequent Pattern Mining (PFPM) is an emerging problem, which consists of identifying frequent patterns that periodically occur over time in a sequence of events. Though PFPM is useful in many domains, traditional algorithms have two important limitations. First, they are not designed to find rare patterns. But discovering rare patterns is useful in many domains (e.g. to study rare diseases). Second, traditional PFPM algorithms are generally designed to find patterns in a single sequence, but identifying periodic patterns that are common to a set of sequences is often desirable (e.g. to find patterns common to several hospital patients or customers). To address these limitations, this paper proposes to discover a novel type of patterns in multiple sequences called Rare Correlated Periodic Patterns. Properties of the problem are studied, and an efficient algorithm named MRCPPS (Mining Rare Correlated Periodic Patterns common to multiple Sequences) is presented to efficiently find these patterns. It relies on a novel RCPPS-list structure to avoid repeatedly scanning the database. Experiments have been done on several real datasets, and it was observed that the proposed MRCPPS algorithm can efficiently discover all rare correlated periodic patterns common to multiple sequences, and filter many non rare and correlated patterns.},
  archive      = {J_DKE},
  author       = {Philippe Fournier-Viger and Peng Yang and Zhitian Li and Jerry Chun-Wei Lin and Rage Uday Kiran},
  doi          = {10.1016/j.datak.2019.101733},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101733},
  shortjournal = {Data Knowl. Eng.},
  title        = {Discovering rare correlated periodic patterns in multiple sequences},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CloudDBGuard: A framework for encrypted data storage in
NoSQL wide column stores. <em>DKE</em>, <em>126</em>, 101732. (<a
href="https://doi.org/10.1016/j.datak.2019.101732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, cloud storage providers are widely used for outsourcing data. These remote cloud servers are not trustworthy when storing sensitive data. In this article we focus on the use case of storing data in a cloud database using a particular sub-category of NoSQL databases — so-called wide column stores. Unfortunately security was not a primary concern of the NoSQL systems designers. Using encryption before outsourcing the data can provide security. Conventional encryption however limits the options for interaction because the encrypted data lacks properties of the plaintext data that the database systems rely on. Various schemes have been proposed for property-preserving encryption in order to overcome these issues, allowing a database to process queries over encrypted data. In this article we comprehensively present details of our framework CloudDBGuard that allows using property-preserving encryption in unmodified wide column stores. It hides the complexity of the encryption and decryption process and allows various adjustments on specific use cases in order to achieve a maximum of security, functionality and performance.},
  archive      = {J_DKE},
  author       = {Lena Wiese and Tim Waage and Michael Brenner},
  doi          = {10.1016/j.datak.2019.101732},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101732},
  shortjournal = {Data Knowl. Eng.},
  title        = {CloudDBGuard: A framework for encrypted data storage in NoSQL wide column stores},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Guest editorial—DaWaK 2018 special issue—trends in big data
analytics. <em>DKE</em>, <em>126</em>, 101730. (<a
href="https://doi.org/10.1016/j.datak.2019.101730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_DKE},
  author       = {Carlos Ordonez and Ladjel Bellatreche},
  doi          = {10.1016/j.datak.2019.101730},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101730},
  shortjournal = {Data Knowl. Eng.},
  title        = {Guest Editorial—DaWaK 2018 special Issue—Trends in big data analytics},
  volume       = {126},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PRIMULE: Privacy risk mitigation for user profiles.
<em>DKE</em>, <em>125</em>, 101786. (<a
href="https://doi.org/10.1016/j.datak.2019.101786">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The availability of mobile phone data has encouraged the development of different data-driven tools, supporting social science studies and providing new data sources to the standard official statistics. However, this particular kind of data are subject to privacy concerns because they can enable the inference of personal and private information. In this paper, we address the privacy issues related to the sharing of user profiles, derived from mobile phone data, by proposing PRIMULE , a privacy risk mitigation strategy . Such a method relies on PRUDEnce (Pratesi et al., 2018), a privacy risk assessment framework that provides a methodology for systematically identifying risky-users in a set of data. An extensive experimentation on real-world data shows the effectiveness of PRIMULE strategy in terms of both quality of mobile user profiles and utility of these profiles for analytical services such as the Sociometer (Furletti et al., 2013), a data mining tool for city users classification.},
  archive      = {J_DKE},
  author       = {Francesca Pratesi and Lorenzo Gabrielli and Paolo Cintia and Anna Monreale and Fosca Giannotti},
  doi          = {10.1016/j.datak.2019.101786},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101786},
  shortjournal = {Data Knowl. Eng.},
  title        = {PRIMULE: Privacy risk mitigation for user profiles},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Utilizing adjacency of colleagues and type correlations for
enhanced link prediction. <em>DKE</em>, <em>125</em>, 101785. (<a
href="https://doi.org/10.1016/j.datak.2019.101785">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discoveries of new relationships in the network of objects have been required in various applications such as social networks, DBLP bibliographic networks and biological networks . Specifically, link prediction in heterogeneous information networks (HINs) that consist of multiple types of nodes and links has received much attention recently because many information networks of the real world are HINs. We observe various factors that affect the existence of a link in HINs. Firstly, certain structural characteristics of nodes whose types are the same as that of a source (or target) node give important information for link prediction. Secondly, in the HINs, there can be meaningful correlation between links of a particular link type and paths of a particular path type (also called a meta-path). In other words, paths of different path types affect the existence of links differently. Finally, we use the number of paths between source and target nodes to measure proximity of two nodes. Based on these observations, we newly propose several features and a prediction model. We show through various experiments that our proposed method works effectively and performs better than the other existing methods.},
  archive      = {J_DKE},
  author       = {Hyun Ji Jeong and Myoung Ho Kim},
  doi          = {10.1016/j.datak.2019.101785},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101785},
  shortjournal = {Data Knowl. Eng.},
  title        = {Utilizing adjacency of colleagues and type correlations for enhanced link prediction},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Using similarity measures in prediction of changes in
financial market stream data—experimental approach. <em>DKE</em>,
<em>125</em>, 101782. (<a
href="https://doi.org/10.1016/j.datak.2019.101782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we experimentally investigated the possibilities of using selected similarity measures for predicting future price directions in the market. The basic premise for this approach was the common assumption relating to the technical analysis, namely that “history repeats itself,” and the “instrument price reflects all factors that have an impact on its value.” This approach has been studied extensively in many publications. We purport that the subjective interpretation of the chart by the decision-maker should be taken into account. As every decision in the market in the case of manual trading or decision support systems is eventually made by a human, it is necessary to emphasize that the same situation in the market may be interpreted in a different manner by two different decision-makers. Our goal is to use the proposed similarity measure to identify past situations that occurred in the market, and invest accordingly. Under these assumptions, we tested the usefulness of selected measures proposed in the literature, as well as the measure proposed by us, on 21 financial instrument datasets divided into three groups (stock companies, currency pairs, and stock indexes). Moreover, we statistically verified the prediction efficiency for different financial instruments, including stocks, currency pairs, and stock indexes. The statistical verification demonstrated that the proposed approach exhibited higher predictive strength than the classical measures proposed in the literature.},
  archive      = {J_DKE},
  author       = {Przemysław Juszczuk and Jan Kozak and Krzysztof Kania},
  doi          = {10.1016/j.datak.2019.101782},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101782},
  shortjournal = {Data Knowl. Eng.},
  title        = {Using similarity measures in prediction of changes in financial market stream data—Experimental approach},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic monitoring of software use with recurrent neural
networks. <em>DKE</em>, <em>125</em>, 101781. (<a
href="https://doi.org/10.1016/j.datak.2019.101781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User interaction with a software may be formalized as a sequence of actions. In this paper we propose two methods – based on different representations of input actions – to address two distinct industrial issues: next action prediction and software crash risk detection. Both methods take advantage of the recurrent structure of Long Short Term Memory neural networks to capture dependencies among our sequential data as well as their capacity to potentially handle different types of input representations for the same data. Given the history of user actions in the interface, our first method aims at predicting the next action. The proposed recurrent neural network outperforms state-of-the-art proactive user interface algorithms with standard one-hot vectors as inputs. Besides, we propose to feed the LSTM with actions embeddings. This continuous representation performs better than one-hot encoded vector LSTM and its lower dimension reduces at the same time the computational cost. Using the same data set, the second method aims at crash risk detection. To address this task, we propose to use feature vectors composed of actions with above average crash probabilities as inputs of the LSTM – with the idea to take advantage of its ability to learn relevant past information to detect crash patterns. The method outperforms state-of-the-art sequence classification methods. Our approaches are demonstrated on medical imaging software logs from ten different hospitals worldwide, though they might be applied to various user interfaces in a wide range of applications.},
  archive      = {J_DKE},
  author       = {Chloé Adam and Antoine Aliotti and Fragkiskos D. Malliaros and Paul-Henry Cournède},
  doi          = {10.1016/j.datak.2019.101781},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101781},
  shortjournal = {Data Knowl. Eng.},
  title        = {Dynamic monitoring of software use with recurrent neural networks},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Corrigendum to “an effective high recall retrieval method”
[data knowl. Eng. 123 (2019) 13]. <em>DKE</em>, <em>125</em>, 101778.
(<a href="https://doi.org/10.1016/j.datak.2019.101778">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_DKE},
  author       = {Justin JongSu Song and Wookey Lee and Jafar Afshar},
  doi          = {10.1016/j.datak.2019.101778},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101778},
  shortjournal = {Data Knowl. Eng.},
  title        = {Corrigendum to “An effective high recall retrieval method” [Data knowl. eng. 123 (2019) 13]},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An overlapping community detection algorithm based on rough
clustering of links. <em>DKE</em>, <em>125</em>, 101777. (<a
href="https://doi.org/10.1016/j.datak.2019.101777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growth of networks is prevalent in almost every field due to the digital transformation of consumers, business and society at large. The unfolding of community structure in such real-world complex networks is crucial since it aids in gaining strategic insights leading to informed decisions. Moreover, the co-occurrence of disjoint, overlapping and nested community patterns in such networks demands methodologically rigorous community detection algorithms so as to foster cumulative tradition in data and knowledge engineering . In this paper, we introduce an algorithm for overlapping community detection based on granular information of links and concepts of rough set theory . First, neighborhood links around each pair of nodes are utilized to form initial link subsets. Subsequently, constrained linkage upper approximation of the link subsets is computed iteratively until convergence. The upper approximation subsets obtained during each iteration are constrained and merged using the notion of mutual link reciprocity. The experimental results on ten real-world networks and comparative evaluation with state-of-the-art community detection algorithms demonstrate the effectiveness of the proposed algorithm.},
  archive      = {J_DKE},
  author       = {Samrat Gupta and Pradeep Kumar},
  doi          = {10.1016/j.datak.2019.101777},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101777},
  shortjournal = {Data Knowl. Eng.},
  title        = {An overlapping community detection algorithm based on rough clustering of links},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning soft domain constraints in a factor graph model for
template-based information extraction. <em>DKE</em>, <em>125</em>,
101764. (<a href="https://doi.org/10.1016/j.datak.2019.101764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to accurately extract key information from textual documents is necessary in several downstream applications e.g., automatic knowledge base population from text, semantic information retrieval, question answering, or text summarization. However, information extraction (IE) systems are far from being errorless and in some cases commit errors that seem obvious to a human expert as they violate common sense or domain knowledge. Towards improving the performance of IE systems, we focus on the question of how domain knowledge can be incorporated into IE models to reduce the number of spurious extractions. Starting from the assumption that such domain knowledge cannot be incorporated explicitly and manually by domain experts due to the amount of effort and technical complexities involved, we propose a machine learning approach in which domain constraints are acquired as a byproduct of learning a model that learns to extract key information in a supervised setting. We frame the task as a template-based information extraction problem in which several dependent slots need to be automatically filled and propose a factor graph based approach to model the joint distribution of slot assignments given a text. Beyond using standard textual features in factors that score the compatibility of slot fillers in relation to the text, we use additional features that are text-independent and capture soft domain constraints. During the training process, these constraints receive a weight as part of the parameter learning process indicating how strongly a constraint should be enforced. These domain constraints are thus ‘soft’ in the sense that they can be violated, but the system learns to penalize solutions that violate them. The soft constraints we introduce come in two flavors: on the one hand we incorporate information about the mean of numerical attributes and use features that indicate how far a certain value is from the mean. We call these features single slot soft constraints . On the other hand, we model the pairwise compatibility between slot filler assignments independent of the textual context, thus modeling the (domain) compatibility of the slot assignments. We call the latter ones pairwise slot soft constraints . As main result of our work, we show that learning pairwise slot soft constraints improves the performance of our extraction model compared to single slot soft constraints by up to 6 points in F 1 F1 , leading to an F 1 F1 score of 0.91 for individual template types. Further, the human readable output format of our model enables the extraction and interpretation of the learned soft constraints. Based on this, we show in an evaluation by domain experts that more than 68\% of the learned soft constraints are regarded as plausible.},
  archive      = {J_DKE},
  author       = {Hendrik ter Horst and Matthias Hartung and Philipp Cimiano and Nicole Brazda and Hans Werner Müller and Roman Klinger},
  doi          = {10.1016/j.datak.2019.101764},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101764},
  shortjournal = {Data Knowl. Eng.},
  title        = {Learning soft domain constraints in a factor graph model for template-based information extraction},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Compressive approaches for cross-language multi-document
summarization. <em>DKE</em>, <em>125</em>, 101763. (<a
href="https://doi.org/10.1016/j.datak.2019.101763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The popularization of social networks and digital documents has quickly increased the multilingual information available on the Internet. However, this huge amount of data cannot be analyzed manually. This paper deals with Cross-Language Text Summarization (CLTS) that produces a summary in a different language from the source documents. We describe three compressive CLTS approaches that analyze the text in the source and target languages to compute the relevance of sentences. Our systems compress sentences at two levels: clusters of similar sentences are compressed using a multi-sentence compression (MSC) method and single sentences are compressed using a Neural Network model . The version of our approach using multi-sentence compression generated more informative French-to-English cross-lingual summaries than extractive state-of-the-art systems. Moreover, these cross-lingual summaries have a grammatical quality similar to extractive approaches.},
  archive      = {J_DKE},
  author       = {Elvys Linhares Pontes and Stéphane Huet and Juan-Manuel Torres-Moreno and Andréa Carneiro Linhares},
  doi          = {10.1016/j.datak.2019.101763},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101763},
  shortjournal = {Data Knowl. Eng.},
  title        = {Compressive approaches for cross-language multi-document summarization},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A knowledge representation of the beginning of the
innovation process: The front end of innovation integrative ontology
(FEI2O). <em>DKE</em>, <em>125</em>, 101760. (<a
href="https://doi.org/10.1016/j.datak.2019.101760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The initial phase of the innovation process is widely accepted as an important driver of positive results for new products and for the success of businesses. The Front End of Innovation (FEI) is a multidisciplinary area that includes a variety of activities, such as ideation, opportunity identification and analysis, feasibility analysis , global trends analysis, concept definition, customer and competitor analysis, and even business model development. Due to the number and variety of FEI responsibilities, this phase entails a considerable level of complexity and decision making. This fact is reflected in the literature, where one finds a variety of FEI approaches and proposals, seldom overlapping and offering no clear consensual guidance. This work aimed at overcoming this gap by proposing an Ontology for the Front End of Innovation as a comprehensive knowledge representation of the FEI, the so-called Front End of Innovation Integrative Ontology (FEI2O). The ontology balanced the differences and addressed the shortcomings of the main FEI Reference Models and included contributions from the field. This research builds on a combination of qualitative and quantitative methodologies. It combines the qualitative methods of interviewing and focus group discussion to collect the views of domain experts, used to refine the artefact and later to evaluate the final ontology. Quantitative analysis of data was carried out using the Attribute Agreement approach. The FEI2O explicitly provides a description of a domain regarding concepts, properties and relations of concepts. The main benefit of the FEI2O is to provide a comprehensive formal reference model and a common vocabulary.},
  archive      = {J_DKE},
  author       = {Ariane Rodrigues Pereira and João José Pinto Ferreira and Alexandra Lopes},
  doi          = {10.1016/j.datak.2019.101760},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101760},
  shortjournal = {Data Knowl. Eng.},
  title        = {A knowledge representation of the beginning of the innovation process: The front end of innovation integrative ontology (FEI2O)},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Building social networking services systems using the
relational shared-nothing parallel DBMS. <em>DKE</em>, <em>125</em>,
101756. (<a href="https://doi.org/10.1016/j.datak.2019.101756">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose methods to enable the relational model to meet scalability and functionality needs of a large-scale social networking services (SNS) system. NewSQL has emerged recently indicating that shared-nothing parallel relational DBMSs can be used to guarantee the ACID properties of transactions while keeping the high scalability of NoSQL. Leading commercial SNS systems, however, rely on a graph – not relational – data model with key–value storage and, for certain operations, suffer overhead of unnecessarily accessing multiple system nodes. Exploiting higher semantics with the relational data model could be the remedy. The solution we offer aims to perform a transaction as a set of independent local transactions whenever possible based on the conceptual semantics of the SNS database schema. First, it hierarchically clusters entities that are sitting on a path of frequently navigated one-to-many relationships, thereby avoiding inter-node joins. Second, when a multi-node delete transaction is performed over many-to-many relationships, it defers deletion of related references until they are accessed later, thereby amortizing the cost of multi-node updates. These solutions have been implemented in Odysseus/SNS — an SNS system using a shared nothing parallel DBMS. Performance evaluation using synthetic workload that reflects the real SNS workload demonstrates significant improvement in processing time. We also note that our work is the first to present the entity-relationship schema and its relational representation of the SNS database.},
  archive      = {J_DKE},
  author       = {Kyu-Young Whang and Inju Na and Tae-Seob Yun and Jin-Ah Park and Kyu-Hyun Cho and Se-Jin Kim and Ilyeop Yi and Byung Suk Lee},
  doi          = {10.1016/j.datak.2019.101756},
  journal      = {Data &amp; Knowledge Engineering},
  pages        = {101756},
  shortjournal = {Data Knowl. Eng.},
  title        = {Building social networking services systems using the relational shared-nothing parallel DBMS},
  volume       = {125},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
