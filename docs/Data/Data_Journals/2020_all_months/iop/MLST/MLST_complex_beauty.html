<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>MLST_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="mlst---118">MLST - 118</h2>
<ul>
<li><details>
<summary>
(2020). PyXtal_FF: A python library for automated force field
generation. <em>MLST</em>, <em>2</em>(2), 027001. (<a
href="https://doi.org/10.1088/2632-2153/abc940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present PyXtal_FF—a package based on Python programming language—for developing machine learning potentials (MLPs). The aim of PyXtal_FF is to promote the application of atomistic simulations through providing several choices of atom-centered descriptors and machine learning regressions in one platform. Based on the given choice of descriptors (including the atom-centered symmetry functions, embedded atom density, SO4 bispectrum, and smooth SO3 power spectrum), PyXtal_FF can train MLPs with either generalized linear regression or neural network models, by simultaneously minimizing the errors of energy/forces/stress tensors in comparison with the data from ab-initio simulations. The trained MLP model from PyXtal_FF is interfaced with the Atomic Simulation Environment (ASE) package, which allows different types of light-weight simulations such as geometry optimization, molecular dynamics simulation, and physical properties prediction. Finally, we will illustrate the performance of PyXtal_FF by applying it to investigate several material systems, including the bulk SiO 2 , high entropy alloy NbMoTaW, and elemental Pt for general purposes. Full documentation of PyXtal_FF is available at https://pyxtal-ff.readthedocs.io .},
  archive      = {J_MLST},
  author       = {Howard Yanxon and David Zagaceta and Binh Tang and David S Matteson and Qiang Zhu},
  doi          = {10.1088/2632-2153/abc940},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {2},
  pages        = {027001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {PyXtal_FF: A python library for automated force field generation},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). In situ compression artifact removal in scientific data
using deep transfer learning and experience replay. <em>MLST</em>,
<em>2</em>(2), 025010. (<a
href="https://doi.org/10.1088/2632-2153/abc326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The massive amount of data produced during simulation on high-performance computers has grown exponentially over the past decade, exacerbating the need for streaming compression and decompression methods for efficient storage and transfer of this data—key to realizing the full potential of large-scale computational science. Lossy compression approaches such as JPEG when applied to scientific simulation data realized as a stream of images can achieve good compression rates but at the cost of introducing compression artifacts and loss of information. This paper develops a unified framework for in situ compression artifact removal in which the fully convolutional neural network architectures are combined with scalable training, transfer learning, and experience replay to achieve superior accuracy and efficiency while significantly decreasing the storage footprint as compared with the traditional optimization-based approaches. We demonstrate the proposed approach and compare it with compressed sensing postprocessing and other baseline deep learning models using climate simulations and nuclear reactor simulations, both of which are driven by hyperbolic partial differential equations. Our approach when applied to remove the compression artifacts on the JPEG-compressed nuclear reactor simulation data (using a transfer-trained model that was pretrained on the climate simulation data and updated incrementally as the nuclear reactor simulation progressed), achieved a significant improvement—mean peak signal-to-noise ratio of 42.438 as compared with 27.725 obtained with the compressed sensing approach.},
  archive      = {J_MLST},
  author       = {Sandeep Madireddy and Ji Hwan Park and Sunwoo Lee and Prasanna Balaprakash and Shinjae Yoo and Wei-keng Liao and Cory D Hauck and M Paul Laiu and Richard Archibald},
  doi          = {10.1088/2632-2153/abc326},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {2},
  pages        = {025010},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {In situ compression artifact removal in scientific data using deep transfer learning and experience replay},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised interpretable learning of topological indices
invariant under permutations of atomic bands. <em>MLST</em>,
<em>2</em>(2), 025008. (<a
href="https://doi.org/10.1088/2632-2153/abcc43">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-band insulating Bloch Hamiltonians with internal or spatial symmetries, such as particle-hole or inversion, may have topologically disconnected sectors of trivial atomic-limit (momentum-independent) Hamiltonians. We present a neural-network-based protocol for finding topologically relevant indices that are invariant under transformations between such trivial atomic-limit Hamiltonians, thus corresponding to the standard classification of band insulators. The work extends the method of &#39;topological data augmentation&#39; for unsupervised learning introduced (2020 Phys. Rev. Res. 2 013354) by also generalizing and simplifying the data generation scheme and by introducing a special &#39;mod&#39; layer of the neural network appropriate for Z n classification. Ensembles of training data are generated by deforming seed objects in a way that preserves a discrete representation of continuity. In order to focus the learning on the topologically relevant indices, prior to the deformation procedure we stack the seed Bloch Hamiltonians with a complete set of symmetry-respecting trivial atomic bands. The obtained datasets are then used for training an interpretable neural network specially designed to capture the topological properties by learning physically relevant momentum space quantities, even in crystalline symmetry classes.},
  archive      = {J_MLST},
  author       = {Oleksandr Balabanov and Mats Granath},
  doi          = {10.1088/2632-2153/abcc43},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {2},
  pages        = {025008},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Unsupervised interpretable learning of topological indices invariant under permutations of atomic bands},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enabling robust offline active learning for machine learning
potentials using simple physics-based priors. <em>MLST</em>,
<em>2</em>(2), 025007. (<a
href="https://doi.org/10.1088/2632-2153/abcc44">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning surrogate models for quantum mechanical simulations have enabled the field to efficiently and accurately study material and molecular systems. Developed models typically rely on a substantial amount of data to make reliable predictions of the potential energy landscape or careful active learning (AL) and uncertainty estimates. When starting with small datasets, convergence of AL approaches is a major outstanding challenge which has limited most demonstrations to online AL. In this work we demonstrate a Δ-machine learning (ML) approach that enables stable convergence in offline AL strategies by avoiding unphysical configurations with initial datasets as little as a single data point. We demonstrate our framework&#39;s capabilities on a structural relaxation, transition state calculation, and molecular dynamics simulation, with the number of first principle calculations being cut down anywhere from 70%–90%. The approach is incorporated and developed alongside AMP torch , an open-source ML potential package, along with interactive Google Colab notebook examples.},
  archive      = {J_MLST},
  author       = {Muhammed Shuaibi and Saurabh Sivakumar and Rui Qi Chen and Zachary W Ulissi},
  doi          = {10.1088/2632-2153/abcc44},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {2},
  pages        = {025007},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Enabling robust offline active learning for machine learning potentials using simple physics-based priors},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Machine learning for analyzing and characterizing
InAsSb-based nBn photodetectors. <em>MLST</em>, <em>2</em>(2), 025006.
(<a href="https://doi.org/10.1088/2632-2153/abcf89">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper discusses two cases of applying artificial neural networks to the capacitance–voltage characteristics of InAsSb-based barrier infrared detectors. In the first case, we discuss a methodology for training a fully-connected feedforward network to predict the capacitance of the device as a function of the absorber, barrier, and contact doping densities, the barrier thickness, and the applied voltage. We verify the model&#39;s performance with physics-based justification of trends observed in single parameter sweeps, partial dependence plots, and two examples of gradient-based sensitivity analysis. The second case focuses on the development of a convolutional neural network that addresses the inverse problem, where a capacitance–voltage profile is used to predict the architectural properties of the device. The advantage of this approach is a more comprehensive characterization of a device by capacitance–voltage profiling than may be possible with other techniques. Finally, both approaches are material and device agnostic, and can be applied to other semiconductor device characteristics.},
  archive      = {J_MLST},
  author       = {Andreu Glasmann and Alexandros Kyrtsos and Enrico Bellotti},
  doi          = {10.1088/2632-2153/abcf89},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {2},
  pages        = {025006},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Machine learning for analyzing and characterizing InAsSb-based nBn photodetectors},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reinforcement learning decoders for fault-tolerant quantum
computation. <em>MLST</em>, <em>2</em>(2), 025005. (<a
href="https://doi.org/10.1088/2632-2153/abc609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Topological error correcting codes, and particularly the surface code, currently provide the most feasible road-map towards large-scale fault-tolerant quantum computation. As such, obtaining fast and flexible decoding algorithms for these codes, within the experimentally realistic and challenging context of faulty syndrome measurements, without requiring any final read-out of the physical qubits, is of critical importance. In this work, we show that the problem of decoding such codes can be naturally reformulated as a process of repeated interactions between a decoding agent and a code environment, to which the machinery of reinforcement learning can be applied to obtain decoding agents. While in principle this framework can be instantiated with environments modelling circuit level noise, we take a first step towards this goal by using deepQ learning to obtain decoding agents for a variety of simplified phenomenological noise models, which yield faulty syndrome measurements without including the propagation of errors which arise in full circuit level noise models.},
  archive      = {J_MLST},
  author       = {Ryan Sweke and Markus S Kesselring and Evert P L van Nieuwenburg and Jens Eisert},
  doi          = {10.1088/2632-2153/abc609},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {2},
  pages        = {025005},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Reinforcement learning decoders for fault-tolerant quantum computation},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Machine learning based quantification of synchrotron
radiation-induced x-ray fluorescence measurements—a case study.
<em>MLST</em>, <em>2</em>(2), 025004. (<a
href="https://doi.org/10.1088/2632-2153/abc9fb">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we describe the use of artificial neural networks (ANNs) for the quantification of x-ray fluorescence measurements. The training data were generated using Monte Carlo simulation, which avoided the use of adapted reference materials. The extension of the available dataset by means of an ANN to generate additional data was demonstrated. Particular emphasis was put on the comparability of simulated and experimental data and how the influence of deviations can be reduced. The search for the optimal hyperparameter, manual and automatic, is also described. For the presented case, we were able to train a network with a mean absolute error of 0.1 weight percent for the synthetic data and 0.7 weight percent for a set of experimental data obtained with certified reference materials.},
  archive      = {J_MLST},
  author       = {A Rakotondrajoa and M Radtke},
  doi          = {10.1088/2632-2153/abc9fb},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {2},
  pages        = {025004},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Machine learning based quantification of synchrotron radiation-induced x-ray fluorescence measurements—a case study},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Measuring transferability issues in machine-learning force
fields: The example of gold–iron interactions with linearized
potentials. <em>MLST</em>, <em>2</em>(2), 025003. (<a
href="https://doi.org/10.1088/2632-2153/abc9fd">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine-learning force fields have been increasingly employed in order to extend the possibility of current first-principles calculations. However, the transferability of the obtained potential cannot always be guaranteed in situations that are outside the original database. To study such limitation, we examined the very difficult case of the interactions in gold–iron nanoparticles. For the machine-learning potential, we employed a linearized formulation that is parameterized using a penalizing regression scheme which allows us to control the complexity of the obtained potential. We showed that while having a more complex potential allows for a better agreement with the training database, it can also lead to overfitting issues and a lower accuracy in untrained systems.},
  archive      = {J_MLST},
  author       = {Magali Benoit and Jonathan Amodeo and Ségolène Combettes and Ibrahim Khaled and Aurélien Roux and Julien Lam},
  doi          = {10.1088/2632-2153/abc9fd},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {2},
  pages        = {025003},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Measuring transferability issues in machine-learning force fields: The example of gold–iron interactions with linearized potentials},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The MLIP package: Moment tensor potentials with MPI and
active learning. <em>MLST</em>, <em>2</em>(2), 025002. (<a
href="https://doi.org/10.1088/2632-2153/abc9fe">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The subject of this paper is the technology (the &#39;how&#39;) of constructing machine-learning interatomic potentials, rather than science (the &#39;what&#39; and &#39;why&#39;) of atomistic simulations using machine-learning potentials. Namely, we illustrate how to construct moment tensor potentials using active learning as implemented in the MLIP package, focusing on the efficient ways to automatically sample configurations for the training set, how expanding the training set changes the error of predictions, how to set up ab initio calculations in a cost-effective manner, etc. The MLIP package (short for Machine-Learning Interatomic Potentials) is available at https://mlip.skoltech.ru/download/ .},
  archive      = {J_MLST},
  author       = {Ivan S Novikov and Konstantin Gubaev and Evgeny V Podryabinkin and Alexander V Shapeev},
  doi          = {10.1088/2632-2153/abc9fe},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {2},
  pages        = {025002},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {The MLIP package: Moment tensor potentials with MPI and active learning},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). InfoCGAN classification of 2D square ising configurations.
<em>MLST</em>, <em>2</em>(2), 025001. (<a
href="https://doi.org/10.1088/2632-2153/abcc45">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An InfoCGAN neural network is trained on 2D square Ising configurations conditioned on the external applied magnetic field and the temperature. The network is composed of two main sub-networks. The generator network learns to generate convincing Ising configurations and the discriminator network learns to discriminate between &#39;real&#39; and &#39;fake&#39; configurations with an additional categorical assignment prediction provided by an auxiliary network. Some of the predicted categorical assignments show agreement with the expected physical phases in the Ising model, the ferromagnetic spin-up and spin down phases as well as the high temperature weak external field phase. Additionally, configurations associated with the crossover phenomena are predicted by the model. The classification probabilities allow for a robust method of estimating the critical temperature in the vanishing field case, showing exceptional agreement with the known physics. This work indicates that a representation learning approach using an adversarial neural network can be used to identify categories that strongly resemble physical phases with no a priori information beyond raw physical configurations and the physical conditions they are subject to. Proper implementation of finite size scaling is essential for a complete machine learning approach in order to bring it in line with established statistical mechanics, which is worthwhile for future study.},
  archive      = {J_MLST},
  author       = {Nicholas Walker and Ka-Ming Tam},
  doi          = {10.1088/2632-2153/abcc45},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {2},
  pages        = {025001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {InfoCGAN classification of 2D square ising configurations},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Machine learning for neutron scattering at ORNL*.
<em>MLST</em>, <em>2</em>(2), 023001. (<a
href="https://doi.org/10.1088/2632-2153/abcf88">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) offers exciting new opportunities to extract more information from scattering data. At neutron scattering user facilities, ML has the potential to help accelerate scientific productivity by empowering facility users with insight into their data which has traditionally been supplied by scattering experts. Such support can help in both speeding up common modeling problems for users, as well as help solve harder problems that are normally time consuming and difficult to address with standard methods. This article explores the recent ML work undertaken at Oak Ridge National Laboratory involving neutron scattering data. We cover materials structure modeling for diffuse scattering, powder diffraction, and small-angle scattering. We also discuss how ML can help to model the response of the instrument more precisely, as well as enable quick extraction of information from neutron data. The application of super-resolution techniques to small-angle scattering and peak extraction for diffraction will be discussed.},
  archive      = {J_MLST},
  author       = {Mathieu Doucet and Anjana M Samarakoon and Changwoo Do and William T Heller and Richard Archibald and D Alan Tennant and Thomas Proffen and Garrett E Granroth},
  doi          = {10.1088/2632-2153/abcf88},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {2},
  pages        = {023001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Machine learning for neutron scattering at ORNL*},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graph neural networks in particle physics. <em>MLST</em>,
<em>2</em>(2), 021001. (<a
href="https://doi.org/10.1088/2632-2153/abbf9a">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Particle physics is a branch of science aiming at discovering the fundamental laws of matter and forces. Graph neural networks are trainable functions which operate on graphs—sets of elements and their pairwise relations—and are a central method within the broader field of geometric deep learning. They are very expressive and have demonstrated superior performance to other classical deep learning approaches in a variety of domains. The data in particle physics are often represented by sets and graphs and as such, graph neural networks offer key advantages. Here we review various applications of graph neural networks in particle physics, including different graph constructions, model architectures and learning objectives, as well as key open problems in particle physics for which graph neural networks are promising.},
  archive      = {J_MLST},
  author       = {Jonathan Shlomi and Peter Battaglia and Jean-Roch Vlimant},
  doi          = {10.1088/2632-2153/abbf9a},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {2},
  pages        = {021001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Graph neural networks in particle physics},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Natural evolution strategies and variational monte carlo.
<em>MLST</em>, <em>2</em>(2), 02LT01. (<a
href="https://doi.org/10.1088/2632-2153/abcb50">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A notion of quantum natural evolution strategies is introduced, which provides a geometric synthesis of a number of known quantum/classical algorithms for performing classical black-box optimization. The recent work of Gomes et al (2019 arXiv:1910.10675) on heuristic combinatorial optimization using neural quantum states is pedagogically reviewed in this context, emphasizing the connection with natural evolution strategies (NES). The algorithmic framework is illustrated for approximate combinatorial optimization problems, and a systematic strategy is found for improving the approximation ratios. In particular, it is found that NES can achieve approximation ratios competitive with widely used heuristic algorithms for Max-Cut, at the expense of increased computation time.},
  archive      = {J_MLST},
  author       = {Tianchen Zhao and Giuseppe Carleo and James Stokes and Shravan Veerapaneni},
  doi          = {10.1088/2632-2153/abcb50},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {2},
  pages        = {02LT01},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Natural evolution strategies and variational monte carlo},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reinforcement learning enhanced quantum-inspired algorithm
for combinatorial optimization. <em>MLST</em>, <em>2</em>(2), 025009.
(<a href="https://doi.org/10.1088/2632-2153/abc328">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum hardware and quantum-inspired algorithms are becoming increasingly popular for combinatorial optimization. However, these algorithms may require careful hyperparameter tuning for each problem instance. We use a reinforcement learning agent in conjunction with a quantum-inspired algorithm to solve the Ising energy minimization problem, which is equivalent to the Maximum Cut problem. The agent controls the algorithm by tuning one of its parameters with the goal of improving recently seen solutions. We propose a new Rescaled Ranked Reward (R3) method that enables a stable single-player version of self-play training and helps the agent escape local optima. The training on any problem instance can be accelerated by applying transfer learning from an agent trained on randomly generated problems. Our approach allows sampling high quality solutions to the Ising problem with high probability and outperforms both baseline heuristics and a black-box hyperparameter optimization approach.},
  archive      = {J_MLST},
  author       = {Dmitrii Beloborodov and A E Ulanov and Jakob N Foerster and Shimon Whiteson and A I Lvovsky},
  doi          = {10.1088/2632-2153/abc328},
  journal      = {Machine Learning: Science and Technology},
  month        = {11},
  number       = {2},
  pages        = {025009},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Reinforcement learning enhanced quantum-inspired algorithm for combinatorial optimization},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Corrigendum: Impact of non-normal error distributions on
the benchmarking and ranking of quantum machine learning models (2020
mach. Learn.: Sci. Technol. 1 035011). <em>MLST</em>, <em>2</em>(1),
019501. (<a href="https://doi.org/10.1088/2632-2153/abc350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_MLST},
  author       = {Pascal Pernot and Bing Huang and Andreas Savin},
  doi          = {10.1088/2632-2153/abc350},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {1},
  pages        = {019501},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Corrigendum: impact of non-normal error distributions on the benchmarking and ranking of quantum machine learning models (2020 mach. learn.: sci. technol. 1 035011)},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Artificial applicability labels for improving policies in
retrosynthesis prediction. <em>MLST</em>, <em>2</em>(1), 017001. (<a
href="https://doi.org/10.1088/2632-2153/abcf90">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated retrosynthetic planning algorithms are a research area of increasing importance. Automated reaction-template extraction from large datasets, in conjunction with neural-network-enhanced tree-search algorithms, can find plausible routes to target compounds in seconds. However, the current method for training neural networks to predict suitable templates for a given target product leads to many predictions that are not applicable in silico . Most templates in the top 50 suggested templates cannot be applied to the target molecule to perform the virtual reaction. Here, we describe how to generate data and train a neural network policy that predicts whether templates are applicable or not. First, we generate a massive training dataset by applying each retrosynthetic template to each product from our reaction database. Second, we train a neural network to perform near-perfect prediction of the applicability labels on a held-out test set. The trained network is then joined with a policy model trained to predict and prioritize templates using the labels from the original dataset. The combined model was found to outperform the policy model in a route-finding task using 1700 compounds from our internal drug-discovery projects.},
  archive      = {J_MLST},
  author       = {Esben Jannik Bjerrum and Amol Thakkar and Ola Engkvist},
  doi          = {10.1088/2632-2153/abcf90},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {1},
  pages        = {017001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Artificial applicability labels for improving policies in retrosynthesis prediction},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving the segmentation of scanning probe microscope
images using convolutional neural networks. <em>MLST</em>,
<em>2</em>(1), 015015. (<a
href="https://doi.org/10.1088/2632-2153/abc81c">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A wide range of techniques can be considered for segmentation of images of nanostructured surfaces. Manually segmenting these images is time-consuming and results in a user-dependent segmentation bias, while there is currently no consensus on the best automated segmentation methods for particular techniques, image classes, and samples. Any image segmentation approach must minimise the noise in the images to ensure accurate and meaningful statistical analysis can be carried out. Here we develop protocols for the segmentation of images of 2D assemblies of gold nanoparticles formed on silicon surfaces via deposition from an organic solvent. The evaporation of the solvent drives far-from-equilibrium self-organisation of the particles, producing a wide variety of nano- and micro-structured patterns. We show that a segmentation strategy using the U-Net convolutional neural network has some benefits over traditional automated approaches and has particular potential in the processing of images of nanostructured systems.},
  archive      = {J_MLST},
  author       = {Steff Farley and Jo E A Hodgkinson and Oliver M Gordon and Joanna Turner and Andrea Soltoggio and Philip J Moriarty and Eugenie Hunsicker},
  doi          = {10.1088/2632-2153/abc81c},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {1},
  pages        = {015015},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Improving the segmentation of scanning probe microscope images using convolutional neural networks},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Online accelerator optimization with a machine
learning-based stochastic algorithm. <em>MLST</em>, <em>2</em>(1),
015014. (<a href="https://doi.org/10.1088/2632-2153/abc81e">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Online optimization is critical for realizing the design performance of accelerators. Highly efficient stochastic optimization algorithms are needed for many online accelerator optimization problems in order to find the global optimum in the non-linear, coupled parameter space. In this study, we propose to use the multi-generation Gaussian process optimizer for online accelerator optimization and demonstrate that the algorithm is significantly more efficient than other stochastic algorithms that are commonly used in the accelerator community.},
  archive      = {J_MLST},
  author       = {Zhe Zhang and Minghao Song and Xiaobiao Huang},
  doi          = {10.1088/2632-2153/abc81e},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {1},
  pages        = {015014},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Online accelerator optimization with a machine learning-based stochastic algorithm},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An automated approach for determining the number of
components in non-negative matrix factorization with application to
mutational signature learning. <em>MLST</em>, <em>2</em>(1), 015013. (<a
href="https://doi.org/10.1088/2632-2153/abc60a">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-negative matrix factorization (NMF) is a popular method for finding a low rank approximation of a matrix, thereby revealing the latent components behind it. In genomics, NMF is widely used to interpret mutation data and derive the underlying mutational processes and their activities. A key challenge in the use of NMF is determining the number of components, or rank of the factorization. Here we propose a novel method, CV2K, to choose this number automatically from data that is based on a detailed cross validation procedure combined with a parsimony consideration. We apply our method for mutational signature analysis and demonstrate its utility on both simulated and real data sets. In comparison to previous approaches, some of which involve human assessment, CV2K leads to improved predictions across a wide range of data sets.},
  archive      = {J_MLST},
  author       = {Gal Gilad and Itay Sason and Roded Sharan},
  doi          = {10.1088/2632-2153/abc60a},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {1},
  pages        = {015013},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {An automated approach for determining the number of components in non-negative matrix factorization with application to mutational signature learning},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Noise2Filter: Fast, self-supervised learning and real-time
reconstruction for 3D computed tomography. <em>MLST</em>, <em>2</em>(1),
015012. (<a href="https://doi.org/10.1088/2632-2153/abbd4d">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At x-ray beamlines of synchrotron light sources, the achievable time-resolution for 3D tomographic imaging of the interior of an object has been reduced to a fraction of a second, enabling rapidly changing structures to be examined. The associated data acquisition rates require sizable computational resources for reconstruction. Therefore, full 3D reconstruction of the object is usually performed after the scan has completed. Quasi-3D reconstruction—where several interactive 2D slices are computed instead of a 3D volume—has been shown to be significantly more efficient, and can enable the real-time reconstruction and visualization of the interior. However, quasi-3D reconstruction relies on filtered backprojection type algorithms, which are typically sensitive to measurement noise. To overcome this issue, we propose Noise2Filter, a learned filter method that can be trained using only the measured data, and does not require any additional training data. This method combines quasi-3D reconstruction, learned filters, and self-supervised learning to derive a tomographic reconstruction method that can be trained in under a minute and evaluated in real-time. We show limited loss of accuracy compared to training with additional training data, and improved accuracy compared to standard filter-based methods.},
  archive      = {J_MLST},
  author       = {Marinus J Lagerwerf and Allard A Hendriksen and Jan-Willem Buurlage and K Joost Batenburg},
  doi          = {10.1088/2632-2153/abbd4d},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {1},
  pages        = {015012},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Noise2Filter: Fast, self-supervised learning and real-time reconstruction for 3D computed tomography},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hybrid classical-quantum workflow for natural language
processing. <em>MLST</em>, <em>2</em>(1), 015011. (<a
href="https://doi.org/10.1088/2632-2153/abbd2e">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Natural language processing (NLP) problems are ubiquitous in classical computing, where they often require significant computational resources to infer sentence meanings. With the appearance of quantum computing hardware and simulators, it is worth developing methods to examine such problems on these platforms. In this manuscript we demonstrate the use of quantum computing models to perform NLP tasks, where we represent corpus meanings, and perform comparisons between sentences of a given structure. We develop a hybrid workflow for representing small and large scale corpus data sets to be encoded, processed, and decoded using a quantum circuit model. In addition, we provide our results showing the efficacy of the method, and release our developed toolkit as an open software suite.},
  archive      = {J_MLST},
  author       = {Lee J O’Riordan and Myles Doyle and Fabio Baruffa and Venkatesh Kannan},
  doi          = {10.1088/2632-2153/abbd2e},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {1},
  pages        = {015011},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {A hybrid classical-quantum workflow for natural language processing},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Detecting symmetries with neural networks. <em>MLST</em>,
<em>2</em>(1), 015010. (<a
href="https://doi.org/10.1088/2632-2153/abbd2d">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifying symmetries in data sets is generally difficult, but knowledge about them is crucial for efficient data handling. Here we present a method how neural networks can be used to identify symmetries. We make extensive use of the structure in the embedding layer of the neural network which allows us to identify whether a symmetry is present and to identify orbits of the symmetry in the input. To determine which continuous or discrete symmetry group is present we analyse the invariant orbits in the input. We present examples based on rotation groups SO ( n ) and the unitary group SU (2). Further we find that this method is useful for the classification of complete intersection Calabi-Yau manifolds where it is crucial to identify discrete symmetries on the input space. For this example we present a novel data representation in terms of graphs.},
  archive      = {J_MLST},
  author       = {Sven Krippendorf and Marc Syvaeri},
  doi          = {10.1088/2632-2153/abbd2d},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {1},
  pages        = {015010},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Detecting symmetries with neural networks},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graph prolongation convolutional networks: Explicitly
multiscale machine learning on graphs with applications to modeling of
cytoskeleton. <em>MLST</em>, <em>2</em>(1), 015009. (<a
href="https://doi.org/10.1088/2632-2153/abb6d2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We define a novel type of ensemble graph convolutional network (GCN) model. Using optimized linear projection operators to map between spatial scales of graph, this ensemble model learns to aggregate information from each scale for its final prediction. We calculate these linear projection operators as the infima of an objective function relating the structure matrices used for each GCN. Equipped with these projections, our model (a Graph Prolongation-Convolutional Network) outperforms other GCN ensemble models at predicting the potential energy of monomer subunits in a coarse-grained mechanochemical simulation of microtubule bending. We demonstrate these performance gains by measuring an estimate of the Floating Point OPerations spent to train each model, as well as wall-clock time. Because our model learns at multiple scales, it is possible to train at each scale according to a predetermined schedule of coarse vs. fine training. We examine several such schedules adapted from the algebraic multigrid literature, and quantify the computational benefit of each. We also compare this model to another model which features an optimized coarsening of the input graph. Finally, we derive backpropagation rules for the input of our network model with respect to its output, and discuss how our method may be extended to very large graphs.},
  archive      = {J_MLST},
  author       = {Cory B Scott and Eric Mjolsness},
  doi          = {10.1088/2632-2153/abb6d2},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {1},
  pages        = {015009},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Graph prolongation convolutional networks: Explicitly multiscale machine learning on graphs with applications to modeling of cytoskeleton},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quantum computation with machine-learning-controlled quantum
stuff. <em>MLST</em>, <em>2</em>(1), 015008. (<a
href="https://doi.org/10.1088/2632-2153/abb215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We formulate the control over quantum matter, so as to perform arbitrary quantum computation, as an optimization problem. We then provide a schematic machine learning algorithm for its solution. Imagine a long strip of &#39;quantum stuff&#39;, endowed with certain assumed physical properties, and equipped with regularly spaced wires to provide input settings and to read off outcomes. After showing how the corresponding map from settings to outcomes can be construed as a quantum circuit, we provide a machine learning framework to tomographically &#39;learn&#39; which settings implement the members of a universal gate set. To that end, we devise a loss function measuring how badly a proposed encoding has failed to implement a given circuit, and prove the existence of &#39;tomographically complete&#39; circuit sets: should a given encoding minimize the loss function for each member of such a set, it also will for an arbitrary circuit. At optimum, arbitrary quantum gates, and thus arbitrary quantum programs, can be implemented using the stuff.},
  archive      = {J_MLST},
  author       = {Lucien Hardy and Adam G M Lewis},
  doi          = {10.1088/2632-2153/abb215},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {1},
  pages        = {015008},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Quantum computation with machine-learning-controlled quantum stuff},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generalizability issues with deep learning models in
medicine and their potential solutions: Illustrated with cone-beam
computed tomography (CBCT) to computed tomography (CT) image conversion.
<em>MLST</em>, <em>2</em>(1), 015007. (<a
href="https://doi.org/10.1088/2632-2153/abb214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalizability is a concern when applying a deep learning (DL) model trained on one dataset to other datasets. It is challenging to demonstrate a DL model&#39;s generalizability efficiently and sufficiently before implementing the model in clinical practice. Training a universal model that works anywhere, anytime, for anybody is unrealistic. In this work, we demonstrate the generalizability problem, then explore potential solutions based on transfer learning by using the cone-beam computed tomography (CBCT) to computed tomography (CT) image conversion task as the testbed. Previous works only studied on one or two anatomical sites and used images from the same vendor&#39;s scanners. Here, we investigated how a model trained for one machine and one anatomical site works on other machines and other anatomical sites. We trained a model on CBCT images acquired from one vendor&#39;s scanners for head and neck cancer patients and applied it to images from another vendor&#39;s scanners and for prostate, pancreatic, and cervical cancer patients. We found that generalizability could be a significant problem for this particular application when applying a trained DL model to datasets from another vendor&#39;s scanners. We then explored three practical solutions based on transfer learning to solve this generalization problem: the target model, which is trained on a target dataset from scratch; the combined model, which is trained on both source and target datasets from scratch; and the adapted model, which fine-tunes the trained source model to a target dataset. We found that when there are sufficient data in the target dataset, all three models can achieve good performance. When the target dataset is limited, the adapted model works the best, which indicates that using the fine-tuning strategy to adapt the trained model to an unseen target dataset is a viable and easy way to implement DL models in the clinic.},
  archive      = {J_MLST},
  author       = {Xiao Liang and Dan Nguyen and Steve B Jiang},
  doi          = {10.1088/2632-2153/abb214},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {1},
  pages        = {015007},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Generalizability issues with deep learning models in medicine and their potential solutions: Illustrated with cone-beam computed tomography (CBCT) to computed tomography (CT) image conversion},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Using deep learning to enhance event geometry reconstruction
for the telescope array surface detector. <em>MLST</em>, <em>2</em>(1),
015006. (<a href="https://doi.org/10.1088/2632-2153/abae74">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The extremely low flux of ultra-high energy cosmic rays (UHECR) makes their direct observation by orbital experiments practically impossible. For this reason all current and planned UHECR experiments detect cosmic rays indirectly by observing the extensive air showers (EAS) initiated by cosmic ray particles in the atmosphere. The world largest statistics of the ultra-high energy EAS events is recorded by the networks of surface stations. In this paper we consider a novel approach for reconstruction of the arrival direction of the primary particle based on the deep convolutional neural network. The latter is using raw time-resolved signals of the set of the adjacent trigger stations as an input. The Telescope Array (TA) Surface Detector (SD) is an array of 507 stations, each containing two layers plastic scintillator with an area of 3 m 2 . The training of the model is performed with the Monte-Carlo dataset. It is shown that within the Monte-Carlo simulations, the new approach yields better resolution than the traditional reconstruction method based on the fitting of the EAS front. The details of the network architecture and its optimization for this particular task are discussed.},
  archive      = {J_MLST},
  author       = {D Ivanov and O E Kalashev and M Yu Kuznetsov and G I Rubtsov and T Sako and Y Tsunesada and Y V Zhezher},
  doi          = {10.1088/2632-2153/abae74},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {1},
  pages        = {015006},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Using deep learning to enhance event geometry reconstruction for the telescope array surface detector},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Site2Vec: A reference frame invariant algorithm for vector
embedding of protein–ligand binding sites. <em>MLST</em>, <em>2</em>(1),
015005. (<a href="https://doi.org/10.1088/2632-2153/abad88">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Protein–ligand interactions are one of the fundamental types of molecular interactions in living systems. Ligands are small molecules that interact with protein molecules at specific regions on their surfaces called binding sites. Binding sites would also determine ADMET properties of a drug molecule. Tasks such as assessment of protein functional similarity and detection of side effects of drugs need identification of similar binding sites of disparate proteins across diverse pathways. To this end, methods for computing similarities between binding sites are still evolving and is an active area of research even today. Machine learning methods for similarity assessment require feature descriptors of binding sites. Traditional methods based on hand engineered motifs and atomic configurations are not scalable across several thousands of sites. In this regard, deep neural network algorithms are now deployed which can capture very complex input feature space. However, one fundamental challenge in applying deep learning to structures of binding sites is the input representation and the reference frame. We report here a novel algorithm, Site2Vec, that derives reference frame invariant vector embedding of a protein–ligand binding site. The method is based on pairwise distances between representative points and chemical compositions in terms of constituent amino acids of a site. The vector embedding serves as a locality sensitive hash function for proximity queries and determining similar sites. The method has been the top performer with more than 95% quality scores in extensive benchmarking studies carried over 10 data sets and against 23 other site comparison methods in the field. The algorithm serves for high throughput processing and has been evaluated for stability with respect to reference frame shifts, coordinate perturbations and residue mutations. We also provide the method as a standalone executable and a web service hosted at (http://services.iittp.ac.in/bioinfo/home).},
  archive      = {J_MLST},
  author       = {Arnab Bhadra and Kalidas Yeturu},
  doi          = {10.1088/2632-2153/abad88},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {1},
  pages        = {015005},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Site2Vec: A reference frame invariant algorithm for vector embedding of protein–ligand binding sites},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). IDQ: Statistical inference of non-gaussian noise with
auxiliary degrees of freedom in gravitational-wave detectors.
<em>MLST</em>, <em>2</em>(1), 015004. (<a
href="https://doi.org/10.1088/2632-2153/abab5f">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gravitational-wave detectors are exquisitely sensitive instruments and routinely enable ground-breaking observations of novel astronomical phenomena. However, they also witness non-stationary, non-Gaussian noise that can be mistaken for astrophysical sources, lower detection confidence, or simply complicate the extraction of signal parameters from noisy data. To address this, we present iDQ, a supervised learning framework to autonomously detect noise artifacts in gravitational-wave detectors based only on auxiliary degrees of freedom insensitive to gravitational waves. iDQ has operated in low latency throughout the advanced detector era at each of the two LIGO interferometers, providing invaluable data quality information about each detection to date in real-time. We document the algorithm, describing the statistical framework and possible applications within gravitational-wave searches. In particular, we construct a likelihood-ratio test that simultaneously accounts for the presence of non-Gaussian noise artifacts and utilizes information from both the observed gravitational-wave strain signal and thousands of auxiliary degrees of freedom. We also present several examples of iDQ&#39;s performance with modern interferometers, showing iDQ&#39;s ability to autonomously reproduce known data quality monitors and identify noise artifacts not flagged by other analyses.},
  archive      = {J_MLST},
  author       = {Reed Essick and Patrick Godwin and Chad Hanna and Lindy Blackburn and Erik Katsavounidis},
  doi          = {10.1088/2632-2153/abab5f},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {1},
  pages        = {015004},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {IDQ: Statistical inference of non-gaussian noise with auxiliary degrees of freedom in gravitational-wave detectors},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fractional deep neural network via constrained optimization.
<em>MLST</em>, <em>2</em>(1), 015003. (<a
href="https://doi.org/10.1088/2632-2153/aba8e7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a novel algorithmic framework for a deep neural network (DNN), which in a mathematically rigorous manner, allows us to incorporate history (or memory) into the network—it ensures all layers are connected to one another. This DNN, called Fractional-DNN, can be viewed as a time-discretization of a fractional in time non-linear ordinary differential equation (ODE). The learning problem then is a minimization problem subject to that fractional ODE as constraints. We emphasize that an analogy between the existing DNN and ODEs, with standard time derivative, is well-known by now. The focus of our work is the Fractional-DNN. Using the Lagrangian approach, we provide a derivation of the backward propagation and the design equations. We test our network on several datasets for classification problems. Fractional-DNN offers various advantages over the existing DNN. The key benefits are a significant improvement to the vanishing gradient issue due to the memory effect, and better handling of nonsmooth data due to the network&#39;s ability to approximate non-smooth functions.},
  archive      = {J_MLST},
  author       = {Harbir Antil and Ratna Khatri and Rainald Löhner and Deepanshu Verma},
  doi          = {10.1088/2632-2153/aba8e7},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {1},
  pages        = {015003},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Fractional deep neural network via constrained optimization},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deeply uncertain: Comparing methods of uncertainty
quantification in deep learning algorithms. <em>MLST</em>,
<em>2</em>(1), 015002. (<a
href="https://doi.org/10.1088/2632-2153/aba6f3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a comparison of methods for uncertainty quantification (UQ) in deep learning algorithms in the context of a simple physical system. Three of the most common uncertainty quantification methods—Bayesian neural networks (BNNs), concrete dropout (CD), and deep ensembles (DEs) — are compared to the standard analytic error propagation. We discuss this comparison in terms endemic to both machine learning (&#39;epistemic&#39; and &#39;aleatoric&#39;) and the physical sciences (&#39;statistical&#39; and &#39;systematic&#39;). The comparisons are presented in terms of simulated experimental measurements of a single pendulum—a prototypical physical system for studying measurement and analysis techniques. Our results highlight some pitfalls that may occur when using these UQ methods. For example, when the variation of noise in the training set is small, all methods predicted the same relative uncertainty independently of the inputs. This issue is particularly hard to avoid in BNN. On the other hand, when the test set contains samples far from the training distribution, we found that no methods sufficiently increased the uncertainties associated to their predictions. This problem was particularly clear for CD. In light of these results, we make some recommendations for usage and interpretation of UQ methods.},
  archive      = {J_MLST},
  author       = {João Caldeira and Brian Nord},
  doi          = {10.1088/2632-2153/aba6f3},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {1},
  pages        = {015002},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Deeply uncertain: Comparing methods of uncertainty quantification in deep learning algorithms},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Compressing deep neural networks on FPGAs to binary and
ternary precision with hls4ml. <em>MLST</em>, <em>2</em>(1), 015001. (<a
href="https://doi.org/10.1088/2632-2153/aba042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present the implementation of binary and ternary neural networks in the hls4ml library, designed to automatically convert deep neural network models to digital circuits with field-programmable gate arrays (FPGA) firmware. Starting from benchmark models trained with floating point precision, we investigate different strategies to reduce the network&#39;s resource consumption by reducing the numerical precision of the network parameters to binary or ternary. We discuss the trade-off between model accuracy and resource consumption. In addition, we show how to balance between latency and accuracy by retaining full precision on a selected subset of network components. As an example, we consider two multiclass classification tasks: handwritten digit recognition with the MNIST data set and jet identification with simulated proton-proton collisions at the CERN Large Hadron Collider. The binary and ternary implementation has similar performance to the higher precision implementation while using drastically fewer FPGA resources.},
  archive      = {J_MLST},
  author       = {Jennifer Ngadiuba and Vladimir Loncar and Maurizio Pierini and Sioni Summers and Giuseppe Di Guglielmo and Javier Duarte and Philip Harris and Dylan Rankin and Sergo Jindariani and Mia Liu and Kevin Pedro and Nhan Tran and Edward Kreinar and Sheila Sagear and Zhenbin Wu and Duc Hoang},
  doi          = {10.1088/2632-2153/aba042},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {1},
  pages        = {015001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Compressing deep neural networks on FPGAs to binary and ternary precision with hls4ml},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enhancing gravitational-wave science with machine learning.
<em>MLST</em>, <em>2</em>(1), 011002. (<a
href="https://doi.org/10.1088/2632-2153/abb93a">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning has emerged as a popular and powerful approach for solving problems in astrophysics. We review applications of machine learning techniques for the analysis of ground-based gravitational-wave (GW) detector data. Examples include techniques for improving the sensitivity of Advanced Laser Interferometer GW Observatory and Advanced Virgo GW searches, methods for fast measurements of the astrophysical parameters of GW sources, and algorithms for reduction and characterization of non-astrophysical detector noise. These applications demonstrate how machine learning techniques may be harnessed to enhance the science that is possible with current and future GW detectors.},
  archive      = {J_MLST},
  author       = {Elena Cuoco and Jade Powell and Marco Cavaglià and Kendall Ackley and Michał Bejger and Chayan Chatterjee and Michael Coughlin and Scott Coughlin and Paul Easter and Reed Essick and Hunter Gabbard and Timothy Gebhard and Shaon Ghosh and Leïla Haegel and Alberto Iess and David Keitel and Zsuzsa Márka and Szabolcs Márka and Filip Morawski and Tri Nguyen and Rich Ormiston and Michael Pürrer and Massimiliano Razzano and Kai Staats and Gabriele Vajente and Daniel Williams},
  doi          = {10.1088/2632-2153/abb93a},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {1},
  pages        = {011002},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Enhancing gravitational-wave science with machine learning},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Randomized algorithms for fast computation of low rank
tensor ring model. <em>MLST</em>, <em>2</em>(1), 011001. (<a
href="https://doi.org/10.1088/2632-2153/abad87">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomized algorithms are efficient techniques for big data tensor analysis. In this tutorial paper, we review and extend a variety of randomized algorithms for decomposing large-scale data tensors in Tensor Ring (TR) format. We discuss both adaptive and nonadaptive randomized algorithms for this task. Our main focus is on the random projection technique as an efficient randomized framework and how it can be used to decompose large-scale data tensors in the TR format. Simulations are provided to support the presentation and efficiency, and performance of the presented algorithms are compared.},
  archive      = {J_MLST},
  author       = {Salman Ahmadi-Asl and Andrzej Cichocki and Anh Huy Phan and Maame G Asante-Mensah and Mirfarid Musavian Ghazani and Toshihisa Tanaka and Ivan Oseledets},
  doi          = {10.1088/2632-2153/abad87},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {1},
  pages        = {011001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Randomized algorithms for fast computation of low rank tensor ring model},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Classifying global state preparation via deep reinforcement
learning. <em>MLST</em>, <em>2</em>(1), 01LT02. (<a
href="https://doi.org/10.1088/2632-2153/abc81f">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum information processing often requires the preparation of arbitrary quantum states, such as all the states on the Bloch sphere for two-level systems. While numerical optimization can prepare individual target states, they lack the ability to find general control protocols that can generate many different target states. Here, we demonstrate global quantum control by preparing a continuous set of states with deep reinforcement learning. The protocols are represented using neural networks, which automatically groups the protocols into similar types, which could be useful for finding classes of protocols and extracting physical insights. As application, we generate arbitrary superposition states for the electron spin in complex multi-level nitrogen-vacancy centers, revealing classes of protocols characterized by specific preparation timescales. Our method could help improve control of near-term quantum computers, quantum sensing devices and quantum simulations.},
  archive      = {J_MLST},
  author       = {Tobias Haug and Wai-Keong Mok and Jia-Bin You and Wenzu Zhang and Ching Eng Png and Leong-Chuan Kwek},
  doi          = {10.1088/2632-2153/abc81f},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {1},
  pages        = {01LT02},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Classifying global state preparation via deep reinforcement learning},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The influence of sorbitol doping on aggregation and
electronic properties of PEDOT:PSS: A theoretical study. <em>MLST</em>,
<em>2</em>(1), 01LT01. (<a
href="https://doi.org/10.1088/2632-2153/ab983b">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many organic electronics applications such as organic solar cells or thermoelectric generators rely on PEDOT:PSS as a conductive polymer that is printable and transparent. It was found that doping PEDOT:PSS with sorbitol enhances the conductivity through morphological changes. However, the microscopic mechanism is not well understood. In this work, we combine computational tools with machine learning to investigate changes in morphological and electronic properties of PEDOT:PSS when doped with sorbitol. We find that sorbitol improves the alignment of PEDOT oligomers, leading to a reduction of energy disorder and an increase in electronic couplings between PEDOT chains. The high accuracy ( r 2 &gt; 0.9) and speed up of energy level predictions of neural networks compared to density functional theory enables us to analyze HOMO energies of PEDOT oligomers as a function of time. We find a surprisingly low degree of static energy disorder compared to other organic semiconductors. This finding might help to better understand the microscopic origin of the high charge carrier mobility of PEDOT:PSS in general and potentially help to design new conductive polymers.},
  archive      = {J_MLST},
  author       = {Pascal Friederich and Salvador León and José Darío Perea and Loïc M Roch and Alán Aspuru-Guzik},
  doi          = {10.1088/2632-2153/ab983b},
  journal      = {Machine Learning: Science and Technology},
  month        = {12},
  number       = {1},
  pages        = {01LT01},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {The influence of sorbitol doping on aggregation and electronic properties of PEDOT:PSS: a theoretical study},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A path towards quantum advantage in training deep generative
models with quantum annealers. <em>MLST</em>, <em>1</em>(4), 045028. (<a
href="https://doi.org/10.1088/2632-2153/aba220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of quantum-classical hybrid (QCH) algorithms is critical to achieve state-of-the-art computational models. A QCH variational autoencoder (QVAE) was introduced in reference [ 1 ] by some of the authors of this paper. QVAE consists of a classical auto-encoding structure realized by traditional deep neural networks to perform inference to and generation from, a discrete latent space. The latent generative process is formalized as thermal sampling from a quantum Boltzmann machine (QBM). This setup allows quantum-assisted training of deep generative models by physically simulating the generative process with quantum annealers. In this paper, we have successfully employed D-Wave quantum annealers as Boltzmann samplers to perform quantum-assisted, end-to-end training of QVAE. The hybrid structure of QVAE allows us to deploy current-generation quantum annealers in QCH generative models to achieve competitive performance on datasets such as MNIST. The results presented in this paper suggest that commercially available quantum annealers can be deployed, in conjunction with well-crafted classical deep neutral networks, to achieve competitive results in unsupervised and semisupervised tasks on large-scale datasets. We also provide evidence that our setup is able to exploit large latent-space QBMs, which develop slowly mixing modes. This expressive latent space results in slow and inefficient classical sampling and paves the way to achieve quantum advantage with quantum annealing in realistic sampling applications.},
  archive      = {J_MLST},
  author       = {Walter Winci and Lorenzo Buffoni and Hossein Sadeghi and Amir Khoshaman and Evgeny Andriyash and Mohammad H Amin},
  doi          = {10.1088/2632-2153/aba220},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045028},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {A path towards quantum advantage in training deep generative models with quantum annealers},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Predicting excited states from ground state wavefunction by
supervised quantum machine learning. <em>MLST</em>, <em>1</em>(4),
045027. (<a href="https://doi.org/10.1088/2632-2153/aba183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Excited states of molecules lie in the heart of photochemistry and chemical reactions. The recent development in quantum computational chemistry leads to inventions of a variety of algorithms that calculate the excited states of molecules on near-term quantum computers, but they require more computational burdens than the algorithms for calculating the ground states. In this study, we propose a scheme of supervised quantum machine learning which predicts the excited-state properties of molecules only from their ground state wavefunction resulting in reducing the computational cost for calculating the excited states. Our model is comprised of a quantum reservoir and a classical machine learning unit which processes the measurement results of single-qubit Pauli operators with the output state from the reservoir. The quantum reservoir effectively transforms the single-qubit operators into complicated multi-qubit ones which contain essential information of the system, so that the classical machine learning unit may decode them appropriately. The number of runs for quantum computers is saved by training only the classical machine learning unit, and the whole model requires modest resources of quantum hardware that may be implemented in current experiments. We illustrate the predictive ability of our model by numerical simulations for small molecules with and without noise inevitable in near-term quantum computers. The results show that our scheme reproduces well the first and second excitation energies as well as the transition dipole moment between the ground states and excited states only from the ground states as inputs. We expect our contribution will enhance the applications of quantum computers in the study of quantum chemistry and quantum materials.},
  archive      = {J_MLST},
  author       = {Hiroki Kawai and Yuya O. Nakagawa},
  doi          = {10.1088/2632-2153/aba183},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045027},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Predicting excited states from ground state wavefunction by supervised quantum machine learning},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Thousands of reactants and transition states for competing
e2 and s<span class="math inline"><sub>N</sub></span><!-- -->2
reactions. <em>MLST</em>, <em>1</em>(4), 045026. (<a
href="https://doi.org/10.1088/2632-2153/aba822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reaction barriers are a crucial ingredient for first principles based computational retro-synthesis efforts as well as for comprehensive reactivity assessments throughout chemical compound space. While extensive databases of experimental results exist, modern quantum machine learning applications require atomistic details which can only be obtained from quantum chemistry protocols. For competing E2 and S _\mathrm{N} 2 reaction channels we report 4,466 transition state and 143,200 reactant complex geometries and energies at MP2/6-311G(d) and single point DF-LCCSD/cc-pVTZ level of theory, respectively, covering the chemical compound space spanned by the substituents NO 2 , CN, CH 3 , and NH 2 and early halogens (F, Cl, Br) and hydrogen as nucleophiles and early halogens as leaving groups. Reactants are chosen such that the activation energy of the competing E2 and S _\mathrm{N} 2 reactions are of comparable magnitude. The correct concerted motion for each of the one-step reactions has been validated for all transition states. We demonstrate how quantum machine learning models can support data set extension, and discuss the distribution of key internal coordinates of the transition states.},
  archive      = {J_MLST},
  author       = {Guido Falk von Rudorff and Stefan N Heinen and Marco Bragato and O Anatole von Lilienfeld},
  doi          = {10.1088/2632-2153/aba822},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045026},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Thousands of reactants and transition states for competing e2 and S$_\mathrm{N}$2 reactions},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards recognizing the light facet of the higgs boson.
<em>MLST</em>, <em>1</em>(4), 045025. (<a
href="https://doi.org/10.1088/2632-2153/aba8e6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Higgs boson couplings to bottom and top quarks have been measured and agree well with the Standard Model predictions. Decays to lighter quarks and gluons, however, remain elusive. Observing these decays is essential to complete the picture of the Higgs boson interactions. In this work, we present the perspectives for the 14 TeV LHC to observe the Higgs boson decay to gluon jets assembling convolutional neural networks, trained to recognize abstract jet images constructed embodying particle flow information, and boosted decision trees with kinetic information from Higgs-strahlung ZH\to \ell^{+}\ell^{-} + gg events. We show that this approach might be able to observe Higgs to gluon decays with a significance of around 2.4 σ improving significantly previous prospects based on cut-and-count analysis. An upper bound of BR ( H → gg )≤1.74 × BR SM ( H → gg ) at 95% confidence level after 3000 fb −1 of data is obtained using these machine learning techniques.},
  archive      = {J_MLST},
  author       = {Alexandre Alves and Felipe F Freitas},
  doi          = {10.1088/2632-2153/aba8e6},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045025},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Towards recognizing the light facet of the higgs boson},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Self-referencing embedded strings (SELFIES): A 100.
<em>MLST</em>, <em>1</em>(4), 045024. (<a
href="https://doi.org/10.1088/2632-2153/aba947">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discovery of novel materials and functional molecules can help to solve some of society&#39;s most urgent challenges, ranging from efficient energy harvesting and storage to uncovering novel pharmaceutical drug candidates. Traditionally matter engineering–generally denoted as inverse design–was based massively on human intuition and high-throughput virtual screening. The last few years have seen the emergence of significant interest in computer-inspired designs based on evolutionary or deep learning methods. The major challenge here is that the standard strings molecular representation SMILES shows substantial weaknesses in that task because large fractions of strings do not correspond to valid molecules. Here, we solve this problem at a fundamental level and introduce S ELFIES (SELF-referencIng Embedded Strings), a string-based representation of molecules which is 100% robust. Every S ELFIES string corresponds to a valid molecule, and S ELFIES can represent every molecule. S ELFIES can be directly applied in arbitrary machine learning models without the adaptation of the models; each of the generated molecule candidates is valid. In our experiments, the model&#39;s internal memory stores two orders of magnitude more diverse molecules than a similar test with SMILES. Furthermore, as all molecules are valid, it allows for explanation and interpretation of the internal working of the generative models.},
  archive      = {J_MLST},
  author       = {Mario Krenn and Florian Häse and AkshatKumar Nigam and Pascal Friederich and Alan Aspuru-Guzik},
  doi          = {10.1088/2632-2153/aba947},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045024},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Self-referencing embedded strings (SELFIES): A 100% robust molecular string representation},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). I- flow: High-dimensional integration and sampling with
normalizing flows. <em>MLST</em>, <em>1</em>(4), 045023. (<a
href="https://doi.org/10.1088/2632-2153/abab62">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many fields of science, high-dimensional integration is required. Numerical methods have been developed to evaluate these complex integrals. We introduce the code i-flow , a Python package that performs high-dimensional numerical integration utilizing normalizing flows. Normalizing flows are machine-learned, bijective mappings between two distributions. i-flow can also be used to sample random points according to complicated distributions in high dimensions. We compare i-flow to other algorithms for high-dimensional numerical integration and show that i-flow outperforms them for high dimensional correlated integrals. The i-flow code is publicly available on gitlab at https://gitlab.com/i-flow/i-flow .},
  archive      = {J_MLST},
  author       = {Christina Gao and Joshua Isaacson and Claudius Krause},
  doi          = {10.1088/2632-2153/abab62},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045023},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {I- flow: High-dimensional integration and sampling with normalizing flows},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pulse shape discrimination and exploration of scintillation
signals using convolutional neural networks. <em>MLST</em>,
<em>1</em>(4), 045022. (<a
href="https://doi.org/10.1088/2632-2153/abb781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We demonstrate the use of a convolutional neural network to perform neutron-gamma pulse shape discrimination, where the only inputs to the network are the raw digitised silicon photomultiplier signals from a dual scintillator detector element made of 6 Li F:ZnS(Ag) scintillator and PVT plastic. A realistic labelled dataset was created to train the network by exposing the detector to an AmBe source, and a data-driven method utilising a separate photomultiplier tube was used to assign labels to the recorded signals. This approach is compared to the charge integration and continuous wavelet transform methods and a simpler artificial neural net. It is found to provide superior levels of discrimination, achieving an area under the curve of 0.996 ± 0.003. We find that the neural network is capable of extracting interpretable features directly from the raw data. In addition, by visualising the high-dimensional representations of the network with the t-SNE algorithm, we discover that not only is this method robust to minor mislabeling of the training dataset but that it is possible to identify an underlying substructure within the signals that goes beyond the original labelling. This technique could be utilised to explore and cluster complex, raw detector data in a novel way that may reveal more insights than standard analysis methods.},
  archive      = {J_MLST},
  author       = {J Griffiths and S Kleinegesse and D Saunders and R Taylor and A Vacheret},
  doi          = {10.1088/2632-2153/abb781},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045022},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Pulse shape discrimination and exploration of scintillation signals using convolutional neural networks},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Structure-property maps with kernel principal covariates
regression. <em>MLST</em>, <em>1</em>(4), 045021. (<a
href="https://doi.org/10.1088/2632-2153/aba9ef">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data analyses based on linear methods constitute the simplest, most robust, and transparent approaches to the automatic processing of large amounts of data for building supervised or unsupervised machine learning models. Principal covariates regression (PCovR) is an underappreciated method that interpolates between principal component analysis and linear regression and can be used conveniently to reveal structure-property relations in terms of simple-to-interpret, low-dimensional maps. Here we provide a pedagogic overview of these data analysis schemes, including the use of the kernel trick to introduce an element of non-linearity while maintaining most of the convenience and the simplicity of linear approaches. We then introduce a kernelized version of PCovR and a sparsified extension, and demonstrate the performance of this approach in revealing and predicting structure-property relations in chemistry and materials science, showing a variety of examples including elemental carbon, porous silicate frameworks, organic molecules, amino acid conformers, and molecular materials.},
  archive      = {J_MLST},
  author       = {Benjamin A Helfrecht and Rose K Cersonsky and Guillaume Fraux and Michele Ceriotti},
  doi          = {10.1088/2632-2153/aba9ef},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045021},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Structure-property maps with kernel principal covariates regression},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Coarse-grain cluster analysis of tensors with application to
climate biome identification. <em>MLST</em>, <em>1</em>(4), 045020. (<a
href="https://doi.org/10.1088/2632-2153/abb676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A tensor provides a concise way to codify the interdependence of complex data. Treating a tensor as a d-way array, each entry records the interaction between the different indices. Clustering provides a way to parse the complexity of the data into more readily understandable information. Clustering methods are heavily dependent on the algorithm of choice, as well as the chosen hyperparameters of the algorithm. However, their sensitivity to data scales is largely unknown. In this work, we apply the discrete wavelet transform to analyze the effects of coarse-graining on clustering tensor data. We are particularly interested in understanding how scale affects clustering of the Earth&#39;s climate system. The discrete wavelet transform allows classification of the Earth&#39;s climate across a multitude of spatial-temporal scales. The discrete wavelet transform is used to produce an ensemble of classification estimates, as opposed to a single classification. Each element of the ensemble is a clustering at a different spatial-temporal scale. Information theoretic approaches are used to identify important scale lengths in clustering the L15 Climate Datset. We also discover a sub-collection of the ensemble that spans the majority of the variance observed, allowing for efficient consensus clustering techniques that can be used to identify climate biomes.},
  archive      = {J_MLST},
  author       = {Derek DeSantis and Phillip J Wolfram and Katrina Bennett and Boian Alexandrov},
  doi          = {10.1088/2632-2153/abb676},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045020},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Coarse-grain cluster analysis of tensors with application to climate biome identification},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep learning of chaos classification. <em>MLST</em>,
<em>1</em>(4), 045019. (<a
href="https://doi.org/10.1088/2632-2153/abb6d3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We train an artificial neural network which distinguishes chaotic and regular dynamics of the two-dimensional Chirikov standard map. We use finite length trajectories and compare the performance with traditional numerical methods which need to evaluate the Lyapunov exponent (LE). The neural network has superior performance for short periods with length down to 10 Lyapunov times on which the traditional LE computation is far from converging. We show the robustness of the neural network to varying control parameters, in particular we train with one set of control parameters, and successfully test in a complementary set. Furthermore, we use the neural network to successfully test the dynamics of discrete maps in different dimensions, e.g. the one-dimensional logistic map and a three-dimensional discrete version of the Lorenz system. Our results demonstrate that a convolutional neural network can be used as an excellent chaos indicator.},
  archive      = {J_MLST},
  author       = {Woo Seok Lee and Sergej Flach},
  doi          = {10.1088/2632-2153/abb6d3},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045019},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Deep learning of chaos classification},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the role of gradients for machine learning of molecular
energies and forces. <em>MLST</em>, <em>1</em>(4), 045018. (<a
href="https://doi.org/10.1088/2632-2153/abba6f">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accuracy of any machine learning potential can only be as good as the data used in the fitting process. The most efficient model therefore selects the training data that will yield the highest accuracy compared to the cost of obtaining the training data. We investigate the convergence of prediction errors of quantum machine learning models for organic molecules trained on energy and force labels, two common data types in molecular simulations. When training models for the potential energy surface of a single molecule, we find that the inclusion of atomic forces in the training data increases the accuracy of the predicted energies and forces 7-fold, compared to models trained on energy only. Surprisingly, for models trained on sets of organic molecules of varying size and composition in non-equilibrium conformations, inclusion of forces in the training does not improve the predicted energies of unseen molecules in new conformations. Predicted forces, however, improve about 7-fold. For the systems studied, we find that force labels and energy labels contribute equally per label to the convergence of the prediction errors. The optimal choice of what type of training data to include depends on several factors: the computational cost of acquiring the force and energy labels for training, the application domain, the property of interest and the complexity of the machine learning model. Based on our observations we describe key considerations for the creation of new datasets for potential energy surfaces of molecules which maximize the efficiency of the resulting machine learning models.},
  archive      = {J_MLST},
  author       = {Anders S Christensen and O Anatole von Lilienfeld},
  doi          = {10.1088/2632-2153/abba6f},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045018},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {On the role of gradients for machine learning of molecular energies and forces},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Determination of latent dimensionality in international
trade flow. <em>MLST</em>, <em>1</em>(4), 045017. (<a
href="https://doi.org/10.1088/2632-2153/aba9ee">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, high-dimensional data is ubiquitous in data science, which necessitates the development of techniques to decompose and interpret such multidimensional (aka tensor) datasets. Finding a low dimensional representation of the data, that is, its inherent structure, is one of the approaches that can serve to understand the dynamics of low dimensional latent features hidden in the data. Moreover, decomposition methods with non-negative constraints are shown to extract more insightful factors. Nonnegative RESCAL is one such technique, particularly well suited to analyze self-relational data, such as dynamic networks found in international trade flows. Particularly, non-negative RESCAL computes a low dimensional tensor representation by finding the latent space containing multiple modalities. Furthermore, estimating the dimensionality of this latent space is crucial for extracting meaningful latent features. Here, to determine the dimensionality of the latent space with non-negative RESCAL, we propose a latent dimension determination method which is based on clustering of the solutions of multiple realizations of non-negative RESCAL decompositions. We demonstrate the performance of our model selection method on synthetic data. We then apply our method to decompose a network of international trade flows data from International Monetary Fund and shows that with a correct latent dimension determination, the resulting features are able to capture relevant empirical facts from economic literature.},
  archive      = {J_MLST},
  author       = {Duc P Truong and Erik Skau and Vladimir I Valtchinov and Boian S Alexandrov},
  doi          = {10.1088/2632-2153/aba9ee},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045017},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Determination of latent dimensionality in international trade flow},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Uncovering interpretable relationships in high-dimensional
scientific data through function preserving projections. <em>MLST</em>,
<em>1</em>(4), 045016. (<a
href="https://doi.org/10.1088/2632-2153/abab60">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many fields of science and engineering, we frequently encounter experiments or simulations datasets that describe the behavior of complex systems and uncovering human interpretable patterns between their inputs and outputs via exploratory data analysis is essential for building intuition and facilitating discovery. Often, we resort to 2D embeddings for examining these high-dimensional relationships (e.g. dimensionality reduction). However, most existing embedding methods treat the dimensions as coordinates for samples in a high-dimensional space, which fail to capture the potential functional relationships, and the few methods that do take function into consideration either only focus on linear patterns or produce non-linear embeddings that are hard to interpret. To address these challenges, we proposed function preserving projections (FPP), which construct 2D linear embeddings optimized to reveal interpretable yet potentially non-linear patterns between the domain and the range of a high-dimensional function. The intuition here is that humans are good at understanding potentially non-linear patterns in 2D but unable to interpret non-linear mapping from high-dimensional space to 2D. Therefore, we should restrict the projection to linear but not the pattern we are seeking. Using FPP on real-world datasets, one can obtain fundamentally new insights about high-dimensional relationships in extremely large datasets that could not be processed with existing dimension reduction methods.},
  archive      = {J_MLST},
  author       = {Shusen Liu and Rushil Anirudh and Jayaraman J Thiagarajan and Peer-Timo Bremer},
  doi          = {10.1088/2632-2153/abab60},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045016},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Uncovering interpretable relationships in high-dimensional scientific data through function preserving projections},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). K-means-driven gaussian process data collection for
angle-resolved photoemission spectroscopy. <em>MLST</em>, <em>1</em>(4),
045015. (<a href="https://doi.org/10.1088/2632-2153/abab61">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose the combination of k-means clustering with Gaussian Process (GP) regression in the analysis and exploration of 4D angle-resolved photoemission spectroscopy (ARPES) data. Using cluster labels as the driving metric on which the GP is trained, this method allows us to reconstruct the experimental phase diagram from as low as 12% of the original dataset size. In addition to the phase diagram, the GP is able to reconstruct spectra in energy-momentum space from this minimal set of data points. These findings suggest that this methodology can be used to improve the efficiency of ARPES data collection strategies for unknown samples. The practical feasibility of implementing this technology at a synchrotron beamline and the overall efficiency implications of this method are discussed with a view on enabling the collection of more samples or rapid identification of regions of interest.},
  archive      = {J_MLST},
  author       = {Charles N Melton and Marcus M Noack and Taisuke Ohta and Thomas E Beechem and Jeremy Robinson and Xiaotian Zhang and Aaron Bostwick and Chris Jozwiak and Roland J Koch and Petrus H Zwart and Alexander Hexemer and Eli Rotenberg},
  doi          = {10.1088/2632-2153/abab61},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045015},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {K-means-driven gaussian process data collection for angle-resolved photoemission spectroscopy},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adversarial reverse mapping of equilibrated condensed-phase
molecular structures. <em>MLST</em>, <em>1</em>(4), 045014. (<a
href="https://doi.org/10.1088/2632-2153/abb6d4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A tight and consistent link between resolutions is crucial to further expand the impact of multiscale modeling for complex materials. We herein tackle the generation of condensed molecular structures as a refinement—backmapping—of a coarse-grained (CG) structure. Traditional schemes start from a rough coarse-to-fine mapping and perform further energy minimization and molecular dynamics simulations to equilibrate the system. In this study we introduce DeepBackmap: A deep neural network based approach to directly predict equilibrated molecular structures for condensed-phase systems. We use generative adversarial networks to learn the Boltzmann distribution from training data and realize reverse mapping by using the CG structure as a conditional input. We apply our method to a challenging condensed-phase polymeric system. We observe that the model trained in a melt has remarkable transferability to the crystalline phase. The combination of data-driven and physics-based aspects of our architecture help reach temperature transferability with only limited training data.},
  archive      = {J_MLST},
  author       = {Marc Stieffenhofer and Michael Wand and Tristan Bereau},
  doi          = {10.1088/2632-2153/abb6d4},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045014},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Adversarial reverse mapping of equilibrated condensed-phase molecular structures},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep reinforcement learning for optical systems: A case
study of mode-locked lasers. <em>MLST</em>, <em>1</em>(4), 045013. (<a
href="https://doi.org/10.1088/2632-2153/abb6d6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We demonstrate that deep reinforcement learning (deep RL) provides a highly effective strategy for the control and self-tuning of optical systems. Deep RL integrates the two leading machine learning architectures of deep neural networks and reinforcement learning to produce robust and stable learning for control. Deep RL is ideally suited for optical systems as the tuning and control relies on interactions with its environment with a goal-oriented objective to achieve optimal immediate or delayed rewards. This allows the optical system to recognize bi-stable structures and navigate, via trajectory planning, to optimally performing solutions, the first such algorithm demonstrated to do so in optical systems. We specifically demonstrate the deep RL architecture on a mode-locked laser, where robust self-tuning and control can be established through access of the deep RL agent to its waveplates and polarizers. We further integrate transfer learning to help the deep RL agent rapidly learn new parameter regimes and generalize its control authority. Additionally, the deep RL learning can be easily integrated with other control paradigms to provide a broad framework to control any optical system.},
  archive      = {J_MLST},
  author       = {Chang Sun and Eurika Kaiser and Steven L Brunton and J Nathan Kutz},
  doi          = {10.1088/2632-2153/abb6d6},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045013},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Deep reinforcement learning for optical systems: A case study of mode-locked lasers},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Baryon density extraction and isotropy analysis of cosmic
microwave background using deep learning. <em>MLST</em>, <em>1</em>(4),
045012. (<a href="https://doi.org/10.1088/2632-2153/abab63">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The discovery of cosmic microwave background (CMB) was a paradigm shift in the study and fundamental understanding of the early Universe and also the Big Bang phenomenon. Cosmic microwave background is one of the richest and intriguing sources of information available to cosmologists and one parameter of special interest is baryon density of the Universe. Baryon density can be primarily estimated by analyzing CMB data or through the study of big bang nucleosynthesis (BBN). Hence, it is necessary that both of the results found through the two methods are in agreement with each other. Although there are some well established statistical methods for the analysis of CMB to estimate baryon density, here we explore the use of deep learning in this respect. We correlate the baryon density obtained from the power spectrum of simulated CMB temperature maps with the corresponding map image and form the dataset for training the neural network model. We analyze the accuracy with which the model is able to predict the results from a relatively abstract dataset considering the fact that CMB is a Gaussian random field. CMB is anisotropic due to temperature fluctuations at small scales but on a larger scale CMB is considered isotropic, here we analyze the isotropy of CMB by training the model with CMB maps centered at different galactic coordinates and compare the predictions of neural network models.},
  archive      = {J_MLST},
  author       = {Amit Mishra and Pranath Reddy and Rahul Nigam},
  doi          = {10.1088/2632-2153/abab63},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045012},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Baryon density extraction and isotropy analysis of cosmic microwave background using deep learning},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reducing autocorrelation times in lattice simulations with
generative adversarial networks. <em>MLST</em>, <em>1</em>(4), 045011.
(<a href="https://doi.org/10.1088/2632-2153/abae73">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Short autocorrelation times are essential for a reliable error assessment in Monte Carlo simulations of lattice systems. In many interesting scenarios, the decay of autocorrelations in the Markov chain is prohibitively slow. Generative samplers can provide statistically independent field configurations, thereby potentially ameliorating these issues. In this work, the applicability of neural samplers to this problem is investigated. Specifically, we work with a generative adversarial network (GAN). We propose to address difficulties regarding its statistical exactness through the implementation of an overrelaxation step, by searching the latent space of the trained generator network. This procedure can be incorporated into a standard Monte Carlo algorithm, which then permits a sensible assessment of ergodicity and balance based on consistency checks. Numerical results for real, scalar φ 4 -theory in two dimensions are presented. We achieve a significant reduction of autocorrelations while accurately reproducing the correct statistics. We discuss possible improvements to the approach as well as potential solutions to persisting issues.},
  archive      = {J_MLST},
  author       = {Jan M Pawlowski and Julian M Urban},
  doi          = {10.1088/2632-2153/abae73},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045011},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Reducing autocorrelation times in lattice simulations with generative adversarial networks},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving the generative performance of chemical
autoencoders through transfer learning. <em>MLST</em>, <em>1</em>(4),
045010. (<a href="https://doi.org/10.1088/2632-2153/abae75">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generative models are a sub-class of machine learning models that are capable of generating new samples with a target set of properties. In chemical and materials applications, these new samples might be drug targets, novel semiconductors, or catalysts constrained to exhibit an application-specific set of properties. Given their potential to yield high-value targets from otherwise intractable design spaces, generative models are currently under intense study with respect to how predictions can be improved through changes in model architecture and data representation. Here we explore the potential of multi-task transfer learning as a complementary approach to improving the validity and property specificity of molecules generated by such models. We have compared baseline generative models trained on a single property prediction task against models trained on additional ancillary prediction tasks and observe a generic positive impact on the validity and specificity of the multi-task models. In particular, we observe that the validity of generated structures is strongly affected by whether or not the models have chemical property data, as opposed to only syntactic structural data, supplied during learning. We demonstrate this effect in both interpolative and extrapolative scenarios (i.e., where the generative targets are poorly represented in training data) for models trained to generate high energy structures and models trained to generated structures with targeted bandgaps within certain ranges. In both instances, the inclusion of additional chemical property data improves the ability of models to generate valid, unique structures with increased property specificity. This approach requires only minor alterations to existing generative models, in many cases leveraging prediction frameworks already native to these models. Additionally, the transfer learning strategy is complementary to ongoing efforts to improve model architectures and data representation and can foreseeably be stacked on top of these developments.},
  archive      = {J_MLST},
  author       = {Nicolae C Iovanac and Brett M Savoie},
  doi          = {10.1088/2632-2153/abae75},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045010},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Improving the generative performance of chemical autoencoders through transfer learning},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Machine learning reveals complex behaviours in optically
trapped particles. <em>MLST</em>, <em>1</em>(4), 045009. (<a
href="https://doi.org/10.1088/2632-2153/abae76">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since their invention in the 1980s, optical tweezers have found a wide range of applications, from biophotonics and mechanobiology to microscopy and optomechanics. Simulations of the motion of microscopic particles held by optical tweezers are often required to explore complex phenomena and to interpret experimental data. For the sake of computational efficiency, these simulations usually model the optical tweezers as an harmonic potential. However, more physically-accurate optical-scattering models are required to accurately model more onerous systems; this is especially true for optical traps generated with complex fields. Although accurate, these models tend to be prohibitively slow for problems with more than one or two degrees of freedom (DoF), which has limited their broad adoption. Here, we demonstrate that machine learning permits one to combine the speed of the harmonic model with the accuracy of optical-scattering models. Specifically, we show that a neural network can be trained to rapidly and accurately predict the optical forces acting on a microscopic particle. We demonstrate the utility of this approach on two phenomena that are prohibitively slow to accurately simulate otherwise: the escape dynamics of swelling microparticles in an optical trap, and the rotation rates of particles in a superposition of beams with opposite orbital angular momenta. Thanks to its high speed and accuracy, this method can greatly enhance the range of phenomena that can be efficiently simulated and studied.},
  archive      = {J_MLST},
  author       = {Isaac C D Lenton and Giovanni Volpe and Alexander B Stilgoe and Timo A Nieminen and Halina Rubinsztein-Dunlop},
  doi          = {10.1088/2632-2153/abae76},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045009},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Machine learning reveals complex behaviours in optically trapped particles},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quantum computing model of an artificial neuron with
continuously valued input data. <em>MLST</em>, <em>1</em>(4), 045008.
(<a href="https://doi.org/10.1088/2632-2153/abaf98">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial neural networks have been proposed as potential algorithms that could benefit from being implemented and run on quantum computers. In particular, they hold promise to greatly enhance Artificial Intelligence tasks, such as image elaboration or pattern recognition. The elementary building block of a neural network is an artificial neuron, i.e. a computational unit performing simple mathematical operations on a set of data in the form of an input vector. Here we show how the design for the implementation of a previously introduced quantum artificial neuron [ npj Quant. Inf. 5 , 26], which fully exploits the use of superposition states to encode binary valued input data, can be further generalized to accept continuous- instead of discrete-valued input vectors, without increasing the number of qubits. This further step is crucial to allow for a direct application of gradient descent based learning procedures, which would not be compatible with binary-valued data encoding.},
  archive      = {J_MLST},
  author       = {Stefano Mangini and Francesco Tacchino and Dario Gerace and Chiara Macchiavello and Daniele Bajoni},
  doi          = {10.1088/2632-2153/abaf98},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045008},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Quantum computing model of an artificial neuron with continuously valued input data},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast reconstruction of single-shot wide-angle diffraction
images through deep learning. <em>MLST</em>, <em>1</em>(4), 045007. (<a
href="https://doi.org/10.1088/2632-2153/abb213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-shot x-ray imaging of short-lived nanostructures such as clusters and nanoparticles near a phase transition or non-crystalizing objects such as large proteins and viruses is currently the most elegant method for characterizing their structure. Using hard x-ray radiation provides scattering images that encode two-dimensional projections, which can be combined to identify the full three-dimensional object structure from multiple identical samples. Wide-angle scattering using XUV or soft x-rays, despite yielding lower resolution, provides three-dimensional structural information in a single shot and has opened routes towards the characterization of non-reproducible objects in the gas phase. The retrieval of the structural information contained in wide-angle scattering images is highly non-trivial, and currently no efficient rigorous algorithm is known. Here we show that deep learning networks, trained with simulated scattering data, allow for fast and accurate reconstruction of shape and orientation of nanoparticles from experimental images. The gain in speed compared to conventional retrieval techniques opens the route for automated structure reconstruction algorithms capable of real-time discrimination and pre-identification of nanostructures in scattering experiments with high repetition rate—thus representing the enabling technology for fast femtosecond nanocrystallography.},
  archive      = {J_MLST},
  author       = {T Stielow and R Schmidt and C Peltz and T Fennel and S Scheel},
  doi          = {10.1088/2632-2153/abb213},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {045007},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Fast reconstruction of single-shot wide-angle diffraction images through deep learning},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep learning of interface structures from simulated 4D STEM
data: Cation intermixing vs. Roughening ∗. <em>MLST</em>, <em>1</em>(4),
04LT01. (<a href="https://doi.org/10.1088/2632-2153/aba32d">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interface structures in complex oxides remain an active area of condensed matter physics research, largely enabled by recent advances in scanning transmission electron microscopy (STEM). Yet the nature of the STEM contrast in which the structure is projected along the given direction precludes separation of possible structural models. Here, we utilize deep convolutional neural networks (DCNN) trained on simulated 4D STEM datasets to predict structural descriptors of interfaces. We focus on the widely studied interface between LaAlO 3 and SrTiO 3 , using dynamical diffraction theory and leveraging high performance computing to simulate thousands of possible 4D STEM datasets to train the DCNN to learn properties of the underlying structures on which the simulations are based. We test the DCNN on simulated data and show that it is possible (with &gt;95% accuracy) to identify a physically rough from a chemically diffuse interface and create a DCNN regression model to predict step positions. We quantify the applicability of the model to different thicknesses and the transferability of the approach. The method shown here is general and can be applied for any inverse imaging problem where forward models are present.},
  archive      = {J_MLST},
  author       = {M P Oxley and J Yin and N Borodinov and S Somnath and M Ziatdinov and A R Lupini and S Jesse and R K Vasudevan and S V Kalinin},
  doi          = {10.1088/2632-2153/aba32d},
  journal      = {Machine Learning: Science and Technology},
  month        = {10},
  number       = {4},
  pages        = {04LT01},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Deep learning of interface structures from simulated 4D STEM data: Cation intermixing vs. roughening ∗},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the impact of selected modern deep-learning techniques to
the performance and celerity of classification models in an experimental
high-energy physics use case. <em>MLST</em>, <em>1</em>(4), 045006. (<a
href="https://doi.org/10.1088/2632-2153/ab983a">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Beginning from a basic neural-network architecture, we test the potential benefits offered by a range of advanced techniques for machine learning, in particular deep learning, in the context of a typical classification problem encountered in the domain of high-energy physics, using a well-studied dataset: the 2014 Higgs ML Kaggle dataset. The advantages are evaluated in terms of both performance metrics and the time required to train and apply the resulting models. Techniques examined include domain-specific data-augmentation, learning rate and momentum scheduling, (advanced) ensembling in both model-space and weight-space, and alternative architectures and connection methods. Following the investigation, we arrive at a model which achieves equal performance to the winning solution of the original Kaggle challenge, whilst being significantly quicker to train and apply, and being suitable for use with both GPU and CPU hardware setups. These reductions in timing and hardware requirements potentially allow the use of more powerful algorithms in HEP analyses, where models must be retrained frequently, sometimes at short notice, by small groups of researchers with limited hardware resources. Additionally, a new wrapper library for PYTORCH called LUMIN is presented, which incorporates all of the techniques studied.},
  archive      = {J_MLST},
  author       = {Giles Chatham Strong},
  doi          = {10.1088/2632-2153/ab983a},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {4},
  pages        = {045006},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {On the impact of selected modern deep-learning techniques to the performance and celerity of classification models in an experimental high-energy physics use case},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). In operando active learning of interatomic interaction
during large-scale simulations. <em>MLST</em>, <em>1</em>(4), 045005.
(<a href="https://doi.org/10.1088/2632-2153/aba373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A well-known drawback of state-of-the-art machine-learning interatomic potentials is their poor ability to extrapolate beyond the training domain. For small-scale problems with tens to hundreds of atoms this can be solved by using active learning which is able to select atomic configurations on which a potential attempts extrapolation and add them to the ab initio -computed training set. In this sense an active learning algorithm can be viewed as an on-the-fly interpolation of an ab initio model. For large-scale problems, possibly involving tens of thousands of atoms, this is not feasible because one cannot afford even a single density functional theory (DFT) computation with such a large number of atoms. This work marks a new milestone toward fully automatic ab initio -accurate large-scale atomistic simulations. We develop an active learning algorithm that identifies local subregions of the simulation region where the potential extrapolates. Then the algorithm constructs periodic configurations out of these local, non-periodic subregions, sufficiently small to be computable with plane-wave DFT codes, in order to obtain accurate ab initio energies. We benchmark our algorithm on the problem of screw dislocation motion in bcc tungsten and show that our algorithm reaches ab initio accuracy, down to typical magnitudes of numerical noise in DFT codes. We show that our algorithm reproduces material properties such as core structure, Peierls barrier, and Peierls stress. This unleashes new capabilities for computational materials science toward applications which have currently been out of scope if approached solely by ab initio methods.},
  archive      = {J_MLST},
  author       = {M Hodapp and A Shapeev},
  doi          = {10.1088/2632-2153/aba373},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {4},
  pages        = {045005},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {In operando active learning of interatomic interaction during large-scale simulations},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Res-CR-net, a residual network with a novel architecture
optimized for the semantic segmentation of microscopy images.
<em>MLST</em>, <em>1</em>(4), 045004. (<a
href="https://doi.org/10.1088/2632-2153/aba8e8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep neural networks (DNN) have been widely used to carry out segmentation tasks in both electron microscopy (EM) and light/fluorescence microscopy (LM/FM). Most DNNs developed for this purpose are based on some variation of the encoder-decoder U-Net architecture. Here we show how Res-CR-Net, a new type of fully convolutional neural network that does not adopt a U-Net architecture, excels at segmentation tasks traditionally considered very hard, like recognizing the contours of nuclei, cytoplasm and mitochondria in densely packed cells in either EM or LM/FM images.},
  archive      = {J_MLST},
  author       = {Hassan Abdallah and Brent Formosa and Asiri Liyanaarachchi and Maranda Saigh and Samantha Silvers and Suzan Arslanturk and Douglas J Taatjes and Lars Larsson and Bhanu P Jena and Domenico L Gatti},
  doi          = {10.1088/2632-2153/aba8e8},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {4},
  pages        = {045004},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Res-CR-net, a residual network with a novel architecture optimized for the semantic segmentation of microscopy images},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Warwick electron microscopy datasets. <em>MLST</em>,
<em>1</em>(4), 045003. (<a
href="https://doi.org/10.1088/2632-2153/ab9c3c">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large, carefully partitioned datasets are essential to train neural networks and standardize performance benchmarks. As a result, we have set up new repositories to make our electron microscopy datasets available to the wider community. There are three main datasets containing 19769 scanning transmission electron micrographs, 17266 transmission electron micrographs, and 98340 simulated exit wavefunctions, and multiple variants of each dataset for different applications. To visualize image datasets, we trained variational autoencoders to encode data as 64-dimensional multivariate normal distributions, which we cluster in two dimensions by t-distributed stochastic neighbor embedding. In addition, we have improved dataset visualization with variational autoencoders by introducing encoding normalization and regularization, adding an image gradient loss, and extending t-distributed stochastic neighbor embedding to account for encoded standard deviations. Our datasets, source code, pretrained models, and interactive visualizations are openly available at https://github.com/Jeffrey-Ede/datasets .},
  archive      = {J_MLST},
  author       = {Jeffrey M Ede},
  doi          = {10.1088/2632-2153/ab9c3c},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {4},
  pages        = {045003},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Warwick electron microscopy datasets},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hierarchical auxiliary learning. <em>MLST</em>,
<em>1</em>(4), 045002. (<a
href="https://doi.org/10.1088/2632-2153/aba7b3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional application of convolutional neural networks (CNNs) for image classification and recognition is based on the assumption that all target classes are equal (i.e. no hierarchy) and exclusive of one another (i.e. no overlap). CNN-based image classifiers built on this assumption, therefore, cannot take into account an innate hierarchy among target classes (e.g. cats and dogs in animal image classification) or additional information that can be easily derived from the data (e.g. numbers larger than five in the recognition of handwritten digits), thereby resulting in scalability issues when the number of target classes is large. Combining two related but slightly different ideas of hierarchical classification and logical learning by auxiliary inputs , we propose a new learning framework called hierarchical auxiliary learning , which not only address the scalability issues with a large number of classes but also could further reduce the classification/recognition errors with a reasonable number of classes. In the hierarchical auxiliary learning, target classes are semantically or non-semantically grouped into superclasses, which turns the original problem of mapping between an image and its target class into a new problem of mapping between a pair of an image and its superclass and the target class. To take the advantage of a superclass as a hint during the learning phase, we introduce an auxiliary block into a neural network, which generates auxiliary scores used as additional information for final classification/recognition; in this paper, we add the auxiliary block between the last residual block and the fully-connected output layer of the ResNet. Experimental results show that the proposed hierarchical auxiliary learning reduces classification errors up to 0.56, 1.6 and 3.56 percent with MNIST, SVHN and CIFAR-10 datasets, respectively.},
  archive      = {J_MLST},
  author       = {Jaehoon Cha and Kyeong Soo Kim and Sanghyuk Lee},
  doi          = {10.1088/2632-2153/aba7b3},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {4},
  pages        = {045002},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Hierarchical auxiliary learning},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Machine learning in physics: The pitfalls of poisoned
training sets. <em>MLST</em>, <em>1</em>(4), 045001. (<a
href="https://doi.org/10.1088/2632-2153/aba821">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Known for their ability to identify hidden patterns in data, artificial neural networks are among the most powerful machine learning tools. Most notably, neural networks have played a central role in identifying states of matter and phase transitions across condensed matter physics. To date, most studies have focused on systems where different phases of matter and their phase transitions are known, and thus the performance of neural networks is well controlled. While neural networks present an exciting new tool to detect new phases of matter, here we demonstrate that when the training sets are poisoned (i.e. poor training data or mislabeled data) it is easy for neural networks to make misleading predictions.},
  archive      = {J_MLST},
  author       = {Chao Fang and Amin Barzeger and Helmut G Katzgraber},
  doi          = {10.1088/2632-2153/aba821},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {4},
  pages        = {045001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Machine learning in physics: The pitfalls of poisoned training sets},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Machine learning and excited-state molecular dynamics.
<em>MLST</em>, <em>1</em>(4), 043001. (<a
href="https://doi.org/10.1088/2632-2153/ab9c3e">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning is employed at an increasing rate in the research field of quantum chemistry. While the majority of approaches target the investigation of chemical systems in their electronic ground state, the inclusion of light into the processes leads to electronically excited states and gives rise to several new challenges. Here, we survey recent advances for excited-state dynamics based on machine learning. In doing so, we highlight successes, pitfalls, challenges and future avenues for machine learning approaches for light-induced molecular processes.},
  archive      = {J_MLST},
  author       = {Julia Westermayr and Philipp Marquetand},
  doi          = {10.1088/2632-2153/ab9c3e},
  journal      = {Machine Learning: Science and Technology},
  month        = {9},
  number       = {4},
  pages        = {043001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Machine learning and excited-state molecular dynamics},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Machine learning enabled discovery of application dependent
design principles for two-dimensional materials. <em>MLST</em>,
<em>1</em>(3), 035015. (<a
href="https://doi.org/10.1088/2632-2153/aba002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The unique electronic and mechanical properties of two-dimensional (2D) materials make them promising next-generation candidates for a variety of applications. Large-scale searches for high-performing 2D materials are limited to calculating descriptors with computationally demanding first-principles density functional theory. In this work, we alleviate this issue by extending and generalizing crystal graph convolutional neural networks to systems with planar periodicity and train an ensemble of models to predict thermodynamic, mechanical and electronic properties. We carry out a screening of nearly 45,000 structures for two separate applications: mechanical strength and photovoltaics. By collecting statistics of the screened candidates, we investigate structural and compositional design principles that impact the properties of the structures surveyed. Our approach recovers some well-accepted design rules: hybrid organic-inorganic perovskites with lead and tin tend to be good candidates for solar cell applications and titanium based MXenes usually have high stiffness coefficients. Interestingly, other members of the group 4 elements also contribute to increasing the mechanical strength of MXenes. For all-inorganic perovskites, we discover some compositions that have not been deeply studied in the field of photovoltaics and thus open up paths for further investigation. We open-source the code-base to spur further development in this space.},
  archive      = {J_MLST},
  author       = {Victor Venturi and Holden L Parks and Zeeshan Ahmad and Venkatasubramanian Viswanathan},
  doi          = {10.1088/2632-2153/aba002},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035015},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Machine learning enabled discovery of application dependent design principles for two-dimensional materials},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Constraining the reionization history using bayesian
normalizing flows. <em>MLST</em>, <em>1</em>(3), 035014. (<a
href="https://doi.org/10.1088/2632-2153/aba6f1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Upcoming experiments such as Hydrogen Epoch of Reionization Array(HERA) and the Square Kilometre Array (SKA) are intended to measure the 21 cm signal over a wide range of redshifts, representing an incredible opportunity in advancing our understanding about the nature of cosmic reionization. At the same time these kind of experiments will present new challenges in processing the extensive amount of data generated, calling for the development of automated methods capable of precisely estimating physical parameters and their uncertainties. In this deliverable we employ Variational Inference, and in particular Bayesian Neural Networks, as an alternative to MCMC in 21 cm observations to report credible estimations for cosmological and astrophysical parameters and assess the correlations among them. Finally, we have implemented the use of bijectors to improve the diagonal Gaussian approximate posteriors and be able to extract significant information from Non-Gaussian signal in the 21 cm dataset.},
  archive      = {J_MLST},
  author       = {Héctor J. Hortúa and Luigi Malagò and Riccardo Volpi},
  doi          = {10.1088/2632-2153/aba6f1},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035014},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Constraining the reionization history using bayesian normalizing flows},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised classification of single-molecule data with
autoencoders and transfer learning. <em>MLST</em>, <em>1</em>(3),
035013. (<a href="https://doi.org/10.1088/2632-2153/aba6f2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Datasets from single-molecule experiments often reflect a large variety of molecular behaviour. The exploration of such datasets can be challenging, especially if knowledge about the data is limited and a priori assumptions about expected data characteristics are to be avoided. Indeed, searching for pre-defined signal characteristics is sometimes useful, but it can also lead to information loss and the introduction of expectation bias. Here, we demonstrate how Transfer Learning-enhanced dimensionality reduction can be employed to identify and quantify hidden features in single-molecule charge transport data, in an unsupervised manner. Taking advantage of open-access neural networks trained on millions of seemingly unrelated image data, our results also show how Deep Learning methodologies can readily be employed, even if the amount of problem-specific, &#39;own&#39; data is limited.},
  archive      = {J_MLST},
  author       = {Anton Vladyka and Tim Albrecht},
  doi          = {10.1088/2632-2153/aba6f2},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035013},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Unsupervised classification of single-molecule data with autoencoders and transfer learning},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A deep neural network to search for new long-lived particles
decaying to jets. <em>MLST</em>, <em>1</em>(3), 035012. (<a
href="https://doi.org/10.1088/2632-2153/ab9023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A tagging algorithm to identify jets that are significantly displaced from the proton-proton (pp) collision region in the CMS detector at the LHC is presented. Displaced jets can arise from the decays of long-lived particles (LLPs), which are predicted by several theoretical extensions of the standard model. The tagger is a multiclass classifier based on a deep neural network, which is parameterised according to the proper decay length c τ 0 of the LLP. A novel scheme is defined to reliably label jets from LLP decays for supervised learning. Samples of pp collision data, recorded by the CMS detector at a centre-of-mass energy of 13 TeV, and simulated events are used to train the neural network. Domain adaptation by backward propagation is performed to improve the simulation modelling of the jet class probability distributions observed in pp collision data. The potential performance of the tagger is demonstrated with a search for long-lived gluinos, a manifestation of split supersymmetric models. The tagger provides a rejection factor of 10 000 for jets from standard model processes, while maintaining an LLP jet tagging efficiency of 30%–80% for gluinos with 1 mm≤ c τ 0 ≤ 10 m. The expected coverage of the parameter space for split supersymmetry is presented.},
  archive      = {J_MLST},
  author       = {The CMS Collaboration},
  doi          = {10.1088/2632-2153/ab9023},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035012},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {A deep neural network to search for new long-lived particles decaying to jets},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Impact of non-normal error distributions on the
benchmarking and ranking of quantum machine learning models.
<em>MLST</em>, <em>1</em>(3), 035011. (<a
href="https://doi.org/10.1088/2632-2153/aba184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum machine learning models have been gaining significant traction within atomistic simulation communities. Conventionally, relative model performances are being assessed and compared using learning curves (prediction error vs. training set size). This article illustrates the limitations of using the Mean Absolute Error (MAE) for benchmarking, which is particularly relevant in the case of non-normal error distributions. We analyze more specifically the prediction error distribution of the kernel ridge regression with SLATM representation and L 2 distance metric (KRR-SLATM-L2) for effective atomization energies of QM7b molecules calculated at the level of theory CCSD(T)/cc-pVDZ. Error distributions of HF and MP2 at the same basis set referenced to CCSD(T) values were also assessed and compared to the KRR model. We show that the true performance of the KRR-SLATM-L2 method over the QM7b dataset is poorly assessed by the Mean Absolute Error, and can be notably improved after adaptation of the learning set.},
  archive      = {J_MLST},
  author       = {Pascal Pernot and Bing Huang and Andreas Savin},
  doi          = {10.1088/2632-2153/aba184},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035011},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Impact of non-normal error distributions on the benchmarking and ranking of quantum machine learning models},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enhancing wildfire spread modelling by building a gridded
fuel moisture content product with machine learning. <em>MLST</em>,
<em>1</em>(3), 035010. (<a
href="https://doi.org/10.1088/2632-2153/aba480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wildland fire decision support systems require accurate predictions of wildland fire spread. Fuel moisture content (FMC) is one of the important parameters controlling the rate of spread of wildland fire. However, dead FMC measurements are provided by a relatively sparse network of remote automatic weather stations (RAWS), while live FMC is relatively infrequently measured manually. We developed a high resolution, gridded, real-time FMC data sets that did not previously exist for assimilation into operational wildland fire prediction systems based on ML. We used surface observations of live and dead FMC to train machine learning models to estimate FMC based on satellite observations. Moderate Resolution Imaging Spectrometer Terra and Aqua reflectances are used to predict the live and dead FMC measured by the Wildland Fire Assessment System and RAWS). We evaluate multiple machine learning methods including multiple linear regression, random forests (RFs), gradient boosted regression and artificial neural networks. The models are trained to learn the relationships between the satellite reflectances, surface weather and soil moisture observations and FMC. After training on data corresponding to the temporally and spatially nearest grid points to the irregularly spaced surface FMC observations, the machine learning models could be applied to all grid cells for a gridded product over the Conterminous United States (CONUS). The results show generally that the rule-based approaches have the lowest errors likely due to the sharp decision boundaries among the predictors, and the RF approach that utilizes bagging to avoid over-fitting has the lowest error on the test dataset. The errors are typically between 25%−33% the typical variability of the FMC data, which indicate the skill of the RF in estimating the FMC based on satellite data and surface characteristics. The FMC gridded product based on the RF runs operationally daily over CONUS and can be assimilated into WRF-Fire for more accurate wildland fire spread predictions.},
  archive      = {J_MLST},
  author       = {Tyler C McCandless and Branko Kosovic and William Petzke},
  doi          = {10.1088/2632-2153/aba480},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035010},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Enhancing wildfire spread modelling by building a gridded fuel moisture content product with machine learning},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A differentiable programming method for quantum control.
<em>MLST</em>, <em>1</em>(3), 035009. (<a
href="https://doi.org/10.1088/2632-2153/ab9802">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal control is highly desirable in many current quantum systems, especially to realize tasks in quantum information processing. We introduce a method based on differentiable programming to leverage explicit knowledge of the differential equations governing the dynamics of the system. In particular, a control agent is represented as a neural network that maps the state of the system at a given time to a control pulse. The parameters of this agent are optimized via gradient information obtained by direct differentiation through both the neural network and the differential equation of the system. This fully differentiable reinforcement learning approach ultimately yields time-dependent control parameters optimizing a desired figure of merit. We demonstrate the method&#39;s viability and robustness to noise in eigenstate preparation tasks for three systems: a single qubit, a chain of qubits, and a quantum parametric oscillator.},
  archive      = {J_MLST},
  author       = {Frank Schäfer and Michal Kloc and Christoph Bruder and Niels Lörch},
  doi          = {10.1088/2632-2153/ab9802},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {035009},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {A differentiable programming method for quantum control},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Wasserstein metric for improved quantum machine learning
with adjacency matrix representations. <em>MLST</em>, <em>1</em>(3),
03LT01. (<a href="https://doi.org/10.1088/2632-2153/aba048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the Wasserstein metric to measure distances between molecules represented by the atom index dependent adjacency &#39;Coulomb&#39; matrix, used in kernel ridge regression based supervised learning. Resulting machine learning models of quantum properties, a.k.a. quantum machine learning models exhibit improved training efficiency and result in smoother predictions of energies related to molecular distortions. We first illustrate smoothness for the continuous extraction of an atom from some organic molecule. Learning curves, quantifying the decay of the atomization energy&#39;s prediction error as a function of training set size, have been obtained for tens of thousands of organic molecules drawn from the QM9 data set. In comparison to conventionally used metrics ( L 1 and L 2 norm), our numerical results indicate systematic improvement in terms of learning curve off-set for random as well as sorted (by norms of row) atom indexing in Coulomb matrices. Our findings suggest that this metric corresponds to a favorable similarity measure which introduces index-invariance in any kernel based model relying on adjacency matrix representations.},
  archive      = {J_MLST},
  author       = {Onur Çaylak and O. Anatole von Lilienfeld and Björn Baumeier},
  doi          = {10.1088/2632-2153/aba048},
  journal      = {Machine Learning: Science and Technology},
  month        = {8},
  number       = {3},
  pages        = {03LT01},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Wasserstein metric for improved quantum machine learning with adjacency matrix representations},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling sequences with quantum states: A look under the
hood. <em>MLST</em>, <em>1</em>(3), 035008. (<a
href="https://doi.org/10.1088/2632-2153/ab8731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classical probability distributions on sets of sequences can be modeled using quantum states. Here, we do so with a quantum state that is pure and entangled. Because it is entangled, the reduced densities that describe subsystems also carry information about the complementary subsystem. This is in contrast to the classical marginal distributions on a subsystem in which information about the complementary system has been integrated out and lost. A training algorithm based on the density matrix renormalization group (DMRG) procedure uses the extra information contained in the reduced densities and organizes it into a tensor network model. An understanding of the extra information contained in the reduced densities allow us to examine the mechanics of this DMRG algorithm and study the generalization error of the resulting model. As an illustration, we work with the even-parity dataset and produce an estimate for the generalization error as a function of the fraction of the dataset used in training.},
  archive      = {J_MLST},
  author       = {Tai-Danae Bradley and E M Stoudenmire and John Terilla},
  doi          = {10.1088/2632-2153/ab8731},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035008},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Modeling sequences with quantum states: A look under the hood},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Machine learning assisted quantum state estimation.
<em>MLST</em>, <em>1</em>(3), 035007. (<a
href="https://doi.org/10.1088/2632-2153/ab9a21">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We build a general quantum state tomography framework that makes use of machine learning techniques to reconstruct quantum states from a given set of coincidence measurements. For a wide range of pure and mixed input states we demonstrate via simulations that our method produces functionally equivalent reconstructed states to that of traditional methods with the added benefit that expensive computations are front-loaded with our system. Further, by training our system with measurement results that include simulated noise sources we are able to demonstrate a significantly enhanced average fidelity when compared to typical reconstruction methods. These enhancements in average fidelity are also shown to persist when we consider state reconstruction from partial tomography data where several measurements are missing. We anticipate that the present results combining the fields of machine intelligence and quantum state estimation will greatly improve and speed up tomography-based quantum experiments.},
  archive      = {J_MLST},
  author       = {Sanjaya Lohani and Brian T Kirby and Michael Brodsky and Onur Danaci and Ryan T Glasser},
  doi          = {10.1088/2632-2153/ab9a21},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035007},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Machine learning assisted quantum state estimation},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Coherent optical communications enhanced by machine
intelligence. <em>MLST</em>, <em>1</em>(3), 035006. (<a
href="https://doi.org/10.1088/2632-2153/ab9c3d">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accuracy in discriminating between different received coherent signals is integral to the operation of many free-space communications protocols, and is often difficult when the receiver measures a weak signal. Here we design an optical communication scheme that uses balanced homodyne detection in combination with an unsupervised generative machine learning and convolutional neural network (CNN) system, and demonstrate its efficacy in a realistic simulated coherent quadrature phase shift keyed (QPSK) communications system. Additionally, we design the neural network system such that it autonomously learns to correct for the noise associated with a weak QPSK signal, which is distributed to the receiver prior to the implementation of the communications. We find that the scheme significantly reduces the overall error probability of the communications system, achieving the classical optimal limit. We anticipate that these results will allow for a significant enhancement of current classical and quantum coherent optical communications technologies.},
  archive      = {J_MLST},
  author       = {Sanjaya Lohani and Ryan T Glasser},
  doi          = {10.1088/2632-2153/ab9c3d},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035006},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Coherent optical communications enhanced by machine intelligence},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Prediction of the evolution of the stress field of
polycrystals undergoing elastic-plastic deformation with a hybrid neural
network model. <em>MLST</em>, <em>1</em>(3), 035005. (<a
href="https://doi.org/10.1088/2632-2153/ab9299">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crystal plasticity theory is often employed to predict the mesoscopic states of polycrystalline metals, and is well-known to be costly to simulate. Using a neural network with convolutional layers encoding correlations in time and space, we were able to predict the evolution of the dominant component of the stress field given only the initial microstructure and external loading. In comparison to our recent work, we were able to predict not only the spatial average of the stress response but the evolution of the field itself. We show that the stress fields and their rates are in good agreement with the two dimensional crystal plasticity data and have no visible artifacts. Furthermore the distribution of stress throughout the elastic to fully plastic transition match the truth provided by held out crystal plasticity data. Lastly we demonstrate the efficacy of the trained model in material characterization and optimization tasks.},
  archive      = {J_MLST},
  author       = {Ari Frankel and Kousuke Tachida and Reese Jones},
  doi          = {10.1088/2632-2153/ab9299},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035005},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Prediction of the evolution of the stress field of polycrystals undergoing elastic-plastic deformation with a hybrid neural network model},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A deep learning functional estimator of optimal dynamics for
sampling large deviations. <em>MLST</em>, <em>1</em>(3), 035004. (<a
href="https://doi.org/10.1088/2632-2153/ab95a1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In stochastic systems, numerically sampling the relevant trajectories for the estimation of the large deviation statistics of time-extensive observables requires overcoming their exponential (in space and time) scarcity. The optimal way to access these rare events is by means of an auxiliary dynamics obtained from the original one through the so-called &#39;generalised Doob transformation&#39;. While this optimal dynamics is guaranteed to exist its use is often impractical, as to define it requires the often impossible task of diagonalising a (tilted) dynamical generator. While approximate schemes have been devised to overcome this issue they are difficult to automate as they tend to require knowledge of the systems under study. Here we address this problem from the perspective of deep learning. We devise an iterative semi-supervised learning scheme which converges to the optimal or Doob dynamics with the clear advantage of requiring no prior knowledge of the system. We test our method in a paradigmatic statistical mechanics model with non-trivial dynamical fluctuations, the fully packed classical dimer model on the square lattice, showing that it compares favourably with more traditional approaches. We discuss broader implications of our results for the study of rare dynamical trajectories.},
  archive      = {J_MLST},
  author       = {Tom H E Oakes and Adam Moss and Juan P Garrahan},
  doi          = {10.1088/2632-2153/ab95a1},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035004},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {A deep learning functional estimator of optimal dynamics for sampling large deviations},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Classical versus quantum models in machine learning:
Insights from a finance application. <em>MLST</em>, <em>1</em>(3),
035003. (<a href="https://doi.org/10.1088/2632-2153/ab9009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although several models have been proposed towards assisting machine learning (ML) tasks with quantum computers, a direct comparison of the expressive power and efficiency of classical versus quantum models for datasets originating from real-world applications is one of the key milestones towards a quantum ready era. Here, we take a first step towards addressing this challenge by performing a comparison of the widely used classical ML models known as restricted Boltzmann machines (RBMs), against a recently proposed quantum model, now known as quantum circuit Born machines (QCBMs). Both models address the same hard tasks in unsupervised generative modeling, with QCBMs exploiting the probabilistic nature of quantum mechanics and a candidate for near-term quantum computers, as experimentally demonstrated in three different quantum hardware architectures to date. To address the question of the performance of the quantum model on real-world classical data sets, we construct scenarios from a probabilistic version out of the well-known portfolio optimization problem in finance, by using time-series pricing data from asset subsets of the S&amp;P500 stock market index. It is remarkable to find that, under the same number of resources in terms of parameters for both classical and quantum models, the quantum models seem to have superior performance on typical instances when compared with the canonical training of the RBMs. Our simulations are grounded on a hardware efficient realization of the QCBMs on ion-trap quantum computers, by using their native gate sets, and therefore readily implementable in near-term quantum devices.},
  archive      = {J_MLST},
  author       = {Javier Alcazar and Vicente Leyton-Ortega and Alejandro Perdomo-Ortiz},
  doi          = {10.1088/2632-2153/ab9009},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035003},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Classical versus quantum models in machine learning: Insights from a finance application},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A general approach to maximise information density in
neutron reflectometry analysis. <em>MLST</em>, <em>1</em>(3), 035002.
(<a href="https://doi.org/10.1088/2632-2153/ab94c4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neutron and x-ray reflectometry are powerful techniques facilitating the study of the structure of interfacial materials. The analysis of these techniques is ill-posed in nature requiring the application of model-dependent methods. This can lead to the over- and under- analysis of experimental data when too many or too few parameters are allowed to vary in the model. In this work, we outline a robust and generic framework for the determination of the set of free parameters that are capable of maximising the information density of the model. This framework involves the determination of the Bayesian evidence for each permutation of free parameters; and is applied to a simple phospholipid monolayer. We believe this framework should become an important component in reflectometry data analysis and hope others more regularly consider the relative evidence for their analytical models.},
  archive      = {J_MLST},
  author       = {Andrew R McCluskey and Joshaniel F K Cooper and Tom Arnold and Tim Snow},
  doi          = {10.1088/2632-2153/ab94c4},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035002},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {A general approach to maximise information density in neutron reflectometry analysis},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Algorithms for tensor network contraction ordering.
<em>MLST</em>, <em>1</em>(3), 035001. (<a
href="https://doi.org/10.1088/2632-2153/ab94c5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Contracting tensor networks is often computationally demanding. Well-designed contraction sequences can dramatically reduce the contraction cost. We explore the performance of simulated annealing and genetic algorithms, two common discrete optimization techniques, to this ordering problem. We benchmark their performance as well as that of the commonly-used greedy search on physically relevant tensor networks. Where computationally feasible, we also compare them with the optimal contraction sequence obtained by an exhaustive search. Furthermore, we present a systematic comparison with state-of-the-art tree decomposition and graph partitioning algorithms in the context of random regular graph tensor networks. We find that the algorithms we consider consistently outperform a greedy search given equal computational resources, with an advantage that scales with tensor network size. We compare the obtained contraction sequences and identify signs of highly non-local optimization, with the more sophisticated algorithms sacrificing run-time early in the contraction for better overall performance.},
  archive      = {J_MLST},
  author       = {Frank Schindler and Adam S Jermyn},
  doi          = {10.1088/2632-2153/ab94c5},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {035001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Algorithms for tensor network contraction ordering},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quantum machine learning and quantum biomimetics: A
perspective. <em>MLST</em>, <em>1</em>(3), 033002. (<a
href="https://doi.org/10.1088/2632-2153/ab9803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantum machine learning has emerged as an exciting and promising paradigm inside quantum technologies. It may permit, on the one hand, to carry out more efficient machine learning calculations by means of quantum devices, while, on the other hand, to employ machine learning techniques to better control quantum systems. Inside quantum machine learning, quantum reinforcement learning aims at developing &#39;intelligent&#39; quantum agents that may interact with the outer world and adapt to it, with the strategy of achieving some final goal. Another paradigm inside quantum machine learning is that of quantum autoencoders, which may allow one for employing fewer resources in a quantum device via a training process. Moreover, the field of quantum biomimetics aims at establishing analogies between biological and quantum systems, to look for previously inadvertent connections that may enable useful applications. Two recent examples are the concepts of quantum artificial life, as well as of quantum memristors. In this Perspective, we give an overview of these topics, describing the related research carried out by the scientific community.},
  archive      = {J_MLST},
  author       = {Lucas Lamata},
  doi          = {10.1088/2632-2153/ab9803},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {033002},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Quantum machine learning and quantum biomimetics: A perspective},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scientific AI in materials science: A path to a sustainable
and scalable paradigm. <em>MLST</em>, <em>1</em>(3), 033001. (<a
href="https://doi.org/10.1088/2632-2153/ab9a20">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently there has been an ever-increasing trend in the use of machine learning (ML) and artificial intelligence (AI) methods by the materials science, condensed matter physics, and chemistry communities. This perspective article identifies key scientific, technical, and social opportunities that the materials community must prioritize to consistently develop and leverage Scientific AI (SciAI) to provide a credible path towards the advancement of current materials-limited technologies. Here we highlight the intersections of these opportunities with a series of proposed paths forward. The opportunities are roughly sorted from scientific/technical ( e.g. development of robust, physically meaningful multiscale material representations) to social ( e.g. promoting an AI-ready workforce). The proposed paths forward range from developing new infrastructure and capabilities to deploying them in industry and academia. We provide a brief introduction to AI in materials science and engineering, followed by detailed discussions of each of the opportunities and paths forward.},
  archive      = {J_MLST},
  author       = {BL DeCost and JR Hattrick-Simpers and Z Trautt and AG Kusne and E Campo and ML Green},
  doi          = {10.1088/2632-2153/ab9a20},
  journal      = {Machine Learning: Science and Technology},
  month        = {7},
  number       = {3},
  pages        = {033001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Scientific AI in materials science: A path to a sustainable and scalable paradigm},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Convolutional neural network classifier for the output of
the time-domain <span class="math inline">ℱ</span>-statistic all-sky
search for continuous gravitational waves. <em>MLST</em>, <em>1</em>(2),
025016. (<a href="https://doi.org/10.1088/2632-2153/ab86c7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Among the astrophysical sources in the Advanced Laser Interferometer Gravitational-Wave Observatory (LIGO) and Advanced Virgo detectors&#39; frequency band are rotating non-axisymmetric neutron stars emitting long-lasting, almost-monochromatic gravitational waves. Searches for these continuous gravitational-wave signals are usually performed in long stretches of data in a matched-filter framework e.g. the \mathcal{F} -statistic method. In an all-sky search for a priori unknown sources, a large number of templates are matched against the data using a pre-defined grid of variables (the gravitational-wave frequency and its derivatives, sky coordinates), subsequently producing a collection of candidate signals , corresponding to the grid points at which the signal reaches a pre-defined signal-to-noise threshold. An astrophysical signature of the signal is encoded in the multi-dimensional vector distribution of the candidate signals. In the first work of this kind, we apply a deep learning approach to classify the distributions. We consider three basic classes: Gaussian noise, astrophysical gravitational-wave signal, and a constant-frequency detector artifact (&#39;stationary line&#39;), the two latter injected into the Gaussian noise. 1D and 2D versions of a convolutional neural network classifier are implemented, trained and tested on a broad range of signal frequencies. We demonstrate that these implementations correctly classify the instances of data at various signal-to-noise ratios and signal frequencies, while also showing concept generalization i.e. satisfactory performance at previously unseen frequencies. In addition we discuss the deficiencies, computational requirements and possible applications of these implementations.},
  archive      = {J_MLST},
  author       = {Filip Morawski and Michał Bejger and Paweł Ciecieląg},
  doi          = {10.1088/2632-2153/ab86c7},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {2},
  pages        = {025016},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Convolutional neural network classifier for the output of the time-domain $\mathcal{F}$-statistic all-sky search for continuous gravitational waves},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A machine learning workflow for molecular analysis:
Application to melting points. <em>MLST</em>, <em>1</em>(2), 025015. (<a
href="https://doi.org/10.1088/2632-2153/ab8aa3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational tools encompassing integrated molecular prediction, analysis, and generation are key for molecular design in a variety of critical applications. In this work, we develop a workflow for molecular analysis (MOLAN) that integrates an ensemble of supervised and unsupervised machine learning techniques to analyze molecular data sets. The MOLAN workflow combines molecular featurization, clustering algorithms, uncertainty analysis, low-bias dataset construction, high-performance regression models, graph-based molecular embeddings and attribution, and a semi-supervised variational autoencoder based on the novel SELFIES representation to enable molecular design. We demonstrate the utility of the MOLAN workflow in the context of a challenging multi-molecule property prediction problem: the determination of melting points solely from single molecule structure. This application serves as a case study for how to employ the MOLAN workflow in the context of molecular property prediction.},
  archive      = {J_MLST},
  author       = {Ganesh Sivaraman and Nicholas E Jackson and Benjamin Sanchez-Lengeling and Álvaro Vázquez-Mayagoitia and Alán Aspuru-Guzik and Venkatram Vishwanath and Juan J de Pablo},
  doi          = {10.1088/2632-2153/ab8aa3},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {2},
  pages        = {025015},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {A machine learning workflow for molecular analysis: Application to melting points},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The probabilistic tensor decomposition toolbox.
<em>MLST</em>, <em>1</em>(2), 025011. (<a
href="https://doi.org/10.1088/2632-2153/ab8241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces the probabilistic tensor decomposition toolbox - a MATLAB toolbox for tensor decomposition using Variational Bayesian inference and Gibbs sampling. An introduction and overview of probabilistic tensor decomposition and its connection with classical tensor decomposition methods based on maximum likelihood is provided. We subsequently describe the probabilistic tensor decomposition toolbox which encompasses the Canonical Polyadic, Tucker, and Tensor Train decomposition models. Currently, unconstrained, non-negative, orthogonal, and sparse factors are supported. Bayesian inference forms a principled way of incorporating prior knowledge, prediction of held-out data, and estimating posterior probabilities. Furthermore, it facilitates automatic model order determination, automatic regularization on factors (e.g. sparsity), and inherently penalizes model complexity which is beneficial when inferring hierarchical models, such as heteroscedastic noise modelling. The toolbox allows researchers to easily apply Bayesian tensor decomposition methods without the need to derive or implement these methods themselves. Furthermore, it serves as a reference implementation for comparing existing and new tensor decomposition methods. The software is available from https://github.com/JesperLH/prob-tensor-toolbox/ .},
  archive      = {J_MLST},
  author       = {Jesper L Hinrich and Kristoffer H Madsen and Morten Mørup},
  doi          = {10.1088/2632-2153/ab8241},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {2},
  pages        = {025011},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {The probabilistic tensor decomposition toolbox},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Synergizing medical imaging and radiotherapy with deep
learning. <em>MLST</em>, <em>1</em>(2), 021001. (<a
href="https://doi.org/10.1088/2632-2153/ab869f">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article reviews deep learning methods for medical imaging (focusing on image reconstruction, segmentation, registration, and radiomics) and radiotherapy (ranging from planning and verification to prediction) as well as the connections between them. Then, future topics are discussed involving semantic analysis through natural language processing and graph neural networks. It is believed that deep learning in particular, and artificial intelligence and machine learning in general, will have a revolutionary potential to advance and synergize medical imaging and radiotherapy for unprecedented smart precision healthcare.},
  archive      = {J_MLST},
  author       = {Hongming Shan and Xun Jia and Pingkun Yan and Yunyao Li and Harald Paganetti and Ge Wang},
  doi          = {10.1088/2632-2153/ab869f},
  journal      = {Machine Learning: Science and Technology},
  month        = {6},
  number       = {2},
  pages        = {021001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Synergizing medical imaging and radiotherapy with deep learning},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Core-collapse supernova gravitational-wave search and deep
learning classification. <em>MLST</em>, <em>1</em>(2), 025014. (<a
href="https://doi.org/10.1088/2632-2153/ab7d31">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe a search and classification procedure for gravitational waves emitted by core-collapse supernova (CCSN) explosions, using a convolutional neural network (CNN) combined with an event trigger generator known as a Wavelet Detection Filter (WDF). We employ both a 1D CNN classification using time series gravitational-wave data as input, and a 2D CNN classification with time-frequency representation of the data as input. To test the accuracies of our 1D and 2D CNN classification, we add CCSN waveforms from the most recent hydrodynamical simulations of neutrino-driven core-collapse to simulated Gaussian colored noise with the Virgo interferometer and the planned Einstein Telescope sensitivity curve. We find classification accuracies, for a single detector, of over ∼ 95 % for both 1D and 2D CNN pipelines. For the first time in machine learning CCSN studies, we add short duration detector noise transients to our data to test the robustness of our method against false alarms created by detector noise artifacts. Further to this, we show that the CNN can distinguish between different types of CCSN waveform models.},
  archive      = {J_MLST},
  author       = {Alberto Iess and Elena Cuoco and Filip Morawski and Jade Powell},
  doi          = {10.1088/2632-2153/ab7d31},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025014},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Core-collapse supernova gravitational-wave search and deep learning classification},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DAVI: Deep learning-based tool for alignment and single
nucleotide variant identification. <em>MLST</em>, <em>1</em>(2), 025013.
(<a href="https://doi.org/10.1088/2632-2153/ab7e19">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Next-generation sequencing (NGS) technologies have provided affordable but errorful ways to generate raw genetic data. To extract variant information from billions of NGS reads is still a daunting task which involves various hand-crafted and parameterized statistical tools. Here we propose a deep neural networks (DNN) based alignment and single nucleotide variant (SNV) identifier tool known as DAVI: deep alignment and variant identification. DAVI consists of models for both global and local alignment and for variant calling. We have evaluated the performance of DAVI against existing state-of-the-art tool sets and found that its accuracy and performance is comparable to existing tools used for bench-marking. We further demonstrate that while existing tools are based on data generated from a specific sequencing technology, the models proposed in DAVI are generic and can be used across different NGS technologies as well as across different species. The use of DAVI will therefore help non-human sequencing projects to benefit from the wealth of human ground truth data. Moreover, this approach is a migration from expert-driven statistical models to generic, automated, self-learning models.},
  archive      = {J_MLST},
  author       = {G Gupta and S Saini},
  doi          = {10.1088/2632-2153/ab7e19},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025013},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {DAVI: Deep learning-based tool for alignment and single nucleotide variant identification},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Randomized CP tensor decomposition. <em>MLST</em>,
<em>1</em>(2), 025012. (<a
href="https://doi.org/10.1088/2632-2153/ab8240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The CANDECOMP/PARAFAC (CP) tensor decomposition is a popular dimensionality-reduction method for multiway data. Dimensionality reduction is often sought after since many high-dimensional tensors have low intrinsic rank relative to the dimension of the ambient measurement space. However, the emergence of &#39;big data&#39; poses significant computational challenges for computing this fundamental tensor decomposition. By leveraging modern randomized algorithms, we demonstrate that coherent structures can be learned from a smaller representation of the tensor in a fraction of the time. Thus, this simple but powerful algorithm enables one to compute the approximate CP decomposition even for massive tensors. The approximation error can thereby be controlled via oversampling and the computation of power iterations. In addition to theoretical results, several empirical results demonstrate the performance of the proposed algorithm.},
  archive      = {J_MLST},
  author       = {N Benjamin Erichson and Krithika Manohar and Steven L Brunton and J Nathan Kutz},
  doi          = {10.1088/2632-2153/ab8240},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025012},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Randomized CP tensor decomposition},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modified independent component analysis for extracting
eigen-modes of a quantum system. <em>MLST</em>, <em>1</em>(2), 025010.
(<a href="https://doi.org/10.1088/2632-2153/ab862d">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamical probes are popular experimental tools for studying quantum systems nowadays. Though a well-defined eigen-mode will manifest itself as a single frequency oscillation in the dynamical probe, the real experimental data can be quite messy because a probe usually excites multiple modes which are also mixed together with random white noise. Moreover, the oscillations usually quickly damp out due to finite decoherence time. These make it difficult to extract the frequencies of these oscillation measurement data, that is, the eigen-energies of these eigen-modes. Here we develop an unsupervised machine learning algorithm to solve this problem. Our method is inspired by the independent component analysis method and its application to the &#39;cocktail party problem&#39;, where the goal is to recover each voice from detectors that detect signals of many mixed voices. We demonstrate the advantage of our method by an example of analyzing the collective mode of a Bose–Einstein condensate of atomic gases. We believe that this method can find broad applications in analyzing data of dynamical experiments in quantum systems of different fields.},
  archive      = {J_MLST},
  author       = {Yadong Wu and Hui Zhai},
  doi          = {10.1088/2632-2153/ab862d},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025010},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Modified independent component analysis for extracting eigen-modes of a quantum system},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Neural networks and kernel ridge regression for excited
states dynamics of CH2NH<span
class="math inline"><sub>2</sub><sup>+</sup></span>: From single-state
to multi-state representations and multi-property machine learning
models. <em>MLST</em>, <em>1</em>(2), 025009. (<a
href="https://doi.org/10.1088/2632-2153/ab88d0">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Excited-state dynamics simulations are a powerful tool to investigate photo-induced reactions of molecules and materials and provide complementary information to experiments. Since the applicability of these simulation techniques is limited by the costs of the underlying electronic structure calculations, we develop and assess different machine learning models for this task. The machine learning models are trained on ab initio calculations for excited electronic states, using the methylenimmonium cation (CH 2 NH _2^+ ) as a model system. Two distinct strategies for modeling excited state properties are tested in this work. The first strategy is to treat each state separately in a kernel ridge regression model and all states together in a multiclass neural network. The second strategy is to instead encode the state as input into the model, which is tested with both models. Numerical evidence suggests that using the state as input yields the best performance. An important goal for excited-state machine learning models is their use in dynamics simulations, which needs not only state-specific information but also couplings, i.e. properties involving pairs of states. Accordingly, we investigate how well machine learning models can predict the couplings. Furthermore, we explore how combining all properties in a single neural network affects the accuracy. Finally, machine learning predicted energies, forces, and couplings are used to carry out excited-state dynamics simulations. Results demonstrate the scopes and possibilities of machine learning to model excited-state properties.},
  archive      = {J_MLST},
  author       = {Julia Westermayr and Felix A Faber and Anders S Christensen and O Anatole von Lilienfeld and Philipp Marquetand},
  doi          = {10.1088/2632-2153/ab88d0},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025009},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Neural networks and kernel ridge regression for excited states dynamics of CH2NH$_2^+$: From single-state to multi-state representations and multi-property machine learning models},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Predicting drug properties with parameter-free machine
learning: Pareto-optimal embedded modeling (POEM). <em>MLST</em>,
<em>1</em>(2), 025008. (<a
href="https://doi.org/10.1088/2632-2153/ab891b">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prediction of absorption, distribution, metabolism, excretion, and toxicity (ADMET) of small molecules from their molecular structure is a central problem in medicinal chemistry with great practical importance in drug discovery. Creating predictive models conventionally requires substantial trial-and-error for the selection of molecular representations, machine learning (ML) algorithms, and hyperparameter tuning. A generally applicable method that performs well on all datasets without tuning would be of great value but is currently lacking. Here, we describe pareto-optimal embedded modeling (POEM), a similarity-based method for predicting molecular properties. POEM is a non-parametric, supervised ML algorithm developed to generate reliable predictive models without need for optimization. POEM&#39;s predictive strength is obtained by combining multiple different representations of molecular structures in a context-specific manner, while maintaining low dimensionality. We benchmark POEM relative to industry-standard ML algorithms and published results across 17 classifications tasks. POEM performs well in all cases and reduces the risk of overfitting.},
  archive      = {J_MLST},
  author       = {Andrew E Brereton and Stephen MacKinnon and Zhaleh Safikhani and Shawn Reeves and Sana Alwash and Vijay Shahani and Andreas Windemuth},
  doi          = {10.1088/2632-2153/ab891b},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025008},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Predicting drug properties with parameter-free machine learning: Pareto-optimal embedded modeling (POEM)},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Development of use-specific high-performance
cyber-nanomaterial optical detectors by effective choice of machine
learning algorithms. <em>MLST</em>, <em>1</em>(2), 025007. (<a
href="https://doi.org/10.1088/2632-2153/ab8967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to their inherent variabilities, nanomaterials-based sensors are challenging to translate into real-world applications, where reliability and reproducibility are key. Machine learning can be a powerful approach for obtaining reliable inferences from data generated by such sensors. Here, we show that the best choice of ML algorithm in a cyber-nanomaterial detector is largely determined by the specific use-considerations, including accuracy, computational cost, speed, and resilience against drifts and long-term ageing effects. When sufficient data and computing resources are provided, the highest sensing accuracy can be achieved by the k-nearest neighbors (kNNs) and Bayesian inference algorithms, however, these algorithms can be computationally expensive for real-time applications. In contrast, artificial neural networks (ANNs) are computationally expensive to train (off-line), but they provide the fastest result under testing conditions (on-line) while remaining reasonably accurate. When access to data is limited, support vector machines (SVMs) can perform well even with small training sample sizes, while other algorithms show considerable reduction in accuracy if data is scarce, hence, setting a lower limit on the size of required training data. We also show by tracking and modeling the long-term drifts of the detector performance over a one year time-frame, it is possible to dramatically improve the predictive accuracy without any re-calibration. Our research shows for the first time that if the ML algorithm is chosen specific to the use-case, low-cost solution-processed cyber-nanomaterial detectors can be practically implemented under diverse operational requirements, despite their inherent variabilities.},
  archive      = {J_MLST},
  author       = {Davoud Hejazi and Shuangjun Liu and Amirreza Farnoosh and Sarah Ostadabbas and Swastik Kar},
  doi          = {10.1088/2632-2153/ab8967},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025007},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Development of use-specific high-performance cyber-nanomaterial optical detectors by effective choice of machine learning algorithms},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Methods for comparing uncertainty quantifications for
material property predictions. <em>MLST</em>, <em>1</em>(2), 025006. (<a
href="https://doi.org/10.1088/2632-2153/ab7e1a">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data science and informatics tools have been proliferating recently within the computational materials science and catalysis fields. This proliferation has spurned the creation of various frameworks for automated materials screening, discovery, and design. Underpinning these frameworks are surrogate models with uncertainty estimates on their predictions. These uncertainty estimates are instrumental for determining which materials to screen next, but the computational catalysis field does not yet have a standard procedure for judging the quality of such uncertainty estimates. Here we present a suite of figures and performance metrics derived from the machine learning community that can be used to judge the quality of such uncertainty estimates. This suite probes the accuracy, calibration, and sharpness of a model quantitatively. We then show a case study where we judge various methods for predicting density-functional-theory-calculated adsorption energies. Of the methods studied here, we find that the best performer is a model where a convolutional neural network is used to supply features to a Gaussian process regressor, which then makes predictions of adsorption energies along with corresponding uncertainty estimates.},
  archive      = {J_MLST},
  author       = {Kevin Tran and Willie Neiswanger and Junwoong Yoon and Qingyang Zhang and Eric Xing and Zachary W Ulissi},
  doi          = {10.1088/2632-2153/ab7e1a},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {025006},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Methods for comparing uncertainty quantifications for material property predictions},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Machine learning at the (sub)atomic scale: Next generation
scanning probe microscopy. <em>MLST</em>, <em>1</em>(2), 023001. (<a
href="https://doi.org/10.1088/2632-2153/ab7d2f">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We discuss the exciting prospects for a step change in our ability to map and modify matter at the atomic/molecular level by embedding machine learning algorithms in scanning probe microscopy (with a particular focus on scanning tunnelling microscopy, STM). This nano-AI hybrid approach has the far-reaching potential to realise a technology capable of the automated analysis, actuation, and assembly of matter with a precision down to the single chemical bond limit.},
  archive      = {J_MLST},
  author       = {Oliver M Gordon and Philip J Moriarty},
  doi          = {10.1088/2632-2153/ab7d2f},
  journal      = {Machine Learning: Science and Technology},
  month        = {5},
  number       = {2},
  pages        = {023001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Machine learning at the (sub)atomic scale: Next generation scanning probe microscopy},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Establishing an evaluation metric to quantify climate change
image realism *. <em>MLST</em>, <em>1</em>(2), 025005. (<a
href="https://doi.org/10.1088/2632-2153/ab7657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With success on controlled tasks, deep generative models are being increasingly applied to humanitarian applications (Nie et al 2017 Int. Conf. on Medical Image Computing and Computer-Assisted Intervention (Berlin: Springer) pp 417–25, Yanardag et al 2017 Deep Empathy ). In this paper, we focus on the evaluation of a conditional generative model that illustrates the consequences of climate change-induced flooding to encourage public interest and awareness on the issue. Because metrics for comparing the realism of different modes in a conditional generative model do not exist, we propose several automated and human-based methods for evaluation. To do this, we adapt several existing metrics and assess the automated metrics against gold standard human evaluation. We find that using Fréchet Inception Distance with embeddings from an intermediary Inception-v3 layer that precedes the auxiliary classifier produces results most correlated with human realism. While insufficient alone to establish a human-correlated automatic evaluation metric, we believe this work begins to bridge the gap between human and automated generative evaluation procedures, and to generate more realistic images of the future consequences of climate change.},
  archive      = {J_MLST},
  author       = {Sharon Zhou and Alexandra Luccioni and Gautier Cosne and Michael S Bernstein and Yoshua Bengio},
  doi          = {10.1088/2632-2153/ab7657},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {025005},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Establishing an evaluation metric to quantify climate change image realism *},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Perspective: New insights from loss function landscapes of
neural networks. <em>MLST</em>, <em>1</em>(2), 023002. (<a
href="https://doi.org/10.1088/2632-2153/ab7aef">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the structure of the loss function landscape for neural networks subject to dataset mislabelling, increased training set diversity, and reduced node connectivity, using various techniques developed for energy landscape exploration. The benchmarking models are classification problems for atomic geometry optimisation and hand-written digit prediction. We consider the effect of varying the size of the atomic configuration space used to generate initial geometries and find that the number of stationary points increases rapidly with the size of the training configuration space. We introduce a measure of node locality to limit network connectivity and perturb permutational weight symmetry, and examine how this parameter affects the resulting landscapes. We find that highly-reduced systems have low capacity and exhibit landscapes with very few minima. On the other hand, small amounts of reduced connectivity can enhance network expressibility and can yield more complex landscapes. Investigating the effect of deliberate classification errors in the training data, we find that the variance in testing AUC, computed over a sample of minima, grows significantly with the training error, providing new insight into the role of the variance-bias trade-off when training under noise. Finally, we illustrate how the number of local minima for networks with two and three hidden layers, but a comparable number of variable edge weights, increases significantly with the number of layers, and as the number of training data decreases. This work helps shed further light on neural network loss landscapes and provides guidance for future work on neural network training and optimisation.},
  archive      = {J_MLST},
  author       = {Sathya R Chitturi and Philipp C Verpoort and Alpha A Lee and David J Wales},
  doi          = {10.1088/2632-2153/ab7aef},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {2},
  pages        = {023002},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Perspective: New insights from loss function landscapes of neural networks},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SMILES-x: Autonomous molecular compounds characterization
for small datasets without descriptors. <em>MLST</em>, <em>1</em>(2),
025004. (<a href="https://doi.org/10.1088/2632-2153/ab57f3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is more and more evidence that machine learning can be successfully applied in materials science and related fields. However, datasets in these fields are often quite small (from tens to several thousands of samples). This means the most advanced machine learning techniques remain neglected, as they are considered to be applicable to big data only. Moreover, materials informatics methods often rely on human-engineered descriptors, that should be carefully chosen, or even created, to fit the physicochemical property that one intends to predict. In this article, we propose a new method that tackles both the issue of small datasets and the difficulty of developing task-specific descriptors. The SMILES-X is an autonomous pipeline for molecular compounds characterisation based on a {Embed-Encode-Attend-Predict} neural architecture with a data-specific Bayesian hyper-parameters optimisation. The only input to the architecture—the SMILES strings—are de-canonicalised in order to efficiently augment the data. One of the key features of the architecture is the attention mechanism, which enables the interpretation of output predictions without extra computational cost. The SMILES-X achieves state-of-the-art results in the inference of aqueous solubility ( {\overline{{RMSE}}}_{{\rm{test}}}\simeq 0.57\pm 0.07 mols/L), hydration free energy ( {\overline{{RMSE}}}_{{\rm{test}}}\simeq 0.81\pm 0.22 kcal/mol, which is ∼24.5% better than molecular dynamics simulations), and octanol/water distribution coefficient ( {\overline{{RMSE}}}_{{\rm{test}}}\simeq 0.59\pm 0.02 for LogD at pH 7.4) of molecular compounds. The SMILES-X is intended to become an important asset in the toolkit of materials scientists and chemists. The source code for the SMILES-X is available at github.com/GLambard/SMILES-X .},
  archive      = {J_MLST},
  author       = {Guillaume Lambard and Ekaterina Gracheva},
  doi          = {10.1088/2632-2153/ab57f3},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {2},
  pages        = {025004},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {SMILES-X: Autonomous molecular compounds characterization for small datasets without descriptors},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A charge density prediction model for hydrocarbons using
deep neural networks. <em>MLST</em>, <em>1</em>(2), 025003. (<a
href="https://doi.org/10.1088/2632-2153/ab5929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The electronic charge density distribution ρ ( r ) of a given material is among the most fundamental quantities in quantum simulations from which many large scale properties and observables can be calculated. Conventionally, ρ ( r ) is obtained using Kohn–Sham density functional theory (KS-DFT) based methods. But, the high computational cost of KS-DFT renders it intractable for systems involving thousands/millions of atoms. Thus, recently there has been efforts to bypass expensive KS equations, and directly predict ρ ( r ) using machine learning (ML) based methods. Here, we build upon one such scheme to create a robust and reliable ρ ( r ) prediction model for a diverse set of hydrocarbons, involving huge chemical and morphological complexity /(saturated, unsaturated molecules, cyclo-groups and amorphous and semi-crystalline polymers). We utilize a grid-based fingerprint to capture the atomic neighborhood around an arbitrary point in space, and map it to the reference ρ ( r ) obtained from standard DFT calculations at that point. Owing to the grid-based learning, dataset sizes exceed billions of points, which is trained using deep neural networks in conjunction with a incremental learning based approach. The accuracy and transferability of the ML approach is demonstrated on not only a diverse test set, but also on a completely unseen system of polystyrene under different strains. Finally, we note that the general approach adopted here could be easily extended to other material systems, and can be used for quick and accurate determination of ρ ( r ) for DFT charge density initialization, computing dipole or quadrupole, and other observables for which reliable density functional are known.},
  archive      = {J_MLST},
  author       = {Deepak Kamal and Anand Chandrasekaran and Rohit Batra and Rampi Ramprasad},
  doi          = {10.1088/2632-2153/ab5929},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {2},
  pages        = {025003},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {A charge density prediction model for hydrocarbons using deep neural networks},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Machine learning the computational cost of quantum
chemistry. <em>MLST</em>, <em>1</em>(2), 025002. (<a
href="https://doi.org/10.1088/2632-2153/ab6ac4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Computational quantum mechanics based molecular and materials design campaigns consume increasingly more high-performance computer resources, making improved job scheduling efficiency desirable in order to reduce carbon footprint or wasteful spending. We introduce quantum machine learning (QML) models of the computational cost of common quantum chemistry tasks. For 2D nonlinear toy systems, single point, geometry optimization, and transition state calculations the out of sample prediction error of QML models of wall times decays systematically with training set size. We present numerical evidence for a toy system containing two functions and three commonly used optimizer and for thousands of organic molecular systems including closed and open shell equilibrium structures, as well as transition states. Levels of electronic structure theory considered include B3LYP/def2-TZVP, MP2/6-311G(d), local CCSD(T)/VTZ-F12, CASSCF/VDZ-F12, and MRCISD+Q-F12/VDZ-F12. In comparison to conventional indiscriminate job treatment, QML based wall time predictions significantly improve job scheduling efficiency for all tasks after training on just thousands of molecules. Resulting reductions in CPU time overhead range from 10% to 90%.},
  archive      = {J_MLST},
  author       = {Stefan Heinen and Max Schwilk and Guido Falk von Rudorff and O Anatole von Lilienfeld},
  doi          = {10.1088/2632-2153/ab6ac4},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {2},
  pages        = {025002},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Machine learning the computational cost of quantum chemistry},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep learning-guided surface characterization for autonomous
hydrogen lithography. <em>MLST</em>, <em>1</em>(2), 025001. (<a
href="https://doi.org/10.1088/2632-2153/ab6d5e">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the development of atom scale devices transitions from novel, proof-of-concept demonstrations to state-of-the-art commercial applications, automated assembly of such devices must be implemented. Here we present an automation method for the identification of defects prior to atomic fabrication via hydrogen lithography using deep learning. We trained a convolutional neural network to locate and differentiate between surface features of the technologically relevant hydrogen-terminated silicon surface imaged using a scanning tunneling microscope. Once the positions and types of surface features are determined, the predefined atomic structures are patterned in a defect-free area. By training the network to differentiate between common defects we are able to avoid charged defects as well as edges of the patterning terraces. Augmentation with previously developed autonomous tip shaping and patterning modules allows for atomic scale lithography with minimal user intervention.},
  archive      = {J_MLST},
  author       = {Mohammad Rashidi and Jeremiah Croshaw and Kieran Mastel and Marcus Tamura and Hedieh Hosseinzadeh and Robert A Wolkow},
  doi          = {10.1088/2632-2153/ab6d5e},
  journal      = {Machine Learning: Science and Technology},
  month        = {3},
  number       = {2},
  pages        = {025001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Deep learning-guided surface characterization for autonomous hydrogen lithography},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interpretable embeddings from molecular simulations using
gaussian mixture variational autoencoders. <em>MLST</em>, <em>1</em>(1),
015012. (<a href="https://doi.org/10.1088/2632-2153/ab80b7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting insight from the enormous quantity of data generated from molecular simulations requires the identification of a small number of collective variables whose corresponding low-dimensional free-energy landscape retains the essential features of the underlying system. Data-driven techniques provide a systematic route to constructing this landscape, without the need for extensive a priori intuition into the relevant driving forces. In particular, autoencoders are powerful tools for dimensionality reduction, as they naturally force an information bottleneck and, thereby, a low-dimensional embedding of the essential features. While variational autoencoders ensure continuity of the embedding by assuming a unimodal Gaussian prior, this is at odds with the multi-basin free-energy landscapes that typically arise from the identification of meaningful collective variables. In this work, we incorporate this physical intuition into the prior by employing a Gaussian mixture variational autoencoder (GMVAE), which encourages the separation of metastable states within the embedding. The GMVAE performs dimensionality reduction and clustering within a single unified framework, and is capable of identifying the inherent dimensionality of the input data, in terms of the number of Gaussians required to categorize the data. We illustrate our approach on two toy models, alanine dipeptide, and a challenging disordered peptide ensemble, demonstrating the enhanced clustering effect of the GMVAE prior compared to standard VAEs. The resulting embeddings appear to be promising representations for constructing Markov state models, highlighting the transferability of the dimensionality reduction from static equilibrium properties to dynamics.},
  archive      = {J_MLST},
  author       = {Yasemin Bozkurt Varolgüneş and Tristan Bereau and Joseph F Rudzinski},
  doi          = {10.1088/2632-2153/ab80b7},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {1},
  pages        = {015012},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Interpretable embeddings from molecular simulations using gaussian mixture variational autoencoders},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive learning rate clipping stabilizes learning.
<em>MLST</em>, <em>1</em>(1), 015011. (<a
href="https://doi.org/10.1088/2632-2153/ab81e2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial neural network training with gradient descent can be destabilized by &#39;bad batches&#39; with high losses. This is often problematic for training with small batch sizes, high order loss functions or unstably high learning rates. To stabilize learning, we have developed adaptive learning rate clipping (ALRC) to limit backpropagated losses to a number of standard deviations above their running means. ALRC is designed to complement existing learning algorithms: Our algorithm is computationally inexpensive, can be applied to any loss function or batch size, is robust to hyperparameter choices and does not affect backpropagated gradient distributions. Experiments with CIFAR-10 supersampling show that ALCR decreases errors for unstable mean quartic error training while stable mean squared error training is unaffected. We also show that ALRC decreases unstable mean squared errors for scanning transmission electron microscopy supersampling and partial scan completion. Our source code is available at https://github.com/Jeffrey-Ede/ALRC .},
  archive      = {J_MLST},
  author       = {Jeffrey M Ede and Richard Beanland},
  doi          = {10.1088/2632-2153/ab81e2},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {1},
  pages        = {015011},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Adaptive learning rate clipping stabilizes learning},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DeepRICH: Learning deeply cherenkov detectors.
<em>MLST</em>, <em>1</em>(1), 015010. (<a
href="https://doi.org/10.1088/2632-2153/ab845a">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Imaging Cherenkov detectors are largely used for particle identification (PID) in nuclear and particle physics experiments, where developing fast reconstruction algorithms is becoming of paramount importance to allow for near real time calibration and data quality control, as well as to speed up offline analysis of large amount of data. In this paper we present DeepRICH, a novel deep learning algorithm for fast reconstruction which can be applied to different imaging Cherenkov detectors. The core of our architecture is a generative model which leverages on a custom Variational Auto-encoder (VAE) combined to Maximum Mean Discrepancy (MMD), with a Convolutional Neural Network (CNN) extracting features from the space of the latent variables for classification. A thorough comparison with the simulation/reconstruction package FastDIRC is discussed in the text. DeepRICH has the advantage to bypass low-level details needed to build a likelihood, allowing for a sensitive improvement in computation time at potentially the same reconstruction performance of other established reconstruction algorithms. In the conclusions, we address the implications and potentialities of this work, discussing possible future extensions and generalization.},
  archive      = {J_MLST},
  author       = {Cristiano Fanelli and Jary Pomponi},
  doi          = {10.1088/2632-2153/ab845a},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {1},
  pages        = {015010},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {DeepRICH: Learning deeply cherenkov detectors},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Machine learning for the solution of the schrödinger
equation. <em>MLST</em>, <em>1</em>(1), 013002. (<a
href="https://doi.org/10.1088/2632-2153/ab7d30">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) methods have recently been increasingly widely used in quantum chemistry. While ML methods are now accepted as high accuracy approaches to construct interatomic potentials for applications, the use of ML to solve the Schrödinger equation, either vibrational or electronic, while not new, is only now making significant headway towards applications. We survey recent uses of ML techniques to solve the Schrödinger equation, including the vibrational Schrödinger equation, the electronic Schrödinger equation and the related problems of constructing functionals for density functional theory (DFT) as well as potentials which enter semi-empirical approximations to DFT. We highlight similarities and differences and specific difficulties that ML faces in these applications and possibilities for cross-fertilization of ideas.},
  archive      = {J_MLST},
  author       = {Sergei Manzhos},
  doi          = {10.1088/2632-2153/ab7d30},
  journal      = {Machine Learning: Science and Technology},
  month        = {4},
  number       = {1},
  pages        = {013002},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Machine learning for the solution of the schrödinger equation},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Framelet pooling aided deep learning network: The method to
process high dimensional medical data. <em>MLST</em>, <em>1</em>(1),
015009. (<a href="https://doi.org/10.1088/2632-2153/ab592b">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning-based analysis of medical images often faces several hurdles, such as the lack of training data, the curse of the dimensionality problem, and generalization issues. One of the main difficulties is that there exists a computational cost problem in dealing with input data of large size matrices which represent medical images. The purpose of this paper is to introduce a framelet-pooling aided deep learning method for mitigating computational bundles caused by large dimensionality. By transforming high dimensional data into low dimensional components by filter banks and preserving detailed information, the proposed method aims to reduce the complexity of the neural network and computational costs significantly during the learning process. Various experiments show that our method is comparable to the standard unreduced learning method, while reducing computational burdens by decomposing large-sized learning tasks into several small-scale learning tasks.},
  archive      = {J_MLST},
  author       = {Chang Min Hyun and Kang Cheol Kim and Hyun Cheol Cho and Jae Kyu Choi and Jin Keun Seo},
  doi          = {10.1088/2632-2153/ab592b},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {1},
  pages        = {015009},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Framelet pooling aided deep learning network: The method to process high dimensional medical data},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep neural network for x-ray photoelectron spectroscopy
data analysis. <em>MLST</em>, <em>1</em>(1), 015008. (<a
href="https://doi.org/10.1088/2632-2153/ab5da6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we characterize the performance of a deep convolutional neural network designed to detect and quantify chemical elements in experimental x-ray photoelectron spectroscopy data. Given the lack of a reliable database in literature, in order to train the neural network we computed a large (&lt;100 k) dataset of synthetic spectra, based on randomly generated materials covered with a layer of adventitious carbon. The trained net performs as well as standard methods on a test set of ≈500 well characterized experimental x-ray photoelectron spectra. Fine details about the net layout, the choice of the loss function and the quality assessment strategies are presented and discussed. Given the synthetic nature of the training set, this approach could be applied to the automatization of any photoelectron spectroscopy system, without the need of experimental reference spectra and with a low computational effort.},
  archive      = {J_MLST},
  author       = {G Drera and C M Kropf and L Sangaletti},
  doi          = {10.1088/2632-2153/ab5da6},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {1},
  pages        = {015008},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Deep neural network for x-ray photoelectron spectroscopy data analysis},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Applying machine learning optimization methods to the
production of a quantum gas. <em>MLST</em>, <em>1</em>(1), 015007. (<a
href="https://doi.org/10.1088/2632-2153/ab6432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We apply three machine learning strategies to optimize the atomic cooling processes utilized in the production of a Bose–Einstein condensate (BEC). For the first time, we optimize both laser cooling and evaporative cooling mechanisms simultaneously. We present the results of an evolutionary optimization method (differential evolution), a method based on non-parametric inference (Gaussian process regression) and a gradient-based function approximator (artificial neural network). Online optimization is performed using no prior knowledge of the apparatus, and the learner succeeds in creating a BEC from completely randomized initial parameters. Optimizing these cooling processes results in a factor of four increase in BEC atom number compared to our manually-optimized parameters. This automated approach can maintain close-to-optimal performance in long-term operation. Furthermore, we show that machine learning techniques can be used to identify the main sources of instability within the apparatus.},
  archive      = {J_MLST},
  author       = {A J Barker and H Style and K Luksch and S Sunami and D Garrick and F Hill and C J Foot and E Bentine},
  doi          = {10.1088/2632-2153/ab6432},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {1},
  pages        = {015007},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Applying machine learning optimization methods to the production of a quantum gas},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Image mapping the temporal evolution of edge characteristics
in tokamaks using neural networks. <em>MLST</em>, <em>1</em>(1), 015006.
(<a href="https://doi.org/10.1088/2632-2153/ab5639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a method for data-driven modelling of the temporal evolution of the plasma and neutral characteristics at the edge of a tokamak using neural networks. Our method proposes a novel fully convolutional network to serve as function approximators in modelling complex nonlinear phenomenon observed in the multi-physics representations of high energy physics. More specifically, we target the evolution of the temperatures, densities and parallel velocities of the electrons, ions and neutral particles at the edge. The central challenge in this context is in modelling together the different physics principles encapsulated in the evolution of plasma and the neutrals. We demonstrate that the inherent differences in nonlinear behaviour can be addressed by forking the network to process the plasma and neutral information individually before integrating as a holistic system. Our approach takes into account the spatial dependencies of the physics parameters across the grid while performing the temporal mappings, ensuring that the underlying physics is factored in and not lost to the black-box. Having used the conventional edge plasma-neutral solver code SOLPS to build the synthetic dataset, our method demonstrates a computational gain of over 5 orders of magnitude over it without a considerable compromise on accuracy.},
  archive      = {J_MLST},
  author       = {Vignesh Gopakumar and D Samaddar},
  doi          = {10.1088/2632-2153/ab5639},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {1},
  pages        = {015006},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Image mapping the temporal evolution of edge characteristics in tokamaks using neural networks},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving the background of gravitational-wave searches for
core collapse supernovae: A machine learning approach. <em>MLST</em>,
<em>1</em>(1), 015005. (<a
href="https://doi.org/10.1088/2632-2153/ab527d">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on the prior O1–O2 observing runs, about 30% of the data collected by Advanced LIGO and Virgo in the next observing runs are expected to be single-interferometer data, i.e. they will be collected at times when only one detector in the network is operating in observing mode. Searches for gravitational-wave signals from supernova events do not rely on matched filtering techniques because of the stochastic nature of the signals. If a Galactic supernova occurs during single-interferometer times, separation of its unmodelled gravitational-wave signal from noise will be even more difficult due to lack of coherence between detectors. We present a novel machine learning method to perform single-interferometer supernova searches based on the standard LIGO-Virgo coherent WaveBurst pipeline. We show that the method may be used to discriminate Galactic gravitational-wave supernova signals from noise transients, decrease the false alarm rate of the search, and improve the supernova detection reach of the detectors.},
  archive      = {J_MLST},
  author       = {M Cavaglià and S Gaudio and T Hansen and K Staats and M Szczepańczyk and M Zanolin},
  doi          = {10.1088/2632-2153/ab527d},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {1},
  pages        = {015005},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Improving the background of gravitational-wave searches for core collapse supernovae: A machine learning approach},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Regularised atomic body-ordered permutation-invariant
polynomials for the construction of interatomic potentials.
<em>MLST</em>, <em>1</em>(1), 015004. (<a
href="https://doi.org/10.1088/2632-2153/ab527c">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the use of invariant polynomials in the construction of data-driven interatomic potentials for material systems. The &#39;atomic body-ordered permutation-invariant polynomials&#39; comprise a systematic basis and are constructed to preserve the symmetry of the potential energy function with respect to rotations and permutations. In contrast to kernel based and artificial neural network models, the explicit decomposition of the total energy as a sum of atomic body-ordered terms allows to keep the dimensionality of the fit reasonably low, up to just 10 for the 5-body terms. The explainability of the potential is aided by this decomposition, as the low body-order components can be studied and interpreted independently. Moreover, although polynomial basis functions are thought to extrapolate poorly, we show that the low dimensionality combined with careful regularisation actually leads to better transferability than the high dimensional, kernel based Gaussian Approximation Potential.},
  archive      = {J_MLST},
  author       = {Cas van der Oord and Geneviève Dusson and Gábor Csányi and Christoph Ortner},
  doi          = {10.1088/2632-2153/ab527c},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {1},
  pages        = {015004},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Regularised atomic body-ordered permutation-invariant polynomials for the construction of interatomic potentials},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Repetitive readout enhanced by machine learning.
<em>MLST</em>, <em>1</em>(1), 015003. (<a
href="https://doi.org/10.1088/2632-2153/ab4e24">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-shot readout is a key component for scalable quantum information processing. However, many solid-state qubits with favorable properties lack the single-shot readout capability. One solution is to use the repetitive quantum-non-demolition readout technique, where the qubit is correlated with an ancilla, which is subsequently read out. The readout fidelity is therefore limited by the back-action on the qubit from the measurement. Traditionally, a threshold method is taken, where only the total photon count is used to discriminate qubit state, discarding all the information of the back-action hidden in the time trace of repetitive readout measurement. Here we show by using machine learning (ML), one obtains higher readout fidelity by taking advantage of the time trace data. ML is able to identify when back-action happened, and correctly read out the original state. Since the information is already recorded (but usually discarded), this improvement in fidelity does not consume additional experimental time, and could be directly applied to preparation-by-measurement and quantum metrology applications involving repetitive readout.},
  archive      = {J_MLST},
  author       = {Genyue Liu and Mo Chen and Yi-Xiang Liu and David Layden and Paola Cappellaro},
  doi          = {10.1088/2632-2153/ab4e24},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {1},
  pages        = {015003},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Repetitive readout enhanced by machine learning},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reinforcement learning for semi-autonomous approximate
quantum eigensolver. <em>MLST</em>, <em>1</em>(1), 015002. (<a
href="https://doi.org/10.1088/2632-2153/ab43b4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The characterization of an operator by its eigenvectors and eigenvalues allows us to know its action over any quantum state. Here, we propose a protocol to obtain an approximation of the eigenvectors of an arbitrary Hermitian quantum operator. This protocol is based on measurement and feedback processes, which characterize a reinforcement learning protocol. Our proposal is composed of two systems, a black box named environment and a quantum state named agent. The role of the environment is to change any quantum state by a unitary matrix {\hat{U}}_{E}={{\rm{e}}}^{-{\rm{i}}\tau {\hat{{ \mathcal O }}}_{E}} where {\hat{{ \mathcal O }}}_{E} is a Hermitian operator, and τ is a real parameter. The agent is a quantum state which adapts to some eigenvector of {\hat{{ \mathcal O }}}_{E} by repeated interactions with the environment, feedback process, and semi-random rotations. With this proposal, we can obtain an approximation of the eigenvectors of a random qubit operator with average fidelity over 90% in less than 10 iterations, and surpass 98% in less than 300 iterations. Moreover, for the two-qubit cases, the four eigenvectors are obtained with fidelities above 89% in 8000 iterations for a random operator, and fidelities of 99% for an operator with the Bell states as eigenvectors. This protocol can be useful to implement semi-autonomous quantum devices which should be capable of extracting information and deciding with minimal resources and without human intervention.},
  archive      = {J_MLST},
  author       = {F Albarrán-Arriagada and J C Retamal and E Solano and L Lamata},
  doi          = {10.1088/2632-2153/ab43b4},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {1},
  pages        = {015002},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Reinforcement learning for semi-autonomous approximate quantum eigensolver},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Embedding human heuristics in machine-learning-enabled probe
microscopy. <em>MLST</em>, <em>1</em>(1), 015001. (<a
href="https://doi.org/10.1088/2632-2153/ab42ec">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scanning probe microscopists generally do not rely on complete images to assess the quality of data acquired during a scan. Instead, assessments of the state of the tip apex, which not only determines the resolution in any scanning probe technique, but can also generate a wide array of frustrating artefacts, are carried out in real time on the basis of a few lines of an image (and, typically, their associated line profiles.) The very small number of machine learning approaches to probe microscopy published to date, however, involve classifications based on full images. Given that data acquisition is the most time-consuming task during routine tip conditioning, automated methods are thus currently extremely slow in comparison to the tried-and-trusted strategies and heuristics used routinely by probe microscopists. Here, we explore various strategies by which different STM image classes (arising from changes in the tip state) can be correctly identified from partial scans. By employing a secondary temporal network and a rolling window of a small group of individual scanlines, we find that tip assessment is possible with a small fraction of a complete image. We achieve this with little-to-no performance penalty—or, indeed, markedly improved performance in some cases—and introduce a protocol to detect the state of the tip apex in real time.},
  archive      = {J_MLST},
  author       = {Oliver M Gordon and Filipe L Q Junqueira and Philip J Moriarty},
  doi          = {10.1088/2632-2153/ab42ec},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {1},
  pages        = {015001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Embedding human heuristics in machine-learning-enabled probe microscopy},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-dimensional potential energy surfaces for molecular
simulations: From empiricism to machine learning. <em>MLST</em>,
<em>1</em>(1), 013001. (<a
href="https://doi.org/10.1088/2632-2153/ab5922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An overview of computational methods to describe high-dimensional potential energy surfaces suitable for atomistic simulations is given. Particular emphasis is put on accuracy, computability, transferability and extensibility of the methods discussed. They include empirical force fields, representations based on reproducing kernels, using permutationally invariant polynomials, neural network-learned representations and combinations thereof. Future directions and potential improvements are discussed primarily from a practical, application-oriented perspective.},
  archive      = {J_MLST},
  author       = {Oliver T Unke and Debasish Koner and Sarbani Patra and Silvan Käser and Markus Meuwly},
  doi          = {10.1088/2632-2153/ab5922},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {1},
  pages        = {013001},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {High-dimensional potential energy surfaces for molecular simulations: From empiricism to machine learning},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Introducing machine learning: Science and technology.
<em>MLST</em>, <em>1</em>(1), 010201. (<a
href="https://doi.org/10.1088/2632-2153/ab6d5d">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the remarkable progress of ever-growing digitalisation and computing capabilities, data has become increasingly abundant, and machine learning has emerged as a key ingredient in many enabling technologies within modern society. Its potential for pushing the frontiers of science is now also clear and has been demonstrated in various domains extending from novel materials design, quantum physics and the simulation of molecules and chemical systems, to particle physics, medical imaging, space science, climate science and drug discovery. Conceived in close consultation with the community, Machine Learning: Science and Technology has been launched as a unique multidisciplinary, open access journal that will bridge the application of machine learning across the natural sciences with new conceptual advances in machine learning methods as motivated by physical insights.},
  archive      = {J_MLST},
  author       = {O Anatole von Lilienfeld},
  doi          = {10.1088/2632-2153/ab6d5d},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {1},
  pages        = {010201},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Introducing machine learning: Science and technology},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Determining optical constants of 2D materials with neural
networks from multi-angle reflectometry data. <em>MLST</em>,
<em>1</em>(1), 01LT01. (<a
href="https://doi.org/10.1088/2632-2153/ab6d5f">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Synthetically generated multi-angle reflectometry data is used to train a neural network based learning system to estimate the refractive index of atomically thin layered materials in the visible part of the electromagnetic spectrum. Unlike previously developed regression based optical characterization methods, the prediction is achieved via classification by using the probabilities of each input element belonging to a label as weighting coefficients in a simple analytical formula. Various types of activation functions and gradient descent optimizers are tested to determine the optimum combination yielding the best performance. For the verification of the proposed method&#39;s accuracy, four different materials are studied. In all cases, the maximum error is calculated to be less than 0.3%. Considering the highly dispersive nature of the studied materials, this result is a substantial improvement in terms of accuracy and efficiency compared to traditional approaches.},
  archive      = {J_MLST},
  author       = {Ergun Simsek},
  doi          = {10.1088/2632-2153/ab6d5f},
  journal      = {Machine Learning: Science and Technology},
  month        = {2},
  number       = {1},
  pages        = {01LT01},
  shortjournal = {Mach. Learn.: Sci. Technol.},
  title        = {Determining optical constants of 2D materials with neural networks from multi-angle reflectometry data},
  volume       = {1},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
