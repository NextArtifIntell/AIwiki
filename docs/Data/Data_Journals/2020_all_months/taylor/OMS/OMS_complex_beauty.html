<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>OMS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="oms---59">OMS - 59</h2>
<ul>
<li><details>
<summary>
(2020). Varying-parameter zhang neural network for approximating
some expressions involving outer inverses. <em>OMS</em>, <em>35</em>(6),
1304‚Äì1330. (<a
href="https://doi.org/10.1080/10556788.2019.1594806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A varying-parameter ZNN (VPZNN) neural design is defined for approximating various generalized inverses and expressions involving generalized inverses of complex matrices. The proposed model is termed as C V P Z N N ( A , F , G ) C V P Z N N ( A , F , G ) CVPZNN(A,F,G) and defined on the basis of the error function which includes three appropriate matrices A , F , G . The C V P Z N N ( A , F , G ) C V P Z N N ( A , F , G ) CVPZNN(A,F,G) evolution design includes so far defined VPZNN models for computing generalized inverses and also generates a number of matrix expressions involving these generalized inverses. Global and super-exponential convergence properties of the proposed model as well as behaviour of its equilibrium state are investigated. Main contribution of the defined model is its generality. Most important particular cases of the defined model are presented in order to show this fact explicitly. Presented simulation results illustrate generality and effectiveness of the discovered ZNN evolution design.},
  archive      = {J_OMS},
  author       = {Predrag S. Stanimiroviƒá and Vasilios N. Katsikis and Zhijun Zhang and Shuai Li and Jianlong Chen and Mengmeng Zhou},
  doi          = {10.1080/10556788.2019.1594806},
  journal      = {Optimization Methods and Software},
  number       = {6},
  pages        = {1304-1330},
  shortjournal = {Optim. Methods Softw.},
  title        = {Varying-parameter zhang neural network for approximating some expressions involving outer inverses},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New versions of newton method: Step-size choice, convergence
domain and under-determined equations. <em>OMS</em>, <em>35</em>(6),
1272‚Äì1303. (<a
href="https://doi.org/10.1080/10556788.2019.1669154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Newton method is one of the most powerful methods for finding solutions of nonlinear equations and for proving their existence. In its ‚Äòpure‚Äô form it has fast convergence near the solution, but small convergence domain. On the other hand damped Newton method has slower convergence rate, but weaker conditions on the initial point. We provide new versions of Newton-like algorithms, resulting in combinations of Newton and damped Newton method with special step-size choice, and estimate its convergence domain. Under some assumptions the convergence is global. Explicit complexity results are also addressed. The adaptive version of the algorithm (with no a priori constants knowledge) is presented. The method is applicable for under-determined equations (with m &lt; n , m being the number of equations and n being the number of variables). The results are specified for systems of quadratic equations, for composite mappings and for one-dimensional equations and inequalities.},
  archive      = {J_OMS},
  author       = {Boris Polyak and Andrey Tremba},
  doi          = {10.1080/10556788.2019.1669154},
  journal      = {Optimization Methods and Software},
  number       = {6},
  pages        = {1272-1303},
  shortjournal = {Optim. Methods Softw.},
  title        = {New versions of newton method: Step-size choice, convergence domain and under-determined equations},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deflation for semismooth equations. <em>OMS</em>,
<em>35</em>(6), 1248‚Äì1271. (<a
href="https://doi.org/10.1080/10556788.2019.1613655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variational inequalities can in general support distinct solutions. In this paper we study an algorithm for computing distinct solutions of a variational inequality, without varying the initial guess supplied to the solver. The central idea is the combination of a semismooth Newton method with a deflation operator that eliminates known solutions from consideration. Given one root of a semismooth residual, deflation constructs a new problem for which a semismooth Newton method will not converge to the known root, even from the same initial guess. This enables the discovery of other roots. We prove the effectiveness of the deflation technique under the same assumptions that guarantee locally superlinear convergence of a semismooth Newton method. We demonstrate its utility on various finite- and infinite-dimensional examples drawn from constrained optimization, game theory, economics and solid mechanics.},
  archive      = {J_OMS},
  author       = {Patrick E. Farrell and Matteo Croci and Thomas M. Surowiec},
  doi          = {10.1080/10556788.2019.1613655},
  journal      = {Optimization Methods and Software},
  number       = {6},
  pages        = {1248-1271},
  shortjournal = {Optim. Methods Softw.},
  title        = {Deflation for semismooth equations},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A superlinearly convergent nonmonotone quasi-newton method
for unconstrained multiobjective optimization. <em>OMS</em>,
<em>35</em>(6), 1223‚Äì1247. (<a
href="https://doi.org/10.1080/10556788.2020.1737691">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose and analyse a nonmonotone quasi-Newton algorithm for unconstrained strongly convex multiobjective optimization. In our method, we allow for the decrease of a convex combination of recent function values. We establish the global convergence and local superlinear rate of convergence under reasonable assumptions. We implement our scheme in the context of BFGS quasi-Newton method for solving unconstrained multiobjective optimization problems. Our numerical results show that the nonmonotone quasi-Newton algorithm uses fewer function evaluations than the monotone quasi-Newton algorithm.},
  archive      = {J_OMS},
  author       = {N. Mahdavi-Amiri and F. Salehi Sadaghiani},
  doi          = {10.1080/10556788.2020.1737691},
  journal      = {Optimization Methods and Software},
  number       = {6},
  pages        = {1223-1247},
  shortjournal = {Optim. Methods Softw.},
  title        = {A superlinearly convergent nonmonotone quasi-newton method for unconstrained multiobjective optimization},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The method of codifferential descent for convex and global
piecewise affine optimization. <em>OMS</em>, <em>35</em>(6), 1191‚Äì1222.
(<a href="https://doi.org/10.1080/10556788.2019.1571590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The class of nonsmooth codifferentiable functions was introduced by professor V.F.¬†Demyanov in the late 1980s. He also proposed a method for minimizing these functions called the method of codifferential descent (MCD). However, until now almost no theoretical results on the performance of this method on particular classes of nonsmooth optimization problems were known. In the first part of the paper, we study the performance of the method of codifferential descent on a class of nonsmooth convex functions satisfying some regularity assumptions, which in the smooth case are reduced to the Lipschitz continuity of the gradient. We prove that in this case the MCD has the iteration complexity bound O ( 1 / Œµ ) . In the second part of the paper we obtain new global optimality conditions for piecewise affine functions in terms of codifferentials. With the use of these conditions we propose a modification of the MCD for minimizing piecewise affine functions (called the method of global codifferential descent) that does not use line search, and discards those ‚Äòpieces‚Äô of the objective functions that are no longer useful for the optimization process. Then we prove that the MCD as well as its modification proposed in the article find a point of global minimum of a nonconvex piecewise affine function in a finite number of steps.},
  archive      = {J_OMS},
  author       = {M. V. Dolgopolik},
  doi          = {10.1080/10556788.2019.1571590},
  journal      = {Optimization Methods and Software},
  number       = {6},
  pages        = {1191-1222},
  shortjournal = {Optim. Methods Softw.},
  title        = {The method of codifferential descent for convex and global piecewise affine optimization},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The maximum tensor complementarity eigenvalues.
<em>OMS</em>, <em>35</em>(6), 1179‚Äì1190. (<a
href="https://doi.org/10.1080/10556788.2018.1528251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes an algorithm for computing the maximum (or minimum) complementarity eigenvalues of tensors. We formulate this as proper polynomial optimization problems that can be efficiently solved by Lasserre type semidefinite relaxations. The algorithm is proved to have finite convergence for generic tensors.},
  archive      = {J_OMS},
  author       = {Jinyan Fan and Jiawang Nie and Ruixue Zhao},
  doi          = {10.1080/10556788.2018.1528251},
  journal      = {Optimization Methods and Software},
  number       = {6},
  pages        = {1179-1190},
  shortjournal = {Optim. Methods Softw.},
  title        = {The maximum tensor complementarity eigenvalues},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On solving the densest k-subgraph problem on large graphs.
<em>OMS</em>, <em>35</em>(6), 1160‚Äì1178. (<a
href="https://doi.org/10.1080/10556788.2019.1595620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The densest k -subgraph problem is the problem of finding a k -vertex subgraph of a graph with the maximum number of edges. In order to solve large instances of the densest k -subgraph problem, we introduce two algorithms that are based on the random coordinate descent approach. Although it is common use to update at most two random coordinates simultaneously in each iteration of an algorithm, our algorithms may simultaneously update many coordinates. We show the benefit of updating more than two coordinates simultaneously for solving the densest k -subgraph problem, and solve large problem instances with up to 2 15 vertices.},
  archive      = {J_OMS},
  author       = {Renata Sotirov},
  doi          = {10.1080/10556788.2019.1595620},
  journal      = {Optimization Methods and Software},
  number       = {6},
  pages        = {1160-1178},
  shortjournal = {Optim. Methods Softw.},
  title        = {On solving the densest k-subgraph problem on large graphs},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimizing streaming graph partitioning via a heuristic
greedy method and caching strategy. <em>OMS</em>, <em>35</em>(6),
1144‚Äì1159. (<a
href="https://doi.org/10.1080/10556788.2018.1553971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph partitioning is an important method for accelerating large distributed graph computation. Streaming graph partitioning is more efficient than offline partitioning, and it has been developed continuously in the application of graph partitioning in recent years. In this work, we first introduce a heuristic greedy streaming partitioning method and show that it outperforms the state-of-the-art streaming partitioning methods, leading to exact balance and fewer cut edges. Second, we propose a cache structure for streaming partitioning, called an adjacent edge structure, which can improve the partition efficiency several times on a single commodity type computer without affecting the partition quality. Regardless as to whether the memory capacity is limited (local cache) or not (global cache), our strategy can also improve the partition quality by restreaming partitioning. Taking linear weight greedy streaming algorithm as an example, the experimental results on 19 real-world graphs show that the average partitioning time of the new method is 4.9 times faster than that of the original method, which proves the effectiveness and superiority of the cache structure mentioned in this paper.},
  archive      = {J_OMS},
  author       = {Qi Li and Jiang Zhong and Zehong Cao and Xue Li},
  doi          = {10.1080/10556788.2018.1553971},
  journal      = {Optimization Methods and Software},
  number       = {6},
  pages        = {1144-1159},
  shortjournal = {Optim. Methods Softw.},
  title        = {Optimizing streaming graph partitioning via a heuristic greedy method and caching strategy},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Time-homogeneous top-k ranking using tensor decompositions.
<em>OMS</em>, <em>35</em>(6), 1119‚Äì1143. (<a
href="https://doi.org/10.1080/10556788.2019.1584623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given N items, all having positive latent strengths, top- K ranking problem aims to identify the K items ( K ‚â§ N K ‚â§ N K‚â§N ) receiving the highest ranks based on partially revealed comparisons among the items. This problem has been widely studied in the case for which comparisons are performed independently. However, identifying the top- K rankings becomes a more intricate task when comparisons per se are performed along some temporal dimension. In this paper, we investigate potential impacts of temporality on sequences of top- K items and propose a time-homogeneous ranking scheme. Our framework relies mainly on tensor decompositions, rank centrality, and an innovative continuous extension of the Bradley‚ÄìTerry‚ÄìLuce (BTL) model. The proposed continuous BTL model extends the win/loss nature of the logistic model to a continuous setting, further reflecting preference degrees that may exist among the compared items. Our computations, which pertain to the analysis of S&amp;P500 data from January 2008 to December 2017, confirm that the proposed top- K ranking scheme is an effective approach to optimize cardinality-constrained portfolios which involve large volumes of noisy and incomplete data.},
  archive      = {J_OMS},
  author       = {Masoud Ataei and Shengyuan Chen and Zijiang Yang and M. Reza Peyghami},
  doi          = {10.1080/10556788.2019.1584623},
  journal      = {Optimization Methods and Software},
  number       = {6},
  pages        = {1119-1143},
  shortjournal = {Optim. Methods Softw.},
  title        = {Time-homogeneous top-K ranking using tensor decompositions},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Splitting proximal with penalization schemes for additive
convex hierarchical minimization problems. <em>OMS</em>, <em>35</em>(6),
1098‚Äì1118. (<a
href="https://doi.org/10.1080/10556788.2018.1556660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a splitting proximal algorithm with penalization for minimizing a finite sum of proper, convex and lower semicontinuous functions subject to the set of minimizers of another proper, convex and lower semicontinuous function. We show convergences of the generated sequence of iterates to an optimal solution of the considered convex hierarchical minimization problem. Some numerical experiments on the regularized least squares problems are given to show the effectiveness of the obtained theoretical results.},
  archive      = {J_OMS},
  author       = {Nimit Nimana and Narin Petrot},
  doi          = {10.1080/10556788.2018.1556660},
  journal      = {Optimization Methods and Software},
  number       = {6},
  pages        = {1098-1118},
  shortjournal = {Optim. Methods Softw.},
  title        = {Splitting proximal with penalization schemes for additive convex hierarchical minimization problems},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). XCT image reconstruction by a modified superiorized
iteration and theoretical analysis. <em>OMS</em>, <em>35</em>(6),
1080‚Äì1097. (<a
href="https://doi.org/10.1080/10556788.2018.1560442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose an improved iteration superiorization method for X-ray computed tomography image reconstruction. We simplify the classic superiorized iteration by removing two constraints imposed on the perturbation. A novel method is proposed to determine the perturbation amount and direction for the superiorized iteration simultaneously. Some theoretical properties (convergence for instance) of the superiorized iteration sequence with the proposed perturbation are analysed. We present a general proof for the convergence of ART-like iterations with summable perturbations. In addition, we prove the convergence of simultaneous iterations without the summable perturbation assumption. Experiments on simulated and real data not only verify the theoretical result but also show that the proposed algorithm is superior to the classic superiorized iteration and can reconstruct desirable images.},
  archive      = {J_OMS},
  author       = {Shousheng Luo and Yanchun Zhang and Tie Zhou and Jinping Song and Yanfei Wang},
  doi          = {10.1080/10556788.2018.1560442},
  journal      = {Optimization Methods and Software},
  number       = {6},
  pages        = {1080-1097},
  shortjournal = {Optim. Methods Softw.},
  title        = {XCT image reconstruction by a modified superiorized iteration and theoretical analysis},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). O-PCF algorithm for one-class classification. <em>OMS</em>,
<em>35</em>(6), 1065‚Äì1079. (<a
href="https://doi.org/10.1080/10556788.2019.1581191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One-class classification, or outlier detection, is of great importance when data can be properly obtained from only one target class. The problem has many applications in various areas when the outlier class, defined as the complementary set to the target class, is absent. In this paper, we develop a novel one-class classification algorithm called the one-class polyhedral conic functions (O-PCF) algorithm. In this algorithm, the decision boundary for the target class is defined by PCFs&#39; level sets. The level set of a PCF is a convex polyhedron; thus, only convex decision boundaries can be obtained with one PCF. However, the target class may have a non-convex structure. Thus, the O-PCF algorithm divides the target class into k clusters and obtains a PCF for each cluster. O-PCF constructs the final classifier as the minimum of k PCFs to generate non-convex separating surfaces. The performance of the O-PCF algorithm is presented in comparison with other methods in the literature. The test results lead us to conclude that the O-PCF algorithm outperforms the other methods in many cases.},
  archive      = {J_OMS},
  author       = {E. Cimen and G. Ozturk},
  doi          = {10.1080/10556788.2019.1581191},
  journal      = {Optimization Methods and Software},
  number       = {6},
  pages        = {1065-1079},
  shortjournal = {Optim. Methods Softw.},
  title        = {O-PCF algorithm for one-class classification},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A one-parameter class of three-term conjugate gradient
methods with an adaptive parameter choice. <em>OMS</em>, <em>35</em>(6),
1051‚Äì1064. (<a
href="https://doi.org/10.1080/10556788.2018.1510926">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a one-parameter class of three-term conjugate gradient methods is proposed for solving large-scale unconstrained optimization problem. Based on eigenvalue study, an adaptive choice for the embedded parameter and the corresponding three-term conjugate gradient method is obtained. In addition, the convergence properties of the obtained method are studied for both convex and general function. Numerical experiments show that the proposed method has efficient and robust performance.},
  archive      = {J_OMS},
  author       = {Shengwei Yao and Liangshuo Ning and Huonian Tu and Jieqiong Xu},
  doi          = {10.1080/10556788.2018.1510926},
  journal      = {Optimization Methods and Software},
  number       = {6},
  pages        = {1051-1064},
  shortjournal = {Optim. Methods Softw.},
  title        = {A one-parameter class of three-term conjugate gradient methods with an adaptive parameter choice},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The rate of convergence of proximal method of multipliers
for nonlinear programming. <em>OMS</em>, <em>35</em>(5), 1022‚Äì1049. (<a
href="https://doi.org/10.1080/10556788.2020.1738435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze the rate of convergence of the proximal method of multipliers for non-convex nonlinear programming problems. First, we prove, under the strict complementarity condition, that the rate of convergence of the proximal method of multipliers is linear and the ratio constant is proportional to 1/ c when the ratio ‚Äñ ( Œº 0 , Œª 0 ) ‚àí ( Œº ‚éØ ‚éØ ‚éØ , Œª ‚éØ ‚éØ ‚éØ ) ‚Äñ / c ‚Äñ ( Œº 0 , Œª 0 ) ‚àí ( Œº ¬Ø , Œª ¬Ø ) ‚Äñ / c ‚Äñ(Œº0,Œª0)‚àí(Œº¬Ø,Œª¬Ø)‚Äñ/c is small enough, which implies that the rate of convergence of the proximal method of multipliers is superlinear when the parameter c increases to + ‚àû . Second, we prove that, without strict complementarity condition, the rate of convergence of the proximal method of multipliers is proportional to 1/ c when c exceeds a threshold.},
  archive      = {J_OMS},
  author       = {Yule Zhang and Jia Wu and Liwei Zhang},
  doi          = {10.1080/10556788.2020.1738435},
  journal      = {Optimization Methods and Software},
  number       = {5},
  pages        = {1022-1049},
  shortjournal = {Optim. Methods Softw.},
  title        = {The rate of convergence of proximal method of multipliers for nonlinear programming},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A stochastic dual dynamic programming method for two-stage
distributionally robust optimization problems. <em>OMS</em>,
<em>35</em>(5), 1002‚Äì1021. (<a
href="https://doi.org/10.1080/10556788.2020.1811705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies a class of two-stage distributionally robust optimization (TDRO) problems which comes from many practical application fields. In order to set up some implementable solution method, we first transfer the TDRO problem to its equivalent robust counterpart (RC) by the duality theorem of optimization. The RC reformulation of TDRO is a semi-infinite stochastic programming. Then we construct a conditional value-at-risk-based sample average approximation model for the RC problem. Furthermore, we analyse the error bound of the approximation model and obtain the convergent results with respect to optimal value and optimal solution set. Finally, a so-called stochastic dual dynamic programming approach is proposed to solve the approximate model. Numerical results validate the solution approach of this paper.},
  archive      = {J_OMS},
  author       = {Xiaojiao Tong and Liu Yang and Xiao Luo and Bo Rao},
  doi          = {10.1080/10556788.2020.1811705},
  journal      = {Optimization Methods and Software},
  number       = {5},
  pages        = {1002-1021},
  shortjournal = {Optim. Methods Softw.},
  title        = {A stochastic dual dynamic programming method for two-stage distributionally robust optimization problems},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Gravity-magnetic cross-gradient joint inversion by the
cyclic gradient method. <em>OMS</em>, <em>35</em>(5), 982‚Äì1001. (<a
href="https://doi.org/10.1080/10556788.2020.1786565">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider a joint-inversion problem using different types of geophysical data: gravity and magnetism. We first formulate two kinds of inverse problems in the famework of the first kind Fredholm integral equations, and then build up a sparse inversion model combining the two inverse problems as well as the cross-gradient term. The cyclic gradient method for quadratic function minimization is extended for solving the corresponding optimization problem. We update the stepsizes in a cyclic way, by combining the approximated Cauchy steps and the fixed steplengths. Theoretical analysis shows that the algorithm converges R-linearly. Experimental tests show that the proposed joint inversion sparse model as well as the proposed cyclic gradient method improve the numerical performances effectively, compared to the state of the art.},
  archive      = {J_OMS},
  author       = {Cong Sun and Yanfei Wang},
  doi          = {10.1080/10556788.2020.1786565},
  journal      = {Optimization Methods and Software},
  number       = {5},
  pages        = {982-1001},
  shortjournal = {Optim. Methods Softw.},
  title        = {Gravity-magnetic cross-gradient joint inversion by the cyclic gradient method},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On a transformation of the ‚àó-congruence sylvester equation
for the least squares optimization. <em>OMS</em>, <em>35</em>(5),
974‚Äì981. (<a
href="https://doi.org/10.1080/10556788.2020.1734004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ‚òÖ ‚òÖ ‚òÖ -congruence Sylvester equation is the matrix equation A X + X ‚ãÜ B = C A X + X ‚ãÜ B = C AX+X‚ãÜB=C , where A ‚àà ùîΩ m √ó n A ‚àà F m √ó n A‚ààFm√ón , B ‚àà ùîΩ n √ó m B ‚àà F n √ó m B‚ààFn√óm and C ‚àà ùîΩ m √ó m C ‚àà F m √ó m C‚ààFm√óm are given, whereas X ‚àà ùîΩ n √ó m X ‚àà F n √ó m X‚ààFn√óm is to be determined. Here, ùîΩ = ‚Ñù F = R F=R or ‚ÑÇ C C , and ‚ãÜ = T ‚ãÜ=T (transposed) or ‚àó (conjugate transposed). Very recently, Satake et¬†al. showed that under some conditions, the matrix equation for the case ‚ãÜ = T is equivalent to the generalized Sylvester equation. In this paper, we demonstrate that the result can be extended to the case ‚ãÜ = ‚àó . Through this extension, the least squares solution of the ‚àó -congruence Sylvester equation may be obtained using well-researched results on the least squares solution of the generalized Sylvester equation.},
  archive      = {J_OMS},
  author       = {Yuki Satake and Tomohiro Sogabe and Tomoya Kemmochi and Shao-Liang Zhang},
  doi          = {10.1080/10556788.2020.1734004},
  journal      = {Optimization Methods and Software},
  number       = {5},
  pages        = {974-981},
  shortjournal = {Optim. Methods Softw.},
  title        = {On a transformation of the ‚àó-congruence sylvester equation for the least squares optimization},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Training GANs with centripetal acceleration. <em>OMS</em>,
<em>35</em>(5), 955‚Äì973. (<a
href="https://doi.org/10.1080/10556788.2020.1754414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training generative adversarial networks (GANs) often suffers from cyclic behaviours of iterates. Based on a simple intuition that the direction of centripetal acceleration of an object moving in uniform circular motion is toward the centre of the circle, we present the Simultaneous Centripetal Acceleration (SCA) method and the Alternating Centripetal Acceleration (ACA) method to alleviate the cyclic behaviours. Under suitable conditions, gradient descent methods with either SCA or ACA are shown to be linearly convergent for bilinear games. Numerical experiments are conducted by applying ACA to existing gradient-based algorithms in a GAN setup scenario, which demonstrate the superiority of ACA.},
  archive      = {J_OMS},
  author       = {Wei Peng and Yu-Hong Dai and Hui Zhang and Lizhi Cheng},
  doi          = {10.1080/10556788.2020.1754414},
  journal      = {Optimization Methods and Software},
  number       = {5},
  pages        = {955-973},
  shortjournal = {Optim. Methods Softw.},
  title        = {Training GANs with centripetal acceleration},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the existence of affine invariant descent directions.
<em>OMS</em>, <em>35</em>(5), 938‚Äì954. (<a
href="https://doi.org/10.1080/10556788.2020.1740221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper begins with a brief review of affine invariance and its significance for iterative algorithms. It then explores the existence of affine invariant descent directions for unconstrained minimization. While there may exist several affine invariant descent directions for smooth functions at a given point, it is shown that for quadratic functions, there exists exactly one invariant descent direction in the strictly convex case and generally none in the case where the Hessian is singular or indefinite. These results can be generalized to smooth nonlinear functions and have implications regarding the initialization of minimization algorithms. They stand in contrast to recent works on constrained convex and nonconvex optimization for which there may exist an affine invariant ‚Äòframe‚Äô that depends on the feasible set and that can be used to define an affine invariant descent direction.},
  archive      = {J_OMS},
  author       = {Yu-Hong Dai and Florian Jarre and Felix Lieder},
  doi          = {10.1080/10556788.2020.1740221},
  journal      = {Optimization Methods and Software},
  number       = {5},
  pages        = {938-954},
  shortjournal = {Optim. Methods Softw.},
  title        = {On the existence of affine invariant descent directions},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stochastic proximal linear method for structured non-convex
problems. <em>OMS</em>, <em>35</em>(5), 921‚Äì937. (<a
href="https://doi.org/10.1080/10556788.2020.1754413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, motivated by the challenging task of learning a deep neural network, we consider optimization problems that consist of minimizing a finite-sum of non-convex and non-smooth functions, where the non-smoothness appears as the maximum of non-convex functions with Lipschitz continuous gradient. Due to the large size of the sum, in practice, we focus here on stochastic first-order methods and propose the Stochastic Proximal Linear Method (SPLM) that is based on minimizing an appropriate majorizer at each iteration and is guaranteed to almost surely converge to a critical point of the objective function, where we also proves its convergence rate in finding critical points.},
  archive      = {J_OMS},
  author       = {Tamir Hazan and Shoham Sabach and Sergey Voldman},
  doi          = {10.1080/10556788.2020.1754413},
  journal      = {Optimization Methods and Software},
  number       = {5},
  pages        = {921-937},
  shortjournal = {Optim. Methods Softw.},
  title        = {Stochastic proximal linear method for structured non-convex problems},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Complexity and performance of an augmented lagrangian
algorithm. <em>OMS</em>, <em>35</em>(5), 885‚Äì920. (<a
href="https://doi.org/10.1080/10556788.2020.1746962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Algencan is a well established safeguarded Augmented Lagrangian algorithm introduced in [R. Andreani, E. G. Birgin, J. M. Mart√≠nez, and M. L. Schuverdt, On Augmented Lagrangian methods with general lower-level constraints , SIAM J. Optim. 18 (2008), pp. 1286‚Äì1309]. Complexity results that report its worst-case behaviour in terms of iterations and evaluations of functions and derivatives that are necessary to obtain suitable stopping criteria are presented in this work. In addition, its computational performance considering all problems from the CUTEst collection is presented, which shows that it is a useful tool for solving large-scale constrained optimization problems.},
  archive      = {J_OMS},
  author       = {E. G. Birgin and J. M. Mart√≠nez},
  doi          = {10.1080/10556788.2020.1746962},
  journal      = {Optimization Methods and Software},
  number       = {5},
  pages        = {885-920},
  shortjournal = {Optim. Methods Softw.},
  title        = {Complexity and performance of an augmented lagrangian algorithm},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient augmented lagrangian method for support vector
machine. <em>OMS</em>, <em>35</em>(4), 855‚Äì883. (<a
href="https://doi.org/10.1080/10556788.2020.1734002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Support vector machine (SVM) has proved to be a successful approach for machine learning. Two typical SVM models are the L1-loss model for support vector classification (SVC) and Œµ -L1-loss model for support vector regression (SVR). Due to the non-smoothness of the L1-loss function in the two models, most of the traditional approaches focus on solving the dual problem. In this paper, we propose an augmented Lagrangian method for the L1-loss model, which is designed to solve the primal problem. By tackling the non-smooth term in the model with Moreau‚ÄìYosida regularization and the proximal operator, the subproblem in augmented Lagrangian method reduces to a non-smooth linear system, which can be solved via the quadratically convergent semismooth Newton&#39;s method. Moreover, the high computational cost in semismooth Newton&#39;s method can be significantly reduced by exploring the sparse structure in the generalized Jacobian. Numerical results on various datasets in LIBLINEAR show that the proposed method is competitive with the most popular solvers in both speed and accuracy.},
  archive      = {J_OMS},
  author       = {Yinqiao Yan and Qingna Li},
  doi          = {10.1080/10556788.2020.1734002},
  journal      = {Optimization Methods and Software},
  number       = {4},
  pages        = {855-883},
  shortjournal = {Optim. Methods Softw.},
  title        = {An efficient augmented lagrangian method for support vector machine},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stability analysis of a class of sparse optimization
problems. <em>OMS</em>, <em>35</em>(4), 836‚Äì854. (<a
href="https://doi.org/10.1080/10556788.2020.1734003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sparse optimization problems arise in many areas of science and engineering, such as compressed sensing, image processing, statistical and machine learning. The ‚Ñì 0 ‚Ñì 0 ‚Ñì0 -minimization problem is one of such optimization problems, which is typically used to deal with signal recovery. The ‚Ñì 1 ‚Ñì 1 ‚Ñì1 -minimization method is one of the popular approaches for solving the ‚Ñì 0 ‚Ñì 0 ‚Ñì0 -minimization problems, and the stability of such a numerical method is vital for solving such sparse optimization problems and particularly vital for signal recovery. In this paper, we establish a stability result for the ‚Ñì 1 -minimization problems associated with a broad class of ‚Ñì 0 -minimization problems. To this goal, we introduce the concept of restricted weak range space property (RSP) of a transposed sensing matrix, which is a generalized version of the weak RSP of the transposed sensing matrix introduced in [Zhao et¬†al., Math. Oper. Res. 44 (2019), pp. 175‚Äì193]. The stability result established in this paper includes several existing ones as special cases.},
  archive      = {J_OMS},
  author       = {Jialiang Xu and Yun-Bin Zhao},
  doi          = {10.1080/10556788.2020.1734003},
  journal      = {Optimization Methods and Software},
  number       = {4},
  pages        = {836-854},
  shortjournal = {Optim. Methods Softw.},
  title        = {Stability analysis of a class of sparse optimization problems},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inexact proximal stochastic second-order methods for
nonconvex composite optimization. <em>OMS</em>, <em>35</em>(4), 808‚Äì835.
(<a href="https://doi.org/10.1080/10556788.2020.1713128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a framework of Inexact Proximal Stochastic Second-order (IPSS) method for solving nonconvex optimization problems, whose objective function consists of an average of finitely many, possibly weakly, smooth functions and a convex but possibly nonsmooth function. At each iteration, IPSS inexactly solves a proximal subproblem constructed by using some positive definite matrix which could capture the second-order information of original problem. Proper tolerances are given for the subproblem solution in order to maintain global convergence and the desired overall complexity of the algorithm. Under mild conditions, we analyse the computational complexity related to the evaluations on the component gradient of the smooth function. We also investigate the number of evaluations of subgradient when using an iterative subgradient method to solve the subproblem. In addition, based on IPSS, we propose a linearly convergent algorithm under the proximal Polyak‚Äì≈Åojasiewicz condition. Finally, we extend the analysis to problems with weakly smooth function and obtain the computational complexity accordingly.},
  archive      = {J_OMS},
  author       = {Xiao Wang and Hongchao Zhang},
  doi          = {10.1080/10556788.2020.1713128},
  journal      = {Optimization Methods and Software},
  number       = {4},
  pages        = {808-835},
  shortjournal = {Optim. Methods Softw.},
  title        = {Inexact proximal stochastic second-order methods for nonconvex composite optimization},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Orthogonal canonical correlation analysis and applications.
<em>OMS</em>, <em>35</em>(4), 787‚Äì807. (<a
href="https://doi.org/10.1080/10556788.2019.1700257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Canonical correlation analysis (CCA) is a cornerstone of linear dimensionality reduction techniques that jointly maps two datasets to achieve maximal correlation. CCA has been widely used in applications for capturing data features of interest. In this paper, we establish a range constrained orthogonal CCA (OCCA) model and its variant and apply them for three data analysis tasks of datasets in real-life applications, namely unsupervised feature fusion, multi-target regression and multi-label classification. Numerical experiments show that the OCCA and its variant produce superior accuracy compared to the traditional CCA.},
  archive      = {J_OMS},
  author       = {Li Wang and Lei-hong Zhang and Zhaojun Bai and Ren-Cang Li},
  doi          = {10.1080/10556788.2019.1700257},
  journal      = {Optimization Methods and Software},
  number       = {4},
  pages        = {787-807},
  shortjournal = {Optim. Methods Softw.},
  title        = {Orthogonal canonical correlation analysis and applications},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Convergence rate of the levenberg-marquardt method under
h√∂lderian local error bound. <em>OMS</em>, <em>35</em>(4), 767‚Äì786. (<a
href="https://doi.org/10.1080/10556788.2019.1694927">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we study the convergence rate of the Levenberg-Marquardt (LM) method under the H√∂lderian local error bound condition and the H√∂lderian continuity of the Jacobian, which are more general than the local error bound condition and the Lipschitz continuity of the Jacobian. Various choices of the LM parameter are also discussed.},
  archive      = {J_OMS},
  author       = {Haiyan Wang and Jinyan Fan},
  doi          = {10.1080/10556788.2019.1694927},
  journal      = {Optimization Methods and Software},
  number       = {4},
  pages        = {767-786},
  shortjournal = {Optim. Methods Softw.},
  title        = {Convergence rate of the levenberg-marquardt method under h√∂lderian local error bound},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accelerated dual-averaging primal‚Äìdual method for composite
convex minimization. <em>OMS</em>, <em>35</em>(4), 741‚Äì766. (<a
href="https://doi.org/10.1080/10556788.2020.1713779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dual averaging-type methods are widely used in industrial machine learning applications due to their ability to promoting solution structure (e.g. sparsity) efficiently. In this paper, we propose a novel accelerated dual-averaging primal‚Äìdual algorithm for minimizing a composite convex function. We also derive a stochastic version of the proposed method that solves empirical risk minimization, and its advantages on handling sparse data are demonstrated both theoretically and empirically.},
  archive      = {J_OMS},
  author       = {Conghui Tan and Yuqiu Qian and Shiqian Ma and Tong Zhang},
  doi          = {10.1080/10556788.2020.1713779},
  journal      = {Optimization Methods and Software},
  number       = {4},
  pages        = {741-766},
  shortjournal = {Optim. Methods Softw.},
  title        = {Accelerated dual-averaging primal‚Äìdual method for composite convex minimization},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Randomized mechanism design for decentralized network
scheduling. <em>OMS</em>, <em>35</em>(4), 722‚Äì740. (<a
href="https://doi.org/10.1080/10556788.2020.1713129">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the network scheduling, jobs (tasks) must be scheduled on uniform machines (processors) connected by a complete graph so as to minimize the total weighted completion time. This setting can be applied in distributed multi-processor computing environments and also in operations research. In this paper, we study the design of randomized decentralized mechanism in the setting where a set of non-preemptive jobs select randomly a machine from a set of uniform machines to be processed on, and each machine can process at most one job at a time. We introduce a new concept of myopic Bayes‚ÄìNash incentive compatibility which weakens the classical Bayes‚ÄìNash incentive compatibility and derive a randomized decentralized mechanism under the assumption that each job is a rational and selfish agent. We show that our mechanism can induce jobs to report truthfully their private information referred to myopic Bayes‚ÄìNash implementability by using a graph theoretic interpretation of the incentive compatibility constraints. Furthermore, we prove that the performance of this mechanism is asymptotically optimal.},
  archive      = {J_OMS},
  author       = {Jian Sun and Dachuan Xu and Deren Han and Wenjing Hou and Xiaoyan Zhang},
  doi          = {10.1080/10556788.2020.1713129},
  journal      = {Optimization Methods and Software},
  number       = {4},
  pages        = {722-740},
  shortjournal = {Optim. Methods Softw.},
  title        = {Randomized mechanism design for decentralized network scheduling},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards solving 2-TBSG efficiently. <em>OMS</em>,
<em>35</em>(4), 706‚Äì721. (<a
href="https://doi.org/10.1080/10556788.2019.1695131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-player turn-based stochastic game (2-TBSG) is a two-player game model which aims to find Nash equilibriums and is widely utilized in reinforcement learning and AI. Inspired by the fact that the simplex method for solving the deterministic discounted Markov decision processes is strongly polynomial independent of the discount factor, we are trying to answer an open problem whether there is a similar algorithm for 2-TBSG. We develop a simplex strategy iteration where one player updates its strategy with a simplex step while the other player finds an optimal counterstrategy in turn, and a modified simplex strategy iteration. Both of them belong to a class of geometrically converging algorithms. We establish the strongly polynomial property of these algorithms by considering a strategy combined from the current strategy and the equilibrium strategy. Moreover, we present a method to transform general 2-TBSGs into special 2-TBSGs where each state has exactly two actions.},
  archive      = {J_OMS},
  author       = {Zeyu Jia and Zaiwen Wen and Yinyu Ye},
  doi          = {10.1080/10556788.2019.1695131},
  journal      = {Optimization Methods and Software},
  number       = {4},
  pages        = {706-721},
  shortjournal = {Optim. Methods Softw.},
  title        = {Towards solving 2-TBSG efficiently},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Gradient methods exploiting spectral properties.
<em>OMS</em>, <em>35</em>(4), 681‚Äì705. (<a
href="https://doi.org/10.1080/10556788.2020.1727476">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new stepsize for the gradient method. It is shown that this new stepsize will converge to the reciprocal of the largest eigenvalue of the Hessian, when Dai-Yang&#39;s asymptotic optimal gradient method (Computational Optimization and Applications, 2006, 33(1): 73‚Äì88) is applied for minimizing quadratic objective functions. Based on this spectral property, we develop a monotone gradient method that takes a certain number of steps using the asymptotically optimal stepsize by Dai and Yang, and then follows by some short steps associated with this new stepsize. By employing one step retard of the asymptotic optimal stepsize, a nonmonotone variant of this method is also proposed. Under mild conditions, R -linear convergence of the proposed methods is established for minimizing quadratic functions. In addition, by combining gradient projection techniques and adaptive nonmonotone line search, we further extend those methods for general bound constrained optimization. Two variants of gradient projection methods combining with the Barzilai-Borwein stepsizes are also proposed. Our numerical experiments on both quadratic and bound constrained optimization indicate that the new proposed strategies and methods are very effective.},
  archive      = {J_OMS},
  author       = {Yakui Huang and Yu-Hong Dai and Xin-Wei Liu and Hongchao Zhang},
  doi          = {10.1080/10556788.2020.1727476},
  journal      = {Optimization Methods and Software},
  number       = {4},
  pages        = {681-705},
  shortjournal = {Optim. Methods Softw.},
  title        = {Gradient methods exploiting spectral properties},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An investigation of newton-sketch and subsampled newton
methods. <em>OMS</em>, <em>35</em>(4), 661‚Äì680. (<a
href="https://doi.org/10.1080/10556788.2020.1725751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sketching, a dimensionality reduction technique, has received much attention in the statistics community. In this paper, we study sketching in the context of Newton&#39;s method for solving finite-sum optimization problems in which the number of variables and data points are both large. We study two forms of sketching that perform dimensionality reduction in data space: Hessian subsampling and randomized Hadamard transformations. Each has its own advantages, and their relative tradeoffs have not been investigated in the optimization literature. Our study focuses on practical versions of the two methods in which the resulting linear systems of equations are solved approximately, at every iteration, using an iterative solver. The advantages of using the conjugate gradient method vs. a stochastic gradient iteration are revealed through a set of numerical experiments, and a complexity analysis of the Hessian subsampling method is presented.},
  archive      = {J_OMS},
  author       = {Albert S. Berahas and Raghu Bollapragada and Jorge Nocedal},
  doi          = {10.1080/10556788.2020.1725751},
  journal      = {Optimization Methods and Software},
  number       = {4},
  pages        = {661-680},
  shortjournal = {Optim. Methods Softw.},
  title        = {An investigation of newton-sketch and subsampled newton methods},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GPU parameter tuning for tall and skinny dense linear least
squares problems. <em>OMS</em>, <em>35</em>(3), 638‚Äì660. (<a
href="https://doi.org/10.1080/10556788.2018.1527331">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear least squares problems (LLSPs) routinely arise in many scientific and engineering problems. One of the fastest ways to solve LLSPs involves performing calculations in parallel on graphics processing units (GPUs). However, GPU algorithms are typically designed for one GPU architecture and may be suboptimal or unusable on another GPU. To design optimal algorithms for any GPU with little need for modifying code, tuneable parameters can simplify the transition of GPU algorithms to different GPU architectures. In this paper, we investigate the benefits of using derivative-free optimization (DFO) and simulation optimization (SO) to systematically optimize tuneable parameters for a GPU or hybrid CPU/GPU LLSP solvers. Computational experiments show that both DFO and SO can be effective tools for determining optimal tuning parameters that can speed up the performance of the popular LLSP solver MAGMA by about 1.8x, compared to MAGMA&#39;s default parameters for large tall and skinny matrices. Using DFO solvers, we were able to identify optimal parameters after enumerating an order of magnitude fewer parameter combinations than with direct enumeration. Additionally, the proposed approach is faster than a state-of-the-art autotuner and provides better tuning parameters.},
  archive      = {J_OMS},
  author       = {Benjamin Sauk and Nikolaos Ploskas and Nikolaos Sahinidis},
  doi          = {10.1080/10556788.2018.1527331},
  journal      = {Optimization Methods and Software},
  number       = {3},
  pages        = {638-660},
  shortjournal = {Optim. Methods Softw.},
  title        = {GPU parameter tuning for tall and skinny dense linear least squares problems},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Seeding and adjoining zero-halo partitioned parallel
scientific codes. <em>OMS</em>, <em>35</em>(3), 618‚Äì637. (<a
href="https://doi.org/10.1080/10556788.2019.1591404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Algorithmic differentiation tools can automate the adjoint transformation of parallel message-passing codes [J. Utke, L. Hasco√´t, P. Heimbach, C. Hill, P. Hovland, and U. Naumann, Toward Adjoinable MPI , in 2009 IEEE International Symposium on Parallel &amp; Distributed Processing , May, IEEE, 2009, pp. 1‚Äì8] using the AMPI library. Nevertheless, a non-trivial and manual step after the differentiation is the initialization of the seed and retrieval of the output values from the differentiated code. Ambiguities in seeding occur in programs where the user is unable to expose the complete program flow with a single entry and single exit point to the AD tool. We present the ambiguities associated with seed initialization and output retrieval for adjoint transformation of halo and zero-halo partitioned MPI programs. We introduce a general framework to eliminate ambiguities in seeding and retrieval for shared-node reduction over +, and * operators using a conceptual master-worker model. The model shows the need for new MPI calls for retrieval and eliminate MPI calls for seed initialization. Different implementations for seeding manually assembled adjoints were inferred from the model, namely, partial and unique seeding. We successfully applied the seeding techniques to a 3D zero-halo partitioned unstructured compressible discrete adjoint solver and highlight the merits and demerits of each strategy.},
  archive      = {J_OMS},
  author       = {P. Mohanamuraly and L. Hasco√´t and J.-D. M√ºller},
  doi          = {10.1080/10556788.2019.1591404},
  journal      = {Optimization Methods and Software},
  number       = {3},
  pages        = {618-637},
  shortjournal = {Optim. Methods Softw.},
  title        = {Seeding and adjoining zero-halo partitioned parallel scientific codes},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Identification of the friction function in a semilinear
system for gas transport through a network. <em>OMS</em>,
<em>35</em>(3), 576‚Äì617. (<a
href="https://doi.org/10.1080/10556788.2019.1692206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An identification problem for the friction parameter in a semilinear system of balance laws, describing the transport of gas through a passive network of pipelines, is considered. The existence of broad solutions to the state system is proven and sensitivity results for the corresponding solution operator are obtained. The existence of solutions to the output least squares formulation of the identification problem, based on noisy measurements over time at fixed spatial positions, is established. Finally, numerical experiments validate the theoretical findings.},
  archive      = {J_OMS},
  author       = {Michael Hinterm√ºller and Nikolai Strogies},
  doi          = {10.1080/10556788.2019.1692206},
  journal      = {Optimization Methods and Software},
  number       = {3},
  pages        = {576-617},
  shortjournal = {Optim. Methods Softw.},
  title        = {Identification of the friction function in a semilinear system for gas transport through a network},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the relation between MPECs and optimization problems in
abs-normal form. <em>OMS</em>, <em>35</em>(3), 560‚Äì575. (<a
href="https://doi.org/10.1080/10556788.2019.1588268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that the problem of unconstrained minimization of a function in abs-normal form is equivalent to identifying a certain stationary point of a counterpart Mathematical Program with Equilibrium Constraints (MPEC). Hence, concepts introduced for the abs-normal forms turn out to be closely related to established concepts in the theory of MPECs. We give a number of proofs of equivalence or implication for the kink qualifications LIKQ and MFKQ. We also show that the counterpart MPEC always satisfies MPEC-ACQ. We then consider non-smooth nonlinear optimization problems (NLPs) where both the objective function and the constraints are presented in the abs-normal form. We show that this extended problem class also has a counterpart MPEC problem.},
  archive      = {J_OMS},
  author       = {L. C. Hegerhorst-Schultchen and C. Kirches and M. C. Steinbach},
  doi          = {10.1080/10556788.2019.1588268},
  journal      = {Optimization Methods and Software},
  number       = {3},
  pages        = {560-575},
  shortjournal = {Optim. Methods Softw.},
  title        = {On the relation between MPECs and optimization problems in abs-normal form},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A derivative-free ùí±ùí∞-algorithm for convex finite-max
problems. <em>OMS</em>, <em>35</em>(3), 521‚Äì559. (<a
href="https://doi.org/10.1080/10556788.2019.1668944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ùí± ùí∞ V U VU -algorithm is a superlinearly convergent method for minimizing nonsmooth, convex functions. At each iteration, the algorithm works with a certain ùí± V V -space and its orthogonal ùí∞ U U -space, such that the nonsmoothness of the objective function is concentrated on its projection onto the ùí± V V -space, and on the ùí∞ U U -space the projection is smooth. This structure allows for an alternation between a Newton-like step where the function is smooth, and a proximal-point step that is used to find iterates with promising V U -decompositions. We establish a derivative-free variant of the V U -algorithm for convex finite-max objective functions. We show global convergence and provide numerical results from a proof-of-concept implementation, which demonstrates the feasibility and practical value of the approach. We also carry out some tests using nonconvex functions and discuss the results.},
  archive      = {J_OMS},
  author       = {Warren Hare and Chayne Planiden and Claudia Sagastiz√°bal},
  doi          = {10.1080/10556788.2019.1668944},
  journal      = {Optimization Methods and Software},
  number       = {3},
  pages        = {521-559},
  shortjournal = {Optim. Methods Softw.},
  title        = {A derivative-free ùí±ùí∞-algorithm for convex finite-max problems},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An alternating augmented lagrangian method for constrained
nonconvex optimization. <em>OMS</em>, <em>35</em>(3), 502‚Äì520. (<a
href="https://doi.org/10.1080/10556788.2019.1576177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of minimizing a smooth nonconvex function over a structured convex feasible set, that is, defined by two sets of constraints that are easy to treat when considered separately. In order to exploit the structure of the problem, we define an equivalent formulation by duplicating the variables and we consider the augmented Lagrangian of this latter formulation. Following the idea of the Alternating Direction Method of Multipliers (ADMM), we propose an algorithm where a two-blocks decomposition method is embedded within an augmented Lagrangian framework. The peculiarities of the proposed algorithm are the following: (1) the computation of the exact solution of a possibly nonconvex subproblem is not required; (2) the penalty parameter is iteratively updated once an approximated stationary point of the augmented Lagrangian is determined. Global convergence results are stated under mild assumptions and without requiring convexity of the objective function. Although the primary aim of the paper is theoretical, we perform numerical experiments on a nonconvex problem arising in machine learning, and the obtained results show the practical advantages of the proposed approach with respect to classical ADMM.},
  archive      = {J_OMS},
  author       = {G. Galvan and M. Lapucci and T. Levato and M. Sciandrone},
  doi          = {10.1080/10556788.2019.1576177},
  journal      = {Optimization Methods and Software},
  number       = {3},
  pages        = {502-520},
  shortjournal = {Optim. Methods Softw.},
  title        = {An alternating augmented lagrangian method for constrained nonconvex optimization},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The ‚Äúidiot‚Äù crash quadratic penalty algorithm for linear
programming and its application to linearizations of quadratic
assignment problems. <em>OMS</em>, <em>35</em>(3), 488‚Äì501. (<a
href="https://doi.org/10.1080/10556788.2019.1604702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide the first meaningful documentation and analysis of the ‚ÄòIdiot‚Äô crash implemented by Forrest in Clp that aims to obtain an approximate solution to linear programming (LP) problems for warm-starting the primal simplex method. The underlying algorithm is a penalty method with naive approximate minimization in each iteration. During initial iterations an approach similar to augmented Lagrangian is used. Later the technique corresponds closely to a classical quadratic penalty method. We discuss the extent to which it can be used to obtain fast approximate solutions of LP problems, in particular when applied to linearizations of quadratic assignment problems.},
  archive      = {J_OMS},
  author       = {I. L. Galabova and J. A. J. Hall},
  doi          = {10.1080/10556788.2019.1604702},
  journal      = {Optimization Methods and Software},
  number       = {3},
  pages        = {488-501},
  shortjournal = {Optim. Methods Softw.},
  title        = {The ‚ÄòIdiot‚Äô crash quadratic penalty algorithm for linear programming and its application to linearizations of quadratic assignment problems},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Trust-region algorithms for training responses: Machine
learning methods using indefinite hessian approximations. <em>OMS</em>,
<em>35</em>(3), 460‚Äì487. (<a
href="https://doi.org/10.1080/10556788.2019.1624747">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine learning (ML) problems are often posed as highly nonlinear and nonconvex unconstrained optimization problems. Methods for solving ML problems based on stochastic gradient descent are easily scaled for very large problems but may involve fine-tuning many hyper-parameters. Quasi-Newton approaches based on the limited-memory Broyden-Fletcher-Goldfarb-Shanno (BFGS) update typically do not require manually tuning hyper-parameters but suffer from approximating a potentially indefinite Hessian with a positive-definite matrix. Hessian-free methods leverage the ability to perform Hessian-vector multiplication without needing the entire Hessian matrix, but each iteration&#39;s complexity is significantly greater than quasi-Newton methods. In this paper we propose an alternative approach for solving ML problems based on a quasi-Newton trust-region framework for solving large-scale optimization problems that allow for indefinite Hessian approximations. Numerical experiments on a standard testing data set show that with a fixed computational time budget, the proposed methods achieve better results than the traditional limited-memory BFGS and the Hessian-free methods.},
  archive      = {J_OMS},
  author       = {Jennifer B. Erway and Joshua Griffin and Roummel F. Marcia and Riadh Omheni},
  doi          = {10.1080/10556788.2019.1624747},
  journal      = {Optimization Methods and Software},
  number       = {3},
  pages        = {460-487},
  shortjournal = {Optim. Methods Softw.},
  title        = {Trust-region algorithms for training responses: Machine learning methods using indefinite hessian approximations},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Separation theorems for the extrema of best piecewise
monotonic approximations to successive data. <em>OMS</em>,
<em>35</em>(3), 439‚Äì459. (<a
href="https://doi.org/10.1080/10556788.2019.1613653">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Separation properties of the local extrema of best piecewise monotonic approximations to measurements of a real univariate function are of fundamental importance to the development of efficient algorithms that calculate these approximations. Piecewise monotonic approximation to n data is expressed as the minimization of some strictly convex function of the data errors subject to the restriction that the piecewise linear interpolant to the approximated values consists of at most k monotonic sections, where k is a prescribed positive integer. The major task is to determine automatically the positions of the joins of these sections, which is a combinatorial problem that can require about O ( n k ‚àí 1 ) combinations in order to find an optimal one. We state theorems which prove the remarkable property that the local maxima of optimal approximations with k ‚àí1 monotonic sections are separated by the local maxima of optimal approximations with k monotonic sections, and local minima are separated similarly. We describe briefly a suitable technique that makes use of this property and gives the global solution in O ( n 2 + k n log 2 ‚Å° n ) computer operations. Some numerical results show large gains in efficiency over existing methods. Further, as an illustration, we apply the technique to 39,082 observations of daily sunspots.},
  archive      = {J_OMS},
  author       = {I. C. Demetriou},
  doi          = {10.1080/10556788.2019.1613653},
  journal      = {Optimization Methods and Software},
  number       = {3},
  pages        = {439-459},
  shortjournal = {Optim. Methods Softw.},
  title        = {Separation theorems for the extrema of best piecewise monotonic approximations to successive data},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Symmetric rank-1 approximation of symmetric high-order
tensors. <em>OMS</em>, <em>35</em>(2), 416‚Äì438. (<a
href="https://doi.org/10.1080/10556788.2019.1678034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding the symmetric rank-1 approximation to a given symmetric tensor is an important problem due to its wide applications and its close relationship to the Z -eigenpair of a tensor. In this paper, we propose a method based on the proximal alternating linearized minimization to directly solve the optimization problem. Global convergence of our algorithm is established. Numerical experiments show that our algorithm is very competitive in speed, accuracy and robustness compared to other state-of-the-art methods.},
  archive      = {J_OMS},
  author       = {Leqin Wu and Xin Liu and Zaiwen Wen},
  doi          = {10.1080/10556788.2019.1678034},
  journal      = {Optimization Methods and Software},
  number       = {2},
  pages        = {416-438},
  shortjournal = {Optim. Methods Softw.},
  title        = {Symmetric rank-1 approximation of symmetric high-order tensors},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modified gradient dynamic approach to the tensor
complementarity problem. <em>OMS</em>, <em>35</em>(2), 394‚Äì415. (<a
href="https://doi.org/10.1080/10556788.2019.1578766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonlinear gradient dynamic approach for solving the tensor complementarity problem (TCP) are presented. Theoretical analysis shows that each of the defined dynamical system models ensures the convergence. The computer-simulation results further substantiate that the considered dynamical system can solve the TCP.},
  archive      = {J_OMS},
  author       = {Xuezhong Wang and Maolin Che and Liqun Qi and Yimin Wei},
  doi          = {10.1080/10556788.2019.1578766},
  journal      = {Optimization Methods and Software},
  number       = {2},
  pages        = {394-415},
  shortjournal = {Optim. Methods Softw.},
  title        = {Modified gradient dynamic approach to the tensor complementarity problem},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A convergent newton algorithm for computing z-eigenvalues of
an almost nonnegative irreducible tensor. <em>OMS</em>, <em>35</em>(2),
377‚Äì393. (<a
href="https://doi.org/10.1080/10556788.2019.1647196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we compute Z-eigenvalues of a class of tensors by studying the properties of semi-symmetric tensor. We prove that ùíú x m ‚àí 1 A x m ‚àí 1 Axm‚àí1 is identical to zero if and only if ùíú s = 0 A s = 0 As=0 , where ùíú s A s As is the associated semi-symmetric tensor of ùíú A A . Based on the semi-symmetric property, an almost nonnegative irreducible tensor is defined. And we use Newton method to compute Z-eigenvalues of this kind of tensor. The convergence of the proposed algorithm can be guaranteed. Numerical results are reported to illustrate the efficiency of our algorithm.},
  archive      = {J_OMS},
  author       = {Xin Zhang and Qin Ni and Zhili Ge},
  doi          = {10.1080/10556788.2019.1647196},
  journal      = {Optimization Methods and Software},
  number       = {2},
  pages        = {377-393},
  shortjournal = {Optim. Methods Softw.},
  title        = {A convergent newton algorithm for computing Z-eigenvalues of an almost nonnegative irreducible tensor},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computation of second-order directional stationary points
for group sparse optimization. <em>OMS</em>, <em>35</em>(2), 348‚Äì376.
(<a href="https://doi.org/10.1080/10556788.2019.1684492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a nonconvex and nonsmooth group sparse optimization problem where the penalty function is the sum of compositions of a folded concave function and the ‚Ñì 2 ‚Ñì 2 ‚Ñì2 vector norm for each group variable. We show that under some mild conditions a first-order directional stationary point is a strict local minimizer that fulfils the first-order growth condition, and a second-order directional stationary point is a strong local minimizer that fulfils the second-order growth condition. In order to compute second-order directional stationary points, we construct a twice continuously differentiable smoothing problem and show that any accumulation point of the sequence of second-order stationary points of the smoothing problem is a second-order directional stationary point of the original problem. We give numerical examples to illustrate how to compute a second-order directional stationary point by the smoothing method.},
  archive      = {J_OMS},
  author       = {Dingtao Peng and Xiaojun Chen},
  doi          = {10.1080/10556788.2019.1684492},
  journal      = {Optimization Methods and Software},
  number       = {2},
  pages        = {348-376},
  shortjournal = {Optim. Methods Softw.},
  title        = {Computation of second-order directional stationary points for group sparse optimization},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stochastic polynomial optimization. <em>OMS</em>,
<em>35</em>(2), 329‚Äì347. (<a
href="https://doi.org/10.1080/10556788.2019.1649672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies stochastic optimization problems with polynomials. We propose an optimization model with sample averages and perturbations. The Lasserre-type Moment-SOS relaxations are used to solve the sample average optimization. Properties of the optimization and its relaxations are studied. Numerical experiments are presented.},
  archive      = {J_OMS},
  author       = {Jiawang Nie and Liu Yang and Suhan Zhong},
  doi          = {10.1080/10556788.2019.1649672},
  journal      = {Optimization Methods and Software},
  number       = {2},
  pages        = {329-347},
  shortjournal = {Optim. Methods Softw.},
  title        = {Stochastic polynomial optimization},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Error estimates for iterative algorithms for minimizing
regularized quadratic subproblems. <em>OMS</em>, <em>35</em>(2),
304‚Äì328. (<a
href="https://doi.org/10.1080/10556788.2019.1670177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive bounds for the objective errors and gradient residuals when finding approximations to the solution of common regularized quadratic optimization problems within evolving Krylov spaces. These provide upper bounds on the number of iterations required to achieve a given stated accuracy. We illustrate the quality of our bounds on given test examples.},
  archive      = {J_OMS},
  author       = {Nicholas I. M. Gould and Valeria Simoncini},
  doi          = {10.1080/10556788.2019.1670177},
  journal      = {Optimization Methods and Software},
  number       = {2},
  pages        = {304-328},
  shortjournal = {Optim. Methods Softw.},
  title        = {Error estimates for iterative algorithms for minimizing regularized quadratic subproblems},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ADMM for multiaffine constrained optimization. <em>OMS</em>,
<em>35</em>(2), 257‚Äì303. (<a
href="https://doi.org/10.1080/10556788.2019.1683553">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We expand the scope of the alternating direction method of multipliers (ADMM). Specifically, we show that ADMM, when employed to solve problems with multiaffine constraints that satisfy certain verifiable assumptions, converges to the set of constrained stationary points if the penalty parameter in the augmented Lagrangian is sufficiently large. When the Kurdyka‚Äì≈Åojasiewicz (K‚Äì≈Å) property holds, this is strengthened to convergence to a single constrained stationary point. Our analysis applies under assumptions that we have endeavoured to make as weak as possible. It applies to problems that involve nonconvex and/or nonsmooth objective terms, in addition to the multiaffine constraints that can involve multiple (three or more) blocks of variables. To illustrate the applicability of our results, we describe examples including nonnegative matrix factorization, sparse learning, risk parity portfolio selection, nonconvex formulations of convex problems and neural network training. In each case, our ADMM approach encounters only subproblems that have closed-form solutions.},
  archive      = {J_OMS},
  author       = {Wenbo Gao and Donald Goldfarb and Frank E. Curtis},
  doi          = {10.1080/10556788.2019.1683553},
  journal      = {Optimization Methods and Software},
  number       = {2},
  pages        = {257-303},
  shortjournal = {Optim. Methods Softw.},
  title        = {ADMM for multiaffine constrained optimization},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A concise second-order complexity analysis for unconstrained
optimization using high-order regularized models. <em>OMS</em>,
<em>35</em>(2), 243‚Äì256. (<a
href="https://doi.org/10.1080/10556788.2019.1678033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An adaptive regularization algorithm is proposed that uses Taylor models of the objective of order p , p ‚â• 2 p ‚â• 2 p‚â•2 , of the unconstrained objective function, and that is guaranteed to find a first- and second-order critical point in at most O ( max { œµ ‚àí p + 1 p 1 , œµ ‚àí p + 1 p ‚àí 1 2 } ) O ( max { œµ 1 ‚àí p + 1 p , œµ 2 ‚àí p + 1 p ‚àí 1 } ) O(max{œµ1‚àíp+1p,œµ2‚àíp+1p‚àí1}) function and derivatives evaluations, where œµ 1 œµ 1 œµ1 and œµ 2 œµ 2 œµ2 are prescribed first- and second-order optimality tolerances. This is a simple algorithm and associated analysis compared to the much more general approach in Cartis et al. [ Sharp worst-case evaluation complexity bounds for arbitrary-order nonconvex optimization with inexpensive constraints , arXiv:1811.01220, 2018] that addresses the complexity of criticality higher-than two; here, we use standard optimality conditions and practical subproblem solves to show a same-order sharp complexity bound for second-order criticality. Our approach also extends the method in Birgin et¬†al. [ Worst-case evaluation complexity for unconstrained nonlinear optimization using high-order regularized models , Math. Prog. A 163(1) (2017), pp. 359‚Äì368] to finding second-order critical points, under the same problem smoothness assumptions as were needed for first-order complexity.},
  archive      = {J_OMS},
  author       = {C. Cartis and N. I. M. Gould and Ph. L. Toint},
  doi          = {10.1080/10556788.2019.1678033},
  journal      = {Optimization Methods and Software},
  number       = {2},
  pages        = {243-256},
  shortjournal = {Optim. Methods Softw.},
  title        = {A concise second-order complexity analysis for unconstrained optimization using high-order regularized models},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysis of the gradient method with an armijo‚Äìwolfe line
search on a class of non-smooth convex functions. <em>OMS</em>,
<em>35</em>(2), 223‚Äì242. (<a
href="https://doi.org/10.1080/10556788.2019.1673388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has long been known that the gradient (steepest descent) method may fail on non-smooth problems, but the examples that have appeared in the literature are either devised specifically to defeat a gradient or subgradient method with an exact line search or are unstable with respect to perturbation of the initial point. We give an analysis of the gradient method with steplengths satisfying the Armijo and Wolfe inexact line search conditions on the non-smooth convex function f ( x ) = a | x ( 1 ) | + ‚àë i = 2 n x ( i ) . We show that if a is sufficiently large, satisfying a condition that depends only on the Armijo parameter, then, when the method is initiated at any point x 0 ‚àà R n with x 0 ( 1 ) ‚â† 0 , the iterates converge to a point x ¬Ø with x ¬Ø ( 1 ) = 0 , although f is unbounded below. We also give conditions under which the iterates f ( x k ) ‚Üí ‚àí ‚àû , using a specific Armijo‚ÄìWolfe bracketing line search. Our experimental results demonstrate that our analysis is reasonably tight.},
  archive      = {J_OMS},
  author       = {Azam Asl and Michael L. Overton},
  doi          = {10.1080/10556788.2019.1673388},
  journal      = {Optimization Methods and Software},
  number       = {2},
  pages        = {223-242},
  shortjournal = {Optim. Methods Softw.},
  title        = {Analysis of the gradient method with an Armijo‚ÄìWolfe line search on a class of non-smooth convex functions},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Preface. <em>OMS</em>, <em>35</em>(2), 221‚Äì222. (<a
href="https://doi.org/10.1080/10556788.2019.1698151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_OMS},
  author       = {Yu-Hong Dai and Xin Liu and Jiawang Nie and Zaiwen Wen},
  doi          = {10.1080/10556788.2019.1698151},
  journal      = {Optimization Methods and Software},
  number       = {2},
  pages        = {221-222},
  shortjournal = {Optim. Methods Softw.},
  title        = {Preface},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A robust multi-batch l-BFGS method for machine learning*.
<em>OMS</em>, <em>35</em>(1), 191‚Äì219. (<a
href="https://doi.org/10.1080/10556788.2019.1658107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes an implementation of the L-BFGS method designed to deal with two adversarial situations. The first occurs in distributed computing environments where some of the computational nodes devoted to the evaluation of the function and gradient are unable to return results on time. A similar challenge occurs in a multi-batch approach in which the data points used to compute function and gradients are purposely changed at each iteration to accelerate the learning process. Difficulties arise because L-BFGS employs gradient differences to update the Hessian approximations, and when these gradients are computed using different data points the updating process can be unstable. This paper shows how to perform stable quasi-Newton updating in the multi-batch setting, studies the convergence properties for both convex and non-convex functions, and illustrates the behaviour of the algorithm in a distributed computing platform on binary classification logistic regression and neural network training problems that arise in machine learning.},
  archive      = {J_OMS},
  author       = {Albert S. Berahas and Martin Tak√°ƒç},
  doi          = {10.1080/10556788.2019.1658107},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {191-219},
  shortjournal = {Optim. Methods Softw.},
  title        = {A robust multi-batch L-BFGS method for machine learning*},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving the performance of DICOPT in convex MINLP problems
using a feasibility pump. <em>OMS</em>, <em>35</em>(1), 171‚Äì190. (<a
href="https://doi.org/10.1080/10556788.2019.1641498">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The solver DICOPT is based on the outer-approximation algorithm used for solving mixed-integer nonlinear programming (MINLP) problems. This algorithm is very effective for solving some types of convex MINLPs. However, it has been observed that DICOPT has difficulties solving instances in which some of the nonlinear constraints are so restrictive that nonlinear subproblems generated by the algorithm are infeasible. This problem is addressed in this paper with a feasibility pump algorithm, which modifies the objective function in order to efficiently find feasible solutions. It has been implemented as a preprocessing algorithm, which is used to initialize both the incumbent and the mixed-integer linear relaxation of the outer-approximation algorithm. Computational comparisons with previous versions of DICOPT on a set of convex MINLPs demonstrate the effectiveness of the proposed algorithm in terms of solution quality and solution time.},
  archive      = {J_OMS},
  author       = {David E. Bernal and Stefan Vigerske and Francisco Trespalacios and Ignacio E. Grossmann},
  doi          = {10.1080/10556788.2019.1641498},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {171-190},
  shortjournal = {Optim. Methods Softw.},
  title        = {Improving the performance of DICOPT in convex MINLP problems using a feasibility pump},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal control problems with control complementarity
constraints: Existence results, optimality conditions, and a penalty
method. <em>OMS</em>, <em>35</em>(1), 142‚Äì170. (<a
href="https://doi.org/10.1080/10556788.2019.1604705">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A special class of optimal control problems with complementarity constraints on the control functions is studied. It is shown that such problems possess optimal solutions whenever the underlying control space is a first-order Sobolev space. After deriving necessary optimality conditions of strong stationarity-type, a penalty method based on the Fischer‚ÄìBurmeister function is suggested and its theoretical properties are analyzed. Finally, the numerical treatment of the problem is discussed and results of computational experiments are presented.},
  archive      = {J_OMS},
  author       = {Christian Clason and Yu Deng and Patrick Mehlitz and Uwe Pr√ºfert},
  doi          = {10.1080/10556788.2019.1604705},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {142-170},
  shortjournal = {Optim. Methods Softw.},
  title        = {Optimal control problems with control complementarity constraints: Existence results, optimality conditions, and a penalty method},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parallel-batching machines scheduling problem with a
truncated time-dependent learning effect via a hybrid CS-JADE algorithm.
<em>OMS</em>, <em>35</em>(1), 116‚Äì141. (<a
href="https://doi.org/10.1080/10556788.2019.1577415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This research investigates the parallel-batching scheduling problems with a time-dependent learning effect where the job processing time is a decreasing function of its starting time. Both the single-machine and parallel-machine circumstances are considered, and the objective is to minimize the makespan. For the single parallel-batching machine scheduling problem, some structural properties and a heuristic algorithm are developed to solve it. Since the parallel-batching machines scheduling problem is NP-hard, a hybrid CS-JADE algorithm combining improved cuckoo search algorithm (CS) and self-adaptive differential evolution (DE) is proposed to solve this problem. The computational experiments indicate that the proposed hybrid algorithm performs well both in effectiveness and efficiency.},
  archive      = {J_OMS},
  author       = {Siwen Liu and Xinbao Liu and Jun Pei and Panos M. Pardalos and Qingru Song},
  doi          = {10.1080/10556788.2019.1577415},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {116-141},
  shortjournal = {Optim. Methods Softw.},
  title        = {Parallel-batching machines scheduling problem with a truncated time-dependent learning effect via a hybrid CS-JADE algorithm},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SDPNAL+: A matlab software for semidefinite programming with
bound constraints (version 1.0). <em>OMS</em>, <em>35</em>(1), 87‚Äì115.
(<a href="https://doi.org/10.1080/10556788.2019.1576176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {S dpnal + is a M ATLAB software package that implements an augmented Lagrangian based method to solve large scale semidefinite programming problems with bound constraints. The implementation was initially based on a majorized semismooth Newton-CG augmented Lagrangian method, here we designed it within an inexact symmetric Gauss-Seidel based semi-proximal ADMM/ALM (alternating direction method of multipliers/augmented Lagrangian method) framework for the purpose of deriving simpler stopping conditions and closing the gap between the practical implementation of the algorithm and the theoretical algorithm. The basic code is written in M ATLAB , but some subroutines in C language are incorporated via Mex files. We also design a convenient interface for users to input their SDP models into the solver. Numerous problems arising from combinatorial optimization and binary integer quadratic programming problems have been tested to evaluate the performance of the solver. Extensive numerical experiments conducted in [L.Q. Yang, D.F. Sun, and K.C. Toh, SDPNAL+: A majorized semismooth Newton-CG augmented Lagrangian method for semidefinite programming with nonnegative constraints , Math. Program. Comput. 7 (2015), pp. 331‚Äì366] show that the proposed method is quite efficient and robust, in that it is able to solve 98.9\% of the 745 test instances of SDP problems arising from various applications to the accuracy of 10 ‚àí 6 in the relative KKT residual.},
  archive      = {J_OMS},
  author       = {Defeng Sun and Kim-Chuan Toh and Yancheng Yuan and Xin-Yuan Zhao},
  doi          = {10.1080/10556788.2019.1576176},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {87-115},
  shortjournal = {Optim. Methods Softw.},
  title        = {SDPNAL+: A matlab software for semidefinite programming with bound constraints (version 1.0)},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the inexact symmetrized globally convergent semi-smooth
newton method for 3D contact problems with tresca friction: The r-linear
convergence rate. <em>OMS</em>, <em>35</em>(1), 65‚Äì86. (<a
href="https://doi.org/10.1080/10556788.2018.1556659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The semi-smooth Newton method for solving discretized contact problems with Tresca friction in three-dimensional space is analysed. The slanting function is approximated to get symmetric inner linear systems. The primal‚Äìdual algorithm is transformed into the dual one so that the conjugate gradient method can be used. The R-linear convergence rate is proved for an inexact globally convergent variant of the method. Numerical experiments conclude the paper.},
  archive      = {J_OMS},
  author       = {R. Kuƒçera and K. Motyƒçkov√° and A. Markopoulos and J. Haslinger},
  doi          = {10.1080/10556788.2018.1556659},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {65-86},
  shortjournal = {Optim. Methods Softw.},
  title        = {On the inexact symmetrized globally convergent semi-smooth newton method for 3D contact problems with tresca friction: The R-linear convergence rate},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Solving mixed-integer nonlinear programmes using adaptively
refined mixed-integer linear programmes. <em>OMS</em>, <em>35</em>(1),
37‚Äì64. (<a href="https://doi.org/10.1080/10556788.2018.1556661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a method for solving mixed-integer nonlinear programmes (MINLPs) to global optimality by discretization of occurring nonlinearities. The main idea is based on using piecewise linear functions to construct mixed-integer linear programme (MIP) relaxations of the underlying MINLP. In order to find a global optimum of the given MINLP, we develop an iterative algorithm which solves MIP relaxations that are adaptively refined. We are able to give convergence results for a wide range of MINLPs requiring only continuous nonlinearities with bounded domains and an oracle computing maxima of the nonlinearities on their domain. Moreover, the practicalness of our approach is shown numerically by an application from the field of gas network optimization.},
  archive      = {J_OMS},
  author       = {Robert Burlacu and Bj√∂rn Gei√üler and Lars Schewe},
  doi          = {10.1080/10556788.2018.1556661},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {37-64},
  shortjournal = {Optim. Methods Softw.},
  title        = {Solving mixed-integer nonlinear programmes using adaptively refined mixed-integer linear programmes},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Convex feasibility problems on uniformly convex metric
spaces. <em>OMS</em>, <em>35</em>(1), 21‚Äì36. (<a
href="https://doi.org/10.1080/10556788.2018.1553970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we extend two important notions, weighted average method and Mann&#39;s iterative method, in the study of convex feasibility problem for general maps defined on p -uniformly convex metric spaces. Then we prove the Œî-convergence theorems for the weighted average sequence and the Mann&#39;s alternating sequence in p -uniformly convex metric spaces.},
  archive      = {J_OMS},
  author       = {Byoung Jin Choi and Un Cig Ji and Yongdo Lim},
  doi          = {10.1080/10556788.2018.1553970},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {21-36},
  shortjournal = {Optim. Methods Softw.},
  title        = {Convex feasibility problems on uniformly convex metric spaces},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient exact approach for the constrained shortest
path tour problem. <em>OMS</em>, <em>35</em>(1), 1‚Äì20. (<a
href="https://doi.org/10.1080/10556788.2018.1548015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a directed graph with non-negative arc lengths, the Constrained Shortest Path Tour Problem ( ùíû ùíÆ ùí´ ùíØ ùí´ C S P T P CSPTP ) is aimed at finding a shortest path from a single-origin to a single-destination, such that a sequence of disjoint and possibly different-sized node subsets are crossed in a given fixed order. Moreover, the optimal path must not include repeated arcs. In this paper, for the ùíû ùíÆ ùí´ ùíØ ùí´ C S P T P CSPTP we propose a new mathematical model and a new efficient Branch &amp; Bound method. Extensive computational experiments have been carried out on a significant set of test problems in order to evaluate empirically the performance of the proposed approach.},
  archive      = {J_OMS},
  author       = {Daniele Ferone and Paola Festa and Francesca Guerriero},
  doi          = {10.1080/10556788.2018.1548015},
  journal      = {Optimization Methods and Software},
  number       = {1},
  pages        = {1-20},
  shortjournal = {Optim. Methods Softw.},
  title        = {An efficient exact approach for the constrained shortest path tour problem},
  volume       = {35},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
