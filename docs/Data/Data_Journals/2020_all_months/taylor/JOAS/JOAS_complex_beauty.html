<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JOAS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="joas---162">JOAS - 162</h2>
<ul>
<li><details>
<summary>
(2020). On bias reduction estimators of skew-normal and skew-t
distributions. <em>JOAS</em>, <em>47</em>(16), 3030–3052. (<a
href="https://doi.org/10.1080/02664763.2019.1710114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A particular concerns of researchers in statistical inference is bias in parameters estimation. Maximum likelihood estimators are often biased and for small sample size, the first order bias of them can be large and so it may influence the efficiency of the estimator. There are different methods for reduction of this bias. In this paper, we proposed a modified maximum likelihood estimator for the shape parameter of two popular skew distributions, namely skew-normal and skew-t, by offering a new method. We show that this estimator has lower asymptotic bias than the maximum likelihood estimator and is more efficient than those based on the existing methods.},
  archive      = {J_JOAS},
  author       = {Mohammad Mahdi Maghami and Mohammad Bahrami and Farkhondeh Alsadat Sajadi},
  doi          = {10.1080/02664763.2019.1710114},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {3030-3052},
  shortjournal = {J. Appl. Stat.},
  title        = {On bias reduction estimators of skew-normal and skew-t distributions},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A skew factor analysis model based on the normal
mean–variance mixture of birnbaum–saunders distribution. <em>JOAS</em>,
<em>47</em>(16), 3007–3029. (<a
href="https://doi.org/10.1080/02664763.2019.1709054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a robust extension of factor analysis model by assuming the multivariate normal mean–variance mixture of Birnbaum–Saunders distribution for the unobservable factors and errors. A computationally analytical EM-based algorithm is developed to find maximum likelihood estimates of the parameters. The asymptotic standard errors of parameter estimates are derived under an information-based paradigm. Numerical merits of the proposed methodology are illustrated using both simulated and real datasets.},
  archive      = {J_JOAS},
  author       = {Farzane Hashemi and Mehrdad Naderi and Ahad Jamalizadeh and Tsung-I Lin},
  doi          = {10.1080/02664763.2019.1709054},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {3007-3029},
  shortjournal = {J. Appl. Stat.},
  title        = {A skew factor analysis model based on the normal mean–variance mixture of Birnbaum–Saunders distribution},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian rank-based hypothesis testing for the rank sum
test, the signed rank test, and spearman’s ρ. <em>JOAS</em>,
<em>47</em>(16), 2984–3006. (<a
href="https://doi.org/10.1080/02664763.2019.1709053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian inference for rank-order problems is frustrated by the absence of an explicit likelihood function. This hurdle can be overcome by assuming a latent normal representation that is consistent with the ordinal information in the data: the observed ranks are conceptualized as an impoverished reflection of an underlying continuous scale, and inference concerns the parameters that govern the latent representation. We apply this generic data-augmentation method to obtain Bayes factors for three popular rank-based tests: the rank sum test, the signed rank test, and Spearman&#39;s ρ s .},
  archive      = {J_JOAS},
  author       = {J. van Doorn and A. Ly and M. Marsman and E.-J. Wagenmakers},
  doi          = {10.1080/02664763.2019.1709053},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {2984-3006},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian rank-based hypothesis testing for the rank sum test, the signed rank test, and spearman&#39;s ρ},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analyzing and clustering students’ application preferences
in higher education. <em>JOAS</em>, <em>47</em>(16), 2961–2983. (<a
href="https://doi.org/10.1080/02664763.2019.1709052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a framework based on a higher education application preference list that allows a different type of flexible aggregation and, hence, the analysis and clustering of application data. Preference lists are converted into scores. The proposed approach is demonstrated in the context of higher education applications in Hungary over the period of 2006–2015. Our method reveals that efforts to leverage center-periphery differences do not fulfill expectations. Furthermore, the student&#39;s top preference is very hard to influence, and recruiters may build their strategy on information about the first and second choices.},
  archive      = {J_JOAS},
  author       = {Zs. T. Kosztyán and É. Orbán-Mihálykó and Cs. Mihálykó and V. V. Csányi and A. Telcs},
  doi          = {10.1080/02664763.2019.1709052},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {2961-2983},
  shortjournal = {J. Appl. Stat.},
  title        = {Analyzing and clustering students&#39; application preferences in higher education},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variable selection in finite mixture of regression models
using the skew-normal distribution. <em>JOAS</em>, <em>47</em>(16),
2941–2960. (<a
href="https://doi.org/10.1080/02664763.2019.1709051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variable selection in finite mixture of regression (FMR) models is frequently used in statistical modeling. The majority of applications of variable selection in FMR models use a normal distribution for regression error. Such assumptions are unsuitable for a set of data containing a group or groups of observations with asymmetric behavior. In this paper, we introduce a variable selection procedure for FMR models using the skew-normal distribution. With appropriate choice of the tuning parameters, we establish the theoretical properties of our procedure, including consistency in variable selection and the oracle property in estimation. To estimate the parameters of the model, a modified EM algorithm for numerical computations is developed. The methodology is illustrated through numerical experiments and a real data example.},
  archive      = {J_JOAS},
  author       = {Junhui Yin and Liucang Wu and Lin Dai},
  doi          = {10.1080/02664763.2019.1709051},
  journal      = {Journal of Applied Statistics},
  number       = {16},
  pages        = {2941-2960},
  shortjournal = {J. Appl. Stat.},
  title        = {Variable selection in finite mixture of regression models using the skew-normal distribution},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New developments in the forecasting of monthly overnight
stays in the north region of portugal. <em>JOAS</em>,
<em>47</em>(13-15), 2927–2940. (<a
href="https://doi.org/10.1080/02664763.2020.1795812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Tourism sector is of strategic importance to the North Region of Portugal and is growing. Forecasting monthly overnight stays in this region is, therefore, a relevant problem. In this paper, we analyze data more recent than those considered in previous studies and use them to develop and compare several forecasting models and methods. We conclude that the best results are achieved by models based on a non-parametric approach not considered so far for these data, the singular spectrum analysis.},
  archive      = {J_JOAS},
  author       = {Isabel Silva and Hugo Alonso},
  doi          = {10.1080/02664763.2020.1795812},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2927-2940},
  shortjournal = {J. Appl. Stat.},
  title        = {New developments in the forecasting of monthly overnight stays in the north region of portugal},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mixed-effects modelling for crossed and nested data: An
analysis of dengue fever in the state of goiás, brazil. <em>JOAS</em>,
<em>47</em>(13-15), 2912–2926. (<a
href="https://doi.org/10.1080/02664763.2020.1736528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dengue fever is a viral disease transmitted by the mosquito Aedes aegypti . In order to avoid epidemics and deaths, this transmitting vector must be controlled. This work assembles, for the first time, data from multiple governmental bodies describing the number of dengue cases reported, and meteorological conditions in 20 cities in the Goiás state, Brazil, from 2008 to 2015. We then apply generalised linear mixed modelling to this novel data set to model dengue occurrences in this state, where the tropical climate favours the proliferation of the main transmitting vector of this disease. The number of reported dengue cases is estimated using meteorological variables as fixed effects, and city and year data are included in the model as random effects. The proposed models can cope with complex data structures, such as nested data, while taking into account the particularities of each year dependent on the city under analysis. The results confirm that precipitation, minimum temperature, and relative air humidity contribute to the increase of dengue cases number, while year and city location are determining factors. Public policies, based on these new results, together with joint actions involving local populations, are essential to combat the vector transmitting dengue and avoid epidemics.},
  archive      = {J_JOAS},
  author       = {A. N. Oliveira and R. Menezes and S. Faria and P. Afonso},
  doi          = {10.1080/02664763.2020.1736528},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2912-2926},
  shortjournal = {J. Appl. Stat.},
  title        = {Mixed-effects modelling for crossed and nested data: An analysis of dengue fever in the state of goiás, brazil},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On rank distribution classifiers for high-dimensional data.
<em>JOAS</em>, <em>47</em>(13-15), 2895–2911. (<a
href="https://doi.org/10.1080/02664763.2020.1768227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial sign and rank-based methods have been studied in the recent literature, especially when the dimension is smaller than the sample size. In this paper, a classification method based on the distribution of rank functions for high-dimensional data is considered with extension to functional data. The method is fully nonparametric in nature. The performance of the classification method is illustrated in comparison with some other classifiers using simulated and real data sets. Supporting code in R are provided for computational implementation of the classification method that will be of use to others.},
  archive      = {J_JOAS},
  author       = {Olusola Samuel Makinde},
  doi          = {10.1080/02664763.2020.1768227},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2895-2911},
  shortjournal = {J. Appl. Stat.},
  title        = {On rank distribution classifiers for high-dimensional data},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A logistic regression model for consumer default risk.
<em>JOAS</em>, <em>47</em>(13-15), 2879–2894. (<a
href="https://doi.org/10.1080/02664763.2020.1759030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, a logistic regression model is applied to credit scoring data from a given Portuguese financial institution to evaluate the default risk of consumer loans. It was found that the risk of default increases with the loan spread, loan term and age of the customer, but decreases if the customer owns more credit cards. Clients receiving the salary in the same banking institution of the loan have less chances of default than clients receiving their salary in another institution. We also found that clients in the lowest income tax echelon have more propensity to default. The model predicted default correctly in 89.79\% of the cases.},
  archive      = {J_JOAS},
  author       = {Eliana Costa e Silva and Isabel Cristina Lopes and Aldina Correia and Susana Faria},
  doi          = {10.1080/02664763.2020.1759030},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2879-2894},
  shortjournal = {J. Appl. Stat.},
  title        = {A logistic regression model for consumer default risk},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Models induced from critical birth–death process with random
initial conditions. <em>JOAS</em>, <em>47</em>(13-15), 2862–2878. (<a
href="https://doi.org/10.1080/02664763.2020.1732309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we study a linear birth–death process starting from random initial conditions. First, we consider these initial conditions as a random number of particles following different standard probabilistic distributions – Negative-Binomial and its closest Geometric, Poisson or Pólya–Aeppli distributions. It is proved analytically and numerically that in these cases the random number of particles alive at any positive time follows the same probability law like the initial condition, but with different parameters depending on time. The random initial conditions cannot change the critical parameter of branching mechanism, but they impact the extinction probability. Finally, the numerical model is extended to an application for studying branching processes with more complex initial conditions. This is demonstrated with a linear birth–death process initialised with Pólya urn sampling scheme. The obtained preliminary results for particle distribution show close relation to Pólya–Aeppli distribution.},
  archive      = {J_JOAS},
  author       = {A. Tchorbadjieff and P. Mayster},
  doi          = {10.1080/02664763.2020.1732309},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2862-2878},
  shortjournal = {J. Appl. Stat.},
  title        = {Models induced from critical birth–death process with random initial conditions},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extremal index blocks estimator: The threshold and the block
size choice. <em>JOAS</em>, <em>47</em>(13-15), 2846–2861. (<a
href="https://doi.org/10.1080/02664763.2020.1720626">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main objective of Statistics of Extremes is the estimation of probabilities of rare events. When extending the analysis of the limiting behaviour of the extreme values from independent and identically distributed sequences to stationary sequences a key parameter appears, the extremal index θ , whose accurate estimation is not easy. Here we focus on the estimation of θ using blocks estimators, that can be constructed by using disjoint or sliding blocks. The asymptotic properties for both procedures were studied and compared but both blocks estimators require the choice of a threshold and a block length. Some criteria have appeared for the choice of those nuisance quantities but some research is still needed. We will show how the threshold and the block size choices can affect the estimates. However the main objective of this work is to revisit another estimation procedure that only depends on the block length, although some conditions on the underlying process need to be verified. The associated estimator presents nice asymptotic properties, and for finite samples is here illustrated a stability criterion for choosing the block length and then obtaining the θ estimate. A large simulation study has been performed and an application to daily mean flow discharge rate in the hydrometric station of Fragas da Torre in river Paiva, data collected from 1 October 1946 to 30 April 2012 is done.},
  archive      = {J_JOAS},
  author       = {D. Prata Gomes and M. Manuela Neves},
  doi          = {10.1080/02664763.2020.1720626},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2846-2861},
  shortjournal = {J. Appl. Stat.},
  title        = {Extremal index blocks estimator: The threshold and the block size choice},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lehmer’s mean-of-order-p extreme value index estimation: A
simulation study and applications. <em>JOAS</em>, <em>47</em>(13-15),
2825–2845. (<a
href="https://doi.org/10.1080/02664763.2019.1694871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The main objective of extreme value theory is essentially the estimation of quantities related to extreme events. One of its main issues has been the estimation of the extreme value index (EVI), a parameter directly related to the tail weight of the distribution. Here we deal with the semi-parametric estimation of the EVI, for heavy tails. A recent class of EVI-estimators, based on the Lehmer&#39;s mean-of-order p (L p p p ), which generalizes the arithmetic mean, is considered. An asymptotic comparison at optimal levels performed in previous works has revealed the competitiveness of this class of EVI-estimators. A large-scale Monte-Carlo simulation study for finite simulated samples has been here performed, showing the behaviour of L p , as a function of p . A bootstrap adaptive choice of ( k , p ) , where k is the number of upper order statistics used in the estimation, and a second algorithm based on a stability criterion are computationally studied and applied to simulated and real data.},
  archive      = {J_JOAS},
  author       = {Helena Penalva and M. Ivette Gomes and Frederico Caeiro and M. Manuela Neves},
  doi          = {10.1080/02664763.2019.1694871},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2825-2845},
  shortjournal = {J. Appl. Stat.},
  title        = {Lehmer&#39;s mean-of-order-p extreme value index estimation: A simulation study and applications},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust inference under r-size-biased sampling without
replacement from finite population. <em>JOAS</em>, <em>47</em>(13-15),
2808–2824. (<a
href="https://doi.org/10.1080/02664763.2019.1711031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The case of size-biased sampling of known order from a finite population without replacement is considered. The behavior of such a sampling scheme is studied with respect to the sampling fraction. Based on a simulation study, it is concluded that such a sample cannot be treated either as a random sample from the parent distribution or as a random sample from the corresponding r-size weighted distribution and as the sampling fraction increases, the biasness in the sample decreases resulting in a transition from an r-size-biased sample to a random sample. A modified version of a likelihood-free method is adopted for making statistical inference for the unknown population parameters, as well as for the size of the population when it is unknown. A simulation study, which takes under consideration the sampling fraction, demonstrates that the proposed method presents better and more robust behavior compared to the approaches, which treat the r-size-biased sample either as a random sample from the parent distribution or as a random sample from the corresponding r-size weighted distribution. Finally, a numerical example which motivates this study illustrates our results.},
  archive      = {J_JOAS},
  author       = {P. Economou and G. Tzavelas and A. Batsidis},
  doi          = {10.1080/02664763.2019.1711031},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2808-2824},
  shortjournal = {J. Appl. Stat.},
  title        = {Robust inference under r-size-biased sampling without replacement from finite population},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new class of weighted bimodal distribution with
application to gamma-ray burst duration data. <em>JOAS</em>,
<em>47</em>(13-15), 2785–2807. (<a
href="https://doi.org/10.1080/02664763.2020.1815669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gamma-ray bursts (GRBs) have been confidently identified thus far and are prescribed to different physical scenarios, black hole mergers, and collapse of massive stars. The distribution of GRBs duration, which is one of the main characteristics of GRBs, is bimodal. Hence, many authors have used mixtures of distribution models to fit them, which suffers serious estimation problems either from classical or Bayesian approaches. Therefore, in this article we introduced a more flexible class of weighted bimodal distribution, called alpha two-piece skew normal (ATPSN), for modeling GRBs duration data set. Some of the main probabilistic and inferential properties of the distribution are discussed and a simulation study is carried out to illustrate the performance of the MLEs.},
  archive      = {J_JOAS},
  author       = {Najme Sharifipanah and Rahim Chinipardaz and Gholam Ali Parham},
  doi          = {10.1080/02664763.2020.1815669},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2785-2807},
  shortjournal = {J. Appl. Stat.},
  title        = {A new class of weighted bimodal distribution with application to gamma-ray burst duration data},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Testing simultaneously different covariance block diagonal
structures – the multi-sample case. <em>JOAS</em>, <em>47</em>(13-15),
2765–2784. (<a
href="https://doi.org/10.1080/02664763.2020.1712590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work a likelihood ratio test which allows to test simultaneously if, several covariance matrices are equal and block diagonal with different specific structures in the diagonal blocks, is developed. The distribution of the likelihood ratio statistic is studied and the expression of its h th null moment are derived. In order to make the test useful in practical terms, near-exact approximations are developed for the likelihood ratio statistic. A practical application to real data set together with numerical studies and simulations are provided in order illustrate the applicability of the test and also to assess the precision of the near-exact approximations developed.},
  archive      = {J_JOAS},
  author       = {F. J. Marques and C. A. Coelho},
  doi          = {10.1080/02664763.2020.1712590},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2765-2784},
  shortjournal = {J. Appl. Stat.},
  title        = {Testing simultaneously different covariance block diagonal structures – the multi-sample case},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computing the exact distribution of the bartlett’s test
statistic by numerical inversion of its characteristic function.
<em>JOAS</em>, <em>47</em>(13-15), 2749–2764. (<a
href="https://doi.org/10.1080/02664763.2019.1675608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Application of the exact statistical inference frequently leads to non-standard probability distributions of the considered estimators or test statistics. The exact distributions of many estimators and test statistics can be specified by their characteristic functions, as is the case for the null distribution of the Bartlett&#39;s test statistic. However, analytical inversion of the characteristic function, if possible, frequently leads to complicated expressions for computing the distribution function and the corresponding quantiles. An efficient alternative is the well-known method based on numerical inversion of the characteristic functions, which is, however, ignored in popular statistical software packages. In this paper, we present the explicit characteristic function of the corrected Bartlett&#39;s test statistic together with the computationally fast and efficient implementation of the approach based on numerical inversion of this characteristic function, suggested for evaluating the exact null distribution used for testing homogeneity of variances in several normal populations, with possibly unequal sample sizes.},
  archive      = {J_JOAS},
  author       = {Viktor Witkovský},
  doi          = {10.1080/02664763.2019.1675608},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2749-2764},
  shortjournal = {J. Appl. Stat.},
  title        = {Computing the exact distribution of the bartlett&#39;s test statistic by numerical inversion of its characteristic function},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Balanced prime basis factorial fixed effects model with
random number of observations. <em>JOAS</em>, <em>47</em>(13-15),
2737–2748. (<a
href="https://doi.org/10.1080/02664763.2019.1679097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Factorial designs are in general more efficient for experiments that involve the study of the effects of two or more factors. In this paper we consider a p U p U pU factorial model with U factors, each one having a p prime number of levels. We consider a balanced ( r replicates per treatment) prime factorial with fixed effects. Our goal is to extend these models to the case where it is not possible to known in advance the number of treatments replicates, r . In these situations is more appropriate to consider r as a realization of a random variable R , which will be assumed to be geometrically distributed. The proposed approach is illustrated through an application considering simulated data.},
  archive      = {J_JOAS},
  author       = {Sandra Oliveira and Célia Nunes and Elsa Moreira and Miguel Fonseca and João T. Mexia},
  doi          = {10.1080/02664763.2019.1679097},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2737-2748},
  shortjournal = {J. Appl. Stat.},
  title        = {Balanced prime basis factorial fixed effects model with random number of observations},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Assessing uncertainty of voter transitions estimated from
aggregated data. Application to the 2017 french presidential election.
<em>JOAS</em>, <em>47</em>(13-15), 2711–2736. (<a
href="https://doi.org/10.1080/02664763.2020.1804842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inferring electoral individual behaviour from aggregated data is a very active research area, with ramifications in sociology and political science. A new approach based on linear programming is proposed to estimate voter transitions among parties (or candidates) between two elections. Compared to other linear and quadratic programming models previously published, our approach presents two important innovations. Firstly, it explicitly deals with new entries and exits in the election census without assuming unrealistic hypotheses, enabling a reasonable estimation of vote behaviour of young electors voting for the first time. Secondly, by exploiting the information contained in the model residuals, we develop a procedure to assess the uncertainty in the estimates. This significantly distinguishes our model from other published mathematical programming methods. The method is illustrated estimating the vote transfer matrix between the first and second rounds of the 2017 French presidential election and measuring its level of uncertainty. Likewise, compared to the most current alternatives based on ecological regression, our approach is considerably simpler and faster, and has provided reasonable results in all the actual elections to which it has been applied. Interested scholars can easily use our procedure with the aid of the R-function provided in the Supplemental Material.},
  archive      = {J_JOAS},
  author       = {Rafael Romero and Jose M. Pavía and Jorge Martín and Gerardo Romero},
  doi          = {10.1080/02664763.2020.1804842},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2711-2736},
  shortjournal = {J. Appl. Stat.},
  title        = {Assessing uncertainty of voter transitions estimated from aggregated data. application to the 2017 french presidential election},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On a new type of birnbaum-saunders models and its inference
and application to fatigue data. <em>JOAS</em>, <em>47</em>(13-15),
2690–2710. (<a
href="https://doi.org/10.1080/02664763.2019.1668365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Birnbaum-Saunders distribution is a widely studied model with diverse applications. Its origins are in the modeling of lifetimes associated with material fatigue. By using a motivating example, we show that, even when lifetime data related to fatigue are modeled, the Birnbaum-Saunders distribution can be unsuitable to fit these data in the distribution tails. Based on the nice properties of the Birnbaum-Saunders model, in this work, we use a modified skew-normal distribution to construct such a model. This allows us to obtain flexibility in skewness and kurtosis, which is controlled by a shape parameter. We provide a mathematical characterization of this new type of Birnbaum-Saunders distribution and then its statistical characterization is derived by using the maximum-likelihood method, including the associated information matrices. In order to improve the inferential performance, we correct the bias of the corresponding estimators, which is supported by a simulation study. To conclude our investigation, we retake the motivating example based on fatigue life data to show the good agreement between the new type of Birnbaum-Saunders distribution proposed in this work and the data, reporting its potential applications.},
  archive      = {J_JOAS},
  author       = {Jaime Arrué and Reinaldo B. Arellano-Valle and Héctor W. Gómez and Víctor Leiva},
  doi          = {10.1080/02664763.2019.1668365},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2690-2710},
  shortjournal = {J. Appl. Stat.},
  title        = {On a new type of birnbaum-saunders models and its inference and application to fatigue data},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian estimation of subset threshold autoregressions:
Short-term forecasting of traffic occupancy. <em>JOAS</em>,
<em>47</em>(13-15), 2658–2689. (<a
href="https://doi.org/10.1080/02664763.2020.1801606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic management authorities in metropolitan areas use real-time systems that analyze high-frequency measurements from fixed sensors, to perform short-term forecasting and incident detection for various locations of a road network. Published research over the last 20 years focused primarily on modeling and forecasting of traffic volumes and speeds. Traffic occupancy approximates vehicular density through the percentage of time a sensor detects a vehicle within a pre-specified time interval. It exhibits weekly periodic patterns and heteroskedasticity and has been used as a metric for characterizing traffic regimes (e.g. free flow, congestion). This article presents a Bayesian three-step model building procedure for parsimonious estimation of Threshold-Autoregressive (TAR) models, designed for location- day- and horizon-specific forecasting of traffic occupancy. In the first step, multiple regime TAR models reformulated as high-dimensional linear regressions are estimated using Bayesian horseshoe priors. Next, significant regimes are identified through a forward selection algorithm based on Kullback-Leibler (KL) distances between the posterior predictive distribution of the full reference model and TAR models with fewer regimes. Given the regimes, the forward selection algorithm can be implemented again to select significant autoregressive terms. In addition to forecasting, the proposed specification and model-building scheme, may assist in determining location-specific congestion thresholds and associations between traffic dynamics observed in different regions of a network. Empirical results applied to data from a traffic forecasting competition, illustrate the efficacy of the proposed procedures in obtaining interpretable models and in producing satisfactory point and density forecasts at multiple horizons.},
  archive      = {J_JOAS},
  author       = {Mario Giacomazzo and Yiannis Kamarianakis},
  doi          = {10.1080/02664763.2020.1801606},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2658-2689},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian estimation of subset threshold autoregressions: Short-term forecasting of traffic occupancy},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Considering the sample sizes as truncated poisson random
variables in mixed effects models. <em>JOAS</em>, <em>47</em>(13-15),
2641–2657. (<a
href="https://doi.org/10.1080/02664763.2019.1641188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When applying analysis of variance, the sample sizes may not be previously known, so it is more appropriate to consider them as realizations of random variables. A motivating example is the collection of observations during a fixed time span in a study comparing, for example, several pathologies of patients arriving at a hospital. This paper extends the theory of analysis of variance to those situations considering mixed effects models. We will assume that the occurrences of observations correspond to a counting process and the sample dimensions have Poisson distribution. The proposed approach is applied to a study of cancer patients.},
  archive      = {J_JOAS},
  author       = {Célia Nunes and Elsa Moreira and Sandra S. Ferreira and Dário Ferreira and João T. Mexia},
  doi          = {10.1080/02664763.2019.1641188},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2641-2657},
  shortjournal = {J. Appl. Stat.},
  title        = {Considering the sample sizes as truncated poisson random variables in mixed effects models},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The linearized alternating direction method of multipliers
for low-rank and fused LASSO matrix regression model. <em>JOAS</em>,
<em>47</em>(13-15), 2623–2640. (<a
href="https://doi.org/10.1080/02664763.2020.1742296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Datasets with matrix and vector form are increasingly popular in modern scientific fields. Based on structures of datasets, matrix and vector coefficients need to be estimated. At present, the matrix regression models were proposed, and they mainly focused on the matrix without vector variables. In order to fully explore complex structures of datasets, we propose a novel matrix regression model which combines fused LASSO and nuclear norm penalty, which can deal with the data containing matrix and vector variables meanwhile. Our main work is to design an efficient algorithm to solve the proposed low-rank and fused LASSO matrix regression model. Following the existing idea, we design the linearized alternating direction method of multipliers and establish its global convergence. Finally, we carry out numerical experiments to demonstrate the efficiency of our method. Especially, we apply our model to two real datasets, i.e. the signal shapes and the trip time prediction from partial trajectories.},
  archive      = {J_JOAS},
  author       = {M. Li and Q. Guo and W. J. Zhai and B. Z. Chen},
  doi          = {10.1080/02664763.2020.1742296},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2623-2640},
  shortjournal = {J. Appl. Stat.},
  title        = {The linearized alternating direction method of multipliers for low-rank and fused LASSO matrix regression model},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Goodness-of-fit tests for the logistic location family.
<em>JOAS</em>, <em>47</em>(13-15), 2610–2622. (<a
href="https://doi.org/10.1080/02664763.2020.1761952">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We construct two U -empirical tests for the logistic location family which are based on appropriate characterization of this family using independent exponential shifts. We study the limiting distributions and local Bahadur efficiency of corresponding test statistics under close alternatives. It turns out that the present tests are considerably more efficient than the recently proposed similar tests based on another characterization. The efficiency calculations are accompanied by the simulation of power for new tests together with the previous ones.Both efficiency and power turn out to be very high. Finally we consider the application of our tests to real data example.},
  archive      = {J_JOAS},
  author       = {Ya. Yu. Nikitin and I. A. Ragozin},
  doi          = {10.1080/02664763.2020.1761952},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2610-2622},
  shortjournal = {J. Appl. Stat.},
  title        = {Goodness-of-fit tests for the logistic location family},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reconceptualizing the p-value from a likelihood ratio test:
A probabilistic pairwise comparison of models based on kullback-leibler
discrepancy measures. <em>JOAS</em>, <em>47</em>(13-15), 2582–2609. (<a
href="https://doi.org/10.1080/02664763.2020.1754360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discrepancy measures are often employed in problems involving the selection and assessment of statistical models. A discrepancy gauges the separation between a fitted candidate model and the underlying generating model. In this work, we consider pairwise comparisons of fitted models based on a probabilistic evaluation of the ordering of the constituent discrepancies. An estimator of the probability is derived using the bootstrap. In the framework of hypothesis testing, nested models are often compared on the basis of the p -value. Specifically, the simpler null model is favored unless the p -value is sufficiently small, in which case the null model is rejected and the more general alternative model is retained. Using suitably defined discrepancy measures, we mathematically show that, in general settings, the likelihood ratio test p -value is approximated by the bootstrapped discrepancy comparison probability (BDCP). We argue that the connection between the p -value and the BDCP leads to potentially new insights regarding the utility and limitations of the p -value. The BDCP framework also facilitates discrepancy-based inferences in settings beyond the limited confines of nested model hypothesis testing.},
  archive      = {J_JOAS},
  author       = {Benjamin Riedle and Andrew A. Neath and Joseph E. Cavanaugh},
  doi          = {10.1080/02664763.2020.1754360},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2582-2609},
  shortjournal = {J. Appl. Stat.},
  title        = {Reconceptualizing the p-value from a likelihood ratio test: A probabilistic pairwise comparison of models based on kullback-leibler discrepancy measures},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Controlling the error probabilities of model selection
information criteria using bootstrapping. <em>JOAS</em>,
<em>47</em>(13-15), 2565–2581. (<a
href="https://doi.org/10.1080/02664763.2019.1701636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Akaike Information Criterion (AIC) and related information criteria are powerful and increasingly popular tools for comparing multiple, non-nested models without the specification of a null model. However, existing procedures for information-theoretic model selection do not provide explicit and uniform control over error rates for the choice between models, a key feature of classical hypothesis testing. We show how to extend notions of Type-I and Type-II error to more than two models without requiring a null. We then present the Error Control for Information Criteria (ECIC) method, a bootstrap approach to controlling Type-I error using Difference of Goodness of Fit (DGOF) distributions. We apply ECIC to empirical and simulated data in time series and regression contexts to illustrate its value for parametric Neyman–Pearson classification. An R package implementing the bootstrap method is publicly available.},
  archive      = {J_JOAS},
  author       = {Michael Cullan and Scott Lidgard and Beckett Sterner},
  doi          = {10.1080/02664763.2019.1701636},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2565-2581},
  shortjournal = {J. Appl. Stat.},
  title        = {Controlling the error probabilities of model selection information criteria using bootstrapping},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inference for bivariate integer-valued moving average models
based on binomial thinning operation. <em>JOAS</em>, <em>47</em>(13-15),
2546–2564. (<a
href="https://doi.org/10.1080/02664763.2020.1747411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series of (small) counts are common in practice and appear in a wide variety of fields. In the last three decades, several models that explicitly account for the discreteness of the data have been proposed in the literature. However, for multivariate time series of counts several difficulties arise and the literature is not so detailed. This work considers Bivariate INteger-valued Moving Average, BINMA, models based on the binomial thinning operation. The main probabilistic and statistical properties of BINMA models are studied. Two parametric cases are analysed, one with the cross-correlation generated through a Bivariate Poisson innovation process and another with a Bivariate Negative Binomial innovation process. Moreover, parameter estimation is carried out by the Generalized Method of Moments. The performance of the model is illustrated with synthetic data as well as with real datasets.},
  archive      = {J_JOAS},
  author       = {Isabel Silva and Maria Eduarda Silva and Cristina Torres},
  doi          = {10.1080/02664763.2020.1747411},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2546-2564},
  shortjournal = {J. Appl. Stat.},
  title        = {Inference for bivariate integer-valued moving average models based on binomial thinning operation},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variability and singularity arising from a
piecewise-deterministic markov process applied to model poor patient
compliance in the multi-IV case. <em>JOAS</em>, <em>47</em>(13-15),
2525–2545. (<a
href="https://doi.org/10.1080/02664763.2019.1711030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a Piecewise-Deterministic Markov Process (PDMP) to model the drug concentration in the case of multiple intravenous-bolus (multi-IV) doses and poor patient adherence situation: the scheduled time and doses of drug administration are not respected by the patient, the drug administration considers switching regime with random drug intake times. We study the randomness of drug concentration and derive probability results on the stochastic dynamics using the PDMP theory, focusing on two aspects of practical relevance: the variability of the concentration and the regularity of its stationary probability distribution. The main result show as the regularity of the concentration is governed by a parameter, which quantifies in a precise way the situations where drug intake times are too scarce concerning the elimination rate. Our approach is novel for the study of the regularity of the stationary distribution in PDMP models. This article extends the results given in [J. Lévy-Véhel and P.E. Lévy-Véhel, Variability and singularity arising from poor compliance in a pharmacodynamical model I: The multi-IV case, J. Pharmacokinet. Pharmacodyn. 40 (2013), pp. 15–39], by considering more realistic irregular dosing schedules. The computations permit precise assessment of the effect of various significant parameters such as the mean rate of intake, the elimination rate, and the mean dose. They quantify how much poor adherence will affect the regimen. Our results help to understand the consequences of poor adherence.},
  archive      = {J_JOAS},
  author       = {Lisandro J. Fermín and Jacques Lévy-Véhel},
  doi          = {10.1080/02664763.2019.1711030},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2525-2545},
  shortjournal = {J. Appl. Stat.},
  title        = {Variability and singularity arising from a piecewise-deterministic markov process applied to model poor patient compliance in the multi-IV case},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inference of progressively type-II censored competing risks
data from chen distribution with an application. <em>JOAS</em>,
<em>47</em>(13-15), 2492–2524. (<a
href="https://doi.org/10.1080/02664763.2020.1815670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the estimation of unknown parameters of Chen distribution is considered under progressive Type-II censoring in the presence of competing failure causes. It is assumed that the latent causes of failures have independent Chen distributions with the common shape parameter, but different scale parameters. From a frequentist perspective, the maximum likelihood estimate of parameters via expectation–maximization (EM) algorithm is obtained. Also, the expected Fisher information matrix based on the missing information principle is computed. By using the obtained expected Fisher information matrix of the MLEs, asymptotic 95\% confidence intervals for the parameters are constructed. We also apply the bootstrap methods (Bootstrap-p and Bootstrap-t) to construct confidence intervals. From Bayesian aspect, the Bayes estimates of the unknown parameters are computed by applying the Markov chain Monte Carlo (MCMC) procedure, the average length and coverage rate of credible intervals are also carried out. The Bayes inference is based on the squared error, LINEX, and general entropy loss functions. The performance of point estimators and confidence intervals is evaluated by a simulation study. Finally, a real-life example is considered for illustrative purposes.},
  archive      = {J_JOAS},
  author       = {Essam A. Ahmed and Ziyad Ali Alhussain and Mukhtar M. Salah and Hanan Haj Ahmed and M. S. Eliwa},
  doi          = {10.1080/02664763.2020.1815670},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2492-2524},
  shortjournal = {J. Appl. Stat.},
  title        = {Inference of progressively type-II censored competing risks data from chen distribution with an application},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Asymptotic normality of the test statistics for the unified
relative dispersion and relative variation indexes. <em>JOAS</em>,
<em>47</em>(13-15), 2479–2491. (<a
href="https://doi.org/10.1080/02664763.2020.1779193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dispersion indexes with respect to the Poisson and binomial distributions are widely used to assess the conformity of the underlying distribution from an observed sample of the count with one or the other of these theoretical distributions. Recently, the exponential variation index has been proposed as an extension to nonnegative continuous data. This paper aims to gather to study the unified definition of these indexes with respect to the relative variability of a nonnegative natural exponential family of distributions through its variance function. We establish the strong consistency of the plug-in estimators of the indexes as well as their asymptotic normalities. Since the exact distributions of the estimators are not available in closed form, we consider the test of hypothesis relying on these estimators as test statistics with their asymptotic distributions. Simulation studies globally suggest good behaviours of these tests of hypothesis procedures. Applicable examples are analysed, including the lesser-known references such as negative binomial and inverse Gaussian, and improving the very usual case of the Poisson dispersion index. Concluding remarks are made with suggestions of possible extensions.},
  archive      = {J_JOAS},
  author       = {Aboubacar Y. Touré and Simplice Dossou-Gbété and Célestin C. Kokonendji},
  doi          = {10.1080/02664763.2020.1779193},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2479-2491},
  shortjournal = {J. Appl. Stat.},
  title        = {Asymptotic normality of the test statistics for the unified relative dispersion and relative variation indexes},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Process capability indices in normal distribution with the
presence of outliers. <em>JOAS</em>, <em>47</em>(13-15), 2443–2478. (<a
href="https://doi.org/10.1080/02664763.2020.1796934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Process capability indices (PCIs) are useful measures to evaluate the performance and capability of a process when it is under control. Assuming the specification variable is distributed from a normal population, several PCIs are derived by the researchers. Also, many scientists have worked on these indices when data are contaminated with outliers as well as in the homogenous case. But, in almost all studies, they evaluated the effect of outliers on the PCIs nonparametrical and used robust methods. Here, the parametric model of outliers is considered and introduced the PCIs based on the outliers model. Therefore, these indices are estimated based on the maximum-likelihood and moment estimator of the unknown parameters of the normal distribution contaminated by outliers. Finally, the performances of these measures as well as their parametric and nonparametric estimators are discussed by using simulation studies and several numerical examples. It has been seen that parametric estimation has better performances than a nonparametric method.},
  archive      = {J_JOAS},
  author       = {M. Jabbari Nooghabi},
  doi          = {10.1080/02664763.2020.1796934},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2443-2478},
  shortjournal = {J. Appl. Stat.},
  title        = {Process capability indices in normal distribution with the presence of outliers},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bias induced by adaptive dose-finding designs.
<em>JOAS</em>, <em>47</em>(13-15), 2431–2442. (<a
href="https://doi.org/10.1080/02664763.2019.1649375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a long literature on bias in maximum likelihood estimators. Here we demonstrate that adaptive dose-finding procedures (such as Continual Reassessment Methods, Up-and-Down and Interval Designs) themselves induce bias. In particular, with Bernoulli responses and dose assignments that depend on prior responses, we provide an explicit formula for the bias of observed response rates. We illustrate the patterns of bias for designs that aim to concentrate dose allocations around a target dose, which represents a specific quantile of a cumulative response-threshold distribution. For such designs, bias tends to be positive above the target dose and negative below it. To our knowledge, this property of dose-finding designs has not previously been recognized by design developers. We discuss the implications of this bias and suggest a simple shrinkage mitigation formula that improves estimation at doses away from the target.},
  archive      = {J_JOAS},
  author       = {Nancy Flournoy and Assaf P. Oron},
  doi          = {10.1080/02664763.2019.1649375},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2431-2442},
  shortjournal = {J. Appl. Stat.},
  title        = {Bias induced by adaptive dose-finding designs},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Models with commutative orthogonal block structure: A
general condition for commutativity. <em>JOAS</em>, <em>47</em>(13-15),
2421–2430. (<a
href="https://doi.org/10.1080/02664763.2020.1765322">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A linear mixed model whose variance-covariance matrix is a linear combination of known pairwise orthogonal projection matrices that add to the identity matrix, is a model with orthogonal block structure (OBS). OBS have estimators with good behavior for estimable vectors and variance components, moreover it may be interesting that the least squares estimators give the best linear unbiased estimators, for estimable vectors. We can achieve it, requiring commutativity between the orthogonal projection matrix, on the space spanned by the mean vector, and the orthogonal projection matrices involved in the expression of the variance-covariance matrix. This commutativity condition defines a more restrict class of OBS, named COBS (model with commutative orthogonal block structure). With this work we aim to present a commutativity condition, resorting to a special class of matrices, named U-matrices.},
  archive      = {J_JOAS},
  author       = {C. Santos and C. Nunes and C. Dias and J.T. Mexia},
  doi          = {10.1080/02664763.2020.1765322},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2421-2430},
  shortjournal = {J. Appl. Stat.},
  title        = {Models with commutative orthogonal block structure: A general condition for commutativity},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Best-subset model selection based on multitudinal
assessments of likelihood improvements. <em>JOAS</em>,
<em>47</em>(13-15), 2384–2420. (<a
href="https://doi.org/10.1080/02664763.2019.1645097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common model selection approach is to select the best model, according to some criterion, from among the collection of models defined by all possible subsets of the explanatory variables. Identifying an optimal subset has proven to be a challenging problem, both statistically and computationally. Our model selection procedure allows the researcher to nominate, a priori, the probability at which models containing false or spurious variables will be selected from among all possible subsets. The procedure determines whether inclusion of each candidate variable results in a sufficiently improved fitting term – and is hence named the SIFT procedure. Two variants are proposed: a naive method based on a set of restrictive assumptions and an empirical permutation-based method. Properties of these methods are investigated within the standard linear modeling framework and performance is evaluated against other model selection techniques. The SIFT procedure behaves as designed – asymptotically selecting variables that characterize the underlying data generating mechanism, while limiting selection of spurious variables to the desired level. The SIFT methodology offers researchers a promising new approach to model selection, providing the ability to control the probability of selecting a model that includes spurious variables to a level based on the context of the application.},
  archive      = {J_JOAS},
  author       = {Knute D. Carter and Joseph E. Cavanaugh},
  doi          = {10.1080/02664763.2019.1645097},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2384-2420},
  shortjournal = {J. Appl. Stat.},
  title        = {Best-subset model selection based on multitudinal assessments of likelihood improvements},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation in additive models and ANOVA-like applications.
<em>JOAS</em>, <em>47</em>(13-15), 2374–2383. (<a
href="https://doi.org/10.1080/02664763.2020.1723501">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A well-known property of cumulant generating function is used to estimate the first four order cumulants, using least-squares estimators. In the case of additive models, empirical best linear unbiased predictors are also obtained. Pairs of independent and identically distributed models associated with the treatments of a base design are used to obtain unbiased estimators for the fourth-order cumulants. An application to real data is presented, showing the good behaviour of the least-squares estimators and the great flexibility of our approach.},
  archive      = {J_JOAS},
  author       = {Patrícia Antunes and Sandra S. Ferreira and Dário Ferreira and Célia Nunes and João Tiago Mexia},
  doi          = {10.1080/02664763.2020.1723501},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2374-2383},
  shortjournal = {J. Appl. Stat.},
  title        = {Estimation in additive models and ANOVA-like applications},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Statistical inference for a general class of distributions
with time-varying parameters. <em>JOAS</em>, <em>47</em>(13-15),
2354–2373. (<a
href="https://doi.org/10.1080/02664763.2020.1763271">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article we are interested in a general class of distributions for independent not necessarily identically distributed random variables, closed under minima, that includes a number of discrete and continuous distributions like the Geometric, Exponential, Weibull or Pareto. The main parameter involved in this class of distributions is assumed to be time varying with several possible modeling options. This is of particular interest in reliability and survival analysis for describing the time to event or failure. The maximum likelihood estimation of the parameters is addressed and the asymptotic properties of the estimators are discussed. We provide real and simulated examples and we explore the accuracy of the estimating procedure as well as the performance of classical model selection criteria in choosing the correct model among a number of competing models for the time-varying parameters of interest.},
  archive      = {J_JOAS},
  author       = {Vlad Stefan Barbu and Alex Karagrigoriou and Andreas Makrides},
  doi          = {10.1080/02664763.2020.1763271},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2354-2373},
  shortjournal = {J. Appl. Stat.},
  title        = {Statistical inference for a general class of distributions with time-varying parameters},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dichotomous unimodal compound models: Application to the
distribution of insurance losses. <em>JOAS</em>, <em>47</em>(13-15),
2328–2353. (<a
href="https://doi.org/10.1080/02664763.2020.1789076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A correct modelization of the insurance losses distribution is crucial in the insurance industry. This distribution is generally highly positively skewed, unimodal hump-shaped, and with a heavy right tail. Compound models are a profitable way to accommodate situations in which some of the probability masses are shifted to the tails of the distribution. Therefore, in this work, a general approach to compound unimodal hump-shaped distributions with a mixing dichotomous distribution is introduced. A 2-parameter unimodal hump-shaped distribution, defined on a positive support, is considered and reparametrized with respect to the mode and to another parameter related to the distribution variability. The compound is performed by scaling the latter parameter by means of a dichotomous mixing distribution that governs the tail behavior of the resulting model. The proposed model can also allow for automatic detection of typical and atypical losses via a simple procedure based on maximum a posteriori probabilities. Unimodal gamma and log-normal are considered as examples of unimodal hump-shaped distributions. The resulting models are firstly evaluated in a sensitivity study and then fitted to two real insurance loss datasets, along with several well-known competitors. Likelihood-based information criteria and risk measures are used to compare the models.},
  archive      = {J_JOAS},
  author       = {Salvatore D. Tomarchio and Antonio Punzo},
  doi          = {10.1080/02664763.2020.1789076},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2328-2353},
  shortjournal = {J. Appl. Stat.},
  title        = {Dichotomous unimodal compound models: Application to the distribution of insurance losses},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ordered quantile normalization: A semiparametric
transformation built for the cross-validation era. <em>JOAS</em>,
<em>47</em>(13-15), 2312–2327. (<a
href="https://doi.org/10.1080/02664763.2019.1630372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Normalization transformations have recently experienced a resurgence in popularity in the era of machine learning, particularly in data preprocessing. However, the classical methods that can be adapted to cross-validation are not always effective. We introduce Ordered Quantile (ORQ) normalization, a one-to-one transformation that is designed to consistently and effectively transform a vector of arbitrary distribution into a vector that follows a normal (Gaussian) distribution. In the absence of ties, ORQ normalization is guaranteed to produce normally distributed transformed data. Once trained, an ORQ transformation can be readily and effectively applied to new data. We compare the effectiveness of the ORQ technique with other popular normalization methods in a simulation study where the true data generating distributions are known. We find that ORQ normalization is the only method that works consistently and effectively, regardless of the underlying distribution. We also explore the use of repeated cross-validation to identify the best normalizing transformation when the true underlying distribution is unknown. We apply our technique and other normalization methods via the bestNormalize R package on a car pricing data set. We built bestNormalize to evaluate the normalization efficacy of many candidate transformations; the package is freely available via the Comprehensive R Archive Network.},
  archive      = {J_JOAS},
  author       = {Ryan A. Peterson and Joseph E. Cavanaugh},
  doi          = {10.1080/02664763.2019.1630372},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2312-2327},
  shortjournal = {J. Appl. Stat.},
  title        = {Ordered quantile normalization: A semiparametric transformation built for the cross-validation era},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On bivariate pseudo-exponential distributions.
<em>JOAS</em>, <em>47</em>(13-15), 2299–2311. (<a
href="https://doi.org/10.1080/02664763.2019.1686132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A bivariate conditionally specified distribution is one in which the dependence relationship between the two random variables is accomplished by defining the distribution of one of the random variables, given the other. One such conditionally specified model is called the pseudo-exponential distribution, where both the marginal distribution of one and the conditional distribution of the other, given the first, are exponential. In this paper, a variation of this conditioning regime is introduced, and its characteristics are contrasted with the original. An example is used to demonstrate the applicability of the new model. Per-capita Gross Domestic Product (GDP) is a measure of a nation&#39;s total annual production of goods and services, divided by its population. Two variations of both the original and the new conditioning regime are applied to GDP and infant mortality data across nations and territories. Possible generalizations are considered.},
  archive      = {J_JOAS},
  author       = {Barry C. Arnold and Matthew A. Arvanitis},
  doi          = {10.1080/02664763.2019.1686132},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2299-2311},
  shortjournal = {J. Appl. Stat.},
  title        = {On bivariate pseudo-exponential distributions},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Editorial to special issue v WCDANM 2018. <em>JOAS</em>,
<em>47</em>(13-15), 2289–2298. (<a
href="https://doi.org/10.1080/02664763.2020.1818489">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOAS},
  author       = {M. Stehlík and L. M. Grilo and P. K. Jordanova},
  doi          = {10.1080/02664763.2020.1818489},
  journal      = {Journal of Applied Statistics},
  number       = {13-15},
  pages        = {2289-2298},
  shortjournal = {J. Appl. Stat.},
  title        = {Editorial to special issue v WCDANM 2018},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Random effect exponentiated-exponential geometric model for
clustered/longitudinal zero-inflated count data. <em>JOAS</em>,
<em>47</em>(12), 2272–2288. (<a
href="https://doi.org/10.1080/02664763.2019.1706726">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For count responses, there are situations in biomedical and sociological applications in which extra zeroes occur. Modeling correlated (e.g. repeated measures and clustered) zero-inflated count data includes special challenges because the correlation between measurements for a subject or a cluster needs to be taken into account. Moreover, zero-inflated count data are often faced with over/under dispersion problem. In this paper, we propose a random effect model for repeated measurements or clustered data with over/under dispersed response called random effect zero-inflated exponentiated-exponential geometric regression model. The proposed method was illustrated through real examples. The performance of the model and asymptotical properties of the estimations were investigated using simulation studies.},
  archive      = {J_JOAS},
  author       = {Leili Tapak and Omid Hamidi and Payam Amini and Geert Verbeke},
  doi          = {10.1080/02664763.2019.1706726},
  journal      = {Journal of Applied Statistics},
  number       = {12},
  pages        = {2272-2288},
  shortjournal = {J. Appl. Stat.},
  title        = {Random effect exponentiated-exponential geometric model for clustered/longitudinal zero-inflated count data},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new poisson liu regression estimator: Method and
application. <em>JOAS</em>, <em>47</em>(12), 2258–2271. (<a
href="https://doi.org/10.1080/02664763.2019.1707485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the estimation of parameters for the Poisson regression model in the presence of high, but imperfect multicollinearity. To mitigate this problem, we suggest using the Poisson Liu Regression Estimator (PLRE) and propose some new approaches to estimate this shrinkage parameter. The small sample statistical properties of these estimators are systematically scrutinized using Monte Carlo simulations. To evaluate the performance of these estimators, we assess the Mean Square Errors (MSE) and the Mean Absolute Percentage Errors (MAPE). The simulation results clearly illustrate the benefit of the methods of estimating these types of shrinkage parameters in finite samples. Finally, we illustrate the empirical relevance of our newly proposed methods using an empirically relevant application. Thus, in summary, via simulations of empirically relevant parameter values, and by a standard empirical application, it is clearly demonstrated that our technique exhibits more precise estimators, compared to traditional techniques – at least when multicollinearity exist among the regressors.},
  archive      = {J_JOAS},
  author       = {Muhammad Qasim and B. M. G. Kibria and Kristofer Månsson and Pär Sjölander},
  doi          = {10.1080/02664763.2019.1707485},
  journal      = {Journal of Applied Statistics},
  number       = {12},
  pages        = {2258-2271},
  shortjournal = {J. Appl. Stat.},
  title        = {A new poisson liu regression estimator: Method and application},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distribution-free monitoring schemes based on order
statistics: A general approach. <em>JOAS</em>, <em>47</em>(12),
2230–2257. (<a
href="https://doi.org/10.1080/02664763.2019.1707518">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we establish a new class of distribution-free Shewhart-type monitoring schemes based on order statistics. The setup of the proposed family of nonparametric control charts is presented in detail. Specific monitoring schemes, already introduced in the literature, are confirmed to be members of the new class. In addition, a new nonparametric monitoring scheme that belongs to the class is established, while explicit formulae for its basic characteristics are reached. The numerical study carried out reveals that the proposed scheme achieves adversarial in-control and out-of-control performance.},
  archive      = {J_JOAS},
  author       = {Ioannis S. Triantafyllou and Nikolaos I. Panayiotou},
  doi          = {10.1080/02664763.2019.1707518},
  journal      = {Journal of Applied Statistics},
  number       = {12},
  pages        = {2230-2257},
  shortjournal = {J. Appl. Stat.},
  title        = {Distribution-free monitoring schemes based on order statistics: A general approach},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quasi-binomial zero-inflated regression model suitable for
variables with bounded support. <em>JOAS</em>, <em>47</em>(12),
2208–2229. (<a
href="https://doi.org/10.1080/02664763.2019.1707517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, a variety of regression models, including zero-inflated and hurdle versions, have been proposed to explain the case of a dependent variable with respect to exogenous covariates. Apart from the classical Poisson, negative binomial and generalised Poisson distributions, many proposals have appeared in the statistical literature, perhaps in response to the new possibilities offered by advanced software that now enables researchers to implement numerous special functions in a relatively simple way. However, we believe that a significant research gap remains, since very little attention has been paid to the quasi-binomial distribution, which was first proposed over fifty years ago. We believe this distribution might constitute a valid alternative to existing regression models, in situations in which the variable has bounded support. Therefore, in this paper we present a zero-inflated regression model based on the quasi-binomial distribution, taking into account the moments and maximum likelihood estimators, and perform a score test to compare the zero-inflated quasi-binomial distribution with the zero-inflated binomial distribution, and the zero-inflated model with the homogeneous model (the model in which covariates are not considered). This analysis is illustrated with two data sets that are well known in the statistical literature and which contain a large number of zeros.},
  archive      = {J_JOAS},
  author       = {E. Gómez–Déniz and D. I. Gallardo and H. W. Gómez},
  doi          = {10.1080/02664763.2019.1707517},
  journal      = {Journal of Applied Statistics},
  number       = {12},
  pages        = {2208-2229},
  shortjournal = {J. Appl. Stat.},
  title        = {Quasi-binomial zero-inflated regression model suitable for variables with bounded support},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mutual information model selection algorithm for time
series. <em>JOAS</em>, <em>47</em>(12), 2192–2207. (<a
href="https://doi.org/10.1080/02664763.2019.1707516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series model selection has been widely studied in recent years. It is of importance to select the best model among candidate models proposed for a series in terms of explaining the procedure that governs the series and providing the most accurate forecast for the future observations. In this study, it is aimed to create an algorithm for order selection in Box–Jenkins models that combines penalized natural logarithm of mutual information among the original series and predictions coming from each candidate. The penalization is achieved by subtracting the number of parameters in each candidate and empirical information the data provide.Simulation studies under various scenarios and applications on real data sets imply that our algorithm offers a promising and satisfactory alternative to its counterparts.},
  archive      = {J_JOAS},
  author       = {Elif Akça and Ceylan Yozgatlıgil},
  doi          = {10.1080/02664763.2019.1707516},
  journal      = {Journal of Applied Statistics},
  number       = {12},
  pages        = {2192-2207},
  shortjournal = {J. Appl. Stat.},
  title        = {Mutual information model selection algorithm for time series},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparametric tests for stratified additive hazards model
based on current status data. <em>JOAS</em>, <em>47</em>(12), 2178–2191.
(<a href="https://doi.org/10.1080/02664763.2019.1707515">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stratified regression models are commonly employed when study subjects may come from possibly different strata such as different medical centers, and for the situation, one common question of interest is to test the existence of the stratum effect. To address this, there exists some literature on the testing of the stratum effects under the framework of the proportional hazards model when one observes right-censored data or interval-censored data. In this paper, we consider the situation under the additive hazards model when one faces current status data, for which there does not seem to exist an established test procedure. The asymptotic distributions of the proposed test procedure are provided. Also a simulation study is performed to evaluate the performance of the proposed method and indicates that it works well for practical situations. The approach is applied to a set of real current status data from a tumorigenicity study.},
  archive      = {J_JOAS},
  author       = {Xiaodong Fan and Shi-shun Zhao and Qingchun Zhang and Jianguo Sun},
  doi          = {10.1080/02664763.2019.1707515},
  journal      = {Journal of Applied Statistics},
  number       = {12},
  pages        = {2178-2191},
  shortjournal = {J. Appl. Stat.},
  title        = {Nonparametric tests for stratified additive hazards model based on current status data},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The multinomial logistic regression model for predicting the
discharge status after liver transplantation: Estimation and diagnostics
analysis. <em>JOAS</em>, <em>47</em>(12), 2159–2177. (<a
href="https://doi.org/10.1080/02664763.2019.1706725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The multinomial logistic regression model (MLRM) can be interpreted as a natural extension of the binomial model with logit link function to situations where the response variable can have three or more possible outcomes. In addition, when the categories of the response variable are nominal, the MLRM can be expressed in terms of two or more logistic models and analyzed in both frequentist and Bayesian approaches. However, few discussions about post modeling in categorical data models are found in the literature, and they mainly use Bayesian inference. The objective of this work is to present classic and Bayesian diagnostic measures for categorical data models. These measures are applied to a dataset (status) of patients undergoing kidney transplantation.},
  archive      = {J_JOAS},
  author       = {E. M. Hashimoto and E. M. M. Ortega and G. M. Cordeiro and A. K. Suzuki and M. W. Kattan},
  doi          = {10.1080/02664763.2019.1706725},
  journal      = {Journal of Applied Statistics},
  number       = {12},
  pages        = {2159-2177},
  shortjournal = {J. Appl. Stat.},
  title        = {The multinomial logistic regression model for predicting the discharge status after liver transplantation: Estimation and diagnostics analysis},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fuzzy process capability indices for simple linear profile.
<em>JOAS</em>, <em>47</em>(12), 2136–2158. (<a
href="https://doi.org/10.1080/02664763.2019.1704225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Process capability indices are numerical tools that quantify how well a process can meet customer requirements, specifications or engineering tolerances. Fuzzy logic is incorporated to deal imprecise, incomplete data along with uncertainty. This paper develops two fuzzy methods for measuring the process capability in simple linear profiles for the circumstances in which lower and upper specification limits are imprecise. To guide practitioners, numerical example is provided.},
  archive      = {J_JOAS},
  author       = {Zainab Abbasi Ganji and Bahram Sadeghpour Gildeh},
  doi          = {10.1080/02664763.2019.1704225},
  journal      = {Journal of Applied Statistics},
  number       = {12},
  pages        = {2136-2158},
  shortjournal = {J. Appl. Stat.},
  title        = {Fuzzy process capability indices for simple linear profile},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Measuring sport performances under pressure by
classification trees with application to basketball shooting.
<em>JOAS</em>, <em>47</em>(12), 2120–2135. (<a
href="https://doi.org/10.1080/02664763.2019.1704702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Measuring players&#39; performance in team sports is fundamental since managers need to evaluate players with respect to the ability to score during crucial moments of the game. Using Classification and Regression Trees (CART) and play-by-play basketball data, we estimate the probabilities to score the shot with respect to a selection of game covariates related to game pressure. We use scoring probabilities to develop a player-specific shooting performance index that takes into account for the difficulty associated to score different types of shots. By applying this procedure to a large sample of 2016–2017 Basketball Champions League (BCL) and 2017–2018 National Basketball Association (NBA) games, we compare the factors affecting shooting performance in Europe and in the United States and we evaluate a selection of players in terms of the proposed shooting performance index with the final aim of providing useful guidelines for the team strategy.},
  archive      = {J_JOAS},
  author       = {Rodolfo Metulini and Mael Le Carre},
  doi          = {10.1080/02664763.2019.1704702},
  journal      = {Journal of Applied Statistics},
  number       = {12},
  pages        = {2120-2135},
  shortjournal = {J. Appl. Stat.},
  title        = {Measuring sport performances under pressure by classification trees with application to basketball shooting},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new heavy-tailed distribution defined on the bounded
interval: The logit slash distribution and its application.
<em>JOAS</em>, <em>47</em>(12), 2097–2119. (<a
href="https://doi.org/10.1080/02664763.2019.1704701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new heavy-tailed and alternative slash type distribution on a bounded interval via a relation of a slash random variable with respect to the standard logistic function to model the real data set with skewed and high kurtosis which includes the outlier observation. Some basic statistical properties of the newly defined distribution are studied. We derive the maximum likelihood, least-square, and weighted least-square estimations of its parameters. We assess the performance of the estimators of these estimation methods by the simulation study. Moreover, an application to real data demonstrates that the proposed distribution can provide a better fit than well-known bounded distributions in the literature when the skewed data set with high kurtosis contains the outlier observations.},
  archive      = {J_JOAS},
  author       = {Mustafa Ç. Korkmaz},
  doi          = {10.1080/02664763.2019.1704701},
  journal      = {Journal of Applied Statistics},
  number       = {12},
  pages        = {2097-2119},
  shortjournal = {J. Appl. Stat.},
  title        = {A new heavy-tailed distribution defined on the bounded interval: The logit slash distribution and its application},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new way for handling mobility in longitudinal data.
<em>JOAS</em>, <em>47</em>(11), 2081–2096. (<a
href="https://doi.org/10.1080/02664763.2019.1704224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the social sciences, applied researchers often face a statistical dilemma when multilevel data is structured such that lower-level units are not purely clustered within higher-level units. To aid applied researchers in appropriately analyzing such data structures, this study proposes a multiple membership growth curve model (MM-GCM). The MM-GCM offers some advantages to other similar modeling approaches, including greater flexibility in modeling the intercept at the time-point most desired for interpretation. A real longitudinal dataset from the field of education with a multiple membership structure, where some students changed schools over time, was used to demonstrate the application of the MM-GCM. Baseline and conditional MM-GCMs are presented, and parameter estimates were compared with two other common approaches to handling such data structures – the final school -GCM that ignores mobile students by only modeling the final school attended and the delete -GCM that deletes mobile students. Additionally, a simulation study was conducted to further assess the impact of ignoring mobility on parameter estimates. The results indicate that ignoring mobility results in substantial bias in model estimates, especially for cluster-level coefficients and variance components.},
  archive      = {J_JOAS},
  author       = {Christopher J. Cappelli and Audrey J. Leroux and Congying Sun},
  doi          = {10.1080/02664763.2019.1704224},
  journal      = {Journal of Applied Statistics},
  number       = {11},
  pages        = {2081-2096},
  shortjournal = {J. Appl. Stat.},
  title        = {A new way for handling mobility in longitudinal data},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Candidate-gene association analysis for a continuous
phenotype with a spike at zero using parent-offspring trios.
<em>JOAS</em>, <em>47</em>(11), 2066–2080. (<a
href="https://doi.org/10.1080/02664763.2019.1704226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose the class of generalized additive models for location, scale and shape in a test for the association of genetic markers with non-normally distributed phenotypes comprising a spike at zero. The resulting statistical test is a generalization of the quantitative transmission disequilibrium test with mating type indicator, which was originally designed for normally distributed quantitative traits and parent-offspring data. As a motivational example, we consider coronary artery calcification (CAC), which can accurately be identified by electron beam tomography. In the investigated regions, individuals will have a continuous measure of the extent of calcium found or they will be calcium-free. Hence, the resulting distribution is a mixed discrete-continuous distribution with spike at zero. We carry out parent-offspring simulations motivated by such CAC measurement values in a screening population to study statistical properties of the proposed test for genetic association. Furthermore, we apply the approach to data of the Genetic Analysis Workshop 16 that are based on real genotype and family data of the Framingham Heart Study, and test the association of selected genetic markers with simulated coronary artery calcification.},
  archive      = {J_JOAS},
  author       = {Nadja Klein and Andrew Entwistle and Albert Rosenberger and Thomas Kneib and Heike Bickeböller},
  doi          = {10.1080/02664763.2019.1704226},
  journal      = {Journal of Applied Statistics},
  number       = {11},
  pages        = {2066-2080},
  shortjournal = {J. Appl. Stat.},
  title        = {Candidate-gene association analysis for a continuous phenotype with a spike at zero using parent-offspring trios},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distribution of the c statistic with applications to the
sample mean of poisson data. <em>JOAS</em>, <em>47</em>(11), 2044–2065.
(<a href="https://doi.org/10.1080/02664763.2019.1704703">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The C statistic, also known as the Cash statistic, is often used in astronomy for the analysis of low-count Poisson data. The main advantage of this statistic, compared to the more commonly used χ 2 statistic, is its applicability without the need to combine data points. This feature has made the C statistic a very useful method to analyze Poisson data that have small (or even null) counts in each resolution element. One of the challenges of the C statistic is that its probability distribution, under the null hypothesis that the data follow a parent model, is not known exactly. This paper presents an effort towards improving our understanding of the C statistic by studying (a) the distribution of C statistic for a fully specified model, (b) the distribution of C min resulting from a maximum-likelihood fit to a simple one-parameter constant model, i.e. a model that represents the sample mean of N Poisson measurements, and (c) the distribution of the associated Δ C statistic that is used for parameter estimation. The results confirm the expectation that, in the high-count limit, both C statistic and C min have the same mean and variance as a χ 2 statistic with same number of degrees of freedom. It is also found that, in the low-count regime, the expectation of the C statistic and C min can be substantially lower than for a χ 2 distribution. The paper makes use of recent X-ray observations of the astronomical source PG 1116+215 to illustrate the application of the C statistic to Poisson data.},
  archive      = {J_JOAS},
  author       = {Massimiliano Bonamente},
  doi          = {10.1080/02664763.2019.1704703},
  journal      = {Journal of Applied Statistics},
  number       = {11},
  pages        = {2044-2065},
  shortjournal = {J. Appl. Stat.},
  title        = {Distribution of the c statistic with applications to the sample mean of poisson data},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variable selection in elliptical linear mixed model.
<em>JOAS</em>, <em>47</em>(11), 2025–2043. (<a
href="https://doi.org/10.1080/02664763.2019.1702928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Variable selection in elliptical Linear Mixed Models (LMMs) with a shrinkage penalty function (SPF) is the main scope of this study. SPFs are applied for parameter estimation and variable selection simultaneously. The smoothly clipped absolute deviation penalty (SCAD) is one of the SPFs and it is adapted into the elliptical LMM in this study. The proposed idea is highly applicable to a variety of models which are set up with different distributions such as normal, student- t , Pearson VII, power exponential and so on. Simulation studies and real data example with one of the elliptical distributions show that if the variable selection is also a concern, it is worthwhile to carry on the variable selection and the parameter estimation simultaneously in the elliptical LMM.},
  archive      = {J_JOAS},
  author       = {Fulya Gokalp Yavuz and Olcay Arslan},
  doi          = {10.1080/02664763.2019.1702928},
  journal      = {Journal of Applied Statistics},
  number       = {11},
  pages        = {2025-2043},
  shortjournal = {J. Appl. Stat.},
  title        = {Variable selection in elliptical linear mixed model},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Goodness-of-fit filtering in classical metric
multidimensional scaling with large datasets. <em>JOAS</em>,
<em>47</em>(11), 2011–2024. (<a
href="https://doi.org/10.1080/02664763.2019.1702929">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metric multidimensional scaling (MDS) is a widely used multivariate method with applications in almost all scientific disciplines. Eigenvalues obtained in the analysis are usually reported in order to calculate the overall goodness-of-fit of the distance matrix. In this paper, we refine MDS goodness-of-fit calculations, proposing additional point and pairwise goodness-of-fit statistics that can be used to filter poorly represented observations in MDS maps. The proposed statistics are especially relevant for large data sets that contain outliers, with typically many poorly fitted observations, and are helpful for improving MDS output and emphasizing the most important features of the dataset. Several goodness-of-fit statistics are considered, and both Euclidean and non-Euclidean distance matrices are considered. Some examples with data from demographic, genetic and geographic studies are shown.},
  archive      = {J_JOAS},
  author       = {Jan Graffelman},
  doi          = {10.1080/02664763.2019.1702929},
  journal      = {Journal of Applied Statistics},
  number       = {11},
  pages        = {2011-2024},
  shortjournal = {J. Appl. Stat.},
  title        = {Goodness-of-fit filtering in classical metric multidimensional scaling with large datasets},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Residualization: Justification, properties and application.
<em>JOAS</em>, <em>47</em>(11), 1990–2010. (<a
href="https://doi.org/10.1080/02664763.2019.1701638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although it is usual to find collinearity in econometric models, it is commonly disregarded. An extended solution is to eliminate the variable causing the problem but, in some cases, this decision can affect the goal of the research. Alternatively, residualization not only allows mitigation of collinearity, but it also provides an alternative interpretation of the coefficients isolating the effect of the residualized variable. This paper fully develops the residualization procedure and justifies its application not only for dealing with multicollinearity but also for separating the individual effects of the regressor variables. This contribution is illustrated by two econometric models with financial and ecological data, although it can also be extended to many different fields.},
  archive      = {J_JOAS},
  author       = {Catalina B. García and Román Salmerón and Claudia García and José García},
  doi          = {10.1080/02664763.2019.1701638},
  journal      = {Journal of Applied Statistics},
  number       = {11},
  pages        = {1990-2010},
  shortjournal = {J. Appl. Stat.},
  title        = {Residualization: Justification, properties and application},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two preprocessing algorithms for climate time series.
<em>JOAS</em>, <em>47</em>(11), 1970–1989. (<a
href="https://doi.org/10.1080/02664763.2019.1701637">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose two preprocessing algorithms suitable for climate time series. The first algorithm detects outliers based on an autoregressive cost update mechanism. The second one is based on the wavelet transform, a method from pattern recognition. In order to benchmark the algorithms&#39; performance we compare them to existing methods based on a synthetic data set. Eventually, for exemplary purposes, the proposed methods are applied to a data set of high-frequent temperature measurements from Novi Sad, Serbia. The results show that both methods together form a powerful tool for signal preprocessing: In case of solitary outliers the autoregressive cost update mechanism prevails, whereas the wavelet-based mechanism is the method of choice in the presence of multiple consecutive outliers.},
  archive      = {J_JOAS},
  author       = {Stephan Schlüter and Milena Kresoja},
  doi          = {10.1080/02664763.2019.1701637},
  journal      = {Journal of Applied Statistics},
  number       = {11},
  pages        = {1970-1989},
  shortjournal = {J. Appl. Stat.},
  title        = {Two preprocessing algorithms for climate time series},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic principal component analysis with missing values.
<em>JOAS</em>, <em>47</em>(11), 1957–1969. (<a
href="https://doi.org/10.1080/02664763.2019.1699910">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dynamic principal component analysis (DPCA), also known as frequency domain principal component analysis, has been developed by Brillinger [ Time Series: Data Analysis and Theory , Vol. 36, SIAM, 1981] to decompose multivariate time-series data into a few principal component series. A primary advantage of DPCA is its capability of extracting essential components from the data by reflecting the serial dependence of them. It is also used to estimate the common component in a dynamic factor model, which is frequently used in econometrics. However, its beneficial property cannot be utilized when missing values are present, which should not be simply ignored when estimating the spectral density matrix in the DPCA procedure. Based on a novel combination of conventional DPCA and self-consistency concept, we propose a DPCA method when missing values are present. We demonstrate the advantage of the proposed method over some existing imputation methods through the Monte Carlo experiments and real data analysis.},
  archive      = {J_JOAS},
  author       = {Junhyeon Kwon and Hee-Seok Oh and Yaeji Lim},
  doi          = {10.1080/02664763.2019.1699910},
  journal      = {Journal of Applied Statistics},
  number       = {11},
  pages        = {1957-1969},
  shortjournal = {J. Appl. Stat.},
  title        = {Dynamic principal component analysis with missing values},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Saddlepoint approximations to tail expectations under
non-gaussian base distributions: Option pricing applications.
<em>JOAS</em>, <em>47</em>(11), 1936–1956. (<a
href="https://doi.org/10.1080/02664763.2019.1703915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The saddlepoint approximation formulas provide versatile tools for analytic approximation of the tail expectation of a random variable by approximating the complex Laplace integral of the tail expectation expressed in terms of the cumulant generating function of the random variable. We generalize the saddlepoint approximation formulas for calculating tail expectations from the usual Gaussian base distribution to an arbitrary base distribution. Specific discussion is presented on the criteria of choosing the base distribution that fits better the underlying distribution. Numerical performance and comparison of accuracy are made among different saddlepoint approximation formulas. Improved accuracy of the saddlepoint approximations to tail expectations is revealed when proper base distributions are chosen. We also demonstrate enhanced accuracy of the generalized saddlepoint approximation formulas under non-Gaussian base distributions in pricing European options on continuous integrated variance under the Heston stochastic volatility model.},
  archive      = {J_JOAS},
  author       = {Yuantao Zhang and Yue Kuen Kwok},
  doi          = {10.1080/02664763.2019.1703915},
  journal      = {Journal of Applied Statistics},
  number       = {11},
  pages        = {1936-1956},
  shortjournal = {J. Appl. Stat.},
  title        = {Saddlepoint approximations to tail expectations under non-gaussian base distributions: Option pricing applications},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). State transition modeling of complex monitored health data.
<em>JOAS</em>, <em>47</em>(11), 1915–1935. (<a
href="https://doi.org/10.1080/02664763.2019.1698523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers the analysis of complex monitored health data, where often one or several signals are reflecting the current health status that can be represented by a finite number of states, in addition to a set of covariates. In particular, we consider a novel application of a non-parametric state intensity regression method in order to study time-dependent effects of covariates on the state transition intensities. The method can handle baseline, time varying as well as dynamic covariates. Because of the non-parametric nature, the method can handle different data types and challenges under minimal assumptions. If the signal that is reflecting the current health status is of continuous nature, we propose the application of a weighted median and a hysteresis filter as data pre-processing steps in order to facilitate robust analysis. In intensity regression, covariates can be aggregated by a suitable functional form over a time history window. We propose to study the estimated cumulative regression parameters for different choices of the time history window in order to investigate short- and long-term effects of the given covariates. The proposed framework is discussed and applied to resuscitation data of newborns collected in Tanzania.},
  archive      = {J_JOAS},
  author       = {Jörn Schulz and Jan Terje Kvaløy and Kjersti Engan and Trygve Eftestøl and Samwel Jatosh and Hussein Kidanto and Hege Ersdal},
  doi          = {10.1080/02664763.2019.1698523},
  journal      = {Journal of Applied Statistics},
  number       = {11},
  pages        = {1915-1935},
  shortjournal = {J. Appl. Stat.},
  title        = {State transition modeling of complex monitored health data},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bivariate negative binomial regression model with excess
zeros and right censoring: An application to indonesian data.
<em>JOAS</em>, <em>47</em>(10), 1901–1914. (<a
href="https://doi.org/10.1080/02664763.2019.1695761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a bivariate hurdle negative binomial (BHNB) regression model with right censoring to model correlated bivariate count data with excess zeros and few extreme observations. The parameters of the BHNB regression model are obtained using maximum likelihood with conjugate gradient optimization. The proposed model is applied to actual survey data where the bivariate outcome is number of days missed from primary activities and number of days spent in bed due to illness during the 4-week period preceding the inquiry date. We compared the right censored BHNB model to the right censored bivariate negative binomial (BNB) model. A simulation study is conducted to discuss some properties of the BHNB model. Our proposed model demonstrated superior performance in goodness-of-fit of estimated frequencies.},
  archive      = {J_JOAS},
  author       = {Seyed Ehsan Saffari and John Carson Allen},
  doi          = {10.1080/02664763.2019.1695761},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {1901-1914},
  shortjournal = {J. Appl. Stat.},
  title        = {Bivariate negative binomial regression model with excess zeros and right censoring: An application to indonesian data},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tests of fit for the gumbel distribution: EDF-based tests
against entropy-based tests. <em>JOAS</em>, <em>47</em>(10), 1885–1900.
(<a href="https://doi.org/10.1080/02664763.2019.1698522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose some tests of fit based on sample entropy for the composite Gumbel (Extreme Value) hypothesis. The proposed test statistics are constructed using different entropy estimates. Through a Monte Carlo simulation, critical values of the test statistics for various sample sizes are obtained. Since the tests based on the empirical distribution function (EDF) are commonly used in practice, the power values of the entropy-based tests with those of the EDF tests are compared against various alternatives and different sample sizes. Finally, two real data sets are modeled by the Gumbel distribution.},
  archive      = {J_JOAS},
  author       = {Hadi Alizadeh Noughabi and Jalil Jarrahiferiz},
  doi          = {10.1080/02664763.2019.1698522},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {1885-1900},
  shortjournal = {J. Appl. Stat.},
  title        = {Tests of fit for the gumbel distribution: EDF-based tests against entropy-based tests},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quantifying conditional probability tables in bayesian
networks: Bayesian regression for scenario-based encoding of elicited
expert assessments on feral pig habitat. <em>JOAS</em>, <em>47</em>(10),
1848–1884. (<a
href="https://doi.org/10.1080/02664763.2019.1697651">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian networks are now widespread for modelling uncertain knowledge. They graph probabilistic relationships, which are quantified using conditional probability tables (CPTs). When empirical data are unavailable, experts may specify CPTs. Here we propose novel methodology for quantifying CPTs: a Bayesian statistical approach to both elicitation and encoding of expert-specified probabilities, in a way that acknowledges their uncertainty. We illustrate this new approach using a case study describing habitat most at risk from feral pigs. For complicated CPTs, it is difficult to elicit all scenarios (CPT entries). Like the CPT Calculator software program, we ask about a few scenarios (e.g. under a one-factor-at-a-time design) to reduce the experts&#39; workload. Unlike CPT Calculator, we adopt a global rather than local regression to ‘fill out’ CPT entries. Unlike other methods for scenario-based elicitation for regression, we capture uncertainty about each probability in a sequence that explicitly controls biases and enhances interpretation. Furthermore, to utilize all elicited information, we introduce Bayesian rather than Classical generalised linear modelling (GLM). For large CPTs (e.g. &gt;3 levels per parent) we show Bayesian GLM supports richer inference, particularly on interactions, even with few scenarios, providing more information regarding accuracy of encoding.},
  archive      = {J_JOAS},
  author       = {Ibrahim Alkhairy and Samantha Low-Choy and Justine Murray and Junhu Wang and Anthony Pettitt},
  doi          = {10.1080/02664763.2019.1697651},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {1848-1884},
  shortjournal = {J. Appl. Stat.},
  title        = {Quantifying conditional probability tables in bayesian networks: Bayesian regression for scenario-based encoding of elicited expert assessments on feral pig habitat},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A class of residuals for outlier identification in zero
adjusted regression models. <em>JOAS</em>, <em>47</em>(10), 1833–1847.
(<a href="https://doi.org/10.1080/02664763.2019.1696759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero adjusted regression models are used to fit variables that are discrete at zero and continuous at some interval of the positive real numbers. Diagnostic analysis in these models is usually performed using the randomized quantile residual, which is useful for checking the overall adequacy of a zero adjusted regression model. However, it may fail to identify some outliers. In this work, we introduce a class of residuals for outlier identification in zero adjusted regression models. Monte Carlo simulation studies and two applications suggest that one of the residuals of the class introduced here has good properties and detects outliers that are not identified by the randomized quantile residual.},
  archive      = {J_JOAS},
  author       = {Gustavo H. A. Pereira and Juliana Scudilio and Manoel Santos-Neto and Denise A. Botter and Mônica C. Sandoval},
  doi          = {10.1080/02664763.2019.1696759},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {1833-1847},
  shortjournal = {J. Appl. Stat.},
  title        = {A class of residuals for outlier identification in zero adjusted regression models},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian t-tests for correlations and partial correlations.
<em>JOAS</em>, <em>47</em>(10), 1820–1832. (<a
href="https://doi.org/10.1080/02664763.2019.1695760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we develop Bayes factor based testing procedures for the presence of a correlation or a partial correlation. The proposed Bayesian tests are obtained by restricting the class of the alternative hypotheses to maximize the probability of rejecting the null hypothesis when the Bayes factor is larger than a specified threshold. It turns out that they depend simply on the frequentist t -statistics with the associated critical values and can thus be easily calculated by using a spreadsheet in Excel and in fact by just adding one more step after one has performed the frequentist correlation tests. In addition, they are able to yield an identical decision with the frequentist paradigm, provided that the evidence threshold of the Bayesian tests is determined by the significance level of the frequentist paradigm. We illustrate the performance of the proposed procedures through simulated and real-data examples.},
  archive      = {J_JOAS},
  author       = {Min Wang and Fang Chen and Tao Lu and Jianping Dong},
  doi          = {10.1080/02664763.2019.1695760},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {1820-1832},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian t-tests for correlations and partial correlations},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust signed-rank estimation and variable selection for
semi-parametric additive partial linear models. <em>JOAS</em>,
<em>47</em>(10), 1794–1819. (<a
href="https://doi.org/10.1080/02664763.2019.1695759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fully nonparametric model may not perform well or when the researcher wants to use a parametric model but the functional form with respect to a subset of the regressors or the density of the errors is not known. This becomes even more challenging when the data contain gross outliers or unusual observations. However, in practice the true covariates are not known in advance, nor is the smoothness of the functional form. A robust model selection approach through which we can choose the relevant covariates components and estimate the smoothing function may represent an appealing tool to the solution. A weighted signed-rank estimation and variable selection under the adaptive lasso for semi-parametric partial additive models is considered in this paper. B-spline is used to estimate the unknown additive nonparametric function. It is shown that despite using B-spline to estimate the unknown additive nonparametric function, the proposed estimator has an oracle property. The robustness of the weighted signed-rank approach for data with heavy-tail, contaminated errors, and data containing high-leverage points are validated via finite sample simulations. A practical application to an economic study is provided using an updated Canadian household gasoline consumption data.},
  archive      = {J_JOAS},
  author       = {Brice M. Nguelifack and Isabelle Kemajou-Brown},
  doi          = {10.1080/02664763.2019.1695759},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {1794-1819},
  shortjournal = {J. Appl. Stat.},
  title        = {Robust signed-rank estimation and variable selection for semi-parametric additive partial linear models},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Location-scale mixed models and goodness-of-fit assessment
applied to insect ecology. <em>JOAS</em>, <em>47</em>(10), 1776–1793.
(<a href="https://doi.org/10.1080/02664763.2019.1693522">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Survival models have been extensively used to analyse time-until-event data. There is a range of extended models that incorporate different aspects, such as overdispersion/frailty, mixtures, and flexible response functions through semi-parametric models. In this work, we show how a useful tool to assess goodness-of-fit, the half-normal plot of residuals with a simulated envelope, implemented in the hnp package in R, can be used on a location-scale modelling context. We fitted a range of survival models to time-until-event data, where the event was an insect predator attacking a larva in a biological control experiment. We started with the Weibull model and then fitted the exponentiated-Weibull location-scale model with regressors both for the location and scale parameters. We performed variable selection for each model and, by producing half-normal plots with simulated envelopes for the deviance residuals of the model fits, we found that the exponentiated-Weibull fitted the data better. We then included a random effect in the exponentiated-Weibull model to accommodate correlated observations. Finally, we discuss possible implications of the results found in the case study.},
  archive      = {J_JOAS},
  author       = {R. A. Moral and J. Hinde and E. M. M. Ortega and C. G. B. Demétrio and W. A. C. Godoy},
  doi          = {10.1080/02664763.2019.1693522},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {1776-1793},
  shortjournal = {J. Appl. Stat.},
  title        = {Location-scale mixed models and goodness-of-fit assessment applied to insect ecology},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Assessing the lifetime performance index with lomax
distribution based on progressive type i interval censored sample.
<em>JOAS</em>, <em>47</em>(10), 1757–1775. (<a
href="https://doi.org/10.1080/02664763.2019.1693523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In manufacturing industry, the lifetime performance index C L C L CL is applied to evaluate the larger-the-better quality features of products. It can quickly show whether the lifetime performance of products meets the desired level. In this article, first we obtain the maximum likelihood estimator of C L C L CL with two unknown parameters in the Lomax distribution on the basis of progressive type I interval censored sample. With the MLE we proposed, some asymptotic confidence intervals of C L are discussed by using the delta method. Furthermore, the MLE of C L is used to establish the hypothesis test procedure under a given lower specification limit L . In addition, we also conduct a hypothesis test procedure when the scale parameter in the Lomax distribution is given. Finally, we illustrate the proposed inspection procedures through a real example. The testing procedure algorithms presented in this paper are efficient and easy to implement.},
  archive      = {J_JOAS},
  author       = {Xuehua Hu and Wenhao Gui},
  doi          = {10.1080/02664763.2019.1693523},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {1757-1775},
  shortjournal = {J. Appl. Stat.},
  title        = {Assessing the lifetime performance index with lomax distribution based on progressive type i interval censored sample},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Clustering of longitudinal interval-valued data via mixture
distribution under covariance separability. <em>JOAS</em>,
<em>47</em>(10), 1739–1756. (<a
href="https://doi.org/10.1080/02664763.2019.1692795">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the clustering of repeatedly measured ‘min-max’ type interval-valued data. We read the data as matrix variate data and assume the covariance matrix is separable for the model-based clustering (M-clustering). The use of a separable covariance matrix introduces several advantages in M-clustering, which include fewer samples required for a valid procedure. In addition, the numerical study shows that this structured matrix allows us to find the correct number of clusters more accurately compared to other commonly assumed covariance matrices. We apply the M-clustering with various covariance structures to clustering the longitudinal blood pressure data from the National Heart, Lung, and Blood Institute Growth and Health Study (NGHS).},
  archive      = {J_JOAS},
  author       = {Seongoh Park and Johan Lim and Hyejeong Choi and Minjung Kwak},
  doi          = {10.1080/02664763.2019.1692795},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {1739-1756},
  shortjournal = {J. Appl. Stat.},
  title        = {Clustering of longitudinal interval-valued data via mixture distribution under covariance separability},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiple imputation of longitudinal categorical data through
bayesian mixture latent markov models. <em>JOAS</em>, <em>47</em>(10),
1720–1738. (<a
href="https://doi.org/10.1080/02664763.2019.1692794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standard latent class modeling has recently been shown to provide a flexible tool for the multiple imputation (MI) of missing categorical covariates in cross-sectional studies. This article introduces an analogous tool for longitudinal studies: MI using Bayesian mixture Latent Markov (BMLM) models. Besides retaining the benefits of latent class models, i.e. respecting the (categorical) measurement scale of the variables and preserving possibly complex relationships between variables within a measurement occasion, the Markov dependence structure of the proposed BMLM model allows capturing lagged dependencies between adjacent time points, while the time-constant mixture structure allows capturing dependencies across all time points, as well as retrieving associations between time-varying and time-constant variables. The performance of the BMLM model for MI is evaluated by means of a simulation study and an empirical experiment, in which it is compared with complete case analysis and MICE. Results show good performance of the proposed method in retrieving the parameters of the analysis model. In contrast, competing methods could provide correct estimates only for some aspects of the data.},
  archive      = {J_JOAS},
  author       = {Davide Vidotto and Jeroen K. Vermunt and Katrijn Van Deun},
  doi          = {10.1080/02664763.2019.1692794},
  journal      = {Journal of Applied Statistics},
  number       = {10},
  pages        = {1720-1738},
  shortjournal = {J. Appl. Stat.},
  title        = {Multiple imputation of longitudinal categorical data through bayesian mixture latent markov models},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inference and diagnostics for heteroscedastic nonlinear
regression models under skew scale mixtures of normal distributions.
<em>JOAS</em>, <em>47</em>(9), 1690–1719. (<a
href="https://doi.org/10.1080/02664763.2019.1691158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The heteroscedastic nonlinear regression model (HNLM) is an important tool in data modeling. In this paper we propose a HNLM considering skew scale mixtures of normal (SSMN) distributions, which allows fitting asymmetric and heavy-tailed data simultaneously. Maximum likelihood (ML) estimation is performed via the expectation-maximization (EM) algorithm. The observed information matrix is derived analytically to account for standard errors. In addition, diagnostic analysis is developed using case-deletion measures and the local influence approach. A simulation study is developed to verify the empirical distribution of the likelihood ratio statistic, the power of the homogeneity of variances test and a study for misspecification of the structure function. The method proposed is also illustrated by analyzing a real dataset.},
  archive      = {J_JOAS},
  author       = {Clécio da Silva Ferreira and Víctor H. Lachos and Aldo M. Garay},
  doi          = {10.1080/02664763.2019.1691158},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {1690-1719},
  shortjournal = {J. Appl. Stat.},
  title        = {Inference and diagnostics for heteroscedastic nonlinear regression models under skew scale mixtures of normal distributions},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonconformance probability of an air quality index.
<em>JOAS</em>, <em>47</em>(9), 1676–1689. (<a
href="https://doi.org/10.1080/02664763.2019.1690639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Air pollution is one of the most important global environmental issues. Taiwan Environmental Protection Agency (EPA) is currently using an Air Quality Index (AQI) to measure and monitor its national air quality. The main objective of this study is to assess the air quality per hour every month in Taichung City of Taiwan from 2014 to 2016 based on the nonconformance probability of the AQI index. The nonconformance probability is defined as the probability that a characteristic of interest falls outside of an acceptance region. A lower confidence bound for the nonconformance probability is applied to test whether the AQI index value exceeds a warning threshold, and then the government could issue warnings according to the decision made by such statistical inference. An unbalanced two-way random effects model is presented for fitting the AQI index values. We evaluate three different lower confidence bound construction methods, including a t-based, an adjusted t-based and a generalized pivotal quantity (GPQ) based methods, through a detailed simulation study. Finally, a hybrid method of the t-based and the adjusted t-based estimators is recommended for practical use.},
  archive      = {J_JOAS},
  author       = {Tsai-Yu Lin and Chen-Tuo Liao and Yen-Ya Liao},
  doi          = {10.1080/02664763.2019.1690639},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {1676-1689},
  shortjournal = {J. Appl. Stat.},
  title        = {Nonconformance probability of an air quality index},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multivariate process dispersion monitoring without
subgrouping. <em>JOAS</em>, <em>47</em>(9), 1652–1675. (<a
href="https://doi.org/10.1080/02664763.2019.1688262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The memory-type adaptive and non-adaptive control charts are among the best control charts for detecting small-to-moderate changes in the process parameter(s). In this paper, we propose the Crosier CUSUM (CCUSUM), EWMA, adaptive CCUSUM (ACCUSUM) and adaptive EWMA (AEWMA) charts for efficiently monitoring the changes in the covariance matrix of a multivariate normal process without subgrouping. Using extensive Monte Carlo simulations, the length characteristics of these control charts are computed. It turns out that the ACCUSUM and AEWMA charts perform uniformly and substantially better than the CCUSUM and EWMA charts when detecting a range of shift sizes in the covariance matrix. Moreover, the AEWMA chart outperforms the ACCUSUM chart. A real dataset is used to explain the implementation of the proposed control charts.},
  archive      = {J_JOAS},
  author       = {Abdul Haq and Michael B. C. Khoo},
  doi          = {10.1080/02664763.2019.1688262},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {1652-1675},
  shortjournal = {J. Appl. Stat.},
  title        = {Multivariate process dispersion monitoring without subgrouping},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the improved estimation of a function of the scale
parameter of an exponential distribution based on doubly censored
sample. <em>JOAS</em>, <em>47</em>(9), 1637–1651. (<a
href="https://doi.org/10.1080/02664763.2019.1688261">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the present article, we have studied the estimation of entropy, that is, a function of scale parameter ln σ ln ⁡ σ ln⁡σ of an exponential distribution based on doubly censored sample when the location parameter is restricted to positive real line. The estimation problem is studied under a general class of bowl-shaped non monotone location invariant loss functions. It is established that the best affine equivariant estimator (BAEE) is inadmissible by deriving an improved estimator. This estimator is non-smooth. Further, we have obtained a smooth improved estimator. A class of estimators is considered and sufficient conditions are derived under which these estimators improve upon the BAEE. In particular, using these results we have obtained the improved estimators for the squared error and the linex loss functions. Finally, we have compared the risk performance of the proposed estimators numerically. One data analysis has been performed for illustrative purposes.},
  archive      = {J_JOAS},
  author       = {Lakshmi Kanta Patra},
  doi          = {10.1080/02664763.2019.1688261},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {1637-1651},
  shortjournal = {J. Appl. Stat.},
  title        = {On the improved estimation of a function of the scale parameter of an exponential distribution based on doubly censored sample},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling material stress using integrated gaussian markov
random fields. <em>JOAS</em>, <em>47</em>(9), 1616–1636. (<a
href="https://doi.org/10.1080/02664763.2019.1686131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The equations of a physical constitutive model for material stress within tantalum grains were solved numerically using a tetrahedrally meshed volume. The resulting output included a scalar vonMises stress for each of the more than 94,000 tetrahedra within the finite element discretization. In this paper, we define an intricate statistical model for the spatial field of vonMises stress which uses the given grain geometry in a fundamental way. Our model relates the three-dimensional field to integrals of latent stochastic processes defined on the vertices of the one- and two-dimensional grain boundaries. An intuitive neighborhood structure of the said boundary nodes suggested the use of a latent Gaussian Markov random field (GMRF). However, despite the potential for computational gains afforded by GMRFs, the integral nature of our model and the sheer number of data points pose substantial challenges for a full Bayesian analysis. To overcome these problems and encourage efficient exploration of the posterior distribution, a number of techniques are now combined: parallel computing, sparse matrix methods, and a modification of a block update strategy within the sampling routine. In addition, we use an auxiliary variables approach to accommodate the presence of outliers in the data.},
  archive      = {J_JOAS},
  author       = {Peter W. Marcy and Scott A. Vander Wiel and Curtis B. Storlie and Veronica Livescu and Curt A. Bronkhorst},
  doi          = {10.1080/02664763.2019.1686131},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {1616-1636},
  shortjournal = {J. Appl. Stat.},
  title        = {Modeling material stress using integrated gaussian markov random fields},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Copula diagnostics for asymmetries and conditional
dependence. <em>JOAS</em>, <em>47</em>(9), 1587–1615. (<a
href="https://doi.org/10.1080/02664763.2019.1685080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vine copulas are constructed from a sequence of trees to represent dependence and conditional dependence, and a set of bivariate copulas that are applied to univariate distributions in tree 1 and to conditional univariate distributions in subsequent trees. Diagnostic methods based on measures of dependence and tail asymmetry are proposed to guide the choice of parametric bivariate copula families assigned to the edges of the trees in the vine and to assess whether a copula is constant over the conditioning value(s) for trees 2 and higher. The measures are conditional measures applied to bivariate conditional distributions in trees 2 and higher. If the diagnostic methods suggest the existence of reflection asymmetry, permutation asymmetry and possible asymmetric tail dependence, then three- or four-parameter bivariate copula families might be needed. Moreover, if the conditional dependence measures or asymmetry measures in trees 2 and up are not constant over the conditioning value(s), then non-constant copulas should be considered. We illustrate the use of the diagnostic methods for a gamma factor model and two real datasets. The examples show that better models are attained by using asymmetric and non-constant copulas under the guidance of the diagnostic tools.},
  archive      = {J_JOAS},
  author       = {Bo Chang and Harry Joe},
  doi          = {10.1080/02664763.2019.1685080},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {1587-1615},
  shortjournal = {J. Appl. Stat.},
  title        = {Copula diagnostics for asymmetries and conditional dependence},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modified likelihood ratio tests for unit gamma regressions.
<em>JOAS</em>, <em>47</em>(9), 1562–1586. (<a
href="https://doi.org/10.1080/02664763.2019.1683152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regression analyses are commonly performed with doubly limited continuous dependent variables; for instance, when modeling the behavior of rates, proportions and income concentration indices. Several models are available in the literature for use with such variables, one of them being the unit gamma regression model. In all such models, parameter estimation is typically performed using the maximum likelihood method and testing inferences on the model&#39;s parameters are usually based on the likelihood ratio test. Such a test can, however, deliver quite imprecise inferences when the sample size is small. In this paper, we propose two modified likelihood ratio test statistics for use with the unit gamma regressions that deliver much more accurate inferences when the number of data points in small. Numerical (i.e. simulation) evidence is presented for both fixed dispersion and varying dispersion models, and also for tests that involve nonnested models. We also present and discuss two empirical applications.},
  archive      = {J_JOAS},
  author       = {Ana C. Guedes and Francisco Cribari-Neto and Patrícia L. Espinheira},
  doi          = {10.1080/02664763.2019.1683152},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {1562-1586},
  shortjournal = {J. Appl. Stat.},
  title        = {Modified likelihood ratio tests for unit gamma regressions},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Statistical inference based on generalized lindley record
values. <em>JOAS</em>, <em>47</em>(9), 1543–1561. (<a
href="https://doi.org/10.1080/02664763.2019.1683153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper addresses the problems of frequentist and Bayesian estimation for the unknown parameters of generalized Lindley distribution based on lower record values. We first derive the exact explicit expressions for the single and product moments of lower record values, and then use these results to compute the means, variances and covariance between two lower record values. We next obtain the maximum likelihood estimators and associated asymptotic confidence intervals. Furthermore, we obtain Bayes estimators under the assumption of gamma priors on both the shape and the scale parameters of the generalized Lindley distribution, and associated the highest posterior density interval estimates. The Bayesian estimation is studied with respect to both symmetric (squared error) and asymmetric (linear-exponential (LINEX)) loss functions. Finally, we compute Bayesian predictive estimates and predictive interval estimates for the future record values. To illustrate the findings, one real data set is analyzed, and Monte Carlo simulations are performed to compare the performances of the proposed methods of estimation and prediction.},
  archive      = {J_JOAS},
  author       = {Sukhdev Singh and Sanku Dey and Devendra Kumar},
  doi          = {10.1080/02664763.2019.1683153},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {1543-1561},
  shortjournal = {J. Appl. Stat.},
  title        = {Statistical inference based on generalized lindley record values},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The role of the p-value in the multitesting problem.
<em>JOAS</em>, <em>47</em>(9), 1529–1542. (<a
href="https://doi.org/10.1080/02664763.2019.1682128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern science frequently involves the analysis of large amount of quantitative information and the simultaneous testing of thousands or even hundreds of thousands null hypotheses. In this context, sometimes, naive deductions derived from the statistical reports substitute the rational thinking. The reproducibility crisis is a direct consequence of the misleading statistical conclusions. In this paper, the authors revisit some of the controversies on the implications derived from the statistical hypothesis testing. They focus on the role of the p -value on the massive multitesting problem and the loss of its standard probabilistic interpretation. The analogy between the hypothesis tests and the usual diagnostic process (both involve a decision-making) is used to point out some limitations in the probabilistic p -value interpretation and to introduce the receiver-operating characteristic, ROC, curve as a useful tool in the large-scale multitesting context. The analysis of the well-known Hedenfalk data illustrates the problem.},
  archive      = {J_JOAS},
  author       = {P. Martínez-Camblor and S. Pérez-Fernández and S. Díaz-Coto},
  doi          = {10.1080/02664763.2019.1682128},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {1529-1542},
  shortjournal = {J. Appl. Stat.},
  title        = {The role of the p-value in the multitesting problem},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On maximum likelihood estimation of the semi-parametric cox
model with time-varying covariates. <em>JOAS</em>, <em>47</em>(9),
1511–1528. (<a
href="https://doi.org/10.1080/02664763.2019.1681946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Including time-varying covariates is a popular extension to the Cox model and a suitable approach for dealing with non-proportional hazards. However, partial likelihood (PL) estimation of this model has three shortcomings: (i) estimated regression coefficients can be less accurate in small samples with heavy censoring; (ii) the baseline hazard is not directly estimated and (iii) a covariance matrix for both the regression coefficients and the baseline hazard is not easily produced. We address these by developing a maximum likelihood (ML) approach to jointly estimate regression coefficients and baseline hazard using a constrained optimisation ensuring the latter&#39;s non-negativity. We demonstrate asymptotic properties of these estimates and show via simulation their increased accuracy compared to PL estimates in small samples and show our method produces smoother baseline hazard estimates than the Breslow estimator. Finally, we apply our method to two examples, including an important real-world financial example to estimate time to default for retail home loans. We demonstrate using our ML estimate for the baseline hazard can give much clearer corroboratory evidence of the ‘humped hazard’, whereby the risk of loan default rises to a peak and then later falls.},
  archive      = {J_JOAS},
  author       = {Mark Thackham and Jun Ma},
  doi          = {10.1080/02664763.2019.1681946},
  journal      = {Journal of Applied Statistics},
  number       = {9},
  pages        = {1511-1528},
  shortjournal = {J. Appl. Stat.},
  title        = {On maximum likelihood estimation of the semi-parametric cox model with time-varying covariates},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reliability analysis for gap null gate based on model
comparison criterion. <em>JOAS</em>, <em>47</em>(8), 1493–1509. (<a
href="https://doi.org/10.1080/02664763.2019.1680615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The length of the gap is the key factor affecting its reliability. Based on the mechanism of the gap null gate, this paper regards the two endpoint thresholds of the gap length as bivariate random variables and establishes successful response models. Score test statistic is presented to test the correlation coefficient. The DIC criterion is also provided to compare the models. With the experimental data of the gap null gate, we build Probit model and Logit model as the successful response models, and prove that the correlation coefficients in the both models can be regarded as 0. By comparing the DIC value, we find that the Probit model is more suitable to describe the distribution of the endpoint thresholds of the reliability window. Finally, both the point estimation and interval estimation results of the reliability window are given to illustrate the feasibility of the method shown in the paper.},
  archive      = {J_JOAS},
  author       = {Mei Li and Houbao Xu},
  doi          = {10.1080/02664763.2019.1680615},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1493-1509},
  shortjournal = {J. Appl. Stat.},
  title        = {Reliability analysis for gap null gate based on model comparison criterion},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Designing a control chart of extended EWMA statistic based
on multiple dependent state sampling. <em>JOAS</em>, <em>47</em>(8),
1482–1492. (<a
href="https://doi.org/10.1080/02664763.2019.1676405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we presented a memory type control chart (CC) based on multiple dependent state (MDS) sampling to pinpoint the slight variation in the process mean for the quality trait of normal distribution (ND). Two pairs of control limits denominated as internal and external control limits are derived using under control mean and variance. The essential steps are taken to get the value of average run length (ARL) for stable and disturb process. Various tables of ARLs are erected using different smoothing constants, shifts and MDS parameter. Comparisons are established to assess the effectiveness of initiated CC with the various existing CC in term of ARL. It has been ascertained that offered CC manifest the best performance in searching out the diminutive changes in the process mean. Two examples, one is based on simulation study and other is related to real-life data, have been discussed for its practical purpose.},
  archive      = {J_JOAS},
  author       = {Muhammad Naveed and Muhammad Azam and Nasrullah Khan and Muhammad Aslam},
  doi          = {10.1080/02664763.2019.1676405},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1482-1492},
  shortjournal = {J. Appl. Stat.},
  title        = {Designing a control chart of extended EWMA statistic based on multiple dependent state sampling},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A comparative analysis of clustering algorithms to identify
the homogeneous rainfall gauge stations of bangladesh. <em>JOAS</em>,
<em>47</em>(8), 1460–1481. (<a
href="https://doi.org/10.1080/02664763.2019.1675606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dealing with individual rainfall station is time consuming as well as prone to more variation. It seems reasonable and advantageous to deal with a group of homogeneous stations rather than an individual station. Such groups can be identified using clustering algorithms, techniques used in the multivariate data analysis. Particularly, in this study, covering both hard and soft clustering approaches, three clustering algorithms namely Agglomerative hierarchical, K -means clustering and Fuzzy C -means methods are chosen due to their popularity. These algorithms are applied over precipitation data recorded by the Bangladesh Meteorology Department, and a comparison among the algorithms is made. Annual and seasonal precipitations from 1977 to 2012 recorded in 30 stations are used in this study. Optimal numbers of clusters in the four precipitation series are determined using the Gap statistic for K -means clustering and using the extended Gap statistic for Fuzzy C -means clustering, and are found as 3, 1, 3 and 2 for annual, pre-monsoon, monsoon and post-monsoon, respectively. This study investigates the clustering methods in terms of the similarity, members and homogeneity, among the clusters formed. The clusters are also characterized to see how they are distributed. Moreover, in terms of cluster homogeneity, Fuzzy C -means algorithm outperforms the other clustering methods.},
  archive      = {J_JOAS},
  author       = {Mohammad Samsul Alam and Sangita Paul},
  doi          = {10.1080/02664763.2019.1675606},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1460-1481},
  shortjournal = {J. Appl. Stat.},
  title        = {A comparative analysis of clustering algorithms to identify the homogeneous rainfall gauge stations of bangladesh},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Physically constrained spatiotemporal modeling: Generating
clear-sky constructions of land surface temperature from sparse,
remotely sensed satellite data. <em>JOAS</em>, <em>47</em>(8),
1439–1459. (<a
href="https://doi.org/10.1080/02664763.2019.1681384">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Satellite remote-sensing is used to collect important atmospheric and geophysical data at various spatial resolutions, providing insight into spatiotemporal surface and climate variability globally. These observations are often plagued with missing spatial and temporal information of Earth&#39;s surface due to (1) cloud cover at the time of a satellite passing and (2) infrequent passing of polar-orbiting satellites. While many methods are available to model missing data in space and time, in the case of land surface temperature (LST) from thermal infrared remote sensing, these approaches generally ignore the temporal pattern called the ‘diurnal cycle’ which physically constrains temperatures to peak in the early afternoon and reach a minimum at sunrise. In order to infill an LST dataset, we parameterize the diurnal cycle into a functional form with unknown spatiotemporal parameters. Using multiresolution spatial basis functions, we estimate these parameters from sparse satellite observations to reconstruct an LST field with continuous spatial and temporal distributions. These estimations may then be used to better inform scientists of spatiotemporal thermal patterns over relatively complex domains. The methodology is demonstrated using data collected by MODIS on NASA&#39;s Aqua and Terra satellites over both Houston, TX and Phoenix, AZ USA.},
  archive      = {J_JOAS},
  author       = {Gavin Q. Collins and Matthew J. Heaton and Leiqiu Hu},
  doi          = {10.1080/02664763.2019.1681384},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1439-1459},
  shortjournal = {J. Appl. Stat.},
  title        = {Physically constrained spatiotemporal modeling: Generating clear-sky constructions of land surface temperature from sparse, remotely sensed satellite data},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving the bridge structure by using linear failure rate
distribution. <em>JOAS</em>, <em>47</em>(8), 1423–1438. (<a
href="https://doi.org/10.1080/02664763.2019.1679098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a system of five components is studied; one of these components is a bridge network component. Each of these components has a non-constant failure rate. The system components have linear failure rate lifetime distribution. The given system is improved by using three methods: reduction, warm standby with perfect switch and warm standby with imperfect switch. The reliability equivalence factors of the bridge structure system are obtained. The γ -fractiles are obtained to compare the original system with these improved systems. Finally, we present numerical results to show the difference between these methods.},
  archive      = {J_JOAS},
  author       = {Abdelfattah Mustafa},
  doi          = {10.1080/02664763.2019.1679098},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1423-1438},
  shortjournal = {J. Appl. Stat.},
  title        = {Improving the bridge structure by using linear failure rate distribution},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inference on a progressive type i interval-censored
truncated normal distribution. <em>JOAS</em>, <em>47</em>(8), 1402–1422.
(<a href="https://doi.org/10.1080/02664763.2019.1679096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the problem of making statistical inference for a truncated normal distribution under progressive type I interval censoring. We obtain maximum likelihood estimators of unknown parameters using the expectation-maximization algorithm and in sequel, we also compute corresponding midpoint estimates of parameters. Estimation based on the probability plot method is also considered. Asymptotic confidence intervals of unknown parameters are constructed based on the observed Fisher information matrix. We obtain Bayes estimators of parameters with respect to informative and non-informative prior distributions under squared error and linex loss functions. We compute these estimates using the importance sampling procedure. The highest posterior density intervals of unknown parameters are constructed as well. We present a Monte Carlo simulation study to compare the performance of proposed point and interval estimators. Analysis of a real data set is also performed for illustration purposes. Finally, inspection times and optimal censoring plans based on the expected Fisher information matrix are discussed.},
  archive      = {J_JOAS},
  author       = {Chandrakant Lodhi and Yogesh Mani Tripathi},
  doi          = {10.1080/02664763.2019.1679096},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1402-1422},
  shortjournal = {J. Appl. Stat.},
  title        = {Inference on a progressive type i interval-censored truncated normal distribution},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comparison of disease prevalence in two populations under
double-sampling scheme with two fallible classifiers. <em>JOAS</em>,
<em>47</em>(8), 1375–1401. (<a
href="https://doi.org/10.1080/02664763.2019.1679727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A disease prevalence can be estimated by classifying subjects according to whether they have the disease. When gold-standard tests are too expensive to be applied to all subjects, partially validated data can be obtained by double-sampling in which all individuals are classified by a fallible classifier, and some of individuals are validated by the gold-standard classifier. However, it could happen in practice that such infallible classifier does not available. In this article, we consider two models in which both classifiers are fallible and propose four asymptotic test procedures for comparing disease prevalence in two groups. Corresponding sample size formulae and validated ratio given the total sample sizes are also derived and evaluated. Simulation results show that (i) Score test performs well and the corresponding sample size formula is also accurate in terms of the empirical power and size in two models; (ii) the Wald test based on the variance estimator with parameters estimated under the null hypothesis outperforms the others even under small sample sizes in Model II, and the sample size estimated by this test is also accurate; (iii) the estimated validated ratios based on all tests are accurate. The malarial data are used to illustrate the proposed methodologies.},
  archive      = {J_JOAS},
  author       = {Shi-Fang Qiu and Jie He and Ji-Ran Tao and Man-Lai Tang and Wai-Yin Poon},
  doi          = {10.1080/02664763.2019.1679727},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1375-1401},
  shortjournal = {J. Appl. Stat.},
  title        = {Comparison of disease prevalence in two populations under double-sampling scheme with two fallible classifiers},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An MCMC computational approach for a continuous time
state-dependent regime switching diffusion process. <em>JOAS</em>,
<em>47</em>(8), 1354–1374. (<a
href="https://doi.org/10.1080/02664763.2019.1677573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-dependent regime switching diffusion processes or hybrid switching diffusion (HSD) processes are hard to simulate with classical methods which leads us to adopt a Markov chain Monte Carlo (MCMC) Bayesian approach very convenient to estimate complicated models such as the HSD one. In the HSD, the diffusion component is dependent on the switching discrete hidden regimes and the transition rates of the regime switching are dependent on the diffusion observations. Since in reality phenomena are only observed in discrete times, data imputation is called for to create more observations so as to have good approximations for the density of the diffusion process. Three categories of entities will be computed in a Bayesian context: The latent imputed observations, the regime switching states, and the parameters of the models. The latent imputed data is updated at random time intervals in block using a Metropolis Hastings algorithm. The switching states are computed by an adaptation of a forward filtering backward smoothing algorithm to the HSD model. The parameters are estimated after prior specifications and conditional posterior densities formulation using Gibbs sampler or Metropolis Hastings algorithm.},
  archive      = {J_JOAS},
  author       = {El Houcine Hibbah and Hamid El Maroufy and Christiane Fuchs and Taib Ziad},
  doi          = {10.1080/02664763.2019.1677573},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1354-1374},
  shortjournal = {J. Appl. Stat.},
  title        = {An MCMC computational approach for a continuous time state-dependent regime switching diffusion process},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SIMPCA: A framework for rotating and sparsifying principal
components. <em>JOAS</em>, <em>47</em>(8), 1325–1353. (<a
href="https://doi.org/10.1080/02664763.2019.1676404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an algorithmic framework for computing sparse components from rotated principal components. This methodology, called SIMPCA, is useful to replace the unreliable practice of ignoring small coefficients of rotated components when interpreting them. The algorithm computes genuinely sparse components by projecting rotated principal components onto subsets of variables. The so simplified components are highly correlated with the corresponding components. By choosing different simplification strategies different sparse solutions can be obtained which can be used to compare alternative interpretations of the principal components. We give some examples of how effective simplified solutions can be achieved with SIMPCA using some publicly available data sets.},
  archive      = {J_JOAS},
  author       = {Giovanni Maria Merola},
  doi          = {10.1080/02664763.2019.1676404},
  journal      = {Journal of Applied Statistics},
  number       = {8},
  pages        = {1325-1353},
  shortjournal = {J. Appl. Stat.},
  title        = {SIMPCA: A framework for rotating and sparsifying principal components},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Correction. <em>JOAS</em>, <em>47</em>(7), i. (<a
href="https://doi.org/10.1080/02664763.2019.1706803">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOAS},
  doi          = {10.1080/02664763.2019.1706803},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {i},
  shortjournal = {J. Appl. Stat.},
  title        = {Correction},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Application of measurement error models to correct for
systematic differences among readers and vendors in echocardiography
measurements: The CARDIA study. <em>JOAS</em>, <em>47</em>(7),
1315–1324. (<a
href="https://doi.org/10.1080/02664763.2019.1686133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We illustrate the application of linear measurement error models to calibrate echocardiography measurements acquired 20 years apart in the CARDIA study. Of 4242 echocardiograms acquired at Year-5 (1990–1991), 36\% were reread 20 years later. Left ventricular (LV) mass and 8 other measurements were assessed. A machine reproducibility study including 96 additional patients also compared Year-5 and Year-25 equipment. A linear measurement error model was developed to calibrate the original Year-5 measurements, incorporating the additional Year-5 reread and machine reproducibility study data, and adjusting for differences among readers and machines. Median (quartiles) of original Year-5 LV mass was 144.4 (117.6, 174.2) g before and 129.9 (103.8, 158.6) g, after calibration. The correlation between original and calibrated LV mass was 0.989 (95\% confidence interval: 0.988, 0.990). The original and calibrated measurements had similar distributions. Additional comparisons of original and calibrated data supported the use of the model. We conclude that systematic differences among readers and machines have been accounted for, and that the calibrated Year-5 measurements can be used in future longitudinal comparisons. It is hoped that this paper will encourage the wider application of measurement error models.},
  archive      = {J_JOAS},
  author       = {Aisha Betoko and Chike Nwabuo and Bharath Ambale Venkatesh and Erin P. Ricketts and Sejong Bae and Colin Wu and Samuel S. Gidding and Kiang Liu and João A. C. Lima and Christopher Cox},
  doi          = {10.1080/02664763.2019.1686133},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1315-1324},
  shortjournal = {J. Appl. Stat.},
  title        = {Application of measurement error models to correct for systematic differences among readers and vendors in echocardiography measurements: The CARDIA study},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Identifying mediating variables with graphical models: An
application to the study of causal pathways in people living with HIV.
<em>JOAS</em>, <em>47</em>(7), 1298–1314. (<a
href="https://doi.org/10.1080/02664763.2019.1669543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We empirically demonstrate that graphical models can be a valuable tool in the identification of mediating variables in causal pathways. We make use of graphical models to elucidate the causal pathway through which the treatment influences the levels of fatigue and weakness in people living with HIV (PLHIV) based on a secondary analysis of a categorical dataset collected in a behavioral clinical trial: is weakness a mediator for the treatment and fatigue, or is fatigue a mediator for the treatment and weakness? Causal mediation analysis could not offer any definite answers to these questions.},
  archive      = {J_JOAS},
  author       = {Adrian Dobra and Katherine Buhikire and Joachim G. Voss},
  doi          = {10.1080/02664763.2019.1669543},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1298-1314},
  shortjournal = {J. Appl. Stat.},
  title        = {Identifying mediating variables with graphical models: An application to the study of causal pathways in people living with HIV},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Principal points analysis via p-median problem for binary
data. <em>JOAS</em>, <em>47</em>(7), 1282–1297. (<a
href="https://doi.org/10.1080/02664763.2019.1675605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analysis with principal points is a useful statistical tool for summarizing large data. In this paper, we propose a subgradient-based algorithm to calculate a set of principal points for multivariate binary data by the formulating it as a p -median problem. This enables us to find a globally optimal set of principal points or an ε -optimal solution in the middle of the calculation by combining an upper bound found using the greedy method. This algorithm is an iterative procedure where each iteration can be calculated in an efficient manner. We investigate the applicability of the proposed framework with questionnaire data and arXiv co-authors data.},
  archive      = {J_JOAS},
  author       = {Haruka Yamashita and Yoshinobu Kawahara},
  doi          = {10.1080/02664763.2019.1675605},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1282-1297},
  shortjournal = {J. Appl. Stat.},
  title        = {Principal points analysis via p-median problem for binary data},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Assigning scores for ordered categorical responses.
<em>JOAS</em>, <em>47</em>(7), 1261–1281. (<a
href="https://doi.org/10.1080/02664763.2019.1674790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deciding on the best statistical method to apply when the response variable is ordinal is essential because the way the categories are ordered in the data is relevant as it could change the results of the analysis. Although the models for continuous variables have similarities to those for ordinal variables, this paper presents the advantages of the use of the ordering information on the outcomes with methods developed for modeling ordinal data such as the ordered stereotype model. The novelty of this article lies in showing the dangers of assigning equally spaced scores to ordered response categories in statistical analysis, which are illustrated with a simulation study and a case study. We propose a new way to use the score parameters, which incorporates the fitted spacing dictated by the data. Additionally, this article uses score parameter estimates in the ordered stereotype model to propose a new measure to calculate continuous medians in the raw data: the adjusted c-median . It benefits the general audience who can easily understand the median as a summary statistic. Supplementary materials for this article are available online.},
  archive      = {J_JOAS},
  author       = {Daniel Fernández and Ivy Liu and Roy Costilla and Peter Yongqi Gu},
  doi          = {10.1080/02664763.2019.1674790},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1261-1281},
  shortjournal = {J. Appl. Stat.},
  title        = {Assigning scores for ordered categorical responses},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A measure of asymmetry for ordinal square contingency tables
with an application to modified LANZA score data. <em>JOAS</em>,
<em>47</em>(7), 1251–1260. (<a
href="https://doi.org/10.1080/02664763.2019.1673325">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical researches, various clinical scales with ordered categories are used to evaluate the efficacy and safety/toxicity of treatments. Such the clinical scales are sometimes summarized on the transition between the baseline and the study end point as a square contingency table. Also, clinical scales may be reclassified into three groups. However, the cutpoints can be varied depending on clinical researches or clinicians. Hence, this paper proposes a measure which is expressed by using same weights for collapsed tables and which can see the directionality for two kinds of asymmetries. Also, this paper shows an application of the proposed measure to clinical data, and that the proposed measure is a useful statistical method for analyzing ordered categorical data.},
  archive      = {J_JOAS},
  author       = {Satoru Shinoda and Kouji Yamamoto and Kouji Tahata and Sadao Tomizawa},
  doi          = {10.1080/02664763.2019.1673325},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1251-1260},
  shortjournal = {J. Appl. Stat.},
  title        = {A measure of asymmetry for ordinal square contingency tables with an application to modified LANZA score data},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Geometry-based distance for clustering amino acids.
<em>JOAS</em>, <em>47</em>(7), 1235–1250. (<a
href="https://doi.org/10.1080/02664763.2019.1673324">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering amino acids is one of the most challenging problems in functional and structural prediction of protein. Previous studies have proposed clusters based on measurements of physical and biochemical characteristics of the amino acids such as volume, area, hydrophilicity, polarity, hydrogen bonding, shape, and charge. These characteristics, although important, are less directly related to the protein structure compared to geometrical characteristics such as dihedral angles between amino acids. We propose using the p -value from a test of equality of dihedral-angle distributions as the basis of a distance measure for the clustering. In this novel approach, an energy test is modified to deal with bivariate angular data and the p -value is obtained via a permutation method. The results indicate that the clusters of amino acids have sensible interpretation where Glycine, Proline, and Asparagine each forms a distinct cluster. A simulation study suggests that this approach has good working characteristics to cluster amino acids.},
  archive      = {J_JOAS},
  author       = {Samira F. Abushilah and Charles C. Taylor and Arief Gusnanto},
  doi          = {10.1080/02664763.2019.1673324},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1235-1250},
  shortjournal = {J. Appl. Stat.},
  title        = {Geometry-based distance for clustering amino acids},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Prediction of appointment no-shows using electronic health
records. <em>JOAS</em>, <em>47</em>(7), 1220–1234. (<a
href="https://doi.org/10.1080/02664763.2019.1672631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Appointment no-shows have a negative impact on patient health and have caused substantial loss in resources and revenue for health care systems. Intervention strategies to reduce no-show rates can be more effective if targeted to the subpopulations of patients with higher risk of not showing to their appointments. We use electronic health records (EHR) from a large medical center to predict no-show patients based on demographic and health care features. We apply sparse Bayesian modeling approaches based on Lasso and automatic relevance determination to predict and identify the most relevant risk factors of no-show patients at a provider level.},
  archive      = {J_JOAS},
  author       = {Qiaohui Lin and Brenda Betancourt and Benjamin A. Goldstein and Rebecca C. Steorts},
  doi          = {10.1080/02664763.2019.1672631},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1220-1234},
  shortjournal = {J. Appl. Stat.},
  title        = {Prediction of appointment no-shows using electronic health records},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Jackknife empirical likelihood test for testing one-sided
lévy distribution. <em>JOAS</em>, <em>47</em>(7), 1208–1219. (<a
href="https://doi.org/10.1080/02664763.2019.1672630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on U-empirical process we construct a jackknife empirical likelihood-based test for testing one-sided Lévy distribution. Adjusted jackknife empirical likelihood test also developed. The simulation study shows that the proposed tests have very good power for various alternatives. Finally, we illustrate the test procedure using three real data sets.},
  archive      = {J_JOAS},
  author       = {Deepesh Bhati and Sudheesh K. Kattumannil},
  doi          = {10.1080/02664763.2019.1672630},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1208-1219},
  shortjournal = {J. Appl. Stat.},
  title        = {Jackknife empirical likelihood test for testing one-sided lévy distribution},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sensitivity index to measure dependence on parameters for
rankings and top-k rankings. <em>JOAS</em>, <em>47</em>(7), 1191–1207.
(<a href="https://doi.org/10.1080/02664763.2019.1671963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a multivariate framework, ranking a data set can be done by using an aggregation function in order to obtain a global score for each individual, and then by using these scores to rank the individuals. The choice of the aggregation function (e.g. a weighted sum) and the choice of the parameters of the function (e.g. the weights) may have a great influence on the obtained ranking. We introduce in this communication a ratio index that can quantify the sensitivity of the data set ranking up to a change of weights. This index is investigated in the general case and in the restricted case of top- k rankings. We also illustrate the interest to use such an index to analyse ranked data sets.},
  archive      = {J_JOAS},
  author       = {Antoine Rolland and Jairo Cugliari},
  doi          = {10.1080/02664763.2019.1671963},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1191-1207},
  shortjournal = {J. Appl. Stat.},
  title        = {Sensitivity index to measure dependence on parameters for rankings and top-k rankings},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Temperature prediction based on a space–time
regression-kriging model. <em>JOAS</em>, <em>47</em>(7), 1168–1190. (<a
href="https://doi.org/10.1080/02664763.2019.1671962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many phenomena exist in the space–time domain, often with a low data sampling rate and sparsely distributed network of observed points. Therefore, spatio-temporal interpolation with high accuracy is necessary. In this paper, a space–time regression-kriging model was introduced and applied to monthly average temperature data. First, a time series decomposition was applied for each station, and a multiple linear regression model was used to fit space–time trends. Second, a valid nonseparable spatio-temporal variogram function was utilized to describe similarities of the residuals in space–time. Finally, space–time kriging was applied to predict monthly air temperature. Jackknife techniques were used to predict the monthly temperature at all stations, with correlation coefficients between predictions and observed data very close to 1. Moreover, to evaluate the advantages of space–time kriging, pure time forecasting also was executed employing an autoregressive integrated moving average (ARIMA) model. The results of these two methods show that both mean absolute error (MAE) and root-mean-square error (RMSE) of space–time prediction are much lower than those of the pure time forecasting. The estimated temperature curves for stations also show that the former present a conspicuous improvement in interpolation accuracy when compared with the latter.},
  archive      = {J_JOAS},
  author       = {Sha Li and Daniel A. Griffith and Hong Shu},
  doi          = {10.1080/02664763.2019.1671962},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1168-1190},
  shortjournal = {J. Appl. Stat.},
  title        = {Temperature prediction based on a space–time regression-kriging model},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evaluation of robust outlier detection methods for
zero-inflated complex data. <em>JOAS</em>, <em>47</em>(7), 1144–1167.
(<a href="https://doi.org/10.1080/02664763.2019.1671961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Outlier detection can be seen as a pre-processing step for locating data points in a data sample, which do not conform to the majority of observations. Various techniques and methods for outlier detection can be found in the literature dealing with different types of data. However, many data sets are inflated by true zeros and, in addition, some components/variables might be of compositional nature. Important examples of such data sets are the Structural Earnings Survey, the Structural Business Statistics, the European Statistics on Income and Living Conditions, tax data or – as in this contribution – household expenditure data which are used, for example, to estimate the Purchase Power Parity of a country. In this work, robust univariate and multivariate outlier detection methods are compared by a complex simulation study that considers various challenges included in data sets, namely structural (true) zeros, missing values, and compositional variables. These circumstances make it difficult or impossible to flag true outliers and influential observations by well-known outlier detection methods. Our aim is to assess the performance of outlier detection methods in terms of their effectiveness to identify outliers when applied to challenging data sets such as the household expenditures data surveyed all over the world. Moreover, different methods are evaluated through a close-to-reality simulation study. Differences in performance of univariate and multivariate robust techniques for outlier detection and their shortcomings are reported. We found that robust multivariate methods outperform robust univariate methods. The best performing methods in finding the outliers and in providing a low false discovery rate were found to be the generalized S estimators (GSE), the BACON-EEM algorithm and a compositional method (CoDa-Cov). In addition, these methods performed also best when the outliers are imputed based on the corresponding outlier detection method and indicators are estimated from the data sets.},
  archive      = {J_JOAS},
  author       = {M. Templ and J. Gussenbauer and P. Filzmoser},
  doi          = {10.1080/02664763.2019.1671961},
  journal      = {Journal of Applied Statistics},
  number       = {7},
  pages        = {1144-1167},
  shortjournal = {J. Appl. Stat.},
  title        = {Evaluation of robust outlier detection methods for zero-inflated complex data},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Correction. <em>JOAS</em>, <em>47</em>(6), i–ii. (<a
href="https://doi.org/10.1080/02664763.2019.1689732">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JOAS},
  doi          = {10.1080/02664763.2019.1689732},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {i-ii},
  shortjournal = {J. Appl. Stat.},
  title        = {Correction},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Forecasting interest rate volatility of the united kingdom:
Evidence from over 150 years of data. <em>JOAS</em>, <em>47</em>(6),
1128–1143. (<a
href="https://doi.org/10.1080/02664763.2019.1666093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study examines the very short, short, medium and long-term forecasting ability of different univariate GARCH models of United Kingdom (UK)&#39;s interest rate volatility, using a long span monthly data from May 1836 to June 2018. The main results show the relevance of considering alternative error distributions to the normal distribution when estimating GARCH-type models. Thus, we obtain that the Asymmetric Power ARCH (A-PARCH) models with skew generalized error distribution are the most accurate models when forecasting UK interest rates, while for the short, medium and long-term term forecasting horizons, GARCH models with generalized error distribution for the error term are the most accurate models in forecasting UK&#39;s interest rates.},
  archive      = {J_JOAS},
  author       = {Hossein Hassani and Mohammad Reza Yeganegi and Juncal Cuñado and Rangan Gupta},
  doi          = {10.1080/02664763.2019.1666093},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1128-1143},
  shortjournal = {J. Appl. Stat.},
  title        = {Forecasting interest rate volatility of the united kingdom: Evidence from over 150 years of data},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Jumps beyond the realms of cricket: India’s performance in
one day internationals and stock market movements. <em>JOAS</em>,
<em>47</em>(6), 1109–1127. (<a
href="https://doi.org/10.1080/02664763.2019.1663157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine the impact of the Indian cricket team&#39;s performance in one-day international cricket matches on return, realized volatility and jumps of the Indian stock market, based on intraday data covering the period of 30th October, 2006 to 31st March, 2017. Using a nonparametric causality-in-quantiles test, we were able to detect evidence of predictability from wins or losses for primarily volatility and jumps, especially over the lower-quantiles of the conditional distributions, with losses having stronger predictability than wins. However, the impact on the stock return is weak and restricted towards the upper end of the conditional distribution.},
  archive      = {J_JOAS},
  author       = {Konstantinos Gkillas and Rangan Gupta and Chi Keung Marco Lau and Muhammad Tahir Suleman},
  doi          = {10.1080/02664763.2019.1663157},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1109-1127},
  shortjournal = {J. Appl. Stat.},
  title        = {Jumps beyond the realms of cricket: India&#39;s performance in one day internationals and stock market movements},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bivariate inverse weibull distribution and its application
in complementary risks model. <em>JOAS</em>, <em>47</em>(6), 1084–1108.
(<a href="https://doi.org/10.1080/02664763.2019.1669542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In reliability and survival analysis the inverse Weibull distribution has been used quite extensively as a heavy tailed distribution with a non-monotone hazard function. Recently a bivariate inverse Weibull (BIW) distribution has been introduced in the literature, where the marginals have inverse Weibull distributions and it has a singular component. Due to this reason this model cannot be used when there are no ties in the data. In this paper we have introduced an absolutely continuous bivariate inverse Weibull (ACBIW) distribution omitting the singular component from the BIW distribution. A natural application of this model can be seen in the analysis of dependent complementary risks data. We discuss different properties of this model and also address the inferential issues both from the classical and Bayesian approaches. In the classical approach, the maximum likelihood estimators cannot be obtained explicitly and we propose to use the expectation maximization algorithm based on the missing value principle. In the Bayesian analysis, we use a very flexible prior on the unknown model parameters and obtain the Bayes estimates and the associated credible intervals using importance sampling technique. Simulation experiments are performed to see the effectiveness of the proposed methods and two data sets have been analyzed to see how the proposed methods and the model work in practice.},
  archive      = {J_JOAS},
  author       = {Shuvashree Mondal and Debasis Kundu},
  doi          = {10.1080/02664763.2019.1669542},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1084-1108},
  shortjournal = {J. Appl. Stat.},
  title        = {A bivariate inverse weibull distribution and its application in complementary risks model},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). TGCnA: Temporal gene coexpression network analysis using a
low-rank plus sparse framework. <em>JOAS</em>, <em>47</em>(6),
1064–1083. (<a
href="https://doi.org/10.1080/02664763.2019.1667311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Various gene network models with distinct physical nature have been widely used in biological studies. For temporal transcriptomic studies, the current dynamic models either ignore the temporal variation in the network structure or fail to scale up to a large number of genes due to severe computational bottlenecks and sample size limitation. Although the correlation-based gene networks are computationally affordable, they have limitations after being applied to gene expression time-course data. We proposed Temporal Gene Coexpression Network Analysis (TGCnA) framework for the transcriptomic time-course data. The mathematical nature of TGCnA is the joint modeling of multiple covariance matrices across time points using a ‘low-rank plus sparse’ framework, in which the network similarity across time points is explicitly modeled in the low-rank component. We demonstrated the advantage of TGCnA in covariance matrix estimation and gene module discovery using both simulation data and real transcriptomic data. The code is available at https://github.com/QiZhangStat/TGCnA .},
  archive      = {J_JOAS},
  author       = {Jinyu Li and Yutong Lai and Chi Zhang and Qi Zhang},
  doi          = {10.1080/02664763.2019.1667311},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1064-1083},
  shortjournal = {J. Appl. Stat.},
  title        = {TGCnA: Temporal gene coexpression network analysis using a low-rank plus sparse framework},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On improved volatility modelling by fitting skewness in ARCH
models. <em>JOAS</em>, <em>47</em>(6), 1031–1063. (<a
href="https://doi.org/10.1080/02664763.2019.1671323">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study ARCH/GARCH effects under possible deviation from normality. Since skewness is the principal cause for deviations from normality in many practical applications, e.g. finance, we study in particular skewness. We propose robust tests for normality both for NoVaS and modified NoVaS transformed and original data. Such an approach is not applicable for EGARCH, but applicable for GARCH-GJR models. A novel test procedure is proposed for the skewness in autoregressive conditional volatility models. The power of the tests is investigated with various underlying models. Applications with financial data show the applicability and the capabilities of the proposed testing procedure.},
  archive      = {J_JOAS},
  author       = {P. Mantalos and A. Karagrigoriou and L. Střelec and P. Jordanova and P. Hermann and J. Kiseľák and J. Hudák and M. Stehlík},
  doi          = {10.1080/02664763.2019.1671323},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1031-1063},
  shortjournal = {J. Appl. Stat.},
  title        = {On improved volatility modelling by fitting skewness in ARCH models},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A cholesky-based estimation for large-dimensional covariance
matrices. <em>JOAS</em>, <em>47</em>(6), 1017–1030. (<a
href="https://doi.org/10.1080/02664763.2019.1664424">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a new method to estimate a large-dimensional covariance matrix when the variables have no natural ordering among themselves. The modified Cholesky decomposition technique is used to provide a set of estimates of the covariance matrix under multiple orderings of variables. The proposed estimator is in the form of a linear combination of these available estimates and the identity matrix. It is positive definite and applicable in large dimensions. The merits of the proposed estimator are demonstrated through the numerical study and a real data example by comparison with several existing methods.},
  archive      = {J_JOAS},
  author       = {Xiaoning Kang and Chaoping Xie and Mingqiu Wang},
  doi          = {10.1080/02664763.2019.1664424},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {1017-1030},
  shortjournal = {J. Appl. Stat.},
  title        = {A cholesky-based estimation for large-dimensional covariance matrices},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sparse graphical models via calibrated concave convex
procedure with application to fMRI data. <em>JOAS</em>, <em>47</em>(6),
997–1016. (<a
href="https://doi.org/10.1080/02664763.2019.1663158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a calibrated concave convex procedure (calibrated CCCP) for high-dimensional graphical model selection. The calibrated CCCP approach for the smoothly clipped absolute deviation (SCAD) penalty is known to be path-consistent with probability converging to one in linear regression models. We implement the calibrated CCCP method with the SCAD penalty for the graphical model selection. We use a quadratic objective function for undirected Gaussian graphical models and adopt the SCAD penalty for sparse estimation. For the tuning procedure, we propose to use columnwise tuning on the quadratic objective function adjusted for test data. In a simulation study, we compare the performance of the proposed method with two existing graphical model estimators for high-dimensional data in terms of matrix error norms and support recovery rate. We also compare the bias and the variance of the estimated matrices. Then, we apply the method to functional magnetic resonance imaging (fMRI) data of an attention deficit hyperactivity disorders (ADHD) patient.},
  archive      = {J_JOAS},
  author       = {Sungtaek Son and Cheolwoo Park and Yongho Jeon},
  doi          = {10.1080/02664763.2019.1663158},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {997-1016},
  shortjournal = {J. Appl. Stat.},
  title        = {Sparse graphical models via calibrated concave convex procedure with application to fMRI data},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Classical methods of estimation on constant stress
accelerated life tests under exponentiated lindley distribution.
<em>JOAS</em>, <em>47</em>(6), 975–996. (<a
href="https://doi.org/10.1080/02664763.2019.1661361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accelerated life testing is adopted in several fields to obtain adequate failure time data of test units in a much shorter time than testing at normal operating conditions. The lifetime of a product at constant level of stress is assumed to have an exponentiated Lindley distribution. In this paper, besides maximum likelihood method, eight other frequentist methods of estimation, namely, method of least square estimation, method of weighted least square estimation, method of maximum product of spacing estimation, method of minimum spacing absolute distance estimation, method of minimum spacing absolute-log distance estimation, method of Cramér-von-Mises estimation, method of Anderson-Darling estimation and Right-tail Anderson-Darling estimation are considered to estimate the parameters of the exponentiated Lindley distribution under constant stress accelerated life testing. Moreover, shape parameter and the reliability function under usual conditions are estimated based on aforementioned methods of estimation. To evaluate the performance of the proposed methods, a simulation study is carried out. The performances of the estimators have been compared in terms of their mean squared error using small, medium and large sample sizes. As an illustration, the model and the proposed methods are applied to two accelerated life test data sets.},
  archive      = {J_JOAS},
  author       = {Sanku Dey and Mazen Nassar},
  doi          = {10.1080/02664763.2019.1661361},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {975-996},
  shortjournal = {J. Appl. Stat.},
  title        = {Classical methods of estimation on constant stress accelerated life tests under exponentiated lindley distribution},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The unit-weibull distribution as an alternative to the
kumaraswamy distribution for the modeling of quantiles conditional on
covariates. <em>JOAS</em>, <em>47</em>(6), 954–974. (<a
href="https://doi.org/10.1080/02664763.2019.1657813">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Beta distribution is the standard model for quantifying the influence of covariates on the mean of a response variable on the unit interval. However, this well-known distribution is no longer useful when we are interested in quantifying the influence of such covariates on the quantiles of the response variable. Unlike Beta, the Kumaraswamy distribution has a closed-form expression for its quantile and can be useful for the modeling of quantiles in the absence/presence of covariates. As an alternative to the Kumaraswamy distribution for the modeling of quantiles, in this paper the unit-Weibull distribution was considered. This distribution was obtained by the transformation of a random variable with Weibull distribution. The same transformation applied to a random variable with Exponentiated Exponential distribution generates the Kumaraswamy distribution. The suitability of our proposal was demonstrated to model quantiles, conditional on covariates, with two simulated examples and three real applications with datasets from health, accounting and social science. For such data sets, the obtained fits of the proposed regression model were compared with those provided by the Beta and Kumaraswamy regression models.},
  archive      = {J_JOAS},
  author       = {J. Mazucheli and A. F. B. Menezes and L. B. Fernandes and R. P. de Oliveira and M. E. Ghitany},
  doi          = {10.1080/02664763.2019.1657813},
  journal      = {Journal of Applied Statistics},
  number       = {6},
  pages        = {954-974},
  shortjournal = {J. Appl. Stat.},
  title        = {The unit-weibull distribution as an alternative to the kumaraswamy distribution for the modeling of quantiles conditional on covariates},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discussion comments on “on moments of the unit lindley
distribution.” <em>JOAS</em>, <em>47</em>(5), 950–953. (<a
href="https://doi.org/10.1080/02664763.2019.1675609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a note about the paper titled ‘On the one parameter unit-Lindley distribution and its associated regression model for proportion data’, Mazucheli et al. [J. Appl. Stat. 46 (2019), pp. 700–714] and Nadarajah and Chan [ On moments of the unit Lindley distribution , J. Appl. Stat. (under review)] claim that ‘The expressions given for the moments and incomplete moments are not correct and not in closed form’. While we agree that they are not in closed form and observe a typo in the expressions for μ ′ k μ k ′ μk′ and T k ( t ) , k = 1 , … , the expressions for μ 1 ′ , μ 2 ′ , μ 3 ′ and μ 4 ′ are, however, entirely correct.},
  archive      = {J_JOAS},
  author       = {J. Mazucheli and A. F. B. Menezes and S. Chakraborty},
  doi          = {10.1080/02664763.2019.1675609},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {950-953},
  shortjournal = {J. Appl. Stat.},
  title        = {Discussion comments on ‘On moments of the unit lindley distribution’},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On moments of the unit lindley distribution. <em>JOAS</em>,
<em>47</em>(5), 947–949. (<a
href="https://doi.org/10.1080/02664763.2019.1675607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mazucheli et al. [ On the one parameter unit-Lindley distribution and its associated regression model for proportion data. J. Appl. Stat. 46 (2019), pp. 700–714] introduced the unit Lindley distribution. The expressions given for the moments and incomplete moments are either not correct or not in closed form. We derive closed form expressions moments and incomplete moments. The expressions are elementary except for the exponential integral.},
  archive      = {J_JOAS},
  author       = {Saralees Nadarajah and Stephen Chan},
  doi          = {10.1080/02664763.2019.1675607},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {947-949},
  shortjournal = {J. Appl. Stat.},
  title        = {On moments of the unit lindley distribution},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spatio-temporal hierarchical bayesian analysis of wildfires
with stochastic partial differential equations. A case study from
valencian community (spain). <em>JOAS</em>, <em>47</em>(5), 927–946. (<a
href="https://doi.org/10.1080/02664763.2019.1661360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spatio-temporal study of wildfires has two complex elements that are the computational efficiency and longtime processing. Modelling the spatial variability of a wildfire could be performed in different ways, and an important issue is the computational facilities that the new methodological techniques afford us. The Markov random fields methods have made possible to build risk maps, but for many forest managers, it is more advantageous to know the size of the fire and its location. In the first part of this work, Stochastic Partial Differential Equation with Integrated Nested Laplace Approximation is utilised to model the size of the forest fires observed in the Valencian Community (Spain) and so it does the inclusion of the time effect, and the study of the emergency calls. The most crucial element in this paper is the inclusion of the improved meshes for the spatial effect and the time, these are, 2d (locations) and 1d (time) respectively. The advantage of the use of spatio-temporal meshes is described with the inclusion of Bayesian methodology in all the scenarios.},
  archive      = {J_JOAS},
  author       = {Pablo Juan Verdoy},
  doi          = {10.1080/02664763.2019.1661360},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {927-946},
  shortjournal = {J. Appl. Stat.},
  title        = {Spatio-temporal hierarchical bayesian analysis of wildfires with stochastic partial differential equations. a case study from valencian community (Spain)},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nomogram construction to predict dyslipidemia based on a
logistic regression analysis. <em>JOAS</em>, <em>47</em>(5), 914–926.
(<a href="https://doi.org/10.1080/02664763.2019.1660760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dyslipidemia is a chronic disease requiring continuous management and is a well-known risk factor for cardiovascular diseases as well as hypertension and diabetes. However, no studies have so far visualized and predicted the probability of dyslipidemia. Hence, this study proposes a nomogram based on a logistic regression model that can visualize its risk factors and predict the probability of developing dyslipidemia. Twelve risk factors for dyslipidemia are identified through a chi-squared test. We then conduct a logistic regression analysis with two interaction variables to obtain a model and build a nomogram for dyslipidemia. Finally, we verify the constructed nomogram using a receiver operation characteristic curve and calibration plot.},
  archive      = {J_JOAS},
  author       = {Ju-Hyun Seo and Hyun-Ji Kim and Jea-Young Lee},
  doi          = {10.1080/02664763.2019.1660760},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {914-926},
  shortjournal = {J. Appl. Stat.},
  title        = {Nomogram construction to predict dyslipidemia based on a logistic regression analysis},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian model selection in linear mixed models for
longitudinal data. <em>JOAS</em>, <em>47</em>(5), 890–913. (<a
href="https://doi.org/10.1080/02664763.2019.1657814">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear mixed models (LMMs) are popular to analyze repeated measurements with a Gaussian response. For longitudinal studies, the LMMs consist of a fixed part expressing the effect of covariates on the mean evolution in time and a random part expressing the variation of the individual curves around the mean curve. Selecting the appropriate fixed and random effect parts is an important modeling exercise. In a Bayesian framework, there is little agreement on the appropriate selection criteria. This paper compares the performance of the deviance information criterion (DIC), the pseudo-Bayes factor and the widely applicable information criterion (WAIC) in LMMs, with an extension to LMMs with skew-normal distributions. We focus on the comparison between the conditional criteria (given random effects) versus the marginal criteria (averaged over random effects). In spite of theoretical arguments, there is not much enthusiasm among applied statisticians to make use of the marginal criteria. We show in an extensive simulation study that the three marginal criteria are superior in choosing the appropriate longitudinal model. In addition, the marginal criteria selected most appropriate model for growth curves of Nigerian chicken. A self-written R function can be combined with standard Bayesian software packages to obtain the marginal selection criteria.},
  archive      = {J_JOAS},
  author       = {Oludare Ariyo and Adrian Quintero and Johanna Muñoz and Geert Verbeke and Emmanuel Lesaffre},
  doi          = {10.1080/02664763.2019.1657814},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {890-913},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian model selection in linear mixed models for longitudinal data},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On e-bayesian estimations for the cumulative hazard rate and
mean residual life under generalized inverted exponential distribution
and type-II censoring. <em>JOAS</em>, <em>47</em>(5), 865–889. (<a
href="https://doi.org/10.1080/02664763.2019.1661359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimation of reliability and hazard rate is necessary in many applications. To this aim, different methods of estimation have been employed. Each method suffers from its own problems such as complexity of calculations, high risk and so on. Toward this end, this study employed a new method, E-Bayesian, for estimating the parametric functions of the Generalized Inverted Exponential distribution, which is one of the most noticeable distributions in lifetime studies. Relations are derived under a squared error loss function, type-II censoring and a conjugate prior. E-Bayesian estimations are obtained based on different priors of the hyperparameters to investigate the influence of different prior distributions on these estimations. The asymptotic behaviors of E-Bayesian estimations and relations among them have been investigated. Finally, a comparison among the maximum likelihood, Bayes and E-Bayesian estimations in different sample sizes are made, using a real data and the Monte Carlo simulation. Simulations show that the new presented method is more efficient than previous methods and is also easy to operate. Also, some comparisons among the results of Generalized Inverted Exponential distribution, Exponential distribution and Generalized Exponential distribution are provided.},
  archive      = {J_JOAS},
  author       = {Hassan Piriaei and Gholamhossein Yari and Rahman Farnoosh},
  doi          = {10.1080/02664763.2019.1661359},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {865-889},
  shortjournal = {J. Appl. Stat.},
  title        = {On E-bayesian estimations for the cumulative hazard rate and mean residual life under generalized inverted exponential distribution and type-II censoring},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The statistical properties of the threshold model and the
feedback leadership condition. <em>JOAS</em>, <em>47</em>(5), 844–864.
(<a href="https://doi.org/10.1080/02664763.2019.1658728">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper analyses the statistical properties of the threshold model of collective actions. We establish the conditions of the threshold model under which the sequence of decisions is exchangeable. Then, if the probability of participation is a continuous and strictly increasing function of the willingness to participate, then the unique urn representation of the threshold model is the Pólya urn. In such a case, the sequence of participation rates converges almost surely to a random variable Z that has a beta distribution. Also, we show the critical mass and self-sustaining participation rates are fixed points of the distribution function. Further, we discuss the non-exchangeable model of collective actions, and we define the feedback leadership condition. We apply these results to characterise the process of the adoption of hybrid corn in Iowa, and the diffusion of the Internet for different levels of income.},
  archive      = {J_JOAS},
  author       = {Alexandra M. Espinosa and Luís Horna},
  doi          = {10.1080/02664763.2019.1658728},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {844-864},
  shortjournal = {J. Appl. Stat.},
  title        = {The statistical properties of the threshold model and the feedback leadership condition},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stability enhanced variable selection for a semiparametric
model with flexible missingness mechanism and its application to the
ChAMP study. <em>JOAS</em>, <em>47</em>(5), 827–843. (<a
href="https://doi.org/10.1080/02664763.2019.1658727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is motivated by the analytical challenges we encounter when analyzing the ChAMP (Chondral Lesions And Meniscus Procedures) study, a randomized controlled trial to compare debridement to observation of chondral lesions in arthroscopic knee surgery. The main outcome, WOMAC (Western Ontario and McMaster Universities Osteoarthritis Index) pain score, is derived from the patient&#39;s responses to the questionnaire collected in the study. The major goal is to identify potentially important variables that contribute to this outcome. In this paper, the model of interest is a semiparametric model for the pain score. To address the missing data issue, we adopt a flexible missingness mechanism that is much more versatile in practice than a single parametric model. Then we propose a pairwise conditional likelihood approach to estimate the unknown parameter in the semiparametric model without the need of modeling its nonparametric counterpart nor the missingness mechanism. For variable selection, we apply a regularization approach with a variety of stability enhanced tuning parameter selection methods. We conduct comprehensive simulation studies to evaluate the performance of the proposed method. We also apply the proposed method to the ChAMP study to demonstrate its usefulness.},
  archive      = {J_JOAS},
  author       = {Yang Yang and Jiwei Zhao and Gregory Wilding and Melissa Kluczynski and Leslie Bisson},
  doi          = {10.1080/02664763.2019.1658727},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {827-843},
  shortjournal = {J. Appl. Stat.},
  title        = {Stability enhanced variable selection for a semiparametric model with flexible missingness mechanism and its application to the ChAMP study},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Skew-normal bayesian spatial heterogeneity panel data
models. <em>JOAS</em>, <em>47</em>(5), 804–826. (<a
href="https://doi.org/10.1080/02664763.2019.1657812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new regression model for the analysis of spatial panel data in the case of spatial heterogeneity and non-normality. In empirical economic research, the normality of error components is a routine assumption for the models with continuous responses. However, such an assumption may not be appropriate in many applications. This work relaxes the normality assumption by using a multivariate skew-normal distribution, which includes the normal distribution as a special case. The methodology is illustrated through a simulation study and application to insurance and gasoline demand data sets. In these analyses, a simple Bayesian framework that implements a Markov chain Monte Carlo algorithm is derived for parameter estimation and inference.},
  archive      = {J_JOAS},
  author       = {Mohadeseh Alsadat Farzammehr and Mohammad Reza Zadkarami and Geoffrey J. McLachlan and Sharon X. Lee},
  doi          = {10.1080/02664763.2019.1657812},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {804-826},
  shortjournal = {J. Appl. Stat.},
  title        = {Skew-normal bayesian spatial heterogeneity panel data models},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new method for determining the benchmark dose tolerable
region and endpoint probabilities for toxicology experiments.
<em>JOAS</em>, <em>47</em>(5), 775–803. (<a
href="https://doi.org/10.1080/02664763.2019.1654985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The increase of exposure to toxic materials and hazardous chemicals is a major concern due to the adverse effect on human health. Among the major concerns of toxicologists is to determine acceptable levels of exposure to hazardous substances. Current approaches often evaluate each endpoint and stressor individually. We propose a novel method to simultaneously determine the Benchmark Dose Tolerable Region (BMDTR) for multiple endpoint and multiple stressor studies by adopting a Bayesian approach. A main concern while assessing the combined toxicological effect of a chemical mixture is the anticipated type of the combined action (i.e. synergistic or antagonistic); thus it was essential to account for interaction effects to handle this situation, imposing more challenges due to the non-linearity of the tolerable region. The proposed method will be evaluated using two approaches, the first one using the estimated value of the posterior median and the second approach using all MCMC samples from the posterior distribution. Furthermore, we propose a new method to determine the endpoint probabilities for each endpoint, which reflects the importance of each endpoint in helping determining the boundaries of the benchmark dose tolerable region (BMDTR).},
  archive      = {J_JOAS},
  author       = {Naha J. Farhat and Edward L. Boone and David J. Edwards},
  doi          = {10.1080/02664763.2019.1654985},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {775-803},
  shortjournal = {J. Appl. Stat.},
  title        = {A new method for determining the benchmark dose tolerable region and endpoint probabilities for toxicology experiments},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian modelling of nonlinear poisson regression with
artificial neural networks. <em>JOAS</em>, <em>47</em>(5), 757–774. (<a
href="https://doi.org/10.1080/02664763.2019.1653268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modelling and prediction of count and rate responses have substantial usage in many fields, including health, finance, social, etc. Conventionally, linear Poisson regression models have been widely used to model these responses. However, the linearity assumption of the systematic component of linear Poisson regression models restricts their capability of handling complex data patterns. In this regard, it is important to develop nonlinear Poisson regression models to capture the inherent variability within the count data. In this study, we introduce a probabilistically driven nonlinear Poisson regression model with Bayesian artificial neural networks (ANN) to model count and rate data. This new nonlinear Poisson regression model developed with Bayesian ANN provides higher prediction accuracies over traditional Poisson or negative binomial regression models as revealed in our simulation and real data studies.},
  archive      = {J_JOAS},
  author       = {Hansapani Rodrigo and Chris Tsokos},
  doi          = {10.1080/02664763.2019.1653268},
  journal      = {Journal of Applied Statistics},
  number       = {5},
  pages        = {757-774},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian modelling of nonlinear poisson regression with artificial neural networks},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A spatial–temporal study of dengue in peninsular malaysia
for the year 2017 in two different space–time model. <em>JOAS</em>,
<em>47</em>(4), 739–756. (<a
href="https://doi.org/10.1080/02664763.2019.1648391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatio-temporal disease mapping models give a great worth in epidemiology, especially in describing the pattern of disease incidence across geographical space and time. This paper analyses the spatial and temporal variability of dengue disease rates based on generalized linear mixed models. For spatio-temporal study, the models incorporate spatially correlated random effects as well as temporal effects. In this study, two different spatial random effects are applied and compared. The first model is based on Leroux spatial model, while the second model is based on the stochastic partial differential equation approach. For the temporal effects, both models follow an autoregressive model of first-order model. The models are fitted within a hierarchical Bayesian framework with integrated nested Laplace approximation methodology. The main objective of this study is to compare both spatio-temporal models in terms of their ability in representing the disease phenomenon. The models are applied to weekly dengue fever data in Peninsular Malaysia reported to the Ministry of Health Malaysia in the year 2017 according to the district level.},
  archive      = {J_JOAS},
  author       = {Nurul Syafiah Abd Naeeim and Nuzlinda Abdul Rahman and Fatin Afiqah Muhammad Fahimi},
  doi          = {10.1080/02664763.2019.1648391},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {739-756},
  shortjournal = {J. Appl. Stat.},
  title        = {A spatial–temporal study of dengue in peninsular malaysia for the year 2017 in two different space–time model},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). EM-test for homogeneity in a two-sample problem with a
mixture structure. <em>JOAS</em>, <em>47</em>(4), 724–738. (<a
href="https://doi.org/10.1080/02664763.2019.1652254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many applications such as case-control studies with contaminated controls, or the test of a treatment effect in the presence of nonresponders in biological experiments or clinical trials, a two-sample problem with one of the samples having a mixture structure often arises. Due to the importance and wide applications of scale mixtures and location mixtures, we consider in this paper the case that the component densities differ only in scale parameters and the case that the component densities differ only in location parameters, and further construct an EM-test for the two-sample problem under each case. We show that both the EM-tests possess a chi-squared null limiting distribution. The local power analysis and sample size calculations are also investigated. Finally, the simulation studies and real data analysis demonstrate that the proposed EM-tests have better performance than the existing methods.},
  archive      = {J_JOAS},
  author       = {Guanfu Liu and Yuejiao Fu and Jianjun Zhang and Xiaolong Pu and Boying Wang},
  doi          = {10.1080/02664763.2019.1652254},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {724-738},
  shortjournal = {J. Appl. Stat.},
  title        = {EM-test for homogeneity in a two-sample problem with a mixture structure},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Do german economic research institutes publish efficient
growth and inflation forecasts? A bayesian analysis. <em>JOAS</em>,
<em>47</em>(4), 698–723. (<a
href="https://doi.org/10.1080/02664763.2019.1652253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We use Bayesian additive regression trees to reexamine the efficiency of growth and inflation forecasts for Germany. To this end, we use forecasts of four leading German economic research institutes for the sample period from 1970 to 2016. We reject the strong form of forecast efficiency and find evidence against the weak form of forecast efficiency for longer-term growth and longer-term inflation forecasts. We cannot reject weak efficiency of short-term growth and inflation forecasts and of forecasts disaggregated at the institute level. We find that Bayesian additive regression trees perform significantly better than a standard linear efficiency-regression model in terms of forecast accuracy.},
  archive      = {J_JOAS},
  author       = {Christoph Behrens and Christian Pierdzioch and Marian Risse},
  doi          = {10.1080/02664763.2019.1652253},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {698-723},
  shortjournal = {J. Appl. Stat.},
  title        = {Do german economic research institutes publish efficient growth and inflation forecasts? a bayesian analysis},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Acceptance sampling plans from truncated life test based on
tsallis q-exponential distribution. <em>JOAS</em>, <em>47</em>(4),
685–697. (<a
href="https://doi.org/10.1080/02664763.2019.1650254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we propose a new single acceptance sampling plan from truncated life test assuming that the quality characteristics follow the Tsallis q -exponential distribution. The proposed plan is given, and then we derived the operation characteristics function and calculate the optimal sample size and producer’&#39;s risk for some given parameter values to measure the performance of this plan. Also, a comparative study with other sampling plan is discussed to show the benefit of the proposed plan, and real data analysis is given to illustrate the applicability of the proposed plan in the industry.},
  archive      = {J_JOAS},
  author       = {Amjad D. Al-Nasser and Mohammed Obeidat},
  doi          = {10.1080/02664763.2019.1650254},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {685-697},
  shortjournal = {J. Appl. Stat.},
  title        = {Acceptance sampling plans from truncated life test based on tsallis q-exponential distribution},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Assessing the impact of the economic crises in 1997 and 2008
on suicides in hong kong, taiwan and south korea using a
strata-bootstrap algorithm. <em>JOAS</em>, <em>47</em>(4), 666–684. (<a
href="https://doi.org/10.1080/02664763.2019.1650008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Asian economic crises of 1997 and the 2008 Global Financial Crisis (GFC) had far-reaching impacts on Asian and other global economies. Turmoil in the banking and finance sectors led to downturns in stock markets, resulting in bankruptcies, house repossessions and high unemployment. These crises have been shown to be correlated with a deterioration in mental health and an increase in suicides, and it is important to understand the implication of these impacts and how such recessions affect the health of affected populations. With the benefit of hindsight, did lessons learned from the negative effects of the 1997 Asian economic recession impact the aftermath of the 2008 GFC in Asian countries? Utilising a framework based on a simple strata-bootstrap algorithm using daily data – where available – we investigate the trend in suicide rates over time in three different populations (Hong Kong, Taiwan and South Korea), and examine whether there were any changes in the pattern of suicide rates in each country subsequent to both the 1997 Asian and 2008. We find that each country responded differently to each of the crises and the suicide rates for certain age-gender specific groups in each country were more affected.},
  archive      = {J_JOAS},
  author       = {Mehdi Soleymani and Paul S. F. Yip},
  doi          = {10.1080/02664763.2019.1650008},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {666-684},
  shortjournal = {J. Appl. Stat.},
  title        = {Assessing the impact of the economic crises in 1997 and 2008 on suicides in hong kong, taiwan and south korea using a strata-bootstrap algorithm},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interpoint distance tests for high-dimensional comparison
studies. <em>JOAS</em>, <em>47</em>(4), 653–665. (<a
href="https://doi.org/10.1080/02664763.2019.1649374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern data collection techniques allow to analyze a very large number of endpoints. In biomedical research, for example, expressions of thousands of genes are commonly measured only on a small number of subjects. In these situations, traditional methods for comparison studies are not applicable. Moreover, the assumption of normal distribution is often questionable for high-dimensional data, and some variables may be at the same time highly correlated with others. Hypothesis tests based on interpoint distances are very appealing for studies involving the comparison of means, because they do not assume data to come from normally distributed populations and comprise tests that are distribution free, unbiased, consistent, and computationally feasible, even if the number of endpoints is much larger than the number of subjects. New tests based on interpoint distances are proposed for multivariate studies involving simultaneous comparison of means and variability, or the whole distribution shapes. The tests are shown to perform well in terms of power, when the endpoints have complex dependence relations, such as in genomic and metabolomic studies. A practical application to a genetic cardiovascular case-control study is discussed.},
  archive      = {J_JOAS},
  author       = {Marco Marozzi and Amitava Mukherjee and Jan Kalina},
  doi          = {10.1080/02664763.2019.1649374},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {653-665},
  shortjournal = {J. Appl. Stat.},
  title        = {Interpoint distance tests for high-dimensional comparison studies},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new procedure for resampled portfolio with shrinkaged
covariance matrix. <em>JOAS</em>, <em>47</em>(4), 642–652. (<a
href="https://doi.org/10.1080/02664763.2019.1648394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dealing with estimation error is an important issue when we implement the mean–variance paradigm for portfolio construction. To tackle the problem, two approaches are proposed in literature, the portfolio resampling technique introduced by Michuad and the well-known shrinkaged covariance matrix method. There are certain evidences on the advantages of shrinkaged covariance over portfolio resampling, however, it is unclear whether a combination of the two approaches could produce a better performance compared with using shrinkaged covariance alone. In this paper, we propose a new algorithm to integrated linear or nonlinear shrinkage estimation with resampled portfolio to achieve a further improvement. Our method are demonstrated via extensive simulation and application in active portfolio management process.},
  archive      = {J_JOAS},
  author       = {Mian Huang and Shangbing Yu},
  doi          = {10.1080/02664763.2019.1648394},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {642-652},
  shortjournal = {J. Appl. Stat.},
  title        = {A new procedure for resampled portfolio with shrinkaged covariance matrix},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparametric optimal designs for degradation tests.
<em>JOAS</em>, <em>47</em>(4), 624–641. (<a
href="https://doi.org/10.1080/02664763.2019.1648392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper discusses the optimal design of a degradation test within the nonparametric framework by assuming the underlying process to be an empirical Lévy process with heterogeneity. The optimization of the experiment relies on the design variables including the sample size, the measurement frequency and total operation time. It is necessary to investigate the effect of the design variables on the estimation of first passage time (FPT). Subject to total experimental cost not exceeding a pre-specified budget, the values of design variables are determined such that the bootstrap mean square error of the 100 p th percentile of the FPT distribution is minimized.},
  archive      = {J_JOAS},
  author       = {Narayanaswamy Balakrishnan and Chengwei Qin},
  doi          = {10.1080/02664763.2019.1648392},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {624-641},
  shortjournal = {J. Appl. Stat.},
  title        = {Nonparametric optimal designs for degradation tests},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Small area mean estimation after effect clustering.
<em>JOAS</em>, <em>47</em>(4), 602–623. (<a
href="https://doi.org/10.1080/02664763.2019.1648390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Providing reliable estimates of subpopulation/area parameters has attracted increased attention due to their importance in applications such as policymaking. Due to low or even no samples from some areas, we must adopt indirect model approaches. Existing indirect small area estimation methods often assume that a single nested error regression model is suitable for all the small areas. In particular, the effects of the auxiliary variables are either fixed or have a single attraction center. In some applications, it can be more appropriate to cluster the small areas so that the effects of the auxiliary variables are fixed but have multiple centers in the nested error regression model. In this paper, we examine an extended nested error regression model in which the auxiliary variables have mixed effects with multiple centers. We use a penalty approach to identify these centers and estimate the model parameters simultaneously. We then propose two new small area mean estimators and construct estimators of their mean square errors. Simulations based on artificial and realistic finite populations show that the new estimators can be efficient. Furthermore, the confidence intervals based on the new methods have accurate coverage probabilities. We illustrate the proposed methods with the Survey of Labour and Income Dynamics conducted in Canada.},
  archive      = {J_JOAS},
  author       = {Zhihuang Yang and Jiahua Chen},
  doi          = {10.1080/02664763.2019.1648390},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {602-623},
  shortjournal = {J. Appl. Stat.},
  title        = {Small area mean estimation after effect clustering},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Skew selection for factor stochastic volatility models.
<em>JOAS</em>, <em>47</em>(4), 582–601. (<a
href="https://doi.org/10.1080/02664763.2019.1646227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes factor stochastic volatility models with skew error distributions. The generalized hyperbolic skew t -distribution is employed for common-factor processes and idiosyncratic shocks. Using a Bayesian sparsity modeling strategy for the skewness parameter provides a parsimonious skew structure for possibly high-dimensional stochastic volatility models. Analyses of daily stock returns are provided. Empirical results show that the skewness is important for common-factor processes but less for idiosyncratic shocks. The sparse skew structure improves prediction and portfolio performance.},
  archive      = {J_JOAS},
  author       = {Jouchi Nakajima},
  doi          = {10.1080/02664763.2019.1646227},
  journal      = {Journal of Applied Statistics},
  number       = {4},
  pages        = {582-601},
  shortjournal = {J. Appl. Stat.},
  title        = {Skew selection for factor stochastic volatility models},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A descriptive study of variable discretization and
cost-sensitive logistic regression on imbalanced credit data.
<em>JOAS</em>, <em>47</em>(3), 568–581. (<a
href="https://doi.org/10.1080/02664763.2019.1643829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Training classification models on imbalanced data tends to result in bias towards the majority class. In this paper, we demonstrate how variable discretization and cost-sensitive logistic regression help mitigate this bias on an imbalanced credit scoring dataset, and further show the application of the variable discretization technique on the data from other domains, demonstrating its potential as a generic technique for classifying imbalanced data beyond credit socring. The performance measurements include ROC curves, Area under ROC Curve (AUC), Type I Error, Type II Error, accuracy, and F1 score. The results show that proper variable discretization and cost-sensitive logistic regression with the best class weights can reduce the model bias and/or variance. From the perspective of the algorithm, cost-sensitive logistic regression is beneficial for increasing the value of predictors even if they are not in their optimized forms while maintaining monotonicity. From the perspective of predictors, the variable discretization performs better than cost-sensitive logistic regression, provides more reasonable coefficient estimates for predictors which have nonlinear relationships against their empirical logit, and is robust to penalty weights on misclassifications of events and non-events determined by their apriori proportions.},
  archive      = {J_JOAS},
  author       = {Lili Zhang and Herman Ray and Jennifer Priestley and Soon Tan},
  doi          = {10.1080/02664763.2019.1643829},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {568-581},
  shortjournal = {J. Appl. Stat.},
  title        = {A descriptive study of variable discretization and cost-sensitive logistic regression on imbalanced credit data},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A mixture model to assess perception of discrimination on
grounds of sexual orientation for policy considerations. <em>JOAS</em>,
<em>47</em>(3), 554–567. (<a
href="https://doi.org/10.1080/02664763.2019.1639643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper explores the relationships between subjective and contextual covariates as determinants of discrimination perception in a large sample survey. A modelling approach is implemented to detect the perception of discrimination on the ground of sexual orientation through expressed ratings, in a cross-country perspective. Exploiting the capabilities of a mixture model, which interprets ordinal evaluations as the combined results of two latent components, the cognitive process of selection among discrete ordered alternatives is discussed in terms of attractiveness towards the item and uncertainty in the response pattern. Gender, age and political orientation turn out to be significant variables at the individual level. Furthermore, the ILGA country score and education level, as contextual variables, exert prominent effects at country level. Empirical evidence from the Special Eurobarometer 2015 on Opinions as well as a simulation experiment support the usefulness of the approach.},
  archive      = {J_JOAS},
  author       = {Stefania Capecchi and Maurizio Curtarelli},
  doi          = {10.1080/02664763.2019.1639643},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {554-567},
  shortjournal = {J. Appl. Stat.},
  title        = {A mixture model to assess perception of discrimination on grounds of sexual orientation for policy considerations},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A closed testing procedure for comparison between successive
variances. <em>JOAS</em>, <em>47</em>(3), 541–553. (<a
href="https://doi.org/10.1080/02664763.2019.1645819">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a sequence of k independent normally distributed random samples from k populations with different means and variances, it is common to know which successive populations differ significantly with respect to a parameter. In this article, a stepwise test procedure is proposed for simultaneously testing the difference between successive normal populations with respect to the variance. The proposed procedure controls the family-wise error rate strongly. Critical constants, obtained numerically, are tabulated for the use of the proposed procedure. An extension of the proposed procedure for detecting the difference between scale parameters of successive two-parameter exponential populations is given. A Monte Carlo simulation study of the power comparison of the proposed procedure with a single-step procedure revealed that the proposed stepwise procedure performs better than the single-step procedure. Finally, a numerical example is also given to illustrate the advantage of the proposed procedure in comparison to the single-step procedure.},
  archive      = {J_JOAS},
  author       = {Navdeep Singh and Parminder Singh},
  doi          = {10.1080/02664763.2019.1645819},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {541-553},
  shortjournal = {J. Appl. Stat.},
  title        = {A closed testing procedure for comparison between successive variances},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysis of batched service time data using gaussian and
semi-parametric kernel models. <em>JOAS</em>, <em>47</em>(3), 524–540.
(<a href="https://doi.org/10.1080/02664763.2019.1645820">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Batched data is a type of data where each observed data value is the sum of a number of grouped (batched) latent ones obtained under different conditions. Batched data arises in various practical backgrounds and is often found in social studies and management sector. The analysis of such data is analytically challenging due to its structural complexity. In this article, we describe how to analyze batched service time data, estimate the mean and variance of each batch that are latent. We in particular focus on the situation when the observed total time includes an unknown proportion of non-service time. To address this problem, we propose a Gaussian model for efficiency as well as a semi-parametric kernel density model for robustness. We evaluate the performance of both proposed methods through simulation studies and then applied our methods to analyze a batched data.},
  archive      = {J_JOAS},
  author       = {Xueying Wang and Chunxiao Zhou and Kepher Makambi and Ao Yuan and Jaeil Ahn},
  doi          = {10.1080/02664763.2019.1645820},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {524-540},
  shortjournal = {J. Appl. Stat.},
  title        = {Analysis of batched service time data using gaussian and semi-parametric kernel models},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On some aspects of a zero-inflated overdispersed model and
its applications. <em>JOAS</em>, <em>47</em>(3), 506–523. (<a
href="https://doi.org/10.1080/02664763.2019.1645098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Count data with excess zeros are so common in several areas of scientific research. In particular, the zero-inflated version of count data models has been used for modelling data sets with excessive number of zeros. In this regard, zero-inflated Poisson distribution has received much attention in the literature. Through this paper, we propose a generalized class of zero-inflated Poisson distribution namely ‘zero-inflated Hermite distribution (ZIHD)’, which can be considered as a more flexible class of zero-inflated Poisson-type distribution suitable for tackling overdispersed data sets. Here we investigate several important properties of the ZIHD along with a discussion on certain inference aspects of the model. Certain test procedures for checking zero-inflation have also been developed and these tests have been investigated by using simulation studies. Further, two real life data applications are given for illustrating the usefulness of the model.},
  archive      = {J_JOAS},
  author       = {C. Satheesh Kumar and Rakhi Ramachandran},
  doi          = {10.1080/02664763.2019.1645098},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {506-523},
  shortjournal = {J. Appl. Stat.},
  title        = {On some aspects of a zero-inflated overdispersed model and its applications},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A geometric approach for computing tolerance bounds for
elastic functional data. <em>JOAS</em>, <em>47</em>(3), 481–505. (<a
href="https://doi.org/10.1080/02664763.2019.1645818">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a method for constructing tolerance bounds for functional data with random warping variability. In particular, we define a generative, probabilistic model for the amplitude and phase components of such observations, which parsimoniously characterizes variability in the baseline data. Based on the proposed model, we define two different types of tolerance bounds that are able to measure both types of variability, and as a result, identify when the data has gone beyond the bounds of amplitude and/or phase. The first functional tolerance bounds are computed via a bootstrap procedure on the geometric space of amplitude and phase functions. The second functional tolerance bounds utilize functional Principal Component Analysis to construct a tolerance factor. This work is motivated by two main applications: process control and disease monitoring. The problem of statistical analysis and modeling of functional data in process control is important in determining when a production has moved beyond a baseline. Similarly, in biomedical applications, doctors use long, approximately periodic signals (such as the electrocardiogram) to diagnose and monitor diseases. In this context, it is desirable to identify abnormalities in these signals. We additionally consider a simulated example to assess our approach and compare it to two existing methods.},
  archive      = {J_JOAS},
  author       = {J. Derek Tucker and John R. Lewis and Caleb King and Sebastian Kurtek},
  doi          = {10.1080/02664763.2019.1645818},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {481-505},
  shortjournal = {J. Appl. Stat.},
  title        = {A geometric approach for computing tolerance bounds for elastic functional data},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The efficiency of run rules schemes for the multivariate
coefficient of variation: A markov chain approach. <em>JOAS</em>,
<em>47</em>(3), 460–480. (<a
href="https://doi.org/10.1080/02664763.2019.1643296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Control charts are one of the important tools to monitor quality. The coefficient of variation (CV) is a common measure of dispersion in many real-life applications. Recently, CV control charts are proposed to monitor processes which do not have a constant mean and a standard deviation which changes with the mean. These processes cannot be monitored by standard control charts which monitor the mean and/or standard deviation. This research proposes the monitoring of the multivariate coefficient of variation (MCV) by means of run rules (RR MCV) control charts, which is not available in the existing literature. The design of these charts is obtained using a Markov-chain approach. The proposed charts are simple to implement. The performance of the RR MCV and Shewhart MCV (SH MCV) charts are compared in terms of the average run length (ARL) and the expected average run length (EARL). An example is illustrated based on a real dataset. The findings revealed that the performance of the proposed charts surpasses the SH MCV chart for detecting small and moderate MCV shifts.},
  archive      = {J_JOAS},
  author       = {XinYing Chew and Khai Wah Khaw and Wai Chung Yeong},
  doi          = {10.1080/02664763.2019.1643296},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {460-480},
  shortjournal = {J. Appl. Stat.},
  title        = {The efficiency of run rules schemes for the multivariate coefficient of variation: A markov chain approach},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysis of interval-censored competing risks data under
missing causes. <em>JOAS</em>, <em>47</em>(3), 439–459. (<a
href="https://doi.org/10.1080/02664763.2019.1642309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, interval-censored competing risks data are analyzed when some of the causes of failure are missing. The vertical modeling approach has been proposed here. This approach utilizes the data to extract information to the maximum possible extent especially when some causes of failure are missing. The maximum likelihood estimates of the model parameters are obtained. The asymptotic confidence intervals for the model parameters are constructed using approaches based on observed Fisher information matrix, and parametric bootstrap. A simulation study is considered in detail to assess the performance of the point and interval estimators. It is observed that the proposed analysis performs better than the complete case analysis. This establishes the fact that the our methodology is an extremely useful technique for interval-censored competing risks data when some of the causes of failure are missing. Such analysis seems to be quite useful for smaller sample sizes where complete case analysis may have a significant impact on the inferential procedures. Through Monte Carlo simulations, the effect of a possible model misspecification is also assessed on the basis of the cumulative incidence function. For illustration purposes, three datasets are analyzed and in all cases the conclusion appears to be quite realistic.},
  archive      = {J_JOAS},
  author       = {Debanjan Mitra and Ujjwal Das and Kalyan Das},
  doi          = {10.1080/02664763.2019.1642309},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {439-459},
  shortjournal = {J. Appl. Stat.},
  title        = {Analysis of interval-censored competing risks data under missing causes},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian analysis of immigration in europe with generalized
logistic regression. <em>JOAS</em>, <em>47</em>(3), 424–438. (<a
href="https://doi.org/10.1080/02664763.2019.1642310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The number of immigrants moving to and settling in Europe has increased over the past decade, making migration one of the most topical and pressing issues in European politics. It is without a doubt that immigration has multiple impacts, in terms of economy, society and culture, on the European Union. It is fundamental to policy-makers to correctly evaluate people&#39;s attitudes towards immigration when designing integration policies. Of critical interest is to properly discriminate between subjects who are favourable towards immigration from those who are against it. Public opinions on migration are typically coded as binary responses in surveys. However, traditional methods, such as the standard logistic regression, may suffer from computational issues and are often not able to accurately model survey information. In this paper we propose an efficient Bayesian approach for modelling binary response data based on the generalized logistic regression. We show how the proposed approach provides an increased flexibility compared to traditional methods, due to its ability to capture heavy and light tails. The power of our methodology is tested through simulation studies and is illustrated using European Social Survey data on immigration collected in different European countries in 2016–2017.},
  archive      = {J_JOAS},
  author       = {Luciana Dalla Valle and Fabrizio Leisen and Luca Rossini and Weixuan Zhu},
  doi          = {10.1080/02664763.2019.1642310},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {424-438},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian analysis of immigration in europe with generalized logistic regression},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Validation of burr XII inverse rayleigh model via a modified
chi-squared goodness-of-fit test. <em>JOAS</em>, <em>47</em>(3),
393–423. (<a
href="https://doi.org/10.1080/02664763.2019.1639642">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a new three parameter distribution called the Burr XII inverse Rayleigh model, this model is a generalization of the inverse Rayleigh distribution using the Burr XII family introduced by Cordeiro et al. [ The burr XII system of densities: properties, regression model and applications . J. Stat. Comput. Simul. 88 (2018), pp. 432–456]. After studying the statistical characterization of this model, we construct a modified chi-squared goodness-of-fit test based on the Nikulin–Rao–Robson statistic in the presence of two cases: censored and complete data. We describe the theory and the mechanism of the Y n 2 statistic test which can be used in survival and reliability data analysis. We use the maximum likelihood estimators based on initial non grouped data. Then, we conduct numerical simulations to reinforce the results. For showing the applicability of our model in various fields, we illustrate it and the proposed test by applications to two real data sets for complete data case and two other data sets in the presence of right censored.},
  archive      = {J_JOAS},
  author       = {Hafida Goual and Haitham M. Yousof},
  doi          = {10.1080/02664763.2019.1639642},
  journal      = {Journal of Applied Statistics},
  number       = {3},
  pages        = {393-423},
  shortjournal = {J. Appl. Stat.},
  title        = {Validation of burr XII inverse rayleigh model via a modified chi-squared goodness-of-fit test},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unsupervised classification of eclipsing binary light curves
through k-medoids clustering. <em>JOAS</em>, <em>47</em>(2), 376–392.
(<a href="https://doi.org/10.1080/02664763.2019.1635574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes k -medoids clustering method to reveal the distinct groups of 1318 variable stars in the Galaxy based on their light curves, where each light curve represents the graph of brightness of the star against time. To overcome the deficiencies of subjective traditional classification, we separate the stars more scientifically according to their geometrical configuration and show that our approach outperforms the existing classification schemes in astronomy. It results in two optimum groups of eclipsing binaries corresponding to bright, massive systems and fainter, less massive systems.},
  archive      = {J_JOAS},
  author       = {Soumita Modak and Tanuka Chattopadhyay and Asis Kumar Chattopadhyay},
  doi          = {10.1080/02664763.2019.1635574},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {376-392},
  shortjournal = {J. Appl. Stat.},
  title        = {Unsupervised classification of eclipsing binary light curves through k-medoids clustering},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new two-parameter exponentiated discrete lindley
distribution: Properties, estimation and applications. <em>JOAS</em>,
<em>47</em>(2), 354–375. (<a
href="https://doi.org/10.1080/02664763.2019.1638893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new two-parameter exponentiated discrete Lindley distribution. A wide range of its structural properties are investigated. This includes the shape of the probability mass function, hazard rate function, moments, skewness, kurtosis, stress–strength reliability, mean residual lifetime, mean past lifetime, order statistics and L-moment statistics. The hazard rate function can be increasing, decreasing, decreasing–increasing–decreasing, increasing–decreasing–increasing, unimodal, bathtub, and J -shaped depending on its parameters values. Two methods are used herein to estimate the model parameters, namely, the maximum likelihood, and the proportion. A detailed simulation study is carried out to examine the bias and mean square error of maximum likelihood and proportion estimators. The flexibility of the proposed model is explained by using four distinctive data sets. It can serve as an alternative model to other lifetime distributions in the existing statistical literature for modeling positive real data in many areas.},
  archive      = {J_JOAS},
  author       = {M. El-Morshedy and M. S. Eliwa and H. Nagy},
  doi          = {10.1080/02664763.2019.1638893},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {354-375},
  shortjournal = {J. Appl. Stat.},
  title        = {A new two-parameter exponentiated discrete lindley distribution: Properties, estimation and applications},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Zero-inflated beta distribution applied to word frequency
and lexical dispersion in corpus linguistics. <em>JOAS</em>,
<em>47</em>(2), 337–353. (<a
href="https://doi.org/10.1080/02664763.2019.1636941">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Corpus linguistics is the study of language as expressed in a body of texts or documents. The relative frequency of a word within a text and the dispersion of the word across the collection of texts provide information about the word&#39;s prominence and diffusion, respectively. In practice, people tend to use a relatively small number of words in a language&#39;s inventory of words and thus a large number of words in the lexicon are rarely employed. The zero-inflated beta distribution enables one to model the relative frequency of a word in a text since some texts may not even contain the word under study. In this paper, the expectation of a word&#39;s prominence and dispersion are defined under the zero-inflated beta model. Estimates of a word&#39;s prominence and dispersion are computed for words in the British National Corpus 1994 (BNC), a 100 million word collection of written and spoken language of a wide range of British English. The relationship between a word&#39;s prominence and dispersion is discussed as well as measures that are functions of both prominence and dispersion.},
  archive      = {J_JOAS},
  author       = {Brent Burch and Jesse Egbert},
  doi          = {10.1080/02664763.2019.1636941},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {337-353},
  shortjournal = {J. Appl. Stat.},
  title        = {Zero-inflated beta distribution applied to word frequency and lexical dispersion in corpus linguistics},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Depth functions and mutidimensional medians on minimal
spanning trees. <em>JOAS</em>, <em>47</em>(2), 323–336. (<a
href="https://doi.org/10.1080/02664763.2019.1636939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on the minimal spanning tree (MST) of the observed data set, the paper introduces new notions of data depth and medians for multivariate data. The MST of a data set of size n is the MST of the complete weighted undirected graph on n vertices, where the edge weights are the pairwise distances of the data points. We study several properties of the MST-based depth functions. We consider the corresponding multidimensional medians, investigate their robustness and computational complexity. An example illustrates the use of the MST-based depth functions.},
  archive      = {J_JOAS},
  author       = {Mengta Yang and Reza Modarres and Lingzhe Guo},
  doi          = {10.1080/02664763.2019.1636939},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {323-336},
  shortjournal = {J. Appl. Stat.},
  title        = {Depth functions and mutidimensional medians on minimal spanning trees},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An extended poisson family of life distribution: A unified
approach in competitive and complementary risks. <em>JOAS</em>,
<em>47</em>(2), 306–322. (<a
href="https://doi.org/10.1080/02664763.2019.1644488">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a new approach to generate flexible parametric families of distributions. These models arise on competitive and complementary risks scenario, in which the lifetime associated with a particular risk is not observable; rather, we observe only the minimum/maximum lifetime value among all risks. The latent variables have a zero-truncated Poisson distribution. For the proposed family of distribution, the extra shape parameter has an important physical interpretation in the competing and complementary risks scenario. The mathematical properties and inferential procedures are discussed. The proposed approach is applied in some existing distributions in which it is fully illustrated by an important data set.},
  archive      = {J_JOAS},
  author       = {Pedro L. Ramos and Dipak K. Dey and Francisco Louzada and Victor H. Lachos},
  doi          = {10.1080/02664763.2019.1644488},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {306-322},
  shortjournal = {J. Appl. Stat.},
  title        = {An extended poisson family of life distribution: A unified approach in competitive and complementary risks},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust inference in the multilevel zero-inflated negative
binomial model. <em>JOAS</em>, <em>47</em>(2), 287–305. (<a
href="https://doi.org/10.1080/02664763.2019.1636942">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A popular way to model correlated count data with excess zeros and over-dispersion simultaneously is by means of the multilevel zero-inflated negative binomial (MZINB) distribution. Due to the complexity of the likelihood of these models, numerical methods such as the EM algorithm are used to estimate parameters. On the other hand, in the presence of outliers or when mixture components are poorly separated, the likelihood-based methods can become unstable. To overcome this challenge, we extend the robust expectation-solution (RES) approach for building a robust estimator of the regression parameters in the MZINB model. This approach achieves robustness by applying robust estimating equations in the S-step instead of estimating equations in the M-step of the EM algorithm. The robust estimation equation in the logistic component only weighs the design matrix (X) and reduces the effect of the leverage points, but in the negative binomial component, the influence of deviations on the response (Y) and design matrix (X) are bound separately. Simulation studies under various settings show that the RES algorithm gives us consistent estimates with smaller biases than the EM algorithm under contaminations. The RES algorithm applies to the data of the DMFT index and the fertility rate data.},
  archive      = {J_JOAS},
  author       = {Eghbal Zandkarimi and Abbas Moghimbeigi and Hossein Mahjub and Reza Majdzadeh},
  doi          = {10.1080/02664763.2019.1636942},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {287-305},
  shortjournal = {J. Appl. Stat.},
  title        = {Robust inference in the multilevel zero-inflated negative binomial model},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Zero-inflated bell regression models for count data.
<em>JOAS</em>, <em>47</em>(2), 265–286. (<a
href="https://doi.org/10.1080/02664763.2019.1636940">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By starting from the one-parameter Bell distribution proposed recently in the statistic literature, we introduce the zero-inflated Bell family of distributions. Additionally, on the basis of the proposed zero-inflated distribution, a novel zero-inflated regression model is proposed, which is quite simple and may be an interesting alternative to usual zero-inflated regression models for count data. We consider a frequentist approach to perform inferences, and the maximum likelihood method is employed to estimate the zero-inflated Bell regression parameters. Monte Carlo simulations indicate that the maximum likelihood method is quite effective to estimate the zero-inflated Bell regression parameters. We also propose the Pearson residuals for the new zero-inflated regression model to assess departures from model assumptions. Additionally, the global and local influence methods are discussed. In particular, the normal curvature for studying local influence is derived under case weighting perturbation scheme. Finally, an application to the count of infected blood cells is considered to illustrate the usefulness of the zero-inflated Bell regression model in practice. The results suggest that the new zero-inflated Bell regression is more appropriate to model these count data than other familiar zero-inflated (or not) regression models commonly used in practice.},
  archive      = {J_JOAS},
  author       = {Artur J. Lemonte and Germán Moreno-Arenas and Fredy Castellares},
  doi          = {10.1080/02664763.2019.1636940},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {265-286},
  shortjournal = {J. Appl. Stat.},
  title        = {Zero-inflated bell regression models for count data},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian change-point modelling of the effects of
3-points-for-a-win rule in football. <em>JOAS</em>, <em>47</em>(2),
248–264. (<a
href="https://doi.org/10.1080/02664763.2019.1635572">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine the effects of the 3-points-for-a-win (3pfaw) rule in the football world. Data that form the basis of our analyses come from seven leagues around the world (Albania, Brazil, England, Germany, Poland, Romania, and Scotland) and consist of mean goals and proportions of decided matches over a period of about six years before- and about seven years after the introduction of the rule in the respective leagues. Bayesian change-point analyses and Shiryaev-Roberts tests show that the rule had no effects on the mean goals but, indeed, had increasing effects on the proportions of decided matches in most of the leagues studied. This, in turn, implies that while the rule has given teams the incentive to aim at winning matches, such aim was not achieved by scoring excess goals. Instead, it was achieved by scoring enough goals in order to win and, at the same time, defending enough in order not to lose. Our results are in accordance with recent findings on comparing the values of attack and defense - that, in top-level football, not conceding a goal is more valuable than scoring a single goal.},
  archive      = {J_JOAS},
  author       = {Gebrenegus Ghilagaber and Parfait Munezero},
  doi          = {10.1080/02664763.2019.1635572},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {248-264},
  shortjournal = {J. Appl. Stat.},
  title        = {Bayesian change-point modelling of the effects of 3-points-for-a-win rule in football},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation in the single change-point hazard function for
interval-censored data with a cure fraction. <em>JOAS</em>,
<em>47</em>(2), 231–247. (<a
href="https://doi.org/10.1080/02664763.2019.1635571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In reliability or survival analysis, the hazard function plays a significant part for it can display the instantaneous failure rate at any time point. In practice, the abrupt change in hazard function at an unknown time point may occur after a maintenance activity or major operation. Under these circumstances, identifying the change point and estimating the size of the change are meaningful. In this paper, we assume that the hazard function is piecewise constant with a single jump at an unknown time. We propose the single change-point model for interval-censored survival data with a cure fraction. Estimation methods for the proposed model are investigated, and large-sample properties of the estimators are established. Simulation studies are carried out to evaluate the performance of the estimating method. The liver cancer data and breast cancer data are analyzed as the applications.},
  archive      = {J_JOAS},
  author       = {Bing Wang and Xiaoguang Wang and Lixin Song},
  doi          = {10.1080/02664763.2019.1635571},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {231-247},
  shortjournal = {J. Appl. Stat.},
  title        = {Estimation in the single change-point hazard function for interval-censored data with a cure fraction},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variable selection under multicollinearity using modified
log penalty. <em>JOAS</em>, <em>47</em>(2), 201–230. (<a
href="https://doi.org/10.1080/02664763.2019.1637829">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To handle the multicollinearity issues in the regression analysis, a class of ‘strictly concave penalty function’ is described in this paper. As an example, a new penalty function called ‘modified log penalty’ is introduced. The penalized estimator based on strictly concave penalties enjoys the oracle property under certain regularity conditions discussed in the literature. In the multicollinearity cases where such conditions are not applicable, the behaviors of the strictly concave penalties are discussed through examples involving strongly correlated covariates. Real data examples and simulation studies are provided to show the finite-sample performance of the modified log penalty in terms of prediction error under scenarios exhibiting multicollinearity.},
  archive      = {J_JOAS},
  author       = {Van Cuong Nguyen and Chi Tim Ng},
  doi          = {10.1080/02664763.2019.1637829},
  journal      = {Journal of Applied Statistics},
  number       = {2},
  pages        = {201-230},
  shortjournal = {J. Appl. Stat.},
  title        = {Variable selection under multicollinearity using modified log penalty},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Detection of excessive activities in time series of graphs.
<em>JOAS</em>, <em>47</em>(1), 176–200. (<a
href="https://doi.org/10.1080/02664763.2019.1634680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considerable efforts have been made to apply scan statistics in detecting fraudulent or excessive activities in dynamic email networks. However, previous studies are mostly based on the fixed and disjoint windows, and on the assumption of short-term stationarity of the series, which might result in loss of information and error in detecting excessive activities. Here we devise scan statistics with variable and overlapping windows on stationary time series of organizational emails with a two-step process, and use likelihood function to rank the clusters. We initially estimate the log-likelihood ratio to obtain a primary cluster of communications using the Poisson model on email count series, and then extract neighborhood ego subnetworks around the observed primary cluster to obtain more refined cluster by invoking the graph invariant betweenness as the locality statistic using the binomial model. The results were then compared with the non-parametric maximum likelihood estimation method, and the residual analysis of ARMA model fitted to the time series of graph edit distance. We demonstrate that the scan statistics with two-step process is effective in detecting excessive activity in large dynamic social networks.},
  archive      = {J_JOAS},
  author       = {Suchismita Goswami and Edward J. Wegman},
  doi          = {10.1080/02664763.2019.1634680},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {176-200},
  shortjournal = {J. Appl. Stat.},
  title        = {Detection of excessive activities in time series of graphs},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Confidence interval, prediction interval and tolerance
limits for a two-parameter rayleigh distribution. <em>JOAS</em>,
<em>47</em>(1), 160–175. (<a
href="https://doi.org/10.1080/02664763.2019.1634681">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problems of interval estimating the parameters and the mean of a two-parameter Rayleigh distribution are considered. We propose pivotal-based methods for constructing confidence intervals for the mean, quantiles, survival probability and for constructing prediction intervals for the mean of a future sample. Pivotal quantities based on the maximum likelihood estimates (MLEs), moment estimates (MEs) and the L-moments estimates (L-MEs) are proposed. Interval estimates based on them are compared via Monte Carlo simulation. Comparison studies indicate that the results based on the MEs and the L-MEs are very similar. The results based on the MLEs are slightly better than those based on the MEs and the L-MEs for small to moderate sample sizes. The methods are illustrated using an example involving lifetime data.},
  archive      = {J_JOAS},
  author       = {K. Krishnamoorthy and Dustin Waguespack and Ngan Hoang-Nguyen-Thuy},
  doi          = {10.1080/02664763.2019.1634681},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {160-175},
  shortjournal = {J. Appl. Stat.},
  title        = {Confidence interval, prediction interval and tolerance limits for a two-parameter rayleigh distribution},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust estimation for longitudinal data based upon minimum
hellinger distance. <em>JOAS</em>, <em>47</em>(1), 150–159. (<a
href="https://doi.org/10.1080/02664763.2019.1635573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized linear mixed models have been widely used in the analysis of correlated data in a lot of research areas. The linear mixed model with normal errors has been a popular model for the analysis of repeated measures and longitudinal data. Outliers, however, can severely have an wrong influence on the linear mixed model. The aforementioned model has not fully taken those severe outliers into consideration. One of the popular robust estimation methods, M-estimator attains robustness at the expense of first-order or second-order efficiency whereas minimum Hellinger distance estimator is efficient and robust. In this paper, we propose more robust Bayesian version of parameter estimation via pseudo posterior distribution based on minimum Hellinger distance. It accommodates an appropriate nonparametric kernel density estimation for longitudinal data to require the proposed cross-validation estimator. We conduct simulation study and real data study with the orthodontic study data and the Alzheimers Disease (AD) study data. In simulation study, the proposed method shows smaller biases, mean squared errors, and standard errors than the (residual) maximum likelihood method (REML) in the presence of outliers or missing values. In real data analysis, standard errors and variance-covariance components for the proposed method in two data sets are shown to be lower than those for REML method.},
  archive      = {J_JOAS},
  author       = {Joonsung Kang},
  doi          = {10.1080/02664763.2019.1635573},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {150-159},
  shortjournal = {J. Appl. Stat.},
  title        = {Robust estimation for longitudinal data based upon minimum hellinger distance},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Impact of missing data on the prediction of random fields.
<em>JOAS</em>, <em>47</em>(1), 132–149. (<a
href="https://doi.org/10.1080/02664763.2019.1633286">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of this paper is to treat the prediction problems where a number of observations are missing to the quarter-plane past of a stationary random field. Our aim is to quantify the influence of missing values on the prediction by giving the simple bounds for the prediction error variance. These bounds allow to characterize the random fields for which the missing observations do not affect the prediction. Simulation experiments and an application to real data are presented.},
  archive      = {J_JOAS},
  author       = {Abdelghani Hamaz and Ouerdia Arezki and Farida Achemine},
  doi          = {10.1080/02664763.2019.1633286},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {132-149},
  shortjournal = {J. Appl. Stat.},
  title        = {Impact of missing data on the prediction of random fields},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Likelihood-based quantile autoregressive distributed lag
models and its applications. <em>JOAS</em>, <em>47</em>(1), 117–131. (<a
href="https://doi.org/10.1080/02664763.2019.1633285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time lag effect exists widely in the course of economic operation. Some economic variables are affected not only by various factors in the current period but also by various factors in the past and even their own past values. As a class of dynamical models, autoregressive distributed lag (ARDL) models are frequently used to conduct dynamic regression analysis. In this paper, we are interested in the quantile regression (QR) modeling of the ARDL model in a dynamic framework. By combining the working likelihood of asymmetric Laplace distribution (ALD) with the expectation–maximization (EM) algorithm into the considered ARDL model, the iterative weighted least square estimators (IWLSE) are derived. Some Monte Carlo simulations are implemented to evaluate the performance of the proposed estimation method. A dataset of the consumption of electricity by residential customers is analyzed to illustrate the application.},
  archive      = {J_JOAS},
  author       = {Yuzhu Tian and Liyong Wang and Manlai Tang and Yanchao Zang and Maozai Tian},
  doi          = {10.1080/02664763.2019.1633285},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {117-131},
  shortjournal = {J. Appl. Stat.},
  title        = {Likelihood-based quantile autoregressive distributed lag models and its applications},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Testing for heteroskedasticity in two-way fixed effects
panel data models. <em>JOAS</em>, <em>47</em>(1), 91–116. (<a
href="https://doi.org/10.1080/02664763.2019.1634682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new method for testing heteroskedasticity in two-way fixed effects panel data models under two important scenarios where the cross-sectional dimension is large and the temporal dimension is either large or fixed. Specifically, we will develop test statistics for both cases under the conditional moment framework, and derive their asymptotic distributions under both the null and alternative hypotheses. The proposed tests are distribution free and can easily be implemented using the simple auxiliary regressions. Simulation studies and two real data analyses demonstrate that our proposed tests perform well in practice, and may have the potential for wide application in econometric models with panel data.},
  archive      = {J_JOAS},
  author       = {Sanying Feng and Gaorong Li and Tiejun Tong and Shuanghua Luo},
  doi          = {10.1080/02664763.2019.1634682},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {91-116},
  shortjournal = {J. Appl. Stat.},
  title        = {Testing for heteroskedasticity in two-way fixed effects panel data models},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RR-plot: A descriptive tool for regression observations.
<em>JOAS</em>, <em>47</em>(1), 76–90. (<a
href="https://doi.org/10.1080/02664763.2019.1631268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a regression depth versus regression depth plot, hereafter RR -plot, for regression observations based on the halfspace regression depth. Areas of application of this tool include: the visualization of hypothesis tests about regression coefficients, and of the comparison between regression observations from different models, etc. Some characterization theorems are also provided to address the rationale of this RR -plot.},
  archive      = {J_JOAS},
  author       = {Xiaohui Liu and Yang He},
  doi          = {10.1080/02664763.2019.1631268},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {76-90},
  shortjournal = {J. Appl. Stat.},
  title        = {RR-plot: A descriptive tool for regression observations},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A kernel nonparametric quantile estimator for right-censored
competing risks data. <em>JOAS</em>, <em>47</em>(1), 61–75. (<a
href="https://doi.org/10.1080/02664763.2019.1631267">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In medical and epidemiological studies, it is often interest to study time-to-event distributions under competing risks that involve two or more failure types. Nonparametric analysis of competing risks is typically focused on the cumulative incidence function or nonparametric quantile function. However, the existing estimators may be very unstable due to their unsmoothness. In this paper, we propose a kernel nonparametric quantile estimator for right-censored competing risks data, which is a smoothed version of Peng and Fine&#39;s nonparametric quantile estimator. We establish the Bahadur representation of the proposed estimator. The convergence rate of the remainder term for the proposed estimator is substantially faster than Peng and Fine&#39;s quantile estimator. The pointwise confidence intervals and simultaneous confidence bands of the quantile functions are also derived. Simulation studies illustrate the good performance of the proposed estimator. The methodology is demonstrated with two applications of the Supreme Court Judge data and AIDSSI data.},
  archive      = {J_JOAS},
  author       = {Caiyun Fan and Gang Ding and Feipeng Zhang},
  doi          = {10.1080/02664763.2019.1631267},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {61-75},
  shortjournal = {J. Appl. Stat.},
  title        = {A kernel nonparametric quantile estimator for right-censored competing risks data},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quantile regression for general spatial panel data models
with fixed effects. <em>JOAS</em>, <em>47</em>(1), 45–60. (<a
href="https://doi.org/10.1080/02664763.2019.1628190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers the quantile regression model with both individual fixed effect and time period effect for general spatial panel data. Fixed effects quantile regression estimators based on instrumental variable method will be proposed. Asymptotic properties of the proposed estimators will be developed. Simulations are conducted to study the performance of the proposed method. We will illustrate our methodologies using a cigarettes demand data set.},
  archive      = {J_JOAS},
  author       = {Xiaowen Dai and Zhen Yan and Maozai Tian and ManLai Tang},
  doi          = {10.1080/02664763.2019.1628190},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {45-60},
  shortjournal = {J. Appl. Stat.},
  title        = {Quantile regression for general spatial panel data models with fixed effects},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Statistical regression analysis of functional and shape
data. <em>JOAS</em>, <em>47</em>(1), 28–44. (<a
href="https://doi.org/10.1080/02664763.2019.1669541">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a multivariate regression model when responses or predictors are on nonlinear manifolds, rather than on Euclidean spaces. The nonlinear constraint makes the problem challenging and needs to be studied carefully. By performing principal component analysis (PCA) on tangent space of manifold, we use principal directions instead in the model. Then, the ordinary regression tools can be utilized. We apply the framework to both shape data (ozone hole contours) and functional data (spectrums of absorbance of meat in Tecator dataset). Specifically, we adopt the square-root velocity function representation and parametrization-invariant metric. Experimental results have shown that we can not only perform powerful regression analysis on the non-Euclidean data but also achieve high prediction accuracy by the constructed model.},
  archive      = {J_JOAS},
  author       = {Mengmeng Guo and Jingyong Su and Li Sun and Guofeng Cao},
  doi          = {10.1080/02664763.2019.1669541},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {28-44},
  shortjournal = {J. Appl. Stat.},
  title        = {Statistical regression analysis of functional and shape data},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Influence analysis for the generalized waring regression
model. <em>JOAS</em>, <em>47</em>(1), 1–27. (<a
href="https://doi.org/10.1080/02664763.2019.1670148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider a regression model under the generalized Waring distribution for modeling count data. We develop and implement local influence diagnostic techniques based on likelihood displacement. Also we develop case-deletion methods. The generalized Waring regression model is presented as a mixture of the Negative Binomial and the Beta II distributions, and it is compared to the Negative Binomial and Waring regression models. Estimation is performed by maximum likelihood function. The influence measures developed in this paper are applied to a Spanish football league data set. Empirical results show that the generalized Waring regression model performs better when compared to the Negative Binomial and Waring regression models. Technical details are presented in the Appendix.},
  archive      = {J_JOAS},
  author       = {Luisa Rivas and Manuel Galea},
  doi          = {10.1080/02664763.2019.1670148},
  journal      = {Journal of Applied Statistics},
  number       = {1},
  pages        = {1-27},
  shortjournal = {J. Appl. Stat.},
  title        = {Influence analysis for the generalized waring regression model},
  volume       = {47},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
