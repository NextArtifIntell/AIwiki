<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JBES_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jbes---70">JBES - 70</h2>
<ul>
<li><details>
<summary>
(2020). Minimum contrast empirical likelihood inference of
discontinuity in density*. <em>JBES</em>, <em>38</em>(4), 934–950. (<a
href="https://doi.org/10.1080/07350015.2019.1617155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the asymptotic properties of a simple empirical-likelihood-based inference method for discontinuity in density. The parameter of interest is a function of two one-sided limits of the probability density function at (possibly) two cut-off points. Our approach is based on the first-order conditions from a minimum contrast problem. We investigate both first-order and second-order properties of the proposed method. We characterize the leading coverage error of our inference method and propose a coverage-error-optimal (CE-optimal, hereafter) bandwidth selector. We show that the empirical likelihood ratio statistic is Bartlett correctable. An important special case is the manipulation testing problem in a regression discontinuity design (RDD), where the parameter of interest is the density difference at a known threshold. In RDD, the continuity of the density of the assignment variable at the threshold is considered as a “no-manipulation” behavioral assumption, which is a testable implication of an identifying condition for the local average treatment effect. When specialized to the manipulation testing problem, the CE-optimal bandwidth selector has an explicit form. We propose a data-driven CE-optimal bandwidth selector for use in practice. Results from Monte Carlo simulations are presented. Usefulness of our method is illustrated by an empirical example.},
  archive      = {J_JBES},
  author       = {Jun Ma and Hugo Jales and Zhengfei Yu},
  doi          = {10.1080/07350015.2019.1617155},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {934-950},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Minimum contrast empirical likelihood inference of discontinuity in density*},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Forecast error variance decompositions with local
projections. <em>JBES</em>, <em>38</em>(4), 921–933. (<a
href="https://doi.org/10.1080/07350015.2019.1610661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose and study properties of an estimator of the forecast error variance decomposition in the local projections framework. We find for empirically relevant sample sizes that, after being bias-corrected with bootstrap, our estimator performs well in simulations. We also illustrate the workings of our estimator empirically for monetary policy and productivity shocks. KEYWORDS: Forecast error variance decomposition; Local projections.},
  archive      = {J_JBES},
  author       = {Yuriy Gorodnichenko and Byoungchan Lee},
  doi          = {10.1080/07350015.2019.1610661},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {921-933},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Forecast error variance decompositions with local projections},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bounds on average and quantile treatment effects on duration
outcomes under censoring, selection, and noncompliance. <em>JBES</em>,
<em>38</em>(4), 901–920. (<a
href="https://doi.org/10.1080/07350015.2019.1609975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of assessing the effects of a treatment on duration outcomes using data from a randomized evaluation with noncompliance. For such settings, we derive nonparametric sharp bounds for average and quantile treatment effects addressing three pervasive problems simultaneously: self-selection into the spell of interest, endogenous censoring of the duration outcome, and noncompliance with the assigned treatment. Ignoring any of these issues could yield biased estimates of the effects. Notably, the proposed bounds do not impose the independent censoring assumption—which is commonly used to address censoring but is likely to fail in important settings—or exclusion restrictions to address endogeneity of censoring and selection. Instead, they employ monotonicity and stochastic dominance assumptions. To illustrate the use of these bounds we assess the effects of the Job Corps (JC) training program on its participants’ last complete employment spell duration. Our estimated bounds suggest that JC participation may increase the average duration of the last complete employment spell before week 208 after randomization by at least 5.6 log points (5.8\%) for individuals who comply with their treatment assignment and experience a complete employment spell whether or not they enrolled in JC. The estimated quantile treatment effects suggest the impacts may be heterogeneous, and strengthen our conclusions based on the estimated average effects.},
  archive      = {J_JBES},
  author       = {German Blanco and Xuan Chen and Carlos A. Flores and Alfonso Flores-Lagunes},
  doi          = {10.1080/07350015.2019.1609975},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {901-920},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Bounds on average and quantile treatment effects on duration outcomes under censoring, selection, and noncompliance},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Matching using sufficient dimension reduction for causal
inference. <em>JBES</em>, <em>38</em>(4), 888–900. (<a
href="https://doi.org/10.1080/07350015.2019.1609974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To estimate causal treatment effects, we propose a new matching approach based on the reduced covariates obtained from sufficient dimension reduction. Compared with the original covariates and the propensity score, which are commonly used for matching in the literature, the reduced covariates are nonparametrically estimable and are effective in imputing the missing potential outcomes, under a mild assumption on the low-dimensional structure of the data. Under the ignorability assumption, the consistency of the proposed approach requires a weaker common support condition. In addition, researchers are allowed to employ different reduced covariates to find matched subjects for different treatment groups. We develop relevant asymptotic results and conduct simulation studies as well as real data analysis to illustrate the usefulness of the proposed approach.},
  archive      = {J_JBES},
  author       = {Wei Luo and Yeying Zhu},
  doi          = {10.1080/07350015.2019.1609974},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {888-900},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Matching using sufficient dimension reduction for causal inference},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian forecasting of many count-valued time series.
<em>JBES</em>, <em>38</em>(4), 872–887. (<a
href="https://doi.org/10.1080/07350015.2019.1604372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop and exemplify application of new classes of dynamic models for time series of nonnegative counts. Our novel univariate models combine dynamic generalized linear models for binary and conditionally Poisson time series, with dynamic random effects for over-dispersion. These models estimate dynamic regression coefficients in both binary and nonzero count components. Sequential Bayesian analysis allows fast, parallel analysis of sets of decoupled time series. New multivariate models then enable information sharing in contexts when data at a more highly aggregated level provide more incisive inferences on shared patterns such as trends and seasonality. A novel multiscale approach—one new example of the concept of decouple/recouple in time series—enables information sharing across series. This incorporates cross-series linkages while insulating parallel estimation of univariate models, and hence enables scalability in the number of series. The major motivating context is supermarket sales forecasting. Detailed examples drawn from a case study in multistep forecasting of sales of a number of related items showcase forecasting of multiple series, with discussion of forecast accuracy metrics, comparisons with existing methods, and broader questions of probabilistic forecast assessment.},
  archive      = {J_JBES},
  author       = {Lindsay R. Berry and Mike West},
  doi          = {10.1080/07350015.2019.1604372},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {872-887},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Bayesian forecasting of many count-valued time series},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A stochastic volatility model with realized measures for
option pricing. <em>JBES</em>, <em>38</em>(4), 856–871. (<a
href="https://doi.org/10.1080/07350015.2019.1604371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Based on the fact that realized measures of volatility are affected by measurement errors, we introduce a new family of discrete-time stochastic volatility models having two measurement equations relating both observed returns and realized measures to the latent conditional variance. A semi-analytical option pricing framework is developed for this class of models. In addition, we provide analytical filtering and smoothing recursions for the basic specification of the model, and an effective MCMC algorithm for its richer variants. The empirical analysis shows the effectiveness of filtering and smoothing realized measures in inflating the latent volatility persistence—the crucial parameter in pricing Standard and Poor’s 500 Index options.},
  archive      = {J_JBES},
  author       = {Giacomo Bormetti and Roberto Casarin and Fulvio Corsi and Giulia Livieri},
  doi          = {10.1080/07350015.2019.1604371},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {856-871},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {A stochastic volatility model with realized measures for option pricing},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multivariate stochastic volatility model with realized
volatilities and pairwise realized correlations. <em>JBES</em>,
<em>38</em>(4), 839–855. (<a
href="https://doi.org/10.1080/07350015.2019.1602048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although stochastic volatility and GARCH (generalized autoregressive conditional heteroscedasticity) models have successfully described the volatility dynamics of univariate asset returns, extending them to the multivariate models with dynamic correlations has been difficult due to several major problems. First, there are too many parameters to estimate if available data are only daily returns, which results in unstable estimates. One solution to this problem is to incorporate additional observations based on intraday asset returns, such as realized covariances. Second, since multivariate asset returns are not synchronously traded, we have to use the largest time intervals such that all asset returns are observed to compute the realized covariance matrices. However, in this study, we fail to make full use of the available intraday informations when there are less frequently traded assets. Third, it is not straightforward to guarantee that the estimated (and the realized) covariance matrices are positive definite. Our contributions are the following: (1) we obtain the stable parameter estimates for the dynamic correlation models using the realized measures, (2) we make full use of intraday informations by using pairwise realized correlations, (3) the covariance matrices are guaranteed to be positive definite, (4) we avoid the arbitrariness of the ordering of asset returns, (5) we propose the flexible correlation structure model (e.g., such as setting some correlations to be zero if necessary), and (6) the parsimonious specification for the leverage effect is proposed. Our proposed models are applied to the daily returns of nine U.S. stocks with their realized volatilities and pairwise realized correlations and are shown to outperform the existing models with respect to portfolio performances.},
  archive      = {J_JBES},
  author       = {Yuta Yamauchi and Yasuhiro Omori},
  doi          = {10.1080/07350015.2019.1602048},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {839-855},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Multivariate stochastic volatility model with realized volatilities and pairwise realized correlations},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Treatment effects with heterogeneous externalities.
<em>JBES</em>, <em>38</em>(4), 826–838. (<a
href="https://doi.org/10.1080/07350015.2019.1592755">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a new method for estimating heterogeneous externalities in policy analysis when social interactions take the linear-in-means form. We establish that the parameters of interest can be identified and consistently estimated using specific functions of the share of the eligible population. We also study the finite sample performance of the proposed estimators using Monte Carlo simulations. The method is illustrated using data on the PROGRESA program. We find that more than 50\% of the effects of the program on schooling attendance are due to externalities, which are heterogeneous within and between poor and nonpoor households.},
  archive      = {J_JBES},
  author       = {Tiziano Arduini and Eleonora Patacchini and Edoardo Rainone},
  doi          = {10.1080/07350015.2019.1592755},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {826-838},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Treatment effects with heterogeneous externalities},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Asymptotically uniform tests after consistent model
selection in the linear regression model. <em>JBES</em>, <em>38</em>(4),
810–825. (<a
href="https://doi.org/10.1080/07350015.2019.1592754">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article specializes the critical value (CV) methods that are based upon (refinements of) Bonferroni bounds, introduced by McCloskey to a problem of inference after consistent model selection in a general linear regression model. The post-selection problem is formulated to mimic common empirical practice and is applicable to both cross-sectional and time series contexts. We provide algorithms for constructing the CVs in this setting and establish uniform asymptotic size results for the resulting tests. The practical implementation of the CVs is illustrated in an empirical application to the effect of classroom size on test scores.},
  archive      = {J_JBES},
  author       = {Adam McCloskey},
  doi          = {10.1080/07350015.2019.1592754},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {810-825},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Asymptotically uniform tests after consistent model selection in the linear regression model},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comparing possibly misspecified forecasts. <em>JBES</em>,
<em>38</em>(4), 796–809. (<a
href="https://doi.org/10.1080/07350015.2019.1585256">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent work has emphasized the importance of evaluating estimates of a statistical functional (such as a conditional mean, quantile, or distribution) using a loss function that is consistent for the functional of interest, of which there is an infinite number. If forecasters all use correctly specified models free from estimation error, and if the information sets of competing forecasters are nested, then the ranking induced by a single consistent loss function is sufficient for the ranking by any consistent loss function. This article shows, via analytical results and realistic simulation-based analyses, that the presence of misspecified models, parameter estimation error, or nonnested information sets, leads generally to sensitivity to the choice of (consistent) loss function. Thus, rather than merely specifying the target functional, which narrows the set of relevant loss functions only to the class of loss functions consistent for that functional, forecast consumers or survey designers should specify the single specific loss function that will be used to evaluate forecasts. An application to survey forecasts of U.S. inflation illustrates the results.},
  archive      = {J_JBES},
  author       = {Andrew J. Patton},
  doi          = {10.1080/07350015.2019.1585256},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {796-809},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Comparing possibly misspecified forecasts},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A smooth nonparametric, multivariate, mixed-data
location-scale test. <em>JBES</em>, <em>38</em>(4), 784–795. (<a
href="https://doi.org/10.1080/07350015.2019.1574227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A number of tests have been proposed for assessing the location-scale assumption that is often invoked by practitioners. Existing approaches include Kolmogorov–Smirnov and Cramer–von Mises statistics that each involve measures of divergence between unknown joint distribution functions and products of marginal distributions. In practice, the unknown distribution functions embedded in these statistics are typically approximated using nonsmooth empirical distribution functions (EDFs). In a recent article, Li, Li, and Racine establish the benefits of smoothing the EDF for inference, though their theoretical results are limited to the case where the covariates are observed and the distributions unobserved, while in the current setting some covariates and their distributions are unobserved (i.e., the test relies on population error terms from a location-scale model) which necessarily involves a separate theoretical approach. We demonstrate how replacing the nonsmooth distributions of unobservables with their kernel-smoothed sample counterparts can lead to substantial power improvements, and extend existing approaches to the smooth multivariate and mixed continuous and discrete data setting in the presence of unobservables. Theoretical underpinnings are provided, Monte Carlo simulations are undertaken to assess finite-sample performance, and illustrative applications are provided.},
  archive      = {J_JBES},
  author       = {Jeffrey S. Racine and Ingrid Van Keilegom},
  doi          = {10.1080/07350015.2019.1574227},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {784-795},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {A smooth nonparametric, multivariate, mixed-data location-scale test},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Which factors are risk factors in asset pricing? A model
scan framework. <em>JBES</em>, <em>38</em>(4), 771–783. (<a
href="https://doi.org/10.1080/07350015.2019.1573684">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A key question for understanding the cross-section of expected returns of equities is the following: which factors, from a given collection of factors, are risk factors, equivalently, which factors are in the stochastic discount factor (SDF)? Though the SDF is unobserved, assumptions about which factors (from the available set of factors) are in the SDF restricts the joint distribution of factors in specific ways, as a consequence of the economic theory of asset pricing. A different starting collection of factors that go into the SDF leads to a different set of restrictions on the joint distribution of factors. The conditional distribution of equity returns has the same restricted form, regardless of what is assumed about the factors in the SDF, as long as the factors are traded, and hence the distribution of asset returns is irrelevant for isolating the risk-factors. The restricted factors models are distinct (nonnested) and do not arise by omitting or including a variable from a full model, thus precluding analysis by standard statistical variable selection methods, such as those based on the lasso and its variants. Instead, we develop what we call a Bayesian model scan strategy in which each factor is allowed to enter or not enter the SDF and the resulting restricted models (of which there are 114,674 in our empirical study) are simultaneously confronted with the data. We use a Student-t distribution for the factors, and model-specific independent Student-t distribution for the location parameters, a training sample to fix prior locations, and a creative way to arrive at the joint distribution of several other model-specific parameters from a single prior distribution. This allows our method to be essentially a scaleable and tuned-black-box method that can be applied across our large model space with little to no user-intervention. The model marginal likelihoods, and implied posterior model probabilities, are compared with the prior probability of 1/114,674 of each model to find the best-supported model, and thus the factors most likely to be in the SDF. We provide detailed simulation evidence about the high finite-sample accuracy of the method. Our empirical study with 13 leading factors reveals that the highest marginal likelihood model is a Student-t distributed factor model with 5 degrees of freedom and 8 risk factors.},
  archive      = {J_JBES},
  author       = {Siddhartha Chib and Xiaming Zeng},
  doi          = {10.1080/07350015.2019.1573684},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {771-783},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Which factors are risk factors in asset pricing? a model scan framework},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparametric estimation of search costs for differentiated
products: Evidence from medigap. <em>JBES</em>, <em>38</em>(4), 754–770.
(<a href="https://doi.org/10.1080/07350015.2019.1573683">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops a method to estimate search frictions as well as preference parameters in differentiated product markets. Search costs are nonparametrically identified, which means our method can be used to estimate search costs in differentiated product markets that lack a suitable search cost shifter. We apply our model to the U.S. Medigap insurance market. We find that search costs are substantial: the estimated median cost of searching for an insurer is $30. Using the estimated parameters we find that eliminating search costs could result in price decreases of as much as $71 (or 4.7\%), along with increases in average consumer welfare of up to $374.},
  archive      = {J_JBES},
  author       = {Haizhen Lin and Matthijs R. Wildenbeest},
  doi          = {10.1080/07350015.2019.1573683},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {754-770},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Nonparametric estimation of search costs for differentiated products: Evidence from medigap},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Partial identification of economic mobility: With an
application to the united states. <em>JBES</em>, <em>38</em>(4),
732–753. (<a
href="https://doi.org/10.1080/07350015.2019.1569527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The economic mobility of individuals and households is of fundamental interest. While many measures of economic mobility exist, reliance on transition matrices remains pervasive due to simplicity and ease of interpretation. However, estimation of transition matrices is complicated by the well-acknowledged problem of measurement error in self-reported and even administrative data. Existing methods of addressing measurement error are complex, rely on numerous strong assumptions, and often require data from more than two periods. In this article, we investigate what can be learned about economic mobility as measured via transition matrices while formally accounting for measurement error in a reasonably transparent manner. To do so, we develop a nonparametric partial identification approach to bound transition probabilities under various assumptions on the measurement error and mobility processes. This approach is applied to panel data from the United States to explore short-run mobility before and after the Great Recession.},
  archive      = {J_JBES},
  author       = {Daniel L. Millimet and Hao Li and Punarjit Roychowdhury},
  doi          = {10.1080/07350015.2019.1569527},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {732-753},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Partial identification of economic mobility: With an application to the united states},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Rejoinder. <em>JBES</em>, <em>38</em>(4), 731. (<a
href="https://doi.org/10.1080/07350015.2020.1791886">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JBES},
  author       = {Isaiah Andrews and Matthew Gentzkow and Jesse M. Shapiro},
  doi          = {10.1080/07350015.2020.1791886},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {731},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Rejoinder},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discussion on “ transparency in structural research” by i.
Andrews, m. Gentkow and j. shapiro. <em>JBES</em>, <em>38</em>(4),
728–730. (<a
href="https://doi.org/10.1080/07350015.2020.1804917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide a complementary approach to global sensitivity analysis that should be useful for empirical work in economics.},
  archive      = {J_JBES},
  author       = {Elie Tamer},
  doi          = {10.1080/07350015.2020.1804917},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {728-730},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Discussion on “ transparency in structural research” by i. andrews, m. gentkow and j. shapiro},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Thoughts on “transparency in structural research.”
<em>JBES</em>, <em>38</em>(4), 726–727. (<a
href="https://doi.org/10.1080/07350015.2020.1796396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JBES},
  author       = {Christopher Taber},
  doi          = {10.1080/07350015.2020.1796396},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {726-727},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Thoughts on “Transparency in structural research”},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discussion of “transparency in structural research” by
isaiah andrews, matthew gentzkow, and jesse shapiro. <em>JBES</em>,
<em>38</em>(4), 723–725. (<a
href="https://doi.org/10.1080/07350015.2020.1790377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JBES},
  author       = {Stéphane Bonhomme},
  doi          = {10.1080/07350015.2020.1790377},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {723-725},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Discussion of “Transparency in structural research” by isaiah andrews, matthew gentzkow, and jesse shapiro},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Transparency in structural research. <em>JBES</em>,
<em>38</em>(4), 711–722. (<a
href="https://doi.org/10.1080/07350015.2020.1796395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a formal definition of transparency in empirical research and apply it to structural estimation in economics. We discuss how some existing practices can be understood as attempts to improve transparency, and we suggest ways to improve current practice, emphasizing approaches that impose a minimal computational burden on the researcher. We illustrate with examples.},
  archive      = {J_JBES},
  author       = {Isaiah Andrews and Matthew Gentzkow and Jesse M. Shapiro},
  doi          = {10.1080/07350015.2020.1796395},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {10},
  number       = {4},
  pages        = {711-722},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Transparency in structural research},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation and selection of spatial weight matrix in a
spatial lag model. <em>JBES</em>, <em>38</em>(3), 693–710. (<a
href="https://doi.org/10.1080/07350015.2019.1569526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial econometric models allow for interactions among variables through the specification of a spatial weight matrix. Practitioners often face the risk of misspecification of such a matrix. In many problems a number of potential specifications exist, such as geographic distances, or various economic quantities among variables. We propose estimating the best linear combination of these specifications, added with a potentially sparse adjustment matrix. The coefficients in the linear combination, together with the sparse adjustment matrix, are subjected to variable selection through the adaptive least absolute shrinkage and selection operator (LASSO). As a special case, if no spatial weight matrices are specified, the sparse adjustment matrix becomes a sparse spatial weight matrix estimator of our model. Our method can therefore, be seen as a unified framework for the estimation and selection of a spatial weight matrix. The rate of convergence of all proposed estimators are determined when the number of time series variables can grow faster than the number of time points for data, while oracle properties for all penalized estimators are presented. Simulations and an application to stocks data confirms the good performance of our procedure.},
  archive      = {J_JBES},
  author       = {Clifford Lam and Pedro C.L. Souza},
  doi          = {10.1080/07350015.2019.1569526},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {693-710},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Estimation and selection of spatial weight matrix in a spatial lag model},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local parametric estimation in high frequency data.
<em>JBES</em>, <em>38</em>(3), 679–692. (<a
href="https://doi.org/10.1080/07350015.2019.1566731">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We give a general time-varying parameter model, where the multidimensional parameter possibly includes jumps. The quantity of interest is defined as the integrated value over time of the parameter process Θ = T − 1 ∫ T 0 θ ∗ t d t Θ = T − 1 ∫ 0 T θ t * d t Θ=T−1∫0Tθt*dt . We provide a local parametric estimator (LPE) of Θ and conditions under which we can show the central limit theorem. Roughly speaking those conditions correspond to some uniform limit theory in the parametric version of the problem. The framework is restricted to the specific convergence rate n 1∕2 . Several examples of LPE are studied: estimation of volatility, powers of volatility, volatility when incorporating trading information and time-varying MA(1).},
  archive      = {J_JBES},
  author       = {Yoann Potiron and Per Mykland},
  doi          = {10.1080/07350015.2019.1566731},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {679-692},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Local parametric estimation in high frequency data},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The pricing of tail risk and the equity premium: Evidence
from international option markets. <em>JBES</em>, <em>38</em>(3),
662–678. (<a
href="https://doi.org/10.1080/07350015.2018.1564318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We explore the pricing of tail risk as manifest in index options across international equity markets. The risk premium associated with negative tail events displays persistent shifts, unrelated to volatility. This tail risk premium is a potent predictor of future returns for all the indices, while the option-implied volatility only forecasts the future return variation. Hence, compensation for negative jump risk is the primary driver of the equity premium, whereas the reward for pure diffusive variance risk is unrelated to future equity returns. We also document pronounced commonalities, suggesting a high degree of integration among the major global equity markets. KEY WORDS: Equity risk premium; International option markets; Predictability; Tail risk; Variance risk premium.},
  archive      = {J_JBES},
  author       = {Torben G. Andersen and Nicola Fusari and Viktor Todorov},
  doi          = {10.1080/07350015.2018.1564318},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {662-678},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {The pricing of tail risk and the equity premium: Evidence from international option markets},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic vector mode regression. <em>JBES</em>,
<em>38</em>(3), 647–661. (<a
href="https://doi.org/10.1080/07350015.2018.1562935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the semiparametric estimation of the conditional mode of a random vector that has a continuous conditional joint density with a well-defined global mode. A novel full-system estimator is proposed and its asymptotic properties are studied. We specifically consider the estimation of vector autoregressive conditional mode models and of systems of linear simultaneous equations defined by mode restrictions. The proposed estimator is easy to implement and simulations suggest that it is reasonably behaved in finite samples. An empirical example illustrates the application of the proposed methods, including its use to obtain multistep forecasts and to construct impulse response functions.},
  archive      = {J_JBES},
  author       = {Gordon C. R. Kemp and Paulo M. D. C. Parente and J. M. C. Santos Silva},
  doi          = {10.1080/07350015.2018.1562935},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {647-661},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Dynamic vector mode regression},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Heterogeneity in expectations, risk tolerance, and household
stock shares: The attenuation puzzle. <em>JBES</em>, <em>38</em>(3),
633–646. (<a
href="https://doi.org/10.1080/07350015.2018.1549560">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article jointly estimates the relationship between stock share and expectations and risk preferences. The survey allows individual-level, quantitative estimates of risk tolerance and of the perceived mean, and variance of stock returns. These estimates have economically and statistically significant association for the distribution of stock shares with relative magnitudes in proportion with the predictions of theories. Incorporating survey measurement error in the estimation model increases the estimated associations 2-fold, but they are still substantially attenuated being only about 5\% of what benchmark finance theories predict. Because of the careful attention in the estimation to measurement error, the attenuation likely arises from economic behavior rather than errors in variables.},
  archive      = {J_JBES},
  author       = {John Ameriks and Gábor Kézdi and Minjoon Lee and Matthew D. Shapiro},
  doi          = {10.1080/07350015.2018.1549560},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {633-646},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Heterogeneity in expectations, risk tolerance, and household stock shares: The attenuation puzzle},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Empirical likelihood for high frequency data. <em>JBES</em>,
<em>38</em>(3), 621–632. (<a
href="https://doi.org/10.1080/07350015.2018.1549051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces empirical likelihood methods for interval estimation and hypothesis testing on volatility measures in some high frequency data environments. We propose a modified empirical likelihood statistic that is asymptotically pivotal under infill asymptotics, where the number of high frequency observations in a fixed time interval increases to infinity. The proposed statistic is extended to be robust to the presence of jumps and microstructure noise. We also provide an empirical likelihood-based test to detect the presence of jumps. Furthermore, we study higher-order properties of a general family of nonparametric likelihood statistics and show that a particular statistic admits a Bartlett correction: a higher-order refinement to achieve better coverage or size properties. Simulation and a real data example illustrate the usefulness of our approach.},
  archive      = {J_JBES},
  author       = {Lorenzo Camponovo and Yukitoshi Matsushita and Taisuke Otsu},
  doi          = {10.1080/07350015.2018.1549051},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {621-632},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Empirical likelihood for high frequency data},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The promise and pitfalls of differences-in-differences:
Reflections on 16 and pregnant and other applications. <em>JBES</em>,
<em>38</em>(3), 613–620. (<a
href="https://doi.org/10.1080/07350015.2018.1546591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We use the exchange between Kearney/Levine and Jaeger/Joyce/Kaestner on 16 and Pregnant to reexamine the use of DiD as a response to the failure of nature to properly design an experiment for us. We argue that (1) any DiD paper should address why the original levels of the experimental and control groups differed, and why this would not impact trends, (2) the parallel trends argument requires a justification of the chosen functional form and that the use of the interaction coefficients in probit and logit may be justified in some cases, and (3) parallel trends in the period prior to treatment is suggestive of counterfactual parallel trends, but parallel pre-trends is neither necessary nor sufficient for the parallel counterfactual trends condition to hold. Importantly, the purely statistical approach uses pretesting and thus, generates the wrong standard errors. Moreover, we underline the dangers of implicitly or explicitly accepting the null hypothesis when failing to reject the absence of a differential pre-trend.},
  archive      = {J_JBES},
  author       = {Ariella Kahn-Lang and Kevin Lang},
  doi          = {10.1080/07350015.2018.1546591},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {613-620},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {The promise and pitfalls of differences-in-differences: Reflections on 16 and pregnant and other applications},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). External validity in fuzzy regression discontinuity designs.
<em>JBES</em>, <em>38</em>(3), 593–612. (<a
href="https://doi.org/10.1080/07350015.2018.1546590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fuzzy regression discontinuity designs identify the local average treatment effect (LATE) for the subpopulation of compliers, and with forcing variable equal to the threshold. We develop methods that assess the external validity of LATE to other compliance groups at the threshold, and allow for identification away from the threshold. Specifically, we focus on the equality of outcome distributions between treated compliers and always-takers, and between untreated compliers and never-takers. These equalities imply continuity of expected outcomes conditional on both the forcing variable and the treatment status. We recommend that researchers plot these conditional expectations and test for discontinuities at the threshold to assess external validity. We provide new commands in STATA and MATLAB to implement our proposed procedures.},
  archive      = {J_JBES},
  author       = {Marinho Bertanha and Guido W. Imbens},
  doi          = {10.1080/07350015.2018.1546590},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {593-612},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {External validity in fuzzy regression discontinuity designs},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A smooth transition finite mixture model for accommodating
unobserved heterogeneity. <em>JBES</em>, <em>38</em>(3), 580–592. (<a
href="https://doi.org/10.1080/07350015.2018.1543126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While the smooth transition (ST) model has become popular in business and economics, the treatment of unobserved heterogeneity within these models has received limited attention. We propose a ST finite mixture (STFM) model which simultaneously estimates the presence of time-varying effects and unobserved heterogeneity in a panel data context. Our objective is to accurately recover the heterogeneous effects of our independent variables of interest while simultaneously allowing these effects to vary over time. Accomplishing this objective may provide valuable insights for managers and policy makers. The STFM model nests several well-known ST and threshold models. We develop the specification, estimation, and model selection criteria for the STFM model using Bayesian methods. We also provide a theoretical assessment of the flexibility of the STFM model when the number of regimes grows with the sample size. In an extensive simulation study, we show that ignoring unobserved heterogeneity can lead to distorted parameter estimates, and that the STFM model is fairly robust when underlying model assumptions are violated. Empirically, we estimate the effects of in-game promotions on game attendance in Major League Baseball. Empirical results show that the STFM model outperforms all its nested versions. Supplementary materials for this article are available online.},
  archive      = {J_JBES},
  author       = {Eelco Kappe and Wayne S. DeSarbo and Marcelo C. Medeiros},
  doi          = {10.1080/07350015.2018.1543126},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {580-592},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {A smooth transition finite mixture model for accommodating unobserved heterogeneity},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new class of change point test statistics of rényi type.
<em>JBES</em>, <em>38</em>(3), 570–579. (<a
href="https://doi.org/10.1080/07350015.2018.1537923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new class of change point test statistics is proposed that utilizes a weighting and trimming scheme for the cumulative sum (CUSUM) process inspired by Rényi. A thorough asymptotic analysis and simulations both demonstrate that this new class of statistics possess superior power compared to traditional change point statistics based on the CUSUM process when the change point is near the beginning or end of the sample. Generalizations of these “Rényi” statistics are also developed to test for changes in the parameters in linear and nonlinear regression models, and in generalized method of moments estimation. In these contexts, we applied the proposed statistics, as well as several others, to test for changes in the coefficients of Fama–French factor models. We observed that the Rényi statistic was the most effective in terms of retrospectively detecting change points that occur near the endpoints of the sample.},
  archive      = {J_JBES},
  author       = {Lajos Horváth and Curtis Miller and Gregory Rice},
  doi          = {10.1080/07350015.2018.1537923},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {570-579},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {A new class of change point test statistics of rényi type},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Heterogeneity and unemployment dynamics. <em>JBES</em>,
<em>38</em>(3), 554–569. (<a
href="https://doi.org/10.1080/07350015.2018.1530116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many previous articles have studied the contribution of inflows and outflows to the cyclical variation in unemployment, but ignored the critical role of unobserved heterogeneity across workers. This article develops new estimates of unemployment inflows and outflows that allow for unobserved heterogeneity as well as direct effects of unemployment duration on unemployment-exit probabilities. With this approach, we can measure the contribution of different shocks to the short-run, medium-run, and long-run variance of unemployment as well as to specific historical episodes. We conclude that changes in the composition of new inflows into unemployment are the most important factor in economic recessions.},
  archive      = {J_JBES},
  author       = {Hie Joo Ahn and James D. Hamilton},
  doi          = {10.1080/07350015.2018.1530116},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {554-569},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Heterogeneity and unemployment dynamics},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Term structures of inflation expectations and real interest
rates. <em>JBES</em>, <em>38</em>(3), 542–553. (<a
href="https://doi.org/10.1080/07350015.2018.1529599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I use a statistical model to combine various surveys to produce a term structure of inflation expectations—inflation expectations at any horizon—and an associated term structure of real interest rates. Inflation expectations extracted from this model track realized inflation quite well, and in terms of forecast accuracy, they are at par with or superior to some popular alternatives. The real interest rates obtained from the model follow Treasury Inflation-Protected Securities rates as well.},
  archive      = {J_JBES},
  author       = {S. Borağan Aruoba},
  doi          = {10.1080/07350015.2018.1529599},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {542-553},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Term structures of inflation expectations and real interest rates},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Implications of return predictability for consumption
dynamics and asset pricing. <em>JBES</em>, <em>38</em>(3), 527–541. (<a
href="https://doi.org/10.1080/07350015.2018.1527702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two broad classes of consumption dynamics—long-run risks and rare disasters—have proven successful in explaining the equity premium puzzle when used in conjunction with recursive preferences. We show that bounds a-là Gallant, Hansen, and Tauchen that restrict the volatility of the stochastic discount factor by conditioning on a set of return predictors constitute a useful tool to discriminate between these alternative dynamics. In particular, we document that models that rely on rare disasters meet comfortably the bounds independently of the forecasting horizon and the asset returns used to construct the bounds. However, the specific nature of disasters is a relevant characteristic at the 1-year horizon: disasters that unfold over multiple years are more successful in meeting the predictors-based bounds than one-period disasters. Instead, at the 5-year horizon, the sole presence of disasters—even if one-period and permanent—is sufficient for the model to satisfy the bounds. Finally, the bounds point to multiple volatility components in consumption as a promising dimension for long-run risk models.},
  archive      = {J_JBES},
  author       = {Carlo A. Favero and Fulvio Ortu and Andrea Tamoni and Haoxi Yang},
  doi          = {10.1080/07350015.2018.1527702},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {527-541},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Implications of return predictability for consumption dynamics and asset pricing},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stationary points for parametric stochastic frontier models.
<em>JBES</em>, <em>38</em>(3), 516–526. (<a
href="https://doi.org/10.1080/07350015.2018.1526088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stationary point results on the normal–half-normal stochastic frontier model are generalized using the theory of the Dirac delta, and distribution-free conditions are established to ensure a stationary point in the likelihood as the variance of the inefficiency distribution goes to zero. Stability of the stationary point and “wrong skew” results are derived or simulated for common parametric assumptions on the model. We discuss identification and extensions to more general stochastic frontier models.},
  archive      = {J_JBES},
  author       = {William C. Horrace and Ian A. Wright},
  doi          = {10.1080/07350015.2018.1526088},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {516-526},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Stationary points for parametric stochastic frontier models},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Somewhere between utopia and dystopia: Choosing from
multiple incomparable prospects. <em>JBES</em>, <em>38</em>(3), 502–515.
(<a href="https://doi.org/10.1080/07350015.2018.1515765">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many fields of decision making, choices have to be made from multiple alternatives, but stochastic dominance rules do not yield a complete ordering due to incomparability of some or all of the prospects. For ranking incomparable prospects, a “Utopia Index” measuring the proximity to a lower envelope of integrated distribution functions is proposed. Economic interpretations in terms of Expected Utility are provided for the envelope and deviations from it. The analysis generalizes the existing Almost Stochastic Dominance concept from pairwise comparison to a joint analysis of an arbitrary number of prospects. The limit distribution for the empirical counterpart of the index for a general class of dynamic processes is derived together with a consistent and feasible inference procedure based on subsampling techniques. Empirical applications to Chinese household income data and historical investment returns data show that, in every choice set, a single prospect is ranked above all alternatives at conventional significance levels, despite the incomparability problem.},
  archive      = {J_JBES},
  author       = {Gordon Anderson and Thierry Post and Yoon-Jae Whang},
  doi          = {10.1080/07350015.2018.1515765},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {502-515},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Somewhere between utopia and dystopia: Choosing from multiple incomparable prospects},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Time series seasonal adjustment using regularized singular
value decomposition. <em>JBES</em>, <em>38</em>(3), 487–501. (<a
href="https://doi.org/10.1080/07350015.2018.1515081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new seasonal adjustment method based on the Regularized Singular Value Decomposition (RSVD) of the matrix obtained by reshaping the seasonal time series data. The method is flexible enough to capture two kinds of seasonality: the fixed seasonality that does not change over time and the time-varying seasonality that varies from one season to another. RSVD represents the time-varying seasonality by a linear combination of several seasonal patterns. The right singular vectors capture multiple seasonal patterns, and the corresponding left singular vectors capture the magnitudes of those seasonal patterns and how they change over time. By assuming the time-varying seasonal patterns change smoothly over time, the RSVD uses penalized least squares with a roughness penalty to effectively extract the left singular vectors. The proposed method applies to seasonal time-series data with a stationary or nonstationary nonseasonal component. The method also has a variant that can handle the case that an abrupt change (i.e., break) may occur in the magnitudes of seasonal patterns. Our proposed method compares favorably with the state-of-art X-13ARIMA-SEATS program on both simulated and real data examples.},
  archive      = {J_JBES},
  author       = {Wei Lin and Jianhua Z. Huang and Tucker McElroy},
  doi          = {10.1080/07350015.2018.1515081},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {7},
  number       = {3},
  pages        = {487-501},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Time series seasonal adjustment using regularized singular value decomposition},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time macroeconomic forecasting with a heteroscedastic
inversion copula. <em>JBES</em>, <em>38</em>(2), 470–486. (<a
href="https://doi.org/10.1080/07350015.2018.1514309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a growing interest in allowing for asymmetry in the density forecasts of macroeconomic variables. In multivariate time series, this can be achieved with a copula model, where both serial and cross-sectional dependence is captured by a copula function, and the margins are nonparametric. Yet most existing copulas cannot capture heteroscedasticity well, which is a feature of many economic and financial time series. To do so, we propose a new copula created by the inversion of a multivariate unobserved component stochastic volatility model, and show how to estimate it using Bayesian methods. We fit the copula model to real-time data on five quarterly U.S. economic and financial variables. The copula model captures heteroscedasticity, dependence in the level, time-variation in higher moments, bounds on variables and other features. Over the window 1975Q1–2016Q2, the real-time density forecasts of all the macroeconomic variables exhibit time-varying asymmetry. In particular, forecasts of GDP growth have increased negative skew during recessions. The point and density forecasts from the copula model are competitive with those from benchmark models—particularly for inflation, a short-term interest rate and current quarter GDP growth. Supplementary materials for this article are available online.},
  archive      = {J_JBES},
  author       = {Rubén Loaiza-Maya and Michael Stanley Smith},
  doi          = {10.1080/07350015.2018.1514309},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {470-486},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Real-time macroeconomic forecasting with a heteroscedastic inversion copula},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Earnings dynamics and measurement error in matched survey
and administrative data. <em>JBES</em>, <em>38</em>(2), 457–469. (<a
href="https://doi.org/10.1080/07350015.2018.1514308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article analyzes earnings dynamics and measurement error using a matched longitudinal sample of individuals’ survey and administrative earnings. In line with previous literature, the reported differences are characterized by both persistent and transitory factors. Estimating a model consistent with past results, survey errors are mean-reverting when administrative reports are assumed correct, but not when this assumption is relaxed. Although most reported earnings variation is true, we conclude that measurement errors dominate observed changes, and that transitory earnings contribute little to overall earnings inequality. The results imply the reliability of matched administrative data should be treated with caution.},
  archive      = {J_JBES},
  author       = {Dean R. Hyslop and Wilbur Townsend},
  doi          = {10.1080/07350015.2018.1514308},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {457-469},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Earnings dynamics and measurement error in matched survey and administrative data},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A comparison of two quantile models with endogeneity.
<em>JBES</em>, <em>38</em>(2), 443–456. (<a
href="https://doi.org/10.1080/07350015.2018.1514307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article studies the relationship between the two most-used quantile models with endogeneity: the instrumental variable quantile regression (IVQR) model (Chernozhukov and Hansen Citation 2005 ) and the local quantile treatment effects (LQTE) model (Abadie, Angrist, and Imbens Citation 2002 ). The key condition of the IVQR model is the rank similarity assumption, a restriction on the evolution of individual ranks across treatment states, under which population quantile treatment effects (QTE) are identified. By contrast, the LQTE model achieves identification through a monotonicity assumption on the selection equation but only identifies QTE for the subpopulation of compliers. This article shows that, despite these differences, there is a close connection between both models: (i) the IVQR estimands correspond to QTE for the compliers at transformed quantile levels and (ii) the IVQR estimand of the average treatment effect is equal to a convex combination of the local average treatment effect and a weighted average of integrated QTE for the compliers. These results do not rely on the rank similarity assumption and therefore provide a characterization of IVQR in settings where this key condition is violated. Underpinning the analysis are novel closed-form representations of the IVQR estimands. I illustrate the theoretical results with two empirical applications.},
  archive      = {J_JBES},
  author       = {Kaspar Wüthrich},
  doi          = {10.1080/07350015.2018.1514307},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {443-456},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {A comparison of two quantile models with endogeneity},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Double-question survey measures for the analysis of
financial bubbles and crashes. <em>JBES</em>, <em>38</em>(2), 428–442.
(<a href="https://doi.org/10.1080/07350015.2018.1513845">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a new double-question survey whereby an individual is presented with two sets of questions; one on beliefs about current asset values and another on price expectations. A theoretical asset pricing model with heterogeneous agents is advanced and the existence of a negative relationship between price expectations and asset valuations is established, and is then tested using survey results on equity, gold, and house prices. Leading indicators of bubbles and crashes are proposed and their potential value is illustrated in the context of a dynamic panel regression of realized house price changes across key Metropolitan Statistical Areas (MSAs) in the U.S. In an out-of-sample forecasting exercise, it is also shown that forecasts of house price changes (pooled across MSAs) that make use of bubble and crash indicators perform significantly better than a benchmark model that only uses lagged and expected house price changes. Supplementary materials for this article are available online.},
  archive      = {J_JBES},
  author       = {M. Hashem Pesaran and Ida Johnsson},
  doi          = {10.1080/07350015.2018.1513845},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {428-442},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Double-question survey measures for the analysis of financial bubbles and crashes},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The role of jumps in volatility spillovers in foreign
exchange markets: Meteor shower and heat waves revisited. <em>JBES</em>,
<em>38</em>(2), 410–427. (<a
href="https://doi.org/10.1080/07350015.2018.1512865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article extends the literature on geographic (heat waves) and intertemporal (meteor showers) foreign exchange volatility transmission to characterize the role of jumps and cross-rate propagation. We employ multivariate heterogenous autoregressive (HAR) models to capture the quasi-long memory properties of volatility and both Shapley–Owen R 2 ’s and portfolio optimization exercises to quantify the contributions of information sets. We conclude that meteor showers (MS) are substantially more influential than heat waves (HW), that jumps play a modest but significant role in volatility transmission, that cross-market propagation of volatility is important, and that allowing for differential HW and MS effects and differential parameters across intraday market segments is valuable. Finally, we illustrate what types of news weaken or strengthen heat wave, meteor shower, continuous, and jump patterns with sensitivity analysis. Supplementary materials for this article are available online.},
  archive      = {J_JBES},
  author       = {Jérôme Lahaye and Christopher Neely},
  doi          = {10.1080/07350015.2018.1512865},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {410-427},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {The role of jumps in volatility spillovers in foreign exchange markets: Meteor shower and heat waves revisited},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Words are the new numbers: A newsy coincident index of the
business cycle. <em>JBES</em>, <em>38</em>(2), 393–409. (<a
href="https://doi.org/10.1080/07350015.2018.1506344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I construct a daily business cycle index based on quarterly GDP growth and textual information contained in a daily business newspaper. The newspaper data are decomposed into time series representing news topics, while the business cycle index is estimated using the topics and a time-varying dynamic factor model where dynamic sparsity is enforced upon the factor loadings using a latent threshold mechanism. The resulting index classifies the phases of the business cycle with almost perfect accuracy and provides broad-based high-frequency information about the type of news that drive or reflect economic fluctuations. In out-of-sample nowcasting experiments, the model is competitive with forecast combination systems and expert judgment, and produces forecasts with predictive power for future revisions in GDP. Thus, news reduces noise. Supplementary materials for this article are available online.},
  archive      = {J_JBES},
  author       = {Leif Anders Thorsrud},
  doi          = {10.1080/07350015.2018.1506344},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {393-409},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Words are the new numbers: A newsy coincident index of the business cycle},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Detecting structural differences in tail dependence of
financial time series. <em>JBES</em>, <em>38</em>(2), 380–392. (<a
href="https://doi.org/10.1080/07350015.2018.1506343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An accurate assessment of tail inequalities and tail asymmetries of financial returns is key for risk management and portfolio allocation. We propose a new test procedure for detecting the full extent of such structural differences in the dependence of bivariate extreme returns. We decompose the testing problem into piecewise multiple comparisons of Cramér–von Mises distances of tail copulas. In this way, tail regions that cause differences in extreme dependence can be located and consequently be targeted by financial strategies. We derive the asymptotic properties of the test and provide a bootstrap approximation for finite samples. Moreover, we account for the multiplicity of the piecewise tail copula comparisons by adjusting individual p -values according to multiple testing techniques. Monte Carlo simulations demonstrate the test’s superior finite-sample properties for common financial tail risk models, both in the iid and the sequentially dependent case. During the last 90 years in U.S. stock markets, our test detects up to 20\% more tail asymmetries than competing tests. This can be attributed to the presence of nonstandard tail dependence structures. We also find evidence for diminishing tail asymmetries during every major financial crisis—except for the 2007–2009 crisis—reflecting a risk-return trade-off for extreme returns. Supplementary materials for this article are available online.},
  archive      = {J_JBES},
  author       = {Carsten Bormann and Melanie Schienle},
  doi          = {10.1080/07350015.2018.1506343},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {380-392},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Detecting structural differences in tail dependence of financial time series},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new approach to identifying the real effects of
uncertainty shocks. <em>JBES</em>, <em>38</em>(2), 367–379. (<a
href="https://doi.org/10.1080/07350015.2018.1506342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces the use of the sign restrictions methodology to identify uncertainty shocks. We apply our methodology to a class of vector autoregression models with stochastic volatility that allow volatility fluctuations to impact the conditional mean. We combine sign restrictions on the conditional mean and conditional second moment impulse responses to identify financial and macro uncertainty shocks. On U.S. data, we find stronger evidence that financial uncertainty shocks lead to a decline in real activity and an easing of the federal funds rate relative to macro uncertainty shocks. Supplementary materials for this article are available online.},
  archive      = {J_JBES},
  author       = {Minchul Shin and Molin Zhong},
  doi          = {10.1080/07350015.2018.1506342},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {367-379},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {A new approach to identifying the real effects of uncertainty shocks},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Is a normal copula the right copula? <em>JBES</em>,
<em>38</em>(2), 350–366. (<a
href="https://doi.org/10.1080/07350015.2018.1505631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive computationally simple and intuitive expressions for score tests of Gaussian copulas against generalized hyperbolic alternatives, including symmetric and asymmetric Student t , and many other examples. We decompose our tests into third and fourth moment components, and obtain one-sided Likelihood Ratio analogs, whose standard asymptotic distribution we provide. Our Monte Carlo exercises confirm the reliable size of parametric bootstrap versions of our tests, and their substantial power gains over alternative procedures. In an empirical application to CRSP stocks, we find that short-term reversals and momentum effects are better captured by non-Gaussian copulas, whose parameters we estimate by indirect inference. Supplementary materials for this article are available online.},
  archive      = {J_JBES},
  author       = {Dante Amengual and Enrique Sentana},
  doi          = {10.1080/07350015.2018.1505631},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {350-366},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Is a normal copula the right copula?},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Change‐point detection in the conditional correlation
structure of multivariate volatility models. <em>JBES</em>,
<em>38</em>(2), 340–349. (<a
href="https://doi.org/10.1080/07350015.2018.1505630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose semiparametric CUSUM tests to detect a change-point in the correlation structures of nonlinear multivariate models with dynamically evolving volatilities. The asymptotic distributions of the proposed statistics are derived under mild conditions. We discuss the applicability of our method to the most often used models, including constant conditional correlation (CCC), dynamic conditional correlation (DCC), BEKK, corrected DCC, and factor models. Our simulations show that, our tests have good size and power properties. Also, even though the near-unit root property distorts the size and power of tests, de-volatizing the data by means of appropriate multivariate volatility models can correct such distortions. We apply the semiparametric CUSUM tests in the attempt to date the occurrence of financial contagion from the US to emerging markets worldwide during the great recession. Supplementary materials for this article are available online.},
  archive      = {J_JBES},
  author       = {Marco Barassi and Lajos Horváth and Yuqian Zhao},
  doi          = {10.1080/07350015.2018.1505630},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {340-349},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Change‐Point detection in the conditional correlation structure of multivariate volatility models},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning and index option returns. <em>JBES</em>,
<em>38</em>(2), 327–339. (<a
href="https://doi.org/10.1080/07350015.2018.1505629">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Little is known about the economic sources that may generate the abnormal returns observed in put index options. We show that the learning process followed by investors may be one such source. We develop an equilibrium model under partial information in which a rational Bayesian learner prices put option contracts. Our model generates put option returns similar to the empirical returns of S&amp;P 500 put index options. This result is not obtained when we analyze alternative setups of the model in which no learning process exists.},
  archive      = {J_JBES},
  author       = {Alejandro Bernales and Gonzalo Cortazar and Luka Salamunic and George Skiadopoulos},
  doi          = {10.1080/07350015.2018.1505629},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {327-339},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Learning and index option returns},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A cautionary tale of evaluating identifying assumptions: Did
reality TV really cause a decline in teenage childbearing?
<em>JBES</em>, <em>38</em>(2), 317–326. (<a
href="https://doi.org/10.1080/07350015.2018.1497510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating policy changes that occur everywhere at the same time is difficult because of the lack of a clear counterfactual. Hoping to address this problem, researchers often proxy for differential exposure using some observed characteristic in the pretreatment period. As a cautionary tale of how difficult identification is in such settings, we re-examine the results of an influential paper by Melissa Kearney and Phillip Levine, who found that the MTV program 16 and Pregnant had a substantial impact on teen birth rates. In what amounts to a difference-in-differences approach, they use the pretreatment levels of MTV viewership across media markets as an instrument. We show that controlling for differential time trends in birth rates by a market&#39;s pretreatment racial/ethnic composition or unemployment rate causes Kearney and Levine&#39;s results to disappear, invalidating the parallel trends assumption necessary for a causal interpretation. Extending the pretreatment period and estimating placebo tests, we find evidence of an “effect” long before 16 and Pregnant started broadcasting. Our results highlight the difficulty of drawing causal inferences from national point-in-time policy changes.},
  archive      = {J_JBES},
  author       = {David A. Jaeger and Theodore J. Joyce and Robert Kaestner},
  doi          = {10.1080/07350015.2018.1497510},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {317-326},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {A cautionary tale of evaluating identifying assumptions: Did reality TV really cause a decline in teenage childbearing?},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Identification and efficiency bounds for the average match
function under conditionally exogenous matching. <em>JBES</em>,
<em>38</em>(2), 303–316. (<a
href="https://doi.org/10.1080/07350015.2018.1497509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider two heterogenous populations of agents who, when matched, jointly produce an output, Y . For example, teachers and classrooms of students together produce achievement, parents raise children, whose life outcomes vary in adulthood, assembly plant managers and workers produce a certain number of cars per month, and lieutenants and their platoons vary in unit effectiveness. Let W ∈ W = { w 1 , … , w J } and X ∈ X = { x 1 , … , x K } denote agent types in the two populations. Consider the following matching mechanism: take a random draw from the W = w j subgroup of the first population and match her with an independent random draw from the X = x k subgroup of the second population. Let β( w j , x k ), the average match function (AMF), denote the expected output associated with this match. We show that (i) the AMF is identified when matching is conditionally exogenous, (ii) conditionally exogenous matching is compatible with a pairwise stable aggregate matching equilibrium under specific informational assumptions, and (iii) we calculate the AMF’s semiparametric efficiency bound.},
  archive      = {J_JBES},
  author       = {Bryan S. Graham and Guido W. Imbens and Geert Ridder},
  doi          = {10.1080/07350015.2018.1497509},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {303-316},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Identification and efficiency bounds for the average match function under conditionally exogenous matching},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Markov-switching three-pass regression filter.
<em>JBES</em>, <em>38</em>(2), 285–302. (<a
href="https://doi.org/10.1080/07350015.2018.1497508">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new approach for the estimation of high-dimensional factor models with regime-switching factor loadings by extending the linear three-pass regression filter to settings where parameters can vary according to Markov processes. The new method, denoted as Markov-switching three-pass regression filter (MS-3PRF), is suitable for datasets with large cross-sectional dimensions, since estimation and inference are straightforward, as opposed to existing regime-switching factor models where computational complexity limits applicability to few variables. In a Monte Carlo experiment, we study the finite sample properties of the MS-3PRF and find that it performs favorably compared with alternative modeling approaches whenever there is structural instability in factor loadings. For empirical applications, we consider forecasting economic activity and bilateral exchange rates, finding that the MS-3PRF approach is competitive in both cases. Supplementary materials for this article are available online.},
  archive      = {J_JBES},
  author       = {Pierre Guérin and Danilo Leiva-Leon and Massimiliano Marcellino},
  doi          = {10.1080/07350015.2018.1497508},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {285-302},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Markov-switching three-pass regression filter},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic effects of credit shocks in a data-rich environment.
<em>JBES</em>, <em>38</em>(2), 272–284. (<a
href="https://doi.org/10.1080/07350015.2018.1497507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine the dynamic effects of credit shocks using a large dataset of U.S. economic and financial indicators in a structural factor model. An identified credit shock resulting in an unanticipated increase in credit spreads causes a large and persistent downturn in indicators of real economic activity, labor market conditions, expectations of future economic conditions, a gradual decline in aggregate price indices, and a decrease in short- and longer-term riskless interest rates. Our identification procedure allows us to perform counterfactual experiments which suggest that credit spread shocks have largely contributed to the deterioration in economic conditions during the Great Recession. Recursive estimation of the model reveals relevant instabilities since 2007 and provides further evidence that monetary policy has partly offset the effects of credit shocks on economic activity. Supplementary materials for this article are available online.},
  archive      = {J_JBES},
  author       = {Jean Boivin and Marc P. Giannoni and Dalibor Stevanović},
  doi          = {10.1080/07350015.2018.1497507},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {272-284},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Dynamic effects of credit shocks in a data-rich environment},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Flexible mixture-amount models using multivariate gaussian
processes. <em>JBES</em>, <em>38</em>(2), 257–271. (<a
href="https://doi.org/10.1080/07350015.2018.1497506">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many products and services can be described as mixtures of components whose proportions sum to one. Specialized models have been developed for relating the mixture component proportions to response variables, such as the preference, quality, and liking of products. If only the mixture component proportions affect the response variable, mixture models suffice to analyze the data. In case the total amount of the mixture also affects the response variable, mixture -amount models are needed. The current strategy for mixture-amount models is to express the response in terms of the mixture component proportions and subsequently specify the corresponding parameters as parametric functions of the amount. Specifying the functional form for these parameters may not be straightforward, and using a flexible functional form usually comes at the cost of a large number of parameters. In this article, we present a new modeling approach that is flexible, but parsimonious in the number of parameters. This new approach uses multivariate Gaussian processes and avoids the necessity to a priori specify the nature of the dependence of the mixture model parameters on the amount of the mixture. We show that this model encompasses two commonly used model specifications as extreme cases. We consider two applications and demonstrate that the new model outperforms standard models for mixture-amount data.},
  archive      = {J_JBES},
  author       = {Aiste Ruseckaite and Dennis Fok and Peter Goos},
  doi          = {10.1080/07350015.2018.1497506},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {257-271},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Flexible mixture-amount models using multivariate gaussian processes},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A stochastic frontier model with endogenous treatment status
and mediator. <em>JBES</em>, <em>38</em>(2), 243–256. (<a
href="https://doi.org/10.1080/07350015.2018.1497504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Government policies are frequently used to promote productivity. Some policies are designed to enhance production technology, while others are meant to improve production efficiency. An important issue to consider when designing and evaluating policies is whether a mediator is required or effective in achieving the desired final outcome. To better understand and evaluate the policies, we propose a new stochastic frontier model with a treatment status and a mediator, both of which are allowed to be endogenous. The model allows us to decompose the total program (treatment) effect into technology and efficiency components, and to investigate whether the effect is derived directly from the program or indirectly through a particular mediator. Supplementary materials for this article are available online.},
  archive      = {J_JBES},
  author       = {Yi-Ting Chen and Yu-Chin Hsu and Hung-Jen Wang},
  doi          = {10.1080/07350015.2018.1497504},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {243-256},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {A stochastic frontier model with endogenous treatment status and mediator},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Testing for an omitted multiplicative long-term component in
GARCH models. <em>JBES</em>, <em>38</em>(2), 229–242. (<a
href="https://doi.org/10.1080/07350015.2018.1482759">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of testing for an omitted multiplicative long-term component in GARCH-type models. Under the alternative, there is a two-component model with a short-term GARCH component that fluctuates around a smoothly time-varying long-term component which is driven by the dynamics of an explanatory variable. We suggest a Lagrange multiplier statistic for testing the null hypothesis that the variable has no explanatory power. We derive the asymptotic theory for our test statistic and investigate its finite sample properties by Monte Carlo simulation. Our test also covers the mixed-frequency case in which the returns are observed at a higher frequency than the explanatory variable. The usefulness of our procedure is illustrated by empirical applications to S&amp;P 500 return data. Supplementary materials for this article are available online.},
  archive      = {J_JBES},
  author       = {Christian Conrad and Melanie Schienle},
  doi          = {10.1080/07350015.2018.1482759},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {4},
  number       = {2},
  pages        = {229-242},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Testing for an omitted multiplicative long-term component in GARCH models},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Testing alphas in conditional time-varying factor models
with high-dimensional assets. <em>JBES</em>, <em>38</em>(1), 214–227.
(<a href="https://doi.org/10.1080/07350015.2018.1482758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For conditional time-varying factor models with high-dimensional assets, this article proposes a high-dimensional alpha (HDA) test to assess whether there exist abnormal returns on securities (or portfolios) over the theoretical expected returns. To employ this test effectively, a constant coefficient test is also introduced. It examines the validity of constant alphas and factor loadings. Simulation studies and an empirical example are presented to illustrate the finite sample performance and the usefulness of the proposed tests. Using the HDA test, the empirical example demonstrates that the FF three-factor model is better than CAPM in explaining the mean-variance efficiency of both the Chinese and U.S. stock markets. Furthermore, our results suggest that the U.S. stock market is more efficient in terms of mean-variance efficiency than the Chinese stock market. Supplementary materials for this article are available online.},
  archive      = {J_JBES},
  author       = {Shujie Ma and Wei Lan and Liangjun Su and Chih-Ling Tsai},
  doi          = {10.1080/07350015.2018.1482758},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {214-227},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Testing alphas in conditional time-varying factor models with high-dimensional assets},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Conditional extremes in asymmetric financial markets.
<em>JBES</em>, <em>38</em>(1), 201–213. (<a
href="https://doi.org/10.1080/07350015.2018.1476248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The global financial crisis of 2007–2009 revealed the great extent to which systemic risk can jeopardize the stability of the entire financial system. An effective methodology to quantify systemic risk is at the heart of the process of identifying the so-called systemically important financial institutions for regulatory purposes as well as to investigate key drivers of systemic contagion. The article proposes a method for dynamic forecasting of CoVaR, a popular measure of systemic risk. As a first step, we develop a semi-parametric framework using asymptotic results in the spirit of extreme value theory (EVT) to model the conditional probability distribution of a bivariate random vector given that one of the components takes on a large value, taking into account important features of financial data such as asymmetry and heavy tails. In the second step, we embed the proposed EVT method into a dynamic framework via a bivariate GARCH process. An empirical analysis is conducted to demonstrate and compare the performance of the proposed methodology relative to a very flexible fully parametric alternative.},
  archive      = {J_JBES},
  author       = {Natalia Nolde and Jinyuan Zhang},
  doi          = {10.1080/07350015.2018.1476248},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {201-213},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Conditional extremes in asymmetric financial markets},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The finite sample performance of inference methods for
propensity score matching and weighting estimators. <em>JBES</em>,
<em>38</em>(1), 183–200. (<a
href="https://doi.org/10.1080/07350015.2018.1476247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the finite sample properties of a range of inference methods for propensity score-based matching and weighting estimators frequently applied to evaluate the average treatment effect on the treated. We analyze both asymptotic approximations and bootstrap methods for computing variances and confidence intervals in our simulation designs, which are based on German register data and U.S. survey data. We vary the design w.r.t. treatment selectivity, effect heterogeneity, share of treated, and sample size. The results suggest that in general, theoretically justified bootstrap procedures (i.e., wild bootstrapping for pair matching and standard bootstrapping for “smoother” treatment effect estimators) dominate the asymptotic approximations in terms of coverage rates for both matching and weighting estimators. Most findings are robust across simulation designs and estimators.},
  archive      = {J_JBES},
  author       = {Hugo Bodory and Lorenzo Camponovo and Martin Huber and Michael Lechner},
  doi          = {10.1080/07350015.2018.1476247},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {183-200},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {The finite sample performance of inference methods for propensity score matching and weighting estimators},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The estimation of compensating wage differentials: Lessons
from the deadliest catch. <em>JBES</em>, <em>38</em>(1), 165–182. (<a
href="https://doi.org/10.1080/07350015.2018.1470000">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {I use longitudinal survey data from commercial fishing deckhands in the Alaskan Bering Sea to provide new insights on empirical methods commonly used to estimate compensating wage differentials and the value of statistical life (VSL). The unique setting exploits intertemporal variation in fatality rates and wages within worker-vessel pairs caused by a combination of weather patterns and policy changes, allowing identification of parameters and biases that it has only been possible to speculate about in more general settings. I show that estimation strategies common in the literature produce biased estimates in this setting, and decompose the bias components due to latent worker, establishment, and job-match heterogeneity. The estimates also remove the confounding effects of endogenous job mobility and dynamic labor market search, narrowing a conceptual gap between search-based hedonic wage theory and its empirical applications. I find that workers’ marginal aversion to fatal risk falls as risk levels rise, which suggests complementarities in the benefits of public safety policies. Supplementary materials for this article are available online.},
  archive      = {J_JBES},
  author       = {Kurt Lavetti},
  doi          = {10.1080/07350015.2018.1470000},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {165-182},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {The estimation of compensating wage differentials: Lessons from the deadliest catch},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Transformation-kernel estimation of copula densities.
<em>JBES</em>, <em>38</em>(1), 148–164. (<a
href="https://doi.org/10.1080/07350015.2018.1469999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The standard kernel estimator of copula densities suffers from boundary biases and inconsistency due to unbounded densities. Transforming the domain of estimation into an unbounded one remedies both problems, but also introduces an unbounded multiplier that may produce erratic boundary behaviors in the final density estimate. We propose an improved transformation-kernel estimator that employs a smooth tapering device to counter the undesirable influence of the multiplier. We establish the theoretical properties of the new estimator and its automatic higher-order improvement under Gaussian copulas. We present two practical methods of smoothing parameter selection. Extensive Monte Carlo simulations demonstrate the competence of the proposed estimator in terms of global and tail performance. Two real-world examples are provided. Supplementary materials for this article are available online.},
  archive      = {J_JBES},
  author       = {Kuangyu Wen and Ximing Wu},
  doi          = {10.1080/07350015.2018.1469999},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {148-164},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Transformation-kernel estimation of copula densities},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mixed marginal copula modeling. <em>JBES</em>,
<em>38</em>(1), 137–147. (<a
href="https://doi.org/10.1080/07350015.2018.1469998">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article extends the literature on copulas with discrete or continuous marginals to the case where some of the marginals are a mixture of discrete and continuous components. We do so by carefully defining the likelihood as the density of the observations with respect to a mixed measure. The treatment is quite general, although we focus on mixtures of Gaussian and Archimedean copulas. The inference is Bayesian with the estimation carried out by Markov chain Monte Carlo. We illustrate the methodology and algorithms by applying them to estimate a multivariate income dynamics model. Supplementary materials for this article are available online.},
  archive      = {J_JBES},
  author       = {David Gunawan and Mohamad A. Khaled and Robert Kohn},
  doi          = {10.1080/07350015.2018.1469998},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {137-147},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Mixed marginal copula modeling},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Choosing prior hyperparameters: With applications to
time-varying parameter models. <em>JBES</em>, <em>38</em>(1), 124–136.
(<a href="https://doi.org/10.1080/07350015.2018.1459302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-varying parameter models with stochastic volatility are widely used to study macroeconomic and financial data. These models are almost exclusively estimated using Bayesian methods. A common practice is to focus on prior distributions that themselves depend on relatively few hyperparameters such as the scaling factor for the prior covariance matrix of the residuals governing time variation in the parameters. The choice of these hyperparameters is crucial because their influence is sizeable for standard sample sizes. In this article, we treat the hyperparameters as part of a hierarchical model and propose a fast, tractable, easy-to-implement, and fully Bayesian approach to estimate those hyperparameters jointly with all other parameters in the model. We show via Monte Carlo simulations that, in this class of models, our approach can drastically improve on using fixed hyperparameters previously proposed in the literature. Supplementary materials for this article are available online.},
  archive      = {J_JBES},
  author       = {Pooyan Amir-Ahmadi and Christian Matthes and Mu-Chun Wang},
  doi          = {10.1080/07350015.2018.1459302},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {124-136},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Choosing prior hyperparameters: With applications to time-varying parameter models},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Testing nowcast monotonicity with estimated factors.
<em>JBES</em>, <em>38</em>(1), 107–123. (<a
href="https://doi.org/10.1080/07350015.2018.1458623">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a test to determine whether “big data” nowcasting methods, which have become an important tool to many public and private institutions, are monotonically improving as new information becomes available. The test is the first to formalize existing evaluation procedures from the nowcasting literature. We place particular emphasis on models involving estimated factors, since factor-based methods are a leading case in the high-dimensional empirical nowcasting literature, although our test is still applicable to small-dimensional set-ups like bridge equations and MIDAS models. Our approach extends a recent methodology for testing many moment inequalities to the case of nowcast monotonicity testing, which allows the number of inequalities to grow with the sample size. We provide results showing the conditions under which both parameter estimation error and factor estimation error can be accommodated in this high-dimensional setting when using the pseudo out-of-sample approach. The finite sample performance of our test is illustrated using a wide range of Monte Carlo simulations, and we conclude with an empirical application of nowcasting U.S. real gross domestic product (GDP) growth and five GDP sub-components. Our test results confirm monotonicity for all but one sub-component (government spending), suggesting that the factor-augmented model may be misspecified for this GDP constituent. Supplementary materials for this article are available online.},
  archive      = {J_JBES},
  author       = {Jack Fosten and Daniel Gutknecht},
  doi          = {10.1080/07350015.2018.1458623},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {107-123},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Testing nowcast monotonicity with estimated factors},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Identifying demand shocks from production data.
<em>JBES</em>, <em>38</em>(1), 93–106. (<a
href="https://doi.org/10.1080/07350015.2018.1458622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standard productivity estimates contain a mixture of cost efficiency and demand conditions. I propose a method to identify the distribution of the demand shock using production data. Identification does not depend on functional form restrictions. It is also robust to dynamic demand considerations and flexible labor. In the parametric case, the ratio of intermediate inputs to the wage bill (input ratio) contains information about the magnitude of the demand shock. The method is tested using data from Spain that contains information on prices and demand conditions. Finally, we generate Monte Carlo simulations to evaluate the method’s performance and sensitivity. Supplementary materials for this article are available online.},
  archive      = {J_JBES},
  author       = {Carlos Daniel Santos},
  doi          = {10.1080/07350015.2018.1458622},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {93-106},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Identifying demand shocks from production data},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Volatility martingale difference divergence matrix and its
application to dimension reduction for multivariate volatility.
<em>JBES</em>, <em>38</em>(1), 80–92. (<a
href="https://doi.org/10.1080/07350015.2018.1458621">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose the so-called volatility martingale difference divergence matrix (VMDDM) to quantify the conditional variance dependence of a random vector Y ∈ R p given X ∈ R q , building on the recent work on martigale difference divergence matrix (MDDM) that measures the conditional mean dependence. We further generalize VMDDM to the time series context and apply it to do dimension reduction for multivariate volatility, following the recent work by Hu and Tsay and Li et al. Unlike the latter two papers, our metric is easy to compute, can fully capture nonlinear serial dependence and involves less user-chosen numbers. Furthermore, we propose a variant of VMDDM and apply it to the estimation of conditional uncorrelated components model (Fan, Wang, and Yao Citation 2008 ). Simulation and data illustration show that our method can perform well in comparison with the existing ones with less computational time, and can outperform others in cases of strong nonlinear dependence.},
  archive      = {J_JBES},
  author       = {Chung Eun Lee and Xiaofeng Shao},
  doi          = {10.1080/07350015.2018.1458621},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {80-92},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Volatility martingale difference divergence matrix and its application to dimension reduction for multivariate volatility},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Large bayesian VARs: A flexible kronecker error covariance
structure. <em>JBES</em>, <em>38</em>(1), 68–79. (<a
href="https://doi.org/10.1080/07350015.2018.1451336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a class of large Bayesian vector autoregressions (BVARs) that allows for non-Gaussian, heteroscedastic, and serially dependent innovations. To make estimation computationally tractable, we exploit a certain Kronecker structure of the likelihood implied by this class of models. We propose a unified approach for estimating these models using Markov chain Monte Carlo (MCMC) methods. In an application that involves 20 macroeconomic variables, we find that these BVARs with more flexible covariance structures outperform the standard variant with independent, homoscedastic Gaussian innovations in both in-sample model-fit and out-of-sample forecast performance.},
  archive      = {J_JBES},
  author       = {Joshua C. C. Chan},
  doi          = {10.1080/07350015.2018.1451336},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {68-79},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Large bayesian VARs: A flexible kronecker error covariance structure},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bootstrapping noncausal autoregressions: With applications
to explosive bubble modeling. <em>JBES</em>, <em>38</em>(1), 55–67. (<a
href="https://doi.org/10.1080/07350015.2018.1448830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we develop new bootstrap-based inference for noncausal autoregressions with heavy-tailed innovations. This class of models is widely used for modeling bubbles and explosive dynamics in economic and financial time series. In the noncausal, heavy-tail framework, a major drawback of asymptotic inference is that it is not feasible in practice as the relevant limiting distributions depend crucially on the (unknown) decay rate of the tails of the distribution of the innovations. In addition, even in the unrealistic case where the tail behavior is known, asymptotic inference may suffer from small-sample issues. To overcome these difficulties, we propose bootstrap inference procedures using parameter estimates obtained with the null hypothesis imposed (the so-called restricted bootstrap). We discuss three different choices of bootstrap innovations: wild bootstrap, based on Rademacher errors; permutation bootstrap; a combination of the two (“permutation wild bootstrap”). Crucially, implementation of these bootstraps do not require any a priori knowledge about the distribution of the innovations, such as the tail index or the convergence rates of the estimators. We establish sufficient conditions ensuring that, under the null hypothesis, the bootstrap statistics estimate consistently particular conditional distributions of the original statistics. In particular, we show that validity of the permutation bootstrap holds without any restrictions on the distribution of the innovations, while the permutation wild and the standard wild bootstraps require further assumptions such as symmetry of the innovation distribution. Extensive Monte Carlo simulations show that the finite sample performance of the proposed bootstrap tests is exceptionally good, both in terms of size and of empirical rejection probabilities under the alternative hypothesis. We conclude by applying the proposed bootstrap inference to Bitcoin/USD exchange rates and to crude oil price data. We find that indeed noncausal models with heavy-tailed innovations are able to fit the data, also in periods of bubble dynamics. Supplementary materials for this article are available online.},
  archive      = {J_JBES},
  author       = {Giuseppe Cavaliere and Heino Bohn Nielsen and Anders Rahbek},
  doi          = {10.1080/07350015.2018.1448830},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {55-67},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Bootstrapping noncausal autoregressions: With applications to explosive bubble modeling},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Shape-constrained kernel-weighted least squares: Estimating
production functions for chilean manufacturing industries.
<em>JBES</em>, <em>38</em>(1), 43–54. (<a
href="https://doi.org/10.1080/07350015.2018.1431128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we examine a novel way of imposing shape constraints on a local polynomial kernel estimator. The proposed approach is referred to as shape constrained kernel-weighted least squares (SCKLS). We prove uniform consistency of the SCKLS estimator with monotonicity and convexity/concavity constraints and establish its convergence rate. In addition, we propose a test to validate whether shape constraints are correctly specified. The competitiveness of SCKLS is shown in a comprehensive simulation study. Finally, we analyze Chilean manufacturing data using the SCKLS estimator and quantify production in the plastics and wood industries. The results show that exporting firms have significantly higher productivity.},
  archive      = {J_JBES},
  author       = {Daisuke Yagi and Yining Chen and Andrew L. Johnson and Timo Kuosmanen},
  doi          = {10.1080/07350015.2018.1431128},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {43-54},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Shape-constrained kernel-weighted least squares: Estimating production functions for chilean manufacturing industries},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Expectations and risk premia at 8:30 a.m.: Deciphering the
responses of bond yields to macroeconomic announcements. <em>JBES</em>,
<em>38</em>(1), 27–42. (<a
href="https://doi.org/10.1080/07350015.2018.1429278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {What explains the sharp movements of the yield curve upon the release of major U.S. macroeconomic announcements? To answer this question, we estimate an arbitrage-free dynamic term structure model with macroeconomic fundamentals as risk factors. We assume that the yield curve reacts to announcements primarily because of the information they contain about the fundamentals of output, inflation, and the Fed’s inflation target. We model the updating process by linking the factor shocks to announcement surprises. Fitting this process to data on yield curve movements in 20-min event windows, we find that most major announcements, especially those about the labor market, are informative largely about the output gap rather than about inflation. The resulting changes in short-rate expectations account for the bulk of observed yield movements. But adjustments in risk premia are also sizable. In partly offsetting the effects of short-rate expectations, these adjustments help to account for the well-known hump-shaped pattern of yield reactions across maturities.},
  archive      = {J_JBES},
  author       = {Peter Hördahl and Eli M. Remolona and Giorgio Valente},
  doi          = {10.1080/07350015.2018.1429278},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {27-42},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Expectations and risk premia at 8:30 a.m.: deciphering the responses of bond yields to macroeconomic announcements},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Rejoinder. <em>JBES</em>, <em>38</em>(1), 25–26. (<a
href="https://doi.org/10.1080/07350015.2019.1681278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JBES},
  author       = {Jeffrey M. Wooldridge and Ying Zhu},
  doi          = {10.1080/07350015.2019.1681278},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {25-26},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Rejoinder},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Debiased inference of average partial effects in
single-index models: Comment on wooldridge and zhu. <em>JBES</em>,
<em>38</em>(1), 19–24. (<a
href="https://doi.org/10.1080/07350015.2019.1681277">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JBES},
  author       = {David A. Hirshberg and Stefan Wager},
  doi          = {10.1080/07350015.2019.1681277},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {19-24},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Debiased inference of average partial effects in single-index models: Comment on wooldridge and zhu},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Inference in approximately sparse correlated random effects
probit models with panel data. <em>JBES</em>, <em>38</em>(1), 1–18. (<a
href="https://doi.org/10.1080/07350015.2019.1681276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a simple procedure based on an existing “debiased” l 1 -regularized method for inference of the average partial effects (APEs) in approximately sparse probit and fractional probit models with panel data, where the number of time periods is fixed and small relative to the number of cross-sectional observations. Our method is computationally simple and does not suffer from the incidental parameters problems that come from attempting to estimate as a parameter the unobserved heterogeneity for each cross-sectional unit. Furthermore, it is robust to arbitrary serial dependence in underlying idiosyncratic errors. Our theoretical results illustrate that inference concerning APEs is more challenging than inference about fixed and low-dimensional parameters, as the former concerns deriving the asymptotic normality for sample averages of linear functions of a potentially large set of components in our estimator when a series approximation for the conditional mean of the unobserved heterogeneity is considered. Insights on the applicability and implications of other existing Lasso-based inference procedures for our problem are provided. We apply the debiasing method to estimate the effects of spending on test pass rates. Our results show that spending has a positive and statistically significant average partial effect; moreover, the effect is comparable to found using standard parametric methods.},
  archive      = {J_JBES},
  author       = {Jeffrey M. Wooldridge and Ying Zhu},
  doi          = {10.1080/07350015.2019.1681276},
  journal      = {Journal of Business &amp; Economic Statistics},
  month        = {1},
  number       = {1},
  pages        = {1-18},
  shortjournal = {J. Busin. Econ. Stat.},
  title        = {Inference in approximately sparse correlated random effects probit models with panel data},
  volume       = {38},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
