<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JASA_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jasa---179">JASA - 179</h2>
<ul>
<li><details>
<summary>
(2020). High-dimensional precision medicine from patient-derived
xenografts. <em>JASA</em>, <em>116</em>(535), 1140–1154. (<a
href="https://doi.org/10.1080/01621459.2020.1828091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complexity of human cancer often results in significant heterogeneity in response to treatment. Precision medicine offers the potential to improve patient outcomes by leveraging this heterogeneity. Individualized treatment rules (ITRs) formalize precision medicine as maps from the patient covariate space into the space of allowable treatments. The optimal ITR is that which maximizes the mean of a clinical outcome in a population of interest. Patient-derived xenograft (PDX) studies permit the evaluation of multiple treatments within a single tumor, and thus are ideally suited for estimating optimal ITRs. PDX data are characterized by correlated outcomes, a high-dimensional feature space, and a large number of treatments. Here we explore machine learning methods for estimating optimal ITRs from PDX data. We analyze data from a large PDX study to identify biomarkers that are informative for developing personalized treatment recommendations in multiple cancers. We estimate optimal ITRs using regression-based (Q-learning) and direct-search methods (outcome weighted learning). Finally, we implement a superlearner approach to combine multiple estimated ITRs and show that the resulting ITR performs better than any of the input ITRs, mitigating uncertainty regarding user choice. Our results indicate that PDX data are a valuable resource for developing individualized treatment strategies in oncology. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Naim U. Rashid and Daniel J. Luckett and Jingxiang Chen and Michael T. Lawson and Longshaokan Wang and Yunshu Zhang and Eric B. Laber and Yufeng Liu and Jen Jen Yeh and Donglin Zeng and Michael R. Kosorok},
  doi          = {10.1080/01621459.2020.1828091},
  journal      = {Journal of the American Statistical Association},
  number       = {535},
  pages        = {1140-1154},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {High-dimensional precision medicine from patient-derived xenografts},
  volume       = {116},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Editorial collaborators. <em>JASA</em>, <em>115</em>(532),
2105–2113. (<a
href="https://doi.org/10.1080/01621459.2020.1846977">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  doi          = {10.1080/01621459.2020.1846977},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {2105-2113},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Editorial collaborators},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The statistical analysis of multivariate failure time data:
A marginal modeling approach. <em>JASA</em>, <em>115</em>(532),
2102–2104. (<a
href="https://doi.org/10.1080/01621459.2020.1846975">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Richard J. Cook},
  doi          = {10.1080/01621459.2020.1846975},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {2102-2104},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {The statistical analysis of multivariate failure time data: A marginal modeling approach.},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Handbook of mixture analysis. <em>JASA</em>,
<em>115</em>(532), 2101–2102. (<a
href="https://doi.org/10.1080/01621459.2020.1846974">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Yen-Chi Chen},
  doi          = {10.1080/01621459.2020.1846974},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {2101-2102},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Handbook of mixture analysis.},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Handbook of approximate bayesian computation. <em>JASA</em>,
<em>115</em>(532), 2100–2101. (<a
href="https://doi.org/10.1080/01621459.2020.1846973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Jordan J. Franks},
  doi          = {10.1080/01621459.2020.1846973},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {2100-2101},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Handbook of approximate bayesian computation.},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Kernel meets sieve: Post-regularization confidence bands for
sparse additive model. <em>JASA</em>, <em>115</em>(532), 2084–2099. (<a
href="https://doi.org/10.1080/01621459.2019.1689984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a novel procedure for constructing confidence bands for components of a sparse additive model. Our procedure is based on a new kernel-sieve hybrid estimator that combines two most popular nonparametric estimation methods in the literature, the kernel regression and the spline method, and is of interest in its own right. Existing methods for fitting sparse additive model are primarily based on sieve estimators, while the literature on confidence bands for nonparametric models are primarily based upon kernel or local polynomial estimators. Our kernel-sieve hybrid estimator combines the best of both worlds and allows us to provide a simple procedure for constructing confidence bands in high-dimensional sparse additive models. We prove that the confidence bands are asymptotically honest by studying approximation with a Gaussian process. Thorough numerical results on both synthetic data and real-world neuroscience data are provided to demonstrate the efficacy of the theory. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Junwei Lu and Mladen Kolar and Han Liu},
  doi          = {10.1080/01621459.2019.1689984},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {2084-2099},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Kernel meets sieve: Post-regularization confidence bands for sparse additive model},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Statistical inference for average treatment effects
estimated by synthetic control methods. <em>JASA</em>,
<em>115</em>(532), 2068–2083. (<a
href="https://doi.org/10.1080/01621459.2019.1686986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The synthetic control (SC) method, a powerful tool for estimating average treatment effects (ATE), is increasingly popular in fields such as statistics, economics, political science, and marketing. The SC is particularly suitable for estimating ATE with a single (or a few) treated unit(s), a fixed number of control units, and large pre and post-treatment periods (which we refer as “long panels”). To date, there has been no formal inference theory for SC ATE estimator with long panels under general conditions. Existing work mostly use placebo tests for inference or some permutation methods when the post-treatment period is small. In this article, we derive the asymptotic distribution of the SC and modified synthetic control (MSC) ATE estimators using projection theory. We show that a properly designed subsampling method can be used to obtain confidence intervals and conduct inference whereas the standard bootstrap cannot. Simulations and an empirical application that examines the effect of opening a physical showroom by an e-tailer demonstrate the usefulness of the MSC method in applications. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Kathleen T. Li},
  doi          = {10.1080/01621459.2019.1686986},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {2068-2083},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistical inference for average treatment effects estimated by synthetic control methods},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive sparse estimation with side information.
<em>JASA</em>, <em>115</em>(532), 2053–2067. (<a
href="https://doi.org/10.1080/01621459.2019.1679639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The article considers the problem of estimating a high-dimensional sparse parameter in the presence of side information that encodes the sparsity structure. We develop a general framework that involves first using an auxiliary sequence to capture the side information, and then incorporating the auxiliary sequence in inference to reduce the estimation risk. The proposed method, which carries out adaptive Stein’s unbiased risk estimate-thresholding using side information (ASUS), is shown to have robust performance and enjoy optimality properties. We develop new theories to characterize regimes in which ASUS far outperforms competitive shrinkage estimators, and establish precise conditions under which ASUS is asymptotically optimal. Simulation studies are conducted to show that ASUS substantially improves the performance of existing methods in many settings. The methodology is applied for analysis of data from single cell virology studies and microarray time course experiments. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Trambak Banerjee and Gourab Mukherjee and Wenguang Sun},
  doi          = {10.1080/01621459.2019.1679639},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {2053-2067},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Adaptive sparse estimation with side information},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian hierarchical models with conjugate full-conditional
distributions for dependent data from the natural exponential family.
<em>JASA</em>, <em>115</em>(532), 2037–2052. (<a
href="https://doi.org/10.1080/01621459.2019.1677471">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a Bayesian approach for analyzing (possibly) high-dimensional dependent data that are distributed according to a member from the natural exponential family of distributions. This problem requires extensive methodological advancements, as jointly modeling high-dimensional dependent data leads to the so-called “big n problem.” The computational complexity of the “big n problem” is further exacerbated when allowing for non-Gaussian data models, as is the case here. Thus, we develop new computationally efficient distribution theory for this setting. In particular, we introduce the “conjugate multivariate distribution,” which is motivated by the Diaconis and Ylvisaker distribution. Furthermore, we provide substantial theoretical and methodological development including: results regarding conditional distributions, an asymptotic relationship with the multivariate normal distribution, conjugate prior distributions, and full-conditional distributions for a Gibbs sampler. To demonstrate the wide-applicability of the proposed methodology, we provide two simulation studies and three applications based on an epidemiology dataset, a federal statistics dataset, and an environmental dataset, respectively. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jonathan R. Bradley and Scott H. Holan and Christopher K. Wikle},
  doi          = {10.1080/01621459.2019.1677471},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {2037-2052},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian hierarchical models with conjugate full-conditional distributions for dependent data from the natural exponential family},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mixed-effect time-varying network model and application in
brain connectivity analysis. <em>JASA</em>, <em>115</em>(532),
2022–2036. (<a
href="https://doi.org/10.1080/01621459.2019.1677242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-varying networks are fast emerging in a wide range of scientific and business applications. Most existing dynamic network models are limited to a single-subject and discrete-time setting. In this article, we propose a mixed-effect network model that characterizes the continuous time-varying behavior of the network at the population level, meanwhile taking into account both the individual subject variability as well as the prior module information. We develop a multistep optimization procedure for a constrained likelihood estimation and derive the associated asymptotic properties. We demonstrate the effectiveness of our method through both simulations and an application to a study of brain development in youth. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jingfei Zhang and Will Wei Sun and Lexin Li},
  doi          = {10.1080/01621459.2019.1677242},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {2022-2036},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Mixed-effect time-varying network model and application in brain connectivity analysis},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Doubly robust inference with nonprobability survey samples.
<em>JASA</em>, <em>115</em>(532), 2011–2021. (<a
href="https://doi.org/10.1080/01621459.2019.1677241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We establish a general framework for statistical inferences with nonprobability survey samples when relevant auxiliary information is available from a probability survey sample. We develop a rigorous procedure for estimating the propensity scores for units in the nonprobability sample, and construct doubly robust estimators for the finite population mean. Variance estimation is discussed under the proposed framework. Results from simulation studies show the robustness and the efficiency of our proposed estimators as compared to existing methods. The proposed method is used to analyze a nonprobability survey sample collected by the Pew Research Center with auxiliary information from the Behavioral Risk Factor Surveillance System and the Current Population Survey. Our results illustrate a general approach to inference with nonprobability samples and highlight the importance and usefulness of auxiliary information from probability survey samples. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yilin Chen and Pengfei Li and Changbao Wu},
  doi          = {10.1080/01621459.2019.1677241},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {2011-2021},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Doubly robust inference with nonprobability survey samples},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Targeted random projection for prediction from
high-dimensional features. <em>JASA</em>, <em>115</em>(532), 1998–2010.
(<a href="https://doi.org/10.1080/01621459.2019.1677240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of computationally efficient prediction with high dimensional and highly correlated predictors when accurate variable selection is effectively impossible. Direct application of penalization or Bayesian methods implemented with Markov chain Monte Carlo can be computationally daunting and unstable. A common solution is first stage dimension reduction through screening or projecting the design matrix to a lower dimensional hyper-plane. Screening is highly sensitive to threshold choice, while projections often have poor performance in very high-dimensions. We propose targeted random projection (TARP) to combine positive aspects of both strategies. TARP uses screening to order the inclusion probabilities of the features in the projection matrix used for dimension reduction, leading to data-informed sparsity. We provide theoretical support for a Bayesian predictive algorithm based on TARP, including statistical and computational complexity guarantees. Examples for simulated and real data applications illustrate gains relative to a variety of competitors. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Minerva Mukhopadhyay and David B. Dunson},
  doi          = {10.1080/01621459.2019.1677240},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1998-2010},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Targeted random projection for prediction from high-dimensional features},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cross-validation with confidence. <em>JASA</em>,
<em>115</em>(532), 1978–1997. (<a
href="https://doi.org/10.1080/01621459.2019.1672556">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-validation is one of the most popular model and tuning parameter selection methods in statistics and machine learning. Despite its wide applicability, traditional cross-validation methods tend to overfit, due to the ignorance of the uncertainty in the testing sample. We develop a novel statistically principled inference tool based on cross-validation that takes into account the uncertainty in the testing sample. This method outputs a set of highly competitive candidate models containing the optimal one with guaranteed probability. As a consequence, our method can achieve consistent variable selection in a classical linear regression setting, for which existing cross-validation methods require unconventional split ratios. When used for tuning parameter selection, the method can provide an alternative trade-off between prediction accuracy and model interpretability than existing variants of cross-validation. We demonstrate the performance of the proposed method in several simulated and real data examples. Supplemental materials for this article can be found online.},
  archive      = {J_JASA},
  author       = {Jing Lei},
  doi          = {10.1080/01621459.2019.1672556},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1978-1997},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Cross-validation with confidence},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The five trolls under the bridge: Principal component
analysis with asynchronous and noisy high frequency data. <em>JASA</em>,
<em>115</em>(532), 1960–1977. (<a
href="https://doi.org/10.1080/01621459.2019.1672555">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a principal component analysis (PCA) for high frequency data. As in Northern fairy tales, there are trolls waiting for the explorer. The first three trolls are market microstructure noise, asynchronous sampling times, and edge effects in estimators. To get around these, a robust estimator of the spot covariance matrix is developed based on the smoothed two-scale realized variance (S-TSRV). The fourth troll is how to pass from estimated time-varying covariance matrix to PCA. Under finite dimensionality, we develop this methodology through the estimation of realized spectral functions. Rates of convergence and central limit theory, as well as an estimator of standard error, are established. The fifth troll is high dimension on top of high frequency, where we also develop PCA. With the help of a new identity concerning the spot principal orthogonal complement, the high-dimensional rates of convergence have been studied after eliminating several strong assumptions in classical PCA. As an application, we show that our first principal component (PC) closely matches but potentially outperforms the S&amp;P 100 market index. From a statistical standpoint, the close match between the first PC and the market index also corroborates this PCA procedure and the underlying S-TSRV matrix, in the sense of Karl Popper. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Dachuan Chen and Per A. Mykland and Lan Zhang},
  doi          = {10.1080/01621459.2019.1672555},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1960-1977},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {The five trolls under the bridge: Principal component analysis with asynchronous and noisy high frequency data},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal designs of two-phase studies. <em>JASA</em>,
<em>115</em>(532), 1946–1959. (<a
href="https://doi.org/10.1080/01621459.2019.1671200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The two-phase design is a cost-effective sampling strategy to evaluate the effects of covariates on an outcome when certain covariates are too expensive to be measured on all study subjects. Under such a design, the outcome and inexpensive covariates are measured on all subjects in the first phase and the first-phase information is used to select subjects for measurements of expensive covariates in the second phase. Previous research on two-phase studies has focused largely on the inference procedures rather than the design aspects. We investigate the design efficiency of the two-phase study, as measured by the semiparametric efficiency bound for estimating the regression coefficients of expensive covariates. We consider general two-phase studies, where the outcome variable can be continuous, discrete, or censored, and the second-phase sampling can depend on the first-phase data in any manner. We develop optimal or approximately optimal two-phase designs, which can be substantially more efficient than the existing designs. We demonstrate the improvements of the new designs over the existing ones through extensive simulation studies and two large medical studies. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Ran Tao and Donglin Zeng and Dan-Yu Lin},
  doi          = {10.1080/01621459.2019.1671200},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1946-1959},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Optimal designs of two-phase studies},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Empirical frequency band analysis of nonstationary time
series. <em>JASA</em>, <em>115</em>(532), 1933–1945. (<a
href="https://doi.org/10.1080/01621459.2019.1671199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The time-varying power spectrum of a time series process is a bivariate function that quantifies the magnitude of oscillations at different frequencies and times. To obtain low-dimensional, parsimonious measures from this functional parameter, applied researchers consider collapsed measures of power within local bands that partition the frequency space. Frequency bands commonly used in the scientific literature were historically derived, but they are not guaranteed to be optimal or justified for adequately summarizing information from a given time series process under current study. There is a dearth of methods for empirically constructing statistically optimal bands for a given signal. The goal of this article is to provide a standardized, unifying approach for deriving and analyzing customized frequency bands. A consistent, frequency-domain, iterative cumulative sum based scanning procedure is formulated to identify frequency bands that best preserve nonstationary information. A formal hypothesis testing procedure is also developed to test which, if any, frequency bands remain stationary. The proposed method is used to analyze heart rate variability of a patient during sleep and uncovers a refined partition of frequency bands that best summarize the time-varying power spectrum. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Scott A. Bruce and Cheng Yong Tang and Martica H. Hall and Robert T. Krafty},
  doi          = {10.1080/01621459.2019.1671199},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1933-1945},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Empirical frequency band analysis of nonstationary time series},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved small-sample estimation of nonlinear
cross-validated prediction metrics. <em>JASA</em>, <em>115</em>(532),
1917–1932. (<a
href="https://doi.org/10.1080/01621459.2019.1668794">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When predicting an outcome is the scientific goal, one must decide on a metric by which to evaluate the quality of predictions. We consider the problem of measuring the performance of a prediction algorithm with the same data that were used to train the algorithm. Typical approaches involve bootstrapping or cross-validation. However, we demonstrate that bootstrap-based approaches often fail and standard cross-validation estimators may perform poorly. We provide a general study of cross-validation-based estimators that highlights the source of this poor performance, and propose an alternative framework for estimation using techniques from the efficiency theory literature. We provide a theorem establishing the weak convergence of our estimators. The general theorem is applied in detail to two specific examples and we discuss possible extensions to other parameters of interest. For the two explicit examples that we consider, our estimators demonstrate remarkable finite-sample improvements over standard approaches. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {David Benkeser and Maya Petersen and Mark J. van der Laan},
  doi          = {10.1080/01621459.2019.1668794},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1917-1932},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Improved small-sample estimation of nonlinear cross-validated prediction metrics},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast and accurate binary response mixed model analysis via
expectation propagation. <em>JASA</em>, <em>115</em>(532), 1902–1916.
(<a href="https://doi.org/10.1080/01621459.2019.1665529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Expectation propagation is a general prescription for approximation of integrals in statistical inference problems. Its literature is mainly concerned with Bayesian inference scenarios. However, expectation propagation can also be used to approximate integrals arising in frequentist statistical inference. We focus on likelihood-based inference for binary response mixed models and show that fast and accurate quadrature-free inference can be realized for the probit link case with multivariate random effects and higher levels of nesting. The approach is supported by asymptotic calculations in which expectation propagation is seen to provide consistent estimation of the exact likelihood surface. Numerical studies reveal the availability of fast, highly accurate and scalable methodology for binary mixed model analysis.},
  archive      = {J_JASA},
  author       = {P. Hall and I.M. Johnstone and J.T. Ormerod and M.P. Wand and J.C.F. Yu},
  doi          = {10.1080/01621459.2019.1665529},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1902-1916},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Fast and accurate binary response mixed model analysis via expectation propagation},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Revealing subgroup structure in ranked data using a bayesian
WAND. <em>JASA</em>, <em>115</em>(532), 1888–1901. (<a
href="https://doi.org/10.1080/01621459.2019.1665528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ranked data arise in many areas of application ranging from the ranking of up-regulated genes for cancer to the ranking of academic statistics journals. Complications can arise when rankers do not report a full ranking of all entities; for example, they might only report their top- M ranked entities after seeing some or all entities. It can also be useful to know whether rankers are equally informative, and whether some entities are effectively judged to be exchangeable. Revealing subgroup structure in the data may also be helpful in understanding the distribution of ranker views. In this paper, we propose a flexible Bayesian nonparametric model for identifying heterogeneous structure and ranker reliability in ranked data. The model is a weighted adapted nested Dirichlet (WAND) process mixture of Plackett–Luce models and inference proceeds through a simple and efficient Gibbs sampling scheme for posterior sampling. The richness of information in the posterior distribution allows us to infer many details of the structure both between ranker groups and between entity groups (within-ranker groups). Our modeling framework also facilitates a flexible representation of the posterior predictive distribution. This flexibility is important as we propose to use the posterior predictive distribution as the basis for addressing the rank aggregation problem, and also for identifying lack of model fit. The methodology is illustrated using several simulation studies and real data examples. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {S. R. Johnson and D. A. Henderson and R. J. Boys},
  doi          = {10.1080/01621459.2019.1665528},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1888-1901},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Revealing subgroup structure in ranked data using a bayesian WAND},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On optimal tests for rotational symmetry against new classes
of hyperspherical distributions. <em>JASA</em>, <em>115</em>(532),
1873–1887. (<a
href="https://doi.org/10.1080/01621459.2019.1665527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the central role played by rotationally symmetric distributions in directional statistics, we consider the problem of testing rotational symmetry on the hypersphere. We adopt a semiparametric approach and tackle problems where the location of the symmetry axis is either specified or unspecified. For each problem, we define two tests and study their asymptotic properties under very mild conditions. We introduce two new classes of directional distributions that extend the rotationally symmetric class and are of independent interest. We prove that each test is locally asymptotically maximin, in the Le Cam sense, for one kind of the alternatives given by the new classes of distributions, for both specified and unspecified symmetry axis. The tests, aimed to detect location- and scatter-like alternatives, are combined into convenient hybrid tests that are consistent against both alternatives. We perform Monte Carlo experiments that illustrate the finite-sample performances of the proposed tests and their agreement with the asymptotic results. Finally, the practical relevance of our tests is illustrated on a real data application from astronomy. The R package rotasym implements the proposed tests and allows practitioners to reproduce the data application. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Eduardo García-Portugués and Davy Paindaveine and Thomas Verdebout},
  doi          = {10.1080/01621459.2019.1665527},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1873-1887},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {On optimal tests for rotational symmetry against new classes of hyperspherical distributions},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep knockoffs. <em>JASA</em>, <em>115</em>(532), 1861–1872.
(<a href="https://doi.org/10.1080/01621459.2019.1660174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a machine for sampling approximate model-X knockoffs for arbitrary and unspecified data distributions using deep generative models. The main idea is to iteratively refine a knockoff sampling mechanism until a criterion measuring the validity of the produced knockoffs is optimized; this criterion is inspired by the popular maximum mean discrepancy in machine learning and can be thought of as measuring the distance to pairwise exchangeability between original and knockoff features. By building upon the existing model-X framework, we thus obtain a flexible and model-free statistical tool to perform controlled variable selection. Extensive numerical experiments and quantitative tests confirm the generality, effectiveness, and power of our deep knockoff machines. Finally, we apply this new method to a real study of mutations linked to changes in drug resistance in the human immunodeficiency virus. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yaniv Romano and Matteo Sesia and Emmanuel Candès},
  doi          = {10.1080/01621459.2019.1660174},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1861-1872},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Deep knockoffs},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust inference using inverse probability weighting.
<em>JASA</em>, <em>115</em>(532), 1851–1860. (<a
href="https://doi.org/10.1080/01621459.2019.1660173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inverse probability weighting (IPW) is widely used in empirical work in economics and other disciplines. As Gaussian approximations perform poorly in the presence of “small denominators,” trimming is routinely employed as a regularization strategy. However, ad hoc trimming of the observations renders usual inference procedures invalid for the target estimand, even in large samples. In this article, we first show that the IPW estimator can have different (Gaussian or non-Gaussian) asymptotic distributions, depending on how “close to zero” the probability weights are and on how large the trimming threshold is. As a remedy, we propose an inference procedure that is robust not only to small probability weights entering the IPW estimator but also to a wide range of trimming threshold choices, by adapting to these different asymptotic distributions. This robustness is achieved by employing resampling techniques and by correcting a non-negligible trimming bias. We also propose an easy-to-implement method for choosing the trimming threshold by minimizing an empirical analogue of the asymptotic mean squared error. In addition, we show that our inference procedure remains valid with the use of a data-driven trimming threshold. We illustrate our method by revisiting a dataset from the National Supported Work program. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Xinwei Ma and Jingshen Wang},
  doi          = {10.1080/01621459.2019.1660173},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1851-1860},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Robust inference using inverse probability weighting},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fixed effects testing in high-dimensional linear mixed
models. <em>JASA</em>, <em>115</em>(532), 1835–1850. (<a
href="https://doi.org/10.1080/01621459.2019.1660172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many scientific and engineering challenges—ranging from pharmacokinetic drug dosage allocation and personalized medicine to marketing mix (4Ps) recommendations—require an understanding of the unobserved heterogeneity to develop the best decision making-processes. In this article, we develop a hypothesis test and the corresponding p -value for testing for the significance of the homogeneous structure in linear mixed models. A robust matching moment construction is used for creating a test that adapts to the size of the model sparsity. When unobserved heterogeneity at a cluster level is constant, we show that our test is both consistent and unbiased even when the dimension of the model is extremely high. Our theoretical results rely on a new family of adaptive sparse estimators of the fixed effects that do not require consistent estimation of the random effects. Moreover, our inference results do not require consistent model selection. We showcase that moment matching can be extended to nonlinear mixed effects models and to generalized linear mixed effects models. In numerical and real data experiments, we find that the developed method is extremely accurate, that it adapts to the size of the underlying model and is decidedly powerful in the presence of irrelevant covariates. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jelena Bradic and Gerda Claeskens and Thomas Gueuning},
  doi          = {10.1080/01621459.2019.1660172},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1835-1850},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Fixed effects testing in high-dimensional linear mixed models},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). IPAD: Stable interpretable forecasting with knockoffs
inference. <em>JASA</em>, <em>115</em>(532), 1822–1834. (<a
href="https://doi.org/10.1080/01621459.2019.1654878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interpretability and stability are two important features that are desired in many contemporary big data applications arising in statistics, economics, and finance. While the former is enjoyed to some extent by many existing forecasting approaches, the latter in the sense of controlling the fraction of wrongly discovered features which can enhance greatly the interpretability is still largely underdeveloped. To this end, in this article, we exploit the general framework of model-X knockoffs introduced recently in Candès, Fan, Janson and Lv [( Citation 2018 ), “Panning for Gold: ‘model X’ Knockoffs for High Dimensional Controlled Variable Selection,” Journal of the Royal Statistical Society, Series B, 80, 551–577], which is nonconventional for reproducible large-scale inference in that the framework is completely free of the use of p -values for significance testing, and suggest a new method of intertwined probabilistic factors decoupling (IPAD) for stable interpretable forecasting with knockoffs inference in high-dimensional models. The recipe of the method is constructing the knockoff variables by assuming a latent factor model that is exploited widely in economics and finance for the association structure of covariates. Our method and work are distinct from the existing literature in which we estimate the covariate distribution from data instead of assuming that it is known when constructing the knockoff variables, our procedure does not require any sample splitting, we provide theoretical justifications on the asymptotic false discovery rate control, and the theory for the power analysis is also established. Several simulation examples and the real data analysis further demonstrate that the newly suggested method has appealing finite-sample performance with desired interpretability and stability compared to some popularly used forecasting methods. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yingying Fan and Jinchi Lv and Mahrad Sharifvaghefi and Yoshimasa Uematsu},
  doi          = {10.1080/01621459.2019.1654878},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1822-1834},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {IPAD: Stable interpretable forecasting with knockoffs inference},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal designs for the two-dimensional interference model.
<em>JASA</em>, <em>115</em>(532), 1812–1821. (<a
href="https://doi.org/10.1080/01621459.2019.1654877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, there have been some major advances in the theory of optimal designs for interference models when the block is arranged in one-dimensional layout. Relatively speaking, the study for two-dimensional interference model is quite limited partly due to technical difficulties. This article tries to fill this gap. Specifically, we set the tone by characterizing all possible universally optimal designs simultaneously through one linear equations system (LES) with respect to the proportions of block arrays. However, such a LES is not readily solvable due to the extremely large number of block arrays. This computational issue could be resolved by identifying a small subset of block arrays with the theoretical guarantee that any optimal design is supported by this subset. The nature of two-dimensional layout of the block has made this task very technically challenging, and we have theoretically derived such subset for any size of the treatment array and any number of treatments under comparison. This facilitates the development of the algorithm for deriving either approximate or exact designs. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {A. S. Hedayat and Heng Xu and Wei Zheng},
  doi          = {10.1080/01621459.2019.1654877},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1812-1821},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Optimal designs for the two-dimensional interference model},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A statistical method for emulation of computer models with
invariance-preserving properties, with application to structural energy
prediction. <em>JASA</em>, <em>115</em>(532), 1798–1811. (<a
href="https://doi.org/10.1080/01621459.2019.1654876">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical design and analysis of computer experiments is a growing area in statistics. Computer models with structural invariance properties now appear frequently in materials science, physics, biology, and other fields. These properties are consequences of dependency on structural geometry, and cannot be accommodated by standard statistical emulation methods. In this article, we propose a statistical framework for building emulators to preserve invariance. The framework uses a weighted complete graph to represent the geometry and introduces a new class of function, called the relabeling symmetric functions, associated with the graph. We establish a characterization theorem of the relabeling symmetric functions and propose a nonparametric kernel method for estimating such functions. The effectiveness of the proposed method is illustrated by examples from materials science. Supplemental material for this article can be found online.},
  archive      = {J_JASA},
  author       = {Xiao Nie and Peter Chien and Dane Morgan and Amy Kaczmarowski},
  doi          = {10.1080/01621459.2019.1654876},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1798-1811},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A statistical method for emulation of computer models with invariance-preserving properties, with application to structural energy prediction},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Functional horseshoe priors for subspace shrinkage.
<em>JASA</em>, <em>115</em>(532), 1784–1797. (<a
href="https://doi.org/10.1080/01621459.2019.1654875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new shrinkage prior on function spaces, called the functional horseshoe (fHS) prior, that encourages shrinkage toward parametric classes of functions. Unlike other shrinkage priors for parametric models, the fHS shrinkage acts on the shape of the function rather than inducing sparsity on model parameters. We study the efficacy of the proposed approach by showing an adaptive posterior concentration property on the function. We also demonstrate consistency of the model selection procedure that thresholds the shrinkage parameter of the fHS prior. We apply the fHS prior to nonparametric additive models and compare its performance with procedures based on the standard horseshoe prior and several penalized likelihood approaches. We find that the new procedure achieves smaller estimation error and more accurate model selection than other procedures in several simulated and real examples. Supplementary materials for this article, which contain additional simulated and real data examples, MCMC diagnostics, and proofs of the theoretical results, are available online.},
  archive      = {J_JASA},
  author       = {Minsuk Shin and Anirban Bhattacharya and Valen E. Johnson},
  doi          = {10.1080/01621459.2019.1654875},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1784-1797},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Functional horseshoe priors for subspace shrinkage},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Corrected bayesian information criterion for stochastic
block models. <em>JASA</em>, <em>115</em>(532), 1771–1783. (<a
href="https://doi.org/10.1080/01621459.2019.1637744">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the number of communities is one of the fundamental problems in community detection. We re-examine the Bayesian paradigm for stochastic block models (SBMs) and propose a “corrected Bayesian information criterion” (CBIC), to determine the number of communities and show that the proposed criterion is consistent under mild conditions as the size of the network and the number of communities go to infinity. The CBIC outperforms those used in Wang and Bickel and Saldana, Yu, and Feng which tend to underestimate and overestimate the number of communities, respectively. The results are further extended to degree corrected SBMs. Numerical studies demonstrate our theoretical results.},
  archive      = {J_JASA},
  author       = {Jianwei Hu and Hong Qin and Ting Yan and Yunpeng Zhao},
  doi          = {10.1080/01621459.2019.1637744},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1771-1783},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Corrected bayesian information criterion for stochastic block models},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Structured latent factor analysis for large-scale data:
Identifiability, estimability, and their implications. <em>JASA</em>,
<em>115</em>(532), 1756–1770. (<a
href="https://doi.org/10.1080/01621459.2019.1635485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract– Latent factor models are widely used to measure unobserved latent traits in social and behavioral sciences, including psychology, education, and marketing. When used in a confirmatory manner, design information is incorporated as zero constraints on corresponding parameters, yielding structured (confirmatory) latent factor models. In this article, we study how such design information affects the identifiability and the estimation of a structured latent factor model. Insights are gained through both asymptotic and nonasymptotic analyses. Our asymptotic results are established under a regime where both the number of manifest variables and the sample size diverge, motivated by applications to large-scale data. Under this regime, we define the structural identifiability of the latent factors and establish necessary and sufficient conditions that ensure structural identifiability. In addition, we propose an estimator which is shown to be consistent and rate optimal when structural identifiability holds. Finally, a nonasymptotic error bound is derived for this estimator, through which the effect of design information is further quantified. Our results shed lights on the design of large-scale measurement in education and psychology and have important implications on measurement validity and reliability.},
  archive      = {J_JASA},
  author       = {Yunxiao Chen and Xiaoou Li and Siliang Zhang},
  doi          = {10.1080/01621459.2019.1635485},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1756-1770},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Structured latent factor analysis for large-scale data: Identifiability, estimability, and their implications},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Detecting strong signals in gene perturbation experiments:
An adaptive approach with power guarantee and FDR control.
<em>JASA</em>, <em>115</em>(532), 1747–1755. (<a
href="https://doi.org/10.1080/01621459.2019.1635484">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The perturbation of a transcription factor should affect the expression levels of its direct targets. However, not all genes showing changes in expression are direct targets. To increase the chance of detecting direct targets, we propose a modified two-group model where the null group corresponds to genes which are not direct targets, but can have small nonzero effects. We model the behavior of genes from the null set by a Gaussian distribution with unknown variance τ 2 . To estimate τ 2 , we focus on a simple estimation approach, the iterated empirical Bayes estimation. We conduct a detailed analysis of the properties of the iterated EB estimate and provide theoretical guarantee of its good performance under mild conditions. We provide simulations comparing the new modeling approach with existing methods, and the new approach shows more stable and better performance under different situations. We also apply it to a real dataset from gene knock-down experiments and obtained better results compared with the original two-group model testing for nonzero effects.},
  archive      = {J_JASA},
  author       = {Leying Guan and Xi Chen and Wing Hung Wong},
  doi          = {10.1080/01621459.2019.1635484},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1747-1755},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Detecting strong signals in gene perturbation experiments: An adaptive approach with power guarantee and FDR control},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Flexible sensitivity analysis for observational studies
without observable implications. <em>JASA</em>, <em>115</em>(532),
1730–1746. (<a
href="https://doi.org/10.1080/01621459.2019.1604369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fundamental challenge in observational causal inference is that assumptions about unconfoundedness are not testable from data. Assessing sensitivity to such assumptions is therefore important in practice. Unfortunately, some existing sensitivity analysis approaches inadvertently impose restrictions that are at odds with modern causal inference methods, which emphasize flexible models for observed data. To address this issue, we propose a framework that allows (1) flexible models for the observed data and (2) clean separation of the identified and unidentified parts of the sensitivity model. Our framework extends an approach from the missing data literature, known as Tukey’s factorization, to the causal inference setting. Under this factorization, we can represent the distributions of unobserved potential outcomes in terms of unidentified selection functions that posit a relationship between treatment assignment and unobserved potential outcomes. The sensitivity parameters in this framework are easily interpreted, and we provide heuristics for calibrating these parameters against observable quantities. We demonstrate the flexibility of this approach in two examples, where we estimate both average treatment effects and quantile treatment effects using Bayesian nonparametric models for the observed data.},
  archive      = {J_JASA},
  author       = {AlexanderM. Franks and Alexander D’Amour and Avi Feller},
  doi          = {10.1080/01621459.2019.1604369},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1730-1746},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Flexible sensitivity analysis for observational studies without observable implications},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Rejoinder to “a tuning-free robust and efficient approach
to high-dimensional regression.” <em>JASA</em>, <em>115</em>(532),
1726–1729. (<a
href="https://doi.org/10.1080/01621459.2020.1843865">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Lan Wang and Bo Peng and Jelena Bradic and Runze Li and Yunan Wu},
  doi          = {10.1080/01621459.2020.1843865},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1726-1729},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Rejoinder to “A tuning-free robust and efficient approach to high-dimensional regression”},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comment on “a tuning-free robust and efficient approach to
high-dimensional regression.” <em>JASA</em>, <em>115</em>(532),
1720–1725. (<a
href="https://doi.org/10.1080/01621459.2020.1837138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Jianqing Fan and Cong Ma and Kaizheng Wang},
  doi          = {10.1080/01621459.2020.1837138},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1720-1725},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Comment on “A tuning-free robust and efficient approach to high-dimensional regression”},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discussion of “a tuning-free robust and efficient approach
to high-dimensional regression.” <em>JASA</em>, <em>115</em>(532),
1717–1719. (<a
href="https://doi.org/10.1080/01621459.2020.1837139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Xiudi Li and Ali Shojaie},
  doi          = {10.1080/01621459.2020.1837139},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1717-1719},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion of “A tuning-free robust and efficient approach to high-dimensional regression”},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comment on “a tuning-free robust and efficient approach to
high-dimensional regression.” <em>JASA</em>, <em>115</em>(532),
1715–1716. (<a
href="https://doi.org/10.1080/01621459.2020.1837141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Po-Ling Loh},
  doi          = {10.1080/01621459.2020.1837141},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1715-1716},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Comment on “A tuning-free robust and efficient approach to high-dimensional regression”},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). A tuning-free robust and efficient approach to
high-dimensional regression. <em>JASA</em>, <em>115</em>(532),
1700–1714. (<a
href="https://doi.org/10.1080/01621459.2020.1840989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a novel approach for high-dimensional regression with theoretical guarantees. The new procedure overcomes the challenge of tuning parameter selection of Lasso and possesses several appealing properties. It uses an easily simulated tuning parameter that automatically adapts to both the unknown random error distribution and the correlation structure of the design matrix. It is robust with substantial efficiency gain for heavy-tailed random errors while maintaining high efficiency for normal random errors. Comparing with other alternative robust regression procedures, it also enjoys the property of being equivariant when the response variable undergoes a scale transformation. Computationally, it can be efficiently solved via linear programming. Theoretically, under weak conditions on the random error distribution, we establish a finite-sample error bound with a near-oracle rate for the new estimator with the simulated tuning parameter. Our results make useful contributions to mending the gap between the practice and theory of Lasso and its variants. We also prove that further improvement in efficiency can be achieved by a second-stage enhancement with some light tuning. Our simulation results demonstrate that the proposed methods often outperform cross-validated Lasso in various settings.},
  archive      = {J_JASA},
  author       = {Lan Wang and Bo Peng and Jelena Bradic and Runze Li and Yunan Wu},
  doi          = {10.1080/01621459.2020.1840989},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1700-1714},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A tuning-free robust and efficient approach to high-dimensional regression},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating the effects of fine particulate matter on 432
cardiovascular diseases using multi-outcome regression with
tree-structured shrinkage. <em>JASA</em>, <em>115</em>(532), 1689–1699.
(<a href="https://doi.org/10.1080/01621459.2020.1722134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The positive relationship between airborne fine particulate matter (PM 2.5 ) and cardiovascular disease (CVD) is established. Little is known about effect size heterogeneity across distinct CVD outcomes. We conducted a multi-outcome case-crossover study of Medicare beneficiaries aged &gt;65 years residing in the mainland USA from 2000 through 2012. The exposure was two-day average PM 2.5 in each individual’s residential zipcode. The outcomes were hospitalization for 432 distinct CVDs defined by the International Classification of Diseases, Revision 9. Our dataset included almost 24 million CVD hospitalizations. We analyzed the data using multi-outcome regression with tree-structured shrinkage (MOReTreeS), a novel method that enables: (1) borrowing of strength across outcomes; (2) data-driven discovery of outcome groups that are similarly affected by the exposure; (3) estimation of a single effect for each group. MOReTreeS grouped 420 outcomes together; for this group, the odds ratio [OR] for hospitalization associated with a 10 μ g m − 3 increase in PM 2.5 was 1.011 (95\% credible interval [CI] = 1.011–1.012). The model identified congestive heart failure as having the strongest positive association with PM 2.5 (OR = 1.019; 95\%CI = 1.017–1.022). Some outcomes exhibited negative associations with PM 2.5 , including aortic dissection, subarachnoid and intracerebral hemorrhage, abdominal aneurysm, and essential hypertension; further research is needed to understand these counterintuitive findings. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Emma G. Thomas and Lorenzo Trippa and Giovanni Parmigiani and Francesca Dominici},
  doi          = {10.1080/01621459.2020.1722134},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1689-1699},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Estimating the effects of fine particulate matter on 432 cardiovascular diseases using multi-outcome regression with tree-structured shrinkage},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal tradeoffs in matched designs comparing US-trained
and internationally trained surgeons. <em>JASA</em>, <em>115</em>(532),
1675–1688. (<a
href="https://doi.org/10.1080/01621459.2020.1720693">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Does receiving a medical education outside the United States impact a surgeon’s performance? We study this question by matching operations performed by internationally trained surgeons to those performed by US-trained surgeons in reanalysis of a large health outcomes study. An effective matched design must achieve several goals, including balancing covariate distributions marginally, ensuring units within individual pairs have similar values on key covariates, and using a sufficiently large sample from the raw data. Yet in our study, optimizing some of these goals forces less desirable results on others. We address such tradeoffs from a multi-objective optimization perspective by creating matched designs that are Pareto optimal with respect to two goals. We provide general tools for generating representative subsets of Pareto optimal solution sets and articulate how they can be used to improve decision-making in observational study design. In the motivating surgical outcomes study, formulating a multi-objective version of the problem helps us balance an important variable without sacrificing two other design goals, average closeness of matched pairs on a multivariate distance and size of the final matched sample. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Samuel D. Pimentel and Rachel R. Kelz},
  doi          = {10.1080/01621459.2020.1720693},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1675-1688},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Optimal tradeoffs in matched designs comparing US-trained and internationally trained surgeons},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Calibration for computer experiments with binary responses
and application to cell adhesion study. <em>JASA</em>,
<em>115</em>(532), 1664–1674. (<a
href="https://doi.org/10.1080/01621459.2019.1699419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Calibration refers to the estimation of unknown parameters which are present in computer experiments but not available in physical experiments. An accurate estimation of these parameters is important because it provides a scientific understanding of the underlying system which is not available in physical experiments. Most of the work in the literature is limited to the analysis of continuous responses. Motivated by a study of cell adhesion experiments, we propose a new calibration framework for binary responses. Its application to the T cell adhesion data provides insight into the unknown values of the kinetic parameters which are difficult to determine by physical experiments due to the limitation of the existing experimental techniques. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Chih-Li Sung and Ying Hung and William Rittase and Cheng Zhu and C. F. J. Wu},
  doi          = {10.1080/01621459.2019.1699419},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1664-1674},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Calibration for computer experiments with binary responses and application to cell adhesion study},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bayesian latent class model to predict kidney obstruction
in the absence of gold standard. <em>JASA</em>, <em>115</em>(532),
1645–1663. (<a
href="https://doi.org/10.1080/01621459.2019.1689983">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kidney obstruction, if untreated in a timely manner, can lead to irreversible loss of renal function. A widely used technology for evaluations of kidneys with suspected obstruction is diuresis renography. However, it is generally very challenging for radiologists who typically interpret renography data in practice to build high level of competency due to the low volume of renography studies and insufficient training. Another challenge is that there is currently no gold standard for detection of kidney obstruction. Seeking to develop a computer-aided diagnostic (CAD) tool that can assist practicing radiologists to reduce errors in the interpretation of kidney obstruction, a recent study collected data from diuresis renography, interpretations on the renography data from highly experienced nuclear medicine experts as well as clinical data. To achieve the objective, we develop a statistical model that can be used as a CAD tool for assisting radiologists in kidney interpretation. We use a Bayesian latent class modeling approach for predicting kidney obstruction through the integrative analysis of time-series renogram data, expert ratings, and clinical variables. A nonparametric Bayesian latent factor regression approach is adopted for modeling renogram curves in which the coefficients of the basis functions are parameterized via the factor loadings dependent on the latent disease status and the extended latent factors that can also adjust for clinical variables. A hierarchical probit model is used for expert ratings, allowing for training with rating data from multiple experts while predicting with at most one expert, which makes the proposed model operable in practice. An efficient MCMC algorithm is developed to train the model and predict kidney obstruction with associated uncertainty. We demonstrate the superiority of the proposed method over several existing methods through extensive simulations. Analysis of the renal study also lends support to the usefulness of our model as a CAD tool to assist less experienced radiologists in the field. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Changgee Chang and Jeong Hoon Jang and Amita Manatunga and Andrew T. Taylor and Qi Long},
  doi          = {10.1080/01621459.2019.1689983},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1645-1663},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A bayesian latent class model to predict kidney obstruction in the absence of gold standard},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparametric bayesian instrumental variable analysis:
Evaluating heterogeneous effects of coronary arterial access site
strategies. <em>JASA</em>, <em>115</em>(532), 1635–1644. (<a
href="https://doi.org/10.1080/01621459.2019.1688663">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Percutaneous coronary interventions (PCIs) are nonsurgical procedures to open blocked blood vessels to the heart, frequently using a catheter to place a stent. The catheter can be inserted into the blood vessels using an artery in the groin or an artery in the wrist. Because clinical trials have indicated that access via the wrist may result in fewer post procedure complications, shortening the length of stay, and ultimately cost less than groin access, adoption of access via the wrist has been encouraged. However, patients treated in usual care are likely to differ from those participating in clinical trials, and there is reason to believe that the effectiveness of wrist access may differ between males and females. Moreover, the choice of artery access strategy is likely to be influenced by patient or physician unmeasured factors. To study the effectiveness of the two artery access site strategies on hospitalization charges, we use data from a state-mandated clinical registry including 7963 patients undergoing PCI. A hierarchical Bayesian likelihood-based instrumental variable analysis under a latent index modeling framework is introduced to jointly model outcomes and treatment status. Our approach accounts for unobserved heterogeneity via a latent factor structure, and permits nonparametric error distributions with Dirichlet process mixture models. Our results demonstrate that artery access in the wrist reduces hospitalization charges compared to access in the groin, with a higher mean reduction for male patients. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Samrachana Adhikari and Sherri Rose and Sharon-Lise Normand},
  doi          = {10.1080/01621459.2019.1688663},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1635-1644},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Nonparametric bayesian instrumental variable analysis: Evaluating heterogeneous effects of coronary arterial access site strategies},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian double feature allocation for phenotyping with
electronic health records. <em>JASA</em>, <em>115</em>(532), 1620–1634.
(<a href="https://doi.org/10.1080/01621459.2019.1686985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electronic health records (EHR) provide opportunities for deeper understanding of human phenotypes—in our case, latent disease—based on statistical modeling. We propose a categorical matrix factorization method to infer latent diseases from EHR data. A latent disease is defined as an unknown biological aberration that causes a set of common symptoms for a group of patients. The proposed approach is based on a novel double feature allocation model which simultaneously allocates features to the rows and the columns of a categorical matrix. Using a Bayesian approach, available prior information on known diseases (e.g., hypertension and diabetes) greatly improves identifiability and interpretability of the latent diseases. We assess the proposed approach by simulation studies including mis-specified models and comparison with sparse latent factor models. In the application to a Chinese EHR dataset, we identify 10 latent diseases, each of which is shared by groups of subjects with specific health traits related to lipid disorder, thrombocytopenia, polycythemia, anemia, bacterial and viral infections, allergy, and malnutrition. The identification of the latent diseases can help healthcare officials better monitor the subjects’ ongoing health conditions and look into potential risk factors and approaches for disease prevention. We cross-check the reported latent diseases with medical literature and find agreement between our discovery and reported findings elsewhere. We provide an R package “dfa” implementing our method and an R shiny web application reporting the findings. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Yang Ni and Peter Müller and Yuan Ji},
  doi          = {10.1080/01621459.2019.1686985},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1620-1634},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian double feature allocation for phenotyping with electronic health records},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Testing the predictability of u.s. Housing price index
returns based on an IVX-AR model. <em>JASA</em>, <em>115</em>(532),
1598–1619. (<a
href="https://doi.org/10.1080/01621459.2019.1686392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We use ten common macroeconomic variables to test for the predictability of the quarterly growth rate of house price index (HPI) in the United States during 1975:Q1–2018:Q2. We extend the instrumental variable based Wald statistic (IVX-KMS) proposed by Kostakis, Magdalinos, and Stamatogiannis to a new instrumental variable based Wald statistic (IVX-AR) which accounts for serial correlation and heteroscedasticity in the error terms of the linear predictive regression model. Simulation results show that the proposed IVX-AR exhibits excellent size control regardless of the degree of serial correlation in the error terms and the persistency in the predictive variables, while IVX-KMS displays severe size distortions. The empirical results indicate that the percentage of residential fixed investment in GDP is fairly a robust predictor of the growth rate of HPI. However, other macroeconomic variables’ strong predictive ability detected by IVX-KMS is likely to be driven by the highly correlated error terms in the predictive regressions and thus becomes insignificant when the proposed IVX-AR method is implemented. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Bingduo Yang and Wei Long and Liang Peng and Zongwu Cai},
  doi          = {10.1080/01621459.2019.1686392},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1598-1619},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Testing the predictability of U.S. housing price index returns based on an IVX-AR model},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian scalar on image regression with nonignorable
nonresponse. <em>JASA</em>, <em>115</em>(532), 1574–1597. (<a
href="https://doi.org/10.1080/01621459.2019.1686391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical imaging has become an increasingly important tool in screening, diagnosis, prognosis, and treatment of various diseases given its information visualization and quantitative assessment. The aim of this article is to develop a Bayesian scalar-on-image regression model to integrate high-dimensional imaging data and clinical data to predict cognitive, behavioral, or emotional outcomes, while allowing for nonignorable missing outcomes. Such a nonignorable nonresponse consideration is motivated by examining the association between baseline characteristics and cognitive abilities for 802 Alzheimer patients enrolled in the Alzheimer’s Disease Neuroimaging Initiative 1 (ADNI1), for which data are partially missing. Ignoring such missing data may distort the accuracy of statistical inference and provoke misleading results. To address this issue, we propose an imaging exponential tilting model to delineate the data missing mechanism and incorporate an instrumental variable to facilitate model identifiability followed by a Bayesian framework with Markov chain Monte Carlo algorithms to conduct statistical inference. This approach is validated in simulation studies where both the finite sample performance and asymptotic properties are evaluated and compared with the model with fully observed data and that with a misspecified ignorable missing mechanism. Our proposed methods are finally carried out on the ADNI1 dataset, which turns out to capture both of those clinical risk factors and imaging regions consistent with the existing literature that exhibits clinical significance. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Xiangnan Feng and Tengfei Li and Xinyuan Song and Hongtu Zhu},
  doi          = {10.1080/01621459.2019.1686391},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1574-1597},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian scalar on image regression with nonignorable nonresponse},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The statistical face of a region under monsoon rainfall in
eastern india. <em>JASA</em>, <em>115</em>(532), 1559–1573. (<a
href="https://doi.org/10.1080/01621459.2019.1681275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A region under rainfall is a contiguous spatial area receiving positive precipitation at a particular time. The probabilistic behavior of such a region is an issue of interest in meteorological studies. A region under rainfall can be viewed as a shape object of a special kind, where scale and rotational invariance are not necessarily desirable attributes of a mathematical representation. For modeling variation in objects of this type, we propose an approximation of the boundary that can be represented as a real valued function, and arrive at further approximation through functional principal component analysis, after suitable adjustment for asymmetry and incompleteness in the data. The analysis of an open access satellite dataset on monsoon precipitation over Eastern Indian subcontinent leads to explanation of most of the variation in shapes of the regions under rainfall through a handful of interpretable functions that can be further approximated parametrically. The most important aspect of shape is found to be the size followed by contraction/elongation, mostly along two pairs of orthogonal axes. The different modes of variation are remarkably stable across calendar years and across different thresholds for minimum size of the region. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Kaushik Jana and Debasis Sengupta and Subrata Kundu and Arindam Chakraborty and Purnima Shaw},
  doi          = {10.1080/01621459.2019.1681275},
  journal      = {Journal of the American Statistical Association},
  number       = {532},
  pages        = {1559-1573},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {The statistical face of a region under monsoon rainfall in eastern india},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Time series clustering and classification. <em>JASA</em>,
<em>115</em>(531), 1558. (<a
href="https://doi.org/10.1080/01621459.2020.1801281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Ming Chen},
  doi          = {10.1080/01621459.2020.1801281},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1558},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Time series clustering and classification},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Statistical computing with r. <em>JASA</em>,
<em>115</em>(531), 1557–1558. (<a
href="https://doi.org/10.1080/01621459.2020.1801280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Ling Leng},
  doi          = {10.1080/01621459.2020.1801280},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1557-1558},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistical computing with r},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Handbook of graphical models. <em>JASA</em>,
<em>115</em>(531), 1555–1557. (<a
href="https://doi.org/10.1080/01621459.2020.1801279">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Genevera I. Allen},
  doi          = {10.1080/01621459.2020.1801279},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1555-1557},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Handbook of graphical models},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Combining multiple observational data sources to estimate
causal effects. <em>JASA</em>, <em>115</em>(531), 1540–1554. (<a
href="https://doi.org/10.1080/01621459.2019.1609973">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The era of big data has witnessed an increasing availability of multiple data sources for statistical analyses. We consider estimation of causal effects combining big main data with unmeasured confounders and smaller validation data with supplementary information on these confounders. Under the unconfoundedness assumption with completely observed confounders, the smaller validation data allow for constructing consistent estimators for causal effects, but the big main data can only give error-prone estimators in general. However, by leveraging the information in the big main data in a principled way, we can improve the estimation efficiencies yet preserve the consistencies of the initial estimators based solely on the validation data. Our framework applies to asymptotically normal estimators, including the commonly used regression imputation, weighting, and matching estimators, and does not require a correct specification of the model relating the unmeasured confounders to the observed variables. We also propose appropriate bootstrap procedures, which makes our method straightforward to implement using software routines for existing estimators. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Shu Yang and Peng Ding},
  doi          = {10.1080/01621459.2019.1609973},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1540-1554},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Combining multiple observational data sources to estimate causal effects},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating optimal dynamic treatment regimes with survival
outcomes. <em>JASA</em>, <em>115</em>(531), 1531–1539. (<a
href="https://doi.org/10.1080/01621459.2019.1629939">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The statistical study of precision medicine is concerned with dynamic treatment regimes (DTRs) in which treatment decisions are tailored to patient-level information. Individuals are followed through multiple stages of clinical intervention, and the goal is to perform inferences on the sequence of personalized treatment decision rules to be applied in practice. Of interest is the identification of an optimal DTR, that is, the sequence of treatment decisions that yields the best expected outcome. Statistical methods for identifying optimal DTRs from observational data are theoretically complex and not easily implementable by researchers, especially when the outcome of interest is survival time. We propose a doubly robust, easy to implement method for estimating optimal DTRs with survival endpoints subject to right-censoring which requires solving a series of weighted generalized estimating equations. We provide a proof of consistency that relies on the balancing property of the weights and derive a formula for the asymptotic variance of the resulting estimators. We illustrate our novel approach with an application to the treatment of rheumatoid arthritis using observational data from the Scottish Early Rheumatoid Arthritis Inception Cohort. Our method, called dynamic weighted survival modeling, has been implemented in the DTRreg R package. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Gabrielle Simoneau and Erica E. M. Moodie and Jagtar S. Nijjar and Robert W. Platt and the Scottish Early Rheumatoid Arthritis Inception Cohort Investigators},
  doi          = {10.1080/01621459.2019.1629939},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1531-1539},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Estimating optimal dynamic treatment regimes with survival outcomes},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Studentized sensitivity analysis for the sample average
treatment effect in paired observational studies. <em>JASA</em>,
<em>115</em>(531), 1518–1530. (<a
href="https://doi.org/10.1080/01621459.2019.1632072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fundamental limitation of causal inference in observational studies is that perceived evidence for an effect might instead be explained by factors not accounted for in the primary analysis. Methods for assessing the sensitivity of a study’s conclusions to unmeasured confounding have been established under the assumption that the treatment effect is constant across all individuals. In the potential presence of unmeasured confounding, it has been argued that certain patterns of effect heterogeneity may conspire with unobserved covariates to render the performed sensitivity analysis inadequate. We present a new method for conducting a sensitivity analysis for the sample average treatment effect in the presence of effect heterogeneity in paired observational studies. Our recommended procedure, called the studentized sensitivity analysis, represents an extension of recent work on studentized permutation tests to the case of observational studies, where randomizations are no longer drawn uniformly. The method naturally extends conventional tests for the sample average treatment effect in paired experiments to the case of unknown, but bounded, probabilities of assignment to treatment. In so doing, we illustrate that concerns about certain sensitivity analyses operating under the presumption of constant effects are largely unwarranted.},
  archive      = {J_JASA},
  author       = {Colin B. Fogarty},
  doi          = {10.1080/01621459.2019.1632072},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1518-1530},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Studentized sensitivity analysis for the sample average treatment effect in paired observational studies},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian inference for sequential treatments under latent
sequential ignorability. <em>JASA</em>, <em>115</em>(531), 1498–1517.
(<a href="https://doi.org/10.1080/01621459.2019.1623039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We focus on causal inference for longitudinal treatments, where units are assigned to treatments at multiple time points, aiming to assess the effect of different treatment sequences on an outcome observed at a final point. A common assumption in similar studies is sequential ignorability (SI): treatment assignment at each time point is assumed independent of future potential outcomes given past observed outcomes and covariates. SI is questionable when treatment participation depends on individual choices, and treatment assignment may depend on unobservable quantities associated with future outcomes. We rely on principal stratification to formulate a relaxed version of SI: latent sequential ignorability (LSI) assumes that treatment assignment is conditionally independent on future potential outcomes given past treatments, covariates, and principal stratum membership, a latent variable defined by the joint value of observed and missing intermediate outcomes. We evaluate SI and LSI, using theoretical arguments and simulation studies to investigate the performance of the two assumptions when one holds and inference is conducted under both. Simulations show that when SI does not hold, inference performed under SI leads to misleading conclusions. Conversely, LSI generally leads to correct posterior distributions, irrespective of which assumption holds.},
  archive      = {J_JASA},
  author       = {Federico Ricciardi and Alessandra Mattei and Fabrizia Mealli},
  doi          = {10.1080/01621459.2019.1623039},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1498-1517},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian inference for sequential treatments under latent sequential ignorability},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Statistical inference for covariate-adaptive randomization
procedures. <em>JASA</em>, <em>115</em>(531), 1488–1497. (<a
href="https://doi.org/10.1080/01621459.2019.1635483">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Covariate-adaptive randomization (CAR) procedures are frequently used in comparative studies to increase the covariate balance across treatment groups. However, because randomization inevitably uses the covariate information when forming balanced treatment groups, the validity of classical statistical methods after such randomization is often unclear. In this article, we derive the theoretical properties of statistical methods based on general CAR under the linear model framework. More importantly, we explicitly unveil the relationship between covariate-adaptive and inference properties by deriving the asymptotic representations of the corresponding estimators. We apply the proposed general theory to various randomization procedures such as complete randomization, rerandomization, pairwise sequential randomization, and Atkinson’s D A -biased coin design and compare their performance analytically. Based on the theoretical results, we then propose a new approach to obtain valid and more powerful tests. These results open a door to understand and analyze experiments based on CAR. Simulation studies provide further evidence of the advantages of the proposed framework and the theoretical results. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Wei Ma and Yichen Qin and Yang Li and Feifang Hu},
  doi          = {10.1080/01621459.2019.1635483},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1488-1497},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistical inference for covariate-adaptive randomization procedures},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficiently inferring the demographic history of many
populations with allele count data. <em>JASA</em>, <em>115</em>(531),
1472–1487. (<a
href="https://doi.org/10.1080/01621459.2019.1635482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sample frequency spectrum (SFS), or histogram of allele counts, is an important summary statistic in evolutionary biology, and is often used to infer the history of population size changes, migrations, and other demographic events affecting a set of populations. The expected multipopulation SFS under a given demographic model can be efficiently computed when the populations in the model are related by a tree, scaling to hundreds of populations. Admixture, back-migration, and introgression are common natural processes that violate the assumption of a tree-like population history, however, and until now the expected SFS could be computed for only a handful of populations when the demographic history is not a tree. In this article, we present a new method for efficiently computing the expected SFS and linear functionals of it, for demographies described by general directed acyclic graphs. This method can scale to more populations than previously possible for complex demographic histories including admixture. We apply our method to an 8-population SFS to estimate the timing and strength of a proposed “basal Eurasian” admixture event in human history. We implement and release our method in a new open-source software package momi2.},
  archive      = {J_JASA},
  author       = {Jack Kamm and Jonathan Terhorst and Richard Durbin and Yun S. Song},
  doi          = {10.1080/01621459.2019.1635482},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1472-1487},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Efficiently inferring the demographic history of many populations with allele count data},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparametric estimation of multivariate mixtures.
<em>JASA</em>, <em>115</em>(531), 1456–1471. (<a
href="https://doi.org/10.1080/01621459.2019.1635481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A multivariate mixture model is determined by three elements: the number of components, the mixing proportions, and the component distributions. Assuming that the number of components is given and that each mixture component has independent marginal distributions, we propose a nonparametric method to estimate the component distributions. The basic idea is to convert the estimation of component density functions to a problem of estimating the coordinates of the component density functions with respect to a good set of basis functions. Specifically, we construct a set of basis functions by using conditional density functions and try to recover the coordinates of component density functions with respect to this set of basis functions. Furthermore, we show that our estimator for the component density functions is consistent. Numerical studies are used to compare our algorithm with other existing nonparametric methods of estimating component distributions under the assumption of conditionally independent marginals.},
  archive      = {J_JASA},
  author       = {Chaowen Zheng and Yichao Wu},
  doi          = {10.1080/01621459.2019.1635481},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1456-1471},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Nonparametric estimation of multivariate mixtures},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simple local polynomial density estimators. <em>JASA</em>,
<em>115</em>(531), 1449–1455. (<a
href="https://doi.org/10.1080/01621459.2019.1635480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces an intuitive and easy-to-implement nonparametric density estimator based on local polynomial techniques. The estimator is fully boundary adaptive and automatic, but does not require prebinning or any other transformation of the data. We study the main asymptotic properties of the estimator, and use these results to provide principled estimation, inference, and bandwidth selection methods. As a substantive application of our results, we develop a novel discontinuity in density testing procedure, an important problem in regression discontinuity designs and other program evaluation settings. An illustrative empirical application is given. Two companion Stata and R software packages are provided.},
  archive      = {J_JASA},
  author       = {Matias D. Cattaneo and Michael Jansson and Xinwei Ma},
  doi          = {10.1080/01621459.2019.1635480},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1449-1455},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Simple local polynomial density estimators},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Principal boundary on riemannian manifolds. <em>JASA</em>,
<em>115</em>(531), 1435–1448. (<a
href="https://doi.org/10.1080/01621459.2019.1610660">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the classification problem and focus on nonlinear methods for classification on manifolds. For multivariate datasets lying on an embedded nonlinear Riemannian manifold within the higher-dimensional ambient space, we aim to acquire a classification boundary for the classes with labels, using the intrinsic metric on the manifolds. Motivated by finding an optimal boundary between the two classes, we invent a novel approach—the principal boundary. From the perspective of classification, the principal boundary is defined as an optimal curve that moves in between the principal flows traced out from two classes of data, and at any point on the boundary, it maximizes the margin between the two classes. We estimate the boundary in quality with its direction, supervised by the two principal flows. We show that the principal boundary yields the usual decision boundary found by the support vector machine in the sense that locally, the two boundaries coincide. Some optimality and convergence properties of the random principal boundary and its population counterpart are also shown. We illustrate how to find, use, and interpret the principal boundary with an application in real data. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Zhigang Yao and Zhenyue Zhang},
  doi          = {10.1080/01621459.2019.1610660},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1435-1448},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Principal boundary on riemannian manifolds},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Statistical analysis of functions on surfaces, with an
application to medical imaging. <em>JASA</em>, <em>115</em>(531),
1420–1434. (<a
href="https://doi.org/10.1080/01621459.2019.1635479">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract– In functional data analysis, data are commonly assumed to be smooth functions on a fixed interval of the real line. In this work, we introduce a comprehensive framework for the analysis of functional data, whose domain is a two-dimensional manifold and the domain itself is subject to variability from sample to sample. We formulate a statistical model for such data, here called functions on surfaces, which enables a joint representation of the geometric and functional aspects, and propose an associated estimation framework. We assess the validity of the framework by performing a simulation study and we finally apply it to the analysis of neuroimaging data of cortical thickness, acquired from the brains of different subjects, and thus lying on domains with different geometries. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Eardi Lila and John A. D. Aston},
  doi          = {10.1080/01621459.2019.1635479},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1420-1434},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistical analysis of functions on surfaces, with an application to medical imaging},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal sparse linear prediction for block-missing
multi-modality data without imputation. <em>JASA</em>,
<em>115</em>(531), 1406–1419. (<a
href="https://doi.org/10.1080/01621459.2019.1632079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In modern scientific research, data are often collected from multiple modalities. Since different modalities could provide complementary information, statistical prediction methods using multimodality data could deliver better prediction performance than using single modality data. However, one special challenge for using multimodality data is related to block-missing data. In practice, due to dropouts or the high cost of measures, the observations of a certain modality can be missing completely for some subjects. In this paper, we propose a new direct sparse regression procedure using covariance from multimodality data (DISCOM). Our proposed DISCOM method includes two steps to find the optimal linear prediction of a continuous response variable using block-missing multimodality predictors. In the first step, rather than deleting or imputing missing data, we make use of all available information to estimate the covariance matrix of the predictors and the cross-covariance vector between the predictors and the response variable. The proposed new estimate of the covariance matrix is a linear combination of the identity matrix, the estimates of the intra-modality covariance matrix and the cross-modality covariance matrix. Flexible estimates for both the sub-Gaussian and heavy-tailed cases are considered. In the second step, based on the estimated covariance matrix and the estimated cross-covariance vector, an extended Lasso-type estimator is used to deliver a sparse estimate of the coefficients in the optimal linear prediction. The number of samples that are effectively used by DISCOM is the minimum number of samples with available observations from two modalities, which can be much larger than the number of samples with complete observations from all modalities. The effectiveness of the proposed method is demonstrated by theoretical studies, simulated examples, and a real application from the Alzheimer’s Disease Neuroimaging Initiative. The comparison between DISCOM and some existing methods also indicates the advantages of our proposed method.},
  archive      = {J_JASA},
  author       = {Guan Yu and Quefeng Li and Dinggang Shen and Yufeng Liu},
  doi          = {10.1080/01621459.2019.1632079},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1406-1419},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Optimal sparse linear prediction for block-missing multi-modality data without imputation},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model-free forward screening via cumulative divergence.
<em>JASA</em>, <em>115</em>(531), 1393–1405. (<a
href="https://doi.org/10.1080/01621459.2019.1632078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature screening plays an important role in the analysis of ultrahigh dimensional data. Due to complicated model structure and high noise level, existing screening methods often suffer from model misspecification and the presence of outliers. To address these issues, we introduce a new metric named cumulative divergence (CD), and develop a CD-based forward screening procedure. This forward screening method is model-free and resistant to the presence of outliers in the response. It also incorporates the joint effects among covariates into the screening process. With a data-driven threshold, the new method can automatically determine the number of features that should be retained after screening. These merits make the CD-based screening very appealing in practice. Under certain regularity conditions, we show that the proposed method possesses sure screening property. The performance of our proposal is illustrated through simulations and a real data example. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Tingyou Zhou and Liping Zhu and Chen Xu and Runze Li},
  doi          = {10.1080/01621459.2019.1632078},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1393-1405},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Model-free forward screening via cumulative divergence},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distribution on warp maps for alignment of open and closed
curves. <em>JASA</em>, <em>115</em>(531), 1378–1392. (<a
href="https://doi.org/10.1080/01621459.2019.1632066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alignment of curve data is an integral part of their statistical analysis, and can be achieved using model- or optimization-based approaches. The parameter space is usually the set of monotone, continuous warp maps of a domain. Infinite-dimensional nature of the parameter space encourages sampling based approaches, which require a distribution on the set of warp maps. Moreover, the distribution should also enable sampling in the presence of important landmark information on the curves which constrain the warp maps. For alignment of closed and open curves in R d , d = 1 , 2 , 3 , possibly with landmark information, we provide a constructive, point-process based definition of a distribution on the set of warp maps of [0, 1] and the unit circle S , that is, (1) simple to sample from, and (2) possesses the desiderata for decomposition of the alignment problem with landmark constraints into multiple unconstrained ones. For warp maps on [0, 1], the distribution is related to the Dirichlet process. We demonstrate its utility by using it as a prior distribution on warp maps in a Bayesian model for alignment of two univariate curves, and as a proposal distribution in a stochastic algorithm that optimizes a suitable alignment functional for higher-dimensional curves. Several examples from simulated and real datasets are provided.},
  archive      = {J_JASA},
  author       = {Karthik Bharath and Sebastian Kurtek},
  doi          = {10.1080/01621459.2019.1632066},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1378-1392},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Distribution on warp maps for alignment of open and closed curves},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A likelihood ratio approach to sequential change point
detection for a general class of parameters. <em>JASA</em>,
<em>115</em>(531), 1361–1377. (<a
href="https://doi.org/10.1080/01621459.2019.1630562">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a new approach for sequential monitoring of a general class of parameters of a d -dimensional time series, which can be estimated by approximately linear functionals of the empirical distribution function. We consider a closed-end method, which is motivated by the likelihood ratio test principle and compare the new method with two alternative procedures. We also incorporate self-normalization such that estimation of the long-run variance is not necessary. We prove that for a large class of testing problems the new detection scheme has asymptotic level α and is consistent. The asymptotic theory is illustrated for the important cases of monitoring a change in the mean, variance, and correlation. By means of a simulation study it is demonstrated that the new test performs better than the currently available procedures for these problems. Finally, the methodology is illustrated by a small data example investigating index prices from the dot-com bubble. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Holger Dette and Josua Gösmann},
  doi          = {10.1080/01621459.2019.1630562},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1361-1377},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A likelihood ratio approach to sequential change point detection for a general class of parameters},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comparing and weighting imperfect models using
d-probabilities. <em>JASA</em>, <em>115</em>(531), 1349–1360. (<a
href="https://doi.org/10.1080/01621459.2019.1611140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new approach for assigning weights to models using a divergence-based method ( D-probabilities ), relying on evaluating parametric models relative to a nonparametric Bayesian reference using Kullback–Leibler divergence. D-probabilities are useful in goodness-of-fit assessments, in comparing imperfect models, and in providing model weights to be used in model aggregation. D-probabilities avoid some of the disadvantages of Bayesian model probabilities, such as large sensitivity to prior choice, and tend to place higher weight on a greater diversity of models. In an application to linear model selection against a Gaussian process reference, we provide simple analytic forms for routine implementation and show that D-probabilities automatically penalize model complexity. Some asymptotic properties are described, and we provide interesting probabilistic interpretations of the proposed model weights. The framework is illustrated through simulation examples and an ozone data application. Supplementary materials for this aricle are available online.},
  archive      = {J_JASA},
  author       = {Meng Li and David B. Dunson},
  doi          = {10.1080/01621459.2019.1611140},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1349-1360},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Comparing and weighting imperfect models using D-probabilities},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tests for scale changes based on pairwise differences.
<em>JASA</em>, <em>115</em>(531), 1336–1348. (<a
href="https://doi.org/10.1080/01621459.2019.1629938">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many applications it is important to know whether the amount of fluctuation in a series of observations changes over time. In this article, we investigate different tests for detecting changes in the scale of mean-stationary time series. The classical approach, based on the CUSUM test applied to the squared centered observations, is very vulnerable to outliers and impractical for heavy-tailed data, which leads us to contemplate test statistics based on alternative, less outlier-sensitive scale estimators. It turns out that the tests based on Gini’s mean difference (the average of all pairwise distances) and generalized Q n estimators (sample quantiles of all pairwise distances) are very suitable candidates. They improve upon the classical test not only under heavy tails or in the presence of outliers, but also under normality. We use recent results on the process convergence of U -statistics and U -quantiles for dependent sequences to derive the limiting distribution of the test statistics and propose estimators for the long-run variance. We show the consistency of the tests and demonstrate the applicability of the new change-point detection methods at two real-life data examples from hydrology and finance. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Carina Gerstenberger and Daniel Vogel and Martin Wendler},
  doi          = {10.1080/01621459.2019.1629938},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1336-1348},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Tests for scale changes based on pairwise differences},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian model search for nonstationary periodic time
series. <em>JASA</em>, <em>115</em>(531), 1320–1335. (<a
href="https://doi.org/10.1080/01621459.2019.1623043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel Bayesian methodology for analyzing nonstationary time series that exhibit oscillatory behavior. We approximate the time series using a piecewise oscillatory model with unknown periodicities, where our goal is to estimate the change-points while simultaneously identifying the potentially changing periodicities in the data. Our proposed methodology is based on a trans-dimensional Markov chain Monte Carlo algorithm that simultaneously updates the change-points and the periodicities relevant to any segment between them. We show that the proposed methodology successfully identifies time changing oscillatory behavior in two applications which are relevant to e-Health and sleep research, namely the occurrence of ultradian oscillations in human skin temperature during the time of night rest, and the detection of instances of sleep apnea in plethysmographic respiratory traces. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Beniamino Hadj-Amar and Bärbel Finkenstädt Rand and Mark Fiecas and Francis Lévi and Robert Huckstepp},
  doi          = {10.1080/01621459.2019.1623043},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1320-1335},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian model search for nonstationary periodic time series},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Likelihood ratio tests for a large directed acyclic graph.
<em>JASA</em>, <em>115</em>(531), 1304–1319. (<a
href="https://doi.org/10.1080/01621459.2019.1623042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inference of directional pairwise relations between interacting units in a directed acyclic graph (DAG), such as a regulatory gene network, is common in practice, imposing challenges because of lack of inferential tools. For example, inferring a specific gene pathway of a regulatory gene network is biologically important. Yet, frequentist inference of directionality of connections remains largely unexplored for regulatory models. In this article, we propose constrained likelihood ratio tests for inference of the connectivity as well as directionality subject to nonconvex acyclicity constraints in a Gaussian directed graphical model. Particularly, we derive the asymptotic distributions of the constrained likelihood ratios in a high-dimensional situation. For testing of connectivity, the asymptotic distribution is either chi-squared or normal depending on if the number of testable links in a DAG model is small. For testing of directionality, the asymptotic distribution is the minimum of d independent chi-squared variables with one-degree of freedom or a generalized Gamma distribution depending on if d is small, where d is number of breakpoints in a hypothesized pathway. Moreover, we develop a computational method to perform the proposed tests, which integrates an alternating direction method of multipliers and difference convex programming. Finally, the power analysis and simulations suggest that the tests achieve the desired objectives of inference. An analysis of an Alzheimer’s disease gene expression dataset illustrates the utility of the proposed method to infer a directed pathway in a gene network.},
  archive      = {J_JASA},
  author       = {Chunlin Li and Xiaotong Shen and Wei Pan},
  doi          = {10.1080/01621459.2019.1623042},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1304-1319},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Likelihood ratio tests for a large directed acyclic graph},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Main effects and interactions in mixed and incomplete data
frames. <em>JASA</em>, <em>115</em>(531), 1292–1303. (<a
href="https://doi.org/10.1080/01621459.2019.1623041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A mixed data frame (MDF) is a table collecting categorical, numerical, and count observations. The use of MDF is widespread in statistics and the applications are numerous from abundance data in ecology to recommender systems. In many cases, an MDF exhibits simultaneously main effects , such as row, column, or group effects and interactions , for which a low-rank model has often been suggested. Although the literature on low-rank approximations is very substantial, with few exceptions, existing methods do not allow to incorporate main effects and interactions while providing statistical guarantees. The present work fills this gap. We propose an estimation method which allows to recover simultaneously the main effects and the interactions. We show that our method is near optimal under conditions which are met in our targeted applications. We also propose an optimization algorithm which provably converges to an optimal solution. Numerical experiments reveal that our method, mimi, performs well when the main effects are sparse and the interaction matrix has low-rank. We also show that mimi compares favorably to existing methods, in particular when the main effects are significantly large compared to the interactions, and when the proportion of missing entries is large. The method is available as an R package on the Comprehensive R Archive Network. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Geneviève Robin and Olga Klopp and Julie Josse and Éric Moulines and Robert Tibshirani},
  doi          = {10.1080/01621459.2019.1623041},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1292-1303},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Main effects and interactions in mixed and incomplete data frames},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simultaneous covariance inference for multimodal integrative
analysis. <em>JASA</em>, <em>115</em>(531), 1279–1291. (<a
href="https://doi.org/10.1080/01621459.2019.1623040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal integrative analysis fuses different types of data collected on the same set of experimental subjects. It is becoming a norm in many branches of scientific research, such as multi-omics and multimodal neuroimaging analysis. In this article, we address the problem of simultaneous covariance inference of associations between multiple modalities, which is of a vital interest in multimodal integrative analysis. Recognizing that there are few readily available solutions in the literature for this type of problem, we develop a new simultaneous testing procedure. It provides an explicit quantification of statistical significance, a much improved detection power, as well as a rigid false discovery control. Our proposal makes novel and useful contributions from both the scientific perspective and the statistical methodological perspective. We demonstrate the efficacy of the new method through both simulations and a multimodal positron emission tomography study of associations between two hallmark pathological proteins of Alzheimer’s disease.},
  archive      = {J_JASA},
  author       = {Yin Xia and Lexin Li and Samuel N. Lockhart and William J. Jagust},
  doi          = {10.1080/01621459.2019.1623040},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1279-1291},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Simultaneous covariance inference for multimodal integrative analysis},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparametric inference for copulas and measures of
dependence under length-biased sampling and informative censoring.
<em>JASA</em>, <em>115</em>(531), 1268–1278. (<a
href="https://doi.org/10.1080/01621459.2019.1611586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Length-biased data are often encountered in cross-sectional surveys and prevalent-cohort studies on disease durations. Under length-biased sampling subjects with longer disease durations have greater chance to be observed. As a result, covariate values linked to the longer survivors are favored by the sampling mechanism. When the sampled durations are also subject to right censoring, the censoring is informative. Modeling dependence structure without adjusting for these issues leads to biased results. In this article, we consider copulas for modeling dependence when the collected data are length-biased and account for both informative censoring and covariate bias that are naturally linked to length-biased sampling. We address nonparametric estimation of the bivariate distribution, copula function and its density, and Kendall and Spearman measures for right-censored length-biased data. The proposed estimator for the bivariate cdf is a Hadamard-differentiable functional of two MLEs (Kaplan–Meier and empirical cdf) and inherits their efficiency. Based on this estimator, we devise two estimators for copula function and a local-polynomial estimator for copula density that accounts for boundary bias. The limiting processes of the estimators are established by deriving their iid representations. As a by-product, we establish the oscillation behavior of the bivariate cdf estimator. In addition, we introduce estimators for Kendall and Spearman measures and study their weak convergence. The proposed method is applied to analyze a set of right-censored length-biased data on survival with dementia, collected as part of a nationwide study in Canada.},
  archive      = {J_JASA},
  author       = {Yassir Rabhi and Taoufik Bouezmarni},
  doi          = {10.1080/01621459.2019.1611586},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1268-1278},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Nonparametric inference for copulas and measures of dependence under length-biased sampling and informative censoring},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). IFusion: Individualized fusion learning. <em>JASA</em>,
<em>115</em>(531), 1251–1267. (<a
href="https://doi.org/10.1080/01621459.2019.1672557">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inferences from different data sources can often be fused together, a process referred to as “fusion learning,” to yield more powerful findings than those from individual data sources alone. Effective fusion learning approaches are in growing demand as increasing number of data sources have become easily available in this big data era. This article proposes a new fusion learning approach, called “ i Fusion,” for drawing efficient individualized inference by fusing learnings from relevant data sources. Specifically, i Fusion (i) summarizes inferences from individual data sources as individual confidence distributions (CDs); (ii) forms a clique of individuals that bear relevance to the target individual and then combines the CDs from those relevant individuals; and, finally, (iii) draws inference for the target individual from the combined CD. In essence, i Fusion strategically “borrows strength” from relevant individuals to enhance the efficiency of the target individual inference while preserving its validity. This article focuses on the setting where each individual study has a number of observations but its inference can be further improved by incorporating additional information from similar studies that is referred to as its clique . Under the setting, i Fusion is shown to achieve oracle property under suitable conditions. It is also shown to be flexible and robust in handling heterogeneity arising from diverse data sources. The development is ideally suited for goal-directed applications. Computationally, i Fusion is parallel in nature and scales up easily for big data. An efficient scalable algorithm is provided for implementation. Simulation studies and a real application in financial forecasting are presented. In effect, this article covers methodology, theory, computation, and application for individualized inference by i Fusion.},
  archive      = {J_JASA},
  author       = {Jieli Shen and Regina Y. Liu and Min-ge Xie},
  doi          = {10.1080/01621459.2019.1672557},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1251-1267},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {IFusion: Individualized fusion learning},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GAP: A general framework for information pooling in
two-sample sparse inference. <em>JASA</em>, <em>115</em>(531),
1236–1250. (<a
href="https://doi.org/10.1080/01621459.2019.1611585">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops a general framework for exploiting the sparsity information in two-sample multiple testing problems. We propose to first construct a covariate sequence, in addition to the usual primary test statistics, to capture the sparsity structure, and then incorporate the auxiliary covariates in inference via a three-step algorithm consisting of grouping, adjusting and pooling (GAP). The GAP procedure provides a simple and effective framework for information pooling. An important advantage of GAP is its capability of handling various dependence structures such as those arise from high-dimensional linear regression, differential correlation analysis, and differential network analysis. We establish general conditions under which GAP is asymptotically valid for false discovery rate control, and show that these conditions are fulfilled in a range of settings, including testing multivariate normal means, high-dimensional linear regression, differential covariance or correlation matrices, and Gaussian graphical models. Numerical results demonstrate that existing methods can be significantly improved by the proposed framework. The GAP procedure is illustrated using a breast cancer study for identifying gene–gene interactions.},
  archive      = {J_JASA},
  author       = {Yin Xia and T. Tony Cai and Wenguang Sun},
  doi          = {10.1080/01621459.2019.1611585},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1236-1250},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {GAP: A general framework for information pooling in two-sample sparse inference},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feature selection by canonical correlation search in
high-dimensional multiresponse models with complex group structures.
<em>JASA</em>, <em>115</em>(531), 1227–1235. (<a
href="https://doi.org/10.1080/01621459.2019.1609972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional multiresponse models with complex group structures in both the response variables and the covariates arise from current researches in important fields such as genetics and medicine. However, no enough research has been done on such models. One of a few researches, if not the only one, is the article by Li, Nan, and Zhu where the sparse group Lasso approach is extended to such models. In this article, we propose a novel approach named the sequential canonical correlation search (SCCS) procedure. In the SCCS procedure, the nonzero group by group blocks of regression coefficients are searched stepwise using a canonical correlation measure. Each step of the procedure consists of a block selection and a sparsity identification. The model selection criterion, EBIC, is used as the stopping rule of the procedure. We establish the selection consistency of the SCCS procedure and conduct simulation studies for the comparison of existing methods. The SCCS procedure has two advantages over the sparse grouped Lasso method: (i) it is more accurate in the identification of nonzero coefficient blocks and their nonzero entries, and (ii) its implementation is not limited by the dimensionality of the models and requires much less computation. A real example in genetic studies is also considered. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Shan Luo and Zehua Chen},
  doi          = {10.1080/01621459.2019.1609972},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1227-1235},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Feature selection by canonical correlation search in high-dimensional multiresponse models with complex group structures},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Testing for jump spillovers without testing for jumps.
<em>JASA</em>, <em>115</em>(531), 1214–1226. (<a
href="https://doi.org/10.1080/01621459.2019.1609971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article develops statistical tools for testing conditional independence among the jump components of the daily quadratic variation, which we estimate using intraday data. To avoid sequential bias distortion, we do not pretest for the presence of jumps. If the null is true, our test statistic based on daily integrated jumps weakly converges to a Gaussian random variable if both assets have jumps. If instead at least one asset has no jumps, then the statistic approaches zero in probability. We show how to compute asymptotically valid bootstrap-based critical values that result in a consistent test with asymptotic size equal to or smaller than the nominal size. Empirically, we study jump linkages between US futures and equity index markets. We find not only strong evidence of jump cross-excitation between the SPDR exchange-traded fund and E-mini futures on the S&amp;P 500 index, but also that integrated jumps in the E-mini futures during the overnight period carry relevant information. Supplementary materials for this article are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Valentina Corradi and Walter Distaso and Marcelo Fernandes},
  doi          = {10.1080/01621459.2019.1609971},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1214-1226},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Testing for jump spillovers without testing for jumps},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A sparse random projection-based test for overall
qualitative treatment effects. <em>JASA</em>, <em>115</em>(531),
1201–1213. (<a
href="https://doi.org/10.1080/01621459.2019.1604368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In contrast to the classical “one-size-fits-all” approach, precision medicine proposes the customization of individualized treatment regimes to account for patients’ heterogeneity in response to treatments. Most of existing works in the literature focused on estimating optimal individualized treatment regimes. However, there has been less attention devoted to hypothesis testing regarding the existence of overall qualitative treatment effects, especially when there are a large number of prognostic covariates. When covariates do not have qualitative treatment effects, the optimal treatment regime will assign the same treatment to all patients regardless of their covariate values. In this article, we consider testing the overall qualitative treatment effects of patients’ prognostic covariates in a high-dimensional setting. We propose a sample splitting method to construct the test statistic, based on a nonparametric estimator of the contrast function. When the dimension of covariates is large, we construct the test based on sparse random projections of covariates into a low-dimensional space. We prove the consistency of our test statistic. In the regular cases, we show the asymptotic power function of our test statistic is asymptotically the same as the “oracle” test statistic which is constructed based on the “optimal” projection matrix. Simulation studies and real data applications validate our theoretical findings. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Chengchun Shi and Wenbin Lu and Rui Song},
  doi          = {10.1080/01621459.2019.1604368},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1201-1213},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A sparse random projection-based test for overall qualitative treatment effects},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distance-based analysis of ordinal data and ordinal time
series. <em>JASA</em>, <em>115</em>(531), 1189–1200. (<a
href="https://doi.org/10.1080/01621459.2019.1604370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dissimilarity of ordinal categories can be expressed with a distance measure. A unified approach relying on expected distances is proposed to obtain well-interpretable measures of location, dispersion, or symmetry of random variables, as well as measures of serial dependence within a given process. For special types of distance, these analytic tools lead to known approaches for ordinal or real-valued random variables. We also analyze the sample counterparts of the proposed measures and derive asymptotic results for practically important cases in ordinal data and time series analysis. Two real applications about the economic situation in Germany and the credit rating of European countries are presented. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Christian H. Weiß},
  doi          = {10.1080/01621459.2019.1604370},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1189-1200},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Distance-based analysis of ordinal data and ordinal time series},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Panel data analysis via mechanistic models. <em>JASA</em>,
<em>115</em>(531), 1178–1188. (<a
href="https://doi.org/10.1080/01621459.2019.1604367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Panel data, also known as longitudinal data, consist of a collection of time series. Each time series, which could itself be multivariate, comprises a sequence of measurements taken on a distinct unit. Mechanistic modeling involves writing down scientifically motivated equations describing the collection of dynamic systems giving rise to the observations on each unit. A defining characteristic of panel systems is that the dynamic interaction between units should be negligible. Panel models therefore consist of a collection of independent stochastic processes, generally linked through shared parameters while also having unit-specific parameters. To give the scientist flexibility in model specification, we are motivated to develop a framework for inference on panel data permitting the consideration of arbitrary nonlinear, partially observed panel models. We build on iterated filtering techniques that provide likelihood-based inference on nonlinear partially observed Markov process models for time series data. Our methodology depends on the latent Markov process only through simulation; this plug-and-play property ensures applicability to a large class of models. We demonstrate our methodology on a toy example and two epidemiological case studies. We address inferential and computational issues arising due to the combination of model complexity and dataset size. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Carles Bretó and Edward L. Ionides and Aaron A. King},
  doi          = {10.1080/01621459.2019.1604367},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1178-1188},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Panel data analysis via mechanistic models},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Template independent component analysis: Targeted and
reliable estimation of subject-level brain networks using big data
population priors. <em>JASA</em>, <em>115</em>(531), 1151–1177. (<a
href="https://doi.org/10.1080/01621459.2019.1679638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large brain imaging databases contain a wealth of information on brain organization in the populations they target, and on individual variability. While such databases have been used to study group-level features of populations directly, they are currently underutilized as a resource to inform single-subject analysis. Here, we propose leveraging the information contained in large functional magnetic resonance imaging (fMRI) databases by establishing population priors to employ in an empirical Bayesian framework. We focus on estimation of brain networks as source signals in independent component analysis (ICA). We formulate a hierarchical “template” ICA model where source signals—including known population brain networks and subject-specific signals—are represented as latent variables. For estimation, we derive an expectation–maximization (EM) algorithm having an explicit solution. However, as this solution is computationally intractable, we also consider an approximate subspace algorithm and a faster two-stage approach. Through extensive simulation studies, we assess performance of both methods and compare with dual regression, a popular but ad-hoc method. The two proposed algorithms have similar performance, and both dramatically outperform dual regression. We also conduct a reliability study utilizing the Human Connectome Project and find that template ICA achieves substantially better performance than dual regression, achieving 75–250\% higher intra-subject reliability. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Amanda F. Mejia and Mary Beth Nebel and Yikai Wang and Brian S. Caffo and Ying Guo},
  doi          = {10.1080/01621459.2019.1679638},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1151-1177},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Template independent component analysis: Targeted and reliable estimation of subject-level brain networks using big data population priors},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Predicting clinical outcomes in glioblastoma: An application
of topological and functional data analysis. <em>JASA</em>,
<em>115</em>(531), 1139–1150. (<a
href="https://doi.org/10.1080/01621459.2019.1671198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Glioblastoma multiforme (GBM) is an aggressive form of human brain cancer that is under active study in the field of cancer biology. Its rapid progression and the relative time cost of obtaining molecular data make other readily available forms of data, such as images, an important resource for actionable measures in patients. Our goal is to use information given by medical images taken from GBM patients in statistical settings. To do this, we design a novel statistic—the smooth Euler characteristic transform (SECT)—that quantifies magnetic resonance images of tumors. Due to its well-defined inner product structure, the SECT can be used in a wider range of functional and nonparametric modeling approaches than other previously proposed topological summary statistics. When applied to a cohort of GBM patients, we find that the SECT is a better predictor of clinical outcomes than both existing tumor shape quantifications and common molecular assays. Specifically, we demonstrate that SECT features alone explain more of the variance in GBM patient survival than gene expression, volumetric features, and morphometric features. The main takeaways from our findings are thus 2-fold. First, they suggest that images contain valuable information that can play an important role in clinical prognosis and other medical decisions. Second, they show that the SECT is a viable tool for the broader study of medical imaging informatics. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Lorin Crawford and Anthea Monod and Andrew X. Chen and Sayan Mukherjee and Raúl Rabadán},
  doi          = {10.1080/01621459.2019.1671198},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1139-1150},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Predicting clinical outcomes in glioblastoma: An application of topological and functional data analysis},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling between-study heterogeneity for improved
replicability in gene signature selection and clinical prediction.
<em>JASA</em>, <em>115</em>(531), 1125–1138. (<a
href="https://doi.org/10.1080/01621459.2019.1671197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the genomic era, the identification of gene signatures associated with disease is of significant interest. Such signatures are often used to predict clinical outcomes in new patients and aid clinical decision-making. However, recent studies have shown that gene signatures are often not replicable. This occurrence has practical implications regarding the generalizability and clinical applicability of such signatures. To improve replicability, we introduce a novel approach to select gene signatures from multiple datasets whose effects are consistently nonzero and account for between-study heterogeneity. We build our model upon some rank-based quantities, facilitating integration over different genomic datasets. A high-dimensional penalized generalized linear mixed model is used to select gene signatures and address data heterogeneity. We compare our method to some commonly used strategies that select gene signatures ignoring between-study heterogeneity. We provide asymptotic results justifying the performance of our method and demonstrate its advantage in the presence of heterogeneity through thorough simulation studies. Lastly, we motivate our method through a case study subtyping pancreatic cancer patients from four gene expression studies. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Naim U. Rashid and Quefeng Li and Jen Jen Yeh and Joseph G. Ibrahim},
  doi          = {10.1080/01621459.2019.1671197},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1125-1138},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Modeling between-study heterogeneity for improved replicability in gene signature selection and clinical prediction},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fine-scale spatiotemporal air pollution analysis using
mobile monitors on google street view vehicles. <em>JASA</em>,
<em>115</em>(531), 1111–1124. (<a
href="https://doi.org/10.1080/01621459.2019.1665526">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People are increasingly concerned with understanding their personal environment, including possible exposure to harmful air pollutants. To make informed decisions on their day-to-day activities, they are interested in real-time information on a localized scale. Publicly available, fine-scale, high-quality air pollution measurements acquired using mobile monitors represent a paradigm shift in measurement technologies. A methodological framework utilizing these increasingly fine-scale measurements to provide real-time air pollution maps and short-term air quality forecasts on a fine-resolution spatial scale could prove to be instrumental in increasing public awareness and understanding. The Google Street View study provides a unique source of data with spatial and temporal complexities, with the potential to provide information about commuter exposure and hot spots within city streets with high traffic. We develop a computationally efficient spatiotemporal model for these data and use the model to make short-term forecasts and high-resolution maps of current air pollution levels. We also show via an experiment that mobile networks can provide more nuanced information than an equally sized fixed-location network. This modeling framework has important real-world implications in understanding citizens’ personal environments, as data production and real-time availability continue to be driven by the ongoing development and improvement of mobile measurement technologies. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Yawen Guan and Margaret C. Johnson and Matthias Katzfuss and Elizabeth Mannshardt and Kyle P. Messier and Brian J. Reich and Joon J. Song},
  doi          = {10.1080/01621459.2019.1665526},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1111-1124},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Fine-scale spatiotemporal air pollution analysis using mobile monitors on google street view vehicles},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multivariate bayesian predictive synthesis in macroeconomic
forecasting. <em>JASA</em>, <em>115</em>(531), 1092–1110. (<a
href="https://doi.org/10.1080/01621459.2019.1660171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present new methodology and a case study in use of a class of Bayesian predictive synthesis (BPS) models for multivariate time series forecasting. This extends the foundational BPS framework to the multivariate setting, with detailed application in the topical and challenging context of multistep macroeconomic forecasting in a monetary policy setting. BPS evaluates—sequentially and adaptively over time—varying forecast biases and facets of miscalibration of individual forecast densities for multiple time series, and—critically—their time-varying inter-dependencies. We define BPS methodology for a new class of dynamic multivariate latent factor models implied by BPS theory. Structured dynamic latent factor BPS is here motivated by the application context—sequential forecasting of multiple U.S. macroeconomic time series with forecasts generated from several traditional econometric time series models. The case study highlights the potential of BPS to improve of forecasts of multiple series at multiple forecast horizons, and its use in learning dynamic relationships among forecasting models or agents. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Kenichiro McAlinn and Knut Are Aastveit and Jouchi Nakajima and Mike West},
  doi          = {10.1080/01621459.2019.1660171},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1092-1110},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Multivariate bayesian predictive synthesis in macroeconomic forecasting},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Genetic variant set-based tests using the generalized
berk–jones statistic with application to a genome-wide association study
of breast cancer. <em>JASA</em>, <em>115</em>(531), 1079–1091. (<a
href="https://doi.org/10.1080/01621459.2019.1660170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Studying the effects of groups of single nucleotide polymorphisms (SNPs), as in a gene, genetic pathway, or network, can provide novel insight into complex diseases such as breast cancer, uncovering new genetic associations and augmenting the information that can be gleaned from studying SNPs individually. Common challenges in set-based genetic association testing include weak effect sizes, correlation between SNPs in a SNP-set, and scarcity of signals, with individual SNP effects often ranging from extremely sparse to moderately sparse in number. Motivated by these challenges, we propose the Generalized Berk–Jones (GBJ) test for the association between a SNP-set and outcome. The GBJ extends the Berk–Jones statistic by accounting for correlation among SNPs, and it provides advantages over the Generalized Higher Criticism test when signals in a SNP-set are moderately sparse. We also provide an analytic p -value calculation for SNP-sets of any finite size, and we develop an omnibus statistic that is robust to the degree of signal sparsity. An additional advantage of our work is the ability to conduct inference using individual SNP summary statistics from a genome-wide association study (GWAS). We evaluate the finite sample performance of the GBJ through simulation and apply the method to identify breast cancer risk genes in a GWAS conducted by the Cancer Genetic Markers of Susceptibility Consortium. Our results suggest evidence of association between FGFR2 and breast cancer and also identify other potential susceptibility genes, complementing conventional SNP-level analysis. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Ryan Sun and Xihong Lin},
  doi          = {10.1080/01621459.2019.1660170},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1079-1091},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Genetic variant set-based tests using the generalized Berk–Jones statistic with application to a genome-wide association study of breast cancer},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian nonparametric policy search with application to
periodontal recall intervals. <em>JASA</em>, <em>115</em>(531),
1066–1078. (<a
href="https://doi.org/10.1080/01621459.2019.1660169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tooth loss from periodontal disease is a major public health burden in the United States. Standard clinical practice is to recommend a dental visit every six months; however, this practice is not evidence-based, and poor dental outcomes and increasing dental insurance premiums indicate room for improvement. We consider a tailored approach that recommends recall time based on patient characteristics and medical history to minimize disease progression without increasing resource expenditures. We formalize this method as a dynamic treatment regime which comprises a sequence of decisions, one per stage of intervention, that follow a decision rule which maps current patient information to a recommendation for their next visit time. The dynamics of periodontal health, visit frequency, and patient compliance are complex, yet the estimated optimal regime must be interpretable to domain experts if it is to be integrated into clinical practice. We combine nonparametric Bayesian dynamics modeling with policy-search algorithms to estimate the optimal dynamic treatment regime within an interpretable class of regimes. Both simulation experiments and application to a rich database of electronic dental records from the HealthPartners HMO shows that our proposed method leads to better dental health without increasing the average recommended recall time relative to competing methods. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Qian Guan and Brian J. Reich and Eric B. Laber and Dipankar Bandyopadhyay},
  doi          = {10.1080/01621459.2019.1660169},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1066-1078},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian nonparametric policy search with application to periodontal recall intervals},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ICeD-t provides accurate estimates of immune cell abundance
in tumor samples by allowing for aberrant gene expression patterns.
<em>JASA</em>, <em>115</em>(531), 1055–1065. (<a
href="https://doi.org/10.1080/01621459.2019.1654874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immunotherapies have attracted lots of research interests recently. The need to understand the underlying mechanisms of immunotherapies and to develop precision immunotherapy regimens has spurred great interest in characterizing immune cell composition within the tumor microenvironment. Several methods have been developed to estimate immune cell composition using gene expression data from bulk tumor samples. However, these methods are not flexible enough to handle aberrant patterns of gene expression data, for example, inconsistent cell type-specific gene expression between purified reference samples and tumor samples. We propose a novel statistical method for expression deconvolution called immune cell deconvolution in tumor tissues (ICeD-T). ICeD-T automatically identifies aberrant genes whose expression are inconsistent with the deconvolution model and down-weights their contributions to cell type abundance estimates. We evaluated the performance of ICeD-T versus existing methods in simulation studies and several real data analyses. ICeD-T displayed comparable or superior performance to these competing methods. Applying these methods to assess the relationship between immunotherapy response and immune cell composition, ICeD-T is able to identify significant associations that are missed by its competitors. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Douglas R. Wilson and Chong Jin and Joseph G. Ibrahim and Wei Sun},
  doi          = {10.1080/01621459.2019.1654874},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1055-1065},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {ICeD-T provides accurate estimates of immune cell abundance in tumor samples by allowing for aberrant gene expression patterns},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local likelihood estimation of complex tail dependence
structures, applied to u.s. Precipitation extremes. <em>JASA</em>,
<em>115</em>(531), 1037–1054. (<a
href="https://doi.org/10.1080/01621459.2019.1647842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To disentangle the complex nonstationary dependence structure of precipitation extremes over the entire contiguous United States (U.S.), we propose a flexible local approach based on factor copula models. Our subasymptotic spatial modeling framework yields nontrivial tail dependence structures, with a weakening dependence strength as events become more extreme; a feature commonly observed with precipitation data but not accounted for in classical asymptotic extreme-value models. To estimate the local extremal behavior, we fit the proposed model in small regional neighborhoods to high threshold exceedances, under the assumption of local stationarity, which allows us to gain in flexibility. By adopting a local censored likelihood approach, we make inference on a fine spatial grid, and we perform local estimation by taking advantage of distributed computing resources and the embarrassingly parallel nature of this estimation procedure. The local model is efficiently fitted at all grid points, and uncertainty is measured using a block bootstrap procedure. We carry out an extensive simulation study to show that our approach can adequately capture complex, nonstationary dependencies, in addition, our study of U.S. winter precipitation data reveals interesting differences in local tail structures over space, which has important implications on regional risk assessment of extreme precipitation events. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Daniela Castro-Camilo and Raphaël Huser},
  doi          = {10.1080/01621459.2019.1647842},
  journal      = {Journal of the American Statistical Association},
  number       = {531},
  pages        = {1037-1054},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Local likelihood estimation of complex tail dependence structures, applied to U.S. precipitation extremes},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Correction. <em>JASA</em>, <em>115</em>(530), 1035–1036. (<a
href="https://doi.org/10.1080/01621459.2020.1724472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  doi          = {10.1080/01621459.2020.1724472},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {1035-1036},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Correction},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Theory of spatial statistics: A concise introduction.
<em>JASA</em>, <em>115</em>(530), 1033–1034. (<a
href="https://doi.org/10.1080/01621459.2020.1759991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Frederic P. Schoenberg},
  doi          = {10.1080/01621459.2020.1759991},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {1033-1034},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Theory of spatial statistics: A concise introduction},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Statistical modelling by exponential families.
<em>JASA</em>, <em>115</em>(530), 1032. (<a
href="https://doi.org/10.1080/01621459.2020.1759989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Yen-Chi Chen},
  doi          = {10.1080/01621459.2020.1759989},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {1032},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistical modelling by exponential families},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sufficient dimension reduction: Methods and applications
with r. <em>JASA</em>, <em>115</em>(530), 1032–1033. (<a
href="https://doi.org/10.1080/01621459.2020.1759990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Daniel J. McDonald},
  doi          = {10.1080/01621459.2020.1759990},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {1032-1033},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Sufficient dimension reduction: Methods and applications with r},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model-based geostatistics for global public health: Methods
and applications. <em>JASA</em>, <em>115</em>(530), 1030–1032. (<a
href="https://doi.org/10.1080/01621459.2020.1759988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Ian Laga and Xiaoyue Niu},
  doi          = {10.1080/01621459.2020.1759988},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {1030-1032},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Model-based geostatistics for global public health: Methods and applications},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Innovative strategies, statistical solutions and simulations
for modern clinical trials. <em>JASA</em>, <em>115</em>(530), 1029–1030.
(<a href="https://doi.org/10.1080/01621459.2020.1759987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Ionut Bebu},
  doi          = {10.1080/01621459.2020.1759987},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {1029-1030},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Innovative strategies, statistical solutions and simulations for modern clinical trials},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Empirical likelihood methods in biomedicine and health.
<em>JASA</em>, <em>115</em>(530), 1028–1029. (<a
href="https://doi.org/10.1080/01621459.2020.1759986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Yichuan Zhao},
  doi          = {10.1080/01621459.2020.1759986},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {1028-1029},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Empirical likelihood methods in biomedicine and health},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A simple two-sample test in high dimensions based on
l2-norm. <em>JASA</em>, <em>115</em>(530), 1011–1027. (<a
href="https://doi.org/10.1080/01621459.2019.1604366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing the equality of two means is a fundamental inference problem. For high-dimensional data, the Hotelling’s T 2 -test either performs poorly or becomes inapplicable. Several modifications have been proposed to address this issue. However, most of them are based on asymptotic normality of the null distributions of their test statistics which inevitably requires strong assumptions on the covariance. We study this problem thoroughly and propose an L 2 -norm based test that works under mild conditions and even when there are fewer observations than the dimension. Specially, to cope with general nonnormality of the null distribution we employ the Welch–Satterthwaite χ 2 -approximation. We derive a sharp upper bound on the approximation error and use it to justify that χ 2 -approximation is preferred to normal approximation. Simple ratio-consistent estimators for the parameters in the χ 2 -approximation are given. Importantly, our test can cope with singularity or near singularity of the covariance which is commonly seen in high dimensions and is the main cause of nonnormality. The power of the proposed test is also investigated. Extensive simulation studies and an application show that our test is at least comparable to and often outperforms several competitors in terms of size control, and the powers are comparable when their sizes are. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jin-Ting Zhang and Jia Guo and Bu Zhou and Ming-Yen Cheng},
  doi          = {10.1080/01621459.2019.1604366},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {1011-1027},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A simple two-sample test in high dimensions based on l2-norm},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Additive functional regression for densities as responses.
<em>JASA</em>, <em>115</em>(530), 997–1010. (<a
href="https://doi.org/10.1080/01621459.2019.1604365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose and investigate additive density regression, a novel additive functional regression model for situations where the responses are random distributions that can be viewed as random densities and the predictors are vectors. Data in the form of samples of densities or distributions are increasingly encountered in statistical analysis and there is a need for flexible regression models that accommodate random densities as responses. Such models are of special interest for multivariate continuous predictors, where unrestricted nonparametric regression approaches are subject to the curse of dimensionality. Additive models can be expected to maintain one-dimensional rates of convergence while permitting a substantial degree of flexibility. This motivates the development of additive regression models for situations where multivariate continuous predictors are coupled with densities as responses. To overcome the problem that distributions do not form a vector space, we utilize a class of transformations that map densities to unrestricted square integrable functions and then deploy an additive functional regression model to fit the responses in the unrestricted space, finally transforming back to density space. We implement the proposed additive model with an extended version of smooth backfitting and establish the consistency of this approach, including rates of convergence. The proposed method is illustrated with an application to the distributions of baby names in the United States.},
  archive      = {J_JASA},
  author       = {Kyunghee Han and Hans-Georg Müller and Byeong U. Park},
  doi          = {10.1080/01621459.2019.1604365},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {997-1010},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Additive functional regression for densities as responses},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Expected conditional characteristic function-based measures
for testing independence. <em>JASA</em>, <em>115</em>(530), 985–996. (<a
href="https://doi.org/10.1080/01621459.2019.1604364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel class of independence measures for testing independence between two random vectors based on the discrepancy between the conditional and the marginal characteristic functions. The relation between our index and other similar measures is studied, which indicates that they all belong to a large framework of reproducing kernel Hilbert space. If one of the variables is categorical, our asymmetric index extends the typical ANOVA to a kernel ANOVA that can test a more general hypothesis of equal distributions among groups. In addition, our index is also applicable when both variables are continuous. We develop two empirical estimates and obtain their respective asymptotic distributions. We illustrate the advantages of our approach by numerical studies across a variety of settings including a real data example. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Chenlu Ke and Xiangrong Yin},
  doi          = {10.1080/01621459.2019.1604364},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {985-996},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Expected conditional characteristic function-based measures for testing independence},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parsimonious model averaging with a diverging number of
parameters. <em>JASA</em>, <em>115</em>(530), 972–984. (<a
href="https://doi.org/10.1080/01621459.2019.1604363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model averaging generally provides better predictions than model selection, but the existing model averaging methods cannot lead to parsimonious models. Parsimony is an especially important property when the number of parameters is large. To achieve a parsimonious model averaging coefficient estimator, we suggest a novel criterion for choosing weights. Asymptotic properties are derived in two practical scenarios: (i) one or more correct models exist in the candidate model set and (ii) all candidate models are misspecified. Under the former scenario, it is proved that our method can put the weight one to the smallest correct model and the resulting model averaging estimators of coefficients have many zeros and thus lead to a parsimonious model. The asymptotic distribution of the estimators is also provided. Under the latter scenario, prediction is mainly focused on and we prove that the proposed procedure is asymptotically optimal in the sense that its squared prediction loss and risk are asymptotically identical to those of the best—but infeasible—model averaging estimator. Numerical analysis shows the promise of the proposed procedure over existing model averaging and selection methods.},
  archive      = {J_JASA},
  author       = {Xinyu Zhang and Guohua Zou and Hua Liang and Raymond J. Carroll},
  doi          = {10.1080/01621459.2019.1604363},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {972-984},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Parsimonious model averaging with a diverging number of parameters},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Long-range dependent curve time series. <em>JASA</em>,
<em>115</em>(530), 957–971. (<a
href="https://doi.org/10.1080/01621459.2019.1604362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce methods and theory for functional or curve time series with long-range dependence. The temporal sum of the curve process is shown to be asymptotically normally distributed, the conditions for this covering a functional version of fractionally integrated autoregressive moving averages. We also construct an estimate of the long-run covariance function, which we use, via functional principal component analysis, in estimating the orthonormal functions spanning the dominant subspace of the curves. In a semiparametric context, we propose an estimate of the memory parameter and establish its consistency. A Monte Carlo study of finite-sample performance is included, along with two empirical applications. The first of these finds a degree of stability and persistence in intraday stock returns. The second finds similarity in the extent of long memory in incremental age-specific fertility rates across some developed nations. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Degui Li and Peter M. Robinson and Han Lin Shang},
  doi          = {10.1080/01621459.2019.1604362},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {957-971},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Long-range dependent curve time series},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). A generalized gaussian process model for computer
experiments with binary time series. <em>JASA</em>, <em>115</em>(530),
945–956. (<a
href="https://doi.org/10.1080/01621459.2019.1604361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-Gaussian observations such as binary responses are common in some computer experiments. Motivated by the analysis of a class of cell adhesion experiments, we introduce a generalized Gaussian process model for binary responses, which shares some common features with standard GP models. In addition, the proposed model incorporates a flexible mean function that can capture different types of time series structures. Asymptotic properties of the estimators are derived, and an optimal predictor as well as its predictive distribution are constructed. Their performance is examined via two simulation studies. The methodology is applied to study computer simulations for cell adhesion experiments. The fitted model reveals important biological information in repeated cell bindings, which is not directly observable in lab experiments. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Chih-Li Sung and Ying Hung and William Rittase and Cheng Zhu and C. F. Jeff Wu},
  doi          = {10.1080/01621459.2019.1604361},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {945-956},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A generalized gaussian process model for computer experiments with binary time series},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Functional censored quantile regression. <em>JASA</em>,
<em>115</em>(530), 931–944. (<a
href="https://doi.org/10.1080/01621459.2019.1602047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a functional censored quantile regression model to describe the time-varying relationship between time-to-event outcomes and corresponding functional covariates. The time-varying effect is modeled as an unspecified function that is approximated via B-splines. A generalized approximate cross-validation method is developed to select the number of knots by minimizing the expected loss. We establish asymptotic properties of the method and the knot selection procedure. Furthermore, we conduct extensive simulation studies to evaluate the finite sample performance of our method. Finally, we analyze the functional relationship between ambulatory blood pressure trajectories and clinical outcome in stroke patients. The results reinforce the importance of the morning blood pressure surge phenomenon, whose effect has caught attention but remains controversial in the medical literature. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Fei Jiang and Qing Cheng and Guosheng Yin and Haipeng Shen},
  doi          = {10.1080/01621459.2019.1602047},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {931-944},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Functional censored quantile regression},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On prediction properties of kriging: Uniform error bounds
and robustness. <em>JASA</em>, <em>115</em>(530), 920–930. (<a
href="https://doi.org/10.1080/01621459.2019.1598868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kriging based on Gaussian random fields is widely used in reconstructing unknown functions. The kriging method has pointwise predictive distributions which are computationally simple. However, in many applications one would like to predict for a range of untried points simultaneously. In this work, we obtain some error bounds for the simple and universal kriging predictor under the uniform metric. It works for a scattered set of input points in an arbitrary dimension, and also covers the case where the covariance function of the Gaussian process is misspecified. These results lead to a better understanding of the rate of convergence of kriging under the Gaussian or the Matérn correlation functions, the relationship between space-filling designs and kriging models, and the robustness of the Matérn correlation functions. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Wenjia Wang and Rui Tuo and C. F. Jeff Wu},
  doi          = {10.1080/01621459.2019.1598868},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {920-930},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {On prediction properties of kriging: Uniform error bounds and robustness},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiresolution functional ANOVA for large-scale, many-input
computer experiments. <em>JASA</em>, <em>115</em>(530), 908–919. (<a
href="https://doi.org/10.1080/01621459.2019.1595630">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Gaussian process is a standard tool for building emulators for both deterministic and stochastic computer experiments. However, application of Gaussian process models is greatly limited in practice, particularly for large-scale and many-input computer experiments that have become typical. We propose a multiresolution functional ANOVA (MRFA) model as a computationally feasible emulation alternative. More generally, this model can be used for large-scale and many-input nonlinear regression problems. An overlapping group lasso approach is used for estimation, ensuring computational feasibility in a large-scale and many-input setting. New results on consistency and inference for the (potentially overlapping) group lasso in a high-dimensional setting are developed and applied to the proposed MRFA model. Importantly, these results allow us to quantify the uncertainty in our predictions. Numerical examples demonstrate that the proposed model enjoys marked computational advantages. Data capabilities, in terms of both sample size and dimension, meet or exceed best available emulation tools while meeting or exceeding emulation accuracy. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Chih-Li Sung and Wenjia Wang and Matthew Plumlee and Benjamin Haaland},
  doi          = {10.1080/01621459.2019.1595630},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {908-919},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Multiresolution functional ANOVA for large-scale, many-input computer experiments},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonnegative matrix factorization via archetypal analysis.
<em>JASA</em>, <em>115</em>(530), 896–907. (<a
href="https://doi.org/10.1080/01621459.2019.1594832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a collection of data points, nonnegative matrix factorization (NMF) suggests expressing them as convex combinations of a small set of “archetypes” with nonnegative entries. This decomposition is unique only if the true archetypes are nonnegative and sufficiently sparse (or the weights are sufficiently sparse), a regime that is captured by the separability condition and its generalizations. In this article, we study an approach to NMF that can be traced back to the work of Cutler and Breiman [( Citation 1994 ), “Archetypal Analysis,” Technometrics , 36, 338–347] and does not require the data to be separable, while providing a generally unique decomposition. We optimize a trade-off between two objectives: we minimize the distance of the data points from the convex envelope of the archetypes (which can be interpreted as an empirical risk), while also minimizing the distance of the archetypes from the convex envelope of the data (which can be interpreted as a data-dependent regularization). The archetypal analysis method of Cutler and Breiman is recovered as the limiting case in which the last term is given infinite weight. We introduce a “uniqueness condition” on the data which is necessary for identifiability. We prove that, under uniqueness (plus additional regularity conditions on the geometry of the archetypes), our estimator is robust. While our approach requires solving a nonconvex optimization problem, we find that standard optimization methods succeed in finding good solutions for both real and synthetic data. Supplementary materials for this article are available online},
  archive      = {J_JASA},
  author       = {Hamid Javadi and Andrea Montanari},
  doi          = {10.1080/01621459.2019.1594832},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {896-907},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Nonnegative matrix factorization via archetypal analysis},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Copula link-based additive models for right-censored event
time data. <em>JASA</em>, <em>115</em>(530), 886–895. (<a
href="https://doi.org/10.1080/01621459.2019.1593178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes an approach to estimate and make inference on the parameters of copula link-based survival models. The methodology allows for the margins to be specified using flexible parametric formulations for time-to-event data, the baseline survival functions to be modeled using monotonic splines, and each parameter of the assumed joint survival distribution to depend on an additive predictor incorporating several types of covariate effects. All the model’s coefficients as well as the smoothing parameters associated with the relevant components in the additive predictors are estimated using a carefully structured efficient and stable penalized likelihood algorithm. Some theoretical properties are also discussed. The proposed modeling framework is evaluated in a simulation study and illustrated using a real dataset. The relevant numerical computations can be easily carried out using the freely available GJRM R package. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Giampiero Marra and Rosalba Radice},
  doi          = {10.1080/01621459.2019.1593178},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {886-895},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Copula link-based additive models for right-censored event time data},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ensemble kalman methods for high-dimensional hierarchical
dynamic space-time models. <em>JASA</em>, <em>115</em>(530), 866–885.
(<a href="https://doi.org/10.1080/01621459.2019.1592753">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new class of filtering and smoothing methods for inference in high-dimensional, nonlinear, non-Gaussian, spatio-temporal state-space models. The main idea is to combine the ensemble Kalman filter and smoother, developed in the geophysics literature, with state-space algorithms from the statistics literature. Our algorithms address a variety of estimation scenarios, including online and off-line state and parameter estimation. We take a Bayesian perspective, for which the goal is to generate samples from the joint posterior distribution of states and parameters. The key benefit of our approach is the use of ensemble Kalman methods for dimension reduction, which allows inference for high-dimensional state vectors. We compare our methods to existing ones, including ensemble Kalman filters, particle filters, and particle MCMC. Using a real data example of cloud motion and data simulated under a number of nonlinear and non-Gaussian scenarios, we show that our approaches outperform these existing methods. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Matthias Katzfuss and Jonathan R. Stroud and Christopher K. Wikle},
  doi          = {10.1080/01621459.2019.1592753},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {866-885},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Ensemble kalman methods for high-dimensional hierarchical dynamic space-time models},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Informed proposals for local MCMC in discrete spaces.
<em>JASA</em>, <em>115</em>(530), 852–865. (<a
href="https://doi.org/10.1080/01621459.2019.1585255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a lack of methodological results to design efficient Markov chain Monte Carlo ( MCMC ) algorithms for statistical models with discrete-valued high-dimensional parameters. Motivated by this consideration, we propose a simple framework for the design of informed MCMC proposals (i.e., Metropolis–Hastings proposal distributions that appropriately incorporate local information about the target) which is naturally applicable to discrete spaces. Using Peskun-type comparisons of Markov kernels, we explicitly characterize the class of asymptotically optimal proposal distributions under this framework, which we refer to as locally balanced proposals. The resulting algorithms are straightforward to implement in discrete spaces and provide orders of magnitude improvements in efficiency compared to alternative MCMC schemes, including discrete versions of Hamiltonian Monte Carlo. Simulations are performed with both simulated and real datasets, including a detailed application to Bayesian record linkage. A direct connection with gradient-based MCMC suggests that locally balanced proposals can be seen as a natural way to extend the latter to discrete spaces. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Giacomo Zanella},
  doi          = {10.1080/01621459.2019.1585255},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {852-865},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Informed proposals for local MCMC in discrete spaces},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Individualized multilayer tensor learning with an
application in imaging analysis. <em>JASA</em>, <em>115</em>(530),
836–851. (<a
href="https://doi.org/10.1080/01621459.2019.1585254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work is motivated by multimodality breast cancer imaging data, which is quite challenging in that the signals of discrete tumor-associated microvesicles are randomly distributed with heterogeneous patterns. This imposes a significant challenge for conventional imaging regression and dimension reduction models assuming a homogeneous feature structure. We develop an innovative multilayer tensor learning method to incorporate heterogeneity to a higher-order tensor decomposition and predict disease status effectively through utilizing subject-wise imaging features and multimodality information. Specifically, we construct a multilayer decomposition which leverages an individualized imaging layer in addition to a modality-specific tensor structure. One major advantage of our approach is that we are able to efficiently capture the heterogeneous spatial features of signals that are not characterized by a population structure as well as integrating multimodality information simultaneously. To achieve scalable computing, we develop a new bi-level block improvement algorithm. In theory, we investigate both the algorithm convergence property, tensor signal recovery error bound and asymptotic consistency for prediction model estimation. We also apply the proposed method for simulated and human breast cancer imaging data. Numerical results demonstrate that the proposed method outperforms other existing competing methods. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Xiwei Tang and Xuan Bi and Annie Qu},
  doi          = {10.1080/01621459.2019.1585254},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {836-851},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Individualized multilayer tensor learning with an application in imaging analysis},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A geometric variational approach to bayesian inference.
<em>JASA</em>, <em>115</em>(530), 822–835. (<a
href="https://doi.org/10.1080/01621459.2019.1585253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel Riemannian geometric framework for variational inference in Bayesian models based on the nonparametric Fisher–Rao metric on the manifold of probability density functions. Under the square-root density representation, the manifold can be identified with the positive orthant of the unit hypersphere S ∞ S ∞ S∞ in 𝕃 2 L 2 L2 , and the Fisher–Rao metric reduces to the standard 𝕃 2 L 2 L2 metric. Exploiting such a Riemannian structure, we formulate the task of approximating the posterior distribution as a variational problem on the hypersphere based on the α -divergence. This provides a tighter lower bound on the marginal distribution when compared to, and a corresponding upper bound unavailable with, approaches based on the Kullback–Leibler divergence. We propose a novel gradient-based algorithm for the variational problem based on Fréchet derivative operators motivated by the geometry of S ∞ , and examine its properties. Through simulations and real data applications, we demonstrate the utility of the proposed geometric framework and algorithm on several Bayesian models. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Abhijoy Saha and Karthik Bharath and Sebastian Kurtek},
  doi          = {10.1080/01621459.2019.1585253},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {822-835},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A geometric variational approach to bayesian inference},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Regression analysis of doubly truncated data. <em>JASA</em>,
<em>115</em>(530), 810–821. (<a
href="https://doi.org/10.1080/01621459.2019.1585252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Doubly truncated data are found in astronomy, econometrics, and survival analysis literature. They arise when each observation is confined to an interval, that is, only those which fall within their respective intervals are observed along with the intervals. Unlike the one-sided truncation that can be handled by counting process-based approach, doubly truncated data are much more difficult to handle. In their analysis of an astronomical dataset, Efron and Petrosian proposed some nonparametric methods for doubly truncated data. Motivated by their approach, as well as by the work of Bhattacharya et al. for right truncated data, we propose a general method for estimating the regression parameter when the dependent variable is subject to the double truncation. It extends the Mann–Whitney-type rank estimator and can be computed easily by existing software packages. Weighted rank estimation is also considered for improving estimation efficiency. We show that the resulting estimators are consistent and asymptotically normal. Resampling schemes are proposed with large sample justification for approximating the limiting distributions. The quasar data in Efron and Petrosian and an AIDS incubation data are analyzed by the new method. Simulation results show that the proposed method works well.},
  archive      = {J_JASA},
  author       = {Zhiliang Ying and Wen Yu and Ziqiang Zhao and Ming Zheng},
  doi          = {10.1080/01621459.2019.1585252},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {810-821},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Regression analysis of doubly truncated data},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Confidence intervals for sparse penalized regression with
random designs. <em>JASA</em>, <em>115</em>(530), 794–809. (<a
href="https://doi.org/10.1080/01621459.2019.1585251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the abundance of large data, sparse penalized regression techniques are commonly used in data analysis due to the advantage of simultaneous variable selection and estimation. A number of convex as well as nonconvex penalties have been proposed in the literature to achieve sparse estimates. Despite intense work in this area, how to perform valid inference for sparse penalized regression with a general penalty remains to be an active research problem. In this article, by making use of state-of-the-art optimization tools in stochastic variational inequality theory, we propose a unified framework to construct confidence intervals for sparse penalized regression with a wide range of penalties, including convex and nonconvex penalties. We study the inference for parameters under the population version of the penalized regression as well as parameters of the underlying linear model. Theoretical convergence properties of the proposed method are obtained. Several simulated and real data examples are presented to demonstrate the validity and effectiveness of the proposed inference procedure. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Guan Yu and Liang Yin and Shu Lu and Yufeng Liu},
  doi          = {10.1080/01621459.2019.1585251},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {794-809},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Confidence intervals for sparse penalized regression with random designs},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Constrained factor models for high-dimensional
matrix-variate time series. <em>JASA</em>, <em>115</em>(530), 775–793.
(<a href="https://doi.org/10.1080/01621459.2019.1584899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional matrix-variate time series data are becoming widely available in many scientific fields, such as economics, biology, and meteorology. To achieve significant dimension reduction while preserving the intrinsic matrix structure and temporal dynamics in such data, Wang, Liu, and Chen proposed a matrix factor model, that is, shown to be able to provide effective analysis. In this article, we establish a general framework for incorporating domain and prior knowledge in the matrix factor model through linear constraints. The proposed framework is shown to be useful in achieving parsimonious parameterization, facilitating interpretation of the latent matrix factor, and identifying specific factors of interest. Fully utilizing the prior-knowledge-induced constraints results in more efficient and accurate modeling, inference, dimension reduction as well as a clear and better interpretation of the results. Constrained, multi-term, and partially constrained factor models for matrix-variate time series are developed, with efficient estimation procedures and their asymptotic properties. We show that the convergence rates of the constrained factor loading matrices are much faster than those of the conventional matrix factor analysis under many situations. Simulation studies are carried out to demonstrate finite-sample performance of the proposed method and its associated asymptotic properties. We illustrate the proposed model with three applications, where the constrained matrix-factor models outperform their unconstrained counterparts in the power of variance explanation under the out-of-sample 10-fold cross-validation setting. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Elynn Y. Chen and Ruey S. Tsay and Rong Chen},
  doi          = {10.1080/01621459.2019.1584899},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {775-793},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Constrained factor models for high-dimensional matrix-variate time series},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation and inference for generalized geoadditive models.
<em>JASA</em>, <em>115</em>(530), 761–774. (<a
href="https://doi.org/10.1080/01621459.2019.1574584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many application areas, data are collected on a count or binary response with spatial covariate information. In this article, we introduce a new class of generalized geoadditive models (GGAMs) for spatial data distributed over complex domains. Through a link function, the proposed GGAM assumes that the mean of the discrete response variable depends on additive univariate functions of explanatory variables and a bivariate function to adjust for the spatial effect. We propose a two-stage approach for estimating and making inferences of the components in the GGAM. In the first stage, the univariate components and the geographical component in the model are approximated via univariate polynomial splines and bivariate penalized splines over triangulation, respectively. In the second stage, local polynomial smoothing is applied to the cleaned univariate data to average out the variation of the first-stage estimators. We investigate the consistency of the proposed estimators and the asymptotic normality of the univariate components. We also establish the simultaneous confidence band for each of the univariate components. The performance of the proposed method is evaluated by two simulation studies. We apply the proposed method to analyze the crash counts data in the Tampa-St. Petersburg urbanized area in Florida. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Shan Yu and Guannan Wang and Li Wang and Chenhui Liu and Lijian Yang},
  doi          = {10.1080/01621459.2019.1574584},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {761-774},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Estimation and inference for generalized geoadditive models},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Category-adaptive variable screening for ultra-high
dimensional heterogeneous categorical data. <em>JASA</em>,
<em>115</em>(530), 747–760. (<a
href="https://doi.org/10.1080/01621459.2019.1573734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The populations of interest in modern studies are very often heterogeneous. The population heterogeneity, the qualitative nature of the outcome variable and the high dimensionality of the predictors pose significant challenge in statistical analysis. In this article, we introduce a category-adaptive screening procedure with high-dimensional heterogeneous data, which is to detect category-specific important covariates. The proposal is a model-free approach without any specification of a regression model and an adaptive procedure in the sense that the set of active variables is allowed to vary across different categories, thus making it more flexible to accommodate heterogeneity. For response-selective sampling data, another main discovery of this article is that the proposed method works directly without any modification. Under mild regularity conditions, the newly procedure is shown to possess the sure screening and ranking consistency properties. Simulation studies contain supportive evidence that the proposed method performs well under various settings and it is effective to extract category-specific information. Applications are illustrated with two real datasets. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jinhan Xie and Yuanyuan Lin and Xiaodong Yan and Niansheng Tang},
  doi          = {10.1080/01621459.2019.1573734},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {747-760},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Category-adaptive variable screening for ultra-high dimensional heterogeneous categorical data},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An extended mallows model for ranked data aggregation.
<em>JASA</em>, <em>115</em>(530), 730–746. (<a
href="https://doi.org/10.1080/01621459.2019.1573733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we study the rank aggregation problem, which aims to find a consensus ranking by aggregating multiple ranking lists. To address the problem probabilistically, we formulate an elaborate ranking model for full and partial rankings by generalizing the Mallows model. Our model assumes that the ranked data are generated through a multistage ranking process that is explicitly governed by parameters that measure the overall quality and stability of the process. The new model is quite flexible and has a closed form expression. Under mild conditions, we can derive a few useful theoretical properties of the model. Furthermore, we propose an efficient statistic called rank coefficient to detect over-correlated rankings and a hierarchical ranking model to fit the data. Through extensive simulation studies and real applications, we evaluate the merits of our models and demonstrate that they outperform the state-of-the-art methods in diverse scenarios. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Han Li and Minxuan Xu and Jun S. Liu and Xiaodan Fan},
  doi          = {10.1080/01621459.2019.1573733},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {730-746},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {An extended mallows model for ranked data aggregation},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Smoothing with couplings of conditional particle filters.
<em>JASA</em>, <em>115</em>(530), 721–729. (<a
href="https://doi.org/10.1080/01621459.2018.1548856">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In state–space models, smoothing refers to the task of estimating a latent stochastic process given noisy measurements related to the process. We propose an unbiased estimator of smoothing expectations. The lack-of-bias property has methodological benefits: independent estimators can be generated in parallel, and CI can be constructed from the central limit theorem to quantify the approximation error. To design unbiased estimators, we combine a generic debiasing technique for Markov chains, with a Markov chain Monte Carlo algorithm for smoothing. The resulting procedure is widely applicable and we show in numerical experiments that the removal of the bias comes at a manageable increase in variance. We establish the validity of the proposed estimators under mild assumptions. Numerical experiments are provided on toy models, including a setting of highly informative observations, and for a realistic Lotka–Volterra model with an intractable transition density. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Pierre E. Jacob and Fredrik Lindsten and Thomas B. Schön},
  doi          = {10.1080/01621459.2018.1548856},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {721-729},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Smoothing with couplings of conditional particle filters},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparametric estimation of copula regression models with
discrete outcomes. <em>JASA</em>, <em>115</em>(530), 707–720. (<a
href="https://doi.org/10.1080/01621459.2018.1546586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate discrete outcomes are common in a wide range of areas including insurance, finance, and biology. When the interplay between outcomes is significant, quantifying dependencies among interrelated variables is of great importance. Due to their ability to accommodate dependence flexibly, copulas are being applied increasingly. Yet, the application of copulas on discrete data is still in its infancy; one of the biggest barriers is the nonuniqueness of copulas, calling into question model interpretations and predictions. In this article, we study copula estimation with discrete outcomes in a regression context. As the marginal distributions vary with covariates, inclusion of continuous regressors expands the region of support for consistent estimation of copulas. Because some properties of continuous outcomes do not carry over to discrete outcomes, specification of a copula model has been a problem. We propose a nonparametric estimator of copulas to identify the “hidden” dependence structure for discrete outcomes and develop its asymptotic properties. The proposed nonparametric estimator can also serve as a diagnostic tool for selecting a parametric form for copulas. In the simulation study, we explore the performance of the proposed estimator under different scenarios and provide guidance on when the choice of copulas is important. The performance of the estimator improves as discreteness diminishes. A practical bandwidth selector is also proposed. An empirical analysis examines a dataset from the Local Government Property Insurance Fund (LGPIF) in the state of Wisconsin. We apply the nonparametric estimator to model the dependence among claim frequencies from different types of insurance coverage. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Lu Yang and Edward W. Frees and Zhengjun Zhang},
  doi          = {10.1080/01621459.2018.1546586},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {707-720},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Nonparametric estimation of copula regression models with discrete outcomes},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating dynamic treatment regimes in mobile health using
v-learning. <em>JASA</em>, <em>115</em>(530), 692–706. (<a
href="https://doi.org/10.1080/01621459.2018.1537919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vision for precision medicine is to use individual patient characteristics to inform a personalized treatment plan that leads to the best possible healthcare for each patient. Mobile technologies have an important role to play in this vision as they offer a means to monitor a patient’s health status in real-time and subsequently to deliver interventions if, when, and in the dose that they are needed. Dynamic treatment regimes formalize individualized treatment plans as sequences of decision rules, one per stage of clinical intervention, that map current patient information to a recommended treatment. However, most existing methods for estimating optimal dynamic treatment regimes are designed for a small number of fixed decision points occurring on a coarse time-scale. We propose a new reinforcement learning method for estimating an optimal treatment regime that is applicable to data collected using mobile technologies in an outpatient setting. The proposed method accommodates an indefinite time horizon and minute-by-minute decision making that are common in mobile health applications. We show that the proposed estimators are consistent and asymptotically normal under mild conditions. The proposed methods are applied to estimate an optimal dynamic treatment regime for controlling blood glucose levels in patients with type 1 diabetes.},
  archive      = {J_JASA},
  author       = {Daniel J. Luckett and Eric B. Laber and Anna R. Kahkoska and David M. Maahs and Elizabeth Mayer-Davis and Michael R. Kosorok},
  doi          = {10.1080/01621459.2018.1537919},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {692-706},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Estimating dynamic treatment regimes in mobile health using V-learning},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-armed angle-based direct learning for estimating
optimal individualized treatment rules with various outcomes.
<em>JASA</em>, <em>115</em>(530), 678–691. (<a
href="https://doi.org/10.1080/01621459.2018.1529597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating an optimal individualized treatment rule (ITR) based on patients’ information is an important problem in precision medicine. An optimal ITR is a decision function that optimizes patients’ expected clinical outcomes. Many existing methods in the literature are designed for binary treatment settings with the interest of a continuous outcome. Much less work has been done on estimating optimal ITRs in multiple treatment settings with good interpretations. In this article, we propose angle-based direct learning (AD-learning) to efficiently estimate optimal ITRs with multiple treatments. Our proposed method can be applied to various types of outcomes, such as continuous, survival, or binary outcomes. Moreover, it has an interesting geometric interpretation on the effect of different treatments for each individual patient, which can help doctors and patients make better decisions. Finite sample error bounds have been established to provide a theoretical guarantee for AD-learning. Finally, we demonstrate the superior performance of our method via an extensive simulation study and real data applications. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Zhengling Qi and Dacheng Liu and Haoda Fu and Yufeng Liu},
  doi          = {10.1080/01621459.2018.1529597},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {678-691},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Multi-armed angle-based direct learning for estimating optimal individualized treatment rules with various outcomes},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Rejoinder. <em>JASA</em>, <em>115</em>(530), 675–677. (<a
href="https://doi.org/10.1080/01621459.2020.1762453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Bradley Efron},
  doi          = {10.1080/01621459.2020.1762453},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {675-677},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Rejoinder},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The data science process: One culture. <em>JASA</em>,
<em>115</em>(530), 672–674. (<a
href="https://doi.org/10.1080/01621459.2020.1762615">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Bin Yu and Rebecca Barter},
  doi          = {10.1080/01621459.2020.1762615},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {672-674},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {The data science process: One culture},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discussion of professor bradley efron’s article on
“prediction, estimation, and attribution.” <em>JASA</em>,
<em>115</em>(530), 667–671. (<a
href="https://doi.org/10.1080/01621459.2020.1762614">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Min-ge Xie and Zheshi Zheng},
  doi          = {10.1080/01621459.2020.1762614},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {667-671},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion of professor bradley efron’s article on “Prediction, estimation, and attribution”},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discussion of “prediction, estimation, and attribution” by
bradley efron. <em>JASA</em>, <em>115</em>(530), 665–666. (<a
href="https://doi.org/10.1080/01621459.2020.1762617">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Professor Efron has presented us with a thought-provoking paper on the relationship between prediction, estimation, and attribution in the modern era of data science. While we appreciate many of his arguments, we see more of a continuum between the old and new methodology, and the opportunity for both to improve through their synergy.},
  archive      = {J_JASA},
  author       = {Jerome Friedman and Trevor Hastie and Robert Tibshirani},
  doi          = {10.1080/01621459.2020.1762617},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {665-666},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion of “Prediction, estimation, and attribution” by bradley efron},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discussion. <em>JASA</em>, <em>115</em>(530), 663–664. (<a
href="https://doi.org/10.1080/01621459.2020.1762616">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {A. C. Davison},
  doi          = {10.1080/01621459.2020.1762616},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {663-664},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comment: When is it data science and when is it data
engineering? <em>JASA</em>, <em>115</em>(530), 660–662. (<a
href="https://doi.org/10.1080/01621459.2020.1762619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Noel Cressie},
  doi          = {10.1080/01621459.2020.1762619},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {660-662},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Comment: When is it data science and when is it data engineering?},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discussion of paper by brad efron. <em>JASA</em>,
<em>115</em>(530), 659. (<a
href="https://doi.org/10.1080/01621459.2020.1762451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {D. R. Cox},
  doi          = {10.1080/01621459.2020.1762451},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {659},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion of paper by brad efron},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discussion of the paper “prediction, estimation, and
attribution” by b. efron. <em>JASA</em>, <em>115</em>(530), 656–658. (<a
href="https://doi.org/10.1080/01621459.2020.1762618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Emmanuel Candès and Chiara Sabatti},
  doi          = {10.1080/01621459.2020.1762618},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {656-658},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion of the paper “Prediction, estimation, and attribution” by b. efron},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Prediction, estimation, and attribution. <em>JASA</em>,
<em>115</em>(530), 636–655. (<a
href="https://doi.org/10.1080/01621459.2020.1762613">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scientific needs and computational limitations of the twentieth century fashioned classical statistical methodology. Both the needs and limitations have changed in the twenty-first, and so has the methodology. Large-scale prediction algorithms—neural nets, deep learning, boosting, support vector machines, random forests—have achieved star status in the popular press. They are recognizable as heirs to the regression tradition, but ones carried out at enormous scale and on titanic datasets. How do these algorithms compare with standard regression techniques such as ordinary least squares or logistic regression? Several key discrepancies will be examined, centering on the differences between prediction and estimation or prediction and attribution (significance testing). Most of the discussion is carried out through small numerical examples.},
  archive      = {J_JASA},
  author       = {Bradley Efron},
  doi          = {10.1080/01621459.2020.1762613},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {636-655},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Prediction, estimation, and attribution},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Statistical topology and the random interstellar medium.
<em>JASA</em>, <em>115</em>(530), 625–635. (<a
href="https://doi.org/10.1080/01621459.2019.1647841">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We use topological methods to investigate the small-scale variation and local spatial characteristics of the interstellar medium (ISM) in three regions of the southern sky. We demonstrate that there are circumstances where topological methods can identify differences in distributions when conventional marginal or correlation analyses may not. We propose a nonparametric method for comparing two fields based on the counts of topological features and the geometry of the associated persistence diagrams. We investigate the expected distribution of topological structures quantified through Betti numbers under Gaussian random field (GRF) assumptions, which underlie many astrophysical models of the ISM. When we apply the methods to the astrophysical data, we find strong evidence that one of the three regions is both topologically dissimilar to the other two and not consistent with an underlying GRF model. This region is proximal to a region of recent star formation whereas the others are more distant. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Robin Henderson and Irina Makarenko and Paul Bushby and Andrew Fletcher and Anvar Shukurov},
  doi          = {10.1080/01621459.2019.1647841},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {625-635},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Statistical topology and the random interstellar medium},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian graphical compositional regression for microbiome
data. <em>JASA</em>, <em>115</em>(530), 610–624. (<a
href="https://doi.org/10.1080/01621459.2019.1647212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important task in microbiome studies is to test the existence of and give characterization to differences in the microbiome composition across groups of samples. Important challenges of this problem include the large within-group heterogeneities among samples and the existence of potential confounding variables that, when ignored, increase the chance of false discoveries and reduce the power for identifying true differences. We propose a probabilistic framework to overcome these issues by combining three ideas: (i) a phylogenetic tree-based decomposition of the cross-group comparison problem into a series of local tests, (ii) a graphical model that links the local tests to allow information sharing across taxa, and (iii) a Bayesian testing strategy that incorporates covariates and integrates out the within-group variation, avoiding potentially unstable point estimates. With the proposed method, we analyze the American Gut data to compare the gut microbiome composition of groups of participants with different dietary habits. Our analysis shows that (i) the frequency of consuming fruit, seafood, vegetable, and whole grain are closely related to the gut microbiome composition and (ii) the conclusion of the analysis can change drastically when different sets of relevant covariates are adjusted, indicating the necessity of carefully selecting and including possible confounders in the analysis when comparing microbiome compositions with data from observational studies. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Jialiang Mao and Yuhan Chen and Li Ma},
  doi          = {10.1080/01621459.2019.1647212},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {610-624},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian graphical compositional regression for microbiome data},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MIMIX: A bayesian mixed-effects model for microbiome data
from designed experiments. <em>JASA</em>, <em>115</em>(530), 599–609.
(<a href="https://doi.org/10.1080/01621459.2019.1626242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in bioinformatics have made high-throughput microbiome data widely available, and new statistical tools are required to maximize the information gained from these data. For example, analysis of high-dimensional microbiome data from designed experiments remains an open area in microbiome research. Contemporary analyses work on metrics that summarize collective properties of the microbiome, but such reductions preclude inference on the fine-scale effects of environmental stimuli on individual microbial taxa. Other approaches model the proportions or counts of individual taxa as response variables in mixed models, but these methods fail to account for complex correlation patterns among microbial communities. In this article, we propose a novel Bayesian mixed-effects model that exploits cross-taxa correlations within the microbiome, a model we call microbiome mixed model (MIMIX). MIMIX offers global tests for treatment effects, local tests and estimation of treatment effects on individual taxa, quantification of the relative contribution from heterogeneous sources to microbiome variability, and identification of latent ecological subcommunities in the microbiome. MIMIX is tailored to large microbiome experiments using a combination of Bayesian factor analysis to efficiently represent dependence between taxa and Bayesian variable selection methods to achieve sparsity. We demonstrate the model using a simulation experiment and on a 2 × 2 factorial experiment of the effects of nutrient supplement and herbivore exclusion on the foliar fungal microbiome of Andropogon gerardii , a perennial bunchgrass, as part of the global Nutrient Network research initiative. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Neal S. Grantham and Yawen Guan and Brian J. Reich and Elizabeth T. Borer and Kevin Gross},
  doi          = {10.1080/01621459.2019.1626242},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {599-609},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {MIMIX: A bayesian mixed-effects model for microbiome data from designed experiments},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian optimal design for ordinary differential equation
models with application in biological science. <em>JASA</em>,
<em>115</em>(530), 583–598. (<a
href="https://doi.org/10.1080/01621459.2019.1617154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian optimal design is considered for experiments where the response distribution depends on the solution to a system of nonlinear ordinary differential equations. The motivation is an experiment to estimate parameters in the equations governing the transport of amino acids through cell membranes in human placentas. Decision-theoretic Bayesian design of experiments for such nonlinear models is conceptually very attractive, allowing the formal incorporation of prior knowledge to overcome the parameter dependence of frequentist design and being less reliant on asymptotic approximations. However, the necessary approximation and maximization of the, typically analytically intractable, expected utility results in a computationally challenging problem. These issues are further exacerbated if the solution to the differential equations is not available in closed-form. This article proposes a new combination of a probabilistic solution to the equations embedded within a Monte Carlo approximation to the expected utility with cyclic descent of a smooth approximation to find the optimal design. A novel precomputation algorithm reduces the computational burden, making the search for an optimal design feasible for bigger problems. The methods are demonstrated by finding new designs for a number of common models derived from differential equations, and by providing optimal designs for the placenta experiment. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Antony M. Overstall and David C. Woods and Ben M. Parker},
  doi          = {10.1080/01621459.2019.1617154},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {583-598},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian optimal design for ordinary differential equation models with application in biological science},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Testing and estimation of social network dependence with
time to event data. <em>JASA</em>, <em>115</em>(530), 570–582. (<a
href="https://doi.org/10.1080/01621459.2019.1617153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, events are spread rapidly along social networks. We are interested in whether people’s responses to an event are affected by their friends’ characteristics. For example, how soon will a person start playing a game given that his/her friends like it? Studying social network dependence is an emerging research area. In this work, we propose a novel latent spatial autocorrelation Cox model to study social network dependence with time-to-event data. The proposed model introduces a latent indicator to characterize whether a person’s survival time might be affected by his or her friends’ features. We first propose a score-type test for detecting the existence of social network dependence. If it exists, we further develop an EM-type algorithm to estimate the model parameters. The performance of the proposed test and estimators are illustrated by simulation studies and an application to a time-to-event dataset about playing a popular mobile game from one of the largest online social network platforms. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Lin Su and Wenbin Lu and Rui Song and Danyang Huang},
  doi          = {10.1080/01621459.2019.1617153},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {570-582},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Testing and estimation of social network dependence with time to event data},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hierarchical space-time modeling of asymptotically
independent exceedances with an application to precipitation data.
<em>JASA</em>, <em>115</em>(530), 555–569. (<a
href="https://doi.org/10.1080/01621459.2019.1617152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The statistical modeling of space-time extremes in environmental applications is key to understanding complex dependence structures in original event data and to generating realistic scenarios for impact models. In this context of high-dimensional data, we propose a novel hierarchical model for high threshold exceedances defined over continuous space and time by embedding a space-time Gamma process convolution for the rate of an exponential variable, leading to asymptotic independence in space and time. Its physically motivated anisotropic dependence structure is based on geometric objects moving through space-time according to a velocity vector. We demonstrate that inference based on weighted pairwise likelihood is fast and accurate. The usefulness of our model is illustrated by an application to hourly precipitation data from a study region in Southern France, where it clearly improves on an alternative censored Gaussian space-time random field model. While classical limit models based on threshold-stability fail to appropriately capture relatively fast joint tail decay rates between asymptotic dependence and classical independence, strong empirical evidence from our application and other recent case studies motivates the use of more realistic asymptotic independence models such as ours. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Jean-Noël Bacro and Carlo Gaetan and Thomas Opitz and Gwladys Toulemonde},
  doi          = {10.1080/01621459.2019.1617152},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {555-569},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Hierarchical space-time modeling of asymptotically independent exceedances with an application to precipitation data},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A large-scale constrained joint modeling approach for
predicting user activity, engagement, and churn with application to
freemium mobile games. <em>JASA</em>, <em>115</em>(530), 538–554. (<a
href="https://doi.org/10.1080/01621459.2019.1611584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a constrained extremely zero inflated joint (CEZIJ) modeling framework for simultaneously analyzing player activity, engagement, and dropouts (churns) in app-based mobile freemium games. Our proposed framework addresses the complex interdependencies between a player’s decision to use a freemium product, the extent of her direct and indirect engagement with the product and her decision to permanently drop its usage. CEZIJ extends the existing class of joint models for longitudinal and survival data in several ways. It not only accommodates extremely zero-inflated responses in a joint model setting but also incorporates domain-specific, convex structural constraints on the model parameters. Longitudinal data from app-based mobile games usually exhibit a large set of potential predictors and choosing the relevant set of predictors is highly desirable for various purposes including improved predictability. To achieve this goal, CEZIJ conducts simultaneous, coordinated selection of fixed and random effects in high-dimensional penalized generalized linear mixed models. For analyzing such large-scale datasets, variable selection and estimation are conducted via a distributed computing based split-and-conquer approach that massively increases scalability and provides better predictive performance over competing predictive methods. Our results reveal codependencies between varied player characteristics that promote player activity and engagement. Furthermore, the predicted churn probabilities exhibit idiosyncratic clusters of player profiles over time based on which marketers and game managers can segment the playing population for improved monetization of app-based freemium games. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Trambak Banerjee and Gourab Mukherjee and Shantanu Dutta and Pulak Ghosh},
  doi          = {10.1080/01621459.2019.1611584},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {538-554},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A large-scale constrained joint modeling approach for predicting user activity, engagement, and churn with application to freemium mobile games},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust clustering with subpopulation-specific deviations.
<em>JASA</em>, <em>115</em>(530), 521–537. (<a
href="https://doi.org/10.1080/01621459.2019.1611583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The National Birth Defects Prevention Study (NBDPS) is a case-control study of birth defects conducted across 10 U.S. states. Researchers are interested in characterizing the etiologic role of maternal diet, collected using a food frequency questionnaire. Because diet is multidimensional, dimension reduction methods such as cluster analysis are often used to summarize dietary patterns. In a large, heterogeneous population, traditional clustering methods, such as latent class analysis, used to estimate dietary patterns can produce a large number of clusters due to a variety of factors, including study size and regional diversity. These factors result in a loss of interpretability of patterns that may differ due to minor consumption changes. Based on adaptation of the local partition process, we propose a new method, robust profile clustering, to handle these data complexities. Here, participants may be clustered at two levels: (1) globally, where women are assigned to an overall population-level cluster via an overfitted finite mixture model, and (2) locally, where regional variations in diet are accommodated via a beta-Bernoulli process dependent on subpopulation differences. We use our method to analyze the NBDPS data, deriving prepregnancy dietary patterns for women in the NBDPS while accounting for regional variability. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Briana J. K. Stephenson and Amy H. Herring and Andrew Olshan},
  doi          = {10.1080/01621459.2019.1611583},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {521-537},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Robust clustering with subpopulation-specific deviations},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bayesian general linear modeling approach to cortical
surface fMRI data analysis. <em>JASA</em>, <em>115</em>(530), 501–520.
(<a href="https://doi.org/10.1080/01621459.2019.1611582">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cortical surface functional magnetic resonance imaging (cs-fMRI) has recently grown in popularity versus traditional volumetric fMRI. In addition to offering better whole-brain visualization, dimension reduction, removal of extraneous tissue types, and improved alignment of cortical areas across subjects, it is also more compatible with common assumptions of Bayesian spatial models. However, as no spatial Bayesian model has been proposed for cs-fMRI data, most analyses continue to employ the classical general linear model (GLM), a “massive univariate” approach. Here, we propose a spatial Bayesian GLM for cs-fMRI, which employs a class of sophisticated spatial processes to model latent activation fields. We make several advances compared with existing spatial Bayesian models for volumetric fMRI. First, we use integrated nested Laplacian approximations, a highly accurate and efficient Bayesian computation technique, rather than variational Bayes. To identify regions of activation, we utilize an excursions set method based on the joint posterior distribution of the latent fields, rather than the marginal distribution at each location. Finally, we propose the first multi-subject spatial Bayesian modeling approach, which addresses a major gap in the existing literature. The methods are very computationally advantageous and are validated through simulation studies and two task fMRI studies from the Human Connectome Project. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Amanda F. Mejia and Yu (Ryan) Yue and David Bolin and Finn Lindgren and Martin A. Lindquist},
  doi          = {10.1080/01621459.2019.1611582},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {501-520},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A bayesian general linear modeling approach to cortical surface fMRI data analysis},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reinforcing the impact of statistics on society.
<em>JASA</em>, <em>115</em>(530), 491–500. (<a
href="https://doi.org/10.1080/01621459.2020.1761217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {What does statistics have to offer science and society, in this age of massive data, machine learning algorithms, and multiple online sources of tools for data analysis? I recall a few situations where statistics made a real difference and reinforced the impact of our discipline on society. Sometimes the difference lay in the insightful analysis and inference enabled by ground-breaking methods in our field like hypothesis testing, likelihood ratios, Bayesian models, jackknife, and bootstrap. But perhaps more often, the impacts came from thoughtful analyses before data were collected, and the questions that arose after the statistical analysis. The impact of understanding the problem, designing the experiment and data collections, conducting the pilot surveys, and raising important questions, is substantial. Through sensible explorations following formal statistical procedures, statisticians have made contributions in many domains. In this presentation, I recall some examples which made a long-lasting impact. Some of them, like randomization in clinical trials, known and familiar to all, are so ingrained in our practice that the role of statistics has been forgotten. Others may be less familiar but nonetheless benefited greatly from the critical input of statisticians. All remind us that our field remains today not only relevant but critical to science and society.},
  archive      = {J_JASA},
  author       = {Karen Kafadar},
  doi          = {10.1080/01621459.2020.1761217},
  journal      = {Journal of the American Statistical Association},
  number       = {530},
  pages        = {491-500},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Reinforcing the impact of statistics on society},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RETRACTED ARTICLE: Smoothing with couplings of conditional
particle filters. <em>JASA</em>, <em>115</em>(529), 489. (<a
href="https://doi.org/10.1080/01621459.2018.1505625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  doi          = {10.1080/01621459.2018.1505625},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {489},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {RETRACTED ARTICLE: Smoothing with couplings of conditional particle filters},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Big data in omics and imaging: Integrated analysis and
causal inference. <em>JASA</em>, <em>115</em>(529), 487–488. (<a
href="https://doi.org/10.1080/01621459.2020.1721249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Oliver Y. Chén},
  doi          = {10.1080/01621459.2020.1721249},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {487-488},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Big data in omics and imaging: Integrated analysis and causal inference.},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multivariate kernel smoothing and its applications.
<em>JASA</em>, <em>115</em>(529), 486. (<a
href="https://doi.org/10.1080/01621459.2020.1721247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Qing Wang},
  doi          = {10.1080/01621459.2020.1721247},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {486},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Multivariate kernel smoothing and its applications},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Theory of stochastic objects: Probability, stochastic
processes and inference. <em>JASA</em>, <em>115</em>(529), 486–487. (<a
href="https://doi.org/10.1080/01621459.2020.1721248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Anita D. Behme},
  doi          = {10.1080/01621459.2020.1721248},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {486-487},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Theory of stochastic objects: Probability, stochastic processes and inference.},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Measuring agreement: Models, methods, and applications.
<em>JASA</em>, <em>115</em>(529), 485–486. (<a
href="https://doi.org/10.1080/01621459.2020.1721246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Noor Azina Ismail},
  doi          = {10.1080/01621459.2020.1721246},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {485-486},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Measuring agreement: Models, methods, and applications.},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The book of why: The new science of cause and effect.
<em>JASA</em>, <em>115</em>(529), 482–485. (<a
href="https://doi.org/10.1080/01621459.2020.1721245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Peter M. Aronow and Fredrik Sävje},
  doi          = {10.1080/01621459.2020.1721245},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {482-485},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {The book of why: The new science of cause and effect},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simulating copulas: Stochastic models, sampling algorithms,
and applications. <em>JASA</em>, <em>115</em>(529), 481–482. (<a
href="https://doi.org/10.1080/01621459.2020.1721244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Thibault Vatter},
  doi          = {10.1080/01621459.2020.1721244},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {481-482},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Simulating copulas: Stochastic models, sampling algorithms, and applications},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation of conditional prevalence from group testing data
with missing covariates. <em>JASA</em>, <em>115</em>(529), 467–480. (<a
href="https://doi.org/10.1080/01621459.2019.1566071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider estimating the conditional prevalence of a disease from data pooled according to the group testing mechanism. Consistent estimators have been proposed in the literature, but they rely on the data being available for all individuals. In infectious disease studies where group testing is frequently applied, the covariate is often missing for some individuals. There, unless the missing mechanism occurs completely at random, applying the existing techniques to the complete cases without adjusting for missingness does not generally provide consistent estimators, and finding appropriate modifications is challenging. We develop a consistent spline estimator, derive its theoretical properties, and show how to adapt local polynomial and likelihood estimators to the missing data problem. We illustrate the numerical performance of our methods on simulated and real examples. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Aurore Delaigle and Wei Huang and Shaoke Lei},
  doi          = {10.1080/01621459.2019.1566071},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {467-480},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Estimation of conditional prevalence from group testing data with missing covariates},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variational inference for stochastic block models from
sampled data. <em>JASA</em>, <em>115</em>(529), 455–466. (<a
href="https://doi.org/10.1080/01621459.2018.1562934">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article deals with nonobserved dyads during the sampling of a network and consecutive issues in the inference of the stochastic block model (SBM). We review sampling designs and recover missing at random (MAR) and not missing at random (NMAR) conditions for the SBM. We introduce variants of the variational EM algorithm for inferring the SBM under various sampling designs (MAR and NMAR) all available as an R package. Model selection criteria based on integrated classification likelihood are derived for selecting both the number of blocks and the sampling design. We investigate the accuracy and the range of applicability of these algorithms with simulations. We explore two real-world networks from ethnology (seed circulation network) and biology (protein–protein interaction network), where the interpretations considerably depend on the sampling designs considered. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Timothée Tabouy and Pierre Barbillon and Julien Chiquet},
  doi          = {10.1080/01621459.2018.1562934},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {455-466},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Variational inference for stochastic block models from sampled data},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Debiased inference on treatment effect in a high-dimensional
model. <em>JASA</em>, <em>115</em>(529), 442–454. (<a
href="https://doi.org/10.1080/01621459.2018.1558062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article concerns the potential bias in statistical inference on treatment effects when a large number of covariates are present in a linear or partially linear model. While the estimation bias in an under-fitted model is well understood, we address a lesser-known bias that arises from an over-fitted model. The over-fitting bias can be eliminated through data splitting at the cost of statistical efficiency, and we show that smoothing over random data splits can be pursued to mitigate the efficiency loss. We also discuss some of the existing methods for debiased inference and provide insights into their intrinsic bias-variance trade-off, which leads to an improvement in bias controls. Under appropriate conditions, we show that the proposed estimators for the treatment effects are asymptotically normal and their variances can be well estimated. We discuss the pros and cons of various methods both theoretically and empirically, and show that the proposed methods are valuable options in post-selection inference. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Jingshen Wang and Xuming He and Gongjun Xu},
  doi          = {10.1080/01621459.2018.1558062},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {442-454},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Debiased inference on treatment effect in a high-dimensional model},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation of the boundary of a variable observed with
symmetric error. <em>JASA</em>, <em>115</em>(529), 425–441. (<a
href="https://doi.org/10.1080/01621459.2018.1555093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider the model Y = X + ε Y = X + ε Y=X+ε with X = τ + Z X = τ + Z X=τ+Z , where τ is an unknown constant (the boundary of X ), Z is a random variable defined on ℝ + R + R+ , ε is a symmetric error, and ε and Z are independent. Based on an iid sample of Y , we aim at identifying and estimating the boundary τ when the law of ε is unknown (apart from symmetry) and in particular its variance is unknown. We propose an estimation procedure based on a minimal distance approach and by making use of Laguerre polynomials. Asymptotic results as well as finite sample simulations are shown. The paper also proposes an extension to stochastic frontier analysis, where the model is conditional to observed variables. The model becomes Y = τ ( w 1 , w 2 ) + Z + ε , where Y is a cost, w 1 are the observed outputs and w 2 represents the observed values of other conditioning variables, so Z is the cost inefficiency. Some simulations illustrate again how the approach works in finite samples, and the proposed procedure is illustrated with data coming from post offices in France.},
  archive      = {J_JASA},
  author       = {Jean-Pierre Florens and Léopold Simar and Ingrid Van Keilegom},
  doi          = {10.1080/01621459.2018.1555093},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {425-441},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Estimation of the boundary of a variable observed with symmetric error},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). L2RM: Low-rank linear regression models for high-dimensional
matrix responses. <em>JASA</em>, <em>115</em>(529), 403–424. (<a
href="https://doi.org/10.1080/01621459.2018.1555092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this article is to develop a low-rank linear regression model to correlate a high-dimensional response matrix with a high-dimensional vector of covariates when coefficient matrices have low-rank structures. We propose a fast and efficient screening procedure based on the spectral norm of each coefficient matrix to deal with the case when the number of covariates is extremely large. We develop an efficient estimation procedure based on the trace norm regularization, which explicitly imposes the low rank structure of coefficient matrices. When both the dimension of response matrix and that of covariate vector diverge at the exponential order of the sample size, we investigate the sure independence screening property under some mild conditions. We also systematically investigate some theoretical properties of our estimation procedure including estimation consistency, rank consistency, and nonasymptotic error bound under some mild conditions. We further establish a theoretical guarantee for the overall solution of our two-step screening and estimation procedure. We examine the finite-sample performance of our screening and estimation methods using simulations and a large-scale imaging genetic dataset collected by the Philadelphia Neurodevelopmental Cohort study. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Dehan Kong and Baiguo An and Jingwen Zhang and Hongtu Zhu},
  doi          = {10.1080/01621459.2018.1555092},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {403-424},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {L2RM: Low-rank linear regression models for high-dimensional matrix responses},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cauchy combination test: A powerful test with analytic
p-value calculation under arbitrary dependency structures.
<em>JASA</em>, <em>115</em>(529), 393–402. (<a
href="https://doi.org/10.1080/01621459.2018.1554485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract– Combining individual p -values to aggregate multiple small effects has a long-standing interest in statistics, dating back to the classic Fisher’s combination test. In modern large-scale data analysis, correlation and sparsity are common features and efficient computation is a necessary requirement for dealing with massive data. To overcome these challenges, we propose a new test that takes advantage of the Cauchy distribution. Our test statistic has a simple form and is defined as a weighted sum of Cauchy transformation of individual p -values. We prove a nonasymptotic result that the tail of the null distribution of our proposed test statistic can be well approximated by a Cauchy distribution under arbitrary dependency structures. Based on this theoretical result, the p -value calculation of our proposed test is not only accurate, but also as simple as the classic z -test or t -test, making our test well suited for analyzing massive data. We further show that the power of the proposed test is asymptotically optimal in a strong sparsity setting. Extensive simulations demonstrate that the proposed test has both strong power against sparse alternatives and a good accuracy with respect to p -value calculations, especially for very small p -values. The proposed test has also been applied to a genome-wide association study of Crohn’s disease and compared with several existing tests. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yaowu Liu and Jun Xie},
  doi          = {10.1080/01621459.2018.1554485},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {393-402},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Cauchy combination test: A powerful test with analytic p-value calculation under arbitrary dependency structures},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Matched learning for optimizing individualized treatment
strategies using electronic health records. <em>JASA</em>,
<em>115</em>(529), 380–392. (<a
href="https://doi.org/10.1080/01621459.2018.1549050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current guidelines for treatment decision making largely rely on data from randomized controlled trials (RCTs) studying average treatment effects. They may be inadequate to make individualized treatment decisions in real-world settings. Large-scale electronic health records (EHR) provide opportunities to fulfill the goals of personalized medicine and learn individualized treatment rules (ITRs) depending on patient-specific characteristics from real-world patient data. In this work, we tackle challenges with EHRs and propose a machine learning approach based on matching (M-learning) to estimate optimal ITRs from EHRs. This new learning method performs matching instead of inverse probability weighting as commonly used in many existing methods for estimating ITRs to more accurately assess individuals’ treatment responses to alternative treatments and alleviate confounding. Matching-based value functions are proposed to compare matched pairs under a unified framework, where various types of outcomes for measuring treatment response (including continuous, ordinal, and discrete outcomes) can easily be accommodated. We establish the Fisher consistency and convergence rate of M-learning. Through extensive simulation studies, we show that M-learning outperforms existing methods when propensity scores are misspecified or when unmeasured confounders are present in certain scenarios. Lastly, we apply M-learning to estimate optimal personalized second-line treatments for type 2 diabetes patients to achieve better glycemic control or reduce major complications using EHRs from New York Presbyterian Hospital. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Peng Wu and Donglin Zeng and Yuanjia Wang},
  doi          = {10.1080/01621459.2018.1549050},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {380-392},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Matched learning for optimizing individualized treatment strategies using electronic health records},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RANK: Large-scale inference with graphical nonlinear
knockoffs. <em>JASA</em>, <em>115</em>(529), 362–379. (<a
href="https://doi.org/10.1080/01621459.2018.1546589">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power and reproducibility are key to enabling refined scientific discoveries in contemporary big data applications with general high-dimensional nonlinear models. In this article, we provide theoretical foundations on the power and robustness for the model-X knockoffs procedure introduced recently in Candès, Fan, Janson and Lv in high-dimensional setting when the covariate distribution is characterized by Gaussian graphical model. We establish that under mild regularity conditions, the power of the oracle knockoffs procedure with known covariate distribution in high-dimensional linear models is asymptotically one as sample size goes to infinity. When moving away from the ideal case, we suggest the modified model-X knockoffs method called graphical nonlinear knockoffs (RANK) to accommodate the unknown covariate distribution. We provide theoretical justifications on the robustness of our modified procedure by showing that the false discovery rate (FDR) is asymptotically controlled at the target level and the power is asymptotically one with the estimated covariate distribution. To the best of our knowledge, this is the first formal theoretical result on the power for the knockoffs procedure. Simulation results demonstrate that compared to existing approaches, our method performs competitively in both FDR control and power. A real dataset is analyzed to further assess the performance of the suggested knockoffs procedure. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yingying Fan and Emre Demirkaya and Gaorong Li and Jinchi Lv},
  doi          = {10.1080/01621459.2018.1546589},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {362-379},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {RANK: Large-scale inference with graphical nonlinear knockoffs},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A randomized exchange algorithm for computing optimal
approximate designs of experiments. <em>JASA</em>, <em>115</em>(529),
348–361. (<a
href="https://doi.org/10.1080/01621459.2018.1546588">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a class of subspace ascent methods for computing optimal approximate designs that covers existing algorithms as well as new and more efficient ones. Within this class of methods, we construct a simple, randomized exchange algorithm (REX). Numerical comparisons suggest that the performance of REX is comparable or superior to that of state-of-the-art methods across a broad range of problem structures and sizes. We focus on the most commonly used criterion of D-optimality, which also has applications beyond experimental design, such as the construction of the minimum-volume ellipsoid containing a given set of data points. For D-optimality, we prove that the proposed algorithm converges to the optimum. We also provide formulas for the optimal exchange of weights in the case of the criterion of A-optimality, which enable one to use REX and some other algorithms for computing A-optimal and I-optimal designs. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Radoslav Harman and Lenka Filová and Peter Richtárik},
  doi          = {10.1080/01621459.2018.1546588},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {348-361},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A randomized exchange algorithm for computing optimal approximate designs of experiments},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PUlasso: High-dimensional variable selection with
presence-only data. <em>JASA</em>, <em>115</em>(529), 334–347. (<a
href="https://doi.org/10.1080/01621459.2018.1546587">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In various real-world problems, we are presented with classification problems with positive and unlabeled data , referred to as presence-only responses. In this article we study variable selection in the context of presence only responses where the number of features or covariates p is large. The combination of presence-only responses and high dimensionality presents both statistical and computational challenges. In this article, we develop the PUlasso algorithm for variable selection and classification with positive and unlabeled responses. Our algorithm involves using the majorization-minimization framework which is a generalization of the well-known expectation-maximization (EM) algorithm. In particular to make our algorithm scalable, we provide two computational speed-ups to the standard EM algorithm. We provide a theoretical guarantee where we first show that our algorithm converges to a stationary point, and then prove that any stationary point within a local neighborhood of the true parameter achieves the minimax optimal mean-squared error under both strict sparsity and group sparsity assumptions. We also demonstrate through simulations that our algorithm outperforms state-of-the-art algorithms in the moderate p settings in terms of classification performance. Finally, we demonstrate that our PUlasso algorithm performs well on a biochemistry example. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Hyebin Song and Garvesh Raskutti},
  doi          = {10.1080/01621459.2018.1546587},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {334-347},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {PUlasso: High-dimensional variable selection with presence-only data},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hierarchical normalized completely random measures to
cluster grouped data. <em>JASA</em>, <em>115</em>(529), 318–333. (<a
href="https://doi.org/10.1080/01621459.2019.1594833">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a Bayesian nonparametric model for clustering grouped data. We adopt a hierarchical approach: at the highest level, each group of data is modeled according to a mixture, where the mixing distributions are conditionally independent normalized completely random measures (NormCRMs) centered on the same base measure, which is itself a NormCRM. The discreteness of the shared base measure implies that the processes at the data level share the same atoms. This desired feature allows to cluster together observations of different groups. We obtain a representation of the hierarchical clustering model by marginalizing with respect to the infinite dimensional NormCRMs. We investigate the properties of the clustering structure induced by the proposed model and provide theoretical results concerning the distribution of the number of clusters, within and between groups. Furthermore, we offer an interpretation in terms of generalized Chinese restaurant franchise process, which allows for posterior inference under both conjugate and nonconjugate models. We develop algorithms for fully Bayesian inference and assess performances by means of a simulation study and a real-data illustration. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Raffaele Argiento and Andrea Cremaschi and Marina Vannucci},
  doi          = {10.1080/01621459.2019.1594833},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {318-333},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Hierarchical normalized completely random measures to cluster grouped data},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ball covariance: A generic measure of dependence in banach
space. <em>JASA</em>, <em>115</em>(529), 307–317. (<a
href="https://doi.org/10.1080/01621459.2018.1543600">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Technological advances in science and engineering have led to the routine collection of large and complex data objects, where the dependence structure among those objects is often of great interest. Those complex objects (e.g., different brain subcortical structures) often reside in some Banach spaces, and hence their relationship cannot be well characterized by most of the existing measures of dependence such as correlation coefficients developed in Hilbert spaces. To overcome the limitations of the existing measures, we propose Ball Covariance as a generic measure of dependence between two random objects in two possibly different Banach spaces. Our Ball Covariance possesses the following attractive properties: (i) It is nonparametric and model-free, which make the proposed measure robust to model mis-specification; (ii) It is nonnegative and equal to zero if and only if two random objects in two separable Banach spaces are independent; (iii) Empirical Ball Covariance is easy to compute and can be used as a test statistic of independence. We present both theoretical and numerical results to reveal the potential power of the Ball Covariance in detecting dependence. Also importantly, we analyze two real datasets to demonstrate the usefulness of Ball Covariance in the complex dependence detection. Supplementary materials for this article are avaiable online.},
  archive      = {J_JASA},
  author       = {Wenliang Pan and Xueqin Wang and Heping Zhang and Hongtu Zhu and Jin Zhu},
  doi          = {10.1080/01621459.2018.1543600},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {307-317},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Ball covariance: A generic measure of dependence in banach space},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). D-CCA: A decomposition-based canonical correlation analysis
for high-dimensional datasets. <em>JASA</em>, <em>115</em>(529),
292–306. (<a
href="https://doi.org/10.1080/01621459.2018.1543599">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A typical approach to the joint analysis of two high-dimensional datasets is to decompose each data matrix into three parts: a low-rank common matrix that captures the shared information across datasets, a low-rank distinctive matrix that characterizes the individual information within a single dataset, and an additive noise matrix. Existing decomposition methods often focus on the orthogonality between the common and distinctive matrices, but inadequately consider the more necessary orthogonal relationship between the two distinctive matrices. The latter guarantees that no more shared information is extractable from the distinctive matrices. We propose decomposition-based canonical correlation analysis (D-CCA), a novel decomposition method that defines the common and distinctive matrices from the ℓ 2 space of random variables rather than the conventionally used Euclidean space, with a careful construction of the orthogonal relationship between distinctive matrices. D-CCA represents a natural generalization of the traditional canonical correlation analysis. The proposed estimators of common and distinctive matrices are shown to be consistent and have reasonably better performance than some state-of-the-art methods in both simulated data and the real data analysis of breast cancer data obtained from The Cancer Genome Atlas. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Hai Shu and Xiao Wang and Hongtu Zhu},
  doi          = {10.1080/01621459.2018.1543599},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {292-306},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {D-CCA: A decomposition-based canonical correlation analysis for high-dimensional datasets},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). From distance correlation to multiscale graph correlation.
<em>JASA</em>, <em>115</em>(529), 280–291. (<a
href="https://doi.org/10.1080/01621459.2018.1543125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding and developing a correlation measure that can detect general dependencies is not only imperative to statistics and machine learning, but also crucial to general scientific discovery in the big data age. In this paper, we establish a new framework that generalizes distance correlation ( Dcorr )—a correlation measure that was recently proposed and shown to be universally consistent for dependence testing against all joint distributions of finite moments—to the multiscale graph correlation ( MGC ). By using the characteristic functions and incorporating the nearest neighbor machinery, we formalize the population version of local distance correlations, define the optimal scale in a given dependency, and name the optimal local correlation as MGC . The new theoretical framework motivates a theoretically sound sample MGC and allows a number of desirable properties to be proved, including the universal consistency, convergence, and almost unbiasedness of the sample version. The advantages of MGC are illustrated via a comprehensive set of simulations with linear, nonlinear, univariate, multivariate, and noisy dependencies, where it loses almost no power in monotone dependencies while achieving better performance in general dependencies, compared to Dcorr and other popular methods. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Cencheng Shen and Carey E. Priebe and Joshua T. Vogelstein},
  doi          = {10.1080/01621459.2018.1543125},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {280-291},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {From distance correlation to multiscale graph correlation},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quantile co-movement in financial markets: A panel quantile
model with unobserved heterogeneity. <em>JASA</em>, <em>115</em>(529),
266–279. (<a
href="https://doi.org/10.1080/01621459.2018.1543598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article introduces a new procedure for analyzing the quantile co-movement of a large number of financial time series based on a large-scale panel data model with factor structures. The proposed method attempts to capture the unobservable heterogeneity of each of the financial time series based on sensitivity to explanatory variables and to the unobservable factor structure. In our model, the dimension of the common factor structure varies across quantiles, and the explanatory variables is allowed to depend on the factor structure. The proposed method allows for both cross-sectional and serial dependence, and heteroscedasticity, which are common in financial markets. We propose new estimation procedures for both frequentist and Bayesian frameworks. Consistency and asymptotic normality of the proposed estimator are established. We also propose a new model selection criterion for determining the number of common factors together with theoretical support. We apply the method to analyze the returns for over 6000 international stocks from over 60 countries during the subprime crisis, European sovereign debt crisis, and subsequent period. The empirical analysis indicates that the common factor structure varies across quantiles. We find that the common factors for the quantiles and the common factors for the mean are different. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Tomohiro Ando and Jushan Bai},
  doi          = {10.1080/01621459.2018.1543598},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {266-279},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Quantile co-movement in financial markets: A panel quantile model with unobserved heterogeneity},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive huber regression. <em>JASA</em>, <em>115</em>(529),
254–265. (<a
href="https://doi.org/10.1080/01621459.2018.1543124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big data can easily be contaminated by outliers or contain variables with heavy-tailed distributions, which makes many conventional methods inadequate. To address this challenge, we propose the adaptive Huber regression for robust estimation and inference. The key observation is that the robustification parameter should adapt to the sample size, dimension and moments for optimal tradeoff between bias and robustness. Our theoretical framework deals with heavy-tailed distributions with bounded ( 1 + δ ) ( 1 + δ ) (1+δ) th moment for any δ &gt; 0 δ &gt; 0 δ&amp;gt;0 . We establish a sharp phase transition for robust estimation of regression parameters in both low and high dimensions: when δ ≥ 1 , the estimator admits a sub-Gaussian-type deviation bound without sub-Gaussian assumptions on the data, while only a slower rate is available in the regime 0 &lt; δ &lt; 1 and the transition is smooth and optimal. In addition, we extend the methodology to allow both heavy-tailed predictors and observation noise. Simulation studies lend further support to the theory. In a genetic study of cancer cell lines that exhibit heavy-tailedness, the proposed methods are shown to be more robust and predictive. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Qiang Sun and Wen-Xin Zhou and Jianqing Fan},
  doi          = {10.1080/01621459.2018.1543124},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {254-265},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Adaptive huber regression},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparametric imputation by data depth. <em>JASA</em>,
<em>115</em>(529), 241–253. (<a
href="https://doi.org/10.1080/01621459.2018.1543123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present single imputation method for missing values which borrows the idea of data depth—a measure of centrality defined for an arbitrary point of a space with respect to a probability distribution or data cloud. This consists in iterative maximization of the depth of each observation with missing values, and can be employed with any properly defined statistical depth function. For each single iteration, imputation reverts to optimization of quadratic, linear, or quasiconcave functions that are solved analytically by linear programming or the Nelder–Mead method. As it accounts for the underlying data topology, the procedure is distribution free, allows imputation close to the data geometry, can make prediction in situations where local imputation ( k -nearest neighbors, random forest) cannot, and has attractive robustness and asymptotic properties under elliptical symmetry. It is shown that a special case—when using the Mahalanobis depth—has direct connection to well-known methods for the multivariate normal model, such as iterated regression and regularized PCA. The methodology is extended to multiple imputation for data stemming from an elliptically symmetric distribution. Simulation and real data studies show good results compared with existing popular alternatives. The method has been implemented as an R-package. Supplementary materials for the article are available online.},
  archive      = {J_JASA},
  author       = {Pavlo Mozharovskyi and Julie Josse and François Husson},
  doi          = {10.1080/01621459.2018.1543123},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {241-253},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Nonparametric imputation by data depth},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation of heterogeneous individual treatment effects
with endogenous treatments. <em>JASA</em>, <em>115</em>(529), 231–240.
(<a href="https://doi.org/10.1080/01621459.2018.1543121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article estimates individual treatment effects (ITE) and its probability distribution in a triangular model with binary-valued endogenous treatments. Our estimation procedure takes two steps. First, we estimate the counterfactual outcome and hence, the ITE for every observational unit in the sample. Second, we estimate the ITE density function of the whole population. Our estimation method does not suffer from the ill-posed inverse problem associated with inverting a nonlinear functional. Asymptotic properties of the proposed method are established. We study its finite sample properties in Monte Carlo experiments. We also illustrate our approach with an empirical application assessing the effects of 401(k) retirement programs on personal savings. Our results show that there exists a small but statistically significant proportion of individuals who experience negative effects, although the majority of ITEs is positive. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Qian Feng and Quang Vuong and Haiqing Xu},
  doi          = {10.1080/01621459.2018.1543121},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {231-240},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Estimation of heterogeneous individual treatment effects with endogenous treatments},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On high-dimensional constrained maximum likelihood
inference. <em>JASA</em>, <em>115</em>(529), 217–230. (<a
href="https://doi.org/10.1080/01621459.2018.1540986">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Inference in a high-dimensional situation may involve regularization of a certain form to treat overparameterization, imposing challenges to inference. The common practice of inference uses either a regularized model, as in inference after model selection, or bias-reduction known as “debias.” While the first ignores statistical uncertainty inherent in regularization, the second reduces the bias inbred in regularization at the expense of increased variance. In this article, we propose a constrained maximum likelihood method for hypothesis testing involving unspecific nuisance parameters, with a focus of alleviating the impact of regularization on inference. Particularly, for general composite hypotheses, we unregularize hypothesized parameters whereas regularizing nuisance parameters through a L 0 -constraint controlling the degree of sparseness. This approach is analogous to semiparametric likelihood inference in a high-dimensional situation. On this ground, for the Gaussian graphical model and linear regression, we derive conditions under which the asymptotic distribution of the constrained likelihood ratio is established, permitting parameter dimension increasing with the sample size. Interestingly, the corresponding limiting distribution is the chi-square or normal, depending on if the co-dimension of a test is finite or increases with the sample size, leading to asymptotic similar tests. This goes beyond the classical Wilks phenomenon. Numerically, we demonstrate that the proposed method performs well against it competitors in various scenarios. Finally, we apply the proposed method to infer linkages in brain network analysis based on MRI data, to contrast Alzheimer’s disease patients against healthy subjects. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Yunzhang Zhu and Xiaotong Shen and Wei Pan},
  doi          = {10.1080/01621459.2018.1540986},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {217-230},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {On high-dimensional constrained maximum likelihood inference},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simultaneous estimation and variable selection for
interval-censored data with broken adaptive ridge regression.
<em>JASA</em>, <em>115</em>(529), 204–216. (<a
href="https://doi.org/10.1080/01621459.2018.1537922">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The simultaneous estimation and variable selection for Cox model has been discussed by several authors when one observes right-censored failure time data. However, there does not seem to exist an established procedure for interval-censored data, a more general and complex type of failure time data, except two parametric procedures. To address this, we propose a broken adaptive ridge (BAR) regression procedure that combines the strengths of the quadratic regularization and the adaptive weighted bridge shrinkage. In particular, the method allows for the number of covariates to be diverging with the sample size. Under some weak regularity conditions, unlike most of the existing variable selection methods, we establish both the oracle property and the grouping effect of the proposed BAR procedure. An extensive simulation study is conducted and indicates that the proposed approach works well in practical situations and deals with the collinearity problem better than the other oracle-like methods. An application is also provided.},
  archive      = {J_JASA},
  author       = {Hui Zhao and Qiwei Wu and Gang Li and Jianguo Sun},
  doi          = {10.1080/01621459.2018.1537922},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {204-216},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Simultaneous estimation and variable selection for interval-censored data with broken adaptive ridge regression},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian repulsive gaussian mixture model. <em>JASA</em>,
<em>115</em>(529), 187–203. (<a
href="https://doi.org/10.1080/01621459.2018.1537918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a general class of Bayesian repulsive Gaussian mixture models that encourage well-separated clusters, aiming at reducing potentially redundant components produced by independent priors for locations (such as the Dirichlet process). The asymptotic results for the posterior distribution of the proposed models are derived, including posterior consistency and posterior contraction rate in the context of nonparametric density estimation. More importantly, we show that compared to the independent prior on the component centers, the repulsive prior introduces additional shrinkage effect on the tail probability of the posterior number of components, which serves as a measurement of the model complexity. In addition, a generalized urn model that allows a random number of components and correlated component centers is developed based on the exchangeable partition distribution, which gives rise to the corresponding blocked-collapsed Gibbs sampler for posterior inference. We evaluate the performance and demonstrate the advantages of the proposed methodology through extensive simulation studies and real data analysis. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Fangzheng Xie and Yanxun Xu},
  doi          = {10.1080/01621459.2018.1537918},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {187-203},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Bayesian repulsive gaussian mixture model},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On degrees of freedom of projection estimators with
applications to multivariate nonparametric regression. <em>JASA</em>,
<em>115</em>(529), 173–186. (<a
href="https://doi.org/10.1080/01621459.2018.1537917">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Abstract– In this article, we consider the nonparametric regression problem with multivariate predictors. We provide a characterization of the degrees of freedom and divergence for estimators of the unknown regression function, which are obtained as outputs of linearly constrained quadratic optimization procedures; namely, minimizers of the least-squares criterion with linear constraints and/or quadratic penalties. As special cases of our results, we derive explicit expressions for the degrees of freedom in many nonparametric regression problems, for example, bounded isotonic regression, multivariate (penalized) convex regression, and additive total variation regularization. Our theory also yields, as special cases, known results on the degrees of freedom of many well-studied estimators in the statistics literature, such as ridge regression, Lasso and generalized Lasso. Our results can be readily used to choose the tuning parameter(s) involved in the estimation procedure by minimizing the Stein’s unbiased risk estimate. As a by-product of our analysis we derive an interesting connection between bounded isotonic regression and isotonic regression on a general partially ordered set, which is of independent interest. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Xi Chen and Qihang Lin and Bodhisattva Sen},
  doi          = {10.1080/01621459.2018.1537917},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {173-186},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {On degrees of freedom of projection estimators with applications to multivariate nonparametric regression},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sensitivity analysis for unmeasured confounding in
meta-analyses. <em>JASA</em>, <em>115</em>(529), 163–172. (<a
href="https://doi.org/10.1080/01621459.2018.1529598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random-effects meta-analyses of observational studies can produce biased estimates if the synthesized studies are subject to unmeasured confounding. We propose sensitivity analyses quantifying the extent to which unmeasured confounding of specified magnitude could reduce to below a certain threshold the proportion of true effect sizes that are scientifically meaningful. We also develop converse methods to estimate the strength of confounding capable of reducing the proportion of scientifically meaningful true effects to below a chosen threshold. These methods apply when a “bias factor” is assumed to be normally distributed across studies or is assessed across a range of fixed values. Our estimators are derived using recently proposed sharp bounds on confounding bias within a single study that do not make assumptions regarding the unmeasured confounders themselves or the functional form of their relationships with the exposure and outcome of interest. We provide an R package, EValue, and a free website that compute point estimates and inference and produce plots for conducting such sensitivity analyses. These methods facilitate principled use of random-effects meta-analyses of observational studies to assess the strength of causal evidence for a hypothesis. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Maya B. Mathur and Tyler J. VanderWeele},
  doi          = {10.1080/01621459.2018.1529598},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {163-172},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Sensitivity analysis for unmeasured confounding in meta-analyses},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). From fixed-x to random-x regression: Bias-variance
decompositions, covariance penalties, and prediction error estimation:
rejoinder. <em>JASA</em>, <em>115</em>(529), 161–162. (<a
href="https://doi.org/10.1080/01621459.2020.1727236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Saharon Rosset and Ryan J. Tibshirani},
  doi          = {10.1080/01621459.2020.1727236},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {161-162},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {From fixed-X to random-X regression: bias-variance decompositions, covariance penalties, and prediction error estimation: rejoinder},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cross-validation, risk estimation, and model selection:
Comment on a paper by rosset and tibshirani. <em>JASA</em>,
<em>115</em>(529), 157–160. (<a
href="https://doi.org/10.1080/01621459.2020.1727235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Stefan Wager},
  doi          = {10.1080/01621459.2020.1727235},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {157-160},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Cross-validation, risk estimation, and model selection: Comment on a paper by rosset and tibshirani},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discussion of “from fixed-x to random-x regression:
Bias-variance decompositions, covariance penalties, and prediction error
estimation.” <em>JASA</em>, <em>115</em>(529), 152–156. (<a
href="https://doi.org/10.1080/01621459.2018.1543597">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JASA},
  author       = {Xiaotong Shen and Hsin-Cheng Huang},
  doi          = {10.1080/01621459.2018.1543597},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {152-156},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Discussion of “From fixed-X to random-X regression: Bias-variance decompositions, covariance penalties, and prediction error estimation”},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). From fixed-x to random-x regression: Bias-variance
decompositions, covariance penalties, and prediction error estimation.
<em>JASA</em>, <em>115</em>(529), 138–151. (<a
href="https://doi.org/10.1080/01621459.2018.1424632">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In statistical prediction, classical approaches for model selection and model evaluation based on covariance penalties are still widely used. Most of the literature on this topic is based on what we call the “Fixed-X” assumption, where covariate values are assumed to be nonrandom. By contrast, it is often more reasonable to take a “Random-X” view, where the covariate values are independently drawn for both training and prediction. To study the applicability of covariance penalties in this setting, we propose a decomposition of Random-X prediction error in which the randomness in the covariates contributes to both the bias and variance components. This decomposition is general, but we concentrate on the fundamental case of ordinary least-squares (OLS) regression. We prove that in this setting the move from Fixed-X to Random-X prediction results in an increase in both bias and variance. When the covariates are normally distributed and the linear model is unbiased, all terms in this decomposition are explicitly computable, which yields an extension of Mallows’ Cp that we call RCp. RCp also holds asymptotically for certain classes of nonnormal covariates. When the noise variance is unknown, plugging in the usual unbiased estimate leads to an approach that we call RCp ^ , which is closely related to Sp, and generalized cross-validation (GCV). For excess bias, we propose an estimate based on the “shortcut-formula” for ordinary cross-validation (OCV), resulting in an approach we call RCp + . Theoretical arguments and numerical simulations suggest that RCp + is typically superior to OCV, though the difference is small. We further examine the Random-X error of other popular estimators. The surprising result we get for ridge regression is that, in the heavily regularized regime, Random-X variance is smaller than Fixed-X variance, which can lead to smaller overall Random-X error. Supplementary materials for this article are available online.},
  archive      = {J_JASA},
  author       = {Saharon Rosset and Ryan J. Tibshirani},
  doi          = {10.1080/01621459.2018.1424632},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {138-151},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {From fixed-X to random-X regression: Bias-variance decompositions, covariance penalties, and prediction error estimation},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generic inference on quantile and quantile effect functions
for discrete outcomes. <em>JASA</em>, <em>115</em>(529), 123–137. (<a
href="https://doi.org/10.1080/01621459.2019.1611581">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantile and quantile effect (QE) functions are important tools for descriptive and causal analysis due to their natural and intuitive interpretation. Existing inference methods for these functions do not apply to discrete random variables. This article offers a simple, practical construction of simultaneous confidence bands for quantile and QE functions of possibly discrete random variables. It is based on a natural transformation of simultaneous confidence bands for distribution functions, which are readily available for many problems. The construction is generic and does not depend on the nature of the underlying problem. It works in conjunction with parametric, semiparametric, and nonparametric modeling methods for observed and counterfactual distributions, and does not depend on the sampling scheme. We apply our method to characterize the distributional impact of insurance coverage on health care utilization and obtain the distributional decomposition of the racial test score gap. We find that universal insurance coverage increases the number of doctor visits across the entire distribution, and that the racial test score gap is small at early ages but grows with age due to socio-economic factors especially at the top of the distribution. Supplementary materials (additional results, R package, replication files) for this article are available online.},
  archive      = {J_JASA},
  author       = {Victor Chernozhukov and Iván Fernández-Val and Blaise Melly and Kaspar Wüthrich},
  doi          = {10.1080/01621459.2019.1611581},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {123-137},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Generic inference on quantile and quantile effect functions for discrete outcomes},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Penalized and constrained optimization: An application to
high-dimensional website advertising. <em>JASA</em>, <em>115</em>(529),
107–122. (<a
href="https://doi.org/10.1080/01621459.2019.1609970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Firms are increasingly transitioning advertising budgets to Internet display campaigns, but this transition poses new challenges. These campaigns use numerous potential metrics for success (e.g., reach or click rate ), and because each website represents a separate advertising opportunity, this is also an inherently high-dimensional problem. Further, advertisers often have constraints they wish to place on their campaign, such as targeting specific sub-populations or websites. These challenges require a method flexible enough to accommodate thousands of websites, as well as numerous metrics and campaign constraints. Motivated by this application, we consider the general constrained high-dimensional problem, where the parameters satisfy linear constraints. We develop the Penalized and Constrained optimization method (PaC) to compute the solution path for high-dimensional, linearly constrained criteria. PaC is extremely general; in addition to internet advertising, we show it encompasses many other potential applications, such as portfolio estimation, monotone curve estimation, and the generalized lasso. Computing the PaC coefficient path poses technical challenges, but we develop an efficient algorithm over a grid of tuning parameters. Through extensive simulations, we show PaC performs well. Finally, we apply PaC to a proprietary dataset in an exemplar Internet advertising case study and demonstrate its superiority over existing methods in this practical setting. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Gareth M. James and Courtney Paulson and Paat Rusmevichientong},
  doi          = {10.1080/01621459.2019.1609970},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {107-122},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Penalized and constrained optimization: An application to high-dimensional website advertising},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quantile function on scalar regression analysis for
distributional data. <em>JASA</em>, <em>115</em>(529), 90–106. (<a
href="https://doi.org/10.1080/01621459.2019.1609969">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Radiomics involves the study of tumor images to identify quantitative markers explaining cancer heterogeneity. The predominant approach is to extract hundreds to thousands of image features, including histogram features comprised of summaries of the marginal distribution of pixel intensities, which leads to multiple testing problems and can miss out on insights not contained in the selected features. In this paper, we present methods to model the entire marginal distribution of pixel intensities via the quantile function as functional data, regressed on a set of demographic, clinical, and genetic predictors to investigate their effects of imaging-based cancer heterogeneity. We call this approach quantile functional regression , regressing subject-specific marginal distributions across repeated measurements on a set of covariates, allowing us to assess which covariates are associated with the distribution in a global sense, as well as to identify distributional features characterizing these differences, including mean, variance, skewness, heavy-tailedness, and various upper and lower quantiles. To account for smoothness in the quantile functions, account for intrafunctional correlation, and gain statistical power, we introduce custom basis functions we call quantlets that are sparse, regularized, near-lossless, and empirically defined, adapting to the features of a given dataset and containing a Gaussian subspace so non-Gaussianness can be assessed. We fit this model using a Bayesian framework that uses nonlinear shrinkage of quantlet coefficients to regularize the functional regression coefficients and provides fully Bayesian inference after fitting a Markov chain Monte Carlo. We demonstrate the benefit of the basis space modeling through simulation studies, and apply the method to Magnetic resonance imaging (MRI)-based radiomic dataset from Glioblastoma Multiforme to relate imaging-based quantile functions to various demographic, clinical, and genetic predictors, finding specific differences in tumor pixel intensity distribution between males and females and between tumors with and without DDIT3 mutations. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Hojin Yang and Veerabhadran Baladandayuthapani and Arvind U.K. Rao and Jeffrey S. Morris},
  doi          = {10.1080/01621459.2019.1609969},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {90-106},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Quantile function on scalar regression analysis for distributional data},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mapping tumor-specific expression QTLs in impure tumor
samples. <em>JASA</em>, <em>115</em>(529), 79–89. (<a
href="https://doi.org/10.1080/01621459.2019.1609968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The study of gene expression quantitative trait loci (eQTL) is an effective approach to illuminate the functional roles of genetic variants. Computational methods have been developed for eQTL mapping using gene expression data from microarray or RNA-seq technology. Application of these methods for eQTL mapping in tumor tissues is problematic because tumor tissues are composed of both tumor and infiltrating normal cells (e.g., immune cells) and eQTL effects may vary between tumor and infiltrating normal cells. To address this challenge, we have developed a new method for eQTL mapping using RNA-seq data from tumor samples. Our method separately estimates the eQTL effects in tumor and infiltrating normal cells using both total expression and allele-specific expression (ASE). We demonstrate that our method controls Type I error rate and has higher power than some alternative approaches. We applied our method to study RNA-seq data from The Cancer Genome Atlas and illustrated the similarities and differences of eQTL effects in tumor and normal cells. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {DouglasR. Wilson and JosephG. Ibrahim and Wei Sun},
  doi          = {10.1080/01621459.2019.1609968},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {79-89},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Mapping tumor-specific expression QTLs in impure tumor samples},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling bronchiolitis incidence proportions in the presence
of spatio-temporal uncertainty. <em>JASA</em>, <em>115</em>(529), 66–78.
(<a href="https://doi.org/10.1080/01621459.2019.1609480">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bronchiolitis (inflammation of the lower respiratory tract) in infants is primarily due to viral infection and is the single most common cause of infant hospitalization in the United States. To increase epidemiological understanding of bronchiolitis (and, subsequently, develop better prevention strategies), this research analyzes data on infant bronchiolitis cases from the U.S. Military Health System between the years 2003–2013 in Norfolk, Virginia, USA. For privacy reasons, child home addresses, birth dates, and diagnosis dates were randomized (jittered) creating spatio-temporal uncertainty in the geographic location and timing of bronchiolitis incidents. Using spatio-temporal point patterns, we created a modeling strategy that accounts for the jittering to estimate and quantify the uncertainty for the incidence proportion (IP) of bronchiolitis. Additionally, we regress the IP onto key covariates including pollution where we adequately account for uncertainty in the pollution levels (i.e., covariate uncertainty) using a land use regression model. Our analysis results indicate that the IP is positively associated with sulfur dioxide and population density. Further, we demonstrate how scientific conclusions may change if various sources of uncertainty (either spatio-temporal or covariate uncertainty) are not accounted for. Code submitted with this article was checked by an Associate Editor for Reproducibility and is available as an online supplement.},
  archive      = {J_JASA},
  author       = {Matthew J. Heaton and Candace Berrett and Sierra Pugh and Amber Evans and Chantel Sloan},
  doi          = {10.1080/01621459.2019.1609480},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {66-78},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Modeling bronchiolitis incidence proportions in the presence of spatio-temporal uncertainty},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Demand models with random partitions. <em>JASA</em>,
<em>115</em>(529), 47–65. (<a
href="https://doi.org/10.1080/01621459.2019.1604360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many economic models of consumer demand require researchers to partition sets of products or attributes prior to the analysis. These models are common in applied problems when the product space is large or spans multiple categories. While the partition is traditionally fixed a priori, we let the partition be a model parameter and propose a Bayesian method for inference. The challenge is that demand systems are commonly multivariate models that are not conditionally conjugate with respect to partition indices, precluding the use of Gibbs sampling. We solve this problem by constructing a new location-scale partition distribution that can generate random-walk Metropolis–Hastings proposals and also serve as a prior. Our method is illustrated in the context of a store-level category demand model, where we find that allowing for partition uncertainty is important for preserving model flexibility, improving demand forecasts, and learning about the structure of demand. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Adam N. Smith and Greg M. Allenby},
  doi          = {10.1080/01621459.2019.1604360},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {47-65},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Demand models with random partitions},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Prediction and inference with missing data in patient alert
systems. <em>JASA</em>, <em>115</em>(529), 32–46. (<a
href="https://doi.org/10.1080/01621459.2019.1604359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe the Bedside Patient Rescue (BPR) project, the goal of which is risk prediction of adverse events for non-intensive care unit patients using ∼100 variables (vitals, lab results, assessments, etc.). There are several missing predictor values for most patients, which in the health sciences is the norm, rather than the exception. A Bayesian approach is presented that addresses many of the shortcomings to standard approaches to missing predictors: (i) treatment of the uncertainty due to imputation is straight-forward in the Bayesian paradigm, (ii) the predictor distribution is flexibly modeled as an infinite normal mixture with latent variables to explicitly account for discrete predictors (i.e., as in multivariate probit regression models), and (iii) certain missing not at random situations can be handled effectively by allowing the indicator of missingness into the predictor distribution only to inform the distribution of the missing variables. The proposed approach also has the benefit of providing a distribution for the prediction, including the uncertainty inherent in the imputation. Therefore, we can ask questions such as: is it possible this individual is at high risk but we are missing too much information to know for sure? How much would we reduce the uncertainty in our risk prediction by obtaining a particular missing value? This approach is applied to the BPR problem resulting in excellent predictive capability to identify deteriorating patients. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement.},
  archive      = {J_JASA},
  author       = {Curtis B. Storlie and Terry M. Therneau and Rickey E. Carter and Nicholas Chia and John R. Bergquist and Jeanne M. Huddleston and Santiago Romero-Brufau},
  doi          = {10.1080/01621459.2019.1604359},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {32-46},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {Prediction and inference with missing data in patient alert systems},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bayesian approach to multistate hidden markov models:
Application to dementia progression. <em>JASA</em>, <em>115</em>(529),
16–31. (<a href="https://doi.org/10.1080/01621459.2019.1594831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {People are living longer than ever before, and with this arises new complications and challenges for humanity. Among the most pressing of these challenges is of understanding the role of aging in the development of dementia. This article is motivated by the Mayo Clinic Study of Aging data for 4742 subjects since 2004, and how it can be used to draw inference on the role of aging in the development of dementia. We construct a hidden Markov model (HMM) to represent progression of dementia from states associated with the buildup of amyloid plaque in the brain, and the loss of cortical thickness. A hierarchical Bayesian approach is taken to estimate the parameters of the HMM with a truly time-inhomogeneous infinitesimal generator matrix, and response functions of the continuous-valued biomarker measurements are cut-point agnostic. A Bayesian approach with these features could be useful in many disease progression models. Additionally, an approach is illustrated for correcting a common bias in delayed enrollment studies, in which some or all subjects are not observed at baseline. Standard software is incapable of accounting for this critical feature, so code to perform the estimation of the model described below is made available online. Code submitted with this article was checked by an Associate Editor for Reproducibility and is available as an online supplement.},
  archive      = {J_JASA},
  author       = {Jonathan P. Williams and Curtis B. Storlie and Terry M. Therneau and Clifford R. Jack Jr and Jan Hannig},
  doi          = {10.1080/01621459.2019.1594831},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {16-31},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A bayesian approach to multistate hidden markov models: Application to dementia progression},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hierarchical model of nonhomogeneous poisson processes for
twitter retweets. <em>JASA</em>, <em>115</em>(529), 1–15. (<a
href="https://doi.org/10.1080/01621459.2019.1585358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a hierarchical model of nonhomogeneous Poisson processes (NHPP) for information diffusion on online social media, in particular Twitter retweets. The retweets of each original tweet are modelled by a NHPP, for which the intensity function is a product of time-decaying components and another component that depends on the follower count of the original tweet author. The latter allows us to explain or predict the ultimate retweet count by a network centrality-related covariate. The inference algorithm enables the Bayes factor to be computed, to facilitate model selection. Finally, the model is applied to the retweet datasets of two hashtags. Supplementary materials for this article, including a standardized description of the materials available for reproducing the work, are available as an online supplement},
  archive      = {J_JASA},
  author       = {Clement Lee and Darren J. Wilkinson},
  doi          = {10.1080/01621459.2019.1585358},
  journal      = {Journal of the American Statistical Association},
  number       = {529},
  pages        = {1-15},
  shortjournal = {J. Am. Stat. Assoc.},
  title        = {A hierarchical model of nonhomogeneous poisson processes for twitter retweets},
  volume       = {115},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
