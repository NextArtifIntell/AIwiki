<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>BIOMET_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="biomet---73">BIOMET - 73</h2>
<ul>
<li><details>
<summary>
(2020). Classification via local manifold approximation.
<em>BIOMET</em>, <em>107</em>(4), 1013–1020. (<a
href="https://doi.org/10.1093/biomet/asaa033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classifiers label data as belonging to one of a set of groups based on input features. It is challenging to achieve accurate classification when the feature distributions in the different classes are complex, with nonlinear, overlapping and intersecting supports. This is particularly true when training data are limited. To address this problem, we propose a new type of classifier based on obtaining a local approximation to the support of the data within each class in a neighbourhood of the feature to be classified, and assigning the feature to the class having the closest support. This general algorithm is referred to as local manifold approximation classification. As a simple and theoretically supported special case, which is shown to have excellent performance across a broad variety of examples, we use spheres for local approximation, obtaining a spherical approximation classifier.},
  archive      = {J_BIOMET},
  author       = {Li, Didong and Dunson, David B},
  doi          = {10.1093/biomet/asaa033},
  journal      = {Biometrika},
  number       = {4},
  pages        = {1013-1020},
  shortjournal = {Biometrika},
  title        = {Classification via local manifold approximation},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient posterior sampling for high-dimensional imbalanced
logistic regression. <em>BIOMET</em>, <em>107</em>(4), 1005–1012. (<a
href="https://doi.org/10.1093/biomet/asaa035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Classification with high-dimensional data is of widespread interest and often involves dealing with imbalanced data. Bayesian classification approaches are hampered by the fact that current Markov chain Monte Carlo algorithms for posterior computation become inefficient as the number |$p$| of predictors or the number |$n$| of subjects to classify gets large, because of the increasing computational time per step and worsening mixing rates. One strategy is to employ a gradient-based sampler to improve mixing while using data subsamples to reduce the per-step computational complexity. However, the usual subsampling breaks down when applied to imbalanced data. Instead, we generalize piecewise-deterministic Markov chain Monte Carlo algorithms to include importance-weighted and mini-batch subsampling. These maintain the correct stationary distribution with arbitrarily small subsamples and substantially outperform current competitors. We provide theoretical support for the proposed approach and demonstrate its performance gains in simulated data examples and an application to cancer data.},
  archive      = {J_BIOMET},
  author       = {Sen, Deborshee and Sachs, Matthias and Lu, Jianfeng and Dunson, David B},
  doi          = {10.1093/biomet/asaa035},
  journal      = {Biometrika},
  number       = {4},
  pages        = {1005-1012},
  shortjournal = {Biometrika},
  title        = {Efficient posterior sampling for high-dimensional imbalanced logistic regression},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Extended stochastic gradient markov chain monte carlo for
large-scale bayesian variable selection. <em>BIOMET</em>,
<em>107</em>(4), 997–1004. (<a
href="https://doi.org/10.1093/biomet/asaa029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stochastic gradient Markov chain Monte Carlo algorithms have received much attention in Bayesian computing for big data problems, but they are only applicable to a small class of problems for which the parameter space has a fixed dimension and the log-posterior density is differentiable with respect to the parameters. This paper proposes an extended stochastic gradient Markov chain Monte Carlo algorithm which, by introducing appropriate latent variables, can be applied to more general large-scale Bayesian computing problems, such as those involving dimension jumping and missing data. Numerical studies show that the proposed algorithm is highly scalable and much more efficient than traditional Markov chain Monte Carlo algorithms.},
  archive      = {J_BIOMET},
  author       = {Song, Qifan and Sun, Yan and Ye, Mao and Liang, Faming},
  doi          = {10.1093/biomet/asaa029},
  journal      = {Biometrika},
  number       = {4},
  pages        = {997-1004},
  shortjournal = {Biometrika},
  title        = {Extended stochastic gradient markov chain monte carlo for large-scale bayesian variable selection},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A unified approach to the calculation of information
operators in semiparametric models. <em>BIOMET</em>, <em>107</em>(4),
983–995. (<a href="https://doi.org/10.1093/biomet/asaa037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The infinite-dimensional information operator for the nuisance parameter plays a key role in semiparametric inference, as it is closely related to the regular estimability of the target parameter. Calculation of information operators has traditionally proceeded in a case-by-case manner and has often entailed lengthy derivations with complicated arguments. We develop a unified framework for this task by exploiting commonality in the form of semiparametric likelihoods. The general formula developed allows one to derive information operators with simple calculus and, if necessary at all, a minimal amount of probabilistic evaluation. This streamlined approach shows its simplicity and versatility in application to a number of existing models as well as a new model of practical interest.},
  archive      = {J_BIOMET},
  author       = {Mao, Lu},
  doi          = {10.1093/biomet/asaa037},
  journal      = {Biometrika},
  number       = {4},
  pages        = {983-995},
  shortjournal = {Biometrika},
  title        = {A unified approach to the calculation of information operators in semiparametric models},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Envelopes in multivariate regression models with
nonlinearity and heteroscedasticity. <em>BIOMET</em>, <em>107</em>(4),
965–981. (<a href="https://doi.org/10.1093/biomet/asaa036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Envelopes have been proposed in recent years as a nascent methodology for sufficient dimension reduction and efficient parameter estimation in multivariate linear models. We extend the classical definition of envelopes in Cook et al. (2010) to incorporate a nonlinear conditional mean function and a heteroscedastic error. Given any two random vectors |${X}\in\mathbb{R}^{p}$| and |${Y}\in\mathbb{R}^{r}$|⁠ , we propose two new model-free envelopes, called the martingale difference divergence envelope and the central mean envelope, and study their relationships to the standard envelope in the context of response reduction in multivariate linear models. The martingale difference divergence envelope effectively captures the nonlinearity in the conditional mean without imposing any parametric structure or requiring any tuning in estimation. Heteroscedasticity, or nonconstant conditional covariance of |${Y}\mid{X}$|⁠ , is further detected by the central mean envelope based on a slicing scheme for the data. We reveal the nested structure of different envelopes: (i) the central mean envelope contains the martingale difference divergence envelope, with equality when |${Y}\mid{X}$| has a constant conditional covariance; and (ii) the martingale difference divergence envelope contains the standard envelope, with equality when |${Y}\mid{X}$| has a linear conditional mean. We develop an estimation procedure that first obtains the martingale difference divergence envelope and then estimates the additional envelope components in the central mean envelope. We establish consistency in envelope estimation of the martingale difference divergence envelope and central mean envelope without stringent model assumptions. Simulations and real-data analysis demonstrate the advantages of the martingale difference divergence envelope and the central mean envelope over the standard envelope in dimension reduction.},
  archive      = {J_BIOMET},
  author       = {Zhang, X and Lee, C E and Shao, X},
  doi          = {10.1093/biomet/asaa036},
  journal      = {Biometrika},
  number       = {4},
  pages        = {965-981},
  shortjournal = {Biometrika},
  title        = {Envelopes in multivariate regression models with nonlinearity and heteroscedasticity},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). General regression model for the subdistribution of a
competing risk under left-truncation and right-censoring.
<em>BIOMET</em>, <em>107</em>(4), 949–964. (<a
href="https://doi.org/10.1093/biomet/asaa034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Left-truncation poses extra challenges for the analysis of complex time-to-event data. We propose a general semiparametric regression model for left-truncated and right-censored competing risks data that is based on a novel weighted conditional likelihood function. Targeting the subdistribution hazard, our parameter estimates are directly interpretable with regard to the cumulative incidence function. We compare different weights from recent literature and develop a heuristic interpretation from a cure model perspective that is based on pseudo risk sets. Our approach accommodates external time-dependent covariate effects on the subdistribution hazard. We establish consistency and asymptotic normality of the estimators and propose a sandwich estimator of the variance. In comprehensive simulation studies we demonstrate solid performance of the proposed method. Comparing the sandwich estimator with the inverse Fisher information matrix, we observe a bias for the inverse Fisher information matrix and diminished coverage probabilities in settings with a higher percentage of left-truncation. To illustrate the practical utility of the proposed method, we study its application to a large HIV vaccine efficacy trial dataset.},
  archive      = {J_BIOMET},
  author       = {Bellach, A and Kosorok, M R and Gilbert, P B and Fine, J P},
  doi          = {10.1093/biomet/asaa034},
  journal      = {Biometrika},
  number       = {4},
  pages        = {949-964},
  shortjournal = {Biometrika},
  title        = {General regression model for the subdistribution of a competing risk under left-truncation and right-censoring},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Regression-adjusted average treatment effect estimates in
stratified randomized experiments. <em>BIOMET</em>, <em>107</em>(4),
935–948. (<a href="https://doi.org/10.1093/biomet/asaa038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear regression is often used in the analysis of randomized experiments to improve treatment effect estimation by adjusting for imbalances of covariates in the treatment and control groups. This article proposes a randomization-based inference framework for regression adjustment in stratified randomized experiments. We re-establish, under mild conditions, the finite-population central limit theorem for a stratified experiment, and we prove that both the stratified difference-in-means estimator and the regression-adjusted average treatment effect estimator are consistent and asymptotically normal; the asymptotic variance of the latter is no greater and typically less than that of the former. We also provide conservative variance estimators that can be used to construct large-sample confidence intervals for the average treatment effect.},
  archive      = {J_BIOMET},
  author       = {Liu, Hanzhong and Yang, Yuehan},
  doi          = {10.1093/biomet/asaa038},
  journal      = {Biometrika},
  number       = {4},
  pages        = {935-948},
  shortjournal = {Biometrika},
  title        = {Regression-adjusted average treatment effect estimates in stratified randomized experiments},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Demystifying a class of multiply robust estimators.
<em>BIOMET</em>, <em>107</em>(4), 919–933. (<a
href="https://doi.org/10.1093/biomet/asaa026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For estimating the population mean of a response variable subject to ignorable missingness, a new class of methods, called multiply robust procedures, has been proposed. The advantage of multiply robust procedures over the traditional doubly robust methods is that they permit the use of multiple candidate models for both the propensity score and the outcome regression, and they are consistent if any one of the multiple models is correctly specified, a property termed multiple robustness. This paper shows that, somewhat surprisingly, multiply robust estimators are special cases of doubly robust estimators, where the final propensity score and outcome regression models are certain combinations of the candidate models. To further improve model specifications in the doubly robust estimators, we adapt a model mixing procedure as an alternative method for combining multiple candidate models. We show that multiple robustness and asymptotic normality can also be achieved by our mixing-based doubly robust estimator. Moreover, our estimator and its theoretical properties are not confined to parametric models. Numerical examples demonstrate that the proposed estimator is comparable to and can even outperform existing multiply robust estimators.},
  archive      = {J_BIOMET},
  author       = {Li, Wei and Gu, Yuwen and Liu, Lan},
  doi          = {10.1093/biomet/asaa026},
  journal      = {Biometrika},
  number       = {4},
  pages        = {919-933},
  shortjournal = {Biometrika},
  title        = {Demystifying a class of multiply robust estimators},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On specification tests for composite likelihood inference.
<em>BIOMET</em>, <em>107</em>(4), 907–917. (<a
href="https://doi.org/10.1093/biomet/asaa039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Composite likelihood functions are often used for inference in applications where the data have a complex structure. While inference based on the composite likelihood can be more robust than inference based on the full likelihood, the inference is not valid if the associated conditional or marginal models are misspecified. In this paper, we propose a general class of specification tests for composite likelihood inference. The test statistics are motivated by the fact that the second Bartlett identity holds for each component of the composite likelihood function when these components are correctly specified. We construct the test statistics based on the discrepancy between the so-called composite information matrix and the sensitivity matrix. As an illustration, we study three important cases of the proposed tests and establish their limiting distributions under both null and local alternative hypotheses. Finally, we evaluate the finite-sample performance of the proposed tests in several examples.},
  archive      = {J_BIOMET},
  author       = {Huang, Jing and Ning, Yang and Reid, Nancy and Chen, Yong},
  doi          = {10.1093/biomet/asaa039},
  journal      = {Biometrika},
  number       = {4},
  pages        = {907-917},
  shortjournal = {Biometrika},
  title        = {On specification tests for composite likelihood inference},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The pitman–yor multinomial process for mixture modelling.
<em>BIOMET</em>, <em>107</em>(4), 891–906. (<a
href="https://doi.org/10.1093/biomet/asaa030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Discrete nonparametric priors play a central role in a variety of Bayesian procedures, most notably when used to model latent features, such as in clustering, mixtures and curve fitting. They are effective and well-developed tools, though their infinite dimensionality is unsuited to some applications. If one restricts to a finite-dimensional simplex, very little is known beyond the traditional Dirichlet multinomial process, which is mainly motivated by conjugacy. This paper introduces an alternative based on the Pitman–Yor process, which provides greater flexibility while preserving analytical tractability. Urn schemes and posterior characterizations are obtained in closed form, leading to exact sampling methods. In addition, the proposed approach can be used to accurately approximate the infinite-dimensional Pitman–Yor process, yielding improvements over existing truncation-based approaches. An application to convex mixture regression for quantitative risk assessment illustrates the theoretical results and compares our approach with existing methods.},
  archive      = {J_BIOMET},
  author       = {Lijoi, Antonio and Prünster, Igor and Rigon, Tommaso},
  doi          = {10.1093/biomet/asaa030},
  journal      = {Biometrika},
  number       = {4},
  pages        = {891-906},
  shortjournal = {Biometrika},
  title        = {The Pitman–Yor multinomial process for mixture modelling},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal bayesian estimation for random dot product graphs.
<em>BIOMET</em>, <em>107</em>(4), 875–889. (<a
href="https://doi.org/10.1093/biomet/asaa031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose and prove the optimality of a Bayesian approach for estimating the latent positions in random dot product graphs, which we call posterior spectral embedding. Unlike classical spectral-based adjacency, or Laplacian spectral embedding, posterior spectral embedding is a fully likelihood-based graph estimation method that takes advantage of the Bernoulli likelihood information of the observed adjacency matrix. We develop a minimax lower bound for estimating the latent positions, and show that posterior spectral embedding achieves this lower bound in the following two senses: it both results in a minimax-optimal posterior contraction rate and yields a point estimator achieving the minimax risk asymptotically. The convergence results are subsequently applied to clustering in stochastic block models with positive semidefinite block probability matrices, strengthening an existing result concerning the number of misclustered vertices. We also study a spectral-based Gaussian spectral embedding as a natural Bayesian analogue of adjacency spectral embedding, but the resulting posterior contraction rate is suboptimal by an extra logarithmic factor. The practical performance of the proposed methodology is illustrated through extensive synthetic examples and the analysis of Wikipedia graph data.},
  archive      = {J_BIOMET},
  author       = {Xie, Fangzheng and Xu, Yanxun},
  doi          = {10.1093/biomet/asaa031},
  journal      = {Biometrika},
  number       = {4},
  pages        = {875-889},
  shortjournal = {Biometrika},
  title        = {Optimal bayesian estimation for random dot product graphs},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inference under unequal probability sampling with the
bayesian exponentially tilted empirical likelihood. <em>BIOMET</em>,
<em>107</em>(4), 857–873. (<a
href="https://doi.org/10.1093/biomet/asaa028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully Bayesian inference in the presence of unequal probability sampling requires stronger structural assumptions on the data-generating distribution than frequentist semiparametric methods, but offers the potential for improved small-sample inference and convenient evidence synthesis. We demonstrate that the Bayesian exponentially tilted empirical likelihood can be used to combine the practical benefits of Bayesian inference with the robustness and attractive large-sample properties of frequentist approaches. Estimators defined as the solutions to unbiased estimating equations can be used to define a semiparametric model through the set of corresponding moment constraints. We prove Bernstein–von Mises theorems which show that the posterior constructed from the resulting exponentially tilted empirical likelihood becomes approximately normal, centred at the chosen estimator with matching asymptotic variance; thus, the posterior has properties analogous to those of the estimator, such as double robustness, and the frequentist coverage of any credible set will be approximately equal to its credibility. The proposed method can be used to obtain modified versions of existing estimators with improved properties, such as guarantees that the estimator lies within the parameter space. Unlike existing Bayesian proposals, our method does not prescribe a particular choice of prior or require posterior variance correction, and simulations suggest that it provides superior performance in terms of frequentist criteria.},
  archive      = {J_BIOMET},
  author       = {Yiu, A and Goudie, R J B and Tom, B D M},
  doi          = {10.1093/biomet/asaa028},
  journal      = {Biometrika},
  number       = {4},
  pages        = {857-873},
  shortjournal = {Biometrika},
  title        = {Inference under unequal probability sampling with the bayesian exponentially tilted empirical likelihood},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation in linear errors-in-variables models with unknown
error distribution. <em>BIOMET</em>, <em>107</em>(4), 841–856. (<a
href="https://doi.org/10.1093/biomet/asaa025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Parameter estimation in linear errors-in-variables models typically requires that the measurement error distribution be known or estimable from replicate data. A generalized method of moments approach can be used to estimate model parameters in the absence of knowledge of the error distributions, but it requires the existence of a large number of model moments. In this paper, parameter estimation based on the phase function, a normalized version of the characteristic function, is considered. This approach requires the model covariates to have asymmetric distributions, while the error distributions are symmetric. Parameters are estimated by minimizing a distance function between the empirical phase functions of the noisy covariates and the outcome variable. No knowledge of the measurement error distribution is needed to calculate this estimator. Both asymptotic and finite-sample properties of the estimator are studied. The connection between the phase function approach and method of moments is also discussed. The estimation of standard errors is considered and a modified bootstrap algorithm for fast computation is proposed. The newly proposed estimator is competitive with the generalized method of moments, despite making fewer model assumptions about the moment structure of the measurement error. Finally, the proposed method is applied to a real dataset containing measurements of air pollution levels.},
  archive      = {J_BIOMET},
  author       = {Nghiem, Linh H and Byrd, Michael C and Potgieter, Cornelis J},
  doi          = {10.1093/biomet/asaa025},
  journal      = {Biometrika},
  number       = {4},
  pages        = {841-856},
  shortjournal = {Biometrika},
  title        = {Estimation in linear errors-in-variables models with unknown error distribution},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A conditional test with demonstrated insensitivity to
unmeasured bias in matched observational studies. <em>BIOMET</em>,
<em>107</em>(4), 827–840. (<a
href="https://doi.org/10.1093/biomet/asaa032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In an observational study matched for observed covariates, an association between treatment received and outcome exhibited may indicate not an effect caused by the treatment, but merely some bias in the allocation of treatments to individuals within matched pairs. The evidence that distinguishes moderate biases from causal effects is unevenly dispersed among possible comparisons in an observational study: some comparisons are insensitive to larger biases than others. Intuitively, larger treatment effects tend to be insensitive to larger unmeasured biases, and perhaps matched pairs can be grouped using covariates, doses or response patterns so that groups of pairs with larger treatment effects may be identified. Even if an investigator has a reasoned conjecture about where to look for insensitive comparisons, that conjecture might prove mistaken, or, when not mistaken, it might be received sceptically by other scientists who doubt the conjecture or judge it to be too convenient in light of its success with the data at hand. In this article a test is proposed that searches for insensitive findings over many comparisons, but controls the probability of falsely rejecting a true null hypothesis of no treatment effect in the presence of a bias of specified magnitude. An example is studied in which the test considers many comparisons and locates an interpretable comparison that is insensitive to larger biases than a conventional comparison based on Wilcoxon’s signed rank statistic applied to all pairs. A simulation examines the power of the proposed test. The method is implemented in the R package dstat , which contains the example and reproduces the analysis.},
  archive      = {J_BIOMET},
  author       = {Rosenbaum, P R},
  doi          = {10.1093/biomet/asaa032},
  journal      = {Biometrika},
  number       = {4},
  pages        = {827-840},
  shortjournal = {Biometrika},
  title        = {A conditional test with demonstrated insensitivity to unmeasured bias in matched observational studies},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multivariate one-sided testing in matched observational
studies as an adversarial game. <em>BIOMET</em>, <em>107</em>(4),
809–825. (<a href="https://doi.org/10.1093/biomet/asaa024">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a multivariate one-sided sensitivity analysis for matched observational studies, appropriate when the researcher has specified that a given causal mechanism should manifest itself in effects on multiple outcome variables in a known direction. The test statistic can be thought of as the solution to an adversarial game, where the researcher determines the best linear combination of test statistics to combat nature’s presentation of the worst-case pattern of hidden bias. The corresponding optimization problem is convex, and can be solved efficiently even for reasonably sized observational studies. Asymptotically, the test statistic converges to a chi-bar-squared distribution under the null, a common distribution in order-restricted statistical inference. The test attains the largest possible design sensitivity over a class of coherent test statistics, and facilitates one-sided sensitivity analyses for individual outcome variables while maintaining familywise error control through its incorporation into closed testing procedures.},
  archive      = {J_BIOMET},
  author       = {Cohen, P L and Olson, M A and Fogarty, C B},
  doi          = {10.1093/biomet/asaa024},
  journal      = {Biometrika},
  number       = {4},
  pages        = {809-825},
  shortjournal = {Biometrika},
  title        = {Multivariate one-sided testing in matched observational studies as an adversarial game},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Combining p-values via averaging. <em>BIOMET</em>,
<em>107</em>(4), 791–808. (<a
href="https://doi.org/10.1093/biomet/asaa027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes general methods for the problem of multiple testing of a single hypothesis, with a standard goal of combining a number of |$p$| -values without making any assumptions about their dependence structure. A result by Rüschendorf (1982) and, independently, Meng (1993) implies that the |$p$| -values can be combined by scaling up their arithmetic mean by a factor of 2, and no smaller factor is sufficient in general. A similar result by Mattner about the geometric mean replaces 2 by e. Based on more recent developments in mathematical finance, specifically, robust risk aggregation techniques, we extend these results to generalized means; in particular, we show that |$K$| |$p$| -values can be combined by scaling up their harmonic mean by a factor of |$\log K$| asymptotically as |$K$| tends to infinity. This leads to a generalized version of the Bonferroni–Holm procedure. We also explore methods using weighted averages of |$p$| -values. Finally, we discuss the efficiency of various methods of combining |$p$| -values and how to choose a suitable method in light of data and prior information.},
  archive      = {J_BIOMET},
  author       = {Vovk, Vladimir and Wang, Ruodu},
  doi          = {10.1093/biomet/asaa027},
  journal      = {Biometrika},
  number       = {4},
  pages        = {791-808},
  shortjournal = {Biometrika},
  title        = {Combining p-values via averaging},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On testing marginal versus conditional independence.
<em>BIOMET</em>, <em>107</em>(4), 771–790. (<a
href="https://doi.org/10.1093/biomet/asaa040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider testing marginal independence versus conditional independence in a trivariate Gaussian setting. The two models are nonnested, and their intersection is a union of two marginal independences. We consider two sequences of such models, one from each type of independence, that are closest to each other in the Kullback–Leibler sense as they approach the intersection. They become indistinguishable if the signal strength, as measured by the product of two correlation parameters, decreases faster than the standard parametric rate. Under local alternatives at such a rate, we show that the asymptotic distribution of the likelihood ratio depends on where and how the local alternatives approach the intersection. To deal with this nonuniformity, we study a class of envelope distributions by taking pointwise suprema over asymptotic cumulative distribution functions. We show that these envelope distributions are well behaved and lead to model selection procedures with rate-free uniform error guarantees and near-optimal power. To control the error even when the two models are indistinguishable, rather than insist on a dichotomous choice, the proposed procedure will choose either or both models.},
  archive      = {J_BIOMET},
  author       = {Guo, F Richard and Richardson, Thomas S},
  doi          = {10.1093/biomet/asaa040},
  journal      = {Biometrika},
  number       = {4},
  pages        = {771-790},
  shortjournal = {Biometrika},
  title        = {On testing marginal versus conditional independence},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). “Unbiased hamiltonian monte carlo with couplings.”
<em>BIOMET</em>, <em>107</em>(3), 769. (<a
href="https://doi.org/10.1093/biomet/asaa060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMET},
  author       = {Heng, J and Jacob, P E},
  doi          = {10.1093/biomet/asaa060},
  journal      = {Biometrika},
  number       = {3},
  pages        = {769},
  shortjournal = {Biometrika},
  title        = {‘Unbiased hamiltonian monte carlo with couplings’},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast closed testing for exchangeable local tests.
<em>BIOMET</em>, <em>107</em>(3), 761–768. (<a
href="https://doi.org/10.1093/biomet/asz082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple hypothesis testing problems arise naturally in science. This note introduces a new fast closed testing method for multiple testing which controls the familywise error rate. Controlling the familywise error rate is state-of-the-art in many important application areas and is preferred over false discovery rate control for many reasons, including that it leads to stronger reproducibility. The closure principle rejects an individual hypothesis if all global nulls of subsets containing it are rejected using some test statistics. It takes exponential time in the worst case. When the tests are symmetric and monotone, the proposed method is an exact algorithm for computing the closure, is quadratic in the number of tests, and is linear in the number of discoveries. Our framework generalizes most examples of closed testing, such as Holm’s method and the Bonferroni method. As a special case of the method, we propose the Simes and higher criticism fusion test, which is powerful both for detecting a few strong signals and for detecting many moderate signals.},
  archive      = {J_BIOMET},
  author       = {Dobriban, E},
  doi          = {10.1093/biomet/asz082},
  journal      = {Biometrika},
  number       = {3},
  pages        = {761-768},
  shortjournal = {Biometrika},
  title        = {Fast closed testing for exchangeable local tests},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bootstrapping m-estimators in generalized autoregressive
conditional heteroscedastic models. <em>BIOMET</em>, <em>107</em>(3),
753–760. (<a href="https://doi.org/10.1093/biomet/asaa023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the weighted bootstrap approximation to the distribution of a class of M-estimators for the parameters of the generalized autoregressive conditional heteroscedastic model. We prove that the bootstrap distribution, given the data, is a consistent estimate in probability of the distribution of the M-estimator, which is asymptotically normal. We propose an algorithm for the computation of M-estimates which at the same time is useful for computing bootstrap replicates from the given data. Our simulation study indicates superior coverage rates for various weighted bootstrap schemes compared with the rates based on the normal approximation and existing bootstrap methods for the generalized autoregressive conditional heteroscedastic model, such as percentile |$t$| -subsampling schemes. Since some familiar bootstrap schemes are special cases of the weighted bootstrap, this paper thus provides a unified theory and algorithm for bootstrapping in generalized autoregressive conditional heteroscedastic models.},
  archive      = {J_BIOMET},
  author       = {Mukherjee, K},
  doi          = {10.1093/biomet/asaa023},
  journal      = {Biometrika},
  number       = {3},
  pages        = {753-760},
  shortjournal = {Biometrika},
  title        = {Bootstrapping M-estimators in generalized autoregressive conditional heteroscedastic models},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian cumulative shrinkage for infinite factorizations.
<em>BIOMET</em>, <em>107</em>(3), 745–752. (<a
href="https://doi.org/10.1093/biomet/asaa008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The dimension of the parameter space is typically unknown in a variety of models that rely on factorizations. For example, in factor analysis the number of latent factors is not known and has to be inferred from the data. Although classical shrinkage priors are useful in such contexts, increasing shrinkage priors can provide a more effective approach that progressively penalizes expansions with growing complexity. In this article we propose a novel increasing shrinkage prior, called the cumulative shrinkage process, for the parameters that control the dimension in overcomplete formulations. Our construction has broad applicability and is based on an interpretable sequence of spike-and-slab distributions which assign increasing mass to the spike as the model complexity grows. Using factor analysis as an illustrative example, we show that this formulation has theoretical and practical advantages relative to current competitors, including an improved ability to recover the model dimension. An adaptive Markov chain Monte Carlo algorithm is proposed, and the performance gains are outlined in simulations and in an application to personality data.},
  archive      = {J_BIOMET},
  author       = {Legramanti, Sirio and Durante, Daniele and Dunson, David B},
  doi          = {10.1093/biomet/asaa008},
  journal      = {Biometrika},
  number       = {3},
  pages        = {745-752},
  shortjournal = {Biometrika},
  title        = {Bayesian cumulative shrinkage for infinite factorizations},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A note on the accuracy of adaptive gauss–hermite quadrature.
<em>BIOMET</em>, <em>107</em>(3), 737–744. (<a
href="https://doi.org/10.1093/biomet/asz080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Numerical quadrature methods are needed for many models in order to approximate integrals in the likelihood function. In this note, we correct the error rate given by Liu &amp; Pierce (1994) for integrals approximated with adaptive Gauss–Hermite quadrature and show that the approximation is less accurate than previously thought. We discuss the relationship between the error rates of adaptive Gauss–Hermite quadrature and Laplace approximation, and provide a theoretical explanation of simulation results obtained in previous studies regarding the accuracy of adaptive Gauss–Hermite quadrature.},
  archive      = {J_BIOMET},
  author       = {Jin, Shaobo and Andersson, Björn},
  doi          = {10.1093/biomet/asz080},
  journal      = {Biometrika},
  number       = {3},
  pages        = {737-744},
  shortjournal = {Biometrika},
  title        = {A note on the accuracy of adaptive Gauss–Hermite quadrature},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). More efficient approximation of smoothing splines via
space-filling basis selection. <em>BIOMET</em>, <em>107</em>(3),
723–735. (<a href="https://doi.org/10.1093/biomet/asaa019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of approximating smoothing spline estimators in a nonparametric regression model. When applied to a sample of size |$n$|⁠ , the smoothing spline estimator can be expressed as a linear combination of |$n$| basis functions, requiring |$O(n^3)$| computational time when the number |$d$| of predictors is two or more. Such a sizeable computational cost hinders the broad applicability of smoothing splines. In practice, the full-sample smoothing spline estimator can be approximated by an estimator based on |$q$| randomly selected basis functions, resulting in a computational cost of |$O(nq^2)$|⁠ . It is known that these two estimators converge at the same rate when |$q$| is of order |$O{n^{2/(pr+1)}}$|⁠ , where |$p\in [1,2]$| depends on the true function and |$r &gt; 1$| depends on the type of spline. Such a |$q$| is called the essential number of basis functions. In this article, we develop a more efficient basis selection method. By selecting basis functions corresponding to approximately equally spaced observations, the proposed method chooses a set of basis functions with great diversity. The asymptotic analysis shows that the proposed smoothing spline estimator can decrease |$q$| to around |$O{n^{1/(pr+1)}}$| when |$d\leq pr+1$|⁠ . Applications to synthetic and real-world datasets show that the proposed method leads to a smaller prediction error than other basis selection methods.},
  archive      = {J_BIOMET},
  author       = {Meng, Cheng and Zhang, Xinlian and Zhang, Jingyi and Zhong, Wenxuan and Ma, Ping},
  doi          = {10.1093/biomet/asaa019},
  journal      = {Biometrika},
  number       = {3},
  pages        = {723-735},
  shortjournal = {Biometrika},
  title        = {More efficient approximation of smoothing splines via space-filling basis selection},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Path weights in concentration graphs. <em>BIOMET</em>,
<em>107</em>(3), 705–722. (<a
href="https://doi.org/10.1093/biomet/asaa010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A graphical model provides a compact and efficient representation of the association structure in a multivariate distribution by means of a graph. Relevant features of the distribution are represented by vertices, edges and higher-order graphical structures such as cliques or paths. Typically, paths play a central role in these models because they determine the dependence relationships between variables. However, while a theory of path coefficients is available for directed graph models, little research exists on the strength of the association represented by a path in an undirected graph. Essentially, it has been shown that the covariance between two variables can be decomposed into a sum of weights associated with each of the paths connecting the two variables in the corresponding concentration graph. In this context, we consider concentration graph models and provide an extensive analysis of the properties of path weights and their interpretation. Specifically, we give an interpretation of covariance weights through their factorization into a partial covariance and an inflation factor. We then extend the covariance decomposition over the paths of an undirected graph to other measures of association, such as the marginal correlation coefficient and a quantity that we call the inflated correlation. Application of these results is illustrated with an analysis of dietary intake networks.},
  archive      = {J_BIOMET},
  author       = {Roverato, Alberto and Castelo, Robert},
  doi          = {10.1093/biomet/asaa010},
  journal      = {Biometrika},
  number       = {3},
  pages        = {705-722},
  shortjournal = {Biometrika},
  title        = {Path weights in concentration graphs},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generalized integration model for improved statistical
inference by leveraging external summary data. <em>BIOMET</em>,
<em>107</em>(3), 689–703. (<a
href="https://doi.org/10.1093/biomet/asaa014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-analysis has become a powerful tool for improving inference by gathering evidence from multiple sources. It pools summary-level data from different studies to improve estimation efficiency with the assumption that all participating studies are analysed under the same statistical model. It is challenging to integrate external summary data calculated from different models with a newly conducted internal study in which individual-level data are collected. We develop a novel statistical inference framework that can effectively synthesize internal and external data for the integrative analysis. The new framework is versatile enough to assimilate various types of summary data from multiple sources. We establish asymptotic properties for the proposed procedure and prove that the new estimate is theoretically more efficient than the internal data based maximum likelihood estimate, as well as a recently developed constrained maximum likelihood approach that incorporates the external information. We illustrate an application of our method by evaluating cervical cancer risk using data from a large cervical screening program.},
  archive      = {J_BIOMET},
  author       = {Zhang, Han and Deng, Lu and Schiffman, Mark and Qin, Jing and Yu, Kai},
  doi          = {10.1093/biomet/asaa014},
  journal      = {Biometrika},
  number       = {3},
  pages        = {689-703},
  shortjournal = {Biometrika},
  title        = {Generalized integration model for improved statistical inference by leveraging external summary data},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive critical value for constrained likelihood ratio
testing. <em>BIOMET</em>, <em>107</em>(3), 677–688. (<a
href="https://doi.org/10.1093/biomet/asaa013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new general method for constrained likelihood ratio testing which, when few constraints are violated, improves upon the existing approach in the literature that compares the likelihood ratio with the quantile of a mixture of chi-squared distributions; the improvement is in terms of both simplicity and power. The proposed method compares the constrained likelihood ratio statistic against the quantile of only one chi-squared random variable with data-dependent degrees of freedom. The new test is shown to have a valid exact significance level |$\alpha$|⁠ . It also has more power than the classical approach against alternatives for which the number of violations is not large. We provide more details for testing a simple order |$\mu_1\leqslant\cdots\leqslant\mu_p$| against all alternatives using the proposed approach and give clear guidelines as to when the new method would be advantageous. A simulation study suggests that for testing a simple order, the new approach is more powerful in many scenarios than the existing method that uses a mixture of chi-squared variables. We illustrate the results of our adaptive procedure using real data on the liquidity preference hypothesis.},
  archive      = {J_BIOMET},
  author       = {Al Mohamad, Diaa and Van Zwet, Erik W and Cator, Eric and Goeman, Jelle J},
  doi          = {10.1093/biomet/asaa013},
  journal      = {Biometrika},
  number       = {3},
  pages        = {677-688},
  shortjournal = {Biometrika},
  title        = {Adaptive critical value for constrained likelihood ratio testing},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generalized instrumental inequalities: Testing the
instrumental variable independence assumption. <em>BIOMET</em>,
<em>107</em>(3), 661–675. (<a
href="https://doi.org/10.1093/biomet/asaa003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new set of testable implications for the instrumental variable independence assumption for discrete treatment, but unrestricted outcome and instruments: generalized instrumental inequalities. When outcome and treatment are both binary, but instruments are unrestricted, we show that the generalized instrumental inequalities are necessary and sufficient to detect all observable violations of the instrumental variable independence assumption. To test the generalized instrumental inequalities, we propose an approach combining a sample splitting procedure and an inference method for intersection bounds. This idea allows one to easily implement the test using existing Stata packages. We apply our proposed strategy to assess the validity of the instrumental variable independence assumption for various instruments used in the returns to college literature.},
  archive      = {J_BIOMET},
  author       = {Kédagni, Désiré and Mourifié, Ismael},
  doi          = {10.1093/biomet/asaa003},
  journal      = {Biometrika},
  number       = {3},
  pages        = {661-675},
  shortjournal = {Biometrika},
  title        = {Generalized instrumental inequalities: Testing the instrumental variable independence assumption},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A robust method for shift detection in time series.
<em>BIOMET</em>, <em>107</em>(3), 647–660. (<a
href="https://doi.org/10.1093/biomet/asaa004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a robust and nonparametric test for the presence of a changepoint in a time series, based on the two-sample Hodges–Lehmann estimator. We develop new limit theory for a class of statistics based on two-sample U-quantile processes in the case of short-range dependent observations. Using this theory, we derive the asymptotic distribution of our test statistic under the null hypothesis of a constant level. The proposed test shows better overall performance under normal, heavy-tailed and skewed distributions than several other modifications of the popular cumulative sums test based on U-statistics, one-sample U-quantiles or M-estimation. The new theory does not involve moment conditions, so any transform of the observed process can be used to test the stability of higher-order characteristics such as variability, skewness and kurtosis.},
  archive      = {J_BIOMET},
  author       = {Dehling, H and Fried, R and Wendler, M},
  doi          = {10.1093/biomet/asaa004},
  journal      = {Biometrika},
  number       = {3},
  pages        = {647-660},
  shortjournal = {Biometrika},
  title        = {A robust method for shift detection in time series},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spatial blind source separation. <em>BIOMET</em>,
<em>107</em>(3), 627–646. (<a
href="https://doi.org/10.1093/biomet/asz079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently a blind source separation model was suggested for spatial data, along with an estimator based on the simultaneous diagonalization of two scatter matrices. The asymptotic properties of this estimator are derived here, and a new estimator based on the joint diagonalization of more than two scatter matrices is proposed. The asymptotic properties and merits of the novel estimator are verified in simulation studies. A real-data example illustrates application of the method.},
  archive      = {J_BIOMET},
  author       = {Bachoc, François and Genton, Marc G and Nordhausen, Klaus and Ruiz-Gazen, Anne and Virta, Joni},
  doi          = {10.1093/biomet/asz079},
  journal      = {Biometrika},
  number       = {3},
  pages        = {627-646},
  shortjournal = {Biometrika},
  title        = {Spatial blind source separation},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sparse semiparametric canonical correlation analysis for
data of mixed types. <em>BIOMET</em>, <em>107</em>(3), 609–625. (<a
href="https://doi.org/10.1093/biomet/asaa007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Canonical correlation analysis investigates linear relationships between two sets of variables, but it often works poorly on modern datasets because of high dimensionality and mixed data types such as continuous, binary and zero-inflated. To overcome these challenges, we propose a semiparametric approach to sparse canonical correlation analysis based on the Gaussian copula. The main result of this paper is a truncated latent Gaussian copula model for data with excess zeros, which allows us to derive a rank-based estimator of the latent correlation matrix for mixed variable types without estimation of marginal transformation functions. The resulting canonical correlation analysis method works well in high-dimensional settings, as demonstrated via numerical studies, and when applied to the analysis of association between gene expression and microRNA data from breast cancer patients.},
  archive      = {J_BIOMET},
  author       = {Yoon, Grace and Carroll, Raymond J and Gaynanova, Irina},
  doi          = {10.1093/biomet/asaa007},
  journal      = {Biometrika},
  number       = {3},
  pages        = {609-625},
  shortjournal = {Biometrika},
  title        = {Sparse semiparametric canonical correlation analysis for data of mixed types},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Empirical likelihood test for a large-dimensional mean
vector. <em>BIOMET</em>, <em>107</em>(3), 591–607. (<a
href="https://doi.org/10.1093/biomet/asaa005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is concerned with empirical likelihood inference on the population mean when the dimension |$p$| and the sample size |$n$| satisfy |$p/n\rightarrow c\in [1,\infty)$|⁠ . As shown in Tsao (2004) , the empirical likelihood method fails with high probability when |$p/n&gt;1/2$| because the convex hull of the |$n$| observations in |$\mathbb{R}^p$| becomes too small to cover the true mean value. Moreover, when |$p&gt; n$|⁠ , the sample covariance matrix becomes singular, and this results in the breakdown of the first sandwich approximation for the log empirical likelihood ratio. To deal with these two challenges, we propose a new strategy of adding two artificial data points to the observed data. We establish the asymptotic normality of the proposed empirical likelihood ratio test. The proposed test statistic does not involve the inverse of the sample covariance matrix. Furthermore, its form is explicit, so the test can easily be carried out with low computational cost. Our numerical comparison shows that the proposed test outperforms some existing tests for high-dimensional mean vectors in terms of power. We also illustrate the proposed procedure with an empirical analysis of stock data.},
  archive      = {J_BIOMET},
  author       = {Cui, Xia and Li, Runze and Yang, Guangren and Zhou, Wang},
  doi          = {10.1093/biomet/asaa005},
  journal      = {Biometrika},
  number       = {3},
  pages        = {591-607},
  shortjournal = {Biometrika},
  title        = {Empirical likelihood test for a large-dimensional mean vector},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation and inference for the indirect effect in
high-dimensional linear mediation models. <em>BIOMET</em>,
<em>107</em>(3), 573–589. (<a
href="https://doi.org/10.1093/biomet/asaa016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mediation analysis is difficult when the number of potential mediators is larger than the sample size. In this paper we propose new inference procedures for the indirect effect in the presence of high-dimensional mediators for linear mediation models. We develop methods for both incomplete mediation, where a direct effect may exist, and complete mediation, where the direct effect is known to be absent. We prove consistency and asymptotic normality of our indirect effect estimators. Under complete mediation, where the indirect effect is equivalent to the total effect, we further prove that our approach gives a more powerful test compared to directly testing for the total effect. We confirm our theoretical results in simulations, as well as in an integrative analysis of gene expression and genotype data from a pharmacogenomic study of drug response. We present a novel analysis of gene sets to understand the molecular mechanisms of drug response, and also identify a genome-wide significant noncoding genetic variant that cannot be detected using standard analysis methods.},
  archive      = {J_BIOMET},
  author       = {Zhou, Ruixuan Rachel and Wang, Liewei and Zhao, Sihai Dave},
  doi          = {10.1093/biomet/asaa016},
  journal      = {Biometrika},
  number       = {3},
  pages        = {573-589},
  shortjournal = {Biometrika},
  title        = {Estimation and inference for the indirect effect in high-dimensional linear mediation models},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A nonparametric approach to high-dimensional k-sample
comparison problems. <em>BIOMET</em>, <em>107</em>(3), 555–572. (<a
href="https://doi.org/10.1093/biomet/asaa015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional |$k$| -sample comparison is a common task in applications. We construct a class of easy-to-implement distribution-free tests based on new nonparametric tools and unexplored connections with spectral graph theory. The test is shown to have various desirable properties and a characteristic exploratory flavour that has practical consequences for statistical modelling. Numerical examples show that the proposed method works surprisingly well across a broad range of realistic situations.},
  archive      = {J_BIOMET},
  author       = {Mukhopadhyay, Subhadeep and Wang, Kaijun},
  doi          = {10.1093/biomet/asaa015},
  journal      = {Biometrika},
  number       = {3},
  pages        = {555-572},
  shortjournal = {Biometrika},
  title        = {A nonparametric approach to high-dimensional k-sample comparison problems},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust estimation of causal effects via a high-dimensional
covariate balancing propensity score. <em>BIOMET</em>, <em>107</em>(3),
533–554. (<a href="https://doi.org/10.1093/biomet/asaa020">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a robust method to estimate the average treatment effects in observational studies when the number of potential confounders is possibly much greater than the sample size. Our method consists of three steps. We first use a class of penalized |$M$| -estimators for the propensity score and outcome models. We then calibrate the initial estimate of the propensity score by balancing a carefully selected subset of covariates that are predictive of the outcome. Finally, the estimated propensity score is used to construct the inverse probability weighting estimator. We prove that the proposed estimator, which we call the high-dimensional covariate balancing propensity score, has the sample boundedness property, is root- |$n$| consistent, asymptotically normal, and semiparametrically efficient when the propensity score model is correctly specified and the outcome model is linear in covariates. More importantly, we show that our estimator remains root- |$n$| consistent and asymptotically normal so long as either the propensity score model or the outcome model is correctly specified. We provide valid confidence intervals in both cases and further extend these results to the case where the outcome model is a generalized linear model. In simulation studies, we find that the proposed methodology often estimates the average treatment effect more accurately than existing methods. We also present an empirical application, in which we estimate the average causal effect of college attendance on adulthood political participation. An open-source software package is available for implementing the proposed methodology.},
  archive      = {J_BIOMET},
  author       = {Ning, Yang and Sida, Peng and Imai, Kosuke},
  doi          = {10.1093/biomet/asaa020},
  journal      = {Biometrika},
  number       = {3},
  pages        = {533-554},
  shortjournal = {Biometrika},
  title        = {Robust estimation of causal effects via a high-dimensional covariate balancing propensity score},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Determining the dependence structure of multivariate
extremes. <em>BIOMET</em>, <em>107</em>(3), 513–532. (<a
href="https://doi.org/10.1093/biomet/asaa018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In multivariate extreme value analysis, the nature of the extremal dependence between variables should be considered when selecting appropriate statistical models. Interest often lies in determining which subsets of variables can take their largest values simultaneously while the others are of smaller order. Our approach to this problem exploits hidden regular variation properties on a collection of nonstandard cones, and provides a new set of indices that reveal aspects of the extremal dependence structure not available through existing measures of dependence. We derive theoretical properties of these indices, demonstrate their utility through a series of examples, and develop methods of inference that also estimate the proportion of extremal mass associated with each cone. We apply the methods to river flows in the U.K., estimating the probabilities of different subsets of sites being large simultaneously.},
  archive      = {J_BIOMET},
  author       = {Simpson, E S and Wadsworth, J L and Tawn, J A},
  doi          = {10.1093/biomet/asaa018},
  journal      = {Biometrika},
  number       = {3},
  pages        = {513-532},
  shortjournal = {Biometrika},
  title        = {Determining the dependence structure of multivariate extremes},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A random-perturbation-based rank estimator of the number of
factors. <em>BIOMET</em>, <em>107</em>(2), 505–511. (<a
href="https://doi.org/10.1093/biomet/asz073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a random-perturbation-based rank estimator of the number of factors of a large-dimensional approximate factor model. An expansion of the rank estimator demonstrates that the random perturbation reduces the biases due to the persistence of the factor series and the dependence between the factor and error series. A central limit theorem for the rank estimator with convergence rate higher than root |$n$| gives a new hypothesis-testing procedure for both one-sided and two-sided alternatives. Simulation studies verify the performance of the test.},
  archive      = {J_BIOMET},
  author       = {Kong, Xinbing},
  doi          = {10.1093/biomet/asz073},
  journal      = {Biometrika},
  number       = {2},
  pages        = {505-511},
  shortjournal = {Biometrika},
  title        = {A random-perturbation-based rank estimator of the number of factors},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Consistency for the tree bootstrap in respondent-driven
sampling. <em>BIOMET</em>, <em>107</em>(2), 497–504. (<a
href="https://doi.org/10.1093/biomet/asz067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Respondent-driven sampling is an approach for estimating features of populations that are difficult to access using standard survey tools, e.g., the fraction of injection drug users who are HIV positive. Baraff et al. (2016) introduced an approach to estimating uncertainty in population proportion estimates from respondent-driven sampling using the tree bootstrap method. In this paper we establish the consistency of this tree bootstrap approach in the case of |$m$| -trees.},
  archive      = {J_BIOMET},
  author       = {Green, A K B and McCormick, T H and Raftery, A E},
  doi          = {10.1093/biomet/asz067},
  journal      = {Biometrika},
  number       = {2},
  pages        = {497-504},
  shortjournal = {Biometrika},
  title        = {Consistency for the tree bootstrap in respondent-driven sampling},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the marginal likelihood and cross-validation.
<em>BIOMET</em>, <em>107</em>(2), 489–496. (<a
href="https://doi.org/10.1093/biomet/asz077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Bayesian statistics, the marginal likelihood, also known as the evidence, is used to evaluate model fit as it quantifies the joint probability of the data under the prior. In contrast, non-Bayesian models are typically compared using cross-validation on held-out data, either through |$k$| -fold partitioning or leave- |$p$| -out subsampling. We show that the marginal likelihood is formally equivalent to exhaustive leave- |$p$| -out crossvalidation averaged over all values of |$p$| and all held-out test sets when using the log posterior predictive probability as the scoring rule. Moreover, the log posterior predictive score is the only coherent scoring rule under data exchangeability. This offers new insight into the marginal likelihood and cross-validation, and highlights the potential sensitivity of the marginal likelihood to the choice of the prior. We suggest an alternative approach using cumulative cross-validation following a preparatory training phase. Our work has connections to prequential analysis and intrinsic Bayes factors, but is motivated in a different way.},
  archive      = {J_BIOMET},
  author       = {Fong, E and Holmes, C C},
  doi          = {10.1093/biomet/asz077},
  journal      = {Biometrika},
  number       = {2},
  pages        = {489-496},
  shortjournal = {Biometrika},
  title        = {On the marginal likelihood and cross-validation},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation of error variance via ridge regression.
<em>BIOMET</em>, <em>107</em>(2), 481–488. (<a
href="https://doi.org/10.1093/biomet/asz074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel estimator of error variance and establish its asymptotic properties based on ridge regression and random matrix theory. The proposed estimator is valid under both low- and high-dimensional models, and performs well not only in nonsparse cases, but also in sparse ones. The finite-sample performance of the proposed method is assessed through an intensive numerical study, which indicates that the method is promising compared with its competitors in many interesting scenarios.},
  archive      = {J_BIOMET},
  author       = {Liu, X and Zheng, S and Feng, X},
  doi          = {10.1093/biomet/asz074},
  journal      = {Biometrika},
  number       = {2},
  pages        = {481-488},
  shortjournal = {Biometrika},
  title        = {Estimation of error variance via ridge regression},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust empirical bayes small area estimation with density
power divergence. <em>BIOMET</em>, <em>107</em>(2), 467–480. (<a
href="https://doi.org/10.1093/biomet/asz075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A two-stage normal hierarchical model called the Fay–Herriot model and the empirical Bayes estimator are widely used to obtain indirect and model-based estimates of means in small areas. However, the performance of the empirical Bayes estimator can be poor when the assumed normal distribution is misspecified. This article presents a simple modification that makes use of density power divergence and proposes a new robust empirical Bayes small area estimator. The mean squared error and estimated mean squared error of the proposed estimator are derived based on the asymptotic properties of the robust estimator of the model parameters. We investigate the numerical performance of the proposed method through simulations and an application to survey data.},
  archive      = {J_BIOMET},
  author       = {Sugasawa, S},
  doi          = {10.1093/biomet/asz075},
  journal      = {Biometrika},
  number       = {2},
  pages        = {467-480},
  shortjournal = {Biometrika},
  title        = {Robust empirical bayes small area estimation with density power divergence},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation from cross-sectional data under a semiparametric
truncation model. <em>BIOMET</em>, <em>107</em>(2), 449–465. (<a
href="https://doi.org/10.1093/biomet/asaa002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-sectional sampling is often used when investigating inter-event times, resulting in left-truncated and right-censored data. In this paper, we consider a semiparametric truncation model in which the truncating variable is assumed to belong to a certain parametric family. We examine two methods of estimating both the truncation and the lifetime distributions. We obtain asymptotic representations of the estimators for the lifetime distribution and establish their weak convergence. Both of the proposed estimators perform better than Wang ’s ( 1991 ) nonparametric maximum likelihood estimator in terms of the integrated mean squared error, when the parametric family for the truncation is sufficiently close to its true distribution. The full likelihood approach is preferable to the conditional likelihood approach in estimating the lifetime distribution, though not necessarily the truncation distribution. In an application to Alzheimer’s disease data, hypothesis tests reject the uniform truncation distribution, but several other parametric models lead to similar behaviour of the truncation and lifetime distributions after disease onset.},
  archive      = {J_BIOMET},
  author       = {Heuchenne, C and De Uña-Álvarez, J and Laurent, G},
  doi          = {10.1093/biomet/asaa002},
  journal      = {Biometrika},
  number       = {2},
  pages        = {449-465},
  shortjournal = {Biometrika},
  title        = {Estimation from cross-sectional data under a semiparametric truncation model},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ensemble estimation and variable selection with
semiparametric regression models. <em>BIOMET</em>, <em>107</em>(2),
433–448. (<a href="https://doi.org/10.1093/biomet/asaa012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider scenarios in which the likelihood function for a semiparametric regression model factors into separate components, with an efficient estimator of the regression parameter available for each component. An optimal weighted combination of the component estimators, named an ensemble estimator, may be employed as an overall estimate of the regression parameter, and may be fully efficient under uncorrelatedness conditions. This approach is useful when the full likelihood function may be difficult to maximize, but the components are easy to maximize. It covers settings where the nuisance parameter may be estimated at different rates in the component likelihoods. As a motivating example we consider proportional hazards regression with prospective doubly censored data, in which the likelihood factors into a current status data likelihood and a left-truncated right-censored data likelihood. Variable selection is important in such regression modelling, but the applicability of existing techniques is unclear in the ensemble approach. We propose ensemble variable selection using the least squares approximation technique on the unpenalized ensemble estimator, followed by ensemble re-estimation under the selected model. The resulting estimator has the oracle property such that the set of nonzero parameters is successfully recovered and the semiparametric efficiency bound is achieved for this parameter set. Simulations show that the proposed method performs well relative to alternative approaches. Analysis of an AIDS cohort study illustrates the practical utility of the method.},
  archive      = {J_BIOMET},
  author       = {Shin, Sunyoung and Liu, Yufeng and Cole, Stephen R and Fine, Jason P},
  doi          = {10.1093/biomet/asaa012},
  journal      = {Biometrika},
  number       = {2},
  pages        = {433-448},
  shortjournal = {Biometrika},
  title        = {Ensemble estimation and variable selection with semiparametric regression models},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Doubly functional graphical models in high dimensions.
<em>BIOMET</em>, <em>107</em>(2), 415–431. (<a
href="https://doi.org/10.1093/biomet/asz072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider estimating a functional graphical model from multivariate functional observations. In functional data analysis, the classical assumption is that each function has been measured over a densely sampled grid. However, in practice the functions have often been observed, with measurement error, at a relatively small number of points. We propose a class of doubly functional graphical models to capture the evolving conditional dependence relationship among a large number of sparsely or densely sampled functions. Our approach first implements a nonparametric smoother to perform functional principal components analysis for each curve, then estimates a functional covariance matrix and finally computes sparse precision matrices, which in turn provide the doubly functional graphical model. We derive some novel concentration bounds, uniform convergence rates and model selection properties of our estimator for both sparsely and densely sampled functional data in the high-dimensional large- |$p$|⁠ , small- |$n$| regime. We demonstrate via simulations that the proposed method significantly outperforms possible competitors. Our proposed method is applied to a brain imaging dataset.},
  archive      = {J_BIOMET},
  author       = {Qiao, Xinghao and Qian, Cheng and James, Gareth M and Guo, Shaojun},
  doi          = {10.1093/biomet/asz072},
  journal      = {Biometrika},
  number       = {2},
  pages        = {415-431},
  shortjournal = {Biometrika},
  title        = {Doubly functional graphical models in high dimensions},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lassoing eigenvalues. <em>BIOMET</em>, <em>107</em>(2),
397–414. (<a href="https://doi.org/10.1093/biomet/asz076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The properties of penalized sample covariance matrices depend on the choice of the penalty function. In this paper, we introduce a class of nonsmooth penalty functions for the sample covariance matrix and demonstrate how their use results in a grouping of the estimated eigenvalues. We refer to the proposed method as lassoing eigenvalues, or the elasso.},
  archive      = {J_BIOMET},
  author       = {Tyler, David E and Yi, Mengxi},
  doi          = {10.1093/biomet/asz076},
  journal      = {Biometrika},
  number       = {2},
  pages        = {397-414},
  shortjournal = {Biometrika},
  title        = {Lassoing eigenvalues},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the use of approximate bayesian computation markov chain
monte carlo with inflated tolerance and post-correction.
<em>BIOMET</em>, <em>107</em>(2), 381–395. (<a
href="https://doi.org/10.1093/biomet/asz078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate Bayesian computation enables inference for complicated probabilistic models with intractable likelihoods using model simulations. The Markov chain Monte Carlo implementation of approximate Bayesian computation is often sensitive to the tolerance parameter: low tolerance leads to poor mixing and large tolerance entails excess bias. We propose an approach that involves using a relatively large tolerance for the Markov chain Monte Carlo sampler to ensure sufficient mixing and post-processing the output, leading to estimators for a range of finer tolerances. We introduce an approximate confidence interval for the related post-corrected estimators and propose an adaptive approximate Bayesian computation Markov chain Monte Carlo algorithm, which finds a balanced tolerance level automatically based on acceptance rate optimization. Our experiments show that post-processing-based estimators can perform better than direct Markov chain Monte Carlo targeting a fine tolerance, that our confidence intervals are reliable, and that our adaptive algorithm leads to reliable inference with little user specification.},
  archive      = {J_BIOMET},
  author       = {Vihola, Matti and Franks, Jordan},
  doi          = {10.1093/biomet/asz078},
  journal      = {Biometrika},
  number       = {2},
  pages        = {381-395},
  shortjournal = {Biometrika},
  title        = {On the use of approximate bayesian computation markov chain monte carlo with inflated tolerance and post-correction},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discontinuous hamiltonian monte carlo for discrete
parameters and discontinuous likelihoods. <em>BIOMET</em>,
<em>107</em>(2), 365–380. (<a
href="https://doi.org/10.1093/biomet/asz083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hamiltonian Monte Carlo has emerged as a standard tool for posterior computation. In this article we present an extension that can efficiently explore target distributions with discontinuous densities. Our extension in particular enables efficient sampling from ordinal parameters through the embedding of probability mass functions into continuous spaces. We motivate our approach through a theory of discontinuous Hamiltonian dynamics and develop a corresponding numerical solver. The proposed solver is the first of its kind, with a remarkable ability to exactly preserve the Hamiltonian. We apply our algorithm to challenging posterior inference problems to demonstrate its wide applicability and competitive performance.},
  archive      = {J_BIOMET},
  author       = {Nishimura, Akihiko and Dunson, David B and Lu, Jianfeng},
  doi          = {10.1093/biomet/asz083},
  journal      = {Biometrika},
  number       = {2},
  pages        = {365-380},
  shortjournal = {Biometrika},
  title        = {Discontinuous hamiltonian monte carlo for discrete parameters and discontinuous likelihoods},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The essential histogram. <em>BIOMET</em>, <em>107</em>(2),
347–364. (<a href="https://doi.org/10.1093/biomet/asz081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The histogram is widely used as a simple, exploratory way of displaying data, but it is usually not clear how to choose the number and size of the bins. We construct a confidence set of distribution functions that optimally deal with the two main tasks of the histogram: estimating probabilities and detecting features such as increases and modes in the distribution. We define the essential histogram as the histogram in the confidence set with the fewest bins. Thus the essential histogram is the simplest visualization of the data that optimally achieves the main tasks of the histogram. The only assumption we make is that the data are independent and identically distributed. We provide a fast algorithm for computing the essential histogram and illustrate our method with examples.},
  archive      = {J_BIOMET},
  author       = {Li, Housen and Munk, Axel and Sieling, Hannes and Walther, Guenther},
  doi          = {10.1093/biomet/asz081},
  journal      = {Biometrika},
  number       = {2},
  pages        = {347-364},
  shortjournal = {Biometrika},
  title        = {The essential histogram},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Testing conditional mean independence for functional data.
<em>BIOMET</em>, <em>107</em>(2), 331–346. (<a
href="https://doi.org/10.1093/biomet/asz070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new nonparametric conditional mean independence test for a response variable |$Y$| and a predictor variable |$X$| where either or both can be function-valued. Our test is built on a new metric, the so-called functional martingale difference divergence, which fully characterizes the conditional mean dependence of |$Y$| given |$X$| and extends the martingale difference divergence proposed by Shao &amp; Zhang (2014) . We define an unbiased estimator of functional martingale difference divergence by using a |$\mathcal{U}$| -centring approach, and we obtain its limiting null distribution under mild assumptions. Since the limiting null distribution is not pivotal, we use the wild bootstrap method to estimate the critical value and show the consistency of the bootstrap test. Our test can detect the local alternative which approaches the null at the rate of |$n^{-1/2}$| with a nontrivial power, where |$n$| is the sample size. Unlike the three tests developed by Kokoszka et al. (2008) , Lei (2014) and Patilea et al. (2016) , our test does not require a finite-dimensional projection or assume a linear model, and it does not involve any tuning parameters. Promising finite-sample performance is demonstrated via simulations, and a real-data illustration is used to compare our test with existing ones.},
  archive      = {J_BIOMET},
  author       = {Lee, C E and Zhang, X and Shao, X},
  doi          = {10.1093/biomet/asz070},
  journal      = {Biometrika},
  number       = {2},
  pages        = {331-346},
  shortjournal = {Biometrika},
  title        = {Testing conditional mean independence for functional data},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Classification with imperfect training labels.
<em>BIOMET</em>, <em>107</em>(2), 311–330. (<a
href="https://doi.org/10.1093/biomet/asaa011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the effect of imperfect training data labels on the performance of classification methods. In a general setting, where the probability that an observation in the training dataset is mislabelled may depend on both the feature vector and the true label, we bound the excess risk of an arbitrary classifier trained with imperfect labels in terms of its excess risk for predicting a noisy label. This reveals conditions under which a classifier trained with imperfect labels remains consistent for classifying uncorrupted test data points. Furthermore, under stronger conditions, we derive detailed asymptotic properties for the popular |$k$| -nearest neighbour, support vector machine and linear discriminant analysis classifiers. One consequence of these results is that the |$k$| -nearest neighbour and support vector machine classifiers are robust to imperfect training labels, in the sense that the rate of convergence of the excess risk of these classifiers remains unchanged; in fact, our theoretical and empirical results even show that in some cases, imperfect labels may improve the performance of these methods. The linear discriminant analysis classifier is shown to be typically inconsistent in the presence of label noise unless the prior probabilities of the classes are equal. Our theoretical results are supported by a simulation study.},
  archive      = {J_BIOMET},
  author       = {Cannings, Timothy I and Fan, Yingying and Samworth, Richard J},
  doi          = {10.1093/biomet/asaa011},
  journal      = {Biometrika},
  number       = {2},
  pages        = {311-330},
  shortjournal = {Biometrika},
  title        = {Classification with imperfect training labels},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive nonparametric regression with the k-nearest
neighbour fused lasso. <em>BIOMET</em>, <em>107</em>(2), 293–310. (<a
href="https://doi.org/10.1093/biomet/asz071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fused lasso, also known as total-variation denoising, is a locally adaptive function estimator over a regular grid of design points. In this article, we extend the fused lasso to settings in which the points do not occur on a regular grid, leading to a method for nonparametric regression. This approach, which we call the |$K$| -nearest-neighbours fused lasso, involves computing the |$K$| -nearest-neighbours graph of the design points and then performing the fused lasso over this graph. We show that this procedure has a number of theoretical advantages over competing methods: specifically, it inherits local adaptivity from its connection to the fused lasso, and it inherits manifold adaptivity from its connection to the |$K$| -nearest-neighbours approach. In a simulation study and an application to flu data, we show that excellent results are obtained. For completeness, we also study an estimator that makes use of an |$\epsilon$| -graph rather than a |$K$| -nearest-neighbours graph and contrast it with the |$K$| -nearest-neighbours fused lasso.},
  archive      = {J_BIOMET},
  author       = {Madrid Padilla, Oscar Hernan and Sharpnack, James and Chen, Yanzhen and Witten, Daniela M},
  doi          = {10.1093/biomet/asz071},
  journal      = {Biometrika},
  number       = {2},
  pages        = {293-310},
  shortjournal = {Biometrika},
  title        = {Adaptive nonparametric regression with the K-nearest neighbour fused lasso},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Rejoinder: “Network cross-validation by edge sampling.”
<em>BIOMET</em>, <em>107</em>(2), 289–292. (<a
href="https://doi.org/10.1093/biomet/asaa021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMET},
  author       = {Li, Tianxi and Levina, Elizaveta and Zhu, Ji},
  doi          = {10.1093/biomet/asaa021},
  journal      = {Biometrika},
  number       = {2},
  pages        = {289-292},
  shortjournal = {Biometrika},
  title        = {Rejoinder: ‘Network cross-validation by edge sampling’},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discussion of “network cross-validation by edge sampling.”
<em>BIOMET</em>, <em>107</em>(2), 285–287. (<a
href="https://doi.org/10.1093/biomet/asaa009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMET},
  author       = {Lei, J and Lin, K Z},
  doi          = {10.1093/biomet/asaa009},
  journal      = {Biometrika},
  number       = {2},
  pages        = {285-287},
  shortjournal = {Biometrika},
  title        = {Discussion of ‘Network cross-validation by edge sampling’},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discussion of “network cross-validation by edge sampling.”
<em>BIOMET</em>, <em>107</em>(2), 281–284. (<a
href="https://doi.org/10.1093/biomet/asaa022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMET},
  author       = {Gao, Chao and Ma, Zongming},
  doi          = {10.1093/biomet/asaa022},
  journal      = {Biometrika},
  number       = {2},
  pages        = {281-284},
  shortjournal = {Biometrika},
  title        = {Discussion of ‘Network cross-validation by edge sampling’},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discussion of “network cross-validation by edge sampling.”
<em>BIOMET</em>, <em>107</em>(2), 277–280. (<a
href="https://doi.org/10.1093/biomet/asaa017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMET},
  author       = {Chang, Jinyuan and Kolaczyk, Eric D and Yao, Qiwei},
  doi          = {10.1093/biomet/asaa017},
  journal      = {Biometrika},
  number       = {2},
  pages        = {277-280},
  shortjournal = {Biometrika},
  title        = {Discussion of ‘Network cross-validation by edge sampling’},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Network cross-validation by edge sampling. <em>BIOMET</em>,
<em>107</em>(2), 257–276. (<a
href="https://doi.org/10.1093/biomet/asaa006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While many statistical models and methods are now available for network analysis, resampling of network data remains a challenging problem. Cross-validation is a useful general tool for model selection and parameter tuning, but it is not directly applicable to networks since splitting network nodes into groups requires deleting edges and destroys some of the network structure. In this paper we propose a new network resampling strategy, based on splitting node pairs rather than nodes, that is applicable to cross-validation for a wide range of network model selection tasks. We provide theoretical justification for our method in a general setting and examples of how the method can be used in specific network model selection and parameter tuning tasks. Numerical results on simulated networks and on a statisticians’ citation network show that the proposed cross-validation approach works well for model selection.},
  archive      = {J_BIOMET},
  author       = {Li, Tianxi and Levina, Elizaveta and Zhu, Ji},
  doi          = {10.1093/biomet/asaa006},
  journal      = {Biometrika},
  number       = {2},
  pages        = {257-276},
  shortjournal = {Biometrika},
  title        = {Network cross-validation by edge sampling},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). “Variance estimation in the particle filter.”
<em>BIOMET</em>, <em>107</em>(1), 255. (<a
href="https://doi.org/10.1093/biomet/asaa001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMET},
  author       = {Lee, A and Whiteley, N},
  doi          = {10.1093/biomet/asaa001},
  journal      = {Biometrika},
  number       = {1},
  pages        = {255},
  shortjournal = {Biometrika},
  title        = {‘Variance estimation in the particle filter’},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Diagnosing missing always at random in multivariate data.
<em>BIOMET</em>, <em>107</em>(1), 246–253. (<a
href="https://doi.org/10.1093/biomet/asz061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Models for analysing multivariate datasets with missing values require strong, often unassessable, assumptions. The most common of these is that the mechanism that created the missing data is ignorable, which is a two-fold assumption dependent on the mode of inference. The first part, which is the focus here, under the Bayesian and direct-likelihood paradigms requires that the missing data be missing at random; in contrast, the frequentist-likelihood paradigm demands that the missing data mechanism always produce missing at random data, a condition known as missing always at random. Under certain regularity conditions, assuming missing always at random leads to a condition that can be tested using the observed data alone, namely that the missing data indicators depend only on fully observed variables. In this note we propose three different diagnostic tests that not only indicate when this assumption is incorrect but also suggest which variables are the most likely culprits. Although missing always at random is not a necessary condition to ensure validity under the Bayesian and direct-likelihood paradigms, it is sufficient, and evidence of its violation should encourage the careful statistician to conduct targeted sensitivity analyses.},
  archive      = {J_BIOMET},
  author       = {Bojinov, Iavor I and Pillai, Natesh S and Rubin, Donald B},
  doi          = {10.1093/biomet/asz061},
  journal      = {Biometrika},
  number       = {1},
  pages        = {246-253},
  shortjournal = {Biometrika},
  title        = {Diagnosing missing always at random in multivariate data},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Measurement errors in the binary instrumental variable
model. <em>BIOMET</em>, <em>107</em>(1), 238–245. (<a
href="https://doi.org/10.1093/biomet/asz060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Instrumental variable methods can identify causal effects even when the treatment and outcome are confounded. We study the problem of imperfect measurements of the binary instrumental variable, treatment and outcome. We first consider nondifferential measurement errors, that is, the mismeasured variable does not depend on other variables given its true value. We show that the measurement error of the instrumental variable does not bias the estimate, that the measurement error of the treatment biases the estimate away from zero, and that the measurement error of the outcome biases the estimate toward zero. Moreover, we derive sharp bounds on the causal effects without additional assumptions. These bounds are informative because they exclude zero. We then consider differential measurement errors, and focus on sensitivity analyses in those settings.},
  archive      = {J_BIOMET},
  author       = {Jiang, Zhichao and Ding, Peng},
  doi          = {10.1093/biomet/asz060},
  journal      = {Biometrika},
  number       = {1},
  pages        = {238-245},
  shortjournal = {Biometrika},
  title        = {Measurement errors in the binary instrumental variable model},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysis of grouped data using conjugate generalized linear
mixed models. <em>BIOMET</em>, <em>107</em>(1), 231–237. (<a
href="https://doi.org/10.1093/biomet/asz053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article concerns a class of generalized linear mixed models for two-level grouped data, where the random effects are uniquely indexed by groups and are independent. We derive necessary and sufficient conditions for the marginal likelihood to be expressed in explicit form. These models are unified under the conjugate generalized linear mixed models framework, where conjugate refers to the fact that the marginal likelihood can be expressed in closed form, rather than implying inference via the Bayesian paradigm. The proposed framework allows simultaneous conjugacy for Gaussian, Poisson and gamma responses, and thus can accommodate both unit- and group-level covariates. Only group-level covariates can be incorporated for the binomial distribution. In a simulation of Poisson data, our framework outperformed its competitors in terms of computational time, and was competitive in terms of robustness against misspecification of the random effects distributions.},
  archive      = {J_BIOMET},
  author       = {Lee, Jarod Y L and Green, Peter J and Ryan, Louise M},
  doi          = {10.1093/biomet/asz053},
  journal      = {Biometrika},
  number       = {1},
  pages        = {231-237},
  shortjournal = {Biometrika},
  title        = {Analysis of grouped data using conjugate generalized linear mixed models},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simplified integrated nested laplace approximation.
<em>BIOMET</em>, <em>107</em>(1), 223–230. (<a
href="https://doi.org/10.1093/biomet/asz044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Integrated nested Laplace approximation provides accurate and efficient approximations for marginal distributions in latent Gaussian random field models. Computational feasibility of the original Rue et al. (2009) methods relies on efficient approximation of Laplace approximations for the marginal distributions of the coefficients of the latent field, conditional on the data and hyperparameters. The computational efficiency of these approximations depends on the Gaussian field having a Markov structure. This note provides equivalent efficiency without requiring the Markov property, which allows for straightforward use of latent Gaussian fields without a sparse structure, such as reduced rank multi-dimensional smoothing splines. The method avoids the approximation for conditional modes used in Rue et al. (2009) , and uses a log determinant approximation based on a simple quasi-Newton update. The latter has a desirable property not shared by the most commonly used variant of the original method.},
  archive      = {J_BIOMET},
  author       = {Wood, Simon N},
  doi          = {10.1093/biomet/asz044},
  journal      = {Biometrika},
  number       = {1},
  pages        = {223-230},
  shortjournal = {Biometrika},
  title        = {Simplified integrated nested laplace approximation},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian sparse multiple regression for simultaneous rank
reduction and variable selection. <em>BIOMET</em>, <em>107</em>(1),
205–221. (<a href="https://doi.org/10.1093/biomet/asz056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a Bayesian methodology aimed at simultaneously estimating low-rank and row-sparse matrices in a high-dimensional multiple-response linear regression model. We consider a carefully devised shrinkage prior on the matrix of regression coefficients which obviates the need to specify a prior on the rank, and shrinks the regression matrix towards low-rank and row-sparse structures. We provide theoretical support to the proposed methodology by proving minimax optimality of the posterior mean under the prediction risk in ultra-high-dimensional settings where the number of predictors can grow subexponentially relative to the sample size. A one-step post-processing scheme induced by group lasso penalties on the rows of the estimated coefficient matrix is proposed for variable selection, with default choices of tuning parameters. We additionally provide an estimate of the rank using a novel optimization function achieving dimension reduction in the covariate space. We exhibit the performance of the proposed methodology in an extensive simulation study and a real data example.},
  archive      = {J_BIOMET},
  author       = {Chakraborty, Antik and Bhattacharya, Anirban and Mallick, Bani K},
  doi          = {10.1093/biomet/asz056},
  journal      = {Biometrika},
  number       = {1},
  pages        = {205-221},
  shortjournal = {Biometrika},
  title        = {Bayesian sparse multiple regression for simultaneous rank reduction and variable selection},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian constraint relaxation. <em>BIOMET</em>,
<em>107</em>(1), 191–204. (<a
href="https://doi.org/10.1093/biomet/asz069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prior information often takes the form of parameter constraints. Bayesian methods include such information through prior distributions having constrained support. By using posterior sampling algorithms, one can quantify uncertainty without relying on asymptotic approximations. However, sharply constrained priors are not necessary in some settings and tend to limit modelling scope to a narrow set of distributions that are tractable computationally. We propose to replace the sharp indicator function of the constraint with an exponential kernel, thereby creating a close-to-constrained neighbourhood within the Euclidean space in which the constrained subspace is embedded. This kernel decays with distance from the constrained space at a rate depending on a relaxation hyperparameter. By avoiding the sharp constraint, we enable use of off-the-shelf posterior sampling algorithms, such as Hamiltonian Monte Carlo, facilitating automatic computation in a broad range of models. We study the constrained and relaxed distributions under multiple settings and theoretically quantify their differences. Application of the method is illustrated through several novel modelling examples.},
  archive      = {J_BIOMET},
  author       = {Duan, Leo L and Young, Alexander L and Nishimura, Akihiko and Dunson, David B},
  doi          = {10.1093/biomet/asz069},
  journal      = {Biometrika},
  number       = {1},
  pages        = {191-204},
  shortjournal = {Biometrika},
  title        = {Bayesian constraint relaxation},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A conditional density estimation partition model using
logistic gaussian processes. <em>BIOMET</em>, <em>107</em>(1), 173–190.
(<a href="https://doi.org/10.1093/biomet/asz064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conditional density estimation seeks to model the distribution of a response variable conditional on covariates. We propose a Bayesian partition model using logistic Gaussian processes to perform conditional density estimation. The partition takes the form of a Voronoi tessellation and is learned from the data using a reversible jump Markov chain Monte Carlo algorithm. The methodology models data in which the density changes sharply throughout the covariate space, and can be used to determine where important changes in the density occur. The Markov chain Monte Carlo algorithm involves a Laplace approximation on the latent variables of the logistic Gaussian process model which marginalizes the parameters in each partition element, allowing an efficient search of the approximate posterior distribution of the tessellation. The method is consistent when the density is piecewise constant in the covariate space or when the density is Lipschitz continuous with respect to the covariates. In simulation and application to wind turbine data, the model successfully estimates the partition structure and conditional distribution.},
  archive      = {J_BIOMET},
  author       = {Payne, R D and Guha, N and Ding, Y and Mallick, B K},
  doi          = {10.1093/biomet/asz064},
  journal      = {Biometrika},
  number       = {1},
  pages        = {173-190},
  shortjournal = {Biometrika},
  title        = {A conditional density estimation partition model using logistic gaussian processes},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On semiparametric estimation of a path-specific effect in
the presence of mediator-outcome confounding. <em>BIOMET</em>,
<em>107</em>(1), 159–172. (<a
href="https://doi.org/10.1093/biomet/asz063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Path-specific effects constitute a broad class of mediated effects from an exposure to an outcome via one or more causal pathways along a set of intermediate variables. Most of the literature concerning estimation of mediated effects has focused on parametric models, with stringent assumptions regarding unmeasured confounding. We consider semiparametric inference of a path-specific effect when these assumptions are relaxed. In particular, we develop a suite of semiparametric estimators for the effect along a pathway through a mediator, but not through an exposure-induced confounder of that mediator. These estimators have different robustness properties, as each depends on different parts of the likelihood of the observed data. One estimator is locally semiparametric efficient and multiply robust. The latter property implies that machine learning can be used to estimate nuisance functions. We demonstrate these properties, as well as finite-sample properties of all the estimators, in a simulation study. We apply our method to an HIV study, in which we estimate the effect comparing two drug treatments on a patient’s average log CD4 count mediated by the patient’s level of adherence, but not by previous experience of toxicity, which is clearly affected by which treatment the patient is assigned to and may confound the effect of the patient’s level of adherence on their virologic outcome.},
  archive      = {J_BIOMET},
  author       = {Miles, C H and Shpitser, I and Kanki, P and Meloni, S and Tchetgen Tchetgen, E J},
  doi          = {10.1093/biomet/asz063},
  journal      = {Biometrika},
  number       = {1},
  pages        = {159-172},
  shortjournal = {Biometrika},
  title        = {On semiparametric estimation of a path-specific effect in the presence of mediator-outcome confounding},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Regularized calibrated estimation of propensity scores with
model misspecification and high-dimensional data. <em>BIOMET</em>,
<em>107</em>(1), 137–158. (<a
href="https://doi.org/10.1093/biomet/asz059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Propensity scores are widely used with inverse probability weighting to estimate treatment effects in observational studies. We study calibrated estimation as an alternative to maximum likelihood estimation for fitting logistic propensity score models. We show that, with possible model misspecification, minimizing the expected calibration loss underlying the calibrated estimators involves reducing both the expected likelihood loss and a measure of relative errors between the limiting and true propensity scores, which governs the mean squared errors of inverse probability weighted estimators. Furthermore, we derive a regularized calibrated estimator by minimizing the calibration loss with a lasso penalty. We develop a Fisher scoring descent algorithm for computing the proposed estimator and provide a high-dimensional analysis of the resulting inverse probability weighted estimators, leveraging the control of relative errors of propensity scores for calibrated estimation. We present a simulation study and an empirical application to demonstrate the advantages of the proposed methods over maximum likelihood and its regularization. The methods are implemented in the R package RCAL .},
  archive      = {J_BIOMET},
  author       = {Tan, Z},
  doi          = {10.1093/biomet/asz059},
  journal      = {Biometrika},
  number       = {1},
  pages        = {137-158},
  shortjournal = {Biometrika},
  title        = {Regularized calibrated estimation of propensity scores with model misspecification and high-dimensional data},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semiparametric estimation of structural failure time models
in continuous-time processes. <em>BIOMET</em>, <em>107</em>(1), 123–136.
(<a href="https://doi.org/10.1093/biomet/asz057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Structural failure time models are causal models for estimating the effect of time-varying treatments on a survival outcome. G-estimation and artificial censoring have been proposed for estimating the model parameters in the presence of time-dependent confounding and administrative censoring. However, most existing methods require manually pre-processing data into regularly spaced data, which may invalidate the subsequent causal analysis. Moreover, the computation and inference are challenging due to the nonsmoothness of artificial censoring. We propose a class of continuous-time structural failure time models that respects the continuous-time nature of the underlying data processes. Under a martingale condition of no unmeasured confounding, we show that the model parameters are identifiable from a potentially infinite number of estimating equations. Using the semiparametric efficiency theory, we derive the first semiparametric doubly robust estimators, which are consistent if the model for the treatment process or the failure time model, but not necessarily both, is correctly specified. Moreover, we propose using inverse probability of censoring weighting to deal with dependent censoring. In contrast to artificial censoring, our weighting strategy does not introduce nonsmoothness in estimation and ensures that resampling methods can be used for inference.},
  archive      = {J_BIOMET},
  author       = {Yang, S and Pieper, K and Cools, F},
  doi          = {10.1093/biomet/asz057},
  journal      = {Biometrika},
  number       = {1},
  pages        = {123-136},
  shortjournal = {Biometrika},
  title        = {Semiparametric estimation of structural failure time models in continuous-time processes},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model-free approach to quantifying the proportion of
treatment effect explained by a surrogate marker. <em>BIOMET</em>,
<em>107</em>(1), 107–122. (<a
href="https://doi.org/10.1093/biomet/asz065">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In randomized clinical trials, the primary outcome, |$Y$|⁠ , often requires long-term follow-up and/or is costly to measure. For such settings, it is desirable to use a surrogate marker, |$S$|⁠ , to infer the treatment effect on |$Y$|⁠ , |$\Delta$|⁠ . Identifying such an |$S$| and quantifying the proportion of treatment effect on |$Y$| explained by the effect on |$S$| are thus of great importance. Most existing methods for quantifying the proportion of treatment effect are model based and may yield biased estimates under model misspecification. Recently proposed nonparametric methods require strong assumptions to ensure that the proportion of treatment effect is in the range |$[0,1]$|⁠ . Additionally, optimal use of |$S$| to approximate |$\Delta$| is especially important when |$S$| relates to |$Y$| nonlinearly. In this paper we identify an optimal transformation of |$S$|⁠ , |$g_{\tiny {\rm{opt}}}(\cdot)$|⁠ , such that the proportion of treatment effect explained can be inferred based on |$g_{\tiny {\rm{opt}}}(S)$|⁠ . In addition, we provide two novel model-free definitions of proportion of treatment effect explained and simple conditions for ensuring that it lies within |$[0,1]$|⁠ . We provide nonparametric estimation procedures and establish asymptotic properties of the proposed estimators. Simulation studies demonstrate that the proposed methods perform well in finite samples. We illustrate the proposed procedures using a randomized study of HIV patients.},
  archive      = {J_BIOMET},
  author       = {Wang, Xuan and Parast, Layla and Tian, Lu and Cai, Tianxi},
  doi          = {10.1093/biomet/asz065},
  journal      = {Biometrika},
  number       = {1},
  pages        = {107-122},
  shortjournal = {Biometrika},
  title        = {Model-free approach to quantifying the proportion of treatment effect explained by a surrogate marker},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Minimal dispersion approximately balancing weights:
Asymptotic properties and practical considerations. <em>BIOMET</em>,
<em>107</em>(1), 93–105. (<a
href="https://doi.org/10.1093/biomet/asz050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weighting methods are widely used to adjust for covariates in observational studies, sample surveys, and regression settings. In this paper, we study a class of recently proposed weighting methods, which find the weights of minimum dispersion that approximately balance the covariates. We call these weights ‘minimal weights’ and study them under a common optimization framework. Our key observation is that finding weights which achieve approximate covariate balance is equivalent to performing shrinkage estimation of the inverse propensity score. This connection leads to both theoretical and practical developments. From a theoretical standpoint, we characterize the asymptotic properties of minimal weights and show that, under standard smoothness conditions on the propensity score function, minimal weights are consistent estimates of the true inverse probability weights. In addition, we show that the resulting weighting estimator is consistent, asymptotically normal and semiparametrically efficient. From a practical standpoint, we give a finite-sample oracle inequality that bounds the loss incurred by balancing more functions of the covariates than strictly needed. This inequality shows that minimal weights implicitly bound the number of active covariate balance constraints. Finally, we provide a tuning algorithm for choosing the degree of approximate balance in minimal weights. The paper concludes with an empirical study which suggests that approximate balance is preferable to exact balance, especially when there is limited overlap in covariate distributions. Further studies show that the root mean squared error of the weighting estimator can be reduced by as much as a half with approximate balance.},
  archive      = {J_BIOMET},
  author       = {Wang, Yixin and Zubizarreta, Jose R},
  doi          = {10.1093/biomet/asz050},
  journal      = {Biometrika},
  number       = {1},
  pages        = {93-105},
  shortjournal = {Biometrika},
  title        = {Minimal dispersion approximately balancing weights: Asymptotic properties and practical considerations},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multisample estimation of bacterial composition matrices in
metagenomics data. <em>BIOMET</em>, <em>107</em>(1), 75–92. (<a
href="https://doi.org/10.1093/biomet/asz062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Metagenomics sequencing is routinely applied to quantify bacterial abundances in microbiome studies, where bacterial composition is estimated based on the sequencing read counts. Due to limited sequencing depth and DNA dropouts, many rare bacterial taxa might not be captured in the final sequencing reads, which results in many zero counts. Naive composition estimation using count normalization leads to many zero proportions, which tend to result in inaccurate estimates of bacterial abundance and diversity. This paper takes a multisample approach to estimation of bacterial abundances in order to borrow information across samples and across species. Empirical results from real datasets suggest that the composition matrix over multiple samples is approximately low rank, which motivates a regularized maximum likelihood estimation with a nuclear norm penalty. An efficient optimization algorithm using the generalized accelerated proximal gradient and Euclidean projection onto simplex space is developed. Theoretical upper bounds and the minimax lower bounds of the estimation errors, measured by the Kullback–Leibler divergence and the Frobenius norm, are established. Simulation studies demonstrate that the proposed estimator outperforms the naive estimators. The method is applied to an analysis of a human gut microbiome dataset.},
  archive      = {J_BIOMET},
  author       = {Cao, Yuanpei and Zhang, Anru and Li, Hongzhe},
  doi          = {10.1093/biomet/asz062},
  journal      = {Biometrika},
  number       = {1},
  pages        = {75-92},
  shortjournal = {Biometrika},
  title        = {Multisample estimation of bacterial composition matrices in metagenomics data},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Consistent community detection in multi-layer network data.
<em>BIOMET</em>, <em>107</em>(1), 61–73. (<a
href="https://doi.org/10.1093/biomet/asz068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider multi-layer network data where the relationships between pairs of elements are reflected in multiple modalities, and may be described by multivariate or even high-dimensional vectors. Under the multi-layer stochastic block model framework we derive consistency results for a least squares estimation of memberships. Our theorems show that, as compared to single-layer community detection, a multi-layer network provides much richer information that allows for consistent community detection from a much sparser network, with required edge density reduced by a factor of the square root of the number of layers. Moreover, the multi-layer framework can detect cohesive community structure across layers, which might be hard to detect by any single-layer or simple aggregation. Simulations and a data example are provided to support the theoretical results.},
  archive      = {J_BIOMET},
  author       = {Lei, Jing and Chen, Kehui and Lynch, Brian},
  doi          = {10.1093/biomet/asz068},
  journal      = {Biometrika},
  number       = {1},
  pages        = {61-73},
  shortjournal = {Biometrika},
  title        = {Consistent community detection in multi-layer network data},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-dimensional causal discovery under non-gaussianity.
<em>BIOMET</em>, <em>107</em>(1), 41–59. (<a
href="https://doi.org/10.1093/biomet/asz055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider graphical models based on a recursive system of linear structural equations. This implies that there is an ordering, |$\sigma$|⁠ , of the variables such that each observed variable |$Y_v$| is a linear function of a variable-specific error term and the other observed variables |$Y_u$| with |$\sigma(u) &lt; \sigma (v)$|⁠ . The causal relationships, i.e., which other variables the linear functions depend on, can be described using a directed graph. It has previously been shown that when the variable-specific error terms are non-Gaussian, the exact causal graph, as opposed to a Markov equivalence class, can be consistently estimated from observational data. We propose an algorithm that yields consistent estimates of the graph also in high-dimensional settings in which the number of variables may grow at a faster rate than the number of observations, but in which the underlying causal structure features suitable sparsity; specifically, the maximum in-degree of the graph is controlled. Our theoretical analysis is couched in the setting of log-concave error distributions.},
  archive      = {J_BIOMET},
  author       = {Wang, Y Samuel and Drton, Mathias},
  doi          = {10.1093/biomet/asz055},
  journal      = {Biometrika},
  number       = {1},
  pages        = {41-59},
  shortjournal = {Biometrika},
  title        = {High-dimensional causal discovery under non-gaussianity},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scalable inference for crossed random effects models.
<em>BIOMET</em>, <em>107</em>(1), 25–40. (<a
href="https://doi.org/10.1093/biomet/asz058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop methodology and complexity theory for Markov chain Monte Carlo algorithms used in inference for crossed random effects models in modern analysis of variance. We consider a plain Gibbs sampler and propose a simple modification, referred to as a collapsed Gibbs sampler. Under some balancedness conditions on the data designs and assuming that precision hyperparameters are known, we demonstrate that the plain Gibbs sampler is not scalable, in the sense that its complexity is worse than proportional to the number of parameters and data, but the collapsed Gibbs sampler is scalable. In simulated and real datasets we show that the explicit convergence rates predicted by our theory closely match the computable, but nonexplicit rates in cases where the design assumptions are violated. We also show empirically that the collapsed Gibbs sampler extended to sample precision hyperparameters significantly outperforms alternative state-of-the-art algorithms.},
  archive      = {J_BIOMET},
  author       = {Papaspiliopoulos, O and Roberts, G O and Zanella, G},
  doi          = {10.1093/biomet/asz058},
  journal      = {Biometrika},
  number       = {1},
  pages        = {25-40},
  shortjournal = {Biometrika},
  title        = {Scalable inference for crossed random effects models},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The hastings algorithm at fifty. <em>BIOMET</em>,
<em>107</em>(1), 1–23. (<a
href="https://doi.org/10.1093/biomet/asz066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a 1970 Biometrika paper, W. K. Hastings developed a broad class of Markov chain algorithms for sampling from probability distributions that are difficult to sample from directly. The algorithm draws a candidate value from a proposal distribution and accepts the candidate with a probability that can be computed using only the unnormalized density of the target distribution, allowing one to sample from distributions known only up to a constant of proportionality. The stationary distribution of the corresponding Markov chain is the target distribution one is attempting to sample from. The Hastings algorithm generalizes the Metropolis algorithm to allow a much broader class of proposal distributions instead of just symmetric cases. An important class of applications for the Hastings algorithm corresponds to sampling from Bayesian posterior distributions, which have densities given by a prior density multiplied by a likelihood function and divided by a normalizing constant equal to the marginal likelihood. The marginal likelihood is typically intractable, presenting a fundamental barrier to implementation in Bayesian statistics. This barrier can be overcome by Markov chain Monte Carlo sampling algorithms. Amazingly, even after 50 years, the majority of algorithms used in practice today involve the Hastings algorithm. This article provides a brief celebration of the continuing impact of this ingenious algorithm on the 50th anniversary of its publication.},
  archive      = {J_BIOMET},
  author       = {Dunson, D B and Johndrow, J E},
  doi          = {10.1093/biomet/asz066},
  journal      = {Biometrika},
  number       = {1},
  pages        = {1-23},
  shortjournal = {Biometrika},
  title        = {The hastings algorithm at fifty},
  volume       = {107},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
