<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JRSSSC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jrsssc---64">JRSSSC - 64</h2>
<ul>
<li><details>
<summary>
(2020). Contents of volume 69, 2020. <em>JRSSSC</em>,
<em>69</em>(5), 1381–1383. (<a
href="https://doi.org/10.1111/rssc.12445">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSC},
  doi          = {10.1111/rssc.12445},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {5},
  pages        = {1381-1383},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Contents of volume 69, 2020},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Index of authors, volume 69, 2020. <em>JRSSSC</em>,
<em>69</em>(5), 1375–1380. (<a
href="https://doi.org/10.1111/rssc.12444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSC},
  doi          = {10.1111/rssc.12444},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {5},
  pages        = {1375-1380},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Index of authors, volume 69, 2020},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Circular regression trees and forests with an application to
probabilistic wind direction forecasting. <em>JRSSSC</em>,
<em>69</em>(5), 1357–1374. (<a
href="https://doi.org/10.1111/rssc.12437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although circular data occur in a wide range of scientific fields, the methodology for distributional modelling and probabilistic forecasting of circular response variables is quite limited. Most of the existing methods are built on generalized linear and additive models, which are often challenging to optimize and interpret. Specifically, capturing abrupt changes or interactions is not straightforward but often relevant, e.g. for modelling wind directions subject to different wind regimes. Additionally, automatic covariate selection is desirable when many predictor variables are available, as is often the case in weather forecasting. To address these challenges we suggest a general distributional approach using regression trees and random forests to obtain probabilistic forecasts for circular responses. Using trees simplifies model estimation as covariates are used only for partitioning the data and subsequently just a simple von Mises distribution is fitted in the resulting subgroups. Circular regression trees are straightforward to interpret, can capture non-linear effects and interactions, and automatically select covariates affecting location and/or scale in the von Mises distribution. Circular random forests regularize and smooth the effects from an ensemble of trees. The new methods are applied to probabilistic wind direction forecasting at two Austrian airports, considering other common approaches as a benchmark.},
  archive      = {J_JRSSSC},
  author       = {Lang, Moritz N. and Schlosser, Lisa and Hothorn, Torsten and Mayr, Georg J. and Stauffer, Reto and Zeileis, Achim},
  doi          = {10.1111/rssc.12437},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {5},
  pages        = {1357-1374},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Circular regression trees and forests with an application to probabilistic wind direction forecasting},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Markov switching modelling of shooting performance
variability and teammate interactions in basketball. <em>JRSSSC</em>,
<em>69</em>(5), 1337–1356. (<a
href="https://doi.org/10.1111/rssc.12442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In basketball, measures of individual player performance provide critical guidance for a broad spectrum of decisions related to training and game strategy. However, most studies on this topic focus on performance level measurement, neglecting other important factors, such as performance variability. Here we model shooting performance variability by using Markov switching models, assuming the existence of two alternating performance regimes related to the positive or negative synergies that specific combinations of players may create on the court. The main goal of this analysis is to investigate the relationships between each player&#39;s performance variability and team line-up composition by assuming shot-varying transition probabilities between regimes. Relationships between pairs of players are then visualized in a network graph, highlighting positive and negative interactions between teammates. On the basis of these interactions, we build a score for the line-ups, which we show correlates with the line-up&#39;s shooting performance. This confirms that interactions between teammates detected by the Markov switching model directly affect team performance, which is information that would be enormously useful to coaches when deciding which players should play together.},
  archive      = {J_JRSSSC},
  author       = {Sandri, Marco and Zuccolotto, Paola and Manisera, Marica},
  doi          = {10.1111/rssc.12442},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {5},
  pages        = {1337-1356},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Markov switching modelling of shooting performance variability and teammate interactions in basketball},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bayesian quest for finding a unified model for predicting
volleyball games. <em>JRSSSC</em>, <em>69</em>(5), 1307–1336. (<a
href="https://doi.org/10.1111/rssc.12436">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Volleyball is a team sport with unique and specific characteristics. We introduce a new two-level hierarchical Bayesian model which accounts for these volleyball-specific characteristics. In the first level, we model the set outcome with a simple logistic regression model. Conditionally on the winner of the set, in the second level, we use a truncated negative binomial distribution for the points earned by the losing team. An additional Poisson-distributed inflation component is introduced to model the extra points played in the case that the two teams have a point difference less than two points. The number of points of the winner within each set is deterministically specified by the winner of the set and the points of the inflation component. The team-specific abilities and the home effect are used as covariates on all layers of the model (set, point and extra inflated points). The implementation of the proposed model on the Italian SuperLega 2017–2018 data shows exceptional reproducibility of the final league table and satisfactory predictive ability.},
  archive      = {J_JRSSSC},
  author       = {Egidi, Leonardo and Ntzoufras, Ioannis},
  doi          = {10.1111/rssc.12436},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {5},
  pages        = {1307-1336},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {A bayesian quest for finding a unified model for predicting volleyball games},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A calibrated sensitivity analysis for matched observational
studies with application to the effect of second-hand smoke exposure on
blood lead levels in children. <em>JRSSSC</em>, <em>69</em>(5),
1285–1305. (<a href="https://doi.org/10.1111/rssc.12443">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We conducted a matched observational study to investigate the causal relationship between second-hand smoke and blood lead levels in children. Our first analysis that assumes no unmeasured confounding suggests evidence of a detrimental effect of second-hand smoke. However, unmeasured confounding is a concern in our study as in other observational studies of second-hand smoke&#39;s effects. A sensitivity analysis asks how sensitive the conclusion is to a hypothesized unmeasured confounder U . For example, in our study, one potential unmeasured confounder is whether the child attends a public or private school. A commonly used sensitivity analysis for matched observational studies adopts a worst-case perspective, which assumes that, in each matched set, the unmeasured confounder is allocated to make the bias worst: in a matched pair, the child with higher blood lead level always attends public school and the other private school. This worst-case allocation of U does not correspond to any realistic distribution of U in the population and is difficult to compare with observed covariates. We proposed a new sensitivity analysis method that addresses these concerns. We apply the new method to our study and find that, to explain away the association between second-hand smoke exposure and blood lead levels as non-causal, the unmeasured confounder would have to be a bigger confounder than any measured confounder.},
  archive      = {J_JRSSSC},
  author       = {Zhang, Bo and Small, Dylan S.},
  doi          = {10.1111/rssc.12443},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {5},
  pages        = {1285-1305},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {A calibrated sensitivity analysis for matched observational studies with application to the effect of second-hand smoke exposure on blood lead levels in children},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian analysis of tests with unknown specificity and
sensitivity. <em>JRSSSC</em>, <em>69</em>(5), 1269–1283. (<a
href="https://doi.org/10.1111/rssc.12435">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When testing for a rare disease, prevalence estimates can be highly sensitive to uncertainty in the specificity and sensitivity of the test. Bayesian inference is a natural way to propagate these uncertainties, with hierarchical modelling capturing variation in these parameters across experiments. Another concern is the people in the sample not being representative of the general population. Statistical adjustment cannot without strong assumptions correct for selection bias in an opt-in sample, but multilevel regression and post-stratification can at least adjust for known differences between the sample and the population. We demonstrate hierarchical regression and post-stratification models with code in Stan and discuss their application to a controversial recent study of SARS-CoV-2 antibodies in a sample of people from the Stanford University area. Wide posterior intervals make it impossible to evaluate the quantitative claims of that study regarding the number of unreported infections. For future studies, the methods described here should facilitate more accurate estimates of disease prevalence from imperfect tests performed on non-representative samples.},
  archive      = {J_JRSSSC},
  author       = {Gelman, Andrew and Carpenter, Bob},
  doi          = {10.1111/rssc.12435},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {5},
  pages        = {1269-1283},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Bayesian analysis of tests with unknown specificity and sensitivity},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adding measurement error to location data to protect subject
confidentiality while allowing for consistent estimation of exposure
effects. <em>JRSSSC</em>, <em>69</em>(5), 1251–1268. (<a
href="https://doi.org/10.1111/rssc.12439">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In public use data sets, it is desirable not to report a respondent&#39;s location precisely to protect subject confidentiality. However, the direct use of perturbed location data to construct explanatory exposure variables for regression models will generally make naive estimates of all parameters biased and inconsistent. We propose an approach where a perturbation vector, consisting of a random distance at a random angle, is added to a respondent&#39;s reported geographic co-ordinates. We show that, as long as the distribution of the perturbation is public and there is an underlying prior population density map, external researchers can construct unbiased and consistent estimates of location-dependent exposure effects by using numerical integration techniques over all possible actual locations, although coefficient confidence intervals are wider than if the true location data were known. We examine our method by using a Monte Carlo simulation exercise and apply it to a real world example using data on perceived and actual distance to a health facility in Tanzania.},
  archive      = {J_JRSSSC},
  author       = {Karra, Mahesh and Canning, David and Sato, Ryoko},
  doi          = {10.1111/rssc.12439},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {5},
  pages        = {1251-1268},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Adding measurement error to location data to protect subject confidentiality while allowing for consistent estimation of exposure effects},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). One-class classification with application to forensic
analysis. <em>JRSSSC</em>, <em>69</em>(5), 1227–1249. (<a
href="https://doi.org/10.1111/rssc.12438">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of broken glass is forensically important to reconstruct the events of a criminal act. In particular, the comparison between the glass fragments found on a suspect (recovered cases) and those collected at the crime scene (control cases) may help the police to identify the offender(s) correctly. The forensic issue can be framed as a one-class classification problem. One-class classification is a recently emerging and special classification task, where only one class is fully known (the so-called target class), whereas information on the others is completely missing. We propose to consider Gini&#39;s classical transvariation probability as a measure of typicality, i.e. a measure of resemblance between an observation and a set of well-known objects (the control cases). The aim of the proposed transvariation-based one-class classifier is to identify the best boundary around the target class, i.e. to recognize as many target objects as possible while rejecting all those deviating from this class.},
  archive      = {J_JRSSSC},
  author       = {Fortunato, Francesca and Anderlucci, Laura and Montanari, Angela},
  doi          = {10.1111/rssc.12438},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {5},
  pages        = {1227-1249},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {One-class classification with application to forensic analysis},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the interplay between exposure misclassification and
informative cluster size. <em>JRSSSC</em>, <em>69</em>(5), 1209–1226.
(<a href="https://doi.org/10.1111/rssc.12430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A recent multigenerational study of diethylstilbestrol and attention deficit hyperactivity disorder exhibited signs of both informative cluster size—the outcome was more prevalent in small families—and exposure misclassification—self-report of familial diethylstilbestrol exposure was substantially mismeasured. Motivated by this, we study the effect of exposure misclassification when cluster size is potentially informative and, in particular, when misclassification is differential by cluster size. We find that: misclassification in an exposure that is related to cluster size induces informativeness when cluster size would otherwise be non-informative; and misclassification that is differential by informative cluster size may attenuate, inflate or possibly reverse the sign of estimates. To mitigate these issues, we propose an observed likelihood correction for joint models of cluster size and outcomes, and an expected estimating equations correction. We evaluate these approaches in simulations and in application to the motivating data from the second Nurses Health Study, NHS II.},
  archive      = {J_JRSSSC},
  author       = {McGee, Glen and Kioumourtzoglou, Marianthi-Anna and Weisskopf, Marc G. and Haneuse, Sebastien and Coull, Brent A.},
  doi          = {10.1111/rssc.12430},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {5},
  pages        = {1209-1226},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {On the interplay between exposure misclassification and informative cluster size},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nested g-computation: A causal approach to analysis of
censored medical costs in the presence of time-varying treatment.
<em>JRSSSC</em>, <em>69</em>(5), 1189–1208. (<a
href="https://doi.org/10.1111/rssc.12441">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rising medical costs are an emerging challenge in policy decisions and resource allocation planning. When cumulative medical cost is the outcome, right censoring induces informative missingness due to heterogeneity in cost accumulation rates across subjects. Inverse weighting approaches have been developed to address the challenge of informative cost trajectories in mean cost estimation, though these approaches generally ignore post-baseline treatment changes. In post-hysterectomy endometrial cancer patients, data from a linked database of Medicare records and the Surveillance, Epidemiology, and End Results programme of the National Cancer Institute reveal substantial within-subject variation in treatment over time. In such a setting, the utility of existing intent-to-treat approaches is generally limited. Estimates of the population mean cost under a hypothetical time-varying treatment regime can better assist with resource allocation when planning for a treatment policy change; such estimates must inherently take time-dependent treatment and confounding into account. We develop a nested g -computation approach to cost analysis to address this challenge, while accounting for censoring. We develop a procedure to evaluate sensitivity to departures from baseline treatment ignorability. We further conduct a variety of simulations and apply our nested g -computation procedure to 2-year costs from endometrial cancer patients.},
  archive      = {J_JRSSSC},
  author       = {Spieker, Andrew J. and Ko, Emily M. and Roy, Jason A. and Mitra, Nandita},
  doi          = {10.1111/rssc.12441},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {5},
  pages        = {1189-1208},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Nested g-computation: A causal approach to analysis of censored medical costs in the presence of time-varying treatment},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cluster analysis of microbiome data by using mixtures of
dirichlet–multinomial regression models. <em>JRSSSC</em>,
<em>69</em>(5), 1163–1187. (<a
href="https://doi.org/10.1111/rssc.12432">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The human gut microbiome is one of the fundamental components of our physiology, and exploring the relationship between biological and environmental covariates and the resulting taxonomic composition of a given microbial community is an active area of research. Previously, a Dirichlet–multinomial regression framework has been suggested to model this relationship, but it did not account for any underlying latent group structure. An underlying group structure of guts (such as enterotypes) has been observed across gut microbiome samples in which guts in the same group share similar biota compositions. In the paper, a finite mixture of Dirichlet–multinomial regression models is proposed that accounts for this underlying group structure and to allow for a probabilistic investigation of the relationship between bacterial abundance and biological and/or environmental covariates within each inferred group. Furthermore, finite mixtures of regression models which incorporate the concomitant effect of the covariates on the resulting mixing proportions are also proposed and examined within the Dirichlet–multinomial framework. We utilize the proposed mixture model to gain insight on underlying subgroups in a microbiome data set comprising tumour and healthy samples and the relationships between covariates and microbial abundance in those subgroups.},
  archive      = {J_JRSSSC},
  author       = {Subedi, Sanjeena and Neish, Drew and Bak, Stephen and Feng, Zeny},
  doi          = {10.1111/rssc.12432},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {5},
  pages        = {1163-1187},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Cluster analysis of microbiome data by using mixtures of Dirichlet–Multinomial regression models},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Landmark proportional subdistribution hazards models for
dynamic prediction of cumulative incidence functions. <em>JRSSSC</em>,
<em>69</em>(5), 1145–1162. (<a
href="https://doi.org/10.1111/rssc.12433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An individualized dynamic risk prediction model that incorporates all available information collected over the follow-up can be used to choose an optimal treatment strategy in realtime, although existing methods have not been designed to handle competing risks. In this study, we developed a landmark proportional subdistribution hazard (PSH) model and a comprehensive supermodel for dynamic risk prediction with competing risks. Simulations showed that our proposed models perform satisfactorily (assessed by the time-dependent relative difference, Brier score and area under the receiver operating characteristics curve) under PSH or non-PSH settings. The models were used to predict the probabilities of developing a distant metastasis among breast cancer patients where death was treated as a competing risk. Prediction can be estimated by using standard statistical packages.},
  archive      = {J_JRSSSC},
  author       = {Liu, Qing and Tang, Gong and Costantino, Joseph P. and Chang, Chung-Chou H.},
  doi          = {10.1111/rssc.12433},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {5},
  pages        = {1145-1162},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Landmark proportional subdistribution hazards models for dynamic prediction of cumulative incidence functions},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hierarchical mixed effect hurdle model for spatiotemporal
count data and its application to identifying factors impacting health
professional shortages. <em>JRSSSC</em>, <em>69</em>(5), 1121–1144. (<a
href="https://doi.org/10.1111/rssc.12434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Count data are common in many fields such as public health. Hurdle models have been developed to model count data when the zero count could be either inflated or deflated. However, when data are repeatedly collected over time and spatially correlated, it is very challenging to model the data appropriately. For example, to study health professional shortage areas, the number of primary care physicians along with other demographic characteristics are collected at the county level in the USA and over different years. Since the data are repeatedly collected over time, counties are nested within the state, and adjacent counties are geographically correlated, the dependence structure of the data is very complex. We develop a Bayesian hurdle model with multilayered random effects to incorporate this complex structure. We use a time-varying random effect for each state to capture the time effect at the state level, and a temporal thin plate spline to capture the spatiotemporal correlation across different counties. We use STAN to obtain samples for inference from the posterior distribution. By using the model proposed, we can identify the important factors which impact health professional shortage areas. Simulation studies also confirm the effectiveness of the model.},
  archive      = {J_JRSSSC},
  author       = {Ghosal, Soutik and Lau, Timothy S. and Gaskins, Jeremy and Kong, Maiying},
  doi          = {10.1111/rssc.12434},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {5},
  pages        = {1121-1144},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {A hierarchical mixed effect hurdle model for spatiotemporal count data and its application to identifying factors impacting health professional shortages},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sensitivity analysis for publication bias in meta-analyses.
<em>JRSSSC</em>, <em>69</em>(5), 1091–1119. (<a
href="https://doi.org/10.1111/rssc.12440">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose sensitivity analyses for publication bias in meta-analyses. We consider a publication process such that ‘statistically significant’ results are more likely to be published than negative or “non-significant” results by an unknown ratio, η . Our proposed methods also accommodate some plausible forms of selection based on a study&#39;s standard error. Using inverse probability weighting and robust estimation that accommodates non-normal population effects, small meta-analyses, and clustering, we develop sensitivity analyses that enable statements such as ‘For publication bias to shift the observed point estimate to the null, “significant” results would need to be at least 30 fold more likely to be published than negative or “non-significant” results’. Comparable statements can be made regarding shifting to a chosen non-null value or shifting the confidence interval. To aid interpretation, we describe empirical benchmarks for plausible values of η across disciplines. We show that a worst-case meta-analytic point estimate for maximal publication bias under the selection model can be obtained simply by conducting a standard meta-analysis of only the negative and ‘non-significant’ studies; this method sometimes indicates that no amount of such publication bias could ‘explain away’ the results. We illustrate the proposed methods by using real meta-analyses and provide an R package: PublicationBias.},
  archive      = {J_JRSSSC},
  author       = {Mathur, Maya B. and VanderWeele, Tyler J.},
  doi          = {10.1111/rssc.12440},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {5},
  pages        = {1091-1119},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Sensitivity analysis for publication bias in meta-analyses},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Burglary in london: Insights from statistical heterogeneous
spatial point processes. <em>JRSSSC</em>, <em>69</em>(5), 1067–1090. (<a
href="https://doi.org/10.1111/rssc.12431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To obtain operational insights regarding the crime of burglary in London, we consider the estimation of the effects of covariates on the intensity of spatial point patterns. Inspired by localized properties of criminal behaviour, we propose a spatial extension to mixtures of generalized linear models from the mixture modelling literature. The Bayesian model proposed is a finite mixture of Poisson generalized linear models such that each location is probabilistically assigned to one of the groups. Each group is characterized by the regression coefficients, which we subsequently use to interpret the localized effects of the covariates. By using a blocks structure of the study region, our approach enables specifying spatial dependence between nearby locations. We estimate the proposed model by using Markov chain Monte Carlo methods and we provide a Python implementation.},
  archive      = {J_JRSSSC},
  author       = {Povala, Jan and Virtanen, Seppo and Girolami, Mark},
  doi          = {10.1111/rssc.12431},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {5},
  pages        = {1067-1090},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Burglary in london: Insights from statistical heterogeneous spatial point processes},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Linear mixed effects models for non-gaussian continuous
repeated measurement data. <em>JRSSSC</em>, <em>69</em>(5), 1015–1065.
(<a href="https://doi.org/10.1111/rssc.12405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the analysis of continuous repeated measurement outcomes that are collected longitudinally. A standard framework for analysing data of this kind is a linear Gaussian mixed effects model within which the outcome variable can be decomposed into fixed effects, time invariant and time-varying random effects, and measurement noise. We develop methodology that, for the first time, allows any combination of these stochastic components to be non-Gaussian, using multivariate normal variance–mean mixtures. To meet the computational challenges that are presented by large data sets, i.e. in the current context, data sets with many subjects and/or many repeated measurements per subject, we propose a novel implementation of maximum likelihood estimation using a computationally efficient subsampling-based stochastic gradient algorithm. We obtain standard error estimates by inverting the observed Fisher information matrix and obtain the predictive distributions for the random effects in both filtering (conditioning on past and current data) and smoothing (conditioning on all data) contexts. To implement these procedures, we introduce an R package: ngme. We reanalyse two data sets, from cystic fibrosis and nephrology research, that were previously analysed by using Gaussian linear mixed effects models.},
  archive      = {J_JRSSSC},
  author       = {Asar, Özgür and Bolin, David and Diggle, Peter J. and Wallin, Jonas},
  doi          = {10.1111/rssc.12405},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {5},
  pages        = {1015-1065},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Linear mixed effects models for non-gaussian continuous repeated measurement data},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The use of sampling weights in m-quantile random-effects
regression: An application to programme for international student
assessment mathematics scores. <em>JRSSSC</em>, <em>69</em>(4),
991–1012. (<a href="https://doi.org/10.1111/rssc.12418">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {M -quantile random-effects regression represents an interesting approach for modelling multilevel data when the researcher is focused on conditional quantiles. When data are obtained from complex survey designs, sampling weights must be incorporated in the analysis. A robust pseudolikelihood approach for accommodating sampling weights in M -quantile random-effects regression is presented. In particular, the method is based on a robustification of the estimating equations. The methodology proposed is applied to the Italian sample of the Programme for International Student Assessment 2015 survey to study the gender gap in mathematics at various quantiles of the conditional distribution. The findings offer a possible explanation of the low proportion of women in science, technology, engineering and mathematics sectors.},
  archive      = {J_JRSSSC},
  author       = {Spagnolo, Francesco Schirripa and Salvati, Nicola and D’Agostino, Antonella and Nicaise, Ides},
  doi          = {10.1111/rssc.12418},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {4},
  pages        = {991-1012},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {The use of sampling weights in M-quantile random-effects regression: An application to programme for international student assessment mathematics scores},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Statistical inference on tree swallow migrations with random
forests. <em>JRSSSC</em>, <em>69</em>(4), 973–989. (<a
href="https://doi.org/10.1111/rssc.12416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bird species’ migratory patterns have typically been studied through individual observations and historical records. In recent years, the eBird citizen science project, which solicits observations from thousands of bird watchers around the world, has opened the door for a data-driven approach to understanding the large-scale geographical movements. Here, we focus on the North American tree swallow ( Tachycineta bicolor ) occurrence patterns throughout the eastern USA. Migratory departure dates for this species are widely believed by both ornithologists and casual observers to vary substantially across years, but the reasons for this are largely unknown. In this work, we present evidence that maximum daily temperature is predictive of tree swallow occurrence. Because it is generally understood that species occurrence is a function of many complex, high order interactions between ecological covariates, we utilize the flexible modelling approach that is offered by random forests. Making use of recent asymptotic results, we provide formal hypothesis tests for predictive significance of various covariates and also develop and implement a permutation-based approach for formally assessing interannual variations by treating the prediction surfaces that are generated by random forests as functional data. Each of these tests suggest that maximum daily temperature is important in predicting migration patterns.},
  archive      = {J_JRSSSC},
  author       = {Coleman, Tim and Mentch, Lucas and Fink, Daniel and La Sorte, Frank A. and Winkler, David W. and Hooker, Giles and Hochachka, Wesley M.},
  doi          = {10.1111/rssc.12416},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {4},
  pages        = {973-989},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Statistical inference on tree swallow migrations with random forests},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating the binary endogenous effect of insurance on
doctor visits by copula-based regression additive models.
<em>JRSSSC</em>, <em>69</em>(4), 953–971. (<a
href="https://doi.org/10.1111/rssc.12419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper estimates the causal effect of having health insurance on healthcare utilization, while accounting for potential endogeneity bias. The topic has important policy implications, because health insurance reforms implemented in the USA in recent decades have focused on extending coverage to the previously uninsured. Consequently, understanding the effects of those reforms requires an accurate estimate of the causal effect of insurance on utilization. However, obtaining such an estimate is complicated by the discreteness inherent in common measures of healthcare usage. The paper presents a flexible estimation approach, based on copula functions, that consistently estimates the coefficient of a binary endogenous regressor in count data settings. The relevant numerical computations can be easily carried out by using the freely available GJRM R package. The empirical results find significant evidence of favourable selection into insurance. Ignoring such selection, insurance appears to increase doctor visit usage by 62\% but, adjusting for it, the effect increases to 134\%.},
  archive      = {J_JRSSSC},
  author       = {Marra, Giampiero and Radice, Rosalba and Zimmer, David M.},
  doi          = {10.1111/rssc.12419},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {4},
  pages        = {953-971},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Estimating the binary endogenous effect of insurance on doctor visits by copula-based regression additive models},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fault isolation for a complex decentralized waste water
treatment facility. <em>JRSSSC</em>, <em>69</em>(4), 931–951. (<a
href="https://doi.org/10.1111/rssc.12429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decentralized waste water treatment facilities monitor many features that are complexly related. The ability to detect the onset of a fault and to identify variables accurately that have shifted because of the fault are vital to maintaining proper system operation and high quality produced water. Various multivariate methods have been proposed to perform fault detection and isolation, but the methods require data to be independent and identically distributed when the process is in control, and most require a distributional assumption. We propose a distribution-free retrospective change-point-detection method for auto-correlated and non-stationary multivariate processes. We detrend the data by using observations from an in-control time period to account for expected changes due to external or user-controlled factors. Next, we perform the fused lasso, which penalizes differences in consecutive observations, to detect faults and to identify shifted variables. To account for auto-correlation, the regularization parameter is chosen by using an estimated effective sample size in the extended Bayesian information criterion. We demonstrate the performance of our method compared with a competitor in simulation. Finally, we apply our method to waste water treatment facility data with a known fault, and the variables identified by our proposed method are consistent with the operators’ diagnosis of the fault&#39;s cause.},
  archive      = {J_JRSSSC},
  author       = {Klanderman, Molly C. and Newhart, Kathryn B. and Cath, Tzahi Y. and Hering, Amanda S.},
  doi          = {10.1111/rssc.12429},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {4},
  pages        = {931-951},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Fault isolation for a complex decentralized waste water treatment facility},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Global forensic geolocation with deep neural networks.
<em>JRSSSC</em>, <em>69</em>(4), 909–929. (<a
href="https://doi.org/10.1111/rssc.12427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important problem in modern forensic analyses is identifying the provenance of materials at a crime scene, such as biological material on a piece of clothing. This procedure, which is known as geolocation, is conventionally guided by expert knowledge of the biological evidence and therefore tends to be application specific, labour intensive and often subjective. Purely data-driven methods have yet to be fully realized in this domain, because in part of the lack of a sufficiently rich source of data. However, high throughput sequencing technologies can identify tens of thousands of fungi and bacteria taxa by using DNA recovered from a single swab collected from nearly any object or surface. This microbial community, or microbiome, may be highly informative of the provenance of the sample, but data on the spatial variation of microbiomes are sparse and high dimensional and have a complex dependence structure that render them difficult to model with standard statistical tools. Deep learning algorithms have generated a tremendous amount of interest within the machine learning community for their predictive performance in high dimensional problems. We present DeepSpace: a new algorithm for geolocation that aggregates over an ensemble of deep neural network classifiers trained on randomly generated Voronoi partitions of a spatial domain. The DeepSpace algorithm makes remarkably good point predictions; for example, when applied to the microbiomes of over 1300 dust samples collected across continental USA, more than half of geolocation predictions produced by this model fall less than 100 km from their true origin, which is a 60\% reduction in error from competing geolocation methods. Moreover, we apply DeepSpace to a novel data set of global dust samples collected from nearly 30 countries, finding that dust-associated fungi alone predict a sample&#39;s country of origin with nearly 90\% accuracy.},
  archive      = {J_JRSSSC},
  author       = {Grantham, Neal S. and Reich, Brian J. and Laber, Eric B. and Pacifici, Krishna and Dunn, Robert R. and Fierer, Noah and Gebert, Matthew and Allwood, Julia S. and Faith, Seth A.},
  doi          = {10.1111/rssc.12427},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {4},
  pages        = {909-929},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Global forensic geolocation with deep neural networks},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simulating gene silencing through intervention analysis.
<em>JRSSSC</em>, <em>69</em>(4), 887–907. (<a
href="https://doi.org/10.1111/rssc.12412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel method for simulating the effects of gene silencing. Our approach combines relevant subject matter information provided by biological pathways with gene expression levels measured in regular conditions to predict the behaviour of the system after one of the genes has been silenced. We achieve this by modelling gene silencing as an external intervention in a causal graphical model. To account for the uncertainty that is associated with the structure learning of the graphical model, we adopt a bootstrap approach. We illustrate our proposal on a Drosophila melanogaster gene silencing experiment.},
  archive      = {J_JRSSSC},
  author       = {Djordjilović, Vera and Chiogna, Monica and Romualdi, Chiara},
  doi          = {10.1111/rssc.12412},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {4},
  pages        = {887-907},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Simulating gene silencing through intervention analysis},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modelling fuel injector spray characteristics in jet engines
by using vine copulas. <em>JRSSSC</em>, <em>69</em>(4), 863–886. (<a
href="https://doi.org/10.1111/rssc.12421">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emission requirements for jet engines are becoming more stringent and the combustion process determines pollutant emissions. Therefore, we model the distribution of fuel drops generated by a fuel injector in a jet engine, which can be assumed to be a five-dimensional problem in terms of drop size, x -position, y -position, x -velocity and y -velocity. The data are generated by numerical simulations of the fuel atomization process for several jet engine operating conditions. In combustion simulations, the variables are usually assumed to be independent at the start of the simulation, which is clearly not so as our data show. The dependence between some of the variables is non-monotone and asymmetric, which makes the modelling task difficult. Our aim is to provide a realistic parametric model for the dependence structure. For this, we employ vine copulas which provide a flexible way to construct a multivariate distribution function. However, we need to use non-standard bivariate copulas as building blocks. Using this copula representation enables us to create realistic samples of fuel spray droplets which improve the prediction of the combustion process and the pollutant emissions. Moreover, this approach is significantly faster than solving the set of differential equations describing fuel disintegration.},
  archive      = {J_JRSSSC},
  author       = {Coblenz, Maximilian and Holz, Simon and Bauer, Hans-Jörg and Grothe, Oliver and Koch, Rainer},
  doi          = {10.1111/rssc.12421},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {4},
  pages        = {863-886},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Modelling fuel injector spray characteristics in jet engines by using vine copulas},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Small sample corrections for wald tests in latent variable
models. <em>JRSSSC</em>, <em>69</em>(4), 841–861. (<a
href="https://doi.org/10.1111/rssc.12414">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Latent variable models are commonly used in psychology and increasingly used for analysing brain imaging data. Such studies typically involve a small number of participants ( n &lt;100), where standard asymptotic results often fail to control the type 1 error appropriately. The paper presents two corrections improving the control of the type 1 error of Wald tests in latent variable models estimated by using maximum likelihood. First, we derive a correction for the bias of the maximum likelihood estimator of the variance parameters. This enables us to estimate corrected standard errors for model parameters and corrected Wald statistics. Second, we use a Student t -distribution instead of a Gaussian distribution to account for the variability of the variance estimator. The degrees of freedom of the Student t -distributions are estimated by using a Satterthwaite approximation. A simulation study based on data from two published brain imaging studies demonstrates that combining these two corrections provides superior control of the type 1 error rate compared with the uncorrected Wald test, despite being conservative for some parameters. The methods proposed are implemented in the R package lavaSearch2, which is available from https://cran.r-project.org/web/packages/lavaSearch2 .},
  archive      = {J_JRSSSC},
  author       = {Ozenne, Brice and Fisher, Patrick M. and Budtz-J⊘rgensen, Esben},
  doi          = {10.1111/rssc.12414},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {4},
  pages        = {841-861},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Small sample corrections for wald tests in latent variable models},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Global household energy model: A multivariate hierarchical
approach to estimating trends in the use of polluting and clean fuels
for cooking. <em>JRSSSC</em>, <em>69</em>(4), 815–839. (<a
href="https://doi.org/10.1111/rssc.12428">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 2017 an estimated 3 billion people used polluting fuels and technologies as their primary cooking solution, with 3.8 million deaths annually attributed to household exposure to the resulting fine particulate matter air pollution. Currently, health burdens are calculated by using aggregations of fuel types, e.g. solid fuels, as country level estimates of the use of specific fuel types, e.g. wood and charcoal, are unavailable. To expand the knowledge base about effects of household air pollution on health, we develop and implement a novel Bayesian hierarchical model, based on generalized Dirichlet–multinomial distributions, that jointly estimates non-linear trends in the use of eight key fuel types, overcoming several data-specific challenges including missing or combined fuel use values. We assess model fit by using within-sample predictive analysis and an out-of-sample prediction experiment to evaluate the model&#39;s forecasting performance.},
  archive      = {J_JRSSSC},
  author       = {Stoner, Oliver and Shaddick, Gavin and Economou, Theo and Gumy, Sophie and Lewis, Jessica and Lucio, Itzel and Ruggeri, Giulia and Adair-Rohani, Heather},
  doi          = {10.1111/rssc.12428},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {4},
  pages        = {815-839},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Global household energy model: A multivariate hierarchical approach to estimating trends in the use of polluting and clean fuels for cooking},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hybrid approach for the stratified mark-specific
proportional hazards model with missing covariates and missing marks,
with application to vaccine efficacy trials. <em>JRSSSC</em>,
<em>69</em>(4), 791–814. (<a
href="https://doi.org/10.1111/rssc.12417">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deployment of the recently licensed tetravalent dengue vaccine based on a chimeric yellow fever virus, CYD-TDV, requires understanding of how the risk of dengue disease in vaccine recipients depends jointly on a host biomarker measured after vaccination (neutralization titre—neutralizing antibodies) and on a ‘mark’ feature of the dengue disease failure event (the amino acid sequence distance of the dengue virus to the dengue sequence represented in the vaccine). The CYD14 phase 3 trial of CYD-TDV measured neutralizing antibodies via case–cohort sampling and the mark in dengue disease failure events, with about a third missing marks. We addressed the question of interest by developing inferential procedures for the stratified mark-specific proportional hazards model with missing covariates and missing marks. Two hybrid approaches are investigated that leverage both augmented inverse probability weighting and nearest neighbourhood hot deck multiple imputation. The two approaches differ in how the imputed marks are pooled in estimation. Our investigation shows that nearest neighbourhood hot deck imputation can lead to biased estimation without properly selected neighbourhoods. Simulations show that the hybrid methods developed perform well with unbiased nearest neighbourhood hot deck imputations from proper neighbourhood selection. The new methods applied to CYD14 show that neutralizing antibody level is strongly inversely associated with the risk of dengue disease in vaccine recipients, more strongly against dengue viruses with shorter distances.},
  archive      = {J_JRSSSC},
  author       = {Sun, Yanqing and Qi, Li and Heng, Fei and Gilbert, Peter B.},
  doi          = {10.1111/rssc.12417},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {4},
  pages        = {791-814},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {A hybrid approach for the stratified mark-specific proportional hazards model with missing covariates and missing marks, with application to vaccine efficacy trials},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inference for extreme values under threshold-based stopping
rules. <em>JRSSSC</em>, <em>69</em>(4), 765–789. (<a
href="https://doi.org/10.1111/rssc.12420">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a propensity for an extreme value analysis to be conducted as a consequence of a large flooding event. This timing of the analysis introduces bias and poor coverage probabilities into the associated risk assessments and leads subsequently to inefficient flood protection schemes. We explore these problems through studying stochastic stopping criteria and propose new likelihood-based inferences that mitigate against these difficulties. Our methods are illustrated through the analysis of the river Lune, following its experiencing the UK&#39;s largest ever measured flow event in 2015. We show that without accounting for this stopping feature there would be substantial overdesign in response to the event.},
  archive      = {J_JRSSSC},
  author       = {Barlow, Anna Maria and Sherlock, Chris and Tawn, Jonathan},
  doi          = {10.1111/rssc.12420},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {4},
  pages        = {765-789},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Inference for extreme values under threshold-based stopping rules},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Causal mechanism of extreme river discharges in the upper
danube basin network. <em>JRSSSC</em>, <em>69</em>(4), 741–764. (<a
href="https://doi.org/10.1111/rssc.12415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extreme hydrological events in the Danube river basin may severely impact human populations, aquatic organisms and economic activity. One often characterizes the joint structure of extreme events by using the theory of multivariate and spatial extremes and its asymptotically justified models. There is interest, however, in cascading extreme events and whether one event causes another. We argue that an improved understanding of the mechanism underlying severe events is achieved by combining extreme value modelling and causal discovery. We construct a causal inference method relying on the notion of the Kolmogorov complexity of extreme conditional quantiles. Tail quantities are derived by using multivariate extreme value models, and causal-induced asymmetries in the data are explored through the minimum description length principle. Our method CausEV for causality for extreme values uncovers causal relationships between summer extreme river discharges in the upper Danube basin and finds significant causal links between the Danube and its Alpine tributary Lech.},
  archive      = {J_JRSSSC},
  author       = {Mhalla, Linda and Chavez-Demoulin, Valérie and Dupuis, Debbie J.},
  doi          = {10.1111/rssc.12415},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {4},
  pages        = {741-764},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Causal mechanism of extreme river discharges in the upper danube basin network},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Longitudinal networks of dyadic relationships using latent
trajectories: Evidence from the european interbank market.
<em>JRSSSC</em>, <em>69</em>(4), 711–739. (<a
href="https://doi.org/10.1111/rssc.12413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Financial markets are ultimately seen as a collection of dyadic transactions. We study the temporal evolution of dyadic relationships in the European interbank market, as induced by monetary transactions registered in the electronic market for interbank deposits (e-MID) during a period of 10 years (2006–2015). In particular, we keep track of how reciprocal exchange patterns have varied with macro events and exogenous shocks and with the emergence of the Global Financial Crisis in 2008. The approach adopted extends the model of Holland and Leinhardt to a longitudinal setting where individuals’ temporal trajectories for the tendency to connect and reciprocate transactions are explicitly modelled through splines or polynomials, and individual-specific parameters. We estimate the model by an iterative algorithm that maximizes the log-likelihood for every ordered pair of units. The empirical application shows that the methodology proposed may be applied to large networks and represents the process of exchange at a fine-grained level. Further results are available in on-line supplementary material .},
  archive      = {J_JRSSSC},
  author       = {Bianchi, Federica and Bartolucci, Francesco and Peluso, Stefano and Mira, Antonietta},
  doi          = {10.1111/rssc.12413},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {4},
  pages        = {711-739},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Longitudinal networks of dyadic relationships using latent trajectories: Evidence from the european interbank market},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The harmonic mean χ2-test to substantiate scientific
findings. <em>JRSSSC</em>, <em>69</em>(3), 697–708. (<a
href="https://doi.org/10.1111/rssc.12410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical methodology plays a crucial role in drug regulation. Decisions by the US Food and Drug Administration or European Medicines Agency are typically made based on multiple primary studies testing the same medical product, where the two-trials rule is the standard requirement, despite shortcomings. A new approach is proposed for this task based on the harmonic mean of the squared study-specific test statistics. Appropriate scaling ensures that, for any number of independent studies, the null distribution is a χ 2 -distribution with 1 degree of freedom. This gives rise to a new method for combining one-sided p -values and calculating confidence intervals for the overall treatment effect. Further properties are discussed and a comparison with the two-trials rule is made, as well as with alternative research synthesis methods. An attractive feature of the new approach is that a claim of success requires each study to be convincing on its own to a certain degree depending on the overall level of significance and the number of studies. The new approach is motivated by and applied to data from five clinical trials investigating the effect of carvedilol for the treatment of patients with moderate to severe heart failure.},
  archive      = {J_JRSSSC},
  author       = {Held, Leonhard},
  doi          = {10.1111/rssc.12410},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {3},
  pages        = {697-708},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {The harmonic mean χ2-test to substantiate scientific findings},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A spatially varying distributed lag model with application
to an air pollution and term low birth weight study. <em>JRSSSC</em>,
<em>69</em>(3), 681–696. (<a
href="https://doi.org/10.1111/rssc.12407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Distributed lag models have been used to identify critical pregnancy periods of exposure (i.e. critical exposure windows) to air pollution in studies of pregnancy outcomes. However, much of the previous work in this area has ignored the possibility of spatial variability in the lagged health effect parameters that may result from exposure characteristics and/or residual confounding. We develop a spatially varying Gaussian process model for critical windows called ‘SpGPCW’ and use it to investigate geographic variability in the association between term low birth weight and average weekly concentrations of ozone and PM 2.5 during pregnancy by using birth records from North Carolina. SpGPCW is designed to accommodate areal level spatial correlation between lagged health effect parameters and temporal smoothness in risk estimation across pregnancy. Through simulation and a real data application, we show that the consequences of ignoring spatial variability in the lagged health effect parameters include less reliable inference for the parameters and diminished ability to identify true critical window sets, and we investigate the use of existing Bayesian model comparison techniques as tools for determining the presence of spatial variability. We find that exposure to PM 2.5 is associated with elevated term low birth weight risk in selected weeks and counties and that ignoring spatial variability results in null associations during these periods. An R package (SpGPCW) has been developed to implement the new method.},
  archive      = {J_JRSSSC},
  author       = {Warren, Joshua L. and Luben, Thomas J. and Chang, Howard H.},
  doi          = {10.1111/rssc.12407},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {3},
  pages        = {681-696},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {A spatially varying distributed lag model with application to an air pollution and term low birth weight study},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bayesian group sequential small n sequential
multiple-assignment randomized trial. <em>JRSSSC</em>, <em>69</em>(3),
663–680. (<a href="https://doi.org/10.1111/rssc.12406">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A small n , sequential, multiple-assignment, randomized trial (called ‘snSMART’) is a small sample multistage design where participants may be rerandomized to treatment on the basis of intermediate end points. This design is motivated by the ‘A randomized multicenter study for isolated skin vasculitis’ trial ( NCT02939573 ): an on-going snSMART design focusing on the evaluation of three drugs for isolated skin vasculitis. By formulating an interim decision rule for removing one of the treatments, we use a Bayesian model and the resulting posterior distributions to provide sufficient evidence that one treatment is inferior to the other treatments before enrolling more participants. By doing so, we can remove the worst performing treatment at an interim analysis and prevent the subsequent participants from receiving the removed treatment. On the basis of simulation results, we have evidence that the treatment response rates can still be unbiasedly and efficiently estimated in our new design, especially for the treatments with higher response rates. In addition, by adjusting the decision rule criteria for the posterior probabilities, we can control the probability of incorrectly removing an effective treatment.},
  archive      = {J_JRSSSC},
  author       = {Chao, Yan-Cheng and Braun, Thomas M. and Tamura, Roy N. and Kidwell, Kelley M.},
  doi          = {10.1111/rssc.12406},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {3},
  pages        = {663-680},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {A bayesian group sequential small n sequential multiple-assignment randomized trial},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generalized partially linear models on riemannian manifolds.
<em>JRSSSC</em>, <em>69</em>(3), 641–661. (<a
href="https://doi.org/10.1111/rssc.12411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce generalized partially linear models with covariates on Riemannian manifolds. These models, like ordinary generalized linear models, are a generalization of partially linear models on Riemannian manifolds that allow for scalar response variables with error distribution models other than a normal distribution. Partially linear models are particularly useful when some of the covariates of the model are elements of a Riemannian manifold, because the curvature of these spaces makes it difficult to define parametric models. The model was developed to address an interesting application: the prediction of children&#39;s garment fit based on three-dimensional scanning of their bodies. For this reason, we focus on logistic and ordinal models and on the important and difficult case where the Riemannian manifold is the three-dimensional case of Kendall&#39;s shape space. An experimental study with a well-known three-dimensional database is carried out to check the goodness of the procedure. Finally, it is applied to a three-dimensional database obtained from an anthropometric survey of the Spanish child population. A comparative study with related techniques is carried out.},
  archive      = {J_JRSSSC},
  author       = {Simó, Amelia and Victoria Ibáñez, M. and Epifanio, Irene and Gimeno, Vicent},
  doi          = {10.1111/rssc.12411},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {3},
  pages        = {641-661},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Generalized partially linear models on riemannian manifolds},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel regularized approach for functional data clustering:
An application to milking kinetics in dairy goats. <em>JRSSSC</em>,
<em>69</em>(3), 623–640. (<a
href="https://doi.org/10.1111/rssc.12404">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by an application to the clustering of milking kinetics of dairy goats, we propose a novel approach for functional data clustering. This issue is of growing interest in precision livestock farming, which is largely based on the development of data acquisition automation and on the development of interpretative tools to capitalize on high throughput raw data and to generate benchmarks for phenotypic traits. The method that we propose in the paper falls in this context. Our methodology relies on a piecewise linear estimation of curves based on a novel regularized change-point-estimation method and on the k -means algorithm applied to a vector of coefficients summarizing the curves. The statistical performance of our method is assessed through numerical experiments and is thoroughly compared with existing experiments. Our technique is finally applied to milk emission kinetics data with the aim of a better characterization of interanimal variability and towards a better understanding of the lactation process.},
  archive      = {J_JRSSSC},
  author       = {Denis, C. and Lebarbier, E. and Lévy-Leduc, C. and Martin, O. and Sansonnet, L.},
  doi          = {10.1111/rssc.12404},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {3},
  pages        = {623-640},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {A novel regularized approach for functional data clustering: An application to milking kinetics in dairy goats},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An optimal design for hierarchical generalized group
testing. <em>JRSSSC</em>, <em>69</em>(3), 607–621. (<a
href="https://doi.org/10.1111/rssc.12409">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Choosing an optimal strategy for hierarchical group testing is an important problem for practitioners who are interested in disease screening with limited resources. For example, when screening for infectious diseases in large populations, it is important to use algorithms that minimize the cost of potentially expensive assays. Black and co-workers described this as an intractable problem unless the number of individuals to screen is small. They proposed an approximation to an optimal strategy that is difficult to implement for large population sizes. We develop an optimal design with respect to the expected total number of tests that can be obtained by using a novel dynamic programming algorithm. We show that this algorithm is substantially more efficient than the approach that was proposed by Black and co-workers. In addition, we compare the two designs for imperfect tests. R code is provided for practitioners.},
  archive      = {J_JRSSSC},
  author       = {Malinovsky, Yaakov and Haber, Gregory and Albert, Paul S.},
  doi          = {10.1111/rssc.12409},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {3},
  pages        = {607-621},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {An optimal design for hierarchical generalized group testing},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A joint confidence region for an overall ranking of
populations. <em>JRSSSC</em>, <em>69</em>(3), 589–606. (<a
href="https://doi.org/10.1111/rssc.12402">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {National statistical agencies lack statistical methodology to express uncertainty in their released estimated overall rankings. For example, the US Census Bureau produced an ‘explicit’ ranking of the states based on observed sample estimates during 2011 of mean travel time to work. Current literature provides measures of uncertainty in estimated individual ranks, but not a direct measure of uncertainty for the estimated overall ranking. We construct and visualize a joint confidence region for the true unknown overall ranking that provides a measure of uncertainty in the estimated overall ranking.},
  archive      = {J_JRSSSC},
  author       = {Klein, Martin and Wright, Tommy and Wieczorek, Jerzy},
  doi          = {10.1111/rssc.12402},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {3},
  pages        = {589-606},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {A joint confidence region for an overall ranking of populations},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variable selection in functional linear concurrent
regression. <em>JRSSSC</em>, <em>69</em>(3), 565–587. (<a
href="https://doi.org/10.1111/rssc.12408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel method for variable selection in functional linear concurrent regression. Our research is motivated by a fisheries footprint study where the goal is to identify important time-varying sociostructural drivers influencing patterns of seafood consumption, and hence the fisheries footprint, over time, as well as estimating their dynamic effects. We develop a variable-selection method in functional linear concurrent regression extending the classically used scalar-on-scalar variable-selection methods like the lasso, smoothly clipped absolute deviation (SCAD) and minimax concave penalty (MCP). We show that in functional linear concurrent regression the variable-selection problem can be addressed as a group lasso, and their natural extension: the group SCAD or a group MCP problem. Through simulations, we illustrate that our method, particularly with the group SCAD or group MCP, can pick out the relevant variables with high accuracy and has minuscule false positive and false negative rate even when data are observed sparsely, are contaminated with noise and the error process is highly non-stationary. We also demonstrate two real data applications of our method in studies of dietary calcium absorption and fisheries footprint in the selection of influential time-varying covariates.},
  archive      = {J_JRSSSC},
  author       = {Ghosal, Rahul and Maity, Arnab and Clark, Timothy and Longo, Stefano B.},
  doi          = {10.1111/rssc.12408},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {3},
  pages        = {565-587},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Variable selection in functional linear concurrent regression},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiple imputation of binary multilevel missing not at
random data. <em>JRSSSC</em>, <em>69</em>(3), 547–564. (<a
href="https://doi.org/10.1111/rssc.12401">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a selection model-based multilevel imputation approach to be used within the fully conditional specification framework for multiple imputation. Concretely, we apply a censored bivariate probit model to describe binary variables assumed to be missing not at random. The first equation of the model defines the regression model for the missing data mechanism. The second equation specifies the regression model of the variable to be imputed. The non-random selection of the binary data is mapped by correlations between the error terms of the two regression models. Hierarchical data structures are modelled by random intercepts in both equations. To fit the novel imputation model we use maximum likelihood and adaptive Gauss–Hermite quadrature. A comprehensive simulation study shows the overall performance of the approach. We test its usefulness for empirical research by applying it to a common problem in social scientific research: the emergence of educational aspirations. Our software is designed to be used in the R package mice.},
  archive      = {J_JRSSSC},
  author       = {Hammon, Angelina and Zinn, Sabine},
  doi          = {10.1111/rssc.12401},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {3},
  pages        = {547-564},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Multiple imputation of binary multilevel missing not at random data},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Structured penalized regression for drug sensitivity
prediction. <em>JRSSSC</em>, <em>69</em>(3), 525–545. (<a
href="https://doi.org/10.1111/rssc.12400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale in vitro drug sensitivity screens are an important tool in personalized oncology to predict the effectiveness of potential cancer drugs. The prediction of the sensitivity of cancer cell lines to a panel of drugs is a multivariate regression problem with high dimensional heterogeneous multiomics data as input data and with potentially strong correlations between the outcome variables which represent the sensitivity to the different drugs. We propose a joint penalized regression approach with structured penalty terms which enable us to utilize the correlation structure between drugs with group-lasso-type penalties and at the same time address the heterogeneity between ‘omics’ data sources by introducing data-source-specific penalty factors to penalize different data sources differently. By combining integrative penalty factors (IPFs) with the tree-guided group lasso, we create a method called ‘IPF-tree-lasso’. We present a unified framework to transform more general IPF-type methods to the original penalized method. Because the structured penalty terms have multiple parameters, we demonstrate how the interval search ‘Efficient parameter selection via global optimization’ algorithm can be used to optimize multiple penalty parameters efficiently. Simulation studies show that IPF-tree-lasso can improve the prediction performance compared with other lasso-type methods, in particular for heterogeneous sources of data. Finally, we employ the new methods to analyse data from the ‘Genomics of drug sensitivity in cancer’ project.},
  archive      = {J_JRSSSC},
  author       = {Zhao, Zhi and Zucknick, Manuela},
  doi          = {10.1111/rssc.12400},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {3},
  pages        = {525-545},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Structured penalized regression for drug sensitivity prediction},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust and adaptive anticoagulant control. <em>JRSSSC</em>,
<em>69</em>(3), 503–524. (<a
href="https://doi.org/10.1111/rssc.12403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a control theory approach to adaptive dose allocation of anticoagulants, based on an analysis of records of 152 patients on long-term warfarin treatment. We consider a selection of statistical models for the relationship between the dose of drug and subsequent blood clotting speed, measured through the international normalized ratio. Our main focus is on subsequent use of the model in guiding the choice of the next dose adaptively as patient-specific information accrues. We compare a naive long-term approach with a proportional-integral-plus method, with parameters estimated by either linear quadratic optimization or by stochastic resource allocation. We demonstrate advantages of the control approaches in comparison with a naive approach in simulations and through calculation of robust stability margins for the observed data.},
  archive      = {J_JRSSSC},
  author       = {Avery, Peter and Clairon, Quentin and Henderson, Robin and James Taylor, C. and Wilson, Emma},
  doi          = {10.1111/rssc.12403},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {3},
  pages        = {503-524},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Robust and adaptive anticoagulant control},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Applied statistics. <em>JRSSSC</em>, <em>69</em>(3),
501–502. (<a href="https://doi.org/10.1111/rssc.12360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_JRSSSC},
  author       = {Friel, N. and Smith, P. W. F. and Canale, A. and Demiris, N. and Fan, Y. and Gaetan, C. and Huang, X. and Iannario, M. and Jaki, T. and Jermyn, I. H. and Kalogeropoulos, K. and Latouche, P. and Liverani, S. and Marra, G. and Mateu, J. and De Menezes, R. X. and Ntzoufras, I. and Oberski, D. and Papamarkou, T. and Prangle, D. and Radice, R. and Sykulski, A.},
  doi          = {10.1111/rssc.12360},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {3},
  pages        = {501-502},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Applied statistics},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Factor-augmented bayesian cointegration models: A case-study
on the soybean crush spread. <em>JRSSSC</em>, <em>69</em>(2), 483–500.
(<a href="https://doi.org/10.1111/rssc.12395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate how vector auto-regressive models can be used to study the soybean crush spread. By crush spread we mean a time series marking the difference between a weighted combination of the value of soymeal and soyoil to the value of the original soybeans. Commodity industry practitioners often use fixed prescribed values for these weights, which do not take into account any time-varying effects or any financial-market-based dynamic features that can be discerned from futures price data. We address this issue by proposing an appropriate time series model with cointegration. Our model consists of an extension of a particular vector auto-regressive model that is used widely in econometrics. Our extensions are inspired by the problem at hand and allow for a time-varying covariance structure and a time-varying intercept to account for seasonality. To perform Bayesian inference we design an efficient Markov chain Monte Carlo algorithm, which is based on the approach of Koop and his co-workerss. Our investigations on prices obtained from futures contracts data confirmed that the added features in our model are useful in reliable statistical determination of the crush spread. Although the interest here is on the soybean crush spread, our approach is applicable also to other tradable spreads such as oil and energy-based crack and spark spreads.},
  archive      = {J_JRSSSC},
  author       = {Marowka, Maciej and Peters, Gareth W. and Kantas, Nikolas and Bagnarosa, Guillaume},
  doi          = {10.1111/rssc.12395},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {2},
  pages        = {483-500},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Factor-augmented bayesian cointegration models: A case-study on the soybean crush spread},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian hierarchical modelling of growth curve derivatives
via sequences of quotient differences. <em>JRSSSC</em>, <em>69</em>(2),
459–481. (<a href="https://doi.org/10.1111/rssc.12399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Growth curve studies are typically conducted to evaluate differences between group or treatment-specific curves. Most analyses focus solely on the growth curves, but it has been argued that the derivative of growth curves can highlight differences between groups that may be masked when considering the raw curves only. Motivated by the desire to estimate derivative curves hierarchically, we introduce a new sequence of quotient differences (empirical derivatives) which, among other things, are well behaved near the boundaries compared with other sequences in the literature. Using the sequence of quotient differences, we develop a Bayesian method to estimate curve derivatives in a multilevel setting (a common scenario in growth studies) and show how the method can be used to estimate individual and group derivative curves and to make comparisons. We apply the new methodology to data collected from a study conducted to explore the effect that radiation-based therapies have on growth in female children diagnosed with acute lymphoblastic leukaemia.},
  archive      = {J_JRSSSC},
  author       = {Page, Garritt L. and Rodríguez-Álvarez, María Xosé and Lee, Dae-Jin},
  doi          = {10.1111/rssc.12399},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {2},
  pages        = {459-481},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Bayesian hierarchical modelling of growth curve derivatives via sequences of quotient differences},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A flexible parametric modelling framework for survival
analysis. <em>JRSSSC</em>, <em>69</em>(2), 429–457. (<a
href="https://doi.org/10.1111/rssc.12398">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a general, flexible, parametric survival modelling framework which encompasses key shapes of hazard functions (constant; increasing; decreasing; up then down; down then up) and various common survival distributions (log-logistic; Burr type XII; Weibull; Gompertz) and includes defective distributions (cure models). This generality is achieved by using four distributional parameters: two scale-type parameters—one of which relates to accelerated failure time (AFT) modelling; the other to proportional hazards (PH) modelling—and two shape parameters. Furthermore, we advocate ‘multiparameter regression’ whereby more than one distributional parameter depends on covariates—rather than the usual convention of having a single covariate-dependent (scale) parameter. This general formulation unifies the most popular survival models, enabling us to consider the practical value of possible modelling choices. In particular, we suggest introducing covariates through just one or other of the two scale parameters (covering AFT and PH models), and through a ‘power’ shape parameter (covering more complex non-AFT or non-PH effects); the other shape parameter remains covariate independent and handles automatic selection of the baseline distribution. We explore inferential issues and compare with alternative models through various simulation studies, with particular focus on evidence concerning the need, or otherwise, to include both AFT and PH parameters. We illustrate the efficacy of our modelling framework by using data from lung cancer, melanoma and kidney function studies. Censoring is accommodated throughout.},
  archive      = {J_JRSSSC},
  author       = {Burke, Kevin and Jones, M. C. and Noufaily, Angela},
  doi          = {10.1111/rssc.12398},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {2},
  pages        = {429-457},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {A flexible parametric modelling framework for survival analysis},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Assessing heterogeneity in transition propensity in
multistate capture–recapture data. <em>JRSSSC</em>, <em>69</em>(2),
413–427. (<a href="https://doi.org/10.1111/rssc.12392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multistate capture–recapture models are a useful tool to help to understand the dynamics of movement within discrete capture–recapture data. The standard multistate capture–recapture model, however, relies on assumptions of homogeneity within the population with respect to survival, capture and transition probabilities. There are many ways in which this model can be generalized so some guidance on what is really needed is highly desirable. Within the paper we derive a new test that can detect heterogeneity in transition propensity and show its good power by using simulation and application to a Canada goose data set. We also demonstrate that existing tests which have traditionally been used to diagnose memory are in fact sensitive to other forms of transition heterogeneity and we propose modified tests which can distinguish between memory and other forms of transition heterogeneity.},
  archive      = {J_JRSSSC},
  author       = {Jeyam, Anita and McCrea, Rachel and Pradel, Roger},
  doi          = {10.1111/rssc.12392},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {2},
  pages        = {413-427},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Assessing heterogeneity in transition propensity in multistate Capture–Recapture data},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Using cox regression to develop linear rank tests with
zero-inflated clustered data. <em>JRSSSC</em>, <em>69</em>(2), 393–411.
(<a href="https://doi.org/10.1111/rssc.12396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-inflated data arise in many fields of study. When comparing zero-inflated data between two groups with independent subjects, a 2 degree-of-freedom test has been developed, which is the sum of a 1 degree-of-freedom Pearson χ 2 -test for the 2×2 table of group versus dichotomized outcome (0,&gt;0) and a 1 degree-of-freedom Wilcoxon rank sum test for the values of the outcome ‘&gt;0’. Here, we extend this 2 degrees-of-freedom test to clustered data settings. We first propose the use of an estimating equations score statistic from a time-varying weighted Cox regression model under naive independence, with a robust sandwich variance estimator to account for clustering. Since our proposed test statistics can be put in the framework of a Cox model, to gain efficiency over naive independence, we apply a generalized estimating equations Cox model with a non-independence ‘working correlation’ between observations in a cluster. The methods proposed are applied to a General Social Survey study of days with mental health problems in a month, in which 52.3\% of subjects report that they have no days with problems: a zero-inflated outcome. A simulation study is used to compare our proposed test statistics with previously proposed zero-inflated test statistics.},
  archive      = {J_JRSSSC},
  author       = {Lipsitz, Stuart R. and Fitzmaurice, Garrett M. and Sinha, Debajyoti and Cole, Alexander P. and Meyer, Christian P. and Trinh, Quoc-Dien},
  doi          = {10.1111/rssc.12396},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {2},
  pages        = {393-411},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Using cox regression to develop linear rank tests with zero-inflated clustered data},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modelling environmental DNA data; bayesian variable
selection accounting for false positive and false negative errors.
<em>JRSSSC</em>, <em>69</em>(2), 377–392. (<a
href="https://doi.org/10.1111/rssc.12390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Environmental DNA is a survey tool with rapidly expanding applications for assessing the presence of a species at surveyed sites. Environmental DNA methodology is known to be prone to false negative and false positive errors at the data collection and laboratory analysis stages. Existing models for environmental DNA data require augmentation with additional sources of information to overcome identifiability issues of the likelihood function and do not account for environmental covariates that predict the probability of species presence or the probabilities of error. We present a novel Bayesian model for analysing environmental DNA data by proposing informative prior distributions for logistic regression coefficients that enable us to overcome parameter identifiability, while performing efficient Bayesian variable selection. Our methodology does not require the use of transdimensional algorithms and provides a general framework for performing Bayesian variable selection under informative prior distributions in logistic regression models.},
  archive      = {J_JRSSSC},
  author       = {Griffin, Jim E. and Matechou, Eleni and Buxton, Andrew S. and Bormpoudakis, Dimitrios and Griffiths, Richard A.},
  doi          = {10.1111/rssc.12390},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {2},
  pages        = {377-392},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Modelling environmental DNA data; bayesian variable selection accounting for false positive and false negative errors},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation and inference in mixed effect regression models
using shape constraints, with application to tree height estimation.
<em>JRSSSC</em>, <em>69</em>(2), 353–375. (<a
href="https://doi.org/10.1111/rssc.12388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimation of tree height given diameter is an important part of the forest inventory analysis of the US Forest Service. Existing methods use parametric models to estimate the curve. We propose a semiparametric model in which log (height) is a smooth, increasing and concave function of diameter, with a random-plot component and fixed effect covariates. Large sample properties and inference methods that work well in practice are derived. Proposed inference methods use approximate normal distributions for the fixed effects and a likelihood ratio test for the significance of the random effect. A closed form approximate prediction method is provided and overall it outperformed competitors for both a simulation and a real data application. The methods are implemented by the cgamm routine in the R package cgam and can be used for a wide range of mixed model applications.},
  archive      = {J_JRSSSC},
  author       = {Liao, Xiyue and Meyer, Mary C.},
  doi          = {10.1111/rssc.12388},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {2},
  pages        = {353-375},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Estimation and inference in mixed effect regression models using shape constraints, with application to tree height estimation},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating seal pup production in the greenland sea by using
bayesian hierarchical modelling. <em>JRSSSC</em>, <em>69</em>(2),
327–352. (<a href="https://doi.org/10.1111/rssc.12397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Greenland Sea is an important breeding ground for harp and hooded seals. Estimates of annual seal pup production are critical factors in the estimation of abundance that is needed for management of the species. These estimates are usually based on counts from aerial photographic surveys. However, only a minor part of the whelping region can be photographed, because of its large extent. To estimate total seal pup production, we propose a Bayesian hierarchical modelling approach motivated by viewing the seal pup appearances as a realization of a log-Gaussian Cox process by using covariate information from satellite imagery as a proxy for ice thickness. For inference, we utilize the stochastic partial differential equation module of the integrated nested Laplace approximation framework. In a case-study using survey data from 2012, we compare our results with existing methodology in a comprehensive cross-validation study. The results of the study indicate that our method improves local estimation performance, and that the increased uncertainty of prediction of our method is required to obtain calibrated count predictions. This suggests that the sampling density of the survey design may not be sufficient to obtain reliable estimates of seal pup production.},
  archive      = {J_JRSSSC},
  author       = {Jullum, Martin and Thorarinsdottir, Thordis and Bachl, Fabian E.},
  doi          = {10.1111/rssc.12397},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {2},
  pages        = {327-352},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Estimating seal pup production in the greenland sea by using bayesian hierarchical modelling},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian protein sequence and structure alignment.
<em>JRSSSC</em>, <em>69</em>(2), 301–325. (<a
href="https://doi.org/10.1111/rssc.12394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The structure of a protein is crucial in determining its functionality and is much more conserved than sequence during evolution. A key task in structural biology is to compare protein structures to determine evolutionary relationships, to estimate the function of newly discovered structures and to predict unknown structures. We propose a Bayesian method for protein structure alignment, with the prior on alignments based on functions which penalize ‘gaps’ in the aligned sequences. We show how a broad class of penalty functions fits into this framework, and how the resulting posterior distribution can be efficiently sampled. A commonly used gap penalty function is shown to be a special case, and we propose a new penalty function which alleviates an undesirable feature of the commonly used penalty. We illustrate our method on benchmark data sets and find that it competes well with popular tools from computational biology. Our method has the benefit of being able potentially to explore multiple competing alignments and to quantify their merits probabilistically. The framework naturally enables further information such as amino acid sequence to be included and could be adapted to other situations such as flexible proteins or domain swaps.},
  archive      = {J_JRSSSC},
  author       = {Fallaize, Christopher J. and Green, Peter J. and Mardia, Kanti V. and Barber, Stuart},
  doi          = {10.1111/rssc.12394},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {2},
  pages        = {301-325},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Bayesian protein sequence and structure alignment},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient data augmentation for multivariate probit models
with panel data: An application to general practitioner decision making
about contraceptives. <em>JRSSSC</em>, <em>69</em>(2), 277–300. (<a
href="https://doi.org/10.1111/rssc.12393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper considers the problem of estimating a multivariate probit model in a panel data setting with emphasis on sampling a high dimensional correlation matrix and improving the overall efficiency of the data augmentation approach. We reparameterize the correlation matrix in a principled way and then carry out efficient Bayesian inference by using Hamiltonian Monte Carlo sampling. We also propose a novel antithetic variable method to generate samples from the posterior distribution of the random effects and regression coefficients, resulting in significant gains in efficiency. We apply the methodology by analysing stated preference data obtained from Australian general practitioners evaluating alternative contraceptive products. Our analysis suggests that the joint probability of discussing combinations of contraceptive products with a patient shows medical practice variation among the general practitioners, which indicates some resistance even to discuss these products, let alone to recommend them.},
  archive      = {J_JRSSSC},
  author       = {Chin, Vincent and Gunawan, David and Fiebig, Denzil G. and Kohn, Robert and Sisson, Scott A.},
  doi          = {10.1111/rssc.12393},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {2},
  pages        = {277-300},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Efficient data augmentation for multivariate probit models with panel data: An application to general practitioner decision making about contraceptives},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The analysis of transformations for profit-and-loss data.
<em>JRSSSC</em>, <em>69</em>(2), 251–275. (<a
href="https://doi.org/10.1111/rssc.12389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyse data on the performance of investment funds, 99 out of 309 of which report a loss, and on the profitability of 1405 firms, 407 of which report losses. The problem in both cases is to use regression to predict performance from sets of explanatory variables. In one case, it is clear from scatter plots of the data that the negative responses have a lower variance than the positive responses and a different relationship with the explanatory variables. Because the data include negative responses, the Box–Cox transformation cannot be used. We develop a robust version of an extension to the Yeo–Johnson transformation which allows different transformations for positive and negative responses. Tests and graphical methods from our robust analysis enable the detection of outliers, the assessment of values of the two transformation parameters and the building of simple regression models. Performance comparisons are made with non-parametric transformations.},
  archive      = {J_JRSSSC},
  author       = {Atkinson, Anthony C. and Riani, Marco and Corbellini, Aldo},
  doi          = {10.1111/rssc.12389},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {2},
  pages        = {251-275},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {The analysis of transformations for profit-and-loss data},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modelling the spatial extent and severity of extreme
european windstorms. <em>JRSSSC</em>, <em>69</em>(2), 223–250. (<a
href="https://doi.org/10.1111/rssc.12391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Windstorms are a primary natural hazard affecting Europe that are commonly linked to substantial property and infrastructural damage and are responsible for the largest spatially aggregated financial losses. Such extreme winds are typically generated by extratropical cyclone systems originating in the North Atlantic and passing over Europe. Previous statistical studies tend to model extreme winds at a given set of sites, corresponding to inference in an Eulerian framework. Such inference cannot incorporate knowledge of the life cycle and progression of extratropical cyclones across the region and is forced to make restrictive assumptions about the extremal dependence structure. We take an entirely different approach which overcomes these limitations by working in a Lagrangian framework. Specifically, we model the development of windstorms over time, preserving the physical characteristics linking the windstorm and the cyclone track, the path of local vorticity maxima, and make a key finding that the spatial extent of extratropical windstorms becomes more localized as its magnitude increases irrespective of the location of the storm track. Our model allows simulation of synthetic windstorm events to derive the joint distributional features over any set of sites giving physically consistent extrapolations to rarer events. From such simulations improved estimates of this hazard can be achieved in terms of both intensity and area affected.},
  archive      = {J_JRSSSC},
  author       = {Sharkey, Paul and Tawn, Jonathan A. and Brown, Simon J.},
  doi          = {10.1111/rssc.12391},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {2},
  pages        = {223-250},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Modelling the spatial extent and severity of extreme european windstorms},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modelling and prediction of financial trading networks: An
application to the new york mercantile exchange natural gas futures
market. <em>JRSSSC</em>, <em>69</em>(1), 195–218. (<a
href="https://doi.org/10.1111/rssc.12387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over recent years there has been a growing interest in using financial trading networks to understand the microstructure of financial markets. Most of the methodologies that have been developed so far for this have been based on the study of descriptive summaries of the networks such as the average node degree and the clustering coefficient. In contrast, this paper develops novel statistical methods for modelling sequences of financial trading networks. Our approach uses a stochastic block model to describe the structure of the network during each period, and then links multiple time periods by using a hidden Markov model. This structure enables us to identify events that affect the structure of the market and make accurate short-term prediction of future transactions. The methodology is illustrated by using data from the New York Mercantile Exchange natural gas futures market from January 2005 to December 2008.},
  archive      = {J_JRSSSC},
  author       = {Betancourt, Brenda and Rodríguez, Abel and Boyd, Naomi},
  doi          = {10.1111/rssc.12387},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {1},
  pages        = {195-218},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Modelling and prediction of financial trading networks: An application to the new york mercantile exchange natural gas futures market},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inference for biomedical data by using diffusion models with
covariates and mixed effects. <em>JRSSSC</em>, <em>69</em>(1), 167–193.
(<a href="https://doi.org/10.1111/rssc.12386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neurobiological data such as electroencephalography measurements pose a statistical challenge due to low spatial resolution and poor signal-to-noise ratio, as well as large variability from subject to subject. We propose a new modelling framework for this type of data based on stochastic processes. Stochastic differential equations with mixed effects are a popular framework for modelling biomedical data, e.g. in pharmacological studies. Whereas the inherent stochasticity of diffusion models accounts for prevalent model uncertainty or misspecification, random-effects model intersubject variability. The two-layer stochasticity, however, renders parameter inference challenging. Estimates are based on the discretized continuous time likelihood and we investigate finite sample and discretization bias. In applications, the comparison of, for example, treatment effects is often of interest. We discuss hypothesis testing and evaluate by simulations. Finally, we apply the framework to a statistical investigation of electroencephalography recordings from epileptic patients. We close the paper by examining asymptotics (the number of subjects going to ∞) of maximum likelihood estimators in multi-dimensional, non-linear and non-homogeneous stochastic differential equations with random effects and included covariates.},
  archive      = {J_JRSSSC},
  author       = {Ruse, Mareile Große and Samson, Adeline and Ditlevsen, Susanne},
  doi          = {10.1111/rssc.12386},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {1},
  pages        = {167-193},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Inference for biomedical data by using diffusion models with covariates and mixed effects},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A time-varying bayesian joint hierarchical copula model for
analysing recurrent events and a terminal event: An application to the
cardiovascular health study. <em>JRSSSC</em>, <em>69</em>(1), 151–166.
(<a href="https://doi.org/10.1111/rssc.12382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent events could be stopped by a terminal event, which commonly occurs in biomedical and clinical studies. Taking the Cardiovascular Health Study as a motivating example, patients can experience recurrent events of myocardial infarction (MI) or stroke during follow-up, which, however, can be truncated by death. Since death could be a devastating complication of MI or stroke recurrences, ignoring dependent censoring when analysing recurrent events may lead to invalid inference. The joint shared frailty model is widely used but with several limitations: two event processes are conditionally independent given the subject level frailty, which could be violated because the dependence may rely on unknown covariates varying across recurrences; the correlation between recurrent events and death is constant over time because of the same frailty within subject, but MI or stroke recurrences could have a time-varying influence on death due to higher risk of another event of MI or stroke after the first. We propose a time-varying joint hierarchical copula model under the Bayesian framework to accommodate correlation between recurrent events and dependence between two event processes which may change over time. The performance of our method is extensively evaluated by simulation studies, and lastly by the Cardiovascular Health Study for illustration.},
  archive      = {J_JRSSSC},
  author       = {Li, Zheng and Chinchilli, Vernon M. and Wang, Ming},
  doi          = {10.1111/rssc.12382},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {1},
  pages        = {151-166},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {A time-varying bayesian joint hierarchical copula model for analysing recurrent events and a terminal event: An application to the cardiovascular health study},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A full bayesian implementation of a generalized partial
credit model with an application to an international disability survey.
<em>JRSSSC</em>, <em>69</em>(1), 131–150. (<a
href="https://doi.org/10.1111/rssc.12385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generalized partial credit models (GPCMs) are ubiquitous in many applications in the health and medical sciences that use item response theory. Such polytomous item response models have a great many uses ranging from assessing and predicting an individual&#39;s latent trait to ordering the items to test the effectiveness of the test instrumentation. By implementing these models in a full Bayesian framework, computed through the use of Markov chain Monte Carlo methods implemented in the efficient STAN software package, the paper exploits the full inferential capability of GPCMs. The GPCMs include explanatory covariate effects which allow simultaneous estimation of regression and item parameters. The Bayesian methods for ranking the items by using the Fisher information criterion are implemented by using Markov chain Monte Carlo sampling. This allows us to propagate fully and to ascertain uncertainty in the inferences by calculating the posterior predictive distribution of the item-specific Fisher information criterion in a novel manner that has not been exploited in the literature before. Lastly, we propose a new Monte Carlo method for predicting the latent trait score of a new individual by approximating the relevant Bayesian predictive distribution. Data from a model disability survey carried out in Sri Lanka by the World Health Organization and the World Bank are used to illustrate the methods. The approaches proposed are shown to provide simultaneous model-based inference for all aspects of disability which can be explained by environmental and socio-economic factors.},
  archive      = {J_JRSSSC},
  author       = {Sahu, Sujit K. and Bass, Mark R. and Sabariego, Carla and Cieza, Alarcos and Fellinghauer, Carolina S. and Chatterji, Somnath},
  doi          = {10.1111/rssc.12385},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {1},
  pages        = {131-150},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {A full bayesian implementation of a generalized partial credit model with an application to an international disability survey},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian modelling of marked point processes with incomplete
records: Volcanic eruptions. <em>JRSSSC</em>, <em>69</em>(1), 109–130.
(<a href="https://doi.org/10.1111/rssc.12380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modelling point processes with incomplete records is a challenging problem, especially when the degree of record completeness varies over time. For volcanic eruption records, we expect the degree of missingness to depend on both the time and the size of an eruption. We propose a time-varying intensity function for a marked point process to model the non-stationary variation of the observed point process caused by missing data. We apply this model to global and regional volcanic eruption records and use Bayesian inference to obtain hazard estimates and their uncertainties based on the observed incomplete records, to carry out residual analysis and to provide forecasts.},
  archive      = {J_JRSSSC},
  author       = {Wang, Ting and Schofield, Matthew and Bebbington, Mark and Kiyosugi, Koji},
  doi          = {10.1111/rssc.12380},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {1},
  pages        = {109-130},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Bayesian modelling of marked point processes with incomplete records: Volcanic eruptions},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating the probability of default for no-default and
low-default portfolios. <em>JRSSSC</em>, <em>69</em>(1), 89–107. (<a
href="https://doi.org/10.1111/rssc.12381">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper proposes a sequential Bayesian updating approach to estimate default probabilities on rating grade level for no- and low-default portfolios. Bayesian sequential updating enables default probabilities to be obtained also for those rating grades for which no defaults have been observed. The advantage of this approach is that it preserves the rank order of rating grades in the case of no defaults. Rank preservation is not ensured when using an identical prior distribution across all rating grades. We discuss Bayesian sequential updating for the beta–binomial model and a model incorporating the asymptotic single-risk factor model of the Basel Accord. Practical aspects such as incorporating information from external sources and the margin of conservatism are addressed.},
  archive      = {J_JRSSSC},
  author       = {Blümke, Oliver},
  doi          = {10.1111/rssc.12381},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {1},
  pages        = {89-107},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Estimating the probability of default for no-default and low-default portfolios},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Selecting biomarkers for building optimal treatment
selection rules by using kernel machines. <em>JRSSSC</em>,
<em>69</em>(1), 69–88. (<a
href="https://doi.org/10.1111/rssc.12379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal biomarker combinations for treatment selection can be derived by minimizing the total burden to the population caused by the targeted disease and its treatment. However, when multiple biomarkers are present, including all in the model can be expensive and can hurt model performance. To remedy this, we consider feature selection in optimization by minimizing an extended total burden that additionally incorporates biomarker costs. Formulating it as a 0-norm penalized weighted classification, we develop various procedures for estimating linear and non-linear combinations. Through simulations and a real data example, we demonstrate the importance of incorporating feature selection and marker cost when deriving treatment selection rules.},
  archive      = {J_JRSSSC},
  author       = {Dasgupta, Sayan and Huang, Ying},
  doi          = {10.1111/rssc.12379},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {1},
  pages        = {69-88},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Selecting biomarkers for building optimal treatment selection rules by using kernel machines},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Zoom-in–out joint graphical lasso for different coarseness
scales. <em>JRSSSC</em>, <em>69</em>(1), 47–67. (<a
href="https://doi.org/10.1111/rssc.12378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new method is proposed to estimate graphical models simultaneously from data obtained at different coarseness scales. Starting from a predefined scale the method offers the possibility to zoom in or out over scales on particular edges. The estimated graphs over the different scales have similar structures although their level of sparsity depends on the scale at which estimation takes place. The method makes it possible to evaluate the evolution of the graphs from the coarsest to the finest scale or vice versa. We select an optimal coarseness scale to be used for further analysis. Simulation studies and an application on functional magnetic resonance brain imaging data show the method&#39;s performance in practice.},
  archive      = {J_JRSSSC},
  author       = {Pircalabelu, Eugen and Claeskens, Gerda and Waldorp, Lourens J.},
  doi          = {10.1111/rssc.12378},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {1},
  pages        = {47-67},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Zoom-In–Out joint graphical lasso for different coarseness scales},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Longitudinal dynamic functional regression. <em>JRSSSC</em>,
<em>69</em>(1), 25–46. (<a
href="https://doi.org/10.1111/rssc.12376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper develops a parsimonious modelling framework to study the time-varying association between scalar outcomes and functional predictors observed at many instances, in longitudinal studies. The methods enable us to reconstruct the full trajectory of the response and are applicable to Gaussian and non-Gaussian responses. The idea is to model the time-varying functional predictors by using orthogonal basis functions and to expand the time-varying regression coefficient by using the same basis. Numerical investigation through simulation studies and data analysis show excellent performance in terms of accurate prediction and efficient computations, when compared with existing alternatives. The methods are inspired and applied to an animal science application, where of interest is to study the association between the feed intake of lactating sows and the minute-by-minute temperature throughout the 21 days of their lactation period. R code and an R illustration are provided.},
  archive      = {J_JRSSSC},
  author       = {Staicu, Ana-Maria and Islam, Md Nazmul and Dumitru, Raluca and van Heugten, Eric},
  doi          = {10.1111/rssc.12376},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {1},
  pages        = {25-46},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Longitudinal dynamic functional regression},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysis of tornado reports through replicated
spatiotemporal point patterns. <em>JRSSSC</em>, <em>69</em>(1), 3–23.
(<a href="https://doi.org/10.1111/rssc.12375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the spatiotemporal distribution of tornado events is increasingly imperative, not only because of the natural phenomenon itself and its tremendous complexity but also because we can potentially reduce the risks that they entail. In particular, the US regions are particularly susceptible to tornadoes and they are the focus and motivation of our statistical analysis. Tornado reports can be treated as spatiotemporal point patterns, and we develop some methods for the analysis of replicated spatiotemporal patterns to identify significant structural differences between cold and warm seasons along the years. We extend some existing spatial techniques to the spatiotemporal context to test the null hypothesis that two (or more) observed spatiotemporal point patterns with replications are realizations of point processes that have the same second-order descriptors. In particular, we develop a non-parametric test to approximate the null distribution of the test statistics. We present intensive simulation studies that demonstrate the validity and power of our test and apply our methods to the motivating problem of tornadoes.},
  archive      = {J_JRSSSC},
  author       = {González, Jonatan A. and Hahn, Ute and Mateu, Jorge},
  doi          = {10.1111/rssc.12375},
  journal      = {Journal of the Royal Statistical Society Series C: Applied Statistics},
  number       = {1},
  pages        = {3-23},
  shortjournal = {J. R. Stat. Soc. Ser. C Appl. Stat.},
  title        = {Analysis of tornado reports through replicated spatiotemporal point patterns},
  volume       = {69},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
