<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>BIOMTC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="biomtc---147">BIOMTC - 147</h2>
<ul>
<li><details>
<summary>
(2020). Acknowledgments referees 2020. <em>BIOMTC</em>,
<em>76</em>(4), 1395–1398. (<a
href="https://doi.org/10.1111/biom.13408">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  doi          = {10.1111/biom.13408},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1395-1398},
  shortjournal = {Biometrics},
  title        = {Acknowledgments referees 2020},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Artificial intelligence for drug development, precision
medicine, and healthcare mark chang boca raton, FL: Chapman and
hall/CRC, 2020. <em>BIOMTC</em>, <em>76</em>(4), 1392–1394. (<a
href="https://doi.org/10.1111/biom.13390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Li-Pang Chen},
  doi          = {10.1111/biom.13390},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1392-1394},
  shortjournal = {Biometrics},
  title        = {Artificial intelligence for drug development, precision medicine, and healthcare mark chang boca raton, FL: Chapman and Hall/CRC, 2020.},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reproducible research in r and r studio, 3rd edition. By
christopher gandrud. <em>BIOMTC</em>, <em>76</em>(4), 1391. (<a
href="https://doi.org/10.1111/biom.13396">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Lucy D&#39;Agostino McGowan},
  doi          = {10.1111/biom.13396},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1391},
  shortjournal = {Biometrics},
  title        = {Reproducible research in r and r studio, 3rd edition. by christopher gandrud.},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Time series clustering and classification elizabeth ann
maharaj, pierpaolo d’urso, and jorge caiado. Boca raton FL: CRC.
<em>BIOMTC</em>, <em>76</em>(4), 1390–1391. (<a
href="https://doi.org/10.1111/biom.13397">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Prabhanjan Narayanachar Tattar},
  doi          = {10.1111/biom.13397},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1390-1391},
  shortjournal = {Biometrics},
  title        = {Time series clustering and classification elizabeth ann maharaj, pierpaolo D&#39;Urso, and jorge caiado. boca raton FL: CRC.},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rejoinder to “CACE and meta-analysis (letter to the editor)”
by stuart baker. <em>BIOMTC</em>, <em>76</em>(4), 1385–1389. (<a
href="https://doi.org/10.1111/biom.13239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Jincheng Zhou and James S. Hodges and Haitao Chu},
  doi          = {10.1111/biom.13239},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1385-1389},
  shortjournal = {Biometrics},
  title        = {Rejoinder to “CACE and meta-analysis (letter to the editor)” by stuart baker},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CACE and meta-analysis (letter to the editor).
<em>BIOMTC</em>, <em>76</em>(4), 1383–1384. (<a
href="https://doi.org/10.1111/biom.13240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Stuart G. Baker},
  doi          = {10.1111/biom.13240},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1383-1384},
  shortjournal = {Biometrics},
  title        = {CACE and meta-analysis (Letter to the editor)},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simple outlier detection for a multi-environmental field
trial. <em>BIOMTC</em>, <em>76</em>(4), 1374–1382. (<a
href="https://doi.org/10.1111/biom.13216">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of plant breeding trials is often to identify crop variety that are well adapt to target environments. These varieties are identified through genomic prediction from the analysis of multi-environmental field trial (MET) using linear mixed models. The occurrence of outliers in MET is common and known to adversely impact the accuracy of genomic prediction yet the detection of outliers are often neglected. A number of reasons stand for this—first, complex data such as a MET give rise to distinct levels of residuals (eg, at a trial level or individual observation level). This complexity offers additional challenges for an outlier detection method. Second, many linear mixed model software packages that cater for complex variance structures needed in the analysis of MET are not well streamlined for diagnostics by practitioners. We demonstrate outlier detection methods that are simple to implement in any linear mixed model software packages and computationally fast. Although these methods are not optimal methods in outlier detection, they offer practical value for ease of application in the analysis pipeline of regularly collected data. These are demonstrated using simulation based on two real bread wheat yield METs. In particular, models that consider analysis of yield trials either independently or jointly (thus borrowing strength across trials) are considered. Case studies are presented to highlight benefit of joint analysis for outlier detection.},
  archive      = {J_BIOMTC},
  author       = {Emi Tanaka},
  doi          = {10.1111/biom.13216},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1374-1382},
  shortjournal = {Biometrics},
  title        = {Simple outlier detection for a multi-environmental field trial},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). PA-CRM: A continuous reassessment method for pediatric phase
i oncology trials with concurrent adult trials. <em>BIOMTC</em>,
<em>76</em>(4), 1364–1373. (<a
href="https://doi.org/10.1111/biom.13217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pediatric phase I trials are usually carried out after the adult trial testing the same agent has started, but not completed yet. As the pediatric trial progresses, in light of the accrued interim data from the concurrent adult trial, the pediatric protocol often is amended to modify the original pediatric dose escalation design. In practice, this is done frequently in an ad hoc way, interrupting patient accrual and slowing down the trial. We developed a pediatric-continuous reassessment method (PA-CRM) to streamline this process, providing a more efficient and rigorous method to find the maximum tolerated dose for pediatric phase I oncology trials. We use a discounted joint likelihood of the adult and pediatric data, with a discount parameter controlling information borrowing between pediatric and adult trials. According to the interim adult and pediatric data, the discount parameter is adaptively updated using the Bayesian model averaging method. Numerical study shows that the PA-CRM improves the efficiency and accuracy of the pediatric trial and is robust to various model assumptions.},
  archive      = {J_BIOMTC},
  author       = {Yimei Li and Ying Yuan},
  doi          = {10.1111/biom.13217},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1364-1373},
  shortjournal = {Biometrics},
  title        = {PA-CRM: A continuous reassessment method for pediatric phase i oncology trials with concurrent adult trials},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Relapse or reinfection: Classification of malaria infection
using transition likelihoods. <em>BIOMTC</em>, <em>76</em>(4),
1351–1363. (<a href="https://doi.org/10.1111/biom.13226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In patients with Plasmodium vivax malaria treated with effective blood-stage therapy, the recurrent illness may occur due to relapse from latent liver-stage infection or reinfection from a new mosquito bite. Classification of the recurrent infection as either relapse or reinfection is critical when evaluating the efficacy of an anti-relapse treatment. Although one can use whether a shared genetic variant exists between baseline and recurrence genotypes to classify the outcome, little has been suggested to use both sharing and nonsharing variants to improve the classification accuracy. In this paper, we develop a novel classification criterion that utilizes transition likelihoods to distinguish relapse from reinfection. When tested in extensive simulation experiments with known outcomes, our classifier has superior operating characteristics. A real data set from 78 Cambodian P. vivax malaria patients was analyzed to demonstrate the practical use of our proposed method.},
  archive      = {J_BIOMTC},
  author       = {Feng-Chang Lin and Quefeng Li and Jessica T. Lin},
  doi          = {10.1111/biom.13226},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1351-1363},
  shortjournal = {Biometrics},
  title        = {Relapse or reinfection: Classification of malaria infection using transition likelihoods},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Using sufficient direction factor model to analyze latent
activities associated with breast cancer survival. <em>BIOMTC</em>,
<em>76</em>(4), 1340–1350. (<a
href="https://doi.org/10.1111/biom.13208">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional gene expression data often exhibit intricate correlation patterns as the result of coordinated genetic regulation. In practice, however, it is difficult to directly measure these coordinated underlying activities. Analysis of breast cancer survival data with gene expressions motivates us to use a two-stage latent factor approach to estimate these unobserved coordinated biological processes. Compared to existing approaches, our proposed procedure has several unique characteristics. In the first stage, an important distinction is that our procedure incorporates prior biological knowledge about gene-pathway membership into the analysis and explicitly model the effects of genetic pathways on the latent factors. Second, to characterize the molecular heterogeneity of breast cancer, our approach provides estimates specific to each cancer subtype. Finally, our proposed framework incorporates sparsity condition due to the fact that genetic networks are often sparse. In the second stage, we investigate the relationship between latent factor activity levels and survival time with censoring using a general dimension reduction model in the survival analysis context. Combining the factor model and sufficient direction model provides an efficient way of analyzing high-dimensional data and reveals some interesting relations in the breast cancer gene expression data.},
  archive      = {J_BIOMTC},
  author       = {Seungchul Baek and Yen-Yi Ho and Yanyuan Ma},
  doi          = {10.1111/biom.13208},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1340-1350},
  shortjournal = {Biometrics},
  title        = {Using sufficient direction factor model to analyze latent activities associated with breast cancer survival},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variable selection in joint frailty models of recurrent and
terminal events. <em>BIOMTC</em>, <em>76</em>(4), 1330–1339. (<a
href="https://doi.org/10.1111/biom.13242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent event data are commonly encountered in biomedical studies. In many situations, they are subject to an informative terminal event, for example, death. Joint modeling of recurrent and terminal events has attracted substantial recent research interests. On the other hand, there may exist a large number of covariates in such data. How to conduct variable selection for joint frailty proportional hazards models has become a challenge in practical data analysis. We tackle this issue on the basis of the “minimum approximated information criterion” method. The proposed method can be conveniently implemented in SAS Proc NLMIXED for commonly used frailty distributions. Its finite-sample behavior is evaluated through simulation studies. We apply the proposed method to model recurrent opportunistic diseases in the presence of death in an AIDS study.},
  archive      = {J_BIOMTC},
  author       = {Dongxiao Han and Xiaogang Su and Liuquan Sun and Zhou Zhang and Lei Liu},
  doi          = {10.1111/biom.13242},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1330-1339},
  shortjournal = {Biometrics},
  title        = {Variable selection in joint frailty models of recurrent and terminal events},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Relative efficiency of using summary versus individual data
in random-effects meta-analysis. <em>BIOMTC</em>, <em>76</em>(4),
1319–1329. (<a href="https://doi.org/10.1111/biom.13238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-analysis is a statistical methodology for combining information from diverse sources so that a more reliable and efficient conclusion can be reached. It can be conducted by either synthesizing study-level summary statistics or drawing inference from an overarching model for individual participant data (IPD) if available. The latter is often viewed as the “gold standard.” For random-effects models, however, it remains not fully understood whether the use of IPD indeed gains efficiency over summary statistics. In this paper, we examine the relative efficiency of the two methods under a general likelihood inference setting. We show theoretically and numerically that summary-statistics-based analysis is at most as efficient as IPD analysis, provided that the random effects follow the Gaussian distribution, and maximum likelihood estimation is used to obtain summary statistics. More specifically, (i) the two methods are equivalent in an asymptotic sense; and (ii) summary-statistics-based inference can incur an appreciable loss of efficiency if the sample sizes are not sufficiently large. Our results are established under the assumption that the between-study heterogeneity parameter remains constant regardless of the sample sizes, which is different from a previous study. Our findings are confirmed by the analyses of simulated data sets and a real-world study of alcohol interventions.},
  archive      = {J_BIOMTC},
  author       = {Ding-Geng Chen and Dungang Liu and Xiaoyi Min and Heping Zhang},
  doi          = {10.1111/biom.13238},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1319-1329},
  shortjournal = {Biometrics},
  title        = {Relative efficiency of using summary versus individual data in random-effects meta-analysis},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating individualized treatment rules with risk
constraint. <em>BIOMTC</em>, <em>76</em>(4), 1310–1318. (<a
href="https://doi.org/10.1111/biom.13232">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individualized treatment rules (ITRs) recommend treatments based on patient-specific characteristics in order to maximize the expected clinical outcome. At the same time, the risks caused by various adverse events cannot be ignored. In this paper, we propose a method to estimate an optimal ITR that maximizes clinical benefit while having the overall risk controlled at a desired level. Our method works for a general setting of multi-category treatment. The proposed procedure employs two shifted ramp losses to approximate the 0-1 loss in the objective function and constraint, respectively, and transforms the estimation problem into a difference of convex functions (DC) programming problem. A relaxed DC algorithm is used to solve the nonconvex constrained optimization problem. Simulations and a real data example are used to demonstrate the finite sample performance of the proposed method.},
  archive      = {J_BIOMTC},
  author       = {Xinyang Huang and Jin Xu},
  doi          = {10.1111/biom.13232},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1310-1318},
  shortjournal = {Biometrics},
  title        = {Estimating individualized treatment rules with risk constraint},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bayesian multi-risks survival (MRS) model in the presence
of double censorings. <em>BIOMTC</em>, <em>76</em>(4), 1297–1309. (<a
href="https://doi.org/10.1111/biom.13228">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-competing risks data include the time to a nonterminating event and the time to a terminating event, while competing risks data include the time to more than one terminating event. Our work is motivated by a prostate cancer study, which has one nonterminating event and two terminating events with both semi-competing risks and competing risks present as well as two censoring times. In this paper, we propose a new multi-risks survival (MRS) model for this type of data. In addition, the proposed MRS model can accommodate noninformative right-censoring times for nonterminating and terminating events. Properties of the proposed MRS model are examined in detail. Theoretical and empirical results show that the estimates of the cumulative incidence function for a nonterminating event may be biased if the information on a terminating event is ignored. A Markov chain Monte Carlo sampling algorithm is also developed. Our methodology is further assessed using simulations and also an analysis of the real data from a prostate cancer study. As a result, a prostate-specific antigen velocity greater than 2.0 ng/mL per year and higher biopsy Gleason scores are positively associated with a shorter time to death due to prostate cancer.},
  archive      = {J_BIOMTC},
  author       = {Mário de Castro and Ming-Hui Chen and Yuanye Zhang and Anthony V. D&#39;Amico},
  doi          = {10.1111/biom.13228},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1297-1309},
  shortjournal = {Biometrics},
  title        = {A bayesian multi-risks survival (MRS) model in the presence of double censorings},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Objective prior distributions for jolly-seber models of
zero-augmented data. <em>BIOMTC</em>, <em>76</em>(4), 1285–1296. (<a
href="https://doi.org/10.1111/biom.13221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Statistical models of capture-recapture data that are used to estimate the dynamics of a population are known collectively as Jolly-Seber (JS) models. State-space versions of these models have been developed for the analysis of zero-augmented data that include the capture histories of the observed individuals and an arbitrarily large number of all-zero capture histories. The number of all-zero capture histories must be sufficiently large to include the unknown number N of individuals in the population that were ever alive during all sampling periods. This definition of N is equivalent to the “superpopulation” of individuals described in several JS models. To fit JS models of zero-augmented data, practitioners often assume a set of independent, uniform prior distributions for the recruitment parameters. However, if the number of capture histories is small compared to N , these uniform priors can exert considerable influence on the posterior distributions of N and other parameters because the uniform priors induce a highly skewed prior on N . In this article, I derive a class of prior distributions for the recruitment parameters of the JS model that can be used to specify objective prior distributions for N , including the discrete-uniform and the improper scale priors as special cases. This class of priors also may be used to specify prior knowledge about recruitment while still preserving the conditions needed to induce an objective prior on N . I use analyses of simulated and real data to illustrate the inferential benefits of this class of prior distributions and to identify circumstances where these benefits are most likely to be realized.},
  archive      = {J_BIOMTC},
  author       = {Robert M. Dorazio},
  doi          = {10.1111/biom.13221},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1285-1296},
  shortjournal = {Biometrics},
  title        = {Objective prior distributions for jolly-seber models of zero-augmented data},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonnegative decomposition of functional count data.
<em>BIOMTC</em>, <em>76</em>(4), 1273–1284. (<a
href="https://doi.org/10.1111/biom.13220">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a novel decomposition of nonnegative functional count data that draws on concepts from nonnegative matrix factorization. Our decomposition, which we refer to as NARFD (nonnegative and regularized function decomposition), enables the study of patterns in variation across subjects in a highly interpretable manner. Prototypic modes of variation are estimated directly on the observed scale of the data, are local, and are transparently added together to reconstruct observed functions. This contrasts with generalized functional principal component analysis, an alternative approach that estimates functional principal components on a transformed scale, produces components that typically vary across the entire functional domain, and reconstructs observations using complex patterns of cancellation and multiplication of functional principal components. NARFD is implemented using an alternating minimization algorithm, and we evaluate our approach in simulations. We apply NARFD to an accelerometer dataset comprising observations of physical activity for healthy older Americans.},
  archive      = {J_BIOMTC},
  author       = {Daniel Backenroth and Russell T. Shinohara and Jennifer A. Schrack and Jeff Goldsmith},
  doi          = {10.1111/biom.13220},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1273-1284},
  shortjournal = {Biometrics},
  title        = {Nonnegative decomposition of functional count data},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Operating characteristics of the rank-based inverse normal
transformation for quantitative trait analysis in genome-wide
association studies. <em>BIOMTC</em>, <em>76</em>(4), 1262–1272. (<a
href="https://doi.org/10.1111/biom.13214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantitative traits analyzed in Genome-Wide Association Studies (GWAS) are often nonnormally distributed. For such traits, association tests based on standard linear regression are subject to reduced power and inflated type I error in finite samples. Applying the rank-based inverse normal transformation (INT) to nonnormally distributed traits has become common practice in GWAS. However, the different variations on INT-based association testing have not been formally defined, and guidance is lacking on when to use which approach. In this paper, we formally define and systematically compare the direct (D-INT) and indirect (I-INT) INT-based association tests. We discuss their assumptions, underlying generative models, and connections. We demonstrate that the relative powers of D-INT and I-INT depend on the underlying data generating process. Since neither approach is uniformly most powerful, we combine them into an adaptive omnibus test (O-INT). O-INT is robust to model misspecification, protects the type I error, and is well powered against a wide range of nonnormally distributed traits. Extensive simulations were conducted to examine the finite sample operating characteristics of these tests. Our results demonstrate that, for nonnormally distributed traits, INT-based tests outperform the standard untransformed association test, both in terms of power and type I error rate control. We apply the proposed methods to GWAS of spirometry traits in the UK Biobank. O-INT has been implemented in the R package RNOmni , which is available on CRAN.},
  archive      = {J_BIOMTC},
  author       = {Zachary R. McCaw and Jacqueline M. Lane and Richa Saxena and Susan Redline and Xihong Lin},
  doi          = {10.1111/biom.13214},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1262-1272},
  shortjournal = {Biometrics},
  title        = {Operating characteristics of the rank-based inverse normal transformation for quantitative trait analysis in genome-wide association studies},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discussion on “testing small study effects in multivariate
meta-analysis” by chuan hong, georgia salanti, sally morton, richard
riley, haitao chu, stephen e kimmel and yong chen. <em>BIOMTC</em>,
<em>76</em>(4), 1260–1261. (<a
href="https://doi.org/10.1111/biom.13344">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {James R. Carpenter and Gerta Rücker and Guido Schwarzer},
  doi          = {10.1111/biom.13344},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1260-1261},
  shortjournal = {Biometrics},
  title        = {Discussion on ‘Testing small study effects in multivariate meta-analysis’ by chuan hong, georgia salanti, sally morton, richard riley, haitao chu, stephen e kimmel and yong chen},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discussion on “testing small study effects in multivariate
meta-analysis” by chuan hong, georgia salanti, sally morton, richard
riley, haitao chu, stephen e. Kimmel, and yong chen. <em>BIOMTC</em>,
<em>76</em>(4), 1255–1259. (<a
href="https://doi.org/10.1111/biom.13343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Hisashi Noma},
  doi          = {10.1111/biom.13343},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1255-1259},
  shortjournal = {Biometrics},
  title        = {Discussion on “Testing small study effects in multivariate meta-analysis” by chuan hong, georgia salanti, sally morton, richard riley, haitao chu, stephen e. kimmel, and yong chen},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discussion on “testing small study effects in multivariate
meta-analysis” by chuan hong, georgia salanti, sally morton, richard
riley, haitao chu, stephen e. Kimmel and yong chen. <em>BIOMTC</em>,
<em>76</em>(4), 1251–1254. (<a
href="https://doi.org/10.1111/biom.13345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Hans C. van Houwelingen},
  doi          = {10.1111/biom.13345},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1251-1254},
  shortjournal = {Biometrics},
  title        = {Discussion on “Testing small study effects in multivariate meta-analysis” by chuan hong, georgia salanti, sally morton, richard riley, haitao chu, stephen e. kimmel and yong chen},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Testing small study effects in multivariate meta-analysis.
<em>BIOMTC</em>, <em>76</em>(4), 1240–1250. (<a
href="https://doi.org/10.1111/biom.13342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Small study effects occur when smaller studies show different, often larger, treatment effects than large ones, which may threaten the validity of systematic reviews and meta-analyses. The most well-known reasons for small study effects include publication bias, outcome reporting bias, and clinical heterogeneity. Methods to account for small study effects in univariate meta-analysis have been extensively studied. However, detecting small study effects in a multivariate meta-analysis setting remains an untouched research area. One of the complications is that different types of selection processes can be involved in the reporting of multivariate outcomes. For example, some studies may be completely unpublished while others may selectively report multiple outcomes. In this paper, we propose a score test as an overall test of small study effects in multivariate meta-analysis. Two detailed case studies are given to demonstrate the advantage of the proposed test over various naive applications of univariate tests in practice. Through simulation studies, the proposed test is found to retain nominal Type I error rates with considerable power in moderate sample size settings. Finally, we also evaluate the concordance between the proposed tests with the naive application of univariate tests by evaluating 44 systematic reviews with multiple outcomes from the Cochrane Database.},
  archive      = {J_BIOMTC},
  author       = {Chuan Hong and Georgia Salanti and Sally C. Morton and Richard D. Riley and Haitao Chu and Stephen E. Kimmel and Yong Chen},
  doi          = {10.1111/biom.13342},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1240-1250},
  shortjournal = {Biometrics},
  title        = {Testing small study effects in multivariate meta-analysis},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semiparametric modelling and estimation of
covariate-adjusted dependence between bivariate recurrent events.
<em>BIOMTC</em>, <em>76</em>(4), 1229–1239. (<a
href="https://doi.org/10.1111/biom.13229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A time-dependent measure, termed the rate ratio, was proposed to assess the local dependence between two types of recurrent event processes in one-sample settings. However, the one-sample work does not consider modeling the dependence by covariates such as subject characteristics and treatments received. The focus of this paper is to understand how and in what magnitude the covariates influence the dependence strength for bivariate recurrent events. We propose the covariate-adjusted rate ratio, a measure of covariate-adjusted dependence. We propose a semiparametric regression model for jointly modeling the frequency and dependence of bivariate recurrent events: the first level is a proportional rates model for the marginal rates and the second level is a proportional rate ratio model for the dependence structure. We develop a pseudo-partial likelihood to estimate the parameters in the proportional rate ratio model. We establish the asymptotic properties of the estimators and evaluate the finite sample performance via simulation studies. We illustrate the proposed models and methods using a soft tissue sarcoma study that examines the effects of initial treatments on the marginal frequencies of local/distant sarcoma recurrence and the dependence structure between the two types of cancer recurrence.},
  archive      = {J_BIOMTC},
  author       = {Jing Ning and Chunyan Cai and Yong Chen and Xuelin Huang and Mei-Cheng Wang},
  doi          = {10.1111/biom.13229},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1229-1239},
  shortjournal = {Biometrics},
  title        = {Semiparametric modelling and estimation of covariate-adjusted dependence between bivariate recurrent events},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On symmetric semiparametric two-sample problem.
<em>BIOMTC</em>, <em>76</em>(4), 1216–1228. (<a
href="https://doi.org/10.1111/biom.13233">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a two-sample problem where data come from symmetric distributions. Usual two-sample data with only magnitudes recorded, arising from case-control studies or logistic discriminant analyses, may constitute a symmetric two-sample problem. We propose a semiparametric model such that, in addition to symmetry, the log ratio of two unknown density functions is modeled in a known parametric form. The new semiparametric model, tailor-made for symmetric two-sample data, can also be viewed as a biased sampling model subject to symmetric constraint. A maximum empirical likelihood estimation approach is adopted to estimate the unknown model parameters, and the corresponding profile empirical likelihood ratio test is utilized to perform hypothesis testing regarding the two population distributions. Symmetry, however, comes with irregularity. It is shown that, under the null hypothesis of equal symmetric distributions, the maximum empirical likelihood estimator has degenerate Fisher information, and the test statistic has a mixture of χ 2 -type asymptotic distribution. Extensive simulation studies have been conducted to demonstrate promising statistical powers under correct and misspecified models. We apply the proposed methods to two real examples.},
  archive      = {J_BIOMTC},
  author       = {Moming Li and Guoqing Diao and Jing Qin},
  doi          = {10.1111/biom.13233},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1216-1228},
  shortjournal = {Biometrics},
  title        = {On symmetric semiparametric two-sample problem},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Censored quantile regression model with time-varying
covariates under length-biased sampling. <em>BIOMTC</em>,
<em>76</em>(4), 1201–1215. (<a
href="https://doi.org/10.1111/biom.13230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quantile regression is a flexible and effective tool for modeling survival data and its relationship with important covariates, which often vary over time. Informative right censoring of data from the prevalent cohort within the population often results in length-biased observations. We propose an estimating equation-based approach to obtain consistent estimators of the regression coefficients of interest based on length-biased observations with time-dependent covariates. In addition, inspired by Zeng and Lin 2008, we also develop a more numerically stable procedure for variance estimation. Large sample properties including consistency and asymptotic normality of the proposed estimator are established. Numerical studies presented demonstrate convincing performance of the proposed estimator under various settings. The application of the proposed method is demonstrated using the Oscar dataset.},
  archive      = {J_BIOMTC},
  author       = {Zexi Cai and Tony Sit},
  doi          = {10.1111/biom.13230},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1201-1215},
  shortjournal = {Biometrics},
  title        = {Censored quantile regression model with time-varying covariates under length-biased sampling},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Doubly robust tests of exposure effects under
high-dimensional confounding. <em>BIOMTC</em>, <em>76</em>(4),
1190–1200. (<a href="https://doi.org/10.1111/biom.13231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {After variable selection, standard inferential procedures for regression parameters may not be uniformly valid ; there is no finite-sample size at which a standard test is guaranteed to approximately attain its nominal size. This problem is exacerbated in high-dimensional settings, where variable selection becomes unavoidable. This has prompted a flurry of activity in developing uniformly valid hypothesis tests for a low-dimensional regression parameter (eg, the causal effect of an exposure A on an outcome Y ) in high-dimensional models. So far there has been limited focus on model misspecification, although this is inevitable in high-dimensional settings. We propose tests of the null that are uniformly valid under sparsity conditions weaker than those typically invoked in the literature, assuming working models for the exposure and outcome are both correctly specified. When one of the models is misspecified, by amending the procedure for estimating the nuisance parameters, our tests continue to be valid; hence, they are doubly robust . Our proposals are straightforward to implement using existing software for penalized maximum likelihood estimation and do not require sample splitting. We illustrate them in simulations and an analysis of data obtained from the Ghent University intensive care unit.},
  archive      = {J_BIOMTC},
  author       = {Oliver Dukes and Vahe Avagyan and Stijn Vansteelandt},
  doi          = {10.1111/biom.13231},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1190-1200},
  shortjournal = {Biometrics},
  title        = {Doubly robust tests of exposure effects under high-dimensional confounding},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ROC-guided survival trees and ensembles. <em>BIOMTC</em>,
<em>76</em>(4), 1177–1189. (<a
href="https://doi.org/10.1111/biom.13213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tree-based methods are popular nonparametric tools in studying time-to-event outcomes. In this article, we introduce a novel framework for survival trees and ensembles, where the trees partition the dynamic survivor population and can handle time-dependent covariates. Using the idea of randomized tests, we develop generalized time-dependent receiver operating characteristic (ROC) curves for evaluating the performance of survival trees. The tree-building algorithm is guided by decision-theoretic criteria based on ROC, targeting specifically for prediction accuracy. To address the instability issue of a single tree, we propose a novel ensemble procedure based on averaging martingale estimating equations, which is different from existing methods that average the predicted survival or cumulative hazard functions from individual trees. Extensive simulation studies are conducted to examine the performance of the proposed methods. We apply the methods to a study on AIDS for illustration.},
  archive      = {J_BIOMTC},
  author       = {Yifei Sun and Sy Han Chiou and Mei-Cheng Wang},
  doi          = {10.1111/biom.13213},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1177-1189},
  shortjournal = {Biometrics},
  title        = {ROC-guided survival trees and ensembles},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Highly efficient stepped wedge designs for clusters of
unequal size. <em>BIOMTC</em>, <em>76</em>(4), 1167–1176. (<a
href="https://doi.org/10.1111/biom.13218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The stepped wedge design (SWD) is a form of cluster randomized trial, usually comparing two treatments, which is divided into time periods and sequences, with clusters allocated to sequences. Typically all sequences start with the standard treatment and end with the new treatment, with the change happening at different times in the different sequences. The clusters will usually differ in size but this is overlooked in much of the existing literature. This paper considers the case when clusters have different sizes and determines how efficient designs can be found. The approach uses an approximation to the variance of the treatment effect, which is expressed in terms of the proportions of clusters and of individuals allocated to each sequence of the design. The roles of these sets of proportions in determining an efficient design are discussed and illustrated using two SWDs, one in the treatment of sexually transmitted diseases and one in renal replacement therapy. Cluster-balanced designs, which allocate equal numbers of clusters to each sequence, are shown to have excellent statistical and practical properties; suggestions are made about the practical application of the results for these designs. The paper concentrates on the cross-sectional case, where subjects are measured once, but it is briefly indicated how the methods can be extended to the closed-cohort design.},
  archive      = {J_BIOMTC},
  author       = {John N. S. Matthews},
  doi          = {10.1111/biom.13218},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1167-1176},
  shortjournal = {Biometrics},
  title        = {Highly efficient stepped wedge designs for clusters of unequal size},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the empirical choice of the time window for restricted
mean survival time. <em>BIOMTC</em>, <em>76</em>(4), 1157–1166. (<a
href="https://doi.org/10.1111/biom.13237">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The t -year mean survival or restricted mean survival time (RMST) has been used as an appealing summary of the survival distribution within a time window [0, t ]. RMST is the patient&#39;s life expectancy until time t and can be estimated nonparametrically by the area under the Kaplan-Meier curve up to t . In a comparative study, the difference or ratio of two RMSTs has been utilized to quantify the between-group-difference as a clinically interpretable alternative summary to the hazard ratio. The choice of the time window [0, t ] may be prespecified at the design stage of the study based on clinical considerations. On the other hand, after the survival data have been collected, the choice of time point t could be data-dependent. The standard inferential procedures for the corresponding RMST, which is also data-dependent, ignore this subtle yet important issue. In this paper, we clarify how to make inference about a random “parameter.” Moreover, we demonstrate that under a rather mild condition on the censoring distribution, one can make inference about the RMST up to t , where t is less than or even equal to the largest follow-up time (either observed or censored) in the study. This finding reduces the subjectivity of the choice of t empirically. The proposal is illustrated with the survival data from a primary biliary cirrhosis study, and its finite sample properties are investigated via an extensive simulation study.},
  archive      = {J_BIOMTC},
  author       = {Lu Tian and Hua Jin and Hajime Uno and Ying Lu and Bo Huang and Keaven M. Anderson and LJ Wei},
  doi          = {10.1111/biom.13237},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1157-1166},
  shortjournal = {Biometrics},
  title        = {On the empirical choice of the time window for restricted mean survival time},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparametric estimation of distributions and diagnostic
accuracy based on group-tested results with differential
misclassification. <em>BIOMTC</em>, <em>76</em>(4), 1147–1156. (<a
href="https://doi.org/10.1111/biom.13236">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article concerns the problem of estimating a continuous distribution in a diseased or nondiseased population when only group-based test results on the disease status are available. The problem is challenging in that individual disease statuses are not observed and testing results are often subject to misclassification, with further complication that the misclassification may be differential as the group size and the number of the diseased individuals in the group vary. We propose a method to construct nonparametric estimation of the distribution and obtain its asymptotic properties. The performance of the distribution estimator is evaluated under various design considerations concerning group sizes and classification errors. The method is exemplified with data from the National Health and Nutrition Examination Survey study to estimate the distribution and diagnostic accuracy of C-reactive protein in blood samples in predicting chlamydia incidence.},
  archive      = {J_BIOMTC},
  author       = {Wei Zhang and Aiyi Liu and Qizhai Li and Paul S. Albert},
  doi          = {10.1111/biom.13236},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1147-1156},
  shortjournal = {Biometrics},
  title        = {Nonparametric estimation of distributions and diagnostic accuracy based on group-tested results with differential misclassification},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bayesian nonparametric testing procedure for paired
samples. <em>BIOMTC</em>, <em>76</em>(4), 1133–1146. (<a
href="https://doi.org/10.1111/biom.13234">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a Bayesian hypothesis testing procedure for comparing the distributions of paired samples. The procedure is based on a flexible model for the joint distribution of both samples. The flexibility is given by a mixture of Dirichlet processes. Our proposal uses a spike-slab prior specification for the base measure of the Dirichlet process and a particular parametrization for the kernel of the mixture in order to facilitate comparisons and posterior inference. The joint model allows us to derive the marginal distributions and test whether they differ or not. The procedure exploits the correlation between samples, relaxes the parametric assumptions, and detects possible differences throughout the entire distributions. A Monte Carlo simulation study comparing the performance of this strategy to other traditional alternatives is provided. Finally, we apply the proposed approach to spirometry data collected in the United States to investigate changes in pulmonary function in children and adolescents in response to air polluting factors.},
  archive      = {J_BIOMTC},
  author       = {Luz Adriana Pereira and Daniel Taylor-Rodríguez and Luis Gutiérrez},
  doi          = {10.1111/biom.13234},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1133-1146},
  shortjournal = {Biometrics},
  title        = {A bayesian nonparametric testing procedure for paired samples},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian modeling of multiple structural connectivity
networks during the progression of alzheimer’s disease. <em>BIOMTC</em>,
<em>76</em>(4), 1120–1132. (<a
href="https://doi.org/10.1111/biom.13235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer&#39;s disease is the most common neurodegenerative disease. The aim of this study is to infer structural changes in brain connectivity resulting from disease progression using cortical thickness measurements from a cohort of participants who were either healthy control, or with mild cognitive impairment, or Alzheimer&#39;s disease patients. For this purpose, we develop a novel approach for inference of multiple networks with related edge values across groups. Specifically, we infer a Gaussian graphical model for each group within a joint framework, where we rely on Bayesian hierarchical priors to link the precision matrix entries across groups. Our proposal differs from existing approaches in that it flexibly learns which groups have the most similar edge values, and accounts for the strength of connection (rather than only edge presence or absence) when sharing information across groups. Our results identify key alterations in structural connectivity that may reflect disruptions to the healthy brain, such as decreased connectivity within the occipital lobe with increasing disease severity. We also illustrate the proposed method through simulations, where we demonstrate its performance in structure learning and precision matrix estimation with respect to alternative approaches.},
  archive      = {J_BIOMTC},
  author       = {Christine B. Peterson and Nathan Osborne and Francesco C. Stingo and Pierrick Bourgeat and James D. Doecke and Marina Vannucci},
  doi          = {10.1111/biom.13235},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1120-1132},
  shortjournal = {Biometrics},
  title        = {Bayesian modeling of multiple structural connectivity networks during the progression of alzheimer&#39;s disease},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Partial least squares for functional joint models with
applications to the alzheimer’s disease neuroimaging initiative study.
<em>BIOMTC</em>, <em>76</em>(4), 1109–1119. (<a
href="https://doi.org/10.1111/biom.13219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many biomedical studies have identified important imaging biomarkers that are associated with both repeated clinical measures and a survival outcome. The functional joint model (FJM) framework, proposed by Li and Luo in 2017, investigates the association between repeated clinical measures and survival data, while adjusting for both high-dimensional images and low-dimensional covariates based on the functional principal component analysis (FPCA). In this paper, we propose a novel algorithm for the estimation of FJM based on the functional partial least squares (FPLS). Our numerical studies demonstrate that, compared to FPCA, the proposed FPLS algorithm can yield more accurate and robust estimation and prediction performance in many important scenarios. We apply the proposed FPLS algorithm to a neuroimaging study. Data used in preparation of this article were obtained from the Alzheimer&#39;s Disease Neuroimaging Initiative (ADNI) database.},
  archive      = {J_BIOMTC},
  author       = {Yue Wang and Joseph G. Ibrahim and Hongtu Zhu},
  doi          = {10.1111/biom.13219},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1109-1119},
  shortjournal = {Biometrics},
  title        = {Partial least squares for functional joint models with applications to the alzheimer&#39;s disease neuroimaging initiative study},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Constructing confidence intervals for selected parameters.
<em>BIOMTC</em>, <em>76</em>(4), 1098–1108. (<a
href="https://doi.org/10.1111/biom.13222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In large-scale problems, it is common practice to select important parameters by a procedure such as the Benjamini and Hochberg procedure and construct confidence intervals (CIs) for further investigation while the false coverage-statement rate (FCR) for the CIs is controlled at a desired level. Although the well-known BY CIs control the FCR, they are uniformly inflated. In this paper, we propose two methods to construct shorter selective CIs. The first method produces shorter CIs by allowing a reduced number of selective CIs. The second method produces shorter CIs by allowing a prefixed proportion of CIs containing the values of uninteresting parameters. We theoretically prove that the proposed CIs are uniformly shorter than BY CIs and control the FCR asymptotically for independent data. Numerical results confirm our theoretical results and show that the proposed CIs still work for correlated data. We illustrate the advantage of the proposed procedures by analyzing the microarray data from a HIV study.},
  archive      = {J_BIOMTC},
  author       = {Haibing Zhao and Xinping Cui},
  doi          = {10.1111/biom.13222},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1098-1108},
  shortjournal = {Biometrics},
  title        = {Constructing confidence intervals for selected parameters},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Weight calibration to improve the efficiency of pure risk
estimates from case-control samples nested in a cohort. <em>BIOMTC</em>,
<em>76</em>(4), 1087–1097. (<a
href="https://doi.org/10.1111/biom.13209">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cohort studies provide information on relative hazards and pure risks of disease. For rare outcomes, large cohorts are needed to have sufficient numbers of events, making it costly to obtain covariate information on all cohort members. We focus on nested case-control designs that are used to estimate relative hazard in the Cox regression model. In 1997, Langholz and Borgan showed that pure risk can also be estimated from nested case-control data. However, these approaches do not take advantage of some covariates that may be available on all cohort members. Researchers have used weight calibration to increase the efficiency of relative hazard estimates from case-cohort studies and nested cased-control studies. Our objective is to extend weight calibration approaches to nested case-control designs to improve precision of estimates of relative hazards and pure risks. We show that calibrating sample weights additionally against follow-up times multiplied by relative hazards during the risk projection period improves estimates of pure risk. Efficiency improvements for relative hazards for variables that are available on the entire cohort also contribute to improved efficiency for pure risks. We develop explicit variance formulas for the weight-calibrated estimates. Simulations show how much precision is improved by calibration and confirm the validity of inference based on asymptotic normality. Examples are provided using data from the American Association of Retired Persons Diet and Health Cohort Study.},
  archive      = {J_BIOMTC},
  author       = {Yei Eun Shin and Ruth M. Pfeiffer and Barry I. Graubard and Mitchell H. Gail},
  doi          = {10.1111/biom.13209},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1087-1097},
  shortjournal = {Biometrics},
  title        = {Weight calibration to improve the efficiency of pure risk estimates from case-control samples nested in a cohort},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On using electronic health records to improve optimal
treatment rules in randomized trials. <em>BIOMTC</em>, <em>76</em>(4),
1075–1086. (<a href="https://doi.org/10.1111/biom.13288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individualized treatment rules (ITRs) tailor medical treatments according to patient-specific characteristics in order to optimize patient outcomes. Data from randomized controlled trials (RCTs) are used to infer valid ITRs using statistical and machine learning methods. However, RCTs are usually conducted under specific inclusion/exclusion criteria, thus limiting their generalizability to a broader patient population in real-world practice settings. Because electronic health records (EHRs) document treatment prescriptions in the real world, transferring information in EHRs to RCTs, if done appropriately, could potentially improve the performance of ITRs, in terms of precision and generalizability. In this work, we propose a new domain adaptation method to learn ITRs by incorporating information from EHRs. Unless we assume that there is no unmeasured confounding in EHRs, we cannot directly learn the optimal ITR from the combined EHR and RCT data. Instead, we first pretrain “super” features from EHRs that summarize physician treatment decisions and patient observed benefits in the real world, as these are likely to be informative of the optimal ITRs. We then augment the feature space of the RCT and learn the optimal ITRs by stratifying by super features using subjects enrolled in RCT. We adopt Q-learning and a modified matched-learning algorithm for estimation. We present heuristic justification of our method and conduct simulation studies to demonstrate the performance of super features. Finally, we apply our method to transfer information learned from EHRs of patients with type 2 diabetes to learn individualized insulin therapies from RCT data.},
  archive      = {J_BIOMTC},
  author       = {Peng Wu and Donglin Zeng and Haoda Fu and Yuanjia Wang},
  doi          = {10.1111/biom.13288},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1075-1086},
  shortjournal = {Biometrics},
  title        = {On using electronic health records to improve optimal treatment rules in randomized trials},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Power analysis for cluster randomized trials with multiple
binary co-primary endpoints. <em>BIOMTC</em>, <em>76</em>(4), 1064–1074.
(<a href="https://doi.org/10.1111/biom.13212">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cluster randomized trials (CRTs) are widely used in different areas of medicine and public health. Recently, with increasing complexity of medical therapies and technological advances in monitoring multiple outcomes, many clinical trials attempt to evaluate multiple co-primary endpoints. In this study, we present a power analysis method for CRTs with K ≥ 2 binary co-primary endpoints. It is developed based on the GEE (generalized estimating equation) approach, and three types of correlations are considered: inter-subject correlation within each endpoint, intra-subject correlation across endpoints, and inter-subject correlation across endpoints. A closed-form joint distribution of the K test statistics is derived, which facilitates the evaluation of power and type I error for arbitrarily constructed hypotheses. We further present a theorem that characterizes the relationship between various correlations and testing power. We assess the performance of the proposed power analysis method based on extensive simulation studies. An application example to a real clinical trial is presented.},
  archive      = {J_BIOMTC},
  author       = {Dateng Li and Jing Cao and Song Zhang},
  doi          = {10.1111/biom.13212},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1064-1074},
  shortjournal = {Biometrics},
  title        = {Power analysis for cluster randomized trials with multiple binary co-primary endpoints},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A unified evaluation of differential vaccine efficacy.
<em>BIOMTC</em>, <em>76</em>(4), 1053–1063. (<a
href="https://doi.org/10.1111/biom.13211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many infectious diseases are well prevented by proper vaccination. However, when a vaccine is not completely efficacious, there is great interest in determining how the vaccine effect differs in subgroups conditional on measured immune responses postvaccination and also according to the type of infecting agent (eg, strain of a virus). The former is often called correlate of protection (CoP) analysis, while the latter has been called sieve analysis. We propose a unified framework for simultaneously assessing CoP and sieve effects of a vaccine in a large Phase III randomized trial. We use flexible parametric models treating times to infection from different agents as competing risks and estimated maximum likelihood to fit the models. The parametric models under competing risks allow for estimation of both cumulative incidence-based contrasts and instantaneous rates. We outline the assumptions with which we can link the observable data to the causal contrasts of interest, propose hypothesis testing procedures, and evaluate our proposed methods in an extensive simulation study.},
  archive      = {J_BIOMTC},
  author       = {Erin E. Gabriel and Michael C. Sachs and Dean A. Follmann and Therese M-L. Andersson},
  doi          = {10.1111/biom.13211},
  journal      = {Biometrics},
  number       = {4},
  pages        = {1053-1063},
  shortjournal = {Biometrics},
  title        = {A unified evaluation of differential vaccine efficacy},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). <em>BIOMTC</em>, <em>76</em>(3), 1047–1048. (<a
href="https://doi.org/10.1111/biom.13350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Peter Müller},
  doi          = {10.1111/biom.13350},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1047-1048},
  shortjournal = {Biometrics},
  title        = {Statistical remedies for medical researchers. peter thall cham, switzerland: Springer nature switzerland AG, 2020, hard cover. pp. 291. US$ 113.63.},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020c). Dynamic treatment regimes: Statistical methods for
precision medicine anastasios a. Tsiatis marie davidian shannon t.
Holloway eric b. Laber boca raton, FL: Chapman and hall/CRC, 2019.
<em>BIOMTC</em>, <em>76</em>(3), 1045–1046. (<a
href="https://doi.org/10.1111/biom.13326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Li-Pang Chen},
  doi          = {10.1111/biom.13326},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1045-1046},
  shortjournal = {Biometrics},
  title        = {Dynamic treatment regimes: statistical methods for precision medicine anastasios a. tsiatis marie davidian shannon t. holloway eric b. laber boca raton, FL: chapman and Hall/CRC, 2019.},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rejoinder to “comment on ‘wang et al. (2005), robust
estimating functions and bias correction for longitudinal data analysis’
by nicola lunardon and giovanna menardi.” <em>BIOMTC</em>,
<em>76</em>(3), 1043–1044. (<a
href="https://doi.org/10.1111/biom.13262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {You-Gan Wang and Xu Lin and Min Zhu},
  doi          = {10.1111/biom.13262},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1043-1044},
  shortjournal = {Biometrics},
  title        = {Rejoinder to “Comment on ‘Wang et al. (2005), robust estimating functions and bias correction for longitudinal data analysis’ by nicola lunardon and giovanna menardi”},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Comment on “wang et al. (2005), robust estimating functions
and bias correction for longitudinal data analysis.” <em>BIOMTC</em>,
<em>76</em>(3), 1040–1042. (<a
href="https://doi.org/10.1111/biom.13263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This note provides a discussion on the manuscript by Wang et al . (2005) who aim to robustify inference for longitudinal data analysis by replacing the ordinary generalized estimating function with an influence-bounded, possibly biased, version. To adjust for the bias of the ensuing robust estimator, the authors provide its analytic approximation by means of asymptotic expansions, and estimate it by plugging-in a nonrobust estimate of the parameter of interest. In this letter, we argue that the proposed bias-corrected estimator is, in fact, nonrobust.},
  archive      = {J_BIOMTC},
  author       = {Nicola Lunardon and Giovanna Menardi},
  doi          = {10.1111/biom.13263},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1040-1042},
  shortjournal = {Biometrics},
  title        = {Comment on “Wang et al. (2005), robust estimating functions and bias correction for longitudinal data analysis”},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rejoinder to “robustness of ANCOVA in randomized trials with
unequal randomization” by jonathan w. bartlett. <em>BIOMTC</em>,
<em>76</em>(3), 1039. (<a
href="https://doi.org/10.1111/biom.13182">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Bingkai Wang and Elizabeth L. Ogburn and Michael Rosenblum},
  doi          = {10.1111/biom.13182},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1039},
  shortjournal = {Biometrics},
  title        = {Rejoinder to “Robustness of ANCOVA in randomized trials with unequal randomization” by jonathan w. bartlett},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robustness of ANCOVA in randomized trials with unequal
randomization. <em>BIOMTC</em>, <em>76</em>(3), 1036–1038. (<a
href="https://doi.org/10.1111/biom.13184">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomized trials with continuous outcomes are often analyzed using analysis of covariance (ANCOVA), with adjustment for prognostic baseline covariates. The ANCOVA estimator of the treatment effect is consistent under arbitrary model misspecification. In an article recently published in the journal, Wang et al proved the model-based variance estimator for the treatment effect is also consistent under outcome model misspecification, assuming the probability of randomization to each treatment is 1/2. In this reader reaction, we derive explicit expressions which show that when randomization is unequal, the model-based variance estimator can be biased upwards or downwards. In contrast, robust sandwich variance estimators can provide asymptotically valid inferences under arbitrary misspecification, even when randomization probabilities are not equal.},
  archive      = {J_BIOMTC},
  author       = {Jonathan W. Bartlett},
  doi          = {10.1111/biom.13184},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1036-1038},
  shortjournal = {Biometrics},
  title        = {Robustness of ANCOVA in randomized trials with unequal randomization},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rejoinder to “on continuous-time capture-recapture in closed
populations.” <em>BIOMTC</em>, <em>76</em>(3), 1034–1035. (<a
href="https://doi.org/10.1111/biom.13183">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Matthew R. Schofield and Richard J. Barker},
  doi          = {10.1111/biom.13183},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1034-1035},
  shortjournal = {Biometrics},
  title        = {Rejoinder to “On continuous-time capture-recapture in closed populations”},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On continuous-time capture-recapture in closed populations.
<em>BIOMTC</em>, <em>76</em>(3), 1028–1033. (<a
href="https://doi.org/10.1111/biom.13185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Schofield et al . (2018, Biometrics 74, 626–635) presented simple and efficient algorithms for fitting continuous-time capture-recapture models based on Poisson processes. They also demonstrated by real examples that the standard method of discretizing continuous-time capture-recapture data and then fitting traditional discrete-time models may lead to information loss in population size estimation. In this article, we aim to clarify that key to the approach of Schofield et al . (2018) is the Poisson model assumed for the number of captures of each individual throughout the study, rather than the fact of data being collected in continuous time. We further show that the method of data discretization works equally well as the method of Schofield et al . (2018), provided that a Poisson model is applied instead of the traditional Bernoulli model to the number of captures for each individual on each sampling occasion.},
  archive      = {J_BIOMTC},
  author       = {Wei Zhang and Simon J. Bonner},
  doi          = {10.1111/biom.13185},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1028-1033},
  shortjournal = {Biometrics},
  title        = {On continuous-time capture-recapture in closed populations},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal sampling design and the accuracy of occupancy
models. <em>BIOMTC</em>, <em>76</em>(3), 1017–1027. (<a
href="https://doi.org/10.1111/biom.13203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present general theoretical limits on the possible accuracy (mean squared error or MSE) of occupancy estimates for a large range of occupancy study designs with imperfect detection and confirm our theoretical results via a simulation study. In particular, we show that for a given total survey effort, the best possible MSE is driven by two design-related factors: the fraction of visits made at occupied sites (regardless of whether that occupancy status is known or not) and the number of visits made to each site with unknown occupancy status (ie, sites with no detections). The limits reveal that there is very little room for improvement over optimal implementations of the three existing occupancy design paradigms: standard design (visit S sites K times each), removal design (visit S sites up to K times each, halting visits to each site following a positive detection), and conditional design (visit S sites once, then resurvey sites with a positive detection an additional K − 1 times). For the small portion of the occupancy-detection parameter space where improvement can be achieved, we introduce a new hybrid survey design with accuracy closer to the theoretical limit, which we illustrate by reanalyzing an existing coyote ( Canis latrans ) camera trap dataset. Our results provide new clarity and intuition regarding key factors of occupancy study design.},
  archive      = {J_BIOMTC},
  author       = {Henry T. Reich},
  doi          = {10.1111/biom.13203},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1017-1027},
  shortjournal = {Biometrics},
  title        = {Optimal sampling design and the accuracy of occupancy models},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating treatment importance in multidrug-resistant
tuberculosis using targeted learning: An observational individual
patient data network meta-analysis. <em>BIOMTC</em>, <em>76</em>(3),
1007–1016. (<a href="https://doi.org/10.1111/biom.13210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Persons with multidrug-resistant tuberculosis (MDR-TB) have a disease resulting from a strain of tuberculosis (TB) that does not respond to at least isoniazid and rifampicin, the two most effective anti-TB drugs. MDR-TB is always treated with multiple antimicrobial agents. Our data consist of individual patient data from 31 international observational studies with varying prescription practices, access to medications, and distributions of antibiotic resistance. In this study, we develop identifiability criteria for the estimation of a global treatment importance metric in the context where not all medications are observed in all studies. With stronger causal assumptions, this treatment importance metric can be interpreted as the effect of adding a medication to the existing treatments. We then use this metric to rank 15 observed antimicrobial agents in terms of their estimated add-on value. Using the concept of transportability, we propose an implementation of targeted maximum likelihood estimation, a doubly robust and locally efficient plug-in estimator, to estimate the treatment importance metric. A clustered sandwich estimator is adopted to compute variance estimates and produce confidence intervals. Simulation studies are conducted to assess the performance of our estimator, verify the double robustness property, and assess the appropriateness of the variance estimation approach.},
  archive      = {J_BIOMTC},
  author       = {Guanbo Wang and Mireille E. Schnitzer and Dick Menzies and Piret Viiklepp and Timothy H. Holtz and Andrea Benedetti},
  doi          = {10.1111/biom.13210},
  journal      = {Biometrics},
  number       = {3},
  pages        = {1007-1016},
  shortjournal = {Biometrics},
  title        = {Estimating treatment importance in multidrug-resistant tuberculosis using targeted learning: An observational individual patient data network meta-analysis},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Identifying disease-associated biomarker network features
through conditional graphical model. <em>BIOMTC</em>, <em>76</em>(3),
995–1006. (<a href="https://doi.org/10.1111/biom.13201">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biomarkers are often organized into networks, in which the strengths of network connections vary across subjects depending on subject-specific covariates (eg, genetic variants). Variation of network connections, as subject-specific feature variables, has been found to predict disease clinical outcome. In this work, we develop a two-stage method to estimate biomarker networks that account for heterogeneity among subjects and evaluate network&#39;s association with disease clinical outcome. In the first stage, we propose a conditional Gaussian graphical model with mean and precision matrix depending on covariates to obtain covariate-dependent networks with connection strengths varying across subjects while assuming homogeneous network structure. In the second stage, we evaluate clinical utility of network measures (connection strengths) estimated from the first stage. The second-stage analysis provides the relative predictive power of between-region network measures on clinical impairment in the context of regional biomarkers and existing disease risk factors. We assess the performance of proposed method by extensive simulation studies and application to a Huntington&#39;s disease (HD) study to investigate the effect of HD causal gene on the rate of change in motor symptom through affecting brain subcortical and cortical gray matter atrophy connections. We show that cortical network connections and subcortical volumes, but not subcortical connections are identified to be predictive of clinical motor function deterioration. We validate these findings in an independent HD study. Lastly, highly similar patterns seen in the gray matter connections and a previous white matter connectivity study suggest a shared biological mechanism for HD and support the hypothesis that white matter loss is a direct result of neuronal loss as opposed to the loss of myelin or dysmyelination.},
  archive      = {J_BIOMTC},
  author       = {Shanghong Xie and Xiang Li and Peter McColgan and Rachael I. Scahill and Donglin Zeng and Yuanjia Wang},
  doi          = {10.1111/biom.13201},
  journal      = {Biometrics},
  number       = {3},
  pages        = {995-1006},
  shortjournal = {Biometrics},
  title        = {Identifying disease-associated biomarker network features through conditional graphical model},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint analysis of single-cell and bulk tissue sequencing
data to infer intratumor heterogeneity. <em>BIOMTC</em>, <em>76</em>(3),
983–994. (<a href="https://doi.org/10.1111/biom.13198">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many computational methods have been developed to discern intratumor heterogeneity (ITH) using DNA sequence data from bulk tumor samples. These methods share an assumption that two mutations arise from the same subclone if they have similar mutant allele-frequencies (MAFs), and thus it is difficult or impossible to distinguish two subclones with similar MAFs. Single-cell DNA sequencing (scDNA-seq) data can be very informative for ITH inference. However, due to the difficulty of DNA amplification, scDNA-seq data are often very noisy. A promising new study design is to collect both bulk and single-cell DNA-seq data and jointly analyze them to mitigate the limitations of each data type. To address the analytic challenges of this new study design, we propose a computational method named BaSiC ( B ulk tumor a nd Si ngle C ell), to discern ITH by jointly analyzing DNA-seq data from bulk tumor and single cells. We demonstrate that BaSiC has comparable or better performance than the methods using either data type. We further evaluate BaSiC using bulk tumor and single-cell DNA-seq data from a breast cancer patient and several leukemia patients.},
  archive      = {J_BIOMTC},
  author       = {Wei Sun and Chong Jin and Jonathan A. Gelfond and Ming-Hui Chen and Joseph G. Ibrahim},
  doi          = {10.1111/biom.13198},
  journal      = {Biometrics},
  number       = {3},
  pages        = {983-994},
  shortjournal = {Biometrics},
  title        = {Joint analysis of single-cell and bulk tissue sequencing data to infer intratumor heterogeneity},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fair regression for health care spending. <em>BIOMTC</em>,
<em>76</em>(3), 973–982. (<a
href="https://doi.org/10.1111/biom.13206">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The distribution of health care payments to insurance plans has substantial consequences for social policy. Risk adjustment formulas predict spending in health insurance markets in order to provide fair benefits and health care coverage for all enrollees, regardless of their health status. Unfortunately, current risk adjustment formulas are known to underpredict spending for specific groups of enrollees leading to undercompensated payments to health insurers. This incentivizes insurers to design their plans such that individuals in undercompensated groups will be less likely to enroll, impacting access to health care for these groups. To improve risk adjustment formulas for undercompensated groups, we expand on concepts from the statistics, computer science, and health economics literature to develop new fair regression methods for continuous outcomes by building fairness considerations directly into the objective function. We additionally propose a novel measure of fairness while asserting that a suite of metrics is necessary in order to evaluate risk adjustment formulas more fully. Our data application using the IBM MarketScan Research Databases and simulation studies demonstrates that these new fair regression methods may lead to massive improvements in group fairness (eg, 98\%) with only small reductions in overall fit (eg, 4\%).},
  archive      = {J_BIOMTC},
  author       = {Anna Zink and Sherri Rose},
  doi          = {10.1111/biom.13206},
  journal      = {Biometrics},
  number       = {3},
  pages        = {973-982},
  shortjournal = {Biometrics},
  title        = {Fair regression for health care spending},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Corrections for measurement error due to delayed onset of
illness for case-crossover designs. <em>BIOMTC</em>, <em>76</em>(3),
963–972. (<a href="https://doi.org/10.1111/biom.13173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epidemiologic studies of the short-term effects of ambient particulate matter (PM) on the risk of acute cardiovascular or cerebrovascular events often use data from administrative databases in which only the date of hospitalization is known. A common study design for analyzing such data is the case-crossover design, in which exposure at a time when a patient experiences an event is compared to exposure at times when the patient did not experience an event within a case-control paradigm. However, the time of true event onset may precede hospitalization by hours or days, which can yield attenuated effect estimates. In this article, we consider a marginal likelihood estimator, a regression calibration estimator, and a conditional score estimator, as well as parametric bootstrap versions of each, to correct for this bias. All considered approaches require validation data on the distribution of the delay times. We compare the performance of the approaches in realistic scenarios via simulation, and apply the methods to analyze data from a Boston-area study of the association between ambient air pollution and acute stroke onset. Based on both simulation and the case study, we conclude that a two-stage regression calibration estimator with a parametric bootstrap bias correction is an effective method for correcting bias in health effect estimates arising from delayed onset in a case-crossover study.},
  archive      = {J_BIOMTC},
  author       = {Brent A. Coull and Seokho Lee and Glen McGee and Justin Manjourides and Murray A. Mittleman and Gregory A. Wellenius},
  doi          = {10.1111/biom.13173},
  journal      = {Biometrics},
  number       = {3},
  pages        = {963-972},
  shortjournal = {Biometrics},
  title        = {Corrections for measurement error due to delayed onset of illness for case-crossover designs},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Power calculation for cross-sectional stepped wedge cluster
randomized trials with variable cluster sizes. <em>BIOMTC</em>,
<em>76</em>(3), 951–962. (<a
href="https://doi.org/10.1111/biom.13164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Standard sample size calculation formulas for stepped wedge cluster randomized trials (SW-CRTs) assume that cluster sizes are equal. When cluster sizes vary substantially, ignoring this variation may lead to an under-powered study. We investigate the relative efficiency of a SW-CRT with varying cluster sizes to equal cluster sizes, and derive variance estimators for the intervention effect that account for this variation under a mixed effects model—a commonly used approach for analyzing data from cluster randomized trials. When cluster sizes vary, the power of a SW-CRT depends on the order in which clusters receive the intervention, which is determined through randomization. We first derive a variance formula that corresponds to any particular realization of the randomized sequence and propose efficient algorithms to identify upper and lower bounds of the power. We then obtain an “expected” power based on a first-order approximation to the variance formula, where the expectation is taken with respect to all possible randomization sequences. Finally, we provide a variance formula for more general settings where only the cluster size arithmetic mean and coefficient of variation, instead of exact cluster sizes, are known in the design stage. We evaluate our methods through simulations and illustrate that the average power of a SW-CRT decreases as the variation in cluster sizes increases, and the impact is largest when the number of clusters is small.},
  archive      = {J_BIOMTC},
  author       = {Linda J Harrison and Tom Chen and Rui Wang},
  doi          = {10.1111/biom.13164},
  journal      = {Biometrics},
  number       = {3},
  pages        = {951-962},
  shortjournal = {Biometrics},
  title        = {Power calculation for cross-sectional stepped wedge cluster randomized trials with variable cluster sizes},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sample size and power for the weighted log-rank test and
kaplan-meier based tests with allowance for nonproportional hazards.
<em>BIOMTC</em>, <em>76</em>(3), 939–950. (<a
href="https://doi.org/10.1111/biom.13196">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Asymptotic distributions under alternative hypotheses and their corresponding sample size and power equations are derived for nonparametric test statistics commonly used to compare two survival curves. Test statistics include the weighted log-rank test and the Wald test for difference in (or ratio of) Kaplan-Meier survival probability, percentile survival, and restricted mean survival time. Accrual, survival, and loss to follow-up are allowed to follow any arbitrary continuous distribution. We show that Schoenfeld&#39;s equation—often used by practitioners to calculate the required number of events for the unweighted log-rank test—can be inaccurate even when the proportional hazards (PH) assumption holds. In fact, it can mislead one to believe that 1:1 is the optimal randomization ratio (RR), when actually power can be gained by assigning more patients to the active arm. Meaningful improvements to Schoenfeld&#39;s equation are made. The present theory should be useful in designing clinical trials, particularly in immuno-oncology where nonproportional hazards are frequently encountered. We illustrate the application of our theory with an example exploring optimal RR under PH and a second example examining the impact of delayed treatment effect. A companion R package npsurvSS is available for download on CRAN.},
  archive      = {J_BIOMTC},
  author       = {Godwin Yung and Yi Liu},
  doi          = {10.1111/biom.13196},
  journal      = {Biometrics},
  number       = {3},
  pages        = {939-950},
  shortjournal = {Biometrics},
  title        = {Sample size and power for the weighted log-rank test and kaplan-meier based tests with allowance for nonproportional hazards},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multilevel mixed effects varying coefficient model with
multilevel predictors and random effects for modeling hospitalization
risk in patients on dialysis. <em>BIOMTC</em>, <em>76</em>(3), 924–938.
(<a href="https://doi.org/10.1111/biom.13205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For patients on dialysis, hospitalizations remain a major risk factor for mortality and morbidity. We use data from a large national database, United States Renal Data System, to model time-varying effects of hospitalization risk factors as functions of time since initiation of dialysis. To account for the three-level hierarchical structure in the data where hospitalizations are nested in patients and patients are nested in dialysis facilities, we propose a multilevel mixed effects varying coefficient model (MME-VCM) where multilevel (patient- and facility-level) random effects are used to model the dependence structure of the data. The proposed MME-VCM also includes multilevel covariates, where baseline demographics and comorbidities are among the patient-level factors, and staffing composition and facility size are among the facility-level risk factors. To address the challenge of high-dimensional integrals due to the hierarchical structure of the random effects, we propose a novel two-step approximate EM algorithm based on the fully exponential Laplace approximation. Inference for the varying coefficient functions and variance components is achieved via derivation of the standard errors using score contributions. The finite sample performance of the proposed estimation procedure is studied through simulations.},
  archive      = {J_BIOMTC},
  author       = {Yihao Li and Danh V. Nguyen and Esra Kürüm and Connie M. Rhee and Yanjun Chen and Kamyar Kalantar-Zadeh and Damla Şentürk},
  doi          = {10.1111/biom.13205},
  journal      = {Biometrics},
  number       = {3},
  pages        = {924-938},
  shortjournal = {Biometrics},
  title        = {A multilevel mixed effects varying coefficient model with multilevel predictors and random effects for modeling hospitalization risk in patients on dialysis},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). From mixed effects modeling to spike and slab variable
selection: A bayesian regression model for group testing data.
<em>BIOMTC</em>, <em>76</em>(3), 913–923. (<a
href="https://doi.org/10.1111/biom.13176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to reductions in both time and cost, group testing is a popular alternative to individual-level testing for disease screening. These reductions are obtained by testing pooled biospecimens (eg, blood, urine, swabs, etc.) for the presence of an infectious agent. However, these reductions come at the expense of data complexity, making the task of conducting disease surveillance more tenuous when compared to using individual-level data. This is because an individual&#39;s disease status may be obscured by a group testing protocol and the effect of imperfect testing. Furthermore, unlike individual-level testing, a given participant could be involved in multiple testing outcomes and/or may never be tested individually. To circumvent these complexities and to incorporate all available information, we propose a Bayesian generalized linear mixed model that accommodates data arising from any group testing protocol, estimates unknown assay accuracy probabilities and accounts for potential heterogeneity in the covariate effects across population subgroups (eg, clinic sites, etc.); this latter feature is of key interest to practitioners tasked with conducting disease surveillance. To achieve model selection, our proposal uses spike and slab priors for both fixed and random effects. The methodology is illustrated through numerical studies and is applied to chlamydia surveillance data collected in Iowa.},
  archive      = {J_BIOMTC},
  author       = {Chase N. Joyner and Christopher S. McMahan and Joshua M. Tebbs and Christopher R. Bilder},
  doi          = {10.1111/biom.13176},
  journal      = {Biometrics},
  number       = {3},
  pages        = {913-923},
  shortjournal = {Biometrics},
  title        = {From mixed effects modeling to spike and slab variable selection: A bayesian regression model for group testing data},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A temporally stratified extension of space-for-time
cormack–jolly–seber for migratory animals. <em>BIOMTC</em>,
<em>76</em>(3), 900–912. (<a
href="https://doi.org/10.1111/biom.13171">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding drivers of temporal variation in demographic parameters is a central goal of mark-recapture analysis. To estimate the survival of migrating animal populations in migration corridors, space-for-time mark–recapture models employ discrete sampling locations in space to monitor marked populations as they move past monitoring sites, rather than the standard practice of using fixed sampling points in time. Because these models focus on estimating survival over discrete spatial segments, model parameters are implicitly integrated over the temporal dimension. Furthermore, modeling the effect of time-varying covariates on model parameters is complicated by unknown passage times for individuals that are not detected at monitoring sites. To overcome these limitations, we extended the Cormack–Jolly–Seber (CJS) framework to estimate temporally stratified survival and capture probabilities by including a discretized arrival time process in a Bayesian framework. We allow for flexibility in the model form by including temporally stratified covariates and hierarchical structures. In addition, we provide tools for assessing model fit and comparing among alternative structural models for the parameters. We demonstrate our framework by fitting three competing models to estimate daily survival, capture, and arrival probabilities at four hydroelectric dams for over 200 000 individually tagged migratory juvenile salmon released into the Snake River, USA.},
  archive      = {J_BIOMTC},
  author       = {Dalton J. Hance and Russell W. Perry and John M. Plumb and Adam C. Pope},
  doi          = {10.1111/biom.13171},
  journal      = {Biometrics},
  number       = {3},
  pages        = {900-912},
  shortjournal = {Biometrics},
  title        = {A temporally stratified extension of space-for-time Cormack–Jolly–Seber for migratory animals},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic modeling of multivariate dimensions and their
temporal relationships using latent processes: Application to
alzheimer’s disease. <em>BIOMTC</em>, <em>76</em>(3), 886–899. (<a
href="https://doi.org/10.1111/biom.13168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer&#39;s disease gradually affects several components including the cerebral dimension with brain atrophies, the cognitive dimension with a decline in various functions, and the functional dimension with impairment in the daily living activities. Understanding how such dimensions interconnect is crucial for Alzheimer&#39;s disease research. However, it requires to simultaneously capture the dynamic and multidimensional aspects and to explore temporal relationships between dimensions. We propose an original dynamic structural model that accounts for all these features. The model defines dimensions as latent processes and combines a multivariate linear mixed model and a system of difference equations to model trajectories and temporal relationships between latent processes in finely discrete time. Dimensions are simultaneously related to their observed (possibly multivariate) markers through nonlinear equations of observation. Parameters are estimated in the maximum likelihood framework enjoying a closed form for the likelihood. We demonstrate in a simulation study that this dynamic model in discrete time benefits the same causal interpretation of temporal relationships as models defined in continuous time as long as the discretization step remains small. The model is then applied to the data of the Alzheimer&#39;s Disease Neuroimaging Initiative. Three longitudinal dimensions (cerebral anatomy, cognitive ability, and functional autonomy) measured by six markers are analyzed, and their temporal structure is contrasted between different clinical stages of Alzheimer&#39;s disease.},
  archive      = {J_BIOMTC},
  author       = {Bachirou O. Taddé and Hélène Jacqmin-Gadda and Jean-François Dartigues and Daniel Commenges and Cécile Proust-Lima},
  doi          = {10.1111/biom.13168},
  journal      = {Biometrics},
  number       = {3},
  pages        = {886-899},
  shortjournal = {Biometrics},
  title        = {Dynamic modeling of multivariate dimensions and their temporal relationships using latent processes: Application to alzheimer&#39;s disease},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discovering structure in multiple outcomes models for tests
of childhood neurodevelopment. <em>BIOMTC</em>, <em>76</em>(3), 874–885.
(<a href="https://doi.org/10.1111/biom.13174">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian model–based clustering provides a powerful and flexible tool that can be incorporated into regression models to better understand the grouping of observations. Using data from the Seychelles Child Development Study, we explore the effect of prenatal methylmercury exposure on 20 neurodevelopmental outcomes measured in 9-year-old children. Rather than cluster individual subjects, we cluster the outcomes within a multiple outcomes model. By using information in the data to nest the outcomes into groups called domains, the model more accurately reflects the shared characteristics of neurodevelopmental domains and improves estimation of the overall and outcome-specific exposure effects by shrinking effects within and between domains selected by the data. The Bayesian paradigm allows for sampling from the posterior distribution of the grouping parameters; thus, inference can be made about group membership and their defining characteristics. We avoid the often difficult and highly subjective requirement of a priori identification of the total number of groups by incorporating a Dirichlet process prior to form a fully Bayesian multiple outcomes model.},
  archive      = {J_BIOMTC},
  author       = {Amy LaLonde and Tanzy Love and Sally W. Thurston and Philip W. Davidson},
  doi          = {10.1111/biom.13174},
  journal      = {Biometrics},
  number       = {3},
  pages        = {874-885},
  shortjournal = {Biometrics},
  title        = {Discovering structure in multiple outcomes models for tests of childhood neurodevelopment},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Order-restricted inference for clustered ROC data with
application to fingerprint matching accuracy. <em>BIOMTC</em>,
<em>76</em>(3), 863–873. (<a
href="https://doi.org/10.1111/biom.13177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Receiver operating characteristic (ROC) curve is commonly used to evaluate and compare the accuracy of classification methods or markers. Estimating ROC curves has been an important problem in various fields including biometric recognition and diagnostic medicine. In real applications, classification markers are often developed under two or more ordered conditions, such that a natural stochastic ordering exists among the observations. Incorporating such a stochastic ordering into estimation can improve statistical efficiency (Davidov and Herman, 2012). In addition, clustered and correlated data arise when multiple measurements are gleaned from the same subject, making estimation of ROC curves complicated due to within-cluster correlations. In this article, we propose to model the ROC curve using a weighted empirical process to jointly account for the order constraint and within-cluster correlation structure. The algebraic properties of resulting summary statistics of the ROC curve such as its area and partial area are also studied. The algebraic expressions reduce to the ones by Davidov and Herman (2012) for independent observations. We derive asymptotic properties of the proposed order-restricted estimators and show that they have smaller mean-squared errors than the existing estimators. Simulation studies also demonstrate better performance of the newly proposed estimators over existing methods for finite samples. The proposed method is further exemplified with the fingerprint matching data from the National Institute of Standards and Technology Special Database 4.},
  archive      = {J_BIOMTC},
  author       = {Wei Zhang and Larry L. Tang and Qizhai Li and Aiyi Liu and Mei-Ling Ting Lee},
  doi          = {10.1111/biom.13177},
  journal      = {Biometrics},
  number       = {3},
  pages        = {863-873},
  shortjournal = {Biometrics},
  title        = {Order-restricted inference for clustered ROC data with application to fingerprint matching accuracy},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Learning-based biomarker-assisted rules for optimized
clinical benefit under a risk constraint. <em>BIOMTC</em>,
<em>76</em>(3), 853–862. (<a
href="https://doi.org/10.1111/biom.13199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Novel biomarkers, in combination with currently available clinical information, have been sought to improve clinical decision making in many branches of medicine, including screening, surveillance, and prognosis. Statistical methods are needed to integrate such diverse information to develop targeted interventions that balance benefit and harm. In the specific setting of disease detection, we propose novel approaches to construct a multiple-marker-based decision rule by directly optimizing a benefit function, while controlling harm at a maximally tolerable level. These new approaches include plug-in and direct-optimization-based algorithms, and they allow for the construction of both nonparametric and parametric rules. A study of asymptotic properties of the proposed estimators is provided. Simulation results demonstrate good clinical utilities for the resulting decision rules under various scenarios. The methods are applied to a biomarker study in prostate cancer surveillance.},
  archive      = {J_BIOMTC},
  author       = {Yanqing Wang and Ying-Qi Zhao and Yingye Zheng},
  doi          = {10.1111/biom.13199},
  journal      = {Biometrics},
  number       = {3},
  pages        = {853-862},
  shortjournal = {Biometrics},
  title        = {Learning-based biomarker-assisted rules for optimized clinical benefit under a risk constraint},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Statistical inference for net benefit measures in biomarker
validation studies. <em>BIOMTC</em>, <em>76</em>(3), 843–852. (<a
href="https://doi.org/10.1111/biom.13190">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Referral strategies based on risk scores and medical tests are commonly proposed. Direct assessment of their clinical utility requires implementing the strategy and is not possible in the early phases of biomarker research. Prior to late-phase studies, net benefit measures can be used to assess the potential clinical impact of a proposed strategy. Validation studies, in which the biomarker defines a prespecified referral strategy, are a gold standard approach to evaluating biomarker potential. Uncertainty, quantified by a confidence interval, is important to consider when deciding whether a biomarker warrants an impact study, does not demonstrate clinical potential, or that more data are needed. We establish distribution theory for empirical estimators of net benefit and propose empirical estimators of variance. The primary results are for the most commonly employed estimators of net benefit: from cohort and unmatched case-control samples, and for point estimates and net benefit curves. Novel estimators of net benefit under stratified two-phase and categorically matched case-control sampling are proposed and distribution theory developed. Results for common variants of net benefit and for estimation from right-censored outcomes are also presented. We motivate and demonstrate the methodology with examples from lung cancer research and highlight its application to study design.},
  archive      = {J_BIOMTC},
  author       = {Tracey L. Marsh and Holly Janes and Margaret S. Pepe},
  doi          = {10.1111/biom.13190},
  journal      = {Biometrics},
  number       = {3},
  pages        = {843-852},
  shortjournal = {Biometrics},
  title        = {Statistical inference for net benefit measures in biomarker validation studies},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating overdispersion in sparse multinomial data.
<em>BIOMTC</em>, <em>76</em>(3), 834–842. (<a
href="https://doi.org/10.1111/biom.13194">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multinomial data arise in many areas of the life sciences, such as mark-recapture studies and phylogenetics, and will often by overdispersed, with the variance being higher than predicted by a multinomial model. The quasi-likelihood approach to modeling this overdispersion involves the assumption that the variance is proportional to that specified by the multinomial model. As this approach does not require specification of the full distribution of the response variable, it can be more robust than fitting a Dirichlet-multinomial model or adding a random effect to the linear predictor. Estimation of the amount of overdispersion is often based on Pearson&#39;s statistic X 2 or the deviance D . For many types of study, such as mark-recapture, the data will be sparse. The estimator based on X 2 can then be highly variable, and that based on D can have a large negative bias. We derive a new estimator, which has a smaller asymptotic variance than that based on X 2 , the difference being most marked for sparse data. We illustrate the numerical difference between the three estimators using a mark-recapture study of swifts and compare their performance via a simulation study. The new estimator has the lowest root mean squared error across a range of scenarios, especially when the data are very sparse.},
  archive      = {J_BIOMTC},
  author       = {Farzana Afroz and Matt Parry and David Fletcher},
  doi          = {10.1111/biom.13194},
  journal      = {Biometrics},
  number       = {3},
  pages        = {834-842},
  shortjournal = {Biometrics},
  title        = {Estimating overdispersion in sparse multinomial data},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A test of homogeneity of distributions when observations are
subject to measurement errors. <em>BIOMTC</em>, <em>76</em>(3), 821–833.
(<a href="https://doi.org/10.1111/biom.13207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When the observed data are contaminated with errors, the standard two-sample testing approaches that ignore measurement errors may produce misleading results, including a higher type-I error rate than the nominal level. To tackle this inconsistency, a nonparametric test is proposed for testing equality of two distributions when the observed contaminated data follow the classical additive measurement error model. The proposed test takes into account the presence of errors in the observed data, and the test statistic is defined in terms of the (deconvoluted) characteristic functions of the latent variables. Proposed method is applicable to a wide range of scenarios as no parametric restrictions are imposed either on the distribution of the underlying latent variables or on the distribution of the measurement errors. Asymptotic null distribution of the test statistic is derived, which is given by an integral of a squared Gaussian process with a complicated covariance structure. For data-based calibration of the test, a new nonparametric Bootstrap method is developed under the two-sample measurement error framework and its validity is established. Finite sample performance of the proposed test is investigated through simulation studies, and the results show superior performance of the proposed method than the standard tests that exhibit inconsistent behavior. Finally, the proposed method was applied to real data sets from the National Health and Nutrition Examination Survey. An R package MEtest is available through CRAN.},
  archive      = {J_BIOMTC},
  author       = {DongHyuk Lee and Soumendra N. Lahiri and Samiran Sinha},
  doi          = {10.1111/biom.13207},
  journal      = {Biometrics},
  number       = {3},
  pages        = {821-833},
  shortjournal = {Biometrics},
  title        = {A test of homogeneity of distributions when observations are subject to measurement errors},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A fast score test for generalized mixture models.
<em>BIOMTC</em>, <em>76</em>(3), 811–820. (<a
href="https://doi.org/10.1111/biom.13204">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In biomedical studies, testing for homogeneity between two groups, where one group is modeled by mixture models, is often of great interest. This paper considers the semiparametric exponential family mixture model proposed by Hong et al . (2017) and studies the score test for homogeneity under this model. The score test is nonregular in the sense that nuisance parameters disappear under the null hypothesis. To address this difficulty, we propose a modification of the score test, so that the resulting test enjoys the Wilks phenomenon. In finite samples, we show that with fixed nuisance parameters the score test is locally most powerful. In large samples, we establish the asymptotic power functions under two types of local alternative hypotheses. Our simulation studies illustrate that the proposed score test is powerful and computationally fast. We apply the proposed score test to an UK ovarian cancer DNA methylation data for identification of differentially methylated CpG sites.},
  archive      = {J_BIOMTC},
  author       = {Rui Duan and Yang Ning and Shuang Wang and Bruce G. Lindsay and Raymond J. Carroll and Yong Chen},
  doi          = {10.1111/biom.13204},
  journal      = {Biometrics},
  number       = {3},
  pages        = {811-820},
  shortjournal = {Biometrics},
  title        = {A fast score test for generalized mixture models},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). G-estimation of structural nested restricted mean time lost
models to estimate effects of time-varying treatments on a failure time
outcome. <em>BIOMTC</em>, <em>76</em>(3), 799–810. (<a
href="https://doi.org/10.1111/biom.13200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {G-estimation of structural nested models (SNMs) plays an important role in estimating the effects of time-varying treatments with appropriate adjustment for time-dependent confounding. As SNMs for a failure time outcome, structural nested accelerated failure time models (SNAFTMs) and structural nested cumulative failure time models have been developed. The latter models are included in the class of structural nested mean models (SNMMs) and are not involved in artificial censoring, which induces several difficulties in g-estimation of SNAFTMs. Recently, restricted mean time lost (RMTL), which corresponds to the area under a distribution function up to a restriction time, is attracting attention in clinical trial communities as an appropriate summary measure of a failure time outcome. In this study, we propose another SNMM for a failure time outcome, which is called structural nested RMTL model (SNRMTLM) and describe randomized and observational g-estimation procedures that use different assumptions for the treatment mechanism in a randomized trial setting. We also provide methods to estimate marginal RMTLs under static treatment regimes using estimated SNRMTLMs. A simulation study evaluates finite-sample performances of the proposed methods compared with the conventional intention-to-treat and per-protocol analyses. We illustrate the proposed methods using data from a randomized controlled trial for cardiovascular disease with treatment changes. G-estimation of SNRMTLMs is a useful tool to estimate the effects of time-varying treatments on a failure time outcome.},
  archive      = {J_BIOMTC},
  author       = {Yasuhiro Hagiwara and Tomohiro Shinozaki and Yutaka Matsuyama},
  doi          = {10.1111/biom.13200},
  journal      = {Biometrics},
  number       = {3},
  pages        = {799-810},
  shortjournal = {Biometrics},
  title        = {G-estimation of structural nested restricted mean time lost models to estimate effects of time-varying treatments on a failure time outcome},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multivariate hierarchical frameworks for modeling delayed
reporting in count data. <em>BIOMTC</em>, <em>76</em>(3), 789–798. (<a
href="https://doi.org/10.1111/biom.13188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many fields and applications, count data can be subject to delayed reporting. This is where the total count, such as the number of disease cases contracted in a given week, may not be immediately available, instead arriving in parts over time. For short-term decision making, the statistical challenge lies in predicting the total count based on any observed partial counts, along with a robust quantification of uncertainty. We discuss previous approaches to modeling delayed reporting and present a multivariate hierarchical framework where the count generating process and delay mechanism are modeled simultaneously in a flexible way. This framework can also be easily adapted to allow for the presence of underreporting in the final observed count. To illustrate our approach and to compare it with existing frameworks, we present a case study of reported dengue fever cases in Rio de Janeiro. Based on both within-sample and out-of-sample posterior predictive model checking and arguments of interpretability, adaptability, and computational efficiency, we discuss the relative merits of different approaches.},
  archive      = {J_BIOMTC},
  author       = {Oliver Stoner and Theo Economou},
  doi          = {10.1111/biom.13188},
  journal      = {Biometrics},
  number       = {3},
  pages        = {789-798},
  shortjournal = {Biometrics},
  title        = {Multivariate hierarchical frameworks for modeling delayed reporting in count data},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating individualized treatment regimes from crossover
designs. <em>BIOMTC</em>, <em>76</em>(3), 778–788. (<a
href="https://doi.org/10.1111/biom.13186">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of precision medicine aims to tailor treatment based on patient-specific factors in a reproducible way. To this end, estimating an optimal individualized treatment regime (ITR) that recommends treatment decisions based on patient characteristics to maximize the mean of a prespecified outcome is of particular interest. Several methods have been proposed for estimating an optimal ITR from clinical trial data in the parallel group setting where each subject is randomized to a single intervention. However, little work has been done in the area of estimating the optimal ITR from crossover study designs. Such designs naturally lend themselves to precision medicine since they allow for observing the response to multiple treatments for each patient. In this paper, we introduce a method for estimating the optimal ITR using data from a 2 × 2 crossover study with or without carryover effects. The proposed method is similar to policy search methods such as outcome weighted learning; however, we take advantage of the crossover design by using the difference in responses under each treatment as the observed reward. We establish Fisher and global consistency, present numerical experiments, and analyze data from a feeding trial to demonstrate the improved performance of the proposed method compared to standard methods for a parallel study design.},
  archive      = {J_BIOMTC},
  author       = {Crystal T. Nguyen and Daniel J. Luckett and Anna R. Kahkoska and Grace E. Shearrer and Donna Spruijt-Metz and Jaimie N. Davis and Michael R. Kosorok},
  doi          = {10.1111/biom.13186},
  journal      = {Biometrics},
  number       = {3},
  pages        = {778-788},
  shortjournal = {Biometrics},
  title        = {Estimating individualized treatment regimes from crossover designs},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating average treatment effects with a double-index
propensity score. <em>BIOMTC</em>, <em>76</em>(3), 767–777. (<a
href="https://doi.org/10.1111/biom.13195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider estimating average treatment effects (ATE) of a binary treatment in observational data when data-driven variable selection is needed to select relevant covariates from a moderately large number of available covariates X . To leverage covariates among X predictive of the outcome for efficiency gain while using regularization to fit a parametric propensity score (PS) model, we consider a dimension reduction of X based on fitting both working PS and outcome models using adaptive LASSO. A novel PS estimator, the Double-index Propensity Score (DiPS), is proposed, in which the treatment status is smoothed over the linear predictors for X from both the initial working models. The ATE is estimated by using the DiPS in a normalized inverse probability weighting estimator, which is found to maintain double robustness and also local semiparametric efficiency with a fixed number of covariates p . Under misspecification of working models, the smoothing step leads to gains in efficiency and robustness over traditional doubly robust estimators. These results are extended to the case where p diverges with sample size and working models are sparse. Simulations show the benefits of the approach in finite samples. We illustrate the method by estimating the ATE of statins on colorectal cancer risk in an electronic medical record study and the effect of smoking on C-reactive protein in the Framingham Offspring Study.},
  archive      = {J_BIOMTC},
  author       = {David Cheng and Abhishek Chakrabortty and Ashwin N. Ananthakrishnan and Tianxi Cai},
  doi          = {10.1111/biom.13195},
  journal      = {Biometrics},
  number       = {3},
  pages        = {767-777},
  shortjournal = {Biometrics},
  title        = {Estimating average treatment effects with a double-index propensity score},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bivariate joint frailty model with mixture framework for
survival analysis of recurrent events with dependent censoring and cure
fraction. <em>BIOMTC</em>, <em>76</em>(3), 753–766. (<a
href="https://doi.org/10.1111/biom.13202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the study of multiple failure time data with recurrent clinical endpoints, the classical independent censoring assumption in survival analysis can be violated when the evolution of the recurrent events is correlated with a censoring mechanism such as death. Moreover, in some situations, a cure fraction appears in the data because a tangible proportion of the study population benefits from treatment and becomes recurrence free and insusceptible to death related to the disease. A bivariate joint frailty mixture cure model is proposed to allow for dependent censoring and cure fraction in recurrent event data. The latency part of the model consists of two intensity functions for the hazard rates of recurrent events and death, wherein a bivariate frailty is introduced by means of the generalized linear mixed model methodology to adjust for dependent censoring. The model allows covariates and frailties in both the incidence and the latency parts, and it further accounts for the possibility of cure after each recurrence. It includes the joint frailty model and other related models as special cases. An expectation-maximization (EM)-type algorithm is developed to provide residual maximum likelihood estimation of model parameters. Through simulation studies, the performance of the model is investigated under different magnitudes of dependent censoring and cure rate. The model is applied to data sets from two colorectal cancer studies to illustrate its practical value.},
  archive      = {J_BIOMTC},
  author       = {Richard Tawiah and Geoffrey J. McLachlan and Shu Kay Ng},
  doi          = {10.1111/biom.13202},
  journal      = {Biometrics},
  number       = {3},
  pages        = {753-766},
  shortjournal = {Biometrics},
  title        = {A bivariate joint frailty model with mixture framework for survival analysis of recurrent events with dependent censoring and cure fraction},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal approximate conversions of odds ratios and hazard
ratios to risk ratios. <em>BIOMTC</em>, <em>76</em>(3), 746–752. (<a
href="https://doi.org/10.1111/biom.13197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Odds ratios approximate risk ratios when the outcome under consideration is rare but can diverge substantially from risk ratios when the outcome is common. In this paper, we derive optimal analytic conversions of odds ratios and hazard ratios to risk ratios that are minimax for the bias ratio when outcome probabilities are specified to fall in any fixed interval. The results for hazard ratios are derived under a proportional hazard assumption for the exposure. For outcome probabilities specified to lie in symmetric intervals centered around 0.5, it is shown that the square-root transformation of the odds ratio is the optimal minimax conversion for the risk ratio. General results for any nonsymmetric interval are given both for odds ratio and for hazard ratio conversions. The results are principally useful when odds ratios or hazard ratios are reported in papers, and the reader does not have access to the data or to information about the overall outcome prevalence.},
  archive      = {J_BIOMTC},
  author       = {Tyler J. VanderWeele},
  doi          = {10.1111/biom.13197},
  journal      = {Biometrics},
  number       = {3},
  pages        = {746-752},
  shortjournal = {Biometrics},
  title        = {Optimal approximate conversions of odds ratios and hazard ratios to risk ratios},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Censored linear regression in the presence or absence of
auxiliary survival information. <em>BIOMTC</em>, <em>76</em>(3),
734–745. (<a href="https://doi.org/10.1111/biom.13193">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been a rising interest in better exploiting auxiliary summary information from large databases in the analysis of smaller-scale studies that collect more comprehensive patient-level information. The purpose of this paper is twofold: first, we propose a novel approach to synthesize information from both the aggregate summary statistics and the individual-level data in censored linear regression. We show that the auxiliary information amounts to a system of nonsmooth estimating equations and thus can be combined with the conventional weighted log-rank estimating equations by using the generalized method of moments (GMM) approach. The proposed methodology can be further extended to account for the potential inconsistency in information from different sources. Second, in the absence of auxiliary information, we propose to improve estimation efficiency by combining the overidentified weighted log-rank estimating equations with different weight functions via the GMM framework. To deal with the nonsmooth GMM-type objective functions, we develop an asymptotics-guided algorithm for parameter and variance estimation. We establish the asymptotic normality of the proposed GMM-type estimators. Simulation studies show that the proposed estimators can yield substantial efficiency gain over the conventional weighted log-rank estimators. The proposed methods are applied to a pancreatic cancer study for illustration.},
  archive      = {J_BIOMTC},
  author       = {Ying Sheng and Yifei Sun and Detian Deng and Chiung-Yu Huang},
  doi          = {10.1111/biom.13193},
  journal      = {Biometrics},
  number       = {3},
  pages        = {734-745},
  shortjournal = {Biometrics},
  title        = {Censored linear regression in the presence or absence of auxiliary survival information},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). One-step targeted maximum likelihood estimation for
time-to-event outcomes. <em>BIOMTC</em>, <em>76</em>(3), 722–733. (<a
href="https://doi.org/10.1111/biom.13172">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers in observational survival analysis are interested in not only estimating survival curve nonparametrically but also having statistical inference for the parameter. We consider right-censored failure time data where we observe n independent and identically distributed observations of a vector random variable consisting of baseline covariates, a binary treatment at baseline, a survival time subject to right censoring, and the censoring indicator. We assume the baseline covariates are allowed to affect the treatment and censoring so that an estimator that ignores covariate information would be inconsistent. The goal is to use these data to estimate the counterfactual average survival curve of the population if all subjects are assigned the same treatment at baseline. Existing observational survival analysis methods do not result in monotone survival curve estimators, which is undesirable and may lose efficiency by not constraining the shape of the estimator using the prior knowledge of the estimand. In this paper, we present a one-step Targeted Maximum Likelihood Estimator (TMLE) for estimating the counterfactual average survival curve. We show that this new TMLE can be executed via recursion in small local updates. We demonstrate the finite sample performance of this one-step TMLE in simulations and an application to a monoclonal gammopathy data.},
  archive      = {J_BIOMTC},
  author       = {Weixin Cai and Mark J. van der Laan},
  doi          = {10.1111/biom.13172},
  journal      = {Biometrics},
  number       = {3},
  pages        = {722-733},
  shortjournal = {Biometrics},
  title        = {One-step targeted maximum likelihood estimation for time-to-event outcomes},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian compartmental models and associated reproductive
numbers for an infection with multiple transmission modes.
<em>BIOMTC</em>, <em>76</em>(3), 711–721. (<a
href="https://doi.org/10.1111/biom.13192">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zoonotic visceral leishmaniasis (ZVL) is a serious neglected tropical disease that is endemic in 98 countries. ZVL is primarily transmitted via a sand fly vector. In the United States, it is enzootic in some canine populations; it is transmitted from infectious mother to pup transplacentally, and vector-borne transmission is absent. This absence affords a unique opportunity to study (1) vertical transmission dynamics in dogs and (2) the importance of vertical transmission in maintaining an infectious reservoir in the presence of a vector. In this paper, we present Bayesian compartmental models and reproductive number formulations to examine (1) and (2), providing a mechanism to plan and evaluate interventions in regions where both transmission modes are present. First, we propose an individual-level susceptible, infectious, removed (SIR) model to study the effect of maternal infection status during pregnancy on pup infection progression. We provide evidence that pups born to diagnostically positive mothers during pregnancy are more likely to become diagnostically positive both earlier in life, and at some point during their lifetime, than those born to diagnostically negative mothers. Second, we propose a population-level SIR model to study the impact of a vertically maintained reservoir on propagating infection in a naive canine population through emergent vector transmission using simulation studies. We also present reproductive numbers to quantify contributions of vertically infected and vector-infected dogs to maintaining infection in the population. We show that a vertically maintained canine reservoir can propagate infection in a theoretical naive population in the presence of a vector.},
  archive      = {J_BIOMTC},
  author       = {Marie V. Ozanne and Grant D. Brown and Angela J. Toepp and Breanna M. Scorza and Jacob J. Oleson and Mary E. Wilson and Christine A. Petersen},
  doi          = {10.1111/biom.13192},
  journal      = {Biometrics},
  number       = {3},
  pages        = {711-721},
  shortjournal = {Biometrics},
  title        = {Bayesian compartmental models and associated reproductive numbers for an infection with multiple transmission modes},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian shrinkage estimation of high dimensional causal
mediation effects in omics studies. <em>BIOMTC</em>, <em>76</em>(3),
700–710. (<a href="https://doi.org/10.1111/biom.13189">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal mediation analysis aims to examine the role of a mediator or a group of mediators that lie in the pathway between an exposure and an outcome. Recent biomedical studies often involve a large number of potential mediators based on high-throughput technologies. Most of the current analytic methods focus on settings with one or a moderate number of potential mediators. With the expanding growth of -omics data, joint analysis of molecular-level genomics data with epidemiological data through mediation analysis is becoming more common. However, such joint analysis requires methods that can simultaneously accommodate high-dimensional mediators and that are currently lacking. To address this problem, we develop a Bayesian inference method using continuous shrinkage priors to extend previous causal mediation analysis techniques to a high-dimensional setting. Simulations demonstrate that our method improves the power of global mediation analysis compared to simpler alternatives and has decent performance to identify true nonnull contributions to the mediation effects of the pathway. The Bayesian method also helps us to understand the structure of the composite null cases for inactive mediators in the pathway. We applied our method to Multi-Ethnic Study of Atherosclerosis and identified DNA methylation regions that may actively mediate the effect of socioeconomic status on cardiometabolic outcomes.},
  archive      = {J_BIOMTC},
  author       = {Yanyi Song and Xiang Zhou and Min Zhang and Wei Zhao and Yongmei Liu and Sharon L. R. Kardia and Ana V. Diez Roux and Belinda L. Needham and Jennifer A. Smith and Bhramar Mukherjee},
  doi          = {10.1111/biom.13189},
  journal      = {Biometrics},
  number       = {3},
  pages        = {700-710},
  shortjournal = {Biometrics},
  title        = {Bayesian shrinkage estimation of high dimensional causal mediation effects in omics studies},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Rejoinder for discussion on “horseshoe-based bayesian
nonparametric estimation of effective population size trajectories.”
<em>BIOMTC</em>, <em>76</em>(3), 695–699. (<a
href="https://doi.org/10.1111/biom.13273">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {James R. Faulkner and Andrew F. Magee and Beth Shapiro and Vladimir N. Minin},
  doi          = {10.1111/biom.13273},
  journal      = {Biometrics},
  number       = {3},
  pages        = {695-699},
  shortjournal = {Biometrics},
  title        = {Rejoinder for discussion on “Horseshoe-based bayesian nonparametric estimation of effective population size trajectories”},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discussion on “horseshoe-based bayesian nonparametric
estimation of effective population size trajectories” by james r.
Faulkner, andrew f. Magee, beth shapiro, and vladimir n. minin.
<em>BIOMTC</em>, <em>76</em>(3), 691–694. (<a
href="https://doi.org/10.1111/biom.13275">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Lorenzo Cappello and Swarnadip Ghosh and Julia A. Palacios},
  doi          = {10.1111/biom.13275},
  journal      = {Biometrics},
  number       = {3},
  pages        = {691-694},
  shortjournal = {Biometrics},
  title        = {Discussion on “Horseshoe-based bayesian nonparametric estimation of effective population size trajectories” by james r. faulkner, andrew f. magee, beth shapiro, and vladimir n. minin},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Horseshoe-based bayesian nonparametric estimation of
effective population size trajectories. <em>BIOMTC</em>, <em>76</em>(3),
677–690. (<a href="https://doi.org/10.1111/biom.13276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phylodynamics is an area of population genetics that uses genetic sequence data to estimate past population dynamics. Modern state-of-the-art Bayesian nonparametric methods for recovering population size trajectories of unknown form use either change-point models or Gaussian process priors. Change-point models suffer from computational issues when the number of change-points is unknown and needs to be estimated. Gaussian process-based methods lack local adaptivity and cannot accurately recover trajectories that exhibit features such as abrupt changes in trend or varying levels of smoothness. We propose a novel, locally adaptive approach to Bayesian nonparametric phylodynamic inference that has the flexibility to accommodate a large class of functional behaviors. Local adaptivity results from modeling the log-transformed effective population size a priori as a horseshoe Markov random field, a recently proposed statistical model that blends together the best properties of the change-point and Gaussian process modeling paradigms. We use simulated data to assess model performance, and find that our proposed method results in reduced bias and increased precision when compared to contemporary methods. We also use our models to reconstruct past changes in genetic diversity of human hepatitis C virus in Egypt and to estimate population size changes of ancient and modern steppe bison. These analyses show that our new method captures features of the population size trajectories that were missed by the state-of-the-art methods.},
  archive      = {J_BIOMTC},
  author       = {James R. Faulkner and Andrew F. Magee and Beth Shapiro and Vladimir N. Minin},
  doi          = {10.1111/biom.13276},
  journal      = {Biometrics},
  number       = {3},
  pages        = {677-690},
  shortjournal = {Biometrics},
  title        = {Horseshoe-based bayesian nonparametric estimation of effective population size trajectories},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). <em>BIOMTC</em>, <em>76</em>(2), 671–672. (<a
href="https://doi.org/10.1111/biom.13293">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Becky Tang and Amy H. Herring},
  doi          = {10.1111/biom.13293},
  journal      = {Biometrics},
  number       = {2},
  pages        = {671-672},
  shortjournal = {Biometrics},
  title        = {Bayesian statistical methods, brian j. reich sujit k. ghosh, boca raton, FL: Chapman and Hall/CRC, 2019, hard cover. pp. 288. US$ 79.96.},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Correction to “doubly-robust dynamic treatment regimen
estimation via weighted least squares,” by michael p. Wallace and erica
e. M. Moodie; 71 (3), 636–644, september 2015. <em>BIOMTC</em>,
<em>76</em>(2), 670. (<a
href="https://doi.org/10.1111/biom.13283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  doi          = {10.1111/biom.13283},
  journal      = {Biometrics},
  number       = {2},
  pages        = {670},
  shortjournal = {Biometrics},
  title        = {Correction to “Doubly-robust dynamic treatment regimen estimation via weighted least squares,” by michael p. wallace and erica e. m. moodie; 71 (3), 636–644, september 2015},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sharp bounds on the relative treatment effect for ordinal
outcomes. <em>BIOMTC</em>, <em>76</em>(2), 664–669. (<a
href="https://doi.org/10.1111/biom.13148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For ordinal outcomes, the average treatment effect is often ill-defined and hard to interpret. Echoing Agresti and Kateri, we argue that the relative treatment effect can be a useful measure, especially for ordinal outcomes, which is defined as γ = pr { Y i ( 1 ) &gt; Y i ( 0 ) } − pr { Y i ( 1 ) &lt; Y i ( 0 ) } , with Y i ( 1 ) and Y i ( 0 ) being the potential outcomes of unit i under treatment and control, respectively. Given the marginal distributions of the potential outcomes, we derive the sharp bounds on γ , which are identifiable parameters based on the observed data. Agresti and Kateri focused on modeling strategies under the assumption of independent potential outcomes, but we allow for arbitrary dependence.},
  archive      = {J_BIOMTC},
  author       = {Jiannan Lu and Yunshu Zhang and Peng Ding},
  doi          = {10.1111/biom.13148},
  journal      = {Biometrics},
  number       = {2},
  pages        = {664-669},
  shortjournal = {Biometrics},
  title        = {Sharp bounds on the relative treatment effect for ordinal outcomes},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The profile inter-unit reliability. <em>BIOMTC</em>,
<em>76</em>(2), 654–663. (<a
href="https://doi.org/10.1111/biom.13167">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To assess the quality of health care, patient outcomes associated with medical providers (eg, dialysis facilities) are routinely monitored in order to identify poor (or excellent) provider performance. Given the high stakes of such evaluations for payment as well as public reporting of quality, it is important to assess the reliability of quality measures. A commonly used metric is the inter-unit reliability (IUR), which is the proportion of variation in the measure that comes from inter-provider differences. Despite its wide use, however, the size of the IUR has little to do with the usefulness of the measure for profiling extreme outcomes. A large IUR can signal the need for further risk adjustment to account for differences between patients treated by different providers, while even measures with an IUR close to zero can be useful for identifying extreme providers. To address these limitations, we propose an alternative measure of reliability, which assesses more directly the value of a quality measure in identifying (or profiling) providers with extreme outcomes. The resulting metric reflects the extent to which the profiling status is consistent over repeated measurements. We use national dialysis data to examine this approach on various measures of dialysis facilities.},
  archive      = {J_BIOMTC},
  author       = {Kevin He and Claudia Dahlerus and Lu Xia and Yanming Li and John D. Kalbfleisch},
  doi          = {10.1111/biom.13167},
  journal      = {Biometrics},
  number       = {2},
  pages        = {654-663},
  shortjournal = {Biometrics},
  title        = {The profile inter-unit reliability},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Designing precision medicine trials to yield a greater
population impact. <em>BIOMTC</em>, <em>76</em>(2), 643–653. (<a
href="https://doi.org/10.1111/biom.13161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditionally, a clinical trial is conducted comparing treatment to standard care for all patients. However, it could be inefficient given patients’ heterogeneous responses to treatments, and rapid advances in the molecular understanding of diseases have made biomarker-based clinical trials increasingly popular. We propose a new targeted clinical trial design, termed as Max-Impact design, which selects the appropriate subpopulation for a clinical trial and aims to optimize population impact once the trial is completed. The proposed design not only gains insights on the patients who would be included in the trial but also considers the benefit to the excluded patients. We develop novel algorithms to construct enrollment rules for optimizing population impact, which are fairly general and can be applied to various types of outcomes. Simulation studies and a data example from the SWOG Cancer Research Network demonstrate the competitive performance of our proposed method compared to traditional untargeted and targeted designs.},
  archive      = {J_BIOMTC},
  author       = {Ying-Qi Zhao and Michael L. LeBlanc},
  doi          = {10.1111/biom.13161},
  journal      = {Biometrics},
  number       = {2},
  pages        = {643-653},
  shortjournal = {Biometrics},
  title        = {Designing precision medicine trials to yield a greater population impact},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian design of biosimilars clinical programs involving
multiple therapeutic indications. <em>BIOMTC</em>, <em>76</em>(2),
630–642. (<a href="https://doi.org/10.1111/biom.13163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a Bayesian design framework for a biosimilars clinical program that entails conducting concurrent trials in multiple therapeutic indications to establish equivalent efficacy for a proposed biologic compared to a reference biologic in each indication to support approval of the proposed biologic as a biosimilar. Our method facilitates information borrowing across indications through the use of a multivariate normal correlated parameter prior (CPP), which is constructed from easily interpretable hyperparameters that represent direct statements about the equivalence hypotheses to be tested. The CPP accommodates different endpoints and data types across indications (eg, binary and continuous) and can, therefore, be used in a wide context of models without having to modify the data (eg, rescaling) to provide reasonable information-borrowing properties. We illustrate how one can evaluate the design using Bayesian versions of the type I error rate and power with the objective of determining the sample size required for each indication such that the design has high power to demonstrate equivalent efficacy in each indication, reasonably high power to demonstrate equivalent efficacy simultaneously in all indications (ie, globally), and reasonable type I error control from a Bayesian perspective. We illustrate the method with several examples, including designing biosimilars trials for follicular lymphoma and rheumatoid arthritis using binary and continuous endpoints, respectively.},
  archive      = {J_BIOMTC},
  author       = {Matthew A. Psioda and Kuolung Hu and Yang Zhang and Jean Pan and Joseph G. Ibrahim},
  doi          = {10.1111/biom.13163},
  journal      = {Biometrics},
  number       = {2},
  pages        = {630-642},
  shortjournal = {Biometrics},
  title        = {Bayesian design of biosimilars clinical programs involving multiple therapeutic indications},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Gene-based association analysis for bivariate time-to-event
data through functional regression with copula models. <em>BIOMTC</em>,
<em>76</em>(2), 619–629. (<a
href="https://doi.org/10.1111/biom.13165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several gene-based association tests for time-to-event traits have been proposed recently to detect whether a gene region (containing multiple variants), as a set, is associated with the survival outcome. However, for bivariate survival outcomes, to the best of our knowledge, there is no statistical method that can be directly applied for gene-based association analysis. Motivated by a genetic study to discover the gene regions associated with the progression of a bilateral eye disease, age-related macular degeneration (AMD), we implement a novel functional regression (FR) method under the copula framework. Specifically, the effects of variants within a gene region are modeled through a functional linear model, which then contributes to the marginal survival functions within the copula. Generalized score test statistics are derived to test for the association between bivariate survival traits and the genetic region. Extensive simulation studies are conducted to evaluate the type I error control and power performance of the proposed approach, with comparisons to several existing methods for a single survival trait, as well as the marginal Cox FR model using the robust sandwich estimator for bivariate survival traits. Finally, we apply our method to a large AMD study, the Age-related Eye Disease Study, and to identify the gene regions that are associated with AMD progression.},
  archive      = {J_BIOMTC},
  author       = {Yue Wei and Yi Liu and Tao Sun and Wei Chen and Ying Ding},
  doi          = {10.1111/biom.13165},
  journal      = {Biometrics},
  number       = {2},
  pages        = {619-629},
  shortjournal = {Biometrics},
  title        = {Gene-based association analysis for bivariate time-to-event data through functional regression with copula models},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysis of secondary phenotypes in multigroup association
studies. <em>BIOMTC</em>, <em>76</em>(2), 606–618. (<a
href="https://doi.org/10.1111/biom.13157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although case-control association studies have been widely used, they are insufficient for many complex diseases, such as Alzheimer&#39;s disease and breast cancer, since these diseases may have multiple subtypes with distinct morphologies and clinical implications. Many multigroup studies, such as the Alzheimer&#39;s Disease Neuroimaging Initiative (ADNI), have been undertaken by recruiting subjects based on their multiclass primary disease status, while extensive secondary outcomes have been collected. The aim of this paper is to develop a general regression framework for the analysis of secondary phenotypes collected in multigroup association studies. Our regression framework is built on a conditional model for the secondary outcome given the multigroup status and covariates and its relationship with the population regression of interest of the secondary outcome given the covariates. Then, we develop generalized estimation equations to estimate the parameters of interest. We use both simulations and a large-scale imaging genetic data analysis from the ADNI to evaluate the effect of the multigroup sampling scheme on standard genome-wide association analyses based on linear regression methods, while comparing it with our statistical methods that appropriately adjust for the multigroup sampling scheme. Data used in preparation of this article were obtained from the ADNI database.},
  archive      = {J_BIOMTC},
  author       = {Fan Zhou and Haibo Zhou and Tengfei Li and Hongtu Zhu},
  doi          = {10.1111/biom.13157},
  journal      = {Biometrics},
  number       = {2},
  pages        = {606-618},
  shortjournal = {Biometrics},
  title        = {Analysis of secondary phenotypes in multigroup association studies},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Rejoinder to “predictively consistent prior effective
sample sizes,” by beat neuenschwander, sebastian weber, heinz schmidli,
and anthony o’hagan. <em>BIOMTC</em>, <em>76</em>(2), 602–605. (<a
href="https://doi.org/10.1111/biom.13245">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Beat Neuenschwander and Sebastian Weber and Heinz Schmidli and Anthony O&#39;Hagan},
  doi          = {10.1111/biom.13245},
  journal      = {Biometrics},
  number       = {2},
  pages        = {602-605},
  shortjournal = {Biometrics},
  title        = {Rejoinder to “Predictively consistent prior effective sample sizes,” by beat neuenschwander, sebastian weber, heinz schmidli, and anthony O&#39;Hagan},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discussion on “predictively consistent prior effective
sample sizes,” by beat neuenschwander, sebastian weber, heinz schmidli,
and anthony o’hagan. <em>BIOMTC</em>, <em>76</em>(2), 599–601. (<a
href="https://doi.org/10.1111/biom.13254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neuenschwander et al . address a seemingly easy but often complicated problem in applied Bayesian methodology. We discuss some issues that relate to the question of why one might care about the effective sample size ( E S S ) in a Bayesian model and the motivation for reporting the E S S .},
  archive      = {J_BIOMTC},
  author       = {Gary L. Rosner and Peter Müller},
  doi          = {10.1111/biom.13254},
  journal      = {Biometrics},
  number       = {2},
  pages        = {599-601},
  shortjournal = {Biometrics},
  title        = {Discussion on “Predictively consistent prior effective sample sizes,” by beat neuenschwander, sebastian weber, heinz schmidli, and anthony O&#39;Hagan},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discussion on “predictively consistent prior effective
sample sizes,” by beat neuenschwander, sebastian weber, heinz schmidli,
and anthony o’hagan. <em>BIOMTC</em>, <em>76</em>(2), 595–598. (<a
href="https://doi.org/10.1111/biom.13246">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Yeonhee Park and Ruitao Lin},
  doi          = {10.1111/biom.13246},
  journal      = {Biometrics},
  number       = {2},
  pages        = {595-598},
  shortjournal = {Biometrics},
  title        = {Discussion on “Predictively consistent prior effective sample sizes,” by beat neuenschwander, sebastian weber, heinz schmidli, and anthony O&#39;Hagan},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discussion on “predictively consistent prior effective
sample sizes,” by beat neuenschwander, sebastian weber, heinz schmidli,
and anthony o’hagan. <em>BIOMTC</em>, <em>76</em>(2), 591–594. (<a
href="https://doi.org/10.1111/biom.13247">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We extend the approach of finding effective sample size for a typical phase II clinical trial having efficacy and toxicity as two components of the response vector. The case of binary efficacy and binary toxicity is illustrated under Dirichlet and multivariate T priors.},
  archive      = {J_BIOMTC},
  author       = {Atanu Biswas and Jean-François Angers},
  doi          = {10.1111/biom.13247},
  journal      = {Biometrics},
  number       = {2},
  pages        = {591-594},
  shortjournal = {Biometrics},
  title        = {Discussion on “Predictively consistent prior effective sample sizes,” by beat neuenschwander, sebastian weber, heinz schmidli, and anthony O&#39;Hagan},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discussion on “predictively consistent prior effective
sample sizes” by beat neuenschwander, sebastian weber, heinz schmidli,
and anthony o’hagan. <em>BIOMTC</em>, <em>76</em>(2), 588–590. (<a
href="https://doi.org/10.1111/biom.13253">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Alexander Kaizer and John Kittelson},
  doi          = {10.1111/biom.13253},
  journal      = {Biometrics},
  number       = {2},
  pages        = {588-590},
  shortjournal = {Biometrics},
  title        = {Discussion on “Predictively consistent prior effective sample sizes” by beat neuenschwander, sebastian weber, heinz schmidli, and anthony O&#39;Hagan},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Predictively consistent prior effective sample sizes.
<em>BIOMTC</em>, <em>76</em>(2), 578–587. (<a
href="https://doi.org/10.1111/biom.13252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Determining the sample size of an experiment can be challenging, even more so when incorporating external information via a prior distribution. Such information is increasingly used to reduce the size of the control group in randomized clinical trials. Knowing the amount of prior information, expressed as an equivalent prior effective sample size (ESS) , clearly facilitates trial designs. Various methods to obtain a prior&#39;s ESS have been proposed recently. They have been justified by the fact that they give the standard ESS for one-parameter exponential families. However, despite being based on similar information-based metrics, they may lead to surprisingly different ESS for nonconjugate settings, which complicates many designs with prior information. We show that current methods fail a basic predictive consistency criterion, which requires the expected posterior-predictive ESS for a sample of size N to be the sum of the prior ESS and N . The expected local-information-ratio ESS is introduced and shown to be predictively consistent. It corrects the ESS of current methods, as shown for normally distributed data with a heavy-tailed Student-t prior and exponential data with a generalized Gamma prior. Finally, two applications are discussed: the prior ESS for the control group derived from historical data and the posterior ESS for hierarchical subgroup analyses.},
  archive      = {J_BIOMTC},
  author       = {Beat Neuenschwander and Sebastian Weber and Heinz Schmidli and Anthony O&#39;Hagan},
  doi          = {10.1111/biom.13252},
  journal      = {Biometrics},
  number       = {2},
  pages        = {578-587},
  shortjournal = {Biometrics},
  title        = {Predictively consistent prior effective sample sizes},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Rejoinder to “assessing the goodness of fit of logistic
regression models in large samples: A modification of the
hosmer-lemeshow test.” <em>BIOMTC</em>, <em>76</em>(2), 575–577. (<a
href="https://doi.org/10.1111/biom.13250">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Giovanni Nattino and Michael L. Pennell and Stanley Lemeshow},
  doi          = {10.1111/biom.13250},
  journal      = {Biometrics},
  number       = {2},
  pages        = {575-577},
  shortjournal = {Biometrics},
  title        = {Rejoinder to “Assessing the goodness of fit of logistic regression models in large samples: A modification of the hosmer-lemeshow test”},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discussion on “assessing the goodness of fit of logistic
regression models in large samples: A modification of the
hosmer-lemeshow test” by giovanni nattino, michael l. Pennell, and
stanley lemeshow. <em>BIOMTC</em>, <em>76</em>(2), 572–574. (<a
href="https://doi.org/10.1111/biom.13248">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Dandan Liu and Bryan E. Shepherd},
  doi          = {10.1111/biom.13248},
  journal      = {Biometrics},
  number       = {2},
  pages        = {572-574},
  shortjournal = {Biometrics},
  title        = {Discussion on “Assessing the goodness of fit of logistic regression models in large samples: A modification of the hosmer-lemeshow test” by giovanni nattino, michael l. pennell, and stanley lemeshow},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discussion of “assessing the goodness-of-fit of logistic
regression models in large samples: A modification of the
hosmer-lemeshow test,” by giovanni nattino, michael l. Pennell, and
stanley lemeshow. <em>BIOMTC</em>, <em>76</em>(2), 569–571. (<a
href="https://doi.org/10.1111/biom.13255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Li-Ching Chen and Jiun-Yi Wang},
  doi          = {10.1111/biom.13255},
  journal      = {Biometrics},
  number       = {2},
  pages        = {569-571},
  shortjournal = {Biometrics},
  title        = {Discussion of “Assessing the goodness-of-fit of logistic regression models in large samples: A modification of the hosmer-lemeshow test,” by giovanni nattino, michael l. pennell, and stanley lemeshow},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discussion on “assessing the goodness of fit of logistic
regression models in large samples: A modification of the
hosmer-lemeshow test” by giovanni nattino, michael l. Pennell, and
stanley lemeshow. <em>BIOMTC</em>, <em>76</em>(2), 564–568. (<a
href="https://doi.org/10.1111/biom.13251">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Ivy Liu and Daniel Fernández},
  doi          = {10.1111/biom.13251},
  journal      = {Biometrics},
  number       = {2},
  pages        = {564-568},
  shortjournal = {Biometrics},
  title        = {Discussion on “Assessing the goodness of fit of logistic regression models in large samples: A modification of the hosmer-lemeshow test” by giovanni nattino, michael l. pennell, and stanley lemeshow},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discussion on “assessing the goodness of fit of logistic
regression models in large samples: A modification of the
hosmer-lemeshow test” by giovanni nattino, michael l. Pennell, and
stanley lemeshow. <em>BIOMTC</em>, <em>76</em>(2), 561–563. (<a
href="https://doi.org/10.1111/biom.13257">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Galit Shmueli},
  doi          = {10.1111/biom.13257},
  journal      = {Biometrics},
  number       = {2},
  pages        = {561-563},
  shortjournal = {Biometrics},
  title        = {Discussion on “Assessing the goodness of fit of logistic regression models in large samples: A modification of the hosmer-lemeshow test” by giovanni nattino, michael l. pennell, and stanley lemeshow},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Assessing the goodness of fit of logistic regression models
in large samples: A modification of the hosmer-lemeshow test.
<em>BIOMTC</em>, <em>76</em>(2), 549–560. (<a
href="https://doi.org/10.1111/biom.13249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Evaluating the goodness of fit of logistic regression models is crucial to ensure the accuracy of the estimated probabilities. Unfortunately, such evaluation is problematic in large samples. Because the power of traditional goodness of fit tests increases with the sample size, practically irrelevant discrepancies between estimated and true probabilities are increasingly likely to cause the rejection of the hypothesis of perfect fit in larger and larger samples. This phenomenon has been widely documented for popular goodness of fit tests, such as the Hosmer-Lemeshow test. To address this limitation, we propose a modification of the Hosmer-Lemeshow approach. By standardizing the noncentrality parameter that characterizes the alternative distribution of the Hosmer-Lemeshow statistic, we introduce a parameter that measures the goodness of fit of a model but does not depend on the sample size. We provide the methodology to estimate this parameter and construct confidence intervals for it. Finally, we propose a formal statistical test to rigorously assess whether the fit of a model, albeit not perfect, is acceptable for practical purposes. The proposed method is compared in a simulation study with a competing modification of the Hosmer-Lemeshow test, based on repeated subsampling. We provide a step-by-step illustration of our method using a model for postneonatal mortality developed in a large cohort of more than 300 000 observations.},
  archive      = {J_BIOMTC},
  author       = {Giovanni Nattino and Michael L. Pennell and Stanley Lemeshow},
  doi          = {10.1111/biom.13249},
  journal      = {Biometrics},
  number       = {2},
  pages        = {549-560},
  shortjournal = {Biometrics},
  title        = {Assessing the goodness of fit of logistic regression models in large samples: A modification of the hosmer-lemeshow test},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multinomial n-mixture models for removal sampling.
<em>BIOMTC</em>, <em>76</em>(2), 540–548. (<a
href="https://doi.org/10.1111/biom.13147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multinomial N -mixture models are commonly used to fit data from a removal sampling protocol. If the mixing distribution is negative binomial, the distribution of the counts does not appear to have been identified, and practitioners approximate the requisite likelihood by placing an upper bound on the embedded infinite sum. In this paper, the distribution which underpins the multinomial N -mixture model with a negative binomial mixing distribution is shown to belong to the broad class of multivariate negative binomial distributions. Specifically, the likelihood can be expressed in closed form as the product of conditional and marginal likelihoods and the information matrix shown to be block diagonal. As a consequence, the nature of the maximum likelihood estimates of the unknown parameters and their attendant standard errors can be examined and tests of the hypothesis of the Poisson against the negative binomial mixing distribution formulated. In addition, appropriate multinomial N -mixture models for data sets which include zero site totals can also be constructed. Two illustrative examples are provided.},
  archive      = {J_BIOMTC},
  author       = {Linda M. Haines},
  doi          = {10.1111/biom.13147},
  journal      = {Biometrics},
  number       = {2},
  pages        = {540-548},
  shortjournal = {Biometrics},
  title        = {Multinomial N-mixture models for removal sampling},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bias correction of bounded location error in binary data.
<em>BIOMTC</em>, <em>76</em>(2), 530–539. (<a
href="https://doi.org/10.1111/biom.13152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Binary regression models for spatial data are commonly used in disciplines such as epidemiology and ecology. Many spatially referenced binary data sets suffer from location error, which occurs when the recorded location of an observation differs from its true location. When location error occurs, values of the covariates associated with the true spatial locations of the observations cannot be obtained. We show how a change of support (COS) can be applied to regression models for binary data to provide coefficient estimates when the true values of the covariates are unavailable, but the unknown location of the observations are contained within nonoverlapping arbitrarily shaped polygons. The COS accommodates spatial and nonspatial covariates and preserves the convenient interpretation of methods such as logistic and probit regression. Using a simulation experiment, we compare binary regression models with a COS to naive approaches that ignore location error. We illustrate the flexibility of the COS by modeling individual-level disease risk in a population using a binary data set where the locations of the observations are unknown but contained within administrative units. Our simulation experiment and data illustration corroborate that conventional regression models for binary data that ignore location error are unreliable, but that the COS can be used to eliminate bias while preserving model choice.},
  archive      = {J_BIOMTC},
  author       = {Nelson B. Walker and Trevor J. Hefley and Daniel P. Walsh},
  doi          = {10.1111/biom.13152},
  journal      = {Biometrics},
  number       = {2},
  pages        = {530-539},
  shortjournal = {Biometrics},
  title        = {Bias correction of bounded location error in binary data},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Equivalence of regression curves sharing common parameters.
<em>BIOMTC</em>, <em>76</em>(2), 518–529. (<a
href="https://doi.org/10.1111/biom.13149">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In clinical trials, the comparison of two different populations is a common problem. Nonlinear (parametric) regression models are commonly used to describe the relationship between covariates, such as concentration or dose, and a response variable in the two groups. In some situations, it is reasonable to assume some model parameters to be the same, for instance, the placebo effect or the maximum treatment effect. In this paper, we develop a (parametric) bootstrap test to establish the similarity of two regression curves sharing some common parameters. We show by theoretical arguments and by means of a simulation study that the new test controls its significance level and achieves a reasonable power. Moreover, it is demonstrated that under the assumption of common parameters, a considerably more powerful test can be constructed compared with the test that does not use this assumption. Finally, we illustrate the potential applications of the new methodology by a clinical trial example.},
  archive      = {J_BIOMTC},
  author       = {Kathrin Möllenhoff and Frank Bretz and Holger Dette},
  doi          = {10.1111/biom.13149},
  journal      = {Biometrics},
  number       = {2},
  pages        = {518-529},
  shortjournal = {Biometrics},
  title        = {Equivalence of regression curves sharing common parameters},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data reduction prior to inference: Are there consequences of
comparing groups using a t-test based on principal component scores?
<em>BIOMTC</em>, <em>76</em>(2), 508–517. (<a
href="https://doi.org/10.1111/biom.13159">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers often use a two-step process to analyze multivariate data. First, dimensionality is reduced using a technique such as principal component analysis, followed by a group comparison using a t -test or analysis of variance. Although this practice is often discouraged, the statistical properties of this procedure are not well understood, starting with the hypothesis being tested. We suggest that this approach might be considering two distinct hypotheses, one of which is a global test of no differences in the mean vectors, and the other being a focused test of a specific linear combination where the coefficients have been estimated from the data. We study the asymptotic properties of the two-sample t -statistic for these two scenarios, assuming a nonsparse setting. We show that the size of the global test agrees with the presumed level but that the test has poor power. In contrast, the size of the focused test can be arbitrarily distorted with certain mean and covariance structures. A simple method is provided to correct the size of the focused test. Data analyses and simulations are used to illustrate the results. Recommendations on the use of this two-step method and the related use of principal components for prediction are provided.},
  archive      = {J_BIOMTC},
  author       = {Edward J. Bedrick},
  doi          = {10.1111/biom.13159},
  journal      = {Biometrics},
  number       = {2},
  pages        = {508-517},
  shortjournal = {Biometrics},
  title        = {Data reduction prior to inference: Are there consequences of comparing groups using a t-test based on principal component scores?},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sequential adaptive variables and subject selection for GEE
methods. <em>BIOMTC</em>, <em>76</em>(2), 496–507. (<a
href="https://doi.org/10.1111/biom.13160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modeling correlated or highly stratified multiple-response data is a common data analysis task in many applications, such as those in large epidemiological studies or multisite cohort studies. The generalized estimating equations method is a popular statistical method used to analyze these kinds of data, because it can manage many types of unmeasured dependence among outcomes. Collecting large amounts of highly stratified or correlated response data is time-consuming; thus, the use of a more aggressive sampling strategy that can accelerate this process—such as the active-learning methods found in the machine-learning literature—will always be beneficial. In this study, we integrate adaptive sampling and variable selection features into a sequential procedure for modeling correlated response data. Besides reporting the statistical properties of the proposed procedure, we also use both synthesized and real data sets to demonstrate the usefulness of our method.},
  archive      = {J_BIOMTC},
  author       = {Zimu Chen and Zhanfeng Wang and Yuan-chin Ivan Chang},
  doi          = {10.1111/biom.13160},
  journal      = {Biometrics},
  number       = {2},
  pages        = {496-507},
  shortjournal = {Biometrics},
  title        = {Sequential adaptive variables and subject selection for GEE methods},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inverse probability weighting methods for cox regression
with right-truncated data. <em>BIOMTC</em>, <em>76</em>(2), 484–495. (<a
href="https://doi.org/10.1111/biom.13162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Right-truncated data arise when observations are ascertained retrospectively, and only subjects who experience the event of interest by the time of sampling are selected. Such a selection scheme, without adjustment, leads to biased estimation of covariate effects in the Cox proportional hazards model. The existing methods for fitting the Cox model to right-truncated data, which are based on the maximization of the likelihood or solving estimating equations with respect to both the baseline hazard function and the covariate effects, are numerically challenging. We consider two alternative simple methods based on inverse probability weighting (IPW) estimating equations, which allow consistent estimation of covariate effects under a positivity assumption and avoid estimation of baseline hazards. We discuss problems of identifiability and consistency that arise when positivity does not hold and show that although the partial tests for null effects based on these IPW methods can be used in some settings even in the absence of positivity, they are not valid in general. We propose adjusted estimating equations that incorporate the probability of observation when it is known from external sources, which results in consistent estimation. We compare the methods in simulations and apply them to the analyses of human immunodeficiency virus latency.},
  archive      = {J_BIOMTC},
  author       = {Bella Vakulenko-Lagun and Micha Mandel and Rebecca A. Betensky},
  doi          = {10.1111/biom.13162},
  journal      = {Biometrics},
  number       = {2},
  pages        = {484-495},
  shortjournal = {Biometrics},
  title        = {Inverse probability weighting methods for cox regression with right-truncated data},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adjusting for time-varying confounders in survival analysis
using structural nested cumulative survival time models.
<em>BIOMTC</em>, <em>76</em>(2), 472–483. (<a
href="https://doi.org/10.1111/biom.13158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accounting for time-varying confounding when assessing the causal effects of time-varying exposures on survival time is challenging. Standard survival methods that incorporate time-varying confounders as covariates generally yield biased effect estimates. Estimators using weighting by inverse probability of exposure can be unstable when confounders are highly predictive of exposure or the exposure is continuous. Structural nested accelerated failure time models (AFTMs) require artificial recensoring, which can cause estimation difficulties. Here, we introduce the structural nested cumulative survival time model (SNCSTM). This model assumes that intervening to set exposure at time t to zero has an additive effect on the subsequent conditional hazard given exposure and confounder histories when all subsequent exposures have already been set to zero. We show how to fit it using standard software for generalized linear models and describe two more efficient, double robust, closed-form estimators. All three estimators avoid the artificial recensoring of AFTMs and the instability of estimators that use weighting by the inverse probability of exposure. We examine the performance of our estimators using a simulation study and illustrate their use on data from the UK Cystic Fibrosis Registry. The SNCSTM is compared with a recently proposed structural nested cumulative failure time model, and several advantages of the former are identified.},
  archive      = {J_BIOMTC},
  author       = {Shaun Seaman and Oliver Dukes and Ruth Keogh and Stijn Vansteelandt},
  doi          = {10.1111/biom.13158},
  journal      = {Biometrics},
  number       = {2},
  pages        = {472-483},
  shortjournal = {Biometrics},
  title        = {Adjusting for time-varying confounders in survival analysis using structural nested cumulative survival time models},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cox regression with survival-time-dependent missing
covariate values. <em>BIOMTC</em>, <em>76</em>(2), 460–471. (<a
href="https://doi.org/10.1111/biom.13155">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Analysis with time-to-event data in clinical and epidemiological studies often encounters missing covariate values, and the missing at random assumption is commonly adopted, which assumes that missingness depends on the observed data, including the observed outcome which is the minimum of survival and censoring time. However, it is conceivable that in certain settings, missingness of covariate values is related to the survival time but not to the censoring time. This is especially so when covariate missingness is related to an unmeasured variable affected by the patient&#39;s illness and prognosis factors at baseline. If this is the case, then the covariate missingness is not at random as the survival time is censored, and it creates a challenge in data analysis. In this article, we propose an approach to deal with such survival-time-dependent covariate missingness based on the well known Cox proportional hazard model. Our method is based on inverse propensity weighting with the propensity estimated by nonparametric kernel regression. Our estimators are consistent and asymptotically normal, and their finite-sample performance is examined through simulation. An application to a real-data example is included for illustration.},
  archive      = {J_BIOMTC},
  author       = {Yanyao Yi and Ting Ye and Menggang Yu and Jun Shao},
  doi          = {10.1111/biom.13155},
  journal      = {Biometrics},
  number       = {2},
  pages        = {460-471},
  shortjournal = {Biometrics},
  title        = {Cox regression with survival-time-dependent missing covariate values},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Penalized survival models for the analysis of alternating
recurrent event data. <em>BIOMTC</em>, <em>76</em>(2), 448–459. (<a
href="https://doi.org/10.1111/biom.13153">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recurrent event data are widely encountered in clinical and observational studies. Most methods for recurrent events treat the outcome as a point process and, as such, neglect any associated event duration. This generally leads to a less informative and potentially biased analysis. We propose a joint model for the recurrent event rate (of incidence) and duration. The two processes are linked through a bivariate normal frailty. For example, when the event is hospitalization, we can treat the time to admission and length-of-stay as two alternating recurrent events. In our method, the regression parameters are estimated through a penalized partial likelihood, and the variance-covariance matrix of the frailty is estimated through a recursive estimating formula. Moreover, we develop a likelihood ratio test to assess the dependence between the incidence and duration processes. Simulation results demonstrate that our method provides accurate parameter estimation, with a relatively fast computation time. We illustrate the methods through an analysis of hospitalizations among end-stage renal disease patients.},
  archive      = {J_BIOMTC},
  author       = {Lili Wang and Kevin He and Douglas E. Schaubel},
  doi          = {10.1111/biom.13153},
  journal      = {Biometrics},
  number       = {2},
  pages        = {448-459},
  shortjournal = {Biometrics},
  title        = {Penalized survival models for the analysis of alternating recurrent event data},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inference in MCMC step selection models. <em>BIOMTC</em>,
<em>76</em>(2), 438–447. (<a
href="https://doi.org/10.1111/biom.13170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Habitat selection models are used in ecology to link the spatial distribution of animals to environmental covariates and identify preferred habitats. The most widely used models of this type, resource selection functions, aim to capture the steady-state distribution of space use of the animal, but they assume independence between the observed locations of an animal. This is unrealistic when location data display temporal autocorrelation. The alternative approach of step selection functions embed habitat selection in a model of animal movement, to account for the autocorrelation. However, inferences from step selection functions depend on the underlying movement model, and they do not readily predict steady-state space use. We suggest an analogy between parameter updates and target distributions in Markov chain Monte Carlo (MCMC) algorithms, and step selection and steady-state distributions in movement ecology, leading to a step selection model with an explicit steady-state distribution. In this framework, we explain how maximum likelihood estimation can be used for simultaneous inference about movement and habitat selection. We describe the local Gibbs sampler, a novel rejection-free MCMC scheme, use it as the basis of a flexible class of animal movement models, and derive its likelihood function for several important special cases. In a simulation study, we verify that maximum likelihood estimation can recover all model parameters. We illustrate the application of the method with data from a zebra.},
  archive      = {J_BIOMTC},
  author       = {Théo Michelot and Paul G. Blackwell and Simon Chamaillé-Jammes and Jason Matthiopoulos},
  doi          = {10.1111/biom.13170},
  journal      = {Biometrics},
  number       = {2},
  pages        = {438-447},
  shortjournal = {Biometrics},
  title        = {Inference in MCMC step selection models},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simultaneous confidence corridors for mean functions in
functional data analysis of imaging data. <em>BIOMTC</em>,
<em>76</em>(2), 427–437. (<a
href="https://doi.org/10.1111/biom.13156">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by recent work involving the analysis of biomedical imaging data, we present a novel procedure for constructing simultaneous confidence corridors for the mean of imaging data. We propose to use flexible bivariate splines over triangulations to handle an irregular domain of the images that is common in brain imaging studies and in other biomedical imaging applications. The proposed spline estimators of the mean functions are shown to be consistent and asymptotically normal under some regularity conditions. We also provide a computationally efficient estimator of the covariance function and derive its uniform consistency. The procedure is also extended to the two-sample case in which we focus on comparing the mean functions from two populations of imaging data. Through Monte Carlo simulation studies, we examine the finite sample performance of the proposed method. Finally, the proposed method is applied to analyze brain positron emission tomography data in two different studies. One data set used in preparation of this article was obtained from the Alzheimer&#39;s Disease Neuroimaging Initiative (ADNI) database.},
  archive      = {J_BIOMTC},
  author       = {Yueying Wang and Guannan Wang and Li Wang and R. Todd Ogden},
  doi          = {10.1111/biom.13156},
  journal      = {Biometrics},
  number       = {2},
  pages        = {427-437},
  shortjournal = {Biometrics},
  title        = {Simultaneous confidence corridors for mean functions in functional data analysis of imaging data},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An adaptive independence test for microbiome community data.
<em>BIOMTC</em>, <em>76</em>(2), 414–426. (<a
href="https://doi.org/10.1111/biom.13154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in sequencing technologies and bioinformatics tools have vastly improved our ability to collect and analyze data from complex microbial communities. A major goal of microbiome studies is to correlate the overall microbiome composition with clinical or environmental variables. La Rosa et al . recently proposed a parametric test for comparing microbiome populations between two or more groups of subjects. However, this method is not applicable for testing the association between the community composition and a continuous variable. Although multivariate nonparametric methods based on permutations are widely used in ecology studies, they lack interpretability and can be inefficient for analyzing microbiome data. We consider the problem of testing for independence between the microbial community composition and a continuous or many-valued variable. By partitioning the range of the variable into a few slices, we formulate the problem as a problem of comparing multiple groups of microbiome samples, with each group indexed by a slice. To model multivariate and over-dispersed count data, we use the Dirichlet-multinomial distribution. We propose an adaptive likelihood-ratio test by learning a good partition or slicing scheme from the data. A dynamic programming algorithm is developed for numerical optimization. We demonstrate the superiority of the proposed test by numerically comparing it with that of La Rosa et al . and other popular approaches on the same topic including PERMANOVA, the distance covariance test, and the microbiome regression-based kernel association test. We further apply it to test the association of gut microbiome with age in three geographically distinct populations and show how the learned partition facilitates differential abundance analysis.},
  archive      = {J_BIOMTC},
  author       = {Yaru Song and Hongyu Zhao and Tao Wang},
  doi          = {10.1111/biom.13154},
  journal      = {Biometrics},
  number       = {2},
  pages        = {414-426},
  shortjournal = {Biometrics},
  title        = {An adaptive independence test for microbiome community data},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A heterogeneity measure for cluster identification with
application to disease mapping. <em>BIOMTC</em>, <em>76</em>(2),
403–413. (<a href="https://doi.org/10.1111/biom.13145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mapping of disease incidence has long been of importance to epidemiology and public health. In this paper, we consider identification of clusters of spatial units with elevated disease rates and develop a new approach that estimates the relative disease risk in association with potential risk factors and simultaneously identifies clusters corresponding to elevated risks. A heterogeneity measure is proposed to enable the comparison of a candidate cluster and its complement under a pair of complementary models. A quasi-likelihood procedure is developed for estimating the model parameters and identifying the clusters. An advantage of our approach over traditional spatial clustering methods is the identification of clusters that can have arbitrary shapes due to abrupt or noncontiguous changes while accounting for risk factors and spatial correlation. Asymptotic properties of the proposed methodology are established and a simulation study shows empirically sound finite-sample properties. The mapping and clustering of enterovirus 71 infections in Taiwan are carried out for illustration.},
  archive      = {J_BIOMTC},
  author       = {Pei-Sheng Lin and Jun Zhu},
  doi          = {10.1111/biom.13145},
  journal      = {Biometrics},
  number       = {2},
  pages        = {403-413},
  shortjournal = {Biometrics},
  title        = {A heterogeneity measure for cluster identification with application to disease mapping},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A spatial open-population capture-recapture model.
<em>BIOMTC</em>, <em>76</em>(2), 392–402. (<a
href="https://doi.org/10.1111/biom.13150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A spatial open-population capture-recapture model is described that extends both the non-spatial open-population model of Schwarz and Arnason and the spatially explicit closed-population model of Borchers and Efford. The superpopulation of animals available for detection at some time during a study is conceived as a two-dimensional Poisson point process. Individual probabilities of birth and death follow the conventional open-population model. Movement between sampling times may be modeled with a dispersal kernel using a recursive Markovian algorithm. Observations arise from distance-dependent sampling at an array of detectors. As in the closed-population spatial model, the observed data likelihood relies on integration over the unknown animal locations; maximization of this likelihood yields estimates of the birth, death, movement, and detection parameters. The models were fitted to data from a live-trapping study of brushtail possums ( Trichosurus vulpecula ) in New Zealand. Simulations confirmed that spatial modeling can greatly reduce the bias of capture-recapture survival estimates and that there is a degree of robustness to misspecification of the dispersal kernel. An R package is available that includes various extensions.},
  archive      = {J_BIOMTC},
  author       = {Murray G. Efford and Matthew R. Schofield},
  doi          = {10.1111/biom.13150},
  journal      = {Biometrics},
  number       = {2},
  pages        = {392-402},
  shortjournal = {Biometrics},
  title        = {A spatial open-population capture-recapture model},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On mendelian randomization analysis of case-control study.
<em>BIOMTC</em>, <em>76</em>(2), 380–391. (<a
href="https://doi.org/10.1111/biom.13166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mendelian randomization (MR) analysis uses genotypes as instruments to estimate the causal effect of an exposure in the presence of unobserved confounders. The existing MR methods focus on the data generated from prospective cohort studies. We develop a procedure for studying binary outcomes under a case-control design. The proposed procedure is built upon two working models commonly used for MR analyses and adopts a quasi-empirical likelihood framework to address the ascertainment bias from case-control sampling. We derive various approaches for estimating the causal effect and hypothesis testing under the empirical likelihood framework. We conduct extensive simulation studies to evaluate the proposed methods. We find that the proposed empirical likelihood estimate is less biased than the existing estimates. Among all the approaches considered, the Lagrange multiplier (LM) test has the highest power, and the confidence intervals derived from the LM test have the most accurate coverage. We illustrate the use of our method in MR analysis of prostate cancer case-control data with vitamin D level as exposure and three single nucleotide polymorphisms as instruments.},
  archive      = {J_BIOMTC},
  author       = {Han Zhang and Jing Qin and Sonja I. Berndt and Demetrius Albanes and Lu Deng and Mitchell H. Gail and Kai Yu},
  doi          = {10.1111/biom.13166},
  journal      = {Biometrics},
  number       = {2},
  pages        = {380-391},
  shortjournal = {Biometrics},
  title        = {On mendelian randomization analysis of case-control study},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Approximation of bias and mean-squared error in two-sample
mendelian randomization analyses. <em>BIOMTC</em>, <em>76</em>(2),
369–379. (<a href="https://doi.org/10.1111/biom.13169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mendelian randomization (MR) is a type of instrumental variable (IV) analysis that uses genetic variants as IVs for a risk factor to study its causal effect on an outcome. Extensive investigations on the performance of IV analysis procedures, such as the one based on the two-stage least squares (2SLS) procedure, have been conducted under the one-sample scenario, where measures on IVs, the risk factor, and the outcome are assumed to be available for each study participant. Recent MR analysis usually is performed with data from two independent or partially overlapping genetic association studies (two-sample setting), with one providing information on the association between the IVs and the outcome, and the other on the association between the IVs and the risk factor. We investigate the performance of 2SLS in the two-sample–based MR when the IVs are weakly associated with the risk factor. We derive closed form formulas for the bias and mean squared error of the 2SLS estimate and verify them with numeric simulations under realistic circumstances. Using these analytic formulas, we can study the pros and cons of conducting MR analysis under one-sample and two-sample settings and assess the impact of having overlapping samples. We also propose and validate a bias-corrected estimator for the causal effect.},
  archive      = {J_BIOMTC},
  author       = {Lu Deng and Han Zhang and Lei Song and Kai Yu},
  doi          = {10.1111/biom.13169},
  journal      = {Biometrics},
  number       = {2},
  pages        = {369-379},
  shortjournal = {Biometrics},
  title        = {Approximation of bias and mean-squared error in two-sample mendelian randomization analyses},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparametric models for longitudinal data: With
implementation in r, colin o. Wu, xin tian, boca raton, FL: CRC press.
<em>BIOMTC</em>, <em>76</em>(1), 363–364. (<a
href="https://doi.org/10.1111/biom.13223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Mohammed Chowdhury},
  doi          = {10.1111/biom.13223},
  journal      = {Biometrics},
  number       = {1},
  pages        = {363-364},
  shortjournal = {Biometrics},
  title        = {Nonparametric models for longitudinal data: with implementation in r, colin o. wu, xin tian, boca raton, FL: CRC press},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spatial data analysis in ecology and agriculture using r,
richard e. Plant, boca raton, FL: CRC press, 2019. <em>BIOMTC</em>,
<em>76</em>(1), 362. (<a
href="https://doi.org/10.1111/biom.13225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Marie-Josée Fortin},
  doi          = {10.1111/biom.13225},
  journal      = {Biometrics},
  number       = {1},
  pages        = {362},
  shortjournal = {Biometrics},
  title        = {Spatial data analysis in ecology and agriculture using r, richard e. plant, boca raton, FL: CRC press, 2019.},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep learning with r, françois chollet, joseph j. Allaire,
shelter island, NY: manning. <em>BIOMTC</em>, <em>76</em>(1), 361–362.
(<a href="https://doi.org/10.1111/biom.13224">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Sehee Kim},
  doi          = {10.1111/biom.13224},
  journal      = {Biometrics},
  number       = {1},
  pages        = {361-362},
  shortjournal = {Biometrics},
  title        = {Deep learning with r, françois chollet, joseph j. allaire, shelter island, NY: Manning},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). <em>BIOMTC</em>, <em>76</em>(1), 359–360. (<a
href="https://doi.org/10.1111/biom.13227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  author       = {Li-Pang Chen},
  doi          = {10.1111/biom.13227},
  journal      = {Biometrics},
  number       = {1},
  pages        = {359-360},
  shortjournal = {Biometrics},
  title        = {Multistate models for the analysis of life history data, richard j. cook, jerald f. lawless, boca raton, FL: Chapman and Hall/CRC, 2020. hard cover. pp. 441. CDN$ 121.00},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Biclustering via sparse clustering. <em>BIOMTC</em>,
<em>76</em>(1), 348–358. (<a
href="https://doi.org/10.1111/biom.13136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In identifying subgroups of a heterogeneous disease or condition, it is often desirable to identify both the observations and the features which differ between subgroups. For instance, it may be that there is a subgroup of individuals with a certain disease who differ from the rest of the population based on the expression profile for only a subset of genes. Identifying the subgroup of patients and subset of genes could lead to better-targeted therapy. We can represent the subgroup of individuals and genes as a bicluster, a submatrix, U , of a larger data matrix, X , such that the features and observations in U differ from those not contained in U . We present a novel two-step method, SC-Biclust, for identifying U . In the first step, the observations in the bicluster are identified to maximize the sum of the weighted between-cluster feature differences. In the second step, features in the bicluster are identified based on their contribution to the clustering of the observations. This versatile method can be used to identify biclusters that differ on the basis of feature means, feature variances, or more general differences. The bicluster identification accuracy of SC-Biclust is illustrated through several simulated studies. Application of SC-Biclust to pain research illustrates its ability to identify biologically meaningful subgroups.},
  archive      = {J_BIOMTC},
  author       = {Erika S. Helgeson and Qian Liu and Guanhua Chen and Michael R. Kosorok and Eric Bair},
  doi          = {10.1111/biom.13136},
  journal      = {Biometrics},
  number       = {1},
  pages        = {348-358},
  shortjournal = {Biometrics},
  title        = {Biclustering via sparse clustering},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Permutation inference methods for multivariate
meta-analysis. <em>BIOMTC</em>, <em>76</em>(1), 337–347. (<a
href="https://doi.org/10.1111/biom.13134">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multivariate meta-analysis is gaining prominence in evidence synthesis research because it enables simultaneous synthesis of multiple correlated outcome data, and random-effects models have generally been used for addressing between-studies heterogeneities. However, coverage probabilities of confidence regions or intervals for standard inference methods for random-effects models (eg, restricted maximum likelihood estimation) cannot retain their nominal confidence levels in general, especially when the number of synthesized studies is small because their validities depend on large sample approximations. In this article, we provide permutation-based inference methods that enable exact joint inferences for average outcome measures without large sample approximations. We also provide accurate marginal inference methods under general settings of multivariate meta-analyses. We propose effective approaches for permutation inferences using optimal weighting based on the efficient score statistic. The effectiveness of the proposed methods is illustrated via applications to bivariate meta-analyses of diagnostic accuracy studies for airway eosinophilia in asthma and a network meta-analysis for antihypertensive drugs on incident diabetes, as well as through simulation experiments. In numerical evaluations performed via simulations, our methods generally provided accurate confidence regions or intervals under a broad range of settings, whereas the current standard inference methods exhibited serious undercoverage properties.},
  archive      = {J_BIOMTC},
  author       = {Hisashi Noma and Kengo Nagashima and Toshi A. Furukawa},
  doi          = {10.1111/biom.13134},
  journal      = {Biometrics},
  number       = {1},
  pages        = {337-347},
  shortjournal = {Biometrics},
  title        = {Permutation inference methods for multivariate meta-analysis},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quantification of prior impact in terms of effective current
sample size. <em>BIOMTC</em>, <em>76</em>(1), 326–336. (<a
href="https://doi.org/10.1111/biom.13124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bayesian methods allow borrowing of historical information through prior distributions. The concept of prior effective sample size (prior ESS) facilitates quantification and communication of such prior information by equating it to a sample size. Prior information can arise from historical observations; thus, the traditional approach identifies the ESS with such a historical sample size. However, this measure is independent of newly observed data, and thus would not capture an actual “loss of information” induced by the prior in case of prior-data conflict. We build on a recent work to relate prior impact to the number of (virtual) samples from the current data model and introduce the effective current sample size (ECSS) of a prior, tailored to the application in Bayesian clinical trial designs. Special emphasis is put on robust mixture, power, and commensurate priors. We apply the approach to an adaptive design in which the number of recruited patients is adjusted depending on the effective sample size at an interim analysis. We argue that the ECSS is the appropriate measure in this case, as the aim is to save current (as opposed to historical) patients from recruitment. Furthermore, the ECSS can help overcome lack of consensus in the ESS assessment of mixture priors and can, more broadly, provide further insights into the impact of priors. An R package accompanies the paper.},
  archive      = {J_BIOMTC},
  author       = {Manuel Wiesenfarth and Silvia Calderazzo},
  doi          = {10.1111/biom.13124},
  journal      = {Biometrics},
  number       = {1},
  pages        = {326-336},
  shortjournal = {Biometrics},
  title        = {Quantification of prior impact in terms of effective current sample size},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian data integration and variable selection for
pan-cancer survival prediction using protein expression data.
<em>BIOMTC</em>, <em>76</em>(1), 316–325. (<a
href="https://doi.org/10.1111/biom.13132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prognostic prediction using molecular information is a challenging area of research, which is essential to develop precision medicine. In this paper, we develop translational models to identify major actionable proteins that are associated with clinical outcomes, like the survival time of patients. There are considerable statistical and computational challenges due to the large dimension of the problems. Furthermore, data are available for different tumor types; hence data integration for various tumors is desirable. Having censored survival outcomes escalates one more level of complexity in the inferential procedure. We develop Bayesian hierarchical survival models, which accommodate all the challenges mentioned here. We use the hierarchical Bayesian accelerated failure time model for survival regression. Furthermore, we assume sparse horseshoe prior distribution for the regression coefficients to identify the major proteomic drivers. We borrow strength across tumor groups by introducing a correlation structure among the prior distributions. The proposed methods have been used to analyze data from the recently curated “The Cancer Proteome Atlas” (TCPA), which contains reverse-phase protein arrays–based high-quality protein expression data as well as detailed clinical annotation, including survival times. Our simulation and the TCPA data analysis illustrate the efficacy of the proposed integrative model, which links different tumors with the correlated prior structures.},
  archive      = {J_BIOMTC},
  author       = {Arnab Kumar Maity and Anirban Bhattacharya and Bani K. Mallick and Veerabhadran Baladandayuthapani},
  doi          = {10.1111/biom.13132},
  journal      = {Biometrics},
  number       = {1},
  pages        = {316-325},
  shortjournal = {Biometrics},
  title        = {Bayesian data integration and variable selection for pan-cancer survival prediction using protein expression data},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An adaptive trial design to optimize dose-schedule regimes
with delayed outcomes. <em>BIOMTC</em>, <em>76</em>(1), 304–315. (<a
href="https://doi.org/10.1111/biom.13116">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a two-stage phase I-II clinical trial design to optimize dose-schedule regimes of an experimental agent within ordered disease subgroups in terms of the toxicity-efficacy trade-off. The design is motivated by settings where prior biological information indicates it is certain that efficacy will improve with ordinal subgroup level. We formulate a flexible Bayesian hierarchical model to account for associations among subgroups and regimes, and to characterize ordered subgroup effects. Sequentially adaptive decision-making is complicated by the problem, arising from the motivating application, that efficacy is scored on day 90 and toxicity is evaluated within 30 days from the start of therapy, while the patient accrual rate is fast relative to these outcome evaluation intervals. To deal with this in a practical manner, we take a likelihood-based approach that treats unobserved toxicity and efficacy outcomes as missing values, and use elicited utilities that quantify the efficacy-toxicity trade-off as a decision criterion. Adaptive randomization is used to assign patients to regimes while accounting for subgroups, with randomization probabilities depending on the posterior predictive distributions of utilities. A simulation study is presented to evaluate the design&#39;s performance under a variety of scenarios, and to assess its sensitivity to the amount of missing data, the prior, and model misspecification.},
  archive      = {J_BIOMTC},
  author       = {Ruitao Lin and Peter F. Thall and Ying Yuan},
  doi          = {10.1111/biom.13116},
  journal      = {Biometrics},
  number       = {1},
  pages        = {304-315},
  shortjournal = {Biometrics},
  title        = {An adaptive trial design to optimize dose-schedule regimes with delayed outcomes},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Testing the heritability and parent-of-origin hypotheses for
ages at onset of psoriatic arthritis under biased sampling.
<em>BIOMTC</em>, <em>76</em>(1), 293–303. (<a
href="https://doi.org/10.1111/biom.13138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The heritability and parent-of-origin effect hypotheses for chronic diseases can be evaluated by estimating and conducting inference about the parameters that measure the within-family dependences in disease onset times. We model the within-family associations in these times using a Gaussian copula whose correlation matrix accommodates the different pairwise family relationships. We derive score-type statistics to test the heritability and parent-of-origin effect hypotheses when the families selection protocol induces a sampling bias. We illustrate the use of the developed methods through an application to a motivating family study in Psoriatic arthritis and provide strong evidence of excessive paternal transmission of risk.},
  archive      = {J_BIOMTC},
  author       = {Lajmi Lakhal-Chaieb and Richard J. Cook and Yujie Zhong},
  doi          = {10.1111/biom.13138},
  journal      = {Biometrics},
  number       = {1},
  pages        = {293-303},
  shortjournal = {Biometrics},
  title        = {Testing the heritability and parent-of-origin hypotheses for ages at onset of psoriatic arthritis under biased sampling},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A general framework for modeling population abundance data.
<em>BIOMTC</em>, <em>76</em>(1), 281–292. (<a
href="https://doi.org/10.1111/biom.13120">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-series data resulting from surveying wild animals are often described using state-space population dynamics models, in particular with Gompertz, Beverton-Holt, or Moran-Ricker latent processes. We show how hidden Markov model methodology provides a flexible framework for fitting a wide range of models to such data. This general approach makes it possible to model abundance on the natural or log scale, include multiple observations at each sampling occasion and compare alternative models using information criteria. It also easily accommodates unequal sampling time intervals, should that possibility occur, and allows testing for density dependence using the bootstrap. The paper is illustrated by replicated time series of red kangaroo abundances, and a univariate time series of ibex counts which are an order of magnitude larger. In the analyses carried out, we fit different latent process and observation models using the hidden Markov framework. Results are robust with regard to the necessary discretization of the state variable. We find no effective difference between the three latent models of the paper in terms of maximized likelihood value for the two applications presented, and also others analyzed. Simulations suggest that ecological time series are not sufficiently informative to distinguish between alternative latent processes for modeling population survey data when data do not indicate strong density dependence.},
  archive      = {J_BIOMTC},
  author       = {Panagiotis Besbeas and Byron J. T. Morgan},
  doi          = {10.1111/biom.13120},
  journal      = {Biometrics},
  number       = {1},
  pages        = {281-292},
  shortjournal = {Biometrics},
  title        = {A general framework for modeling population abundance data},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improving estimation efficiency for regression with MNAR
covariates. <em>BIOMTC</em>, <em>76</em>(1), 270–280. (<a
href="https://doi.org/10.1111/biom.13131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For regression with covariates missing not at random where the missingness depends on the missing covariate values, complete-case (CC) analysis leads to consistent estimation when the missingness is independent of the response given all covariates, but it may not have the desired level of efficiency. We propose a general empirical likelihood framework to improve estimation efficiency over the CC analysis. We expand on methods in Bartlett et al. (2014, Biostatistics 15 , 719–730) and Xie and Zhang (2017, Int J Biostat 13 , 1–20) that improve efficiency by modeling the missingness probability conditional on the response and fully observed covariates by allowing the possibility of modeling other data distribution-related quantities. We also give guidelines on what quantities to model and demonstrate that our proposal has the potential to yield smaller biases than existing methods when the missingness probability model is incorrect. Simulation studies are presented, as well as an application to data collected from the US National Health and Nutrition Examination Survey.},
  archive      = {J_BIOMTC},
  author       = {Menglu Che and Peisong Han and Jerald F. Lawless},
  doi          = {10.1111/biom.13131},
  journal      = {Biometrics},
  number       = {1},
  pages        = {270-280},
  shortjournal = {Biometrics},
  title        = {Improving estimation efficiency for regression with MNAR covariates},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distance-based analysis of variance for brain connectivity.
<em>BIOMTC</em>, <em>76</em>(1), 257–269. (<a
href="https://doi.org/10.1111/biom.13123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of neuroimaging dedicated to mapping connections in the brain is increasingly being recognized as key for understanding neurodevelopment and pathology. Networks of these connections are quantitatively represented using complex structures, including matrices, functions, and graphs, which require specialized statistical techniques for estimation and inference about developmental and disorder-related changes. Unfortunately, classical statistical testing procedures are not well suited to high-dimensional testing problems. In the context of global or regional tests for differences in neuroimaging data, traditional analysis of variance (ANOVA) is not directly applicable without first summarizing the data into univariate or low-dimensional features, a process that might mask the salient features of high-dimensional distributions. In this work, we consider a general framework for two-sample testing of complex structures by studying generalized within-group and between-group variances based on distances between complex and potentially high-dimensional observations. We derive an asymptotic approximation to the null distribution of the ANOVA test statistic, and conduct simulation studies with scalar and graph outcomes to study finite sample properties of the test. Finally, we apply our test to our motivating study of structural connectivity in autism spectrum disorder.},
  archive      = {J_BIOMTC},
  author       = {Russell T. Shinohara and Haochang Shou and Marco Carone and Robert Schultz and Birkan Tunc and Drew Parker and Melissa Lynne Martin and Ragini Verma},
  doi          = {10.1111/biom.13123},
  journal      = {Biometrics},
  number       = {1},
  pages        = {257-269},
  shortjournal = {Biometrics},
  title        = {Distance-based analysis of variance for brain connectivity},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A functional generalized f-test for signal detection with
applications to event-related potentials significance analysis.
<em>BIOMTC</em>, <em>76</em>(1), 246–256. (<a
href="https://doi.org/10.1111/biom.13118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the analysis of complex dependent functional data such as event-related brain potentials (ERP), this paper considers a time-varying coefficient multivariate regression model with fixed-time covariates for testing global hypotheses about population mean curves. Based on a reduced-rank modeling of the time correlation of the stochastic process of pointwise test statistics, a functional generalized F-test is proposed and its asymptotic null distribution is derived. Our analytical results show that the proposed test is more powerful than functional analysis of variance testing methods and competing signal detection procedures for dependent data. Simulation studies confirm such power gain for data with patterns of dependence similar to those observed in ERPs. The new testing procedure is illustrated with an analysis of the ERP data from a study of neural correlates of impulse control.},
  archive      = {J_BIOMTC},
  author       = {David Causeur and Ching-Fan Sheu and Emeline Perthame and Flavia Rufini},
  doi          = {10.1111/biom.13118},
  journal      = {Biometrics},
  number       = {1},
  pages        = {246-256},
  shortjournal = {Biometrics},
  title        = {A functional generalized F-test for signal detection with applications to event-related potentials significance analysis},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Randomization inference with general interference and
censoring. <em>BIOMTC</em>, <em>76</em>(1), 235–245. (<a
href="https://doi.org/10.1111/biom.13125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interference occurs between individuals when the treatment (or exposure) of one individual affects the outcome of another individual. Previous work on causal inference methods in the presence of interference has focused on the setting where it is a priori assumed that there is “partial interference,” in the sense that individuals can be partitioned into groups wherein there is no interference between individuals in different groups. Bowers et al. (2012, Political Anal , 21, 97–124) and Bowers et al. (2016, Political Anal , 24, 395–403) consider randomization-based inferential methods that allow for more general interference structures in the context of randomized experiments. In this paper, extensions of Bowers et al . that allow for failure time outcomes subject to right censoring are proposed. Permitting right-censored outcomes is challenging because standard randomization-based tests of the null hypothesis of no treatment effect assume that whether an individual is censored does not depend on treatment. The proposed extension of Bowers et al . to allow for censoring entails adapting the method of Wang et al. (2010, Biostatistics , 11, 676–692) for two-sample survival comparisons in the presence of unequal censoring. The methods are examined via simulation studies and utilized to assess the effects of cholera vaccination in an individually randomized trial of 73 000 children and women in Matlab, Bangladesh.},
  archive      = {J_BIOMTC},
  author       = {Wen Wei Loh and Michael G. Hudgens and John D. Clemens and Mohammad Ali and Michael E. Emch},
  doi          = {10.1111/biom.13125},
  journal      = {Biometrics},
  number       = {1},
  pages        = {235-245},
  shortjournal = {Biometrics},
  title        = {Randomization inference with general interference and censoring},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design and analysis of bridging studies with prior
probabilities on the null and alternative hypotheses. <em>BIOMTC</em>,
<em>76</em>(1), 224–234. (<a
href="https://doi.org/10.1111/biom.13175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The pharmaceutical industry and regulatory agencies are increasingly interested in conducting bridging studies in order to bring an approved drug product from the original region (eg, United States or European Union) to a new region (eg, Asian-Pacific countries). In this article, we provide a new methodology for the design and analysis of bridging studies by assuming prior knowledge on how the null and alternative hypotheses in the original, foreign study are related to the null and alternative hypotheses in the bridging study and setting the type I error for the bridging study according to the strength of the foreign-study evidence. The new methodology accounts for randomness in the foreign-study evidence and controls the average type I error of the bridging study over all possibilities of the foreign-study evidence. In addition, the new methodology increases statistical power, when compared to approaches that do not use foreign-study evidence, and it allows for the possibility of not conducting the bridging study when the foreign-study evidence is unfavorable. Finally, we conducted extensive simulation studies to demonstrate the usefulness of the proposed methodology.},
  archive      = {J_BIOMTC},
  author       = {Donglin Zeng and Zhiying Pan and D. Y. Lin},
  doi          = {10.1111/biom.13175},
  journal      = {Biometrics},
  number       = {1},
  pages        = {224-234},
  shortjournal = {Biometrics},
  title        = {Design and analysis of bridging studies with prior probabilities on the null and alternative hypotheses},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Novel two-phase sampling designs for studying binary
outcomes. <em>BIOMTC</em>, <em>76</em>(1), 210–223. (<a
href="https://doi.org/10.1111/biom.13140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In biomedical cohort studies for assessing the association between an outcome variable and a set of covariates, usually, some covariates can only be measured on a subgroup of study subjects. An important design question is—which subjects to select into the subgroup to increase statistical efficiency. When the outcome is binary, one may adopt a case-control sampling design or a balanced case-control design where cases and controls are further matched on a small number of complete discrete covariates. While the latter achieves success in estimating odds ratio (OR) parameters for the matching covariates, similar two-phase design options have not been explored for the remaining covariates, especially the incompletely collected ones. This is of great importance in studies where the covariates of interest cannot be completely collected. To this end, assuming that an external model is available to relate the outcome and complete covariates, we propose a novel sampling scheme that oversamples cases and controls with worse goodness-of-fit based on the external model and further matches them on complete covariates similarly to the balanced design. We develop a pseudolikelihood method for estimating OR parameters. Through simulation studies and explorations in a real-cohort study, we find that our design generally leads to reduced asymptotic variances of the OR estimates and the reduction for the matching covariates is comparable to that of the balanced design.},
  archive      = {J_BIOMTC},
  author       = {Le Wang and Matthew L. Williams and Yong Chen and Jinbo Chen},
  doi          = {10.1111/biom.13140},
  journal      = {Biometrics},
  number       = {1},
  pages        = {210-223},
  shortjournal = {Biometrics},
  title        = {Novel two-phase sampling designs for studying binary outcomes},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A response-adaptive randomization procedure for multi-armed
clinical trials with normally distributed outcomes. <em>BIOMTC</em>,
<em>76</em>(1), 197–209. (<a
href="https://doi.org/10.1111/biom.13119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel response-adaptive randomization procedure for multi-armed trials with continuous outcomes that are assumed to be normally distributed. Our proposed rule is non -myopic, and oriented toward a patient benefit objective, yet maintains computational feasibility. We derive our response-adaptive algorithm based on the Gittins index for the multi-armed bandit problem, as a modification of the method first introduced in Villar et al . ( Biometrics , 71, pp. 969-978). The resulting procedure can be implemented under the assumption of both known or unknown variance. We illustrate the proposed procedure by simulations in the context of phase II cancer trials. Our results show that, in a multi-armed setting, there are efficiency and patient benefit gains of using a response-adaptive allocation procedure with a continuous endpoint instead of a binary one. These gains persist even if an anticipated low rate of missing data due to deaths, dropouts, or complete responses is imputed online through a procedure first introduced in this paper. Additionally, we discuss how there are response-adaptive designs that outperform the traditional equal randomized design both in terms of efficiency and patient benefit measures in the multi-armed trial context.},
  archive      = {J_BIOMTC},
  author       = {S. Faye Williamson and Sofía S. Villar},
  doi          = {10.1111/biom.13119},
  journal      = {Biometrics},
  number       = {1},
  pages        = {197-209},
  shortjournal = {Biometrics},
  title        = {A response-adaptive randomization procedure for multi-armed clinical trials with normally distributed outcomes},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive treatment allocation for comparative clinical
studies with recurrent events data. <em>BIOMTC</em>, <em>76</em>(1),
183–196. (<a href="https://doi.org/10.1111/biom.13117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In long-term clinical studies, recurrent event data are sometimes collected and used to contrast the efficacies of two different treatments. The event reoccurrence rates can be compared using the popular negative binomial model, which incorporates information related to patient heterogeneity into a data analysis. For treatment allocation, a balanced approach in which equal sample sizes are obtained for both treatments is predominately adopted. However, if one treatment is superior, then it may be desirable to allocate fewer subjects to the less-effective treatment. To accommodate this objective, a sequential response-adaptive treatment allocation procedure is derived based on the doubly adaptive biased coin design. Our proposed treatment allocation schemes have been shown to be capable of reducing the number of subjects receiving the inferior treatment while simultaneously retaining a test power level that is comparable to that of a balanced design. The redesign of a clinical study illustrates the advantages of using our procedure.},
  archive      = {J_BIOMTC},
  author       = {Jingya Gao and Pei-Fang Su and Feifang Hu and Siu Hung Cheung},
  doi          = {10.1111/biom.13117},
  journal      = {Biometrics},
  number       = {1},
  pages        = {183-196},
  shortjournal = {Biometrics},
  title        = {Adaptive treatment allocation for comparative clinical studies with recurrent events data},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An online updating approach for testing the proportional
hazards assumption with streams of survival data. <em>BIOMTC</em>,
<em>76</em>(1), 171–182. (<a
href="https://doi.org/10.1111/biom.13137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Cox model—which remains the first choice for analyzing time-to-event data, even for large data sets—relies on the proportional hazards (PH) assumption. When survival data arrive sequentially in chunks, a fast and minimally storage intensive approach to test the PH assumption is desirable. We propose an online updating approach that updates the standard test statistic as each new block of data becomes available and greatly lightens the computational burden. Under the null hypothesis of PH, the proposed statistic is shown to have the same asymptotic distribution as the standard version computed on an entire data stream with the data blocks pooled into one data set. In simulation studies, the test and its variant based on most recent data blocks maintain their sizes when the PH assumption holds and have substantial power to detect different violations of the PH assumption. We also show in simulation that our approach can be used successfully with “big data” that exceed a single computer&#39;s computational resources. The approach is illustrated with the survival analysis of patients with lymphoma cancer from the Surveillance, Epidemiology, and End Results Program. The proposed test promptly identified deviation from the PH assumption, which was not captured by the test based on the entire data.},
  archive      = {J_BIOMTC},
  author       = {Yishu Xue and HaiYing Wang and Jun Yan and Elizabeth D. Schifano},
  doi          = {10.1111/biom.13137},
  journal      = {Biometrics},
  number       = {1},
  pages        = {171-182},
  shortjournal = {Biometrics},
  title        = {An online updating approach for testing the proportional hazards assumption with streams of survival data},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A geostatistical framework for combining spatially
referenced disease prevalence data from multiple diagnostics.
<em>BIOMTC</em>, <em>76</em>(1), 158–170. (<a
href="https://doi.org/10.1111/biom.13142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiple diagnostic tests are often used due to limited resources or because they provide complementary information on the epidemiology of a disease under investigation. Existing statistical methods to combine prevalence data from multiple diagnostics ignore the potential overdispersion induced by the spatial correlations in the data. To address this issue, we develop a geostatistical framework that allows for joint modelling of data from multiple diagnostics by considering two main classes of inferential problems: (a) to predict prevalence for a gold-standard diagnostic using low-cost and potentially biased alternative tests; (b) to carry out joint prediction of prevalence from multiple tests. We apply the proposed framework to two case studies: mapping Loa loa prevalence in Central and West Africa, using miscroscopy, and a questionnaire-based test called RAPLOA; mapping Plasmodium falciparum malaria prevalence in the highlands of Western Kenya using polymerase chain reaction and a rapid diagnostic test. We also develop a Monte Carlo procedure based on the variogram in order to identify parsimonious geostatistical models that are compatible with the data. Our study highlights (a) the importance of accounting for diagnostic-specific residual spatial variation and (b) the benefits accrued from joint geostatistical modelling so as to deliver more reliable and precise inferences on disease prevalence.},
  archive      = {J_BIOMTC},
  author       = {Benjamin Amoah and Peter J. Diggle and Emanuele Giorgi},
  doi          = {10.1111/biom.13142},
  journal      = {Biometrics},
  number       = {1},
  pages        = {158-170},
  shortjournal = {Biometrics},
  title        = {A geostatistical framework for combining spatially referenced disease prevalence data from multiple diagnostics},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Data-adaptive longitudinal model selection in causal
inference with collaborative targeted minimum loss-based estimation.
<em>BIOMTC</em>, <em>76</em>(1), 145–157. (<a
href="https://doi.org/10.1111/biom.13135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal inference methods have been developed for longitudinal observational study designs where confounding is thought to occur over time. In particular, one may estimate and contrast the population mean counterfactual outcome under specific exposure patterns. In such contexts, confounders of the longitudinal treatment-outcome association are generally identified using domain-specific knowledge. However, this may leave an analyst with a large set of potential confounders that may hinder estimation. Previous approaches to data-adaptive model selection for this type of causal parameter were limited to the single time-point setting. We develop a longitudinal extension of a collaborative targeted minimum loss-based estimation (C-TMLE) algorithm that can be applied to perform variable selection in the models for the probability of treatment with the goal of improving the estimation of the population mean counterfactual outcome under a fixed exposure pattern. We investigate the properties of this method through a simulation study, comparing it to G-Computation and inverse probability of treatment weighting. We then apply the method in a real-data example to evaluate the safety of trimester-specific exposure to inhaled corticosteroids during pregnancy in women with mild asthma. The data for this study were obtained from the linkage of electronic health databases in the province of Quebec, Canada. The C-TMLE covariate selection approach allowed for a reduction of the set of potential confounders, which included baseline and longitudinal variables.},
  archive      = {J_BIOMTC},
  author       = {Mireille E. Schnitzer and Joel Sango and Steve Ferreira Guerra and Mark J. van der Laan},
  doi          = {10.1111/biom.13135},
  journal      = {Biometrics},
  number       = {1},
  pages        = {145-157},
  shortjournal = {Biometrics},
  title        = {Data-adaptive longitudinal model selection in causal inference with collaborative targeted minimum loss-based estimation},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semiparametric mixed-scale models using shared bayesian
forests. <em>BIOMTC</em>, <em>76</em>(1), 131–144. (<a
href="https://doi.org/10.1111/biom.13107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper demonstrates the advantages of sharing information about unknown features of covariates across multiple model components in various nonparametric regression problems including multivariate, heteroscedastic, and semicontinuous responses. In this paper, we present a methodology which allows for information to be shared nonparametrically across various model components using Bayesian sum-of-tree models. Our simulation results demonstrate that sharing of information across related model components is often very beneficial, particularly in sparse high-dimensional problems in which variable selection must be conducted. We illustrate our methodology by analyzing medical expenditure data from the Medical Expenditure Panel Survey (MEPS). To facilitate the Bayesian nonparametric regression analysis, we develop two novel models for analyzing the MEPS data using Bayesian additive regression trees—a heteroskedastic log-normal hurdle model with a “shrink-toward-homoskedasticity” prior and a gamma hurdle model.},
  archive      = {J_BIOMTC},
  author       = {Antonio R. Linero and Debajyoti Sinha and Stuart R. Lipsitz},
  doi          = {10.1111/biom.13107},
  journal      = {Biometrics},
  number       = {1},
  pages        = {131-144},
  shortjournal = {Biometrics},
  title        = {Semiparametric mixed-scale models using shared bayesian forests},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust inference for the stepped wedge design.
<em>BIOMTC</em>, <em>76</em>(1), 119–130. (<a
href="https://doi.org/10.1111/biom.13106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stepped wedge designed trials are a type of cluster-randomized study in which the intervention is introduced to each cluster in a random order over time. This design is often used to assess the effect of a new intervention as it is rolled out across a series of clinics or communities. Based on a permutation argument, we derive a closed-form expression for an estimate of the intervention effect, along with its standard error, for a stepped wedge design trial. We show that these estimates are robust to misspecification of both the mean and covariance structure of the underlying data-generating mechanism, thereby providing a robust approach to inference for the intervention effect in stepped wedge designs. We use simulations to evaluate the type 1 error and power of the proposed estimate and to compare the performance of the proposed estimate to the optimal estimate when the correct model specification is known. The limitations, possible extensions, and open problems regarding the method are discussed.},
  archive      = {J_BIOMTC},
  author       = {James P. Hughes and Patrick J. Heagerty and Fan Xia and Yuqi Ren},
  doi          = {10.1111/biom.13106},
  journal      = {Biometrics},
  number       = {1},
  pages        = {119-130},
  shortjournal = {Biometrics},
  title        = {Robust inference for the stepped wedge design},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust inference on the average treatment effect using the
outcome highly adaptive lasso. <em>BIOMTC</em>, <em>76</em>(1), 109–118.
(<a href="https://doi.org/10.1111/biom.13121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many estimators of the average effect of a treatment on an outcome require estimation of the propensity score, the outcome regression, or both. It is often beneficial to utilize flexible techniques, such as semiparametric regression or machine learning, to estimate these quantities. However, optimal estimation of these regressions does not necessarily lead to optimal estimation of the average treatment effect, particularly in settings with strong instrumental variables. A recent proposal addressed these issues via the outcome-adaptive lasso, a penalized regression technique for estimating the propensity score that seeks to minimize the impact of instrumental variables on treatment effect estimators. However, a notable limitation of this approach is that its application is restricted to parametric models. We propose a more flexible alternative that we call the outcome highly adaptive lasso. We discuss the large sample theory for this estimator and propose closed-form confidence intervals based on the proposed estimator. We show via simulation that our method offers benefits over several popular approaches.},
  archive      = {J_BIOMTC},
  author       = {Cheng Ju and David Benkeser and Mark J. van der Laan},
  doi          = {10.1111/biom.13121},
  journal      = {Biometrics},
  number       = {1},
  pages        = {109-118},
  shortjournal = {Biometrics},
  title        = {Robust inference on the average treatment effect using the outcome highly adaptive lasso},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Global identifiability of latent class models with
applications to diagnostic test accuracy studies: A gröbner basis
approach. <em>BIOMTC</em>, <em>76</em>(1), 98–108. (<a
href="https://doi.org/10.1111/biom.13133">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identifiability of statistical models is a fundamental regularity condition that is required for valid statistical inference. Investigation of model identifiability is mathematically challenging for complex models such as latent class models. Jones et al . used Goodman&#39;s technique to investigate the identifiability of latent class models with applications to diagnostic tests in the absence of a gold standard test. The tool they used was based on examining the singularity of the Jacobian or the Fisher information matrix, in order to obtain insights into local identifiability (ie, there exists a neighborhood of a parameter such that no other parameter in the neighborhood leads to the same probability distribution as the parameter). In this paper, we investigate a stronger condition: global identifiability (ie, no two parameters in the parameter space give rise to the same probability distribution), by introducing a powerful mathematical tool from computational algebra: the Gröbner basis. With several existing well-known examples, we argue that the Gröbner basis method is easy to implement and powerful to study global identifiability of latent class models, and is an attractive alternative to the information matrix analysis by Rothenberg and the Jacobian analysis by Goodman and Jones et al .},
  archive      = {J_BIOMTC},
  author       = {Rui Duan and Ming Cao and Yang Ning and Mingfu Zhu and Bin Zhang and Aidan McDermott and Haitao Chu and Xiaohua Zhou and Jason H. Moore and Joseph G. Ibrahim and Daniel O. Scharfstein and Yong Chen},
  doi          = {10.1111/biom.13133},
  journal      = {Biometrics},
  number       = {1},
  pages        = {98-108},
  shortjournal = {Biometrics},
  title        = {Global identifiability of latent class models with applications to diagnostic test accuracy studies: A gröbner basis approach},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bayesian approach to joint modeling of matrix-valued
imaging data and treatment outcome with applications to depression
studies. <em>BIOMTC</em>, <em>76</em>(1), 87–97. (<a
href="https://doi.org/10.1111/biom.13151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a unified Bayesian joint modeling framework for studying association between a binary treatment outcome and a baseline matrix-valued predictor. Specifically, a joint modeling approach relating an outcome to a matrix-valued predictor through a probabilistic formulation of multilinear principal component analysis is developed. This framework establishes a theoretical relationship between the outcome and the matrix-valued predictor, although the predictor is not explicitly expressed in the model. Simulation studies are provided showing that the proposed method is superior or competitive to other methods, such as a two-stage approach and a classical principal component regression in terms of both prediction accuracy and estimation of association; its advantage is most notable when the sample size is small and the dimensionality in the imaging covariate is large. Finally, our proposed joint modeling approach is shown to be a very promising tool in an application exploring the association between baseline electroencephalography data and a favorable response to treatment in a depression treatment study by achieving a substantial improvement in prediction accuracy in comparison to competing methods.},
  archive      = {J_BIOMTC},
  author       = {Bei Jiang and Eva Petkova and Thaddeus Tarpey and R. Todd Ogden},
  doi          = {10.1111/biom.13151},
  journal      = {Biometrics},
  number       = {1},
  pages        = {87-97},
  shortjournal = {Biometrics},
  title        = {A bayesian approach to joint modeling of matrix-valued imaging data and treatment outcome with applications to depression studies},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation of covariance matrix of multivariate longitudinal
data using modified choleksky and hypersphere decompositions.
<em>BIOMTC</em>, <em>76</em>(1), 75–86. (<a
href="https://doi.org/10.1111/biom.13113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linear models are typically used to analyze multivariate longitudinal data. With these models, estimating the covariance matrix is not easy because the covariance matrix should account for complex correlated structures: the correlation between responses at each time point, the correlation within separate responses over time, and the cross-correlation between different responses at different times. In addition, the estimated covariance matrix should satisfy the positive definiteness condition, and it may be heteroscedastic. However, in practice, the structure of the covariance matrix is assumed to be homoscedastic and highly parsimonious, such as exchangeable or autoregressive with order one. These assumptions are too strong and result in inefficient estimates of the effects of covariates. Several studies have been conducted to solve these restrictions using modified Cholesky decomposition (MCD) and linear covariance models. However, modeling the correlation between responses at each time point is not easy because there is no natural ordering of the responses. In this paper, we use MCD and hypersphere decomposition to model the complex correlation structures for multivariate longitudinal data. We observe that the estimated covariance matrix using the decompositions is positive-definite and can be heteroscedastic and that it is also interpretable. The proposed methods are illustrated using data from a nonalcoholic fatty liver disease study.},
  archive      = {J_BIOMTC},
  author       = {Keunbaik Lee and Hyunsoon Cho and Min-Sun Kwak and Eun Jin Jang},
  doi          = {10.1111/biom.13113},
  journal      = {Biometrics},
  number       = {1},
  pages        = {75-86},
  shortjournal = {Biometrics},
  title        = {Estimation of covariance matrix of multivariate longitudinal data using modified choleksky and hypersphere decompositions},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Integrative factorization of bidimensionally linked
matrices. <em>BIOMTC</em>, <em>76</em>(1), 61–74. (<a
href="https://doi.org/10.1111/biom.13141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Advances in molecular “omics” technologies have motivated new methodologies for the integration of multiple sources of high-content biomedical data. However, most statistical methods for integrating multiple data matrices only consider data shared vertically (one cohort on multiple platforms) or horizontally (different cohorts on a single platform). This is limiting for data that take the form of bidimensionally linked matrices (eg, multiple cohorts measured on multiple platforms), which are increasingly common in large-scale biomedical studies. In this paper, we propose bidimensional integrative factorization (BIDIFAC) for integrative dimension reduction and signal approximation of bidimensionally linked data matrices. Our method factorizes data into (a) globally shared, (b) row-shared, (c) column-shared, and (d) single-matrix structural components, facilitating the investigation of shared and unique patterns of variability. For estimation, we use a penalized objective function that extends the nuclear norm penalization for a single matrix. As an alternative to the complicated rank selection problem, we use results from the random matrix theory to choose tuning parameters. We apply our method to integrate two genomics platforms (messenger RNA and microRNA expression) across two sample cohorts (tumor samples and normal tissue samples) using the breast cancer data from the Cancer Genome Atlas. We provide R code for fitting BIDIFAC, imputing missing values, and generating simulated data.},
  archive      = {J_BIOMTC},
  author       = {Jun Young Park and Eric F. Lock},
  doi          = {10.1111/biom.13141},
  journal      = {Biometrics},
  number       = {1},
  pages        = {61-74},
  shortjournal = {Biometrics},
  title        = {Integrative factorization of bidimensionally linked matrices},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Building generalized linear models with ultrahigh
dimensional features: A sequentially conditional approach.
<em>BIOMTC</em>, <em>76</em>(1), 47–60. (<a
href="https://doi.org/10.1111/biom.13122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conditional screening approaches have emerged as a powerful alternative to the commonly used marginal screening, as they can identify marginally weak but conditionally important variables. However, most existing conditional screening methods need to fix the initial conditioning set, which may determine the ultimately selected variables. If the conditioning set is not properly chosen, the methods may produce false negatives and positives. Moreover, screening approaches typically need to involve tuning parameters and extra modeling steps in order to reach a final model. We propose a sequential conditioning approach by dynamically updating the conditioning set with an iterative selection process. We provide its theoretical properties under the framework of generalized linear models. Powered by an extended Bayesian information criterion as the stopping rule, the method will lead to a final model without the need to choose tuning parameters or threshold parameters. The practical utility of the proposed method is examined via extensive simulations and analysis of a real clinical study on predicting multiple myeloma patients’ response to treatment based on their genomic profiles.},
  archive      = {J_BIOMTC},
  author       = {Qi Zheng and Hyokyoung G. Hong and Yi Li},
  doi          = {10.1111/biom.13122},
  journal      = {Biometrics},
  number       = {1},
  pages        = {47-60},
  shortjournal = {Biometrics},
  title        = {Building generalized linear models with ultrahigh dimensional features: A sequentially conditional approach},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Testing independence between two random sets for the
analysis of colocalization in bioimaging. <em>BIOMTC</em>,
<em>76</em>(1), 36–46. (<a
href="https://doi.org/10.1111/biom.13115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Colocalization aims at characterizing spatial associations between two fluorescently tagged biomolecules by quantifying the co-occurrence and correlation between the two channels acquired in fluorescence microscopy. Colocalization is presented either as the degree of overlap between the two channels or the overlays of the red and green images, with areas of yellow indicating colocalization of the molecules. This problem remains an open issue in diffraction-limited microscopy and raises new challenges with the emergence of superresolution imaging, a microscopic technique awarded by the 2014 Nobel prize in chemistry. We propose GcoPS, for Geo-coPositioning System, an original method that exploits the random sets structure of the tagged molecules to provide an explicit testing procedure. Our simulation study shows that GcoPS unequivocally outperforms the best competitive methods in adverse situations (noise, irregularly shaped fluorescent patterns, and different optical resolutions). GcoPS is also much faster, a decisive advantage to face the huge amount of data in superresolution imaging. We demonstrate the performances of GcoPS on two biological real data sets, obtained by conventional diffraction-limited microscopy technique and by superresolution technique, respectively.},
  archive      = {J_BIOMTC},
  author       = {Frédéric Lavancier and Thierry Pécot and Liu Zengzhen and Charles Kervrann},
  doi          = {10.1111/biom.13115},
  journal      = {Biometrics},
  number       = {1},
  pages        = {36-46},
  shortjournal = {Biometrics},
  title        = {Testing independence between two random sets for the analysis of colocalization in bioimaging},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Structured gene-environment interaction analysis.
<em>BIOMTC</em>, <em>76</em>(1), 23–35. (<a
href="https://doi.org/10.1111/biom.13139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the etiology, progression, and treatment of complex diseases, gene-environment (G-E) interactions have important implications beyond the main G and E effects. G-E interaction analysis can be more challenging with higher dimensionality and need for accommodating the “main effects, interactions” hierarchy. In recent literature, an array of novel methods, many of which are based on the penalization technique, have been developed. In most of these studies, however, the structures of G measurements, for example, the adjacency structure of single nucleotide polymorphisms (SNPs; attributable to their physical adjacency on the chromosomes) and the network structure of gene expressions (attributable to their coordinated biological functions and correlated measurements) have not been well accommodated. In this study, we develop structured G-E interaction analysis, where such structures are accommodated using penalization for both the main G effects and interactions. Penalization is also applied for regularized estimation and selection. The proposed structured interaction analysis can be effectively realized. It is shown to have consistency properties under high-dimensional settings. Simulations and analysis of GENEVA diabetes data with SNP measurements and TCGA melanoma data with gene expression measurements demonstrate its competitive practical performance.},
  archive      = {J_BIOMTC},
  author       = {Mengyun Wu and Qingzhao Zhang and Shuangge Ma},
  doi          = {10.1111/biom.13139},
  journal      = {Biometrics},
  number       = {1},
  pages        = {23-35},
  shortjournal = {Biometrics},
  title        = {Structured gene-environment interaction analysis},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Large scale maximum average power multiple inference on
time-course count data with application to RNA-seq analysis.
<em>BIOMTC</em>, <em>76</em>(1), 9–22. (<a
href="https://doi.org/10.1111/biom.13144">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Experiments that longitudinally collect RNA sequencing (RNA-seq) data can provide transformative insights in biology research by revealing the dynamic patterns of genes. Such experiments create a great demand for new analytic approaches to identify differentially expressed (DE) genes based on large-scale time-course count data. Existing methods, however, are suboptimal with respect to power and may lack theoretical justification. Furthermore, most existing tests are designed to distinguish among conditions based on overall differential patterns across time, though in practice, a variety of composite hypotheses are of more scientific interest. Finally, some current methods may fail to control the false discovery rate. In this paper, we propose a new model and testing procedure to address the above issues simultaneously. Specifically, conditional on a latent Gaussian mixture with evolving means, we model the data by negative binomial distributions. Motivated by Storey (2007) and Hwang and Liu (2010), we introduce a general testing framework based on the proposed model and show that the proposed test enjoys the optimality property of maximum average power. The test allows not only identification of traditional DE genes but also testing of a variety of composite hypotheses of biological interest. We establish the identifiability of the proposed model, implement the proposed method via efficient algorithms, and demonstrate its good performance via simulation studies. The procedure reveals interesting biological insights, when applied to data from an experiment that examines the effect of varying light environments on the fundamental physiology of the marine diatom Phaeodactylum tricornutum .},
  archive      = {J_BIOMTC},
  author       = {Meng Cao and Wen Zhou and F. Jay Breidt and Graham Peers},
  doi          = {10.1111/biom.13144},
  journal      = {Biometrics},
  number       = {1},
  pages        = {9-22},
  shortjournal = {Biometrics},
  title        = {Large scale maximum average power multiple inference on time-course count data with application to RNA-seq analysis},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Report of the editors—2019. <em>BIOMTC</em>, <em>76</em>(1),
5–8. (<a href="https://doi.org/10.1111/biom.13215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOMTC},
  doi          = {10.1111/biom.13215},
  journal      = {Biometrics},
  number       = {1},
  pages        = {5-8},
  shortjournal = {Biometrics},
  title        = {Report of the editors—2019},
  volume       = {76},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
