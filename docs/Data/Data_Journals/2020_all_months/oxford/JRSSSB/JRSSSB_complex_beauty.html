<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JRSSSB_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="jrsssb---51">JRSSSB - 51</h2>
<ul>
<li><details>
<summary>
(2020). Contents of volume 82, 2020. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>82</em>(5), 1397–1398. (<a
href="https://doi.org/10.1111/rssb.12395">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  doi     = {10.1111/rssb.12395},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {5},
  pages   = {1397-1398},
  title   = {Contents of volume 82, 2020},
  volume  = {82},
  year    = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spatiotemporal modelling using integro-difference equations
with bivariate stable kernels. <em>Journal of the Royal Statistical
Society: Series B (Statistical Methodology)</em>, <em>82</em>(5),
1371–1392. (<a href="https://doi.org/10.1111/rssb.12393">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {An integro-difference equation can be represented as a hierarchical spatiotemporal dynamic model using appropriate parameterizations. The dynamics of the process defined by an integro-difference equation depends on the choice of a bivariate kernel distribution, where more flexible shapes generally result in more flexible models. Under a Bayesian modelling framework, we consider the use of the stable family of distributions for the kernel, as they are infinitely divisible and offer a variety of tail behaviours, orientations and skewness. Many of the attributes of the bivariate stable distribution are controlled by a measure, which we model using a flexible Bernstein polynomial basis prior. The method is the first attempt to incorporate non-Gaussian kernels in a two-dimensional integro-difference equation model and will be shown to improve prediction over the Gaussian kernel model for a data set of Pacific sea surface temperatures.},
  archive  = {J},
  author   = {Robert Richardson and Athanasios Kottas and Bruno Sansó},
  doi      = {10.1111/rssb.12393},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {5},
  pages    = {1371-1392},
  title    = {Spatiotemporal modelling using integro-difference equations with bivariate stable kernels},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modified likelihood root in high dimensions. <em>Journal of
the Royal Statistical Society: Series B (Statistical Methodology)</em>,
<em>82</em>(5), 1349–1369. (<a
href="https://doi.org/10.1111/rssb.12389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We examine a higher order approximation to the significance function with increasing numbers of nuisance parameters, based on the normal approximation to an adjusted log-likelihood root. We show that the rate of the correction for nuisance parameters is larger than the correction for non-normality, when the parameter dimension p is O ( n α ) for α &lt; 1 2 . We specialize the results to linear exponential families and location–scale families and illustrate these with simulations.},
  archive  = {J},
  author   = {Yanbo Tang and Nancy Reid},
  doi      = {10.1111/rssb.12389},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {5},
  pages    = {1349-1369},
  title    = {Modified likelihood root in high dimensions},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Beta–negative binomial auto-regressions for modelling
integer-valued time series with extreme observations. <em>Journal of the
Royal Statistical Society: Series B (Statistical Methodology)</em>,
<em>82</em>(5), 1325–1347. (<a
href="https://doi.org/10.1111/rssb.12394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The paper introduces a general class of heavy-tailed auto-regressions for modelling integer-valued time series with outliers. The specification proposed is based on a heavy-tailed mixture of negative binomial distributions that features an observation-driven dynamic equation for the conditional expectation. The existence of a stationary and ergodic solution for the class of auto-regressive processes is shown under general conditions. The estimation of the model can be easily performed by maximum likelihood given the closed form of the likelihood function. The strong consistency and the asymptotic normality of the estimator are formally derived. Two examples of specifications illustrate the flexibility of the approach and the relevance of the theoretical results. In particular, a linear dynamic equation and a score-driven equation for the conditional expectation are studied. The score-driven specification is shown to be particularly appealing as it delivers a robust filtering method that attenuates the effect of outliers. Empirical applications to the series of narcotics trafficking reports in Sydney and the euro–pound sterling exchange rate illustrate the effectiveness of the method in handling extreme observations.},
  archive  = {J},
  author   = {Paolo Gorgi},
  doi      = {10.1111/rssb.12394},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {5},
  pages    = {1325-1347},
  title    = {Beta–negative binomial auto-regressions for modelling integer-valued time series with extreme observations},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust tests for treatment effect in survival analysis under
covariate-adaptive randomization. <em>Journal of the Royal Statistical
Society: Series B (Statistical Methodology)</em>, <em>82</em>(5),
1301–1323. (<a href="https://doi.org/10.1111/rssb.12392">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Covariate-adaptive randomization is popular in clinical trials with sequentially arrived patients for balancing treatment assignments across prognostic factors that may have influence on the response. However, existing theory on tests for the treatment effect under covariate-adaptive randomization is limited to tests under linear or generalized linear models, although the covariate-adaptive randomization method has been used in survival analysis for a long time. Often, practitioners will simply adopt a conventional test to compare two treatments, which is controversial since tests derived under simple randomization may not be valid in terms of type I error under other randomization schemes. We derive the asymptotic distribution of the partial likelihood score function under covariate-adaptive randomization and a working model that is subject to possible model misspecification. Using this general result, we prove that the partial likelihood score test that is robust against model misspecification under simple randomization is no longer robust but conservative under covariate-adaptive randomization. We also show that the unstratified log-rank test is conservative and the stratified log-rank test remains valid under covariate-adaptive randomization. We propose a modification to variance estimation in the partial likelihood score test, which leads to a score test that is valid and robust against arbitrary model misspecification under a large family of covariate-adaptive randomization schemes including simple randomization. Furthermore, we show that the modified partial likelihood score test derived under a correctly specified model is more powerful than log-rank-type tests in terms of Pitman&#39;s asymptotic relative efficiency. Simulation studies about the type I error and power of various tests are presented under several popular randomization schemes.},
  archive  = {J},
  author   = {Ting Ye and Jun Shao},
  doi      = {10.1111/rssb.12392},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {5},
  pages    = {1301-1323},
  title    = {Robust tests for treatment effect in survival analysis under covariate-adaptive randomization},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A simple new approach to variable selection in regression,
with application to genetic fine mapping. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>82</em>(5), 1273–1300. (<a
href="https://doi.org/10.1111/rssb.12388">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We introduce a simple new approach to variable selection in linear regression, with a particular focus on quantifying uncertainty in which variables should be selected . The approach is based on a new model—the ‘sum of single effects’ model, called ‘ SuSiE ’—which comes from writing the sparse vector of regression coefficients as a sum of ‘single-effect’ vectors, each with one non-zero element. We also introduce a corresponding new fitting procedure—iterative Bayesian stepwise selection (IBSS)—which is a Bayesian analogue of stepwise selection methods. IBSS shares the computational simplicity and speed of traditional stepwise methods but, instead of selecting a single variable at each step, IBSS computes a distribution on variables that captures uncertainty in which variable to select. We provide a formal justification of this intuitive algorithm by showing that it optimizes a variational approximation to the posterior distribution under SuSiE. Further, this approximate posterior distribution naturally yields convenient novel summaries of uncertainty in variable selection, providing a credible set of variables for each selection. Our methods are particularly well suited to settings where variables are highly correlated and detectable effects are sparse, both of which are characteristics of genetic fine mapping applications. We demonstrate through numerical experiments that our methods outperform existing methods for this task, and we illustrate their application to fine mapping genetic variants influencing alternative splicing in human cell lines. We also discuss the potential and challenges for applying these methods to generic variable-selection problems.},
  archive  = {J},
  author   = {Gao Wang and Abhishek Sarkar and Peter Carbonetto and Matthew Stephens},
  doi      = {10.1111/rssb.12388},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {5},
  pages    = {1273-1300},
  title    = {A simple new approach to variable selection in regression, with application to genetic fine mapping},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating densities with non-linear support by using
fisher–gaussian kernels. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>82</em>(5), 1249–1271. (<a
href="https://doi.org/10.1111/rssb.12390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Current tools for multivariate density estimation struggle when the density is concentrated near a non-linear subspace or manifold. Most approaches require the choice of a kernel, with the multivariate Gaussian kernel by far the most commonly used. Although heavy-tailed and skewed extensions have been proposed, such kernels cannot capture curvature in the support of the data. This leads to poor performance unless the sample size is very large relative to the dimension of the data.  The paper proposes a novel generalization of the Gaussian distribution, which includes an additional curvature parameter.  We refer to the proposed class as Fisher–Gaussian kernels, since they arise by sampling from a von Mises–Fisher density on the sphere and adding Gaussian noise. The Fisher–Gaussian density has an analytic form and is amenable to straightforward implementation within Bayesian mixture models by using Markov chain Monte Carlo sampling.  We provide theory on large support and illustrate gains relative to competitors in simulated and real data applications.},
  archive  = {J},
  author   = {Minerva Mukhopadhyay and Didong Li and David B. Dunson},
  doi      = {10.1111/rssb.12390},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {5},
  pages    = {1249-1271},
  title    = {Estimating densities with non-linear support by using Fisher–Gaussian kernels},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An information theoretic approach for selecting arms in
clinical trials. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>82</em>(5), 1223–1247. (<a
href="https://doi.org/10.1111/rssb.12391">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The question of selecting the ‘best’ among different choices is a common problem in statistics. In drug development, our motivating setting, the question becomes, for example, which treatment gives the best response rate. Motivated by recent developments in the theory of context-dependent information measures, we propose a flexible response-adaptive experimental design based on a novel criterion governing treatment arm selections which can be used in adaptive experiments with simple (e.g. binary) and complex (e.g. co-primary, ordinal or nested) end points. It was found that, for specific choices of the context-dependent measure, the criterion leads to a reliable selection of the correct arm without any parametric or monotonicity assumptions and provides noticeable gains in settings with costly observations. The asymptotic properties of the design are studied for different allocation rules, and the small sample size behaviour is evaluated in simulations in the context of phase II clinical trials with different end points. We compare the proposed design with currently used alternatives and discuss its practical implementation.},
  archive  = {J},
  author   = {Pavel Mozgunov and Thomas Jaki},
  doi      = {10.1111/rssb.12391},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {5},
  pages    = {1223-1247},
  title    = {An information theoretic approach for selecting arms in clinical trials},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quasi-stationary monte carlo and the ScaLE algorithm.
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>82</em>(5), 1167–1221. (<a
href="https://doi.org/10.1111/rssb.12365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper introduces a class of Monte Carlo algorithms which are based on the simulation of a Markov process whose quasi-stationary distribution coincides with a distribution of interest. This differs fundamentally from, say, current Markov chain Monte Carlo methods which simulate a Markov chain whose stationary distribution is the target. We show how to approximate distributions of interest by carefully combining sequential Monte Carlo methods with methodology for the exact simulation of diffusions. The methodology introduced here is particularly promising in that it is applicable to the same class of problems as gradient-based Markov chain Monte Carlo algorithms but entirely circumvents the need to conduct Metropolis–Hastings type accept–reject steps while retaining exactness : the paper gives theoretical guarantees ensuring that the algorithm has the correct limiting target distribution. Furthermore, this methodology is highly amenable to ‘big data’ problems. By employing a modification to existing naive subsampling and control variate techniques it is possible to obtain an algorithm which is still exact but has sublinear iterative cost as a function of data size.},
  archive  = {J},
  author   = {Murray Pollock and Paul Fearnhead and Adam M. Johansen and Gareth O. Roberts},
  doi      = {10.1111/rssb.12365},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {5},
  pages    = {1167-1221},
  title    = {Quasi-stationary monte carlo and the ScaLE algorithm},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal alpha spending for sequential analysis with binomial
data. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>82</em>(4), 1141–1164. (<a
href="https://doi.org/10.1111/rssb.12379">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {For sequential analysis hypothesis testing, various alpha spending functions have been proposed. Given a prespecified overall alpha level and power, we derive the optimal alpha spending function that minimizes the expected time to signal for continuous as well as group sequential analysis. If there is also a restriction on the maximum sample size or on the expected sample size, we do the same. Alternatively, for fixed overall alpha, power and expected time to signal, we derive the optimal alpha spending function that minimizes the expected sample size. The method constructs alpha spending functions that are uniformly better than any other method, such as the classical Wald, Pocock or O’Brien–Fleming methods. The results are based on exact calculations using linear programming. All numerical examples were run by using the R Sequential package.},
  archive  = {J},
  author   = {Ivair R. Silva and Martin Kulldorff and W. Katherine Yih},
  doi      = {10.1111/rssb.12379},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {1141-1164},
  title    = {Optimal alpha spending for sequential analysis with binomial data},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Superconsistent estimation of points of impact in
non-parametric regression with functional predictors. <em>Journal of the
Royal Statistical Society: Series B (Statistical Methodology)</em>,
<em>82</em>(4), 1115–1140. (<a
href="https://doi.org/10.1111/rssb.12386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Predicting scalar outcomes by using functional predictors is a classical problem in functional data analysis. In many applications, however, only specific locations or time points of the functional predictors have an influence on the outcome. Such ‘points of impact’ are typically unknown and must be estimated in addition to estimating the usual model components. We show that our points-of-impact estimator enjoys a superconsistent rate of convergence and does not require knowledge or pre-estimates of the unknown model components. This remarkable result facilitates the subsequent estimation of the remaining model components as shown in the theoretical part, where we consider the case of non-parametric models and the practically relevant case of generalized linear models. The finite sample properties of our estimators are assessed by means of a simulation study. Our methodology is motivated by data from a psychological experiment in which the participants were asked to rate their emotional state continuously while watching an affective video eliciting a varying intensity of emotional reactions.},
  archive  = {J},
  author   = {Dominik Poß and Dominik Liebl and Alois Kneip and Hedwig Eisenbarth and Tor D. Wager and Lisa Feldman Barrett},
  doi      = {10.1111/rssb.12386},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {1115-1140},
  title    = {Superconsistent estimation of points of impact in non-parametric regression with functional predictors},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quasi-bayes properties of a procedure for sequential
learning in mixture models. <em>Journal of the Royal Statistical
Society: Series B (Statistical Methodology)</em>, <em>82</em>(4),
1087–1114. (<a href="https://doi.org/10.1111/rssb.12385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Bayesian methods are often optimal, yet increasing pressure for fast computations, especially with streaming data, brings renewed interest in faster, possibly suboptimal, solutions. The extent to which these algorithms approximate Bayesian solutions is a question of interest, but often unanswered. We propose a methodology to address this question in predictive settings, when the algorithm can be reinterpreted as a probabilistic predictive rule. We specifically develop the proposed methodology for a recursive procedure for on-line learning in non-parametric mixture models, which is often referred to as Newton&#39;s algorithm. This algorithm is simple and fast; however, its approximation properties are unclear. By reinterpreting it as a predictive rule, we can show that it underlies a statistical model which is, asymptotically, a Bayesian, exchangeable mixture model. In this sense, the recursive rule provides a quasi-Bayes solution. Although the algorithm offers only a point estimate, our clean statistical formulation enables us to provide the asymptotic posterior distribution and asymptotic credible intervals for the mixing distribution. Moreover, it gives insights for tuning the parameters, as we illustrate in simulation studies, and paves the way to extensions in various directions. Beyond mixture models, our approach can be applied to other predictive algorithms.},
  archive  = {J},
  author   = {Sandra Fortini and Sonia Petrone},
  doi      = {10.1111/rssb.12385},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {1087-1114},
  title    = {Quasi-bayes properties of a procedure for sequential learning in mixture models},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Visualizing the effects of predictor variables in black box
supervised learning models. <em>Journal of the Royal Statistical
Society: Series B (Statistical Methodology)</em>, <em>82</em>(4),
1059–1086. (<a href="https://doi.org/10.1111/rssb.12377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In many supervised learning applications, understanding and visualizing the effects of the predictor variables on the predicted response is of paramount importance. A shortcoming of black box supervised learning models (e.g. complex trees, neural networks, boosted trees, random forests, nearest neighbours, local kernel-weighted methods and support vector regression) in this regard is their lack of interpretability or transparency. Partial dependence plots, which are the most popular approach for visualizing the effects of the predictors with black box supervised learning models, can produce erroneous results if the predictors are strongly correlated, because they require extrapolation of the response at predictor values that are far outside the multivariate envelope of the training data. As an alternative to partial dependence plots, we present a new visualization approach that we term accumulated local effects plots, which do not require this unreliable extrapolation with correlated predictors. Moreover, accumulated local effects plots are far less computationally expensive than partial dependence plots. We also provide an R package ALEPlot as supplementary material to implement our proposed method.},
  archive  = {J},
  author   = {Daniel W. Apley and Jingyu Zhu},
  doi      = {10.1111/rssb.12377},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {1059-1086},
  title    = {Visualizing the effects of predictor variables in black box supervised learning models},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive designs for optimal observed fisher information.
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>82</em>(4), 1029–1058. (<a
href="https://doi.org/10.1111/rssb.12378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Expected Fisher information can be found a priori and as a result its inverse is the primary variance approximation used in the design of experiments. This is in contrast with the common claim that the inverse of the observed Fisher information is a better approximation of the variance of the maximum likelihood estimator. Observed Fisher information cannot be known a priori ; however, if an experiment is conducted sequentially, in a series of runs, the observed Fisher information from previous runs is known. In the current work, two adaptive designs are proposed that use the observed Fisher information from previous runs to inform the design of future runs.},
  archive  = {J},
  author   = {Adam Lane},
  doi      = {10.1111/rssb.12378},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {1029-1058},
  title    = {Adaptive designs for optimal observed fisher information},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). False discovery and its control in low rank estimation.
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>82</em>(4), 997–1027. (<a
href="https://doi.org/10.1111/rssb.12387">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Models specified by low rank matrices are ubiquitous in contemporary applications. In many of these problem domains, the row–column space structure of a low rank matrix carries information about some underlying phenomenon, and it is of interest in inferential settings to evaluate the extent to which the row–column spaces of an estimated low rank matrix signify discoveries about the phenomenon. However, in contrast with variable selection, we lack a formal framework to assess true or false discoveries in low rank estimation; in particular, the key source of difficulty is that the standard notion of a discovery is a discrete notion that is ill suited to the smooth structure underlying low rank matrices. We address this challenge via a geometric reformulation of the concept of a discovery, which then enables a natural definition in the low rank case. We describe and analyse a generalization of the stability selection method of Meinshausen and Bühlmann to control for false discoveries in low rank estimation, and we demonstrate its utility compared with previous approaches via numerical experiments.},
  archive  = {J},
  author   = {Armeen Taeb and Parikshit Shah and Venkat Chandrasekaran},
  doi      = {10.1111/rssb.12387},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {997-1027},
  title    = {False discovery and its control in low rank estimation},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A scalable estimate of the out-of-sample prediction error
via approximate leave-one-out cross-validation. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>82</em>(4), 965–996. (<a
href="https://doi.org/10.1111/rssb.12374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The paper considers the problem of out-of-sample risk estimation under the high dimensional settings where standard techniques such as K -fold cross-validation suffer from large biases. Motivated by the low bias of the leave-one-out cross-validation method, we propose a computationally efficient closed form approximate leave-one-out formula ALO for a large class of regularized estimators. Given the regularized estimate, calculating ALO requires a minor computational overhead. With minor assumptions about the data-generating process, we obtain a finite sample upper bound for the difference between leave-one-out cross-validation and approximate leave-one-out cross-validation, |LO−ALO|. Our theoretical analysis illustrates that |LO−ALO|→0 with overwhelming probability, when n , p →∞, where the dimension p of the feature vectors may be comparable with or even greater than the number of observations, n . Despite the high dimensionality of the problem, our theoretical results do not require any sparsity assumption on the vector of regression coefficients. Our extensive numerical experiments show that |LO−ALO| decreases as n and p increase, revealing the excellent finite sample performance of approximate leave-one-out cross-validation. We further illustrate the usefulness of our proposed out-of-sample risk estimation method by an example of real recordings from spatially sensitive neurons (grid cells) in the medial entorhinal cortex of a rat.},
  archive  = {J},
  author   = {Kamiar Rahnama Rad and Arian Maleki},
  doi      = {10.1111/rssb.12374},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {965-996},
  title    = {A scalable estimate of the out-of-sample prediction error via approximate leave-one-out cross-validation},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A unified data-adaptive framework for high dimensional
change point detection. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>82</em>(4), 933–963. (<a
href="https://doi.org/10.1111/rssb.12375">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In recent years, change point detection for a high dimensional data sequence has become increasingly important in many scientific fields such as biology and finance. The existing literature develops a variety of methods designed for either a specified parameter (e.g. the mean or covariance) or a particular alternative pattern (sparse or dense), but not for both scenarios simultaneously. To overcome this limitation, we provide a general framework for developing tests that are suitable for a large class of parameters, and also adaptive to various alternative scenarios. In particular, by generalizing the classical cumulative sum statistic, we construct the U -statistic-based cumulative sum matrix C . Two cases corresponding to common or different change point locations across the components are considered. We then propose two types of individual test statistics by aggregating C on the basis of the adjusted L p -norm with p ∈ {1,…,∞}. Combining the corresponding individual tests, we construct two types of data-adaptive tests for the two cases, which are both powerful under various alternative patterns. A multiplier bootstrap method is introduced for approximating the proposed test statistics’ limiting distributions. With flexible dependence structure across co-ordinates and mild moment conditions, we show the optimality of our methods theoretically in terms of size and power by allowing the dimension d and the number of parameters q to be much larger than the sample size n . An R package called AdaptiveCpt is developed to implement our algorithms. Extensive simulation studies provide further support for our theory. An application to a comparative genomic hybridization data set also demonstrates the usefulness of our proposed methods.},
  archive  = {J},
  author   = {Bin Liu and Cheng Zhou and Xinsheng Zhang and Yufeng Liu},
  doi      = {10.1111/rssb.12375},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {933-963},
  title    = {A unified data-adaptive framework for high dimensional change point detection},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graphical models for extremes. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>82</em>(4), 871–932. (<a
href="https://doi.org/10.1111/rssb.12355">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Conditional independence, graphical models and sparsity are key notions for parsimonious statistical models and for understanding the structural relationships in the data. The theory of multivariate and spatial extremes describes the risk of rare events through asymptotically justified limit models such as max-stable and multivariate Pareto distributions. Statistical modelling in this field has been limited to moderate dimensions so far, partly owing to complicated likelihoods and a lack of understanding of the underlying probabilistic structures. We introduce a general theory of conditional independence for multivariate Pareto distributions that enables the definition of graphical models and sparsity for extremes. A Hammersley–Clifford theorem links this new notion to the factorization of densities of extreme value models on graphs. For the popular class of Hüsler–Reiss distributions we show that, similarly to the Gaussian case, the sparsity pattern of a general extremal graphical model can be read off from suitable inverse covariance matrices. New parametric models can be built in a modular way and statistical inference can be simplified to lower dimensional marginals. We discuss learning of minimum spanning trees and model selection for extremal graph structures, and we illustrate their use with an application to flood risk assessment on the Danube river.},
  archive  = {J},
  author   = {Sebastian Engelke and Adrien S. Hitz},
  doi      = {10.1111/rssb.12355},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {4},
  pages    = {871-932},
  title    = {Graphical models for extremes},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reply to the correction by grover and kaur: A new randomized
response model. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>82</em>(3), 865–868. (<a
href="https://doi.org/10.1111/rssb.12376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {Sarjinder Singh},
  doi     = {10.1111/rssb.12376},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {3},
  pages   = {865-868},
  title   = {Reply to the correction by grover and kaur: A new randomized response model},
  volume  = {82},
  year    = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust testing in generalized linear models by sign flipping
score contributions. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>82</em>(3), 841–864. (<a
href="https://doi.org/10.1111/rssb.12369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Generalized linear models are often misspecified because of overdispersion, heteroscedasticity and ignored nuisance variables. Existing quasi-likelihood methods for testing in misspecified models often do not provide satisfactory type I error rate control. We provide a novel semiparametric test, based on sign flipping individual score contributions. The parameter tested is allowed to be multi-dimensional and even high dimensional. Our test is often robust against the mentioned forms of misspecification and provides better type I error control than its competitors. When nuisance parameters are estimated, our basic test becomes conservative. We show how to take nuisance estimation into account to obtain an asymptotically exact test. Our proposed test is asymptotically equivalent to its parametric counterpart.},
  archive  = {J},
  author   = {Jesse Hemerik and Jelle J. Goeman and Livio Finos},
  doi      = {10.1111/rssb.12369},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {3},
  pages    = {841-864},
  title    = {Robust testing in generalized linear models by sign flipping score contributions},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On bandwidth choice for spatial data density estimation.
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>82</em>(3), 817–840. (<a
href="https://doi.org/10.1111/rssb.12367">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Bandwidth choice is crucial in spatial kernel estimation in exploring non-Gaussian complex spatial data. The paper investigates the choice of adaptive and non-adaptive bandwidths for density estimation given data on a spatial lattice. An adaptive bandwidth depends on local data and hence adaptively conforms with local features of the spatial data. We propose a spatial cross-validation (SCV) choice of a global bandwidth. This is done first with a pilot density involved in the expression for the adaptive bandwidth. The optimality of the procedure is established, and it is shown that a non-adaptive bandwidth choice comes out as a special case. Although the cross-validation idea has been popular for choosing a non-adaptive bandwidth in data-driven smoothing of independent and time series data, its theory and application have not been much investigated for spatial data. For the adaptive case, there is little theory even for independent data. Conditions that ensure asymptotic optimality of the SCV-selected bandwidth are derived, actually, also extending time series and independent data optimality results. Further, for the adaptive bandwidth with an estimated pilot density, oracle properties of the resultant density estimator are obtained asymptotically as if the true pilot were known. Numerical simulations show that finite sample performance of the SCV adaptive bandwidth choice works quite well. It outperforms the existing R routines such as the ‘rule of thumb’ and the so-called ‘second-generation’ Sheather–Jones bandwidths for moderate and big data sets. An empirical application to a set of spatial soil data is further implemented with non-Gaussian features significantly identified.},
  archive  = {J},
  author   = {Zhenyu Jiang and Nengxiang Ling and Zudi Lu and Dag Tj⊘stheim and Qiang Zhang},
  doi      = {10.1111/rssb.12367},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {3},
  pages    = {817-840},
  title    = {On bandwidth choice for spatial data density estimation},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inference for two-stage sampling designs. <em>Journal of the
Royal Statistical Society: Series B (Statistical Methodology)</em>,
<em>82</em>(3), 797–815. (<a
href="https://doi.org/10.1111/rssb.12368">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Two-stage sampling designs are commonly used for household and health surveys. To produce reliable estimators with associated confidence intervals, some basic statistical properties like consistency and asymptotic normality of the Horvitz–Thompson estimator are desirable, along with the consistency of associated variance estimators. These properties have been mainly studied for single-stage sampling designs. In this work, we prove the consistency of the Horvitz–Thompson estimator and of associated variance estimators for a general class of two-stage sampling designs, under mild assumptions. We also study two-stage sampling with a large entropy sampling design at the first stage and prove that the Horvitz–Thompson estimator is asymptotically normally distributed through a coupling argument. When the first-stage sampling fraction is negligible, simplified variance estimators which do not require estimating the variance within the primary sampling units are proposed and shown to be consistent. An application to a panel for urban policy, which is the initial motivation for this work, is also presented.},
  archive  = {J},
  author   = {Guillaume Chauvet and Audrey-Anne Vallée},
  doi      = {10.1111/rssb.12368},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {3},
  pages    = {797-815},
  title    = {Inference for two-stage sampling designs},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Goodness-of-fit testing in high dimensional generalized
linear models. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>82</em>(3), 773–795. (<a
href="https://doi.org/10.1111/rssb.12371">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose a family of tests to assess the goodness of fit of a high dimensional generalized linear model. Our framework is flexible and may be used to construct an omnibus test or directed against testing specific non-linearities and interaction effects, or for testing the significance of groups of variables. The methodology is based on extracting left-over signal in the residuals from an initial fit of a generalized linear model. This can be achieved by predicting this signal from the residuals by using modern powerful regression or machine learning methods such as random forests or boosted trees. Under the null hypothesis that the generalized linear model is correct, no signal is left in the residuals and our test statistic has a Gaussian limiting distribution, translating to asymptotic control of type I error. Under a local alternative, we establish a guarantee on the power of the test. We illustrate the effectiveness of the methodology on simulated and real data examples by testing goodness of fit in logistic regression models. Software implementing the methodology is available in the R package GRPtests.},
  archive  = {J},
  author   = {Jana Janková and Rajen D. Shah and Peter Bühlmann and Richard J. Samworth},
  doi      = {10.1111/rssb.12371},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {3},
  pages    = {773-795},
  title    = {Goodness-of-fit testing in high dimensional generalized linear models},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal, two-stage, adaptive enrichment designs for
randomized trials, using sparse linear programming. <em>Journal of the
Royal Statistical Society: Series B (Statistical Methodology)</em>,
<em>82</em>(3), 749–772. (<a
href="https://doi.org/10.1111/rssb.12366">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Adaptive enrichment designs involve preplanned rules for modifying enrolment criteria based on accruing data in a randomized trial. We focus on designs where the overall population is partitioned into two predefined subpopulations, e.g. based on a biomarker or risk score measured at baseline. The goal is to learn which populations benefit from an experimental treatment. Two critical components of adaptive enrichment designs are the decision rule for modifying enrolment, and the multiple-testing procedure. We provide a general method for simultaneously optimizing these components for two-stage, adaptive enrichment designs. We minimize the expected sample size under constraints on power and the familywise type I error rate. It is computationally infeasible to solve this optimization problem directly because of its non-convexity. The key to our approach is a novel, discrete representation of this optimization problem as a sparse linear program, which is large but computationally feasible to solve by using modern optimization techniques. We provide an R package that implements our method and is compatible with linear program solvers in several software languages. Our approach produces new, approximately optimal trial designs.},
  archive  = {J},
  author   = {Michael Rosenblum and Ethan X. Fang and Han Liu},
  doi      = {10.1111/rssb.12366},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {3},
  pages    = {749-772},
  title    = {Optimal, two-stage, adaptive enrichment designs for randomized trials, using sparse linear programming},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Causal isotonic regression. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>82</em>(3), 719–747. (<a
href="https://doi.org/10.1111/rssb.12372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In observational studies, potential confounders may distort the causal relationship between an exposure and an outcome. However, under some conditions, a causal dose–response curve can be recovered by using the G -computation formula. Most classical methods for estimating such curves when the exposure is continuous rely on restrictive parametric assumptions, which carry significant risk of model misspecification. Non-parametric estimation in this context is challenging because in a non-parametric model these curves cannot be estimated at regular rates. Many available non-parametric estimators are sensitive to the selection of certain tuning parameters, and performing valid inference with such estimators can be difficult. We propose a non-parametric estimator of a causal dose–response curve known to be monotone. We show that our proposed estimation procedure generalizes the classical least squares isotonic regression estimator of a monotone regression function. Specifically, it does not involve tuning parameters and is invariant to strictly monotone transformations of the exposure variable. We describe theoretical properties of our proposed estimator, including its irregular limit distribution and the potential for doubly robust inference. Furthermore, we illustrate its performance via numerical studies and use it to assess the relationship between body mass index and immune response in human immunodeficiency virus vaccine trials.},
  archive  = {J},
  author   = {Ted Westling and Peter Gilbert and Marco Carone},
  doi      = {10.1111/rssb.12372},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {3},
  pages    = {719-747},
  title    = {Causal isotonic regression},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A flexible framework for hypothesis testing in high
dimensions. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>82</em>(3), 685–718. (<a
href="https://doi.org/10.1111/rssb.12373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Hypothesis testing in the linear regression model is a fundamental statistical problem. We consider linear regression in the high dimensional regime where the number of parameters exceeds the number of samples ( p &gt; n ). To make informative inference, we assume that the model is approximately sparse, i.e. the effect of covariates on the response can be well approximated by conditioning on a relatively small number of covariates whose identities are unknown. We develop a framework for testing very general hypotheses regarding the model parameters. Our framework encompasses testing whether the parameter lies in a convex cone, testing the signal strength, and testing arbitrary functionals of the parameter. We show that the procedure proposed controls the type I error, and we also analyse the power of the procedure. Our numerical experiments confirm our theoretical findings and demonstrate that we control the false positive rate (type I error) near the nominal level and have high power. By duality between hypotheses testing and confidence intervals, the framework proposed can be used to obtain valid confidence intervals for various functionals of the model parameters. For linear functionals, the length of confidence intervals is shown to be minimax rate optimal.},
  archive  = {J},
  author   = {Adel Javanmard and Jason D. Lee},
  doi      = {10.1111/rssb.12373},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {3},
  pages    = {685-718},
  title    = {A flexible framework for hypothesis testing in high dimensions},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Causal mediation analysis for stochastic interventions.
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>82</em>(3), 661–683. (<a
href="https://doi.org/10.1111/rssb.12362">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Mediation analysis in causal inference has traditionally focused on binary exposures and deterministic interventions, and a decomposition of the average treatment effect in terms of direct and indirect effects. We present an analogous decomposition of the population intervention effect , defined through stochastic interventions on the exposure. Population intervention effects provide a generalized framework in which a variety of interesting causal contrasts can be defined, including effects for continuous and categorical exposures. We show that identification of direct and indirect effects for the population intervention effect requires weaker assumptions than its average treatment effect counterpart, under the assumption of no mediator–outcome confounders affected by exposure. In particular, identification of direct effects is guaranteed in experiments that randomize the exposure and the mediator. We propose various estimators of the direct and indirect effects, including substitution, reweighted and efficient estimators based on flexible regression techniques, allowing for multivariate mediators. Our efficient estimator is asymptotically linear under a condition requiring n 1/4 -consistency of certain regression functions. We perform a simulation study in which we assess the finite sample properties of our proposed estimators. We present the results of an illustrative study where we assess the effect of participation in a sports team on the body mass index among children, using mediators such as exercise habits, daily consumption of snacks and overweight status.},
  archive  = {J},
  author   = {Iván Díaz and Nima S. Hejazi},
  doi      = {10.1111/rssb.12362},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {3},
  pages    = {661-683},
  title    = {Causal mediation analysis for stochastic interventions},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Testing relevant hypotheses in functional time series via
self-normalization. <em>Journal of the Royal Statistical Society: Series
B (Statistical Methodology)</em>, <em>82</em>(3), 629–660. (<a
href="https://doi.org/10.1111/rssb.12370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We develop methodology for testing relevant hypotheses about functional time series in a tuning-free way. Instead of testing for exact equality, e.g. for the equality of two mean functions from two independent time series, we propose to test the null hypothesis of no relevant deviation. In the two-sample problem this means that an L 2 -distance between the two mean functions is smaller than a prespecified threshold. For such hypotheses self-normalization, which was introduced in 2010 by Shao, and Shao and Zhang and is commonly used to avoid the estimation of nuisance parameters, is not directly applicable. We develop new self-normalized procedures for testing relevant hypotheses in the one-sample, two-sample and change point problem and investigate their asymptotic properties. Finite sample properties of the tests proposed are illustrated by means of a simulation study and data examples. Our main focus is on functional time series, but extensions to other settings are also briefly discussed.},
  archive  = {J},
  author   = {Holger Dette and Kevin Kokot and Stanislav Volgushev},
  doi      = {10.1111/rssb.12370},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {3},
  pages    = {629-660},
  title    = {Testing relevant hypotheses in functional time series via self-normalization},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust estimation via robust gradient estimation.
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>82</em>(3), 601–627. (<a
href="https://doi.org/10.1111/rssb.12364">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We provide a new computationally efficient class of estimators for risk minimization. We show that these estimators are robust for general statistical models, under varied robustness settings, including in the classical Huber ε -contamination model, and in heavy-tailed settings. Our workhorse is a novel robust variant of gradient descent, and we provide conditions under which our gradient descent variant provides accurate estimators in a general convex risk minimization problem. We provide specific consequences of our theory for linear regression and logistic regression and for canonical parameter estimation in an exponential family. These results provide some of the first computationally tractable and provably robust estimators for these canonical statistical models. Finally, we study the empirical performance of our proposed methods on synthetic and real data sets, and we find that our methods convincingly outperform a variety of baselines.},
  archive  = {J},
  author   = {Adarsh Prasad and Arun Sai Suggala and Sivaraman Balakrishnan and Pradeep Ravikumar},
  doi      = {10.1111/rssb.12364},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {3},
  pages    = {601-627},
  title    = {Robust estimation via robust gradient estimation},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unbiased markov chain monte carlo methods with couplings.
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>82</em>(3), 543–600. (<a
href="https://doi.org/10.1111/rssb.12336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Markov chain Monte Carlo (MCMC) methods provide consistent approximations of integrals as the number of iterations goes to ∞. MCMC estimators are generally biased after any fixed number of iterations. We propose to remove this bias by using couplings of Markov chains together with a telescopic sum argument of Glynn and Rhee. The resulting unbiased estimators can be computed independently in parallel. We discuss practical couplings for popular MCMC algorithms. We establish the theoretical validity of the estimators proposed and study their efficiency relative to the underlying MCMC algorithms. Finally, we illustrate the performance and limitations of the method on toy examples, on an Ising model around its critical temperature, on a high dimensional variable-selection problem, and on an approximation of the cut distribution arising in Bayesian inference for models made of multiple modules.},
  archive  = {J},
  author   = {Pierre E. Jacob and John O’Leary and Yves F. Atchadé},
  doi      = {10.1111/rssb.12336},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {3},
  pages    = {543-600},
  title    = {Unbiased markov chain monte carlo methods with couplings},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiply robust causal inference with double-negative
control adjustment for categorical unmeasured confounding. <em>Journal
of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>82</em>(2), 521–540. (<a
href="https://doi.org/10.1111/rssb.12361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Unmeasured confounding is a threat to causal inference in observational studies. In recent years, the use of negative controls to mitigate unmeasured confounding has gained increasing recognition and popularity. Negative controls have a long-standing tradition in laboratory sciences and epidemiology to rule out non-causal explanations, although they have been used primarily for bias detection. Recently, Miao and colleagues have described sufficient conditions under which a pair of negative control exposure and outcome variables can be used to identify non-parametrically the average treatment effect (ATE) from observational data subject to uncontrolled confounding. We establish non-parametric identification of the ATE under weaker conditions in the case of categorical unmeasured confounding and negative control variables. We also provide a general semiparametric framework for obtaining inferences about the ATE while leveraging information about a possibly large number of measured covariates. In particular, we derive the semiparametric efficiency bound in the non-parametric model, and we propose multiply robust and locally efficient estimators when non-parametric estimation may not be feasible. We assess the finite sample performance of our methods in extensive simulation studies. Finally, we illustrate our methods with an application to the post-licensure surveillance of vaccine safety among children.},
  archive  = {J},
  author   = {Xu Shi and Wang Miao and Jennifer C. Nelson and Eric J. Tchetgen Tchetgen},
  doi      = {10.1111/rssb.12361},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {2},
  pages    = {521-540},
  title    = {Multiply robust causal inference with double-negative control adjustment for categorical unmeasured confounding},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exchangeable random measures for sparse and modular graphs
with overlapping communities. <em>Journal of the Royal Statistical
Society: Series B (Statistical Methodology)</em>, <em>82</em>(2),
487–520. (<a href="https://doi.org/10.1111/rssb.12363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose a novel statistical model for sparse networks with overlapping community structure. The model is based on representing the graph as an exchangeable point process and naturally generalizes existing probabilistic models with overlapping block structure to the sparse regime. Our construction builds on vectors of completely random measures and has interpretable parameters, each node being assigned a vector representing its levels of affiliation to some latent communities. We develop methods for efficient simulation of this class of random graphs and for scalable posterior inference. We show that the approach proposed can recover interpretable structure of real world networks and can handle graphs with thousands of nodes and tens of thousands of edges.},
  archive  = {J},
  author   = {Adrien Todeschini and Xenia Miscouridou and François Caron},
  doi      = {10.1111/rssb.12363},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {2},
  pages    = {487-520},
  title    = {Exchangeable random measures for sparse and modular graphs with overlapping communities},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sumca: Simple, unified, monte-carlo-assisted approach to
second-order unbiased mean-squared prediction error estimation.
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>82</em>(2), 467–485. (<a
href="https://doi.org/10.1111/rssb.12358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose a simple, unified, Monte-Carlo-assisted approach (called ‘Sumca’) to second-order unbiased estimation of the mean-squared prediction error (MSPE) of a small area predictor. The MSPE estimator proposed is easy to derive, has a simple expression and applies to a broad range of predictors that include the traditional empirical best linear unbiased predictor, empirical best predictor and post-model-selection empirical best linear unbiased predictor and empirical best predictor as special cases. Furthermore, the leading term of the MSPE estimator proposed is guaranteed positive; the lower order term corresponds to a bias correction, which can be evaluated via a Monte Carlo method. The computational burden for the Monte Carlo evaluation is much less, compared with other Monte-Carlo-based methods that have been used for producing second-order unbiased MSPE estimators, such as the double bootstrap and Monte Carlo jackknife. The Sumca estimator also has a nice stability feature. Theoretical and empirical results demonstrate properties and advantages of the Sumca estimator.},
  archive  = {J},
  author   = {Jiming Jiang and Mahmoud Torabi},
  doi      = {10.1111/rssb.12358},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {2},
  pages    = {467-485},
  title    = {Sumca: Simple, unified, monte-carlo-assisted approach to second-order unbiased mean-squared prediction error estimation},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Doubly robust inference when combining probability and
non-probability samples with high dimensional data. <em>Journal of the
Royal Statistical Society: Series B (Statistical Methodology)</em>,
<em>82</em>(2), 445–465. (<a
href="https://doi.org/10.1111/rssb.12354">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We consider integrating a non-probability sample with a probability sample which provides high dimensional representative covariate information of the target population. We propose a two-step approach for variable selection and finite population inference. In the first step, we use penalized estimating equations with folded concave penalties to select important variables and show selection consistency for general samples. In the second step, we focus on a doubly robust estimator of the finite population mean and re-estimate the nuisance model parameters by minimizing the asymptotic squared bias of the doubly robust estimator. This estimating strategy mitigates the possible first-step selection error and renders the doubly robust estimator root n consistent if either the sampling probability or the outcome model is correctly specified.},
  archive  = {J},
  author   = {Shu Yang and Jae Kwang Kim and Rui Song},
  doi      = {10.1111/rssb.12354},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {2},
  pages    = {445-465},
  title    = {Doubly robust inference when combining probability and non-probability samples with high dimensional data},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model misspecification in approximate bayesian computation:
Consequences and diagnostics. <em>Journal of the Royal Statistical
Society: Series B (Statistical Methodology)</em>, <em>82</em>(2),
421–444. (<a href="https://doi.org/10.1111/rssb.12356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We analyse the behaviour of approximate Bayesian computation (ABC) when the model generating the simulated data differs from the actual data-generating process, i.e. when the data simulator in ABC is misspecified. We demonstrate both theoretically and in simple, but practically relevant, examples that when the model is misspecified different versions of ABC can yield substantially different results. Our theoretical results demonstrate that even though the model is misspecified, under regularity conditions, the accept–reject ABC approach concentrates posterior mass on an appropriately defined pseudotrue parameter value. However, under model misspecification the ABC posterior does not yield credible sets with valid frequentist coverage and has non-standard asymptotic behaviour. In addition, we examine the theoretical behaviour of the popular local regression adjustment to ABC under model misspecification and demonstrate that this approach concentrates posterior mass on a pseudotrue value that is completely different from accept–reject ABC. Using our theoretical results, we suggest two approaches to diagnose model misspecification in ABC. All theoretical results and diagnostics are illustrated in a simple running example.},
  archive  = {J},
  author   = {David T. Frazier and Christian P. Robert and Judith Rousseau},
  doi      = {10.1111/rssb.12356},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {2},
  pages    = {421-444},
  title    = {Model misspecification in approximate bayesian computation: Consequences and diagnostics},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semisupervised inference for explained variance in high
dimensional linear regression and its applications. <em>Journal of the
Royal Statistical Society: Series B (Statistical Methodology)</em>,
<em>82</em>(2), 391–419. (<a
href="https://doi.org/10.1111/rssb.12357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The paper considers statistical inference for the explained variance under the high dimensional linear model Y = Xβ + ε in the semisupervised setting, where β is the regression vector and Σ is the design covariance matrix. A calibrated estimator, which efficiently integrates both labelled and unlabelled data, is proposed. It is shown that the estimator achieves the minimax optimal rate of convergence in the general semisupervised framework. The optimality result characterizes how the unlabelled data contribute to the estimation accuracy. Moreover, the limiting distribution for the proposed estimator is established and the unlabelled data have also proved useful in reducing the length of the confidence interval for the explained variance. The method proposed is extended to semisupervised inference for the unweighted quadratic functional . The inference results obtained are then applied to a range of high dimensional statistical problems, including signal detection and global testing, prediction accuracy evaluation and confidence ball construction. The numerical improvement of incorporating the unlabelled data is demonstrated through simulation studies and an analysis of estimating heritability for a yeast segregant data set with multiple traits.},
  archive  = {J},
  author   = {T. Tony Cai and Zijian Guo},
  doi      = {10.1111/rssb.12357},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {2},
  pages    = {391-419},
  title    = {Semisupervised inference for explained variance in high dimensional linear regression and its applications},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Right singular vector projection graphs: Fast high
dimensional covariance matrix estimation under latent confounding.
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>82</em>(2), 361–389. (<a
href="https://doi.org/10.1111/rssb.12359">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We consider the problem of estimating a high dimensional p × p covariance matrix Σ, given n observations of confounded data with covariance , where Γ is an unknown p × q matrix of latent factor loadings. We propose a simple and scalable estimator based on the projection onto the right singular vectors of the observed data matrix, which we call right singular vector projection (RSVP). Our theoretical analysis of this method reveals that, in contrast with approaches based on the removal of principal components, RSVP can cope well with settings where the smallest eigenvalue of is relatively close to the largest eigenvalue of Σ, as well as when the eigenvalues of are diverging fast. RSVP does not require knowledge or estimation of the number of latent factors q , but it recovers Σ only up to an unknown positive scale factor. We argue that this suffices in many applications, e.g. if an estimate of the correlation matrix is desired. We also show that, by using subsampling, we can further improve the performance of the method. We demonstrate the favourable performance of RSVP through simulation experiments and an analysis of gene expression data sets collated by the GTEX consortium.},
  archive  = {J},
  author   = {Rajen D. Shah and Benjamin Frot and Gian-Andrea Thanei and Nicolai Meinshausen},
  doi      = {10.1111/rssb.12359},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {2},
  pages    = {361-389},
  title    = {Right singular vector projection graphs: Fast high dimensional covariance matrix estimation under latent confounding},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sparse principal component analysis via axis-aligned random
projections. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>82</em>(2), 329–359. (<a
href="https://doi.org/10.1111/rssb.12360">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We introduce a new method for sparse principal component analysis, based on the aggregation of eigenvector information from carefully selected axis-aligned random projections of the sample covariance matrix. Unlike most alternative approaches, our algorithm is non-iterative, so it is not vulnerable to a bad choice of initialization. We provide theoretical guarantees under which our principal subspace estimator can attain the minimax optimal rate of convergence in polynomial time. In addition, our theory provides a more refined understanding of the statistical and computational trade-off in the problem of sparse principal component estimation, revealing a subtle interplay between the effective sample size and the number of random projections that are required to achieve the minimax optimal rate. Numerical studies provide further insight into the procedure and confirm its highly competitive finite sample performance.},
  archive  = {J},
  author   = {Milana Gataric and Tengyao Wang and Richard J. Samworth},
  doi      = {10.1111/rssb.12360},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {2},
  pages    = {329-359},
  title    = {Sparse principal component analysis via axis-aligned random projections},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Functional models for time-varying random objects.
<em>Journal of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>82</em>(2), 275–327. (<a
href="https://doi.org/10.1111/rssb.12337">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Functional data analysis provides a popular toolbox of functional models for the analysis of samples of random functions that are real valued. In recent years, samples of time-varying object data such as time-varying networks that are not in a vector space have been increasingly collected. These data can be viewed as elements of a general metric space that lacks local or global linear structure and therefore common approaches that have been used with great success for the analysis of functional data, such as functional principal component analysis, cannot be applied. We propose metric covariance , a novel association measure for paired object data lying in a metric space (Ω, d ) that we use to define a metric autocovariance function for a sample of random Ω-valued curves, where Ω generally will not have a vector space or manifold structure. The proposed metric autocovariance function is non-negative definite when the squared semimetric d 2 is of negative type. Then the eigenfunctions of the linear operator with the autocovariance function as kernel can be used as building blocks for an object functional principal component analysis for Ω-valued functional data, including time-varying probability distributions, covariance matrices and time dynamic networks. Analogues of functional principal components for time-varying objects are obtained by applying Fréchet means and projections of distance functions of the random object trajectories in the directions of the eigenfunctions, leading to real-valued Fréchet scores . Using the notion of generalized Fréchet integrals , we construct object functional principal components that lie in the metric space Ω. We establish asymptotic consistency of the sample-based estimators for the corresponding population targets under mild metric entropy conditions on Ω and continuity of the Ω-valued random curves. These concepts are illustrated with samples of time-varying probability distributions for human mortality, time-varying covariance matrices derived from trading patterns and time-varying networks that arise from New York taxi trips.},
  archive  = {J},
  author   = {Paromita Dubey and Hans-Georg Müller},
  doi      = {10.1111/rssb.12337},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {2},
  pages    = {275-327},
  title    = {Functional models for time-varying random objects},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Correction: “A new randomized response model.” <em>Journal
of the Royal Statistical Society: Series B (Statistical
Methodology)</em>, <em>82</em>(1), 269–271. (<a
href="https://doi.org/10.1111/rssb.12341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We point out a minor mistake in published in 2006, ‘A new randomized response model’, which as been cited by various researchers, though no one has pointed out the mistake.},
  archive  = {J},
  author   = {Lovleen Kumar Grover and Amanpreet Kaur},
  doi      = {10.1111/rssb.12341},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {1},
  pages    = {269-271},
  title    = {Correction: ‘A new randomized response model’},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rerandomization and regression adjustment. <em>Journal of
the Royal Statistical Society: Series B (Statistical Methodology)</em>,
<em>82</em>(1), 241–268. (<a
href="https://doi.org/10.1111/rssb.12353">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Randomization is a basis for the statistical inference of treatment effects without strong assumptions on the outcome-generating process. Appropriately using covariates further yields more precise estimators in randomized experiments. R. A. Fisher suggested blocking on discrete covariates in the design stage or conducting analysis of covariance in the analysis stage. We can embed blocking in a wider class of experimental design called rerandomization, and extend the classical analysis of covariance to more general regression adjustment. Rerandomization trumps complete randomization in the design stage, and regression adjustment trumps the simple difference-in-means estimator in the analysis stage. It is then intuitive to use both rerandomization and regression adjustment. Under the randomization inference framework, we establish a unified theory allowing the designer and analyser to have access to different sets of covariates. We find that asymptotically, for any given estimator with or without regression adjustment, rerandomization never hurts either the sampling precision or the estimated precision, and, for any given design with or without rerandomization, our regression-adjusted estimator never hurts the estimated precision. Therefore, combining rerandomization and regression adjustment yields better coverage properties and thus improves statistical inference. To quantify these statements theoretically, we discuss optimal regression-adjusted estimators in terms of the sampling precision and the estimated precision, and then measure the additional gains of the designer and the analyser. We finally suggest the use of rerandomization in the design and regression adjustment in the analysis followed by the Huber–White robust standard error.},
  archive  = {J},
  author   = {Xinran Li and Peng Ding},
  doi      = {10.1111/rssb.12353},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {1},
  pages    = {241-268},
  title    = {Rerandomization and regression adjustment},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multivariate type g matérn stochastic partial differential
equation random fields. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>82</em>(1), 215–239. (<a
href="https://doi.org/10.1111/rssb.12351">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {For many applications with multivariate data, random-field models capturing departures from Gaussianity within realizations are appropriate. For this reason, we formulate a new class of multivariate non-Gaussian models based on systems of stochastic partial differential equations with additive type G noise whose marginal covariance functions are of Matérn type. We consider four increasingly flexible constructions of the noise, where the first two are similar to existing copula-based models. In contrast with these, the last two constructions can model non-Gaussian spatial data without replicates. Computationally efficient methods for likelihood-based parameter estimation and probabilistic prediction are proposed, and the flexibility of the models suggested is illustrated by numerical examples and two statistical applications.},
  archive  = {J},
  author   = {David Bolin and Jonas Wallin},
  doi      = {10.1111/rssb.12351},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {1},
  pages    = {215-239},
  title    = {Multivariate type g matérn stochastic partial differential equation random fields},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust inference on population indirect causal effects: The
generalized front door criterion. <em>Journal of the Royal Statistical
Society: Series B (Statistical Methodology)</em>, <em>82</em>(1),
199–214. (<a href="https://doi.org/10.1111/rssb.12345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Standard methods for inference about direct and indirect effects require stringent no-unmeasured-confounding assumptions which often fail to hold in practice, particularly in observational studies. The goal of the paper is to introduce a new form of indirect effect, the population intervention indirect effect, that can be non-parametrically identified in the presence of an unmeasured common cause of exposure and outcome. This new type of indirect effect captures the extent to which the effect of exposure is mediated by an intermediate variable under an intervention that holds the component of exposure directly influencing the outcome at its observed value. The population intervention indirect effect is in fact the indirect component of the population intervention effect, introduced by Hubbard and Van der Laan. Interestingly, our identification criterion generalizes Judea Pearl&#39;s front door criterion as it does not require no direct effect of exposure not mediated by the intermediate variable. For inference, we develop both parametric and semiparametric methods, including a novel doubly robust semiparametric locally efficient estimator, that perform very well in simulation studies. Finally, the methods proposed are used to measure the effectiveness of monetary saving recommendations among women enrolled in a maternal health programme in Tanzania.},
  archive  = {J},
  author   = {Isabel R. Fulcher and Ilya Shpitser and Stella Marealle and Eric J. Tchetgen Tchetgen},
  doi      = {10.1111/rssb.12345},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {1},
  pages    = {199-214},
  title    = {Robust inference on population indirect causal effects: The generalized front door criterion},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The conditional permutation test for independence while
controlling for confounders. <em>Journal of the Royal Statistical
Society: Series B (Statistical Methodology)</em>, <em>82</em>(1),
175–197. (<a href="https://doi.org/10.1111/rssb.12340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose a general new method, the conditional permutation test , for testing the conditional independence of variables X and Y given a potentially high dimensional random vector Z that may contain confounding factors. The test permutes entries of X non-uniformly, to respect the existing dependence between X and Z and thus to account for the presence of these confounders. Like the conditional randomization test of Candès and co-workers in 2018, our test relies on the availability of an approximation to the distribution of X | Z —whereas their test uses this estimate to draw new X -values, for our test we use this approximation to design an appropriate non-uniform distribution on permutations of the X -values already seen in the true data. We provide an efficient Markov chain Monte Carlo sampler for the implementation of our method and establish bounds on the type I error in terms of the error in the approximation of the conditional distribution of X | Z , finding that, for the worst-case test statistic, the inflation in type I error of the conditional permutation test is no larger than that of the conditional randomization test. We validate these theoretical results with experiments on simulated data and on the Capital Bikeshare data set.},
  archive  = {J},
  author   = {Thomas B. Berrett and Yi Wang and Rina Foygel Barber and Richard J. Samworth},
  doi      = {10.1111/rssb.12340},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {1},
  pages    = {175-197},
  title    = {The conditional permutation test for independence while controlling for confounders},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian empirical likelihood inference with complex survey
data. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>82</em>(1), 155–174. (<a
href="https://doi.org/10.1111/rssb.12342">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We propose a Bayesian empirical likelihood approach to survey data analysis on a vector of finite population parameters defined through estimating equations. Our method allows overidentified estimating equation systems and is applicable to both smooth and non-differentiable estimating functions. Our proposed Bayesian estimator is design consistent for general sampling designs and the Bayesian credible intervals are calibrated in the sense of having asymptotically valid design-based frequentist properties under single-stage unequal probability sampling designs with small sampling fractions. Large sample properties of the Bayesian inference proposed are established for both non-informative and informative priors under the design-based framework. We also propose a Bayesian model selection procedure with complex survey data and show that it works for general sampling designs. An efficient Markov chain Monte Carlo procedure is described for the required computation of the posterior distribution for general vector parameters. Simulation studies and an application to a real survey data set are included to examine the finite sample performances of the methods proposed as well as the effect of different types of prior and different types of sampling design.},
  archive  = {J},
  author   = {Puying Zhao and Malay Ghosh and J. N. K. Rao and Changbao Wu},
  doi      = {10.1111/rssb.12342},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {1},
  pages    = {155-174},
  title    = {Bayesian empirical likelihood inference with complex survey data},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bayesian hierarchical model for related densities by using
pólya trees. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>82</em>(1), 127–153. (<a
href="https://doi.org/10.1111/rssb.12346">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Bayesian hierarchical models are used to share information between related samples and to obtain more accurate estimates of sample level parameters, common structure and variation between samples. When the parameter of interest is the distribution or density of a continuous variable, a hierarchical model for continuous distributions is required. Various such models have been described in the literature using extensions of the Dirichlet process and related processes, typically as a distribution on the parameters of a mixing kernel. We propose a new hierarchical model based on the Pólya tree, which enables direct modelling of densities and enjoys some computational advantages over the Dirichlet process. The Pólya tree also enables more flexible modelling of the variation between samples, providing more informed shrinkage and permitting posterior inference on the dispersion function, which quantifies the variation between sample densities. We also show how the model can be extended to cluster samples in situations where the observed samples are believed to have been drawn from several latent populations.},
  archive  = {J},
  author   = {Jonathan Christensen and Li Ma},
  doi      = {10.1111/rssb.12346},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {1},
  pages    = {127-153},
  title    = {A bayesian hierarchical model for related densities by using pólya trees},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Targeted sampling from massive block model graphs with
personalized PageRank. <em>Journal of the Royal Statistical Society:
Series B (Statistical Methodology)</em>, <em>82</em>(1), 99–126. (<a
href="https://doi.org/10.1111/rssb.12349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The paper provides statistical theory and intuition for personalized PageRank (called ‘PPR’): a popular technique that samples a small community from a massive network. We study a setting where the entire network is expensive to obtain thoroughly or to maintain, but we can start from a seed node of interest and ‘crawl’ the network to find other nodes through their connections. By crawling the graph in a designed way, the PPR vector can be approximated without querying the entire massive graph, making it an alternative to snowball sampling. Using the degree-corrected stochastic block model, we study whether the PPR vector can select nodes that belong to the same block as the seed node. We provide a simple and interpretable form for the PPR vector, highlighting its biases towards high degree nodes outside the target block. We examine a simple adjustment based on node degrees and establish consistency results for PPR clustering that allows for directed graphs. These results are enabled by recent technical advances showing the elementwise convergence of eigenvectors. We illustrate the method with the massive Twitter friendship graph, which we crawl by using the Twitter application programming interface. We find that the adjusted and unadjusted PPR techniques are complementary approaches, where the adjustment makes the results particularly localized around the seed node, and that the bias adjustment greatly benefits from degree regularization.},
  archive  = {J},
  author   = {Fan Chen and Yini Zhang and Karl Rohe},
  doi      = {10.1111/rssb.12349},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {1},
  pages    = {99-126},
  title    = {Targeted sampling from massive block model graphs with personalized PageRank},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Renewable estimation and incremental inference in
generalized linear models with streaming data sets. <em>Journal of the
Royal Statistical Society: Series B (Statistical Methodology)</em>,
<em>82</em>(1), 69–97. (<a
href="https://doi.org/10.1111/rssb.12352">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {The paper presents an incremental updating algorithm to analyse streaming data sets using generalized linear models. The method proposed is formulated within a new framework of renewable estimation and incremental inference, in which the maximum likelihood estimator is renewed with current data and summary statistics of historical data. Our framework can be implemented within a popular distributed computing environment, known as Apache Spark, to scale up computation. Consisting of two data-processing layers, the rho architecture enables us to accommodate inference-related statistics and to facilitate sequential updating of the statistics used in both estimation and inference. We establish estimation consistency and asymptotic normality of the proposed renewable estimator, in which the Wald test is utilized for an incremental inference. Our methods are examined and illustrated by various numerical examples from both simulation experiments and a real world data analysis.},
  archive  = {J},
  author   = {Lan Luo and Peter X.-K. Song},
  doi      = {10.1111/rssb.12352},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {1},
  pages    = {69-97},
  title    = {Renewable estimation and incremental inference in generalized linear models with streaming data sets},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Making sense of sensitivity: Extending omitted variable
bias. <em>Journal of the Royal Statistical Society: Series B
(Statistical Methodology)</em>, <em>82</em>(1), 39–67. (<a
href="https://doi.org/10.1111/rssb.12348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We extend the omitted variable bias framework with a suite of tools for sensitivity analysis in regression models that does not require assumptions on the functional form of the treatment assignment mechanism nor on the distribution of the unobserved confounders, naturally handles multiple confounders, possibly acting non-linearly, exploits expert knowledge to bound sensitivity parameters and can be easily computed by using only standard regression results. In particular, we introduce two novel sensitivity measures suited for routine reporting. The robustness value describes the minimum strength of association that unobserved confounding would need to have, both with the treatment and with the outcome, to change the research conclusions. The partial R 2 of the treatment with the outcome shows how strongly confounders explaining all the residual outcome variation would have to be associated with the treatment to eliminate the estimated effect. Next, we offer graphical tools for elaborating on problematic confounders, examining the sensitivity of point estimates and t -values, as well as ‘extreme scenarios’. Finally, we describe problems with a common ‘benchmarking’ practice and introduce a novel procedure to bound the strength of confounders formally on the basis of a comparison with observed covariates. We apply these methods to a running example that estimates the effect of exposure to violence on attitudes toward peace.},
  archive  = {J},
  author   = {Carlos Cinelli and Chad Hazlett},
  doi      = {10.1111/rssb.12348},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {1},
  pages    = {39-67},
  title    = {Making sense of sensitivity: Extending omitted variable bias},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiscale inference and long-run variance estimation in
non-parametric regression with time series errors. <em>Journal of the
Royal Statistical Society: Series B (Statistical Methodology)</em>,
<em>82</em>(1), 5–37. (<a
href="https://doi.org/10.1111/rssb.12347">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {We develop new multiscale methods to test qualitative hypotheses about the function m in the non-parametric regression model Y t , T = m ( t / T )+ ɛ t with time series errors ɛ t . In time series applications, m represents a non-parametric time trend. Practitioners are often interested in whether the trend m has certain shape properties. For example, they would like to know whether m is constant or whether it is increasing or decreasing in certain time intervals. Our multiscale methods enable us to test for such shape properties of the trend m . To perform the methods, we require an estimator of the long-run error variance . We propose a new difference-based estimator of σ 2 for the case that { ɛ t } belongs to the class of auto-regressive AR(∞) processes. In the technical part of the paper, we derive asymptotic theory for the proposed multiscale test and the estimator of the long-run error variance. The theory is complemented by a simulation study and an empirical application to climate data.},
  archive  = {J},
  author   = {Marina Khismatullina and Michael Vogt},
  doi      = {10.1111/rssb.12347},
  journal  = {Journal of the Royal Statistical Society: Series B},
  number   = {1},
  pages    = {5-37},
  title    = {Multiscale inference and long-run variance estimation in non-parametric regression with time series errors},
  volume   = {82},
  year     = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Report of the editors—2019. <em>Journal of the Royal
Statistical Society: Series B (Statistical Methodology)</em>,
<em>82</em>(1), 3–4. (<a
href="https://doi.org/10.1111/rssb.12350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive = {J},
  author  = {David Dunson and Simon Wood},
  doi     = {10.1111/rssb.12350},
  journal = {Journal of the Royal Statistical Society: Series B},
  number  = {1},
  pages   = {3-4},
  title   = {Report of the editors—2019},
  volume  = {82},
  year    = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
