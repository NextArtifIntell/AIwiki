<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>COMJNL_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="comjnl---135">COMJNL - 135</h2>
<ul>
<li><details>
<summary>
(2020). The notion of transparency order, revisited.
<em>COMJNL</em>, <em>63</em>(12), 1915–1938. (<a
href="https://doi.org/10.1093/comjnl/bxaa069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We revisit the definition of transparency order (TO) and that of modified transparency order (MTO) as well, which were proposed to measure the resistance of substitution boxes (S-boxes) against differential power analysis (DPA). We spot a definitional flaw in original TO, which is proved to significantly affect the soundness of TO. Regretfully, MTO overlooks this flaw, yet it happens to incur no bad effects on the correctness of MTO, even though the start point of this formulation is highly questionable. It is also this neglect that made MTO consider a variant of multi-bit DPA attack, which was mistakenly thought to appropriately serve as an alternative powerful attack. This implies the soundness of MTO is also more or less arguable. Therefore, we fix this definitional flaw and provide a revised definition named reVisited TO (VTO). For demonstrating validity and soundness of VTO, we present simulated and practical DPA attacks on implementations of ; and ; S-boxes. In addition, we also illustrate the soundness of VTO in masked S-boxes. Furthermore, as a concrete application of VTO, we present the distribution of VTO values of optimal affine equivalence classes of ; S-boxes and give some recommended guidelines on how to select ; S-boxes with higher DPA resistance at the identical level of implementation cost.},
  archive      = {J_COMJNL},
  author       = {Li, Huizhong and Zhou, Yongbin and Ming, Jingdian and Yang, Guang and Jin, Chengbin},
  doi          = {10.1093/comjnl/bxaa069},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {1915-1938},
  shortjournal = {Comput. J.},
  title        = {The notion of transparency order, revisited},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Public-key encryption in the standard model against strong
leakage adversary. <em>COMJNL</em>, <em>63</em>(12), 1904–1914. (<a
href="https://doi.org/10.1093/comjnl/bxaa055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the years, security against adaptively chosen-ciphertext attacks (CCA2) is considered as the strongest security definition for public-key encryption schemes. With the uprise of side-channel attacks, new security definitions are proposed, addressing leakage of secret keys together with the standard CCA2 definition. Among the new security definitions, security against continuous and after-the-fact leakage-resilient CCA2 can be considered as the strongest security definition, which is called as security against (continuous) adaptively chosen-ciphertext leakage attacks (continuous CCLA2). In this paper, we present a construction of a public-key encryption scheme, namely LR-PKE, which satisfies the aforementioned security definition. The security of our public-key encryption scheme is proven in the standard model, under decision BDH assumption. Thus, we emphasize that our public-key encryption scheme LR-PKE is (continuous) CCLA2-secure in the standard model. For our construction of LR-PKE, we have used a strong one-time signature scheme and a leakage-resilient refreshing protocol as underlying building blocks. The leakage bound is ; bits per leakage query, for a security parameter ; and a statistical security parameter ; , such that ; and ; is a function of ; . It is possible to see that LR-PKE is efficient enough to be used for real-world usage.},
  archive      = {J_COMJNL},
  author       = {Alawatugoda, Janaka},
  doi          = {10.1093/comjnl/bxaa055},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {1904-1914},
  shortjournal = {Comput. J.},
  title        = {Public-key encryption in the standard model against strong leakage adversary},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New blind filter protocol: An improved privacy-preserving
scheme for location-based services. <em>COMJNL</em>, <em>63</em>(12),
1886–1903. (<a href="https://doi.org/10.1093/comjnl/bxaa043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Location-based services have attracted much attention in both academia and industry. However, protecting user’s privacy while providing accurate service for users remains challenging. In most of the existing research works, a semi-trusted proxy is employed to act on behalf of a user to minimize the computation and communication costs of the user. However, user privacy, e.g. location privacy, cannot be protected against the proxy. In this paper, we design a new blind filter protocol where a user can employ a semi-trusted proxy to determine whether a point of interest is within a circular area centered at the user’s location. During the protocol, neither the proxy nor the location-based service provider can obtain the location of the user and the query results. Moreover, each type of query is controlled by an access tree and only the users whose attributes satisfy this access tree can complete the specific type of query. Security analysis and efficiency experiments validate that the proposed protocol is secure and efficient in terms of the computation and communication overhead.},
  archive      = {J_COMJNL},
  author       = {Li, Zhidan and Li, Wenmin and Gao, Fei and Yu, Ping and Zhang, Hua and Jin, Zhengping and Wen, Qiaoyan},
  doi          = {10.1093/comjnl/bxaa043},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {1886-1903},
  shortjournal = {Comput. J.},
  title        = {New blind filter protocol: An improved privacy-preserving scheme for location-based services},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Linearly homomorphic signatures from lattices.
<em>COMJNL</em>, <em>63</em>(12), 1871–1885. (<a
href="https://doi.org/10.1093/comjnl/bxaa034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Linearly homomorphic signatures (LHSs) allow any entity to linearly combine a set of signatures and to provide authentication service for the corresponding (combined) data. The public key of the current known LHSs from lattices in the standard model requires ; matrices and ; vectors, where ; is the length of file identifier and ; is the maximum data set size that linear functions support. In this paper, we construct two lattice-based LHS schemes with provable security in the standard model and both schemes can authenticate vectors defined over finite field. First, we present a basic LHS scheme satisfying selective security, based on the full-rank difference hash functions. Second, we modify the chameleon hash function constructed by (Cash, D., Hofheinz, D., Kiltz, E. and Peikert, C. (2010) Bonsai Trees, or How to Delegate a Lattice Basis. In Proc. EUROCRYPT 10, Monaco/French Riviera, May 30 to June 3, pp. 523–552. Springer, Berlin) to construct a linearly homomorphic chameleon hash function (LHCHF), which can be applied to all transformations from selectively secure LHS scheme that authenticates vectors defined over finite field ; (; ) to fully secure one, except for a new one that authenticates vectors defined over a small field. Starting from LHCFH and the basic scheme as above, we obtain a fully secure LHS scheme. Both schemes can be used to sign multiple files and have relatively short public keys consisting of ; matrices and ; vectors.},
  archive      = {J_COMJNL},
  author       = {Lin, Cheng-Jun and Xue, Rui and Yang, Shao-Jun and Huang, Xinyi and Li, Shimin},
  doi          = {10.1093/comjnl/bxaa034},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {1871-1885},
  shortjournal = {Comput. J.},
  title        = {Linearly homomorphic signatures from lattices},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved meet-in-the-middle attacks on reduced-round
deoxys-BC-256. <em>COMJNL</em>, <em>63</em>(12), 1859–1870. (<a
href="https://doi.org/10.1093/comjnl/bxaa028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In ASIACRYPT 2014, Jean ; proposed the authentication encryption scheme Deoxys, which is one of the third-round candidates in CAESAR competition. Its internal block cipher is called Deoxys-BC that adopts the tweakey frame. Deoxys-BC has two versions of the tweakey size that are 256 bits and 384 bits, denoted by Deoxys-BC-256 and Deoxys-BC-384, respectively. In this paper, we revaluate the security of Deoxys-BC-256 against the meet-in-the-middle attack to obtain some new results. First, we append one round at the top and two rounds at the bottom of a 6-round distinguisher to form a 9-round truncated differential path with the probability of ; . Based on it, the adversary can attack 9-round Deoxys-BC-256 with ; chosen plaintext-tweaks, ; encryptions and ; blocks. Second, we construct a new 6.5-round distinguisher to form 10-round attacking path with the probability of ; . On the basis of it, the adversary could attack 10-round Deoxys-BC-256 with ; chosen plaintext-tweaks, ; encryptions and ; blocks. These two attacks improve the previous cryptanalytic results on reduced-round Deoxys-BC-256 against the meet-in-the-middle attack.},
  archive      = {J_COMJNL},
  author       = {Liu, Ya and Shi, Bing and Gu, Dawu and Zhao, Fengyu and Li, Wei and Liu, Zhiqiang},
  doi          = {10.1093/comjnl/bxaa028},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {1859-1870},
  shortjournal = {Comput. J.},
  title        = {Improved meet-in-the-middle attacks on reduced-round deoxys-BC-256},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new framework of IND-CCA secure public key encryption with
keyword search. <em>COMJNL</em>, <em>63</em>(12), 1849–1858. (<a
href="https://doi.org/10.1093/comjnl/bxaa014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of cloud computing, public key encryption with keyword search (PEKS) is an extremely useful cryptographic tool for searching on encryption data, whose strongest security notion is ; (; ). Adballa ; presented a transformation from identity based encryption (IBE) to PEKS in the Theory of Cryptography Conference 2010. This paper proposes a new framework of ; secure PEKS in the standard model. Our main technical tool is a newly introduced notion of smooth projective hash function with key mapping, in which the hash key ; is mapped into another mapping projection key ; besides the classical projection key ; . Finally, we provide an instantiation of our framework based on symmetric eXternal Diffie–Hellman assumption.},
  archive      = {J_COMJNL},
  author       = {Ma, Sha and Huang, Qiong},
  doi          = {10.1093/comjnl/bxaa014},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {1849-1858},
  shortjournal = {Comput. J.},
  title        = {A new framework of IND-CCA secure public key encryption with keyword search},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the general construction of tightly secure identity-based
signature schemes. <em>COMJNL</em>, <em>63</em>(12), 1835–1848. (<a
href="https://doi.org/10.1093/comjnl/bxaa011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A tightly secure scheme has a reduction, where the reduction loss is a small constant. Identity-based signature (IBS) is an important cryptographic primitive, and tightly secure IBS schemes enjoy the advantage that the security parameter can be optimal to achieve a certain security level. General constructions of IBS schemes (Bellare, M., Namprempre, C., and Neven, G. (2004) Security Proofs for Identity-Based Identification and Signature Schemes. In ; , May 2–6, pp. 268–286. Springer, Berlin, Interlaken, Switzerland; Galindo, D., Herranz, J., and Kiltz, E. (2006) On the Generic Construction of Identity-Based Signatures With Additional Properties. In ; , December 3–7, pp. 178–193. Springer, Berlin, Shanghai, China) and their security have been extensively studied. However, the security is not tight and how to generally construct a tightly secure IBS scheme remains unknown. In this paper, we concentrate on the general constructions of IBS schemes. We first take an insight into previous constructions and analyze the reason why it cannot achieve tight security. To further study possible tightly secure constructions, we propose another general construction, which could be seen as a different framework of IBS schemes. Our construction requires two traditional signature schemes, whereas the construction by Bellare ; uses one scheme in a two-round iteration. There are no additional operations in our general construction. Its main advantage is providing the possibility of achieving tight security for IBS schemes in the random oracle model. Combining two known signature schemes, we present an efficient IBS scheme with tight security as an example.},
  archive      = {J_COMJNL},
  author       = {Wu, Ge and Zhao, Zhen and Guo, Fuchun and Susilo, Willy and Zhang, Futai},
  doi          = {10.1093/comjnl/bxaa011},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {1835-1848},
  shortjournal = {Comput. J.},
  title        = {On the general construction of tightly secure identity-based signature schemes},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reusable fuzzy extractor based on the LPN assumption.
<em>COMJNL</em>, <em>63</em>(12), 1826–1834. (<a
href="https://doi.org/10.1093/comjnl/bxaa010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A fuzzy extractor derives uniformly random strings from noisy sources that are neither reliably reproducible nor uniformly random. The basic definition of fuzzy extractor was first formally introduced by Dodis ; and has achieved various applications in cryptographic systems. However, it has been proved that a fuzzy extractor could become totally insecure when the same noisy random source is extracted multiple times. To solve this problem, the reusable fuzzy extractor is proposed. In this paper, we propose the first reusable fuzzy extractor based on the LPN assumption, which is efficient and resilient to linear fraction of errors. Furthermore, our construction serves as an alternative post-quantum reusable fuzzy extractor.},
  archive      = {J_COMJNL},
  author       = {Li, Yiming and Liu, Shengli and Gu, Dawu and Chen, Kefei},
  doi          = {10.1093/comjnl/bxaa010},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {1826-1834},
  shortjournal = {Comput. J.},
  title        = {Reusable fuzzy extractor based on the LPN assumption},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New automatic search method for truncated-differential
characteristics application to midori, SKINNY and CRAFT.
<em>COMJNL</em>, <em>63</em>(12), 1813–1825. (<a
href="https://doi.org/10.1093/comjnl/bxaa004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, using Mixed-Integer Linear Programming, a new automatic search tool for truncated differential characteristic is presented. Our method models the problem of finding a maximal probability truncated differential characteristic, being able to distinguish the cipher from a pseudo-random permutation. Using this method, we analyze Midori64, SKINNY64/X and CRAFT block ciphers, for all of which the existing results are improved. In all cases, the truncated differential characteristic is much more efficient than the (upper bound of) bit-wise differential characteristic proven by the designers, for any number of rounds. More specifically, the highest possible rounds, for which an efficient differential characteristic can exist for Midori64, SKINNY64/X and CRAFT are 6, 7 and 10 rounds, respectively, for which differential characteristics with maximum probabilities of ; , ; and ; (may) exist. Using our new method, we introduce new truncated differential characteristics for these ciphers with respective probabilities ; , ; and ; at the same number of rounds. Moreover, the longest truncated differential characteristics found for SKINNY64/X and CRAFT have 10 and 12 rounds, respectively. This method can be used as a new tool for differential analysis of SPN block ciphers.},
  archive      = {J_COMJNL},
  author       = {Ebrahimi Moghaddam, AmirHossein and Ahmadian, Zahra},
  doi          = {10.1093/comjnl/bxaa004},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {1813-1825},
  shortjournal = {Comput. J.},
  title        = {New automatic search method for truncated-differential characteristics application to midori, SKINNY and CRAFT},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A guess-and-determine attack on SNOW-v stream cipher.
<em>COMJNL</em>, <em>63</em>(12), 1789–1812. (<a
href="https://doi.org/10.1093/comjnl/bxaa003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The 5G mobile communication system is coming with a main objective, known also as IMT-2020, that intends to increase the current data rates up to several gigabits per second. To meet an accompanying demand of the super high-speed encryption, EIA and EEA algorithms face some challenges. The 3GPP standardization organization expects to increase the security level to 256-bit key length, and the international cryptographic field responds actively in cipher designs and standard applications. SNOW-V is such a proposal offered by the SNOW family design team, with a revision of the SNOW 3G architecture in terms of linear feedback shift register (LFSR) and finite state machine (FSM), where the LFSR part is new and operates eight times the speed of the FSM, consisting of two shift registers and each feeding into the other, and the FSM increases to three 128-bit registers and employs two instances of full AES encryption round function for update. It takes a 128-bit IV, employs 896-bit internal state and produces 128-bit keystream blocks. The result is competitive in pure software environment, making use of both AES-NI and AVX acceleration instructions. Thus, the security evaluation of SNOW-V is essential and urgent, since there is scarcely any definite security bound for it. In this paper, we propose a byte-based guess-and-determine attack on SNOW-V with complexity ; using only seven keystream blocks. We first improve the heuristic guessing-path auto-searching algorithm based on dynamic programming by adding initial guessing set, which is iteratively modified by sieving out the unnecessary guessing variables, in order to correct the guessing path according to the cipher structure and finally launch smaller guessing basis. For the specific design, we split all the computing units into bytes and rewrite all the internal operations correspondingly. We establish a backward-clock linear equation system according to the circular construction of the LFSR part. Then we further simplify the equations to adapt to the input requirements of the heuristic guessing-path auto-searching algorithm. Finally, the derived guessing path needs modification for the pre-simplification and post-reduction. This is the first complete guess-and-determine attack on SNOW-V as well as the first specific security evaluation to the full cipher.},
  archive      = {J_COMJNL},
  author       = {Jiao, Lin and Li, Yongqiang and Hao, Yonglin},
  doi          = {10.1093/comjnl/bxaa003},
  journal      = {The Computer Journal},
  number       = {12},
  pages        = {1789-1812},
  shortjournal = {Comput. J.},
  title        = {A guess-and-determine attack on SNOW-V stream cipher},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). EmailDetective: An email authorship identification and
verification model. <em>COMJNL</em>, <em>63</em>(11), 1775–1787. (<a
href="https://doi.org/10.1093/comjnl/bxaa059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emails are often used to illegal cybercrime today, so it is important to verify the identity of the email author. This paper proposes a general model for solving the problem of anonymous email author attribution, which can be used in ; and ; . The first situation is to find the author of an anonymous email among the many suspected targets. Another situation is to verify if an email was written by the sender. This paper extracts features from the email header and email body and analyzes the writing style and other behaviors of email authors. The behaviors of email authors are extracted through a statistical algorithm from email headers. Moreover, the author’s writing style in the email body is extracted by a ; algorithm. This model combines multiple factors to solve the problem of anonymous email author attribution. The experiments proved that the accuracy and other indicators of proposed model are better than other methods. In email authorship verification experiment, our average accuracy, average recall and average F1-score reached 89.9\%. In email authorship identification experiment, our model’s accuracy rate is 98.9\% for 10 authors, 92.9\% for 25 authors and 89.5\% for 50 authors.},
  archive      = {J_COMJNL},
  author       = {Fang, Yong and Yang, Yue and Huang, Cheng},
  doi          = {10.1093/comjnl/bxaa059},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {1775-1787},
  shortjournal = {Comput. J.},
  title        = {EmailDetective: An email authorship identification and verification model},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The construction of a majority-voting ensemble based on the
interrelation and amount of information of features. <em>COMJNL</em>,
<em>63</em>(11), 1756–1774. (<a
href="https://doi.org/10.1093/comjnl/bxz118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduced a new ensemble learning algorithm called VIBES, which is better in terms of performance when compared to 85 machine learning algorithms in WEKA tool. This new algorithm is based on three major processes: (i) making an assumption regarding whether features are dependent on or independent of each other, (ii) computing the amount of information of features when it is assumed that they are dependent on each other and then sorting them in a descending manner based on the amount of information, (iii) speeding up the algorithm by optimizing the forward search algorithm that is used in the construction of the final hypothesis from base learner hypotheses. As a result of these processes, it has been seen in the experiments that choosing the relevant assumption can boost learning performance if features are independent of each other; considering features according to the amount of information provides high accuracy and diversity of base learner models. According to experiment results, the algorithm that has been developed has the highest average classification accuracy rate across the 33 datasets. The highest and the lowest average classification accuracy rates have been found to be 89.80 and 78.03\%, respectively.},
  archive      = {J_COMJNL},
  author       = {Aydın, Fatih and Aslan, Zafer},
  doi          = {10.1093/comjnl/bxz118},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {1756-1774},
  shortjournal = {Comput. J.},
  title        = {The construction of a majority-voting ensemble based on the interrelation and amount of information of features},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Weighed query-specific distance and hybrid NARX neural
network for video object retrieval. <em>COMJNL</em>, <em>63</em>(11),
1738–1755. (<a href="https://doi.org/10.1093/comjnl/bxz113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The technical revolution in the field of video recording using the surveillance videos has increased the amount of the video databases that caused the need for an efficient video management system. This paper proposes a hybrid model using the nearest search algorithm (NSA) and the Levenberg–Marquardt (LM)-based non-linear autoregressive exogenous (NARX) neural network for performing the video object retrieval using the trajectories. Initially, the position of the objects in the video are retrieved using NSA and NARX individually, and they are averaged to determine the position of the object. The positions determined using the hybrid model is compared with the original database, and the trajectories of the objects are retrieved based on the minimum distance, which depends on the weighed query-specific distance. Experiments have been carried out using seven videos taken from the CAVIAR dataset, and the performance of the proposed method is compared with the existing methods. This proposed method found to be better than the existing method with respect to multiple object tracking precision (MOTP), multiple object tracking accuracy (MOTA), average tracking accuracy (ATA), precision, recall and F-measure that results a greater MOTP rate of 0.8796, precision rate of 0.8154, recall rate of 0.8408, the F-measure at a rate of 0.8371, MOTA of 0.8459 and ATA of 0.8324.},
  archive      = {J_COMJNL},
  author       = {Ghuge, C A and Chandra Prakash, V and Ruikar, Sachin D},
  doi          = {10.1093/comjnl/bxz113},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {1738-1755},
  shortjournal = {Comput. J.},
  title        = {Weighed query-specific distance and hybrid NARX neural network for video object retrieval},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Detection of splicing forgery using differential evolution
and wavelet decomposition. <em>COMJNL</em>, <em>63</em>(11), 1727–1737.
(<a href="https://doi.org/10.1093/comjnl/bxz107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we have proposed a computationally efficient algorithm to detect splicing (copy-create) forgery, our proposed method is developed by using differential evolution and wavelet decomposition, the differential evolution algorithm automatically generates customized parameter values of tampered images, and wavelet decomposition is used to process large-size images under block-based framework. Our proposed method is resilient to distortions, such as the addition of Gaussian noise, scaling and compression of the forged images.},
  archive      = {J_COMJNL},
  author       = {Kashyap, Abhishek and Suresh, B and Gupta, Hariom},
  doi          = {10.1093/comjnl/bxz107},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {1727-1737},
  shortjournal = {Comput. J.},
  title        = {Detection of splicing forgery using differential evolution and wavelet decomposition},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hydrographical flow modelling of the river severn using
particle swarm optimization. <em>COMJNL</em>, <em>63</em>(11),
1713–1726. (<a href="https://doi.org/10.1093/comjnl/bxz106">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A model is presented to model hydrographical flow, which we apply to flood forecasting in the River Severn catchment area. The approach uses Particle Swarm Optimization (PSO), a swarm computation heuristic, to produce a predictive model of hydrographical flow. Hydrological flow data from 1980 to 1990 are considered, comprising the daily average flow through the River Severn and its tributaries. PSO models are developed from each year of data and are applied to predict flow in the other 10 years; model performance is shown to be largely independent of the training year, suggesting the catchment system is stable and the approach is robust. Importantly, and in contrast to most of the existing alternatives, flow is derived from data measurements taken 2 days previously, as demanded for early-warning flood prediction. The cross-validated model for prediction of extreme (Q95) events R;  = 0.96, significantly improving upon multiple linear regression R;  = 0.93, the best performing of current existing methods.},
  archive      = {J_COMJNL},
  author       = {Kenny, Ian},
  doi          = {10.1093/comjnl/bxz106},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {1713-1726},
  shortjournal = {Comput. J.},
  title        = {Hydrographical flow modelling of the river severn using particle swarm optimization},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Artificial intelligence-based model for drought prediction
and forecasting. <em>COMJNL</em>, <em>63</em>(11), 1704–1712. (<a
href="https://doi.org/10.1093/comjnl/bxz105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Drought is considered as one of the most extremely destructive natural disasters with catastrophic impact on hydrological balance, agriculture outcome, wildlife habitat and financial budget. Therefore, there is a need for an efficient system to predict and forecast drought situations. There are a number of drought indices to assess the severity of droughts considering different causing factors. Most of them does not take important factors into consideration. Internet of Things (IoT) has demonstrated phenomenal growth and has successfully worked in monitoring environmental conditions. This paper proposes an IoT-enabled fog-based framework for the prediction and forecasting of droughts. At the fog layer, the dimensions of the data are decreased using singular vector decomposition. Artificial neural network with genetic algorithm classifier is used to assess drought severity category to the given event and Holt-Winters method is used to predict the future drought conditions. The proposed system is implemented using datasets from government agencies and it proves its effectiveness in assessing drought severity level.},
  archive      = {J_COMJNL},
  author       = {Kaur, Amandeep and Sood, Sandeep K},
  doi          = {10.1093/comjnl/bxz105},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {1704-1712},
  shortjournal = {Comput. J.},
  title        = {Artificial intelligence-based model for drought prediction and forecasting},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Information propagation and public opinion evolution model
based on artificial neural network in online social network.
<em>COMJNL</em>, <em>63</em>(11), 1689–1703. (<a
href="https://doi.org/10.1093/comjnl/bxz104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a new information dissemination and opinion evolution IPNN (Information Propagation Neural Network) model based on artificial neural network. The feedforward network, feedback network and dynamic evolution algorithms are designed and implemented. Firstly, according to the ‘six degrees separation’ theory of information dissemination, a seven-layer neural network underlying framework with input layer, propagation layer and termination layer is constructed; secondly, the information sharing and information interaction evolution process between nodes are described by using the event information forward propagation algorithm, opinion difference reverse propagation algorithm; finally, the external factors of online social network information dissemination is considered, the impact of external behavior patterns is measured by media public opinion guidance and network structure dynamic update operations. Simulation results show that the proposed new mathematical model reveals the relationship between the state of micro-network nodes and the evolution of macro-network public opinion. It accurately depicts the internal information interaction mechanism and diffusion mechanism in online social network. Furthermore, it reveals the process of network public opinion formation and the nature of public opinion explosion in online social network. It provides a new scientific method and research approach for the study of social network public opinion evolution.},
  archive      = {J_COMJNL},
  author       = {Liu, Xiaoyang and He, Daobing},
  doi          = {10.1093/comjnl/bxz104},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {1689-1703},
  shortjournal = {Comput. J.},
  title        = {Information propagation and public opinion evolution model based on artificial neural network in online social network},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Direction-based spatial skyline for retrieving
arbitrary-shaped surrounding objects. <em>COMJNL</em>, <em>63</em>(11),
1668–1688. (<a href="https://doi.org/10.1093/comjnl/bxz099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retrieval of arbitrary-shaped surrounding data objects has many potential applications in spatial databases including nearby arbitrary-shaped object-of-interests retrieval surrounding a user. In this paper, we propose ; concept to determine directional similarity among spatial data objects. Then, we propose a novel query, called ; (DSS), which retrieves non-dominated arbitrary-shaped surrounding data objects in spatial databases for a user. The proposed DSS query is ; as well as ; . We develop efficient algorithms for processing DSS queries in spatial databases by designing novel data pruning techniques using R-Tree data indexing scheme. Finally, we demonstrate the effectiveness and efficiency of our approach by conducting extensive experiments with real datasets.},
  archive      = {J_COMJNL},
  author       = {Shen, Bojie and Islam, Saiful and Taniar, David},
  doi          = {10.1093/comjnl/bxz099},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {1668-1688},
  shortjournal = {Comput. J.},
  title        = {Direction-based spatial skyline for retrieving arbitrary-shaped surrounding objects},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The NoisyOffice database: A corpus to train supervised
machine learning filters for image processing. <em>COMJNL</em>,
<em>63</em>(11), 1658–1667. (<a
href="https://doi.org/10.1093/comjnl/bxz098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the ‘NoisyOffice’ database. It consists of images of printed text documents with noise mainly caused by uncleanliness from a generic office, such as coffee stains and footprints on documents or folded and wrinkled sheets with degraded printed text. This corpus is intended to train and evaluate supervised learning methods for cleaning, binarization and enhancement of noisy images of grayscale text documents. As an example, several experiments of image enhancement and binarization are presented by using deep learning techniques. Also, double-resolution images are also provided for testing super-resolution methods. The corpus is freely available at UCI Machine Learning Repository. Finally, a challenge organized by Kaggle Inc. to denoise images, using the database, is described in order to show its suitability for benchmarking of image processing systems.},
  archive      = {J_COMJNL},
  author       = {Castro-Bleda, M J and España-Boquera, S and Pastor-Pellicer, J and Zamora-Martínez, F},
  doi          = {10.1093/comjnl/bxz098},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {1658-1667},
  shortjournal = {Comput. J.},
  title        = {The NoisyOffice database: A corpus to train supervised machine learning filters for image processing},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A smart disaster management framework for wildfire detection
and prediction. <em>COMJNL</em>, <em>63</em>(11), 1644–1657. (<a
href="https://doi.org/10.1093/comjnl/bxz091">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wildfires are exorbitantly cataclysmic disasters that lead to the destruction of forest cover, wildlife, land resources, human assets, reduced soil fertility and global warming. Every year wildfires wreck havoc across the globe. Therefore, there is a need of an efficient and reliable system for real-time wildfire monitoring to dilute their disastrous effects. Internet of Things (IoT) has demonstrated remarkable evolution and has been successfully adopted in environmental monitoring domain. Therefore, timely detection and prediction of wildfires is the need of the hour. The proliferation of the IoT has been witnessed in the environment monitoring domain for detection and prediction of several environmental hazards. This research proposes an integrated IoT–fog–cloud framework for real-time detection and prediction of forest fires. Initially, a Bayesian belief network is used to detect the outbreak of wildfire at fog layer followed by real-time alert generation to the forest department offices and fire-fighting stations. Cloud layer-assisted fuzzy-based long-term wildfire prediction and monitoring is responsible for determining the susceptibility of a forest terrain to wildfire outbreak based on wildfire susceptibility index (WSI). Furthermore, WSI is used for risk zone mapping of forest terrains.},
  archive      = {J_COMJNL},
  author       = {Kaur, Harkiran and Sood, Sandeep K},
  doi          = {10.1093/comjnl/bxz091},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {1644-1657},
  shortjournal = {Comput. J.},
  title        = {A smart disaster management framework for wildfire detection and prediction},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Effective user preference clustering in web service
applications. <em>COMJNL</em>, <em>63</em>(11), 1633–1643. (<a
href="https://doi.org/10.1093/comjnl/bxz090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The research on personalized recommendation of Web services plays an important role in the field of Web services technology applications. Fortunately, not all users have completely different service preferences. Due to the same application scenarios and personal interests, some users have the same preferences for certain types of Web services. This paper explores the problem of user clustering in the service environment, grouping users according to their service preferences. It helps service providers to identify and characterize the preferences of similar users and provide them with customized services. We propose two combination-based clustering algorithms which make full use of the advantages of the ; -means algorithm and the affinity propagation algorithm. In addition, a three-stage clustering process is elaborated to improve the accuracy of user clustering. To reduce the time complexity of the algorithms, we create a parallel execution model of the algorithms implemented by a higher-order MapReduce sequence linking technology. Extensive experiments on simulated datasets and real datasets are performed on the comparisons between the proposed algorithms and the other combination-based clustering algorithms. The experimental results substantiate that the proposed algorithms can effectively distinguish user group with different preferences.},
  archive      = {J_COMJNL},
  author       = {Wang, Yan and Zhou, Jian-tao and Li, Xinyuan and Song, Xiaoyu},
  doi          = {10.1093/comjnl/bxz090},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {1633-1643},
  shortjournal = {Comput. J.},
  title        = {Effective user preference clustering in web service applications},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Building a fuzzy logic-based artificial neural network to
uplift recommendation accuracy. <em>COMJNL</em>, <em>63</em>(11),
1624–1632. (<a href="https://doi.org/10.1093/comjnl/bxz086">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of the internet, the recommender system escorts the users in a customized way to nominate items from a massive set of possible alternatives. The emergence of overspecification in recommender system has emphasized negative effects on the context of prediction. The drift of user interest over time is one of the challenging affairs in present personalized recommender system. In this paper, we present a neural network model to improve the recommendation performance along with usage of fuzzy-based clustering to decide membership value of users and matching imputation to cutback sparsity to some extent. We evaluate our model on the MovieLens dataset and show that our model not only elevates accuracy, but also considers the order in which recommendation should be given. We compare the proposed model with a number of state-of-the-art personalization methods and show the dominance of our model using accuracy metrics such as root-mean-square error and mean absolute error.},
  archive      = {J_COMJNL},
  author       = {Sinha, Bam Bahadur and Dhanalakshmi, R},
  doi          = {10.1093/comjnl/bxz086},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {1624-1632},
  shortjournal = {Comput. J.},
  title        = {Building a fuzzy logic-based artificial neural network to uplift recommendation accuracy},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Driving route recommendation with profit maximization in
ride sharing. <em>COMJNL</em>, <em>63</em>(11), 1607–1623. (<a
href="https://doi.org/10.1093/comjnl/bxz075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the positive impact of ride sharing on urban traffic and environment, it has attracted a lot of research attention recently. However, most existing researches focused on the profit maximization or the itinerary minimization of drivers, only rare work has covered on adjustable price function and matching algorithm for the batch requests. In this paper, we propose a request matching algorithm and an adjustable price function that benefits drivers as well as passengers. Our request-matching algorithm consists of an exact search algorithm and a group search algorithm. The exact search algorithm consists of three steps. The first step is to prune some invalid groups according to the total number of passengers and the capacity of vehicles. The second step is to filter out all candidate groups according to the compatibility of requests in same group. The third step is to obtain the most profitable group by the adjustable price function, and recommend the most profitable group to drivers. In order to enhance the efficiency of the exact search algorithm, we further design an improved group search algorithm based on the idea of original simulated annealing. Extensive experimental results show that our method can improve the income of drivers, and reduce the expense of passengers. Meanwhile, ride sharing can also keep the utilization rate of seats 80\%, driving distance is reduced by 30\%.},
  archive      = {J_COMJNL},
  author       = {Huang, Longji and Huang, Jianbin and Xu, Yueshen and Zhao, Zhiqiang and Zhang, Zhenghao},
  doi          = {10.1093/comjnl/bxz075},
  journal      = {The Computer Journal},
  number       = {11},
  pages        = {1607-1623},
  shortjournal = {Comput. J.},
  title        = {Driving route recommendation with profit maximization in ride sharing},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interference and coverage modeling for indoor terahertz
communications with beamforming antennas. <em>COMJNL</em>,
<em>63</em>(10), 1597–1606. (<a
href="https://doi.org/10.1093/comjnl/bxaa083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A general framework to investigate the interference and coverage probability is proposed in this paper for indoor terahertz (THz) communications with beamforming antennas. Due to the multipath effects of THz band (0.1–10 THz), the line of sight and non-line of sight interference from users and access points (APs) (both equipped with beamforming antennas) are separately analyzed based on distance-dependent probability functions. Moreover, to evaluate the effects of obstacles in real applications, a Poisson distribution blockage model is implemented. Moreover, the coverage probability is derived by means of signal to interference plus noise ratio (SINR). Numerical results are conducted to present the interference and coverage probability with different parameters, including the indoor area size, SINR threshold, numbers of interfering users and APs and half-power bandwidth of beamforming antenna.},
  archive      = {J_COMJNL},
  author       = {Wang, Chao-Chao and Wang, Wan-Liang and Yao, Xin-Wei},
  doi          = {10.1093/comjnl/bxaa083},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {1597-1606},
  shortjournal = {Comput. J.},
  title        = {Interference and coverage modeling for indoor terahertz communications with beamforming antennas},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On performance improvement of reversible data hiding with
contrast enhancement. <em>COMJNL</em>, <em>63</em>(10), 1584–1596. (<a
href="https://doi.org/10.1093/comjnl/bxaa072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reversible data hiding (RDH) with contrast enhancement (RDH-CE) is a special type of RDH in improving the subjective visual perception by enhancing the image contrast during the process of data embedding. In RDH-CE, data hiding is achieved via pairwise histogram expansion, and the embedding rate can be increased by performing multiple cycles of histogram expansions. However, when embedding rate gets high, human visible image degradation is observed. Previous work designed an upper bound of the embedding level for RDH-CE, which effectively avoids image over-sharping but offers limited embedding capacity. In this paper, a better tunable bound is designed to enhance the embedding capacity of RDH-CE by exploiting the characteristics of histogram distribution. Furthermore, the objective distortion introduced by histogram pre-shifting is minimized when the embedding level is no more than the upper bound, and the human visible degradation is minimized when the embedding level exceeds the limitation of the proposed upper bound. Experimental results validate that the proposed method provides appropriate upper bound of the embedding level, increases the effective embedding capacity and offers better image contrast.},
  archive      = {J_COMJNL},
  author       = {Chen, Haishan and Yuan, Junying and Hong, Wien and Ni, Jiangqun and Chen, Tung-Shou},
  doi          = {10.1093/comjnl/bxaa072},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {1584-1596},
  shortjournal = {Comput. J.},
  title        = {On performance improvement of reversible data hiding with contrast enhancement},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model-based comparison of cloud-edge computing resource
allocation policies. <em>COMJNL</em>, <em>63</em>(10), 1564–1583. (<a
href="https://doi.org/10.1093/comjnl/bxaa062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid and widespread adoption of internet of things-related services advances the development of the cloud-edge framework, including multiple cloud datacenters (CDCs) and edge micro-datacenters (EDCs). This paper aims to apply analytical modeling techniques to assess the effectiveness of cloud-edge computing resource allocation policies from the perspective of improving the performance of cloud-edge service. We focus on two types of physical device (PD)-allocation policies that define how to select a PD from a CDC/EDC for service provision. The first is randomly selecting a PD, denoted as RandAvail. The other is denoted as SEQ, in which an available idle PD is selected to serve client requests only after the waiting queues of all busy PDs are full. We first present the models in the case of an On–Off request arrival process and verify the approximate accuracy of the proposed models through simulations. Then, we apply analytical models for comparing RandAvail and SEQ policies, in terms of request rejection probability and mean response time, under various system parameter settings.},
  archive      = {J_COMJNL},
  author       = {Jiang, Lili and Chang, Xiaolin and Yang, Runkai and Mišić, Jelena and Mišić, Vojislav B},
  doi          = {10.1093/comjnl/bxaa062},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {1564-1583},
  shortjournal = {Comput. J.},
  title        = {Model-based comparison of cloud-edge computing resource allocation policies},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). QoS enhancement in VoD systems: Load management and
replication policy optimization perspectives. <em>COMJNL</em>,
<em>63</em>(10), 1547–1563. (<a
href="https://doi.org/10.1093/comjnl/bxaa060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The amount of online video content is exponentially increasing, which spurs its access demands. Providing optimal quality of service (QoS) for this ever-increasing video data is a challenging task due to the number of QoS constraints. The system resources, the distributed system platform and the transport protocol thus all need to collaborate to guarantee an acceptable level of QoS for the optimal video streaming process. In this paper, we present a comprehensive survey on QoS management for the video-on-demand systems. First, we focus on load management and replication algorithms in content delivery networks and peer-to-peer (P2P) networks for their shortcomings. We also address the problem of admission control and resource allocation with the objectives of congestion avoidance and frame-loss reduction. Besides, we introduce and discuss various replication schemes. For both the client–server architecture and P2P networks, we highlight the need for a specific storage management policy to preserve system reliability and content availability. We also focus on content distribution and streaming protocols scaling. We deduce that content availability is linked to the characteristics and the performance of the streaming protocols. Finally, we create a comparison table that presents the different contributions of the discussed approaches as well as their limitations. We believe that such a comprehensive survey provides useful insights and contributes to the related domains.},
  archive      = {J_COMJNL},
  author       = {Alaya, Bechir and Khan, Rehanullah},
  doi          = {10.1093/comjnl/bxaa060},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {1547-1563},
  shortjournal = {Comput. J.},
  title        = {QoS enhancement in VoD systems: Load management and replication policy optimization perspectives},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal slack stealing servicing for real-time energy
harvesting systems. <em>COMJNL</em>, <em>63</em>(10), 1537–1546. (<a
href="https://doi.org/10.1093/comjnl/bxaa047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of real-time scheduling in uniprocessor devices powered by energy harvesters. In particular, we focus on mixed sets of tasks with time and energy constraints: hard deadline periodic tasks and soft aperiodic tasks without deadlines. We present an optimal aperiodic servicing algorithm that minimizes the response times of aperiodic tasks without compromising the schedulability of hard deadline periodic tasks. The server, called Slack Stealing with energy Preserving (SSP), is designed based on a slack stealing mechanism that profits whenever possible from available spare processing time and energy. We analytically establish the optimality of SSP. Our simulation results validate our theoretical analysis.},
  archive      = {J_COMJNL},
  author       = {El Osta, Rola and Chetto, Maryline and El Ghor, Hussein},
  doi          = {10.1093/comjnl/bxaa047},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {1537-1546},
  shortjournal = {Comput. J.},
  title        = {Optimal slack stealing servicing for real-time energy harvesting systems},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Virtual block group: A scalable blockchain model with
partial node storage and distributed hash table. <em>COMJNL</em>,
<em>63</em>(10), 1524–1536. (<a
href="https://doi.org/10.1093/comjnl/bxaa046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The inability to scale is one of the most concerning problems looming in blockchain systems, where every node has to store all contents of the ledger database locally, leading to centralization and higher operation costs. In this paper, we propose a model named virtual block group (VBG), which aims at addressing the node storage scalability problem. Adopting the VBG model, each node only needs to store part of block data and saves the VBG storage index to distributed hash table by taking block data as a resource, thus improving the query efficiency of block data. With the incentive mechanism of block data storage, and the storage verification and audit mechanism of block data, the security and reliability of block data storage can be ensured. The analysis and calculation show that this model saves hard drive storage space of the node to a greater extent with a shorter time of requesting block data, in the premise of ensuring secure and reliable block data. Compared to other technologies such as sharding, our model does not change the consensus mechanism or the network topology and retains the reliability and security of the original blockchain system.},
  archive      = {J_COMJNL},
  author       = {Yu, Bin and Li, Xiaofeng and Zhao, He},
  doi          = {10.1093/comjnl/bxaa046},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {1524-1536},
  shortjournal = {Comput. J.},
  title        = {Virtual block group: A scalable blockchain model with partial node storage and distributed hash table},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel hybrid UE selection scheme for efficient data
offloading using D2D communication. <em>COMJNL</em>, <em>63</em>(10),
1513–1523. (<a href="https://doi.org/10.1093/comjnl/bxaa041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exponential growth in mobile broadband data traffic with demand for faster data connectivity has become the most engaging challenges for mobile operators. They are facing an enormous data load in the core network and are finding new solutions to offload data to other complementary technologies. Mobile data offloading using device-to-device (D2D) communication stands out as the promising and the low-cost solution to reduce the burden on cellular network. Data offloading is the process of reducing the load in the cellular medium by using alternative wireless technologies for bearing data using opportunistic assignment of nodes. In this paper, iNHeRENT, a Novel HybRid user equipment (UE) selection scheme using D2D communication in next generation wireless networks that provides better offloading efficiency and throughput than the existing schemes, is proposed. Here, a small set of Wi-Fi-enabled hybrid user equipment (; *) is chosen to offload cellular data in an efficient way. The objective of the work is to use minimum number of ; * to cover maximum number of UE in the serving area of an evolved Node B and to offload maximum amount of data. A ; * is a special UE with both cellular and Wi-Fi interfaces enabled to offload data. The coverage, throughput, packet delivery ratio and offloading efficiency metrics for the selected number of ; * are considered, and it is found that an offloading efficiency of 95.45\% was achieved for a minimum number of 7\% ; * using iNHeRENT.},
  archive      = {J_COMJNL},
  author       = {C G, Balaji and A, Anu Monisha and K, Murugan},
  doi          = {10.1093/comjnl/bxaa041},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {1513-1523},
  shortjournal = {Comput. J.},
  title        = {A novel hybrid UE selection scheme for efficient data offloading using D2D communication},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RGIM: An integrated approach to improve QoS in AODV, DSR and
DSDV routing protocols for FANETS using the chain mobility model.
<em>COMJNL</em>, <em>63</em>(10), 1500–1512. (<a
href="https://doi.org/10.1093/comjnl/bxaa040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Flying ; networks (FANETs) are a collection of unmanned aerial vehicles that communicate without any predefined infrastructure. FANET, being one of the most researched topics nowadays, finds its scope in many complex applications like drones used for military applications, border surveillance systems and other systems like civil applications in traffic monitoring and disaster management. Quality of service (QoS) performance parameters for routing e.g. delay, packet delivery ratio, jitter and throughput in FANETs are quite difficult to improve. Mobility models play an important role in evaluating the performance of the routing protocols. In this paper, the integration of two selected mobility models, i.e. random waypoint and Gauss–Markov model, is implemented. As a result, the random Gauss integrated model is proposed for evaluating the performance of AODV (; on-demand distance vector), DSR (dynamic source routing) and DSDV (destination-Sequenced distance vector) routing protocols. The simulation is done with an NS2 simulator for various scenarios by varying the number of nodes and taking low- and high-node speeds of 50 and 500, respectively. The experimental results show that the proposed model improves the QoS performance parameters of AODV, DSR and DSDV protocol.},
  archive      = {J_COMJNL},
  author       = {Kaur, Parampreet and Singh, Ashima and Gill, Sukhpal Singh},
  doi          = {10.1093/comjnl/bxaa040},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {1500-1512},
  shortjournal = {Comput. J.},
  title        = {RGIM: An integrated approach to improve QoS in AODV, DSR and DSDV routing protocols for FANETS using the chain mobility model},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hierarchical identity-based signature in polynomial rings.
<em>COMJNL</em>, <em>63</em>(10), 1490–1499. (<a
href="https://doi.org/10.1093/comjnl/bxaa033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical identity-based signature (HIBS) plays a core role in a large community as it significantly reduces the workload of the root private key generator. To make HIBS still available and secure in post-quantum era, constructing lattice-based schemes is a promising option. In this paper, we present an efficient HIBS scheme in polynomial rings. Although there are many lattice-based signatures proposed in recent years, to the best of our knowledge, our HIBS scheme is the first ring-based construction. In the center of our construction are two new algorithms to extend lattice trapdoors to higher dimensions, which are non-trivial and of independent interest. With these techniques, the security of the new scheme can be proved, assuming the hardness of the Ring-SIS problem. Since operations in the ring setting are much faster than those over integers and the new construction is the first ring-base HIBS scheme, our scheme is more efficient and practical in terms of computation and storage cost when comparing to the previous constructions.},
  archive      = {J_COMJNL},
  author       = {Yang, Zhichao and Duong, Dung H and Susilo, Willy and Yang, Guomin and Li, Chao and Chen, Rongmao},
  doi          = {10.1093/comjnl/bxaa033},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {1490-1499},
  shortjournal = {Comput. J.},
  title        = {Hierarchical identity-based signature in polynomial rings},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rider-rank algorithm-based feature extraction for re-ranking
the webpages in the search engine. <em>COMJNL</em>, <em>63</em>(10),
1479–1489. (<a href="https://doi.org/10.1093/comjnl/bxaa032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The webpage re-ranking is a challenging task while retrieving the webpages based on the query of the user. Even though the webpages in the search engines are ordered depends on the importance of the content, retrieving the necessary documents based on the input query is quite difficult. Hence, it is required to re-rank the webpages available in the websites based on the features of the pages in the search engines, like Google and Bing. Thus, an effective Rider-Rank algorithm is proposed to re-rank the webpages based on the Rider Optimization Algorithm (ROA). The input queries are forwarded to different search engines, and the webpages generated from the search engines with respect to the input query are gathered. Initially, the keywords are generated for the webpages. Then, the top keyword is selected, and the features are extracted from the top keyword using factor-based, text-based and rank-based features of the webpage. Finally, the webpages are re-ranked using the Rider-Rank algorithm. The performance of the proposed approach is analyzed based on the metrics, such as F-measure, recall and precision. From the analysis, it can be shown that the proposed algorithm obtains the F-measure, recall and precision of 0.90, 0.98 and 0.84, respectively.},
  archive      = {J_COMJNL},
  author       = {Sankpal, Lata Jaywant and Patil, Suhas H},
  doi          = {10.1093/comjnl/bxaa032},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {1479-1489},
  shortjournal = {Comput. J.},
  title        = {Rider-rank algorithm-based feature extraction for re-ranking the webpages in the search engine},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Who face me? Neighbor identification with LED camera
communication. <em>COMJNL</em>, <em>63</em>(10), 1463–1478. (<a
href="https://doi.org/10.1093/comjnl/bxaa021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neighbor discovery is a fundamental task to support many other services in wireless multihop networks (WMN). Most existing related methods in WMN rely heavily on the information of radio waves. To extend the way to the neighbor discovery, this paper introduces another interesting way, visual light communication, to explore its property of linear transmission. We apply light emitting diode (LED) array and camera and present a novel communication system, named LED array to camera system (LC). This paper also designs a novel protocol, named LED to camera protocol (LCP), for it. Equipped with an LC, each node in a WMN can determine the precise direction and distance of its neighbors and recognize their identities. Furthermore, this paper also develops a method to infer the topology of the whole network. We design the hardware for LC and conduct extensive experiments to implement the protocol LCP. The average latency of the neighbor discovery is measured and can be as small as 1.087 seconds. LCP can achieve centimeter-level accuracy in distance and direction to averages of 0.37 cm and 1.67 degrees in these real experiments. The relative accuracies in distance and direction measurement are 99.11\% and 88.92\% on average, respectively. The simulation of the topology inference is also performed to show the feasibility and accuracy with the local distance and direction information.},
  archive      = {J_COMJNL},
  author       = {Zhang, Jianhui and Zhang, Tianhao and Jiang, Feilong and Zhao, Bei},
  doi          = {10.1093/comjnl/bxaa021},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {1463-1478},
  shortjournal = {Comput. J.},
  title        = {Who face me? neighbor identification with LED camera communication},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hidden markov model-based load balancing in data center
networks. <em>COMJNL</em>, <em>63</em>(10), 1449–1462. (<a
href="https://doi.org/10.1093/comjnl/bxz142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern data centers provide multiple parallel paths for end-to-end communications. Recent studies have been done on how to allocate rational paths for data flows to increase the throughput of data center networks. A centralized load balancing algorithm can improve the rationality of the path selection by using path bandwidth information. However, to ensure the accuracy of the information, current centralized load balancing algorithms monitor all the link bandwidth information in the path to determine the path bandwidth. Due to the excessive link bandwidth information monitored by the controller, however, much time is consumed, which is unacceptable for modern data centers. This paper proposes an algorithm called hidden Markov Model-based Load Balancing (HMMLB). HMMLB utilizes the hidden Markov Model (HMM) to select paths for data flows with fewer monitored links, less time cost, and approximate the same network throughput rate as a traditional centralized load balancing algorithm. To generate HMMLB, this research first turns the problem of path selection into an HMM problem. Secondly, deploying traditional centralized load balancing algorithms in the data center topology to collect training data. Finally, training the HMM with the collected data. Through simulation experiments, this paper verifies HMMLB’s effectiveness.},
  archive      = {J_COMJNL},
  author       = {He, Binjie and Zhang, Dong and Zhao, Chang},
  doi          = {10.1093/comjnl/bxz142},
  journal      = {The Computer Journal},
  number       = {10},
  pages        = {1449-1462},
  shortjournal = {Comput. J.},
  title        = {Hidden markov model-based load balancing in data center networks},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Diagnosability of the cayley graph generated by complete
graph with missing edges under the MM<span
class="math inline"><sup>*</sup></span> model. <em>COMJNL</em>,
<em>63</em>(9), 1438–1447. (<a
href="https://doi.org/10.1093/comjnl/bxz096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diagnosability of a multiprocessor system is an important research topic. The system and an interconnection network have an underlying topology, which is usually presented by a graph. Under the Maeng and Malek&#39;s (MM) model, to diagnose the system, a node sends the same task to two of its neighbors, and then compares their responses. The MM; is a special case of the MM model and each node must test all pairs of its adjacent nodes. In 2009, Chiang and Tan (Using node diagnosability to determine ; -diagnosability under the comparison diagnosis (cd) model. ; , ; , 251–259) proposed a new viewpoint for fault diagnosis of the system, namely, the node diagnosability. As a new topology structure of interconnection networks, the nest graph ; has many good properties. In this paper, we study the local diagnosability of ; and show it has the strong local diagnosability property even if there exist ; missing edges in it under the MM; model, and the result is optimal with respect to the number of missing edges.},
  archive      = {J_COMJNL},
  author       = {Ren, Yunxia and Wang, Shiying},
  doi          = {10.1093/comjnl/bxz096},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {1438-1447},
  shortjournal = {Comput. J.},
  title        = {Diagnosability of the cayley graph generated by complete graph with missing edges under the MM$^{\ast }$ model},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Relative assortativity index: A quantitative metric to
assess the impact of link prediction techniques on assortativity of
complex networks. <em>COMJNL</em>, <em>63</em>(9), 1417–1437. (<a
href="https://doi.org/10.1093/comjnl/bxz089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a quantitative metric (called relative assortativity index, ; ) to assess the extent with which a real-world network would become relatively more assortative due to link addition(s) using a link prediction technique. Our methodology is as follows: for a link prediction technique applied on a particular real-world network, we keep track of the assortativity index values incurred during the sequence of link additions until there is negligible change in the assortativity index values for successive link additions. We count the number of network instances for which the assortativity index after a link addition is greater or lower than the assortativity index prior to the link addition and refer to these counts as ; and ; , respectively. RAI is computed as (relative assortativity count − relative dissortativity count) / (relative assortativity count + relative dissortativity count). We analyzed a suite of 80 real-world networks across different domains using 3 representative neighborhood-based link prediction techniques (Preferential attachment, Adamic Adar and Jaccard coefficients [JACs]). We observe the RAI values for the JAC technique to be positive and larger for several real-world networks, while most of the biological networks exhibited positive RAI values for all the three techniques.},
  archive      = {J_COMJNL},
  author       = {Meghanathan, Natarajan},
  doi          = {10.1093/comjnl/bxz089},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {1417-1437},
  shortjournal = {Comput. J.},
  title        = {Relative assortativity index: A quantitative metric to assess the impact of link prediction techniques on assortativity of complex networks},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Note on applications of linearly many faults.
<em>COMJNL</em>, <em>63</em>(9), 1406–1416. (<a
href="https://doi.org/10.1093/comjnl/bxz088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most graphs have this property: after removing a linear number of vertices from a graph, the surviving graph is either connected or consists of a large connected component and small components containing a small number of vertices. This property can be applied to derive fault-tolerance related network parameters: extra edge connectivity and component edge connectivity. Using this general property, we obtained the ; -extra edge connectivity and ; -component edge connectivity of augmented cubes, Cayley graphs generated by transposition trees, complete cubic networks (including hierarchical cubic networks), generalized exchanged hypercubes (including exchanged hypercubes) and dual-cube-like graphs (including dual cubes).},
  archive      = {J_COMJNL},
  author       = {Gu, Mei-Mei and Hao, Rong-Xia and Cheng, Eddie},
  doi          = {10.1093/comjnl/bxz088},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {1406-1416},
  shortjournal = {Comput. J.},
  title        = {Note on applications of linearly many faults},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Characterization of diagnosabilities on the bounded PMC
model. <em>COMJNL</em>, <em>63</em>(9), 1397–1405. (<a
href="https://doi.org/10.1093/comjnl/bxz083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a new digragh model for system level fault diagnosis, which is called the ; -bounded Preparata–Metze–Chien (PMC) model (shortly, ; -BPMC). The ; -BPMC model projects a system such that the number of faulty processors that test faulty processors with the test results ; does not exceed ; provided that the upper bound on the number of faulty processors is ; . This novel testing model compromisingly generalizes PMC model (Preparata, F.P., Metze, G. and Chien R.T. (1967) On the connection assignment problem of diagnosable systems. ; , 848–854) and Barsi–Grandoni–Maestrini model (Barsi, F., Grandoni, F. and Maestrini, P. (1976) A theory of diagnosability of digital systems. ; , 585–593). Then we present some characterizations for one-step diagnosibility under the ; -bounded PMC model, and determine the diagnosabilities of some special regular networks. Meanwhile, we establish the characterizations of ; -diagnosability and three configurations of ; -diagnosable system under the ; -BPMC model.},
  archive      = {J_COMJNL},
  author       = {Lian, Guanqin and Zhou, Shuming and Hsieh, Sun-Yuan and Chen, Gaolin and Liu, Jiafei and Gu, Zhendong},
  doi          = {10.1093/comjnl/bxz083},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {1397-1405},
  shortjournal = {Comput. J.},
  title        = {Characterization of diagnosabilities on the bounded PMC model},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hitting times for random walks on sierpiński graphs and
hierarchical graphs. <em>COMJNL</em>, <em>63</em>(9), 1385–1396. (<a
href="https://doi.org/10.1093/comjnl/bxz080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Sierpiński graphs and hierarchical graphs are two much studied self-similar networks, both of which are iteratively constructed and have the same number of vertices and edges at any iteration, but display entirely different topological properties. Both graphs have a large variety of applications: Sierpiński graphs have a close connection with WK-recursive networks that are employed extensively in the design and implementation of local area networks and parallel processing architectures, while hierarchical graphs can be used to model complex networks. In this paper, we study hitting times for several absorbing random walks in Sierpiński graphs and hierarchical graphs. For all considered random walks, we determine exact solutions to hitting times for both graphs. The obtained explicit expressions indicate that the hitting times in both graphs behave quite differently. We show that the structural difference of the graphs is responsible for the disparate behaviors of their hitting times.},
  archive      = {J_COMJNL},
  author       = {Qi, Yi and Dong, Yuze and Zhang, Zhongzhi and Zhang, Zhang},
  doi          = {10.1093/comjnl/bxz080},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {1385-1396},
  shortjournal = {Comput. J.},
  title        = {Hitting times for random walks on sierpiński graphs and hierarchical graphs},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A kind of conditional vertex connectivity of cayley graphs
generated by wheel graphs. <em>COMJNL</em>, <em>63</em>(9), 1372–1384.
(<a href="https://doi.org/10.1093/comjnl/bxz077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let ; be a connected graph. A subset ; is called an ; -vertex-cut, if ; is disconnected and each vertex in ; has at least ; neighbors in ; . The cardinality of a minimum ; -vertex-cut is the ; -vertex-connectivity of ; and is denoted by ; . ; -vertex-connectivity is a new measure to study the fault tolerance of network structures beyond connectivity. In this paper, we study ; -vertex-connectivity and ; -vertex-connectivity of Cayley graphs generated by wheel graphs, which are denoted by ; , and show that ; for ; for ; .},
  archive      = {J_COMJNL},
  author       = {Luo, Zuwen and Xu, Liqiong},
  doi          = {10.1093/comjnl/bxz077},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {1372-1384},
  shortjournal = {Comput. J.},
  title        = {A kind of conditional vertex connectivity of cayley graphs generated by wheel graphs},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). 1,2,3-restricted connectivity of <span
class="math inline">(<em>n</em>, <em>k</em>)</span>-enhanced hypercubes.
<em>COMJNL</em>, <em>63</em>(9), 1355–1371. (<a
href="https://doi.org/10.1093/comjnl/bxz071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The connectivity of a graph is a classic measure for fault tolerance of the network. Restricted connectivity measure is a crucial subject for a multiprocessor system’s ability to tolerate fault processors, and improves the connectivity measurement accuracy. Furthermore, if a network possesses a restricted connectivity property, it is more reliable with a lower vertex failure rate compared with other networks. The ; -dimensional enhanced hypercube, denoted by ; , a variant of hypercube, which is a well-known interconnection network. In this paper, we analyze the fault tolerant properties for ; -enhanced hypercube, and establish the ; -restricted connectivity of ; and ; -restricted connectivity of ; -enhanced hypercube ; . Furthermore, we propose the tight upper bound of ; -restricted connectivity of ; . Moreover, we show many figures to better illustrate the process of the proofs.},
  archive      = {J_COMJNL},
  author       = {Yu, Hui and Yang, Jiejie and Lin, Limei and Huang, Yanze and Li, Jine and Chen, Riqing},
  doi          = {10.1093/comjnl/bxz071},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {1355-1371},
  shortjournal = {Comput. J.},
  title        = {{1,2,3}-restricted connectivity of $(n,k)$-enhanced hypercubes},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive replica creation and selection strategies for
latency-aware application in collaborative edge-cloud system.
<em>COMJNL</em>, <em>63</em>(9), 1338–1354. (<a
href="https://doi.org/10.1093/comjnl/bxz070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are many research problems in cloud replica management such as low data reliability, unbalanced node load and large resource consumption. The strategy and status of replica creation, replica placement and replica selection are analyzed. The replica creation based on access tendency (DRC-AT), the replica placement based on user request response time and storage capacity (DRP-RS) and the replica selection based on response time (DRS-RT) are proposed. The DRC-AT algorithm introduces the two parameters of file popularity and period value of file popularity, calculates the file access tendency periodically and decides the creation and deletion of the replica of the file according to the size of the file access tendency. The DRP-RS algorithm evaluates the user’s request response time and storage capacity to select the best node set to place the replica. The DRS-RT algorithm returns to the user the node with the strongest service capability that contains the user’s requested data. Experiments show that the algorithm can improve the speed of data reading by the client, improve the resource utilization, balance the load of the node and improve the overall performance of the system.},
  archive      = {J_COMJNL},
  author       = {Li, Chunlin and Zhang, YiHan and Luo, Youlong},
  doi          = {10.1093/comjnl/bxz070},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {1338-1354},
  shortjournal = {Comput. J.},
  title        = {Adaptive replica creation and selection strategies for latency-aware application in collaborative edge-cloud system},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). NuDist: An efficient local search algorithm for (weighted)
partial MaxSAT. <em>COMJNL</em>, <em>63</em>(9), 1321–1337. (<a
href="https://doi.org/10.1093/comjnl/bxz063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Maximum satisfiability (MaxSAT) is the optimization version of the satisfiability (SAT). Partial MaxSAT (PMS) generalizes SAT and MaxSAT by introducing hard and soft clauses, while Weighted PMS (WPMS) is the weighted version of PMS where each soft clause has a weight. These two problems have many important real-world applications. Local search is a popular method for solving (W)PMS. Recently, significant progress has been made in this direction by tailoring local search for (W)PMS, and a representative algorithm is the Dist algorithm. In this paper, we propose two ideas to improve Dist, including a clause-weighting scheme and a variable-selection heuristic. The resulting algorithm is called NuDist. Extensive experiments on PMS and WPMS benchmarks from the MaxSAT Evaluations (MSE) 2016 and 2017 show that NuDist significantly outperforms state-of-the-art local search solvers and performs better than state-of-the-art complete solvers including Open-WBO and WPM3 on MSE 2017 benchmarks. Also, empirical analyses confirm the effectiveness of the proposed ideas.},
  archive      = {J_COMJNL},
  author       = {Lei, Zhendong and Cai, Shaowei},
  doi          = {10.1093/comjnl/bxz063},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {1321-1337},
  shortjournal = {Comput. J.},
  title        = {NuDist: An efficient local search algorithm for (Weighted) partial MaxSAT},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On computing component (edge) connectivities of balanced
hypercubes. <em>COMJNL</em>, <em>63</em>(9), 1311–1320. (<a
href="https://doi.org/10.1093/comjnl/bxz058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For an integer ; , the ; -component connectivity (resp. ; -component edge connectivity) of a graph ; , denoted by ; (resp. ; ), is the minimum number of vertices (resp. edges) whose removal from ; results in a disconnected graph with at least ; components. The two parameters naturally generalize the classical connectivity and edge connectivity of graphs defined in term of the minimum vertex-cut and the minimum edge-cut, respectively. The two kinds of connectivities can help us to measure the robustness of the graph corresponding to a network. In this paper, by exploring algebraic and combinatorial properties of ; -dimensional balanced hypercubes ; , we obtain the ; -component (edge) connectivity ; (; ). For ; -component connectivity, we prove that ; for ; , ; for ; , ; for ; . For ; -component edge connectivity, we prove that ; , ; for ; and ; for ; . Moreover, we also prove ; for ; and the upper bound of ; we obtained is tight for ; .},
  archive      = {J_COMJNL},
  author       = {Gu, Mei-Mei and Chang, Jou-Ming and Hao, Rong-Xia},
  doi          = {10.1093/comjnl/bxz058},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {1311-1320},
  shortjournal = {Comput. J.},
  title        = {On computing component (Edge) connectivities of balanced hypercubes},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Observation tree approach: Active learning relying on
testing. <em>COMJNL</em>, <em>63</em>(9), 1298–1310. (<a
href="https://doi.org/10.1093/comjnl/bxz056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The correspondence of active learning and testing of finite-state machines (FSMs) has been known for a while; however, it was not utilized in the learning. We propose a new framework called the observation tree approach that allows one to use the testing theory to improve the performance of active learning. The improvement is demonstrated on three novel learning algorithms that implement the observation tree approach. They outperform the standard learning algorithms, such as the L* algorithm, in the setting where a minimally adequate teacher provides counterexamples. Moreover, they can also significantly reduce the dependency on the teacher using the assumption of extra states that is well-established in the testing of FSMs. This is immensely helpful as a teacher does not have to be available if one learns a model of a black box, such as a system only accessible via a network.},
  archive      = {J_COMJNL},
  author       = {Soucha, Michal and Bogdanov, Kirill},
  doi          = {10.1093/comjnl/bxz056},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {1298-1310},
  shortjournal = {Comput. J.},
  title        = {Observation tree approach: Active learning relying on testing},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An efficient framework for a third party auditor in cloud
computing environments. <em>COMJNL</em>, <em>63</em>(9), 1285–1297. (<a
href="https://doi.org/10.1093/comjnl/bxz045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud Service Providers supply services to clients in terms of their demands. They need to be constantly under monitoring for their services with respect to consensus agreements between clients and service providers. A Third Party Auditor or TPA as a trusted organization appears to be necessary to monitor executing agreements of cloud services. Using a third party as an extra component creates cost overheads for clients in a cloud environment. Thus, introducing a cost efficient framework for a cloud environment which includes a third party is an eminent achievement to make a TPA feasible and practical in cloud environments. In this paper, we propose a TPA framework for monitoring service level agreements between cloud service providers and cloud clients using several cloud resources. This framework employs different types of service deployments from various cloud service providers excluding the cloud service provider which is being monitored. Then, we demonstrate that the framework can mitigate costs of a third party auditor in a cloud environment. Simulations of trends for costs exhibits cost efficiency of at least forty percent over ten years when a TPA follows our proposed framework in comparison to other frameworks. Finally, we provide an analysis to compare characteristics of our framework with other frameworks and discuss the advantages of our proposed framework. Our results indicate that TPA as a component of the framework not only reduces overall costs of its presentation in a cloud environment but additionally improves management efficiency and security.},
  archive      = {J_COMJNL},
  author       = {Mahdavi-Hezavehi, S and Alimardani, Y and Rahmani, R},
  doi          = {10.1093/comjnl/bxz045},
  journal      = {The Computer Journal},
  number       = {9},
  pages        = {1285-1297},
  shortjournal = {Comput. J.},
  title        = {An efficient framework for a third party auditor in cloud computing environments},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Secure outsourcing algorithms for composite modular
exponentiation based on single untrusted cloud. <em>COMJNL</em>,
<em>63</em>(8), 1271. (<a
href="https://doi.org/10.1093/comjnl/bxz165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modular exponentiation, as a fundamental operation used in many public-key cryptosystems, has always be considered to be very time-consuming. It is difficult for some devices with limited computation capability, such as mobile devices and low-cost radio frequency identification (RFID) tags, to perform large-scale modular exponentiations. In cryptosystems, one typical case of modular exponentiation is that the modulus is a composite number. For instance, in RSA algorithm, the modulus is the product of two distinct prime numbers. In this paper, we investigate how to securely and efficiently outsource composite modular exponentiations and put forward two secure outsourcing algorithms for composite modular exponentiations based on single untrusted cloud. The first algorithm, named MCExp, is designed for outsourcing single composite modular exponentiation, i.e. ; mod ; . The second algorithm, named SMCExp, is designed for outsourcing simultaneous composite modular exponentiation, i.e. ; mod ; . Different from algorithms based on two untrusted servers, the proposed algorithms are very practical because they avoid the strong assumption that there must exist two servers without collusion. The proposed algorithms not only protect the privacy of the exponent and the base simultaneously, but also enable users to verify the correctness of the result returned by the cloud with high probability. Compared with using the square-and-multiply algorithm, the user can achieve higher efficiency by using the proposed algorithms. Besides, we prove the security of our algorithms and conduct several experiments to demonstrate the efficiency of the proposed algorithms. Finally, we show that the proposed algorithms can be used to construct the secure outsourcing algorithms for Shamir’s identity-based signature and identity-based multi-signature.},
  archive      = {J_COMJNL},
  author       = {Su, Qianqian and Zhang, Rui and Xue, Rui},
  doi          = {10.1093/comjnl/bxz165},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1271},
  shortjournal = {Comput. J.},
  title        = {Secure outsourcing algorithms for composite modular exponentiation based on single untrusted cloud},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Leakage-free ID-based signature, revisited. <em>COMJNL</em>,
<em>63</em>(8), 1263–1270. (<a
href="https://doi.org/10.1093/comjnl/bxz160">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Tseng et al. proposed a new notion for identity-based signature (IBS) scheme to resist ephemeral secret leakage (ESL) attacks, called leakage-free identity-based signature (leakage-free IBS), and devised the first secure leakage-free IBS scheme. However, they only considered the situation of the leakage of ephemeral secrets used for generating the signatures. Notice that the private key extraction procedure is probabilistic as well in their scheme, that is, there are ephemeral secrets used by the key generation center to generate the signers’ private keys. It is practical to consider that if the adversary comprises these ephemeral secrets, then he can reveal the master key of the system. Therefore, it is desired to introduce a new security notion for the leakage-free IBS schemes to consider the ESL attacks on both private key extraction and signing procedures. In this paper, we present such security notion. Moreover, we propose two IBS schemes that are proved to be secure under the new security notion.},
  archive      = {J_COMJNL},
  author       = {Lin, Xi-Jun and Sun, Lin and Qu, Haipeng},
  doi          = {10.1093/comjnl/bxz160},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1263-1270},
  shortjournal = {Comput. J.},
  title        = {Leakage-free ID-based signature, revisited},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the security of a certificateless signcryption with known
session-specific temporary information security in the standard model.
<em>COMJNL</em>, <em>63</em>(8), 1259–1262. (<a
href="https://doi.org/10.1093/comjnl/bxz157">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rastegari ; . recently proposed a certificateless signcryption (CL-SC) scheme. They claimed that their scheme is the first secure CL-SC scheme, which captures the known session-specific temporary information security (KSSTIS), in the standard model. In this paper, we point out that their scheme is insecure, which implies that how to construct a secure CL-SC scheme with KSSTIS in the standard model is still an open problem.},
  archive      = {J_COMJNL},
  author       = {Lin, Xi-Jun and Sun, Lin and Yan, Zhen and Zhang, Xiaoshuai and Qu, Haipeng},
  doi          = {10.1093/comjnl/bxz157},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1259-1262},
  shortjournal = {Comput. J.},
  title        = {On the security of a certificateless signcryption with known session-specific temporary information security in the standard model},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An adaptively secure functional encryption for randomized
functions. <em>COMJNL</em>, <em>63</em>(8), 1247–1258. (<a
href="https://doi.org/10.1093/comjnl/bxz154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Functional encryption (FE) can provide a fine-grained access control on the encrypted message. Therefore, it has been applied widely in security business. The previous works about functional encryptions most focused on the deterministic functions. The randomized algorithm has wide application, such as securely encryption algorithms against chosen ciphertext attack, privacy-aware auditing. Based on this, FE for randomized functions was proposed. The existing constructions are provided in a weaker selective security model, where the adversary is forced to output the challenge message before the start of experiment. This security is not enough in some scenes. In this work, we present a novel construction for FE, which supports the randomized functionalities. We use the technology of key encapsulated mechanism to achieve adaptive security under the simulated environment, where the adversary is allowed to adaptively choose the challenge message at any point in time. Our construction is built based on indistinguishability obfuscation, non-interactive witness indistinguishable proofs and perfectly binding commitment scheme.},
  archive      = {J_COMJNL},
  author       = {Liu, Muhua and Zhang, Ping},
  doi          = {10.1093/comjnl/bxz154},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1247-1258},
  shortjournal = {Comput. J.},
  title        = {An adaptively secure functional encryption for randomized functions},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Practical key-recovery attacks on round-reduced ketje jr,
xoodoo-AE and xoodyak. <em>COMJNL</em>, <em>63</em>(8), 1231–1246. (<a
href="https://doi.org/10.1093/comjnl/bxz152">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new conditional cube attack was proposed by Li ; at ToSC 2019 for cryptanalysis of ;  keyed modes. In this paper, we find a new property of Li ; ’s method. The conditional cube attack is modified and applied to cryptanalysis of 5-round ; , 6-round ; -AE and ; , where ;  is among the third round CAESAR competition candidates and ;  is a Round 2 submission of the ongoing NIST lightweight cryptography project. For the updated conditional cube attack, all our results are shown to be of practical time complexity with negligible memory cost, and test codes are provided. Notably, our results on ;  represent the first third-party cryptanalysis for ; .},
  archive      = {J_COMJNL},
  author       = {Zhou, Haibo and Li, Zheng and Dong, Xiaoyang and Jia, Keting and Meier, Willi},
  doi          = {10.1093/comjnl/bxz152},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1231-1246},
  shortjournal = {Comput. J.},
  title        = {Practical key-recovery attacks on round-reduced ketje jr, xoodoo-AE and xoodyak},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Improved proofs of retrievability and replication for data
availability in cloud storage. <em>COMJNL</em>, <em>63</em>(8),
1216–1230. (<a href="https://doi.org/10.1093/comjnl/bxz151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a high level of data availability and reliability, a common strategy for cloud service providers is to rely on replication, i.e. storing several replicas onto different servers. To provide cloud users with a strong guarantee that all replicas required by them are actually stored, many multi-replica integrity auditing schemes were proposed. However, most existing solutions are not resource economical since users need to create and upload replicas of their files by themselves. A multi-replica solution called Mirror is presented to overcome the problems, but we find that it is vulnerable to storage saving attack, by which a dishonest provider can considerably save storage costs compared to the costs of storing all the replicas honestly—while still can pass any challenge successfully. In addition, we also find that Mirror is easily subject to substitution attack and forgery attack, which pose new security risks for cloud users. To address the problems, we propose some simple yet effective countermeasures and an improved proofs of retrievability and replication scheme, which can resist the aforesaid attacks and maintain the advantages of Mirror, such as economical bandwidth and efficient verification. Experimental results show that our scheme exhibits comparable performance with Mirror while achieving high security.},
  archive      = {J_COMJNL},
  author       = {Guo, Wei and Qin, Sujuan and Lu, Jun and Gao, Fei and Jin, Zhengping and Wen, Qiaoyan},
  doi          = {10.1093/comjnl/bxz151},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1216-1230},
  shortjournal = {Comput. J.},
  title        = {Improved proofs of retrievability and replication for data availability in cloud storage},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Practical attribute-based conjunctive keyword search scheme.
<em>COMJNL</em>, <em>63</em>(8), 1203–1215. (<a
href="https://doi.org/10.1093/comjnl/bxz140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To date cloud computing may provide considerable storage and computational power for cloud-based applications to support cryptographic operations. Due to this benefit, attribute-based keyword search (ABKS) is able to be implemented in cloud context in order to protect the search privacy of data owner/user. ABKS is a cryptographic primitive that can provide secure search services for users but also realize fine-grained access control over data. However, there have been two potential problems that prevent the scalability of ABKS applications. First of all, most of the existing ABKS schemes suffer from the ; (KGA). Second, match privacy should be considered while supporting multi-keyword search. In this paper, we design an efficient method to combine the keyword search process in ABKS with inner product encryption and deploy several proposed techniques to ensure the flexibility of retrieval mode, the security and efficiency of our scheme. We later put forward an attribute-based conjunctive keyword search scheme against outside KGA to solve the aforementioned problems. We provide security notions for two types of adversaries and our construction is proved secure against chosen keyword attack and outside KGA. Finally, all-side simulation with real-world data set is implemented for the proposed scheme, and the results of the simulation show that our scheme achieves stronger security without yielding significant cost of storage and computation.},
  archive      = {J_COMJNL},
  author       = {Chen, Yang and Li, Wenmin and Gao, Fei and Liang, Kaitai and Zhang, Hua and Wen, Qiaoyan},
  doi          = {10.1093/comjnl/bxz140},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1203-1215},
  shortjournal = {Comput. J.},
  title        = {Practical attribute-based conjunctive keyword search scheme},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multivariate blind ring signature scheme. <em>COMJNL</em>,
<em>63</em>(8), 1194–1202. (<a
href="https://doi.org/10.1093/comjnl/bxz128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blind signatures are an important and useful tool in designing digital cash schemes and electronic voting protocols. Ring signatures on the other hand provide the anonymity of the signer within the ring of users. In order to fit to some real-life applications, it is useful to combine both protocols to create a blind ring signature scheme, which utilizes all of their features. In this paper, we propose, for the first time, a post-quantum blind ring signature scheme. Our scheme is constructed based on multivariate public key cryptography, which is one of the main candidates for post-quantum cryptography.},
  archive      = {J_COMJNL},
  author       = {Hoang Duong, Dung and Susilo, Willy and Tran, Ha Thanh Nguyen},
  doi          = {10.1093/comjnl/bxz128},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1194-1202},
  shortjournal = {Comput. J.},
  title        = {A multivariate blind ring signature scheme},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Implementation and evaluation of the mCityPASS protocol for
secure and private access to associated touristic services.
<em>COMJNL</em>, <em>63</em>(8), 1168–1193. (<a
href="https://doi.org/10.1093/comjnl/bxz126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {are managed by some city councils in order to make a step toward the SmartCity concept. ; enable the integration of different touristic services so as to facilitate the access to transport, cultural sites and touristic attractions to visitors. In this paper, we introduce a secure mobile system called ; that allows the smart management and secure access to this kind of service. We have implemented the system in a smartphone platform in such a way that the mobile device contains the purchased access element that holds a set of e-tickets that enables the visitors of a city to make use of the specified touristic services. The paper presents the details of the whole implementation of the system and an evaluation of the performance and the effectiveness of it. Also, an analysis of the privacy and the security properties of the presented scheme has been conducted. As a result, we can assure that the system preserves the privacy of the tourists and fulfills the more challenging security properties: fairness, non-overspending, exculpability, unsplittability and selective reusability.},
  archive      = {J_COMJNL},
  author       = {Payeras-Capella, M Magdalena and Mut-Puigserver, Macia and Conejero-Alberola, Pau and Castella-Roca, Jordi and Huguet-Rotger, Llorenç},
  doi          = {10.1093/comjnl/bxz126},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1168-1193},
  shortjournal = {Comput. J.},
  title        = {Implementation and evaluation of the mCityPASS protocol for secure and private access to associated touristic services},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Specifying a new requirement model for secure adaptive
systems. <em>COMJNL</em>, <em>63</em>(8), 1148–1167. (<a
href="https://doi.org/10.1093/comjnl/bxz124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Security is a growing concern in developing software systems. It is important to face unknown threats in order to make the system continue operating properly. Threats are vague and attack methods change frequently. Coping with such changes is a major feature of an adaptive software. Therefore, designing an adaptive secure software is an appropriate solution to address software security challenges. Through estimation of maximum amount of system assets security, one can determine whether the system is protecting the assets or not; if not, reconfiguration can be employed. This paper proposes a new requirement model for secure adaptive systems using fuzzy, goal modeling and Description Logic concepts. The model contains three phases of modeling security aspects of the system, identifying formalizations and relations between the requirements and monitoring and adapting, when needed. To illustrate the relations between the requirements, goal modeling is used in the first phase and fuzzy Description Logic in the second phase. For the third phase, four algorithms are proposed to monitor and determine whether reconfiguration is needed or not. Theorems are given to prove concept satisfaction of the requirements. Furthermore, examples and case studies are discussed to evaluate and show applicability of the proposed model.},
  archive      = {J_COMJNL},
  author       = {Alyari, Robab and Karimpour, Jaber and Izadkhah, Habib},
  doi          = {10.1093/comjnl/bxz124},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1148-1167},
  shortjournal = {Comput. J.},
  title        = {Specifying a new requirement model for secure adaptive systems},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A provably secure certificateless proxy signature scheme
against malicious-but-passive KGC attacks. <em>COMJNL</em>,
<em>63</em>(8), 1139–1147. (<a
href="https://doi.org/10.1093/comjnl/bxz122">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In certificateless proxy signature (CLPS), the key generation center is responsible for initializing the system parameters and can obtain the opportunity to adaptively set some trapdoors in them when wanting to launch some attacks. Until now, how to withstand the malicious-but-passive key generation center (MKGC) attacks in CLPS is still an interesting problem. In this paper, we focus on the challenging issue and introduce a CLPS scheme provably secure in the standard model. To the best of our knowledge, we are the first to demonstrate its security under MKGC attacks by adopting the technology of embedding the classic difficulty problems into the target entity public key rather than the system parameters during the security proof process.},
  archive      = {J_COMJNL},
  author       = {Yang, Wenjie and Weng, Jian and Huang, Xinyi and Yang, Anjia},
  doi          = {10.1093/comjnl/bxz122},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1139-1147},
  shortjournal = {Comput. J.},
  title        = {A provably secure certificateless proxy signature scheme against malicious-but-passive KGC attacks},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Byte2vec: Malware representation and feature selection for
android. <em>COMJNL</em>, <em>63</em>(8), 1125–1138. (<a
href="https://doi.org/10.1093/comjnl/bxz121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Malware detection based on static features and without code disassembling is a challenging path of research. Obfuscation makes the static analysis of malware even more challenging. This paper extends static malware detection beyond byte level ; -grams and detecting important strings. We propose a model (Byte2vec) with the capabilities of both binary file feature representation and feature selection for malware detection. Byte2vec embeds the semantic similarity of byte level codes into a feature vector (byte vector) and also into a context vector. The learned feature vectors of Byte2vec, using skip-gram with negative-sampling topology, are combined with byte-level term-frequency (; ) for malware detection. We also show that the distance between a feature vector and its corresponding context vector provides a useful measure to rank features. The top ranked features are successfully used for malware detection. We show that this feature selection algorithm is an unsupervised version of mutual information (MI). We test the proposed scheme on four freely available Android malware datasets including one obfuscated malware dataset. The model is trained only on clean APKs. The results show that the model outperforms MI in a low-dimensional feature space and is competitive with MI and other state-of-the-art models in higher dimensions. In particular, our tests show very promising results on a wide range of obfuscated malware with a false negative rate of only 0.3\% and a false positive rate of 2.0\%. The detection results on obfuscated malware show the advantage of the unsupervised feature selection algorithm compared with the MI-based method.},
  archive      = {J_COMJNL},
  author       = {Yousefi-Azar, Mahmood and Hamey, Len and Varadharajan, Vijay and Chen, Shiping},
  doi          = {10.1093/comjnl/bxz121},
  journal      = {The Computer Journal},
  number       = {8},
  pages        = {1125-1138},
  shortjournal = {Comput. J.},
  title        = {Byte2vec: Malware representation and feature selection for android},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). CalBehav: A machine learning-based personalized calendar
behavioral model using time-series smartphone data. <em>COMJNL</em>,
<em>63</em>(7), 1109–1123. (<a
href="https://doi.org/10.1093/comjnl/bxz117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The electronic calendar is a valuable resource nowadays for managing our daily life appointments or schedules, also known as events, ranging from professional to highly personal. Researchers have studied various types of calendar events to predict smartphone user behavior for incoming mobile communications. However, these studies typically do not take into account ; between individuals. In the real world, smartphone users can differ widely from each other in how they respond to incoming communications during their scheduled events. Moreover, an individual user may respond the incoming communications differently in different contexts subject to what type of event is scheduled in her personal calendar. Thus, a ; calendar-based behavioral model for individual smartphone users does not necessarily reflect their behavior to the incoming communications. In this paper, we present a ; based context-aware model that is ; and ; identifies individual’s dominant behavior for their scheduled events using logged ; smartphone data, and shortly name as ; . The experimental results based on real datasets from calendar and phone logs, show that this data-driven personalized model is more effective for intelligently managing the incoming mobile communications compared to existing calendar-based approaches.},
  archive      = {J_COMJNL},
  author       = {Sarker, Iqbal H and Colman, Alan and Han, Jun and Kayes, A S M and Watters, Paul},
  doi          = {10.1093/comjnl/bxz117},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1109-1123},
  shortjournal = {Comput. J.},
  title        = {CalBehav: A machine learning-based personalized calendar behavioral model using time-series smartphone data},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pruning neural networks using multi-armed bandits.
<em>COMJNL</em>, <em>63</em>(7), 1099–1108. (<a
href="https://doi.org/10.1093/comjnl/bxz078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The successful application of deep learning has led to increasing expectations of their use in embedded systems. This, in turn, has created the need to find ways of reducing the size of neural networks. Decreasing the size of a neural network requires deciding which weights should be removed without compromising accuracy, which is analogous to the kind of problems addressed by multi-armed bandits (MABs). Hence, this paper explores the use of MABs for reducing the number of parameters of a neural network. Different MAB algorithms, namely ; -greedy, win-stay, lose-shift, UCB1, KL-UCB, BayesUCB, UGapEb, successive rejects and Thompson sampling are evaluated and their performance compared to existing approaches. The results show that MAB pruning methods, especially those based on UCB, outperform other pruning methods.},
  archive      = {J_COMJNL},
  author       = {Ameen, Salem and Vadera, Sunil},
  doi          = {10.1093/comjnl/bxz078},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1099-1108},
  shortjournal = {Comput. J.},
  title        = {Pruning neural networks using multi-armed bandits},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-band contourlet transform for adaptive remote sensing
image denoising. <em>COMJNL</em>, <em>63</em>(7), 1084–1098. (<a
href="https://doi.org/10.1093/comjnl/bxz073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The ability to remove noise from remote sensing images, while retaining the important features of the images, is becoming increasingly important. In this paper, we introduce the multi-band contourlet transform, a new method for adaptively denoising remote sensing images. We describe existing methods that use multi-resolution analysis transforms for denoising images and discuss their respective advantages and disadvantages. We then introduce our novel denoising method, which exploits the advantages of existing methods. We summarize the results of a comprehensive set of experiments designed to evaluate the performance of our method and compare it with the performance of existing methods. The results demonstrate that our method is superior to existing methods, both in terms of its ability to denoise images and to retain salient features of those images following denoising.},
  archive      = {J_COMJNL},
  author       = {Wang, Haijiang and Wang, Jingpu and Yao, Fuqi and Ma, Yongqiang and Li, Lihong and Yang, Qinke},
  doi          = {10.1093/comjnl/bxz073},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1084-1098},
  shortjournal = {Comput. J.},
  title        = {Multi-band contourlet transform for adaptive remote sensing image denoising},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Study on data transfer in meteorological forecast of small
and medium-sized cities and its application in zhaoqing city.
<em>COMJNL</em>, <em>63</em>(7), 1076–1083. (<a
href="https://doi.org/10.1093/comjnl/bxz087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Using historical data, a machine learning model is usually built to forecast the future meteorological elements such as temperature, precipitation, etc. However, for numerous small and medium-sized cities, it is a challenging task because the maintained data of these cities are usually very limited due to historical or infrastructural reasons. So it is difficult to build an accurate forecast model in small and medium-sized cities. Aiming at this problem, a forecast method based on transfer learning method is proposed. Using instance-based transfer learning, this method extends the data of the target city by transferring the data from related cities and then builds a forecast model based on the extended dataset, so that the problem of insufficient samples in machine learning is solved. As a case study, the proposed technique is applied in Zhaoqing City, China. In the experiments, the data of temperature sequence and the precipitation sequence of Gaoyao weather station in Zhaoqing district are extended according to the data of related cities. The transferred temperature data and precipitation data are collected from 1884 to 1997 in Hong Kong and 1908 to 2016 in Guangzhou, respectively. Then temperature and precipitation forecasting models are built based on least square method and autoregressive integrated moving average. The experimental results have been verified by the actual situation. The results justify the effectiveness of the proposed method in building accurate meteorological forecasting model with limited data, and the superiority over existing techniques.},
  archive      = {J_COMJNL},
  author       = {Huang, Tianwen and Jiao, Fei},
  doi          = {10.1093/comjnl/bxz087},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1076-1083},
  shortjournal = {Comput. J.},
  title        = {Study on data transfer in meteorological forecast of small and medium-sized cities and its application in zhaoqing city},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A metric to assess the readability of video closed captions
for the persons with low literacy skills. <em>COMJNL</em>,
<em>63</em>(7), 1063–1075. (<a
href="https://doi.org/10.1093/comjnl/bxz074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The power of the WWW (World Wide Web) lies in the adoption of multiple types of media, which makes effective communication of content. Videos have emerged into a quick and efficient information delivery channel for users. To make a video accessible to persons with disabilities, several approaches have been proposed, which include various accessibility features such as closed captions and video descriptions. Captioning agencies use English as a primary language for creating captions, which cause problems for persons with low literacy skills. Ambiguity in the video affects the persons with low literacy skills and senior citizens and it leads them to consume more time in identifying/capturing the intended meaning. As the closed captions run along with the video frame, the comprehension is an issue. For predicting the readability assessment of closed captions for persons with low literacy skills, we developed a statistical formula based on the lexical and semantical ambiguity of the context. As a case study, caption files of top 50 English movies (as per IMDB top rated movies chart) were chosen and their readability scores were calculated. In addition, a machine learning model was developed using the ambiguity feature set in assessing the readability score of the closed captions. The result shows, our model is predicting the readability score with an accuracy of 92.6\%.},
  archive      = {J_COMJNL},
  author       = {Pantula, Muralidhar and Kuppusamy, K S},
  doi          = {10.1093/comjnl/bxz074},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1063-1075},
  shortjournal = {Comput. J.},
  title        = {A metric to assess the readability of video closed captions for the persons with low literacy skills},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evolutionary algorithms for k-anonymity in social networks
based on clustering approach. <em>COMJNL</em>, <em>63</em>(7),
1039–1062. (<a href="https://doi.org/10.1093/comjnl/bxz069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The usage of social networks shows a growing trend in recent years. Due to a large number of online social networking users, there is a lot of data within these networks. Recently, advances in technology have made it possible to extract useful information about individuals and the interactions among them. In parallel, several methods and techniques were proposed to preserve the users’ privacy through the anonymization of social network graphs. In this regard, the utilization of the k-anonymity method, where k is the required threshold of structural anonymity, is among the most useful techniques. In this technique, the nodes are clustered together to form the super-nodes of size at least k. Our main idea in this paper is, initially, to optimize the clustering process in the k-anonymity method by means of the particle swarm optimization (PSO) algorithm in order to minimize the normalized structural information loss (NSIL), which is equal to maximizing 1-NSIL. Although the proposed PSO-based method shows a higher convergence rate than the previously introduced genetic algorithm (GA) method, it did not provide a lower NSIL value. Therefore, in order to achieve the NSIL value provided by GA optimization while preserving the high convergence rate obtained from the PSO algorithm, we present hybrid solutions based on the GA and PSO algorithms. Eventually, in order to achieve indistinguishable nodes, the edge generalization process is employed based on their relationships. The simulation results demonstrate the efficiency of the proposed model to balance the maximized 1-NSIL and the algorithm’s convergence rate.},
  archive      = {J_COMJNL},
  author       = {Yazdanjue, Navid and Fathian, Mohammad and Amiri, Babak},
  doi          = {10.1093/comjnl/bxz069},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1039-1062},
  shortjournal = {Comput. J.},
  title        = {Evolutionary algorithms for k-anonymity in social networks based on clustering approach},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dropout with tabu strategy for regularizing deep neural
networks. <em>COMJNL</em>, <em>63</em>(7), 1031–1038. (<a
href="https://doi.org/10.1093/comjnl/bxz062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dropout has been proven to be an effective technique for regularizing and preventing the co-adaptation of neurons in deep neural networks (DNN). It randomly drops units with a probability of ; during the training stage of DNN to avoid overfitting. The working mechanism of dropout can be interpreted as approximately and exponentially combining many different neural network architectures efficiently, leading to a powerful ensemble. In this work, we propose a novel diversification strategy for dropout, which aims at generating more different neural network architectures in less numbers of iterations. The dropped units in the last forward propagation will be marked. Then the selected units for dropping in the current forward propagation will be retained if they have been marked in the last forward propagation, i.e., we only mark the units from the last forward propagation. We call this new regularization scheme Tabu dropout, whose significance lies in that it does not have extra parameters compared with the standard dropout strategy and is computationally efficient as well. Experiments conducted on four public datasets show that Tabu dropout improves the performance of the standard dropout, yielding better generalization capability.},
  archive      = {J_COMJNL},
  author       = {Ma, Zongjie and Sattar, Abdul and Zhou, Jun and Chen, Qingliang and Su, Kaile},
  doi          = {10.1093/comjnl/bxz062},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1031-1038},
  shortjournal = {Comput. J.},
  title        = {Dropout with tabu strategy for regularizing deep neural networks},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Video hashing with DCT and NMF. <em>COMJNL</em>,
<em>63</em>(7), 1017–1030. (<a
href="https://doi.org/10.1093/comjnl/bxz060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video hashing is a novel technique of multimedia processing and finds applications in video retrieval, video copy detection, anti-piracy search and video authentication. In this paper, we propose a robust video hashing based on discrete cosine transform (DCT) and non-negative matrix decomposition (NMF). The proposed video hashing extracts secure features from a normalized video via random partition and dominant DCT coefficients, and exploits NMF to learn a compact representation from the secure features. Experiments with 2050 videos are carried out to validate efficiency of the proposed video hashing. The results show that the proposed video hashing is robust to many digital operations and reaches good discrimination. Receiver operating characteristic (ROC) curve comparisons illustrate that the proposed video hashing outperforms some state-of-the-art algorithms in classification between robustness and discrimination.},
  archive      = {J_COMJNL},
  author       = {Tang, Zhenjun and Chen, Lv and Yao, Heng and Zhang, Xianquan and Yu, Chunqiang},
  doi          = {10.1093/comjnl/bxz060},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1017-1030},
  shortjournal = {Comput. J.},
  title        = {Video hashing with DCT and NMF},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards a semantic framework for lifelong integrated
competency management and development. <em>COMJNL</em>, <em>63</em>(7),
1004–1016. (<a href="https://doi.org/10.1093/comjnl/bxz067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the domain of technology-enhanced competency-based learning and training, there is an increased interest in the integration of competency-related information for supporting competency-driven decision-making purposes. Indeed, since competency development draws upon several related areas, including teaching subjects, instructional design, learning resource annotation, e-Portfolios and motivated by the need for an integrated and semantic-based approach to competency management and development, a series of ontological structures have been formalized and developed for each of these areas. This paper aims to provide a framework specification for lifelong competency management and development, called LCMDF. The main advantage of this framework lies in its ability to provide a unifying semantic foundation in the form of a set of controlled vocabularies for describing competencies and their related details within the contexts of technology-enhanced competency-based learning and training. Moreover, this framework provides a novel integrated model to support a wide range of use cases. The proposed framework results from reusing widespread international standards for competency modeling which helps designing and implementing interoperability architecture of semantically-enhanced competency-based learning/human resource (HR) systems.},
  archive      = {J_COMJNL},
  author       = {Rezgui, Kalthoum and Mhiri, Hédia},
  doi          = {10.1093/comjnl/bxz067},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {1004-1016},
  shortjournal = {Comput. J.},
  title        = {Towards a semantic framework for lifelong integrated competency management and development},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep reinforcement learning with adaptive update target
combination. <em>COMJNL</em>, <em>63</em>(7), 995–1003. (<a
href="https://doi.org/10.1093/comjnl/bxz066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Simple and efficient exploration remains a core challenge in deep reinforcement learning. While many exploration methods can be applied to high-dimensional tasks, these methods manually adjust exploration parameters according to domain knowledge. This paper proposes a novel method that can automatically balance exploration and exploitation, as well as combine on-policy and off-policy update targets through a dynamic weighted way based on value difference. The proposed method does not directly affect the probability of a selected action but utilizes the value difference produced during the learning process to adjust update target for guiding the direction of agent’s learning. We demonstrate the performance of the proposed method on CartPole-v1, MountainCar-v0, and LunarLander-v2 classic control tasks from the OpenAI Gym. Empirical evaluation results show that by integrating on-policy and off-policy update targets dynamically, this method exhibits superior performance and stability than does the exclusive use of the update target.},
  archive      = {J_COMJNL},
  author       = {Xu, Z and Cao, L and Chen, X},
  doi          = {10.1093/comjnl/bxz066},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {995-1003},
  shortjournal = {Comput. J.},
  title        = {Deep reinforcement learning with adaptive update target combination},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). DeepDetect: Detection of distributed denial of service
attacks using deep learning. <em>COMJNL</em>, <em>63</em>(7), 983–994.
(<a href="https://doi.org/10.1093/comjnl/bxz064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {At the advent of advanced wireless technology and contemporary computing paradigms, Distributed Denial of Service (DDoS) attacks on Web-based services have not only increased exponentially in number, but also in the degree of sophistication; hence the need for detecting these attacks within the ocean of communication packets is extremely important. DDoS attacks were initially projected toward the network and transport layers. Over the years, attackers have shifted their offensive strategies toward the application layer. The application layer attacks are potentially more detrimental and stealthier because of the attack traffic and the benign traffic flows being indistinguishable. The distributed nature of these attacks is difficult to combat as they may affect tangible computing resources apart from network bandwidth consumption. In addition, smart devices connected to the Internet can be infected and used as botnets to launch DDoS attacks. In this paper, we propose a novel deep neural network-based detection mechanism that uses feed-forward back-propagation for accurately discovering multiple application layer DDoS attacks. The proposed neural network architecture can identify and use the most relevant high level features of packet flows with an accuracy of 98\% on the state-of-the-art dataset containing various forms of DDoS attacks.},
  archive      = {J_COMJNL},
  author       = {Asad, Muhammad and Asim, Muhammad and Javed, Talha and Beg, Mirza O and Mujtaba, Hasan and Abbas, Sohail},
  doi          = {10.1093/comjnl/bxz064},
  journal      = {The Computer Journal},
  number       = {7},
  pages        = {983-994},
  shortjournal = {Comput. J.},
  title        = {DeepDetect: Detection of distributed denial of service attacks using deep learning},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel CMA+DD_LMS blind equalization algorithm for
underwater acoustic communication. <em>COMJNL</em>, <em>63</em>(6),
974–981. (<a href="https://doi.org/10.1093/comjnl/bxaa013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The performance of underwater acoustic communication system is affected seriously by inter-symbol interference caused by multipath effects. Therefore, a novel blind equalization algorithm based on constant modulus algorithm (CMA) and decision-directed least mean square (DD_LMS) is adopted to improve the equalization ability of the system. Firstly, the LMS algorithm is improved by introducing inverse hyperbolic sine function and three adjustment factors to control step-size and the appropriate parameter values are set through the simulation of three adjustment factors. Secondly, the error values of the step-size function are replaced with error expectations to improve the anti-noise performance. Finally, the improved step-size function is introduced into the CMA and DD_LMS algorithm and the difference of the iteration error of adjacent ; times is used as the switching condition of the dual mode algorithm. The results show that the algorithm has good equalization and anti-noise performance at both high and low signal-to-noise ratio (SNR), especially at low SNR, its steady-state error is ~10 dB lower than the traditional CMA and its convergence speed is ~15\% higher than the traditional CMA. This algorithm can be used to effectively improve the communication efficiency of the communication system of underwater robots, which has good application value.},
  archive      = {J_COMJNL},
  author       = {Sun, Jianqiu and Li, Xingguang and Chen, Kang and Cui, Wei and Chu, Ming},
  doi          = {10.1093/comjnl/bxaa013},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {974-981},
  shortjournal = {Comput. J.},
  title        = {A novel CMA+DD_LMS blind equalization algorithm for underwater acoustic communication},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). RIoT: A routing protocol for the internet of things.
<em>COMJNL</em>, <em>63</em>(6), 958–973. (<a
href="https://doi.org/10.1093/comjnl/bxaa012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The routing protocol for low-power and lossy networks (RPL) is a standard routing framework for Internet of Things (IoT). It supports multipoint-to-point (MP-to-P), point-to-point (P-to-P) and point-to-multipoint (P-to-MP) communications. It is known that RPL’s control overhead can result in the protocol’s poor performance in P-to-P and P-to-MP communications especially in its non-storing mode of operation. Here, we present a routing protocol for the Internet of Things (RIoT) that supports MP-to-P, P-to-P and P-to-MP communications. The protocol can construct P-to-P and P-to-MP routes with relatively lower control overhead. Another salient feature of RIoT is that it supports multiple gateways in the same network with an aim to reduce memory requirement for storing a forwarding table. Furthermore, RIoT is also capable of handling mobility-based IoT use cases. To facilitate communication among nodes connected to different gateways in the same network, here we also present an inter-gateway communication mechanism. We implemented RIoT in the Contiki operating system, and it is extensively evaluated using emulation and real testbed-based experiments. We analyzed the impact of the number of gateways, radio duty cycling (RDC) and mobility on the routing protocols’ performance. Our results demonstrate that either with or without RDC RIoT demonstrates statistically significantly better packet delivery ratio, per-packet end-to-end delay and control overhead compared to the RPL-based protocol. RIoT’s multi-gateway communication architecture substantially reduces the memory requirement to store a forwarding table. Our results also demonstrate that multiple gateways in a network reduce the network partitioning problem in mobile scenarios. Hence, RIoT also demonstrates better performance in mobile scenarios compared to the RPL-based protocol.},
  archive      = {J_COMJNL},
  author       = {Omer Farooq, Muhammad},
  doi          = {10.1093/comjnl/bxaa012},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {958-973},
  shortjournal = {Comput. J.},
  title        = {RIoT: A routing protocol for the internet of things},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian-based spectrum sensing and optimal channel
estimation for MAC layer protocol in cognitive radio sensor networks.
<em>COMJNL</em>, <em>63</em>(6), 942–957. (<a
href="https://doi.org/10.1093/comjnl/bxaa002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cognitive radio (CR) is an intelligent and adaptive radio technology that automatically detects the available channels in the wireless spectrum and sometimes changes the transmission parameters to enable effective communication. Spectrum sensing in CR prevents harmful interference with the licensed users and maximizes the spectrum utilization. Thus, this paper proposes a technique for optimal channel estimation and spectrum sensing for MAC layer protocol in CR networks such that the scheduling issues are addressed. Initially, in the CR networks, spectrum sensing is done using the proposed optimal naive Bayes classifier (ONBC) based on the signal statistics, such as energy and likelihood ratio. The ONBC is developed by integrating the bat–bird swarm algorithm (BBSA) with the naive Bayes classifier, which works based on the Bayesian concept. The BBSA is newly developed by integrating the bird swarm algorithm (BSA) and bat algorithm. Finally, the channel estimation is done using the pilot-based sequential procedure and least square estimation (LSE). The analysis of the proposed method is done in the Rayleigh and Rician environments using 256 and 512 sub-carriers. From the results, it is exposed that the proposed BBSA + LSE pilot-based sequential method obtains the bit error rate, normalized energy and Probability detection (PD) of is 0.0126, 0.8446 and 0.9355, respectively.},
  archive      = {J_COMJNL},
  author       = {Maisuria, Jemish V and Mehta, Saurabh N},
  doi          = {10.1093/comjnl/bxaa002},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {942-957},
  shortjournal = {Comput. J.},
  title        = {Bayesian-based spectrum sensing and optimal channel estimation for MAC layer protocol in cognitive radio sensor networks},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Architecture and system design for marine cloud computing
assets. <em>COMJNL</em>, <em>63</em>(6), 927–941. (<a
href="https://doi.org/10.1093/comjnl/bxz169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Ocean provides benefits of free cooling for cloud computing platforms. However, the use of the ocean for hosting cloud platforms needs to consider three challenges. The first challenge is identifying suitable underwater locations for siting underwater data centres. The second is designing a low-cost method for acquiring underwater data centres. The third is designing a mechanism ensuring that the use of the ocean for hosting data centres is scalable. This paper proposes the intelligent marine compute locator (IMCL) to identify suitable locations for siting underwater data centres. The proposed IMCL determines the specific heat capacity of different ocean locations at multiple epochs. In addition, the conversion of end-of-life vessels into artificial reefs that host open-source disaggregated hardware computing payload is proposed to reduce acquisition costs. The use of disaggregated architecture enables multiple cloud service providers to use limited ocean locations. The formulated metrics are the power usage effectiveness (PUE) and ocean space utilization (OSU). Simulations show that the use of disaggregated design architecture instead of non-disaggregated architecture (existing mechanism) enhances the PUE and OSU by 4.4 and 16.4\% on average, respectively.},
  archive      = {J_COMJNL},
  author       = {Periola, A A and Alonge, A A and Ogudo, K A},
  doi          = {10.1093/comjnl/bxz169},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {927-941},
  shortjournal = {Comput. J.},
  title        = {Architecture and system design for marine cloud computing assets},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive filter with type-2 fuzzy system and
optimization-based kernel interpolation for satellite image denoising.
<em>COMJNL</em>, <em>63</em>(6), 913–926. (<a
href="https://doi.org/10.1093/comjnl/bxz168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Satellite image denoising is a recent trend in image processing, but faces many challenges due to the environmental factors. Previous works have developed many filters for denoising the hyperspectral satellite images. Accordingly, this work utilizes an adaptive filter with the type 2 fuzzy system and the optimization-based kernel interpolation for the satellite image denoising. Here, the image denoising has been done through three steps, namely noise identification, noise correction and image enhancement. Initially, the type 2 fuzzy system identifies the noisy pixels in the satellite image and converts the image into a binary image, which is passed through the adaptive nonlocal mean filter (ANLMF) for the noise correction. Finally, the kernel-based interpolation scheme carries out the image enhancement, which is done through the proposed chronological Jaya optimization algorithm (chronological JOA) that is developed by modifying Jaya optimization algorithm (JOA) with the chronological idea. The performance of the proposed denoising scheme is analyzed by considering the satellite images from two standard databases, namely Indian pines database and NRSC/ISRO satellite database. Also, the comparative analysis is performed with the state-of-the-art denoising methods using the evaluation metrics, peak signal to noise ratio (PSNR), structural similarity index (SSIM) and second derivative-like measure of enhancement (SDME). From the results, it is exposed that the proposed adaptive filter with the chronological JOA has the improved performance with the PSNR of 22.0408 dB, SDME of 244.133 dB and SSIM of 0.872.},
  archive      = {J_COMJNL},
  author       = {Mahalakshmi, T and Sreenivas, Alluri},
  doi          = {10.1093/comjnl/bxz168},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {913-926},
  shortjournal = {Comput. J.},
  title        = {Adaptive filter with type-2 fuzzy system and optimization-based kernel interpolation for satellite image denoising},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MapReduce and optimized deep network for rainfall prediction
in agriculture. <em>COMJNL</em>, <em>63</em>(6), 900–912. (<a
href="https://doi.org/10.1093/comjnl/bxz164">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rainfall prediction is the active area of research as it enables the farmers to move with the effective decision-making regarding agriculture in both cultivation and irrigation. The existing prediction models are scary as the prediction of rainfall depended on three major factors including the humidity, rainfall and rainfall recorded in the previous years, which resulted in huge time consumption and leveraged huge computational efforts associated with the analysis. Thus, this paper introduces the rainfall prediction model based on the deep learning network, convolutional long short-term memory (convLSTM) system, which promises a prediction based on the spatial-temporal patterns. The weights of the convLSTM are tuned optimally using the proposed Salp-stochastic gradient descent algorithm (S-SGD), which is the integration of Salp swarm algorithm (SSA) in the stochastic gradient descent (SGD) algorithm in order to facilitate the global optimal tuning of the weights and to assure a better prediction accuracy. On the other hand, the proposed deep learning framework is built in the MapReduce framework that enables the effective handling of the big data. The analysis using the rainfall prediction database reveals that the proposed model acquired the minimal mean square error (MSE) and percentage root mean square difference (PRD) of 0.001 and 0.0021.},
  archive      = {J_COMJNL},
  author       = {S, Oswalt Manoj and J P, Ananth},
  doi          = {10.1093/comjnl/bxz164},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {900-912},
  shortjournal = {Comput. J.},
  title        = {MapReduce and optimized deep network for rainfall prediction in agriculture},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Balancing power and performance in HPC clouds.
<em>COMJNL</em>, <em>63</em>(6), 880–899. (<a
href="https://doi.org/10.1093/comjnl/bxz150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With energy consumption in high-performance computing clouds growing rapidly, energy saving has become an important topic. Virtualization provides opportunities to save energy by enabling one physical machine (PM) to host multiple virtual machines (VMs). Dynamic voltage and frequency scaling (DVFS) is another technology to reduce energy consumption. However, in heterogeneous cloud environments where DVFS may be applied at the chip level or the core level, it is a great challenge to combine these two technologies efficiently. On per-core DVFS servers, cloud managers should carefully determine VM placements to minimize performance interference. On full-chip DVFS servers, cloud managers further face the choice of whether to combine VMs with different characteristics to reduce performance interference or to combine VMs with similar characteristics to take better advantage of DVFS. This paper presents a novel mechanism combining a VM placement algorithm and a frequency scaling method. We formulate this VM placement problem as an integer programming (IP) to find appropriate placement configurations, and we utilize support vector machines to select suitable frequencies. We conduct detailed experiments and simulations, showing that our scheme effectively reduces energy consumption with modest impact on performance. Particularly, the total energy delay product is reduced by up to 60\%.},
  archive      = {J_COMJNL},
  author       = {Chen, Lixia and Li, Jian and Ma, Ruhui and Guan, Haibing and Jacobsen, Hans-Arno},
  doi          = {10.1093/comjnl/bxz150},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {880-899},
  shortjournal = {Comput. J.},
  title        = {Balancing power and performance in HPC clouds},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An approach for image search and retrieval by cluster-based
indexing of binary MKSIFT codes. <em>COMJNL</em>, <em>63</em>(6),
857–879. (<a href="https://doi.org/10.1093/comjnl/bxz145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {.},
  archive      = {J_COMJNL},
  author       = {Mathan Kumar, B and PushpaLakshmi, R},
  doi          = {10.1093/comjnl/bxz145},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {857-879},
  shortjournal = {Comput. J.},
  title        = {An approach for image search and retrieval by cluster-based indexing of binary MKSIFT codes},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiantenna receiver signal detection in AmBC based on
cluster analysis. <em>COMJNL</em>, <em>63</em>(6), 844–856. (<a
href="https://doi.org/10.1093/comjnl/bxz143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ambient backscatter communication (AmBC) has great application prospects in the green Internet of Things due to its shared nature of energy and spectrum. In this paper, we have done research on the signal detection problem in the AmBC system and proposed to use the k-means clustering analysis algorithm to detect the received backscattered signal. At the same time, in order to obtain a more obvious clustering center, this paper introduces a multi-antenna receiver channel detection mechanism. The proposed method can compare and analyze the energy received by the receiver antennas in an extremely short time slots. The channel state information between the reader and the backscatter tag is obtained, and the optimal communication channel is selected, thereby effectively improving the clustering effect. Finally, a large number of experimental simulations are provided to compare and analyze the corresponding BER performance to confirm our theoretical research.},
  archive      = {J_COMJNL},
  author       = {Zhao, Jumin and Zhang, Liang and Li, Deng&#39;ao},
  doi          = {10.1093/comjnl/bxz143},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {844-856},
  shortjournal = {Comput. J.},
  title        = {Multiantenna receiver signal detection in AmBC based on cluster analysis},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Congestion-free transient plane (CFTP) using bandwidth
sharing during link failures in SDN. <em>COMJNL</em>, <em>63</em>(6),
832–843. (<a href="https://doi.org/10.1093/comjnl/bxz137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software-defined networking (SDN) is an emerging trend where the control plane and the data plane are separated from each other, culminating in effective bandwidth utilization. This separation also allows multi-vendor interoperability. Link failure is a major problem in networking and must be detected as soon as possible because when a link fails the path becomes congested and packet loss occurs, delaying the delivery of packets to the destination. Backup paths must be configured immediately when a failure is detected in the network to speed up packet delivery, avoid congestion and packet loss and provide faster convergence. Various SDN segment protection algorithms that efficiently reduce CPU cycles and flow table entries exist, but each has drawbacks. An independent transient plane technique can be used to reduce packet loss but is not as efficient when multiple flows try to share the same link. The proposed work focuses on reducing congestion, providing faster convergence with minimal packet loss and effectively utilizing link bandwidth using bandwidth-sharing techniques. An analysis and related studies show that this method performs better and offers a more reliable network without loss, while simultaneously ensuring the swift delivery of data packets toward the destination without congestion, compared to the other existing schemes.},
  archive      = {J_COMJNL},
  author       = {Vanamoorthy, Muthumanikandan and Chinnaiah, Valliyammai},
  doi          = {10.1093/comjnl/bxz137},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {832-843},
  shortjournal = {Comput. J.},
  title        = {Congestion-free transient plane (CFTP) using bandwidth sharing during link failures in SDN},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Monarch-EWA: Monarch-earthworm-based secure routing protocol
in IoT. <em>COMJNL</em>, <em>63</em>(6), 817–831. (<a
href="https://doi.org/10.1093/comjnl/bxz135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Routing in the Internet of Things (IoT) renders the protection against various network attacks as any attacker intrudes the routing mechanism for establishing the destructive mechanisms against the network, which insists the essentiality of the security protocols in IoT. Thus, the paper proposes a secure protocol based on an optimization algorithm, Monarch-Earthworm Algorithm (Monarch-EWA), which is the modification of the Monarch Butterfly algorithm using the Earthworm Optimization Algorithm (EWA) in order to render effective security to the network. Initially, the effective nodes are selected using the Deep Convolutional Neural Network (deep CNN) classifier based on the factors, trust and energy of the node, and stochastic gradient descent algorithm trains the deep CNN classifier. The secure nodes are involved in routing for which the secure multipath is chosen optimally using the proposed Monarch-EWA, which chooses the secure multipath based on the factors, energy and trust. The analysis of the proposed method in the presence of attacks, such as black hole, message replicate and distributed denial of service, reveals that the proposed method outperformed the existing methods. The proposed Monarch-EWA protocol acquired the maximal energy, throughput and detection rate of 0.2268 J, 48.2759\% and 82.6231\%, respectively, with the minimal delay of 0.0959 ms.},
  archive      = {J_COMJNL},
  author       = {Sugave, Shounak and Jagdale, Balaso},
  doi          = {10.1093/comjnl/bxz135},
  journal      = {The Computer Journal},
  number       = {6},
  pages        = {817-831},
  shortjournal = {Comput. J.},
  title        = {Monarch-EWA: Monarch-earthworm-based secure routing protocol in IoT},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lookback-guess-next optimizer: Feedback-guided random search
technique with biased mapping for solving unconstrained optimization
problems. <em>COMJNL</em>, <em>63</em>(5), 791–816. (<a
href="https://doi.org/10.1093/comjnl/bxz046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Finding global optima for functions is a very important problem. Although a large number of methods have been proposed for solving this problem, more effective and efficient methods are greatly required. This paper proposes an innovative method that combines different effective techniques for speeding up the convergence to the solution and greatly improving its precision. In particular, the method uses feedback-guided random search technique to identify the promising regions of the domains and uses the biased mapping technique to focus the search on these promising regions, without ignoring the other regions of the domains. Therefore, at any point of time, the domain of each variable is entirely covered with much more emphasis on the promising regions. Experiments with our prototype implementation showed that our method is efficient, effective, and outperformed the state-of-art techniques.},
  archive      = {J_COMJNL},
  author       = {Al-Muhammed, Muhammed Jassem},
  doi          = {10.1093/comjnl/bxz046},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {791-816},
  shortjournal = {Comput. J.},
  title        = {Lookback-guess-next optimizer: Feedback-guided random search technique with biased mapping for solving unconstrained optimization problems},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deriving specifications of control programs for cyber
physical systems. <em>COMJNL</em>, <em>63</em>(5), 774–790. (<a
href="https://doi.org/10.1093/comjnl/bxz019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cyber physical systems (CPS) exist in a physical environment and comprise both physical components and a control program. Physical components are inherently liable to failure and yet an overall CPS is required to operate safely, reliably and cost effectively. This paper proposes a framework for deriving the specification of the software control component of a CPS from an understanding of the behaviour required of the overall system in its physical environment. The two key elements of this framework are (i) an extension to the use of rely/guarantee conditions to allow specifications to be obtained systematically from requirements (as expressed in terms of the required behaviour in the environment) and nested assumptions (about the physical components of the CPS); and (ii) the use of time bands to record the temporal properties required of the CPS at a number of different granularities. The key contribution is in combining these ideas; using time bands overcomes a significant drawback in earlier work. The paper also addresses the means by which the reliability of a CPS can be addressed by challenging each rely condition in the derived specification and, where appropriate, improve robustness and/or define weaker guarantees that can be delivered with respect to the corresponding weaker rely conditions.},
  archive      = {J_COMJNL},
  author       = {Burns, Alan and Hayes, Ian J and Jones, Cliff B},
  doi          = {10.1093/comjnl/bxz019},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {774-790},
  shortjournal = {Comput. J.},
  title        = {Deriving specifications of control programs for cyber physical systems},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simultaneous fault models for the generation and location of
efficient error detection mechanisms. <em>COMJNL</em>, <em>63</em>(5),
758–773. (<a href="https://doi.org/10.1093/comjnl/bxz022">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The application of machine learning to software fault injection data has been shown to be an effective approach for the generation of efficient error detection mechanisms (EDMs). However, such approaches to the design of EDMs have invariably adopted a fault model with a single-fault assumption, limiting the relevance of the detectors and their evaluation. Software containing more than a single fault is commonplace, with safety standards recognizing that critical failures are often the result of unlikely or unforeseen combinations of faults. This paper addresses this shortcoming, demonstrating that it is possible to generate efficient EDMs under simultaneous fault models. In particular, it is shown that (i) efficient EDMs can be designed using fault injection data collected under models accounting for the occurrence of simultaneous faults, (ii) exhaustive fault injection under a simultaneous bit flip model can yield improved EDM efficiency, (iii) exhaustive fault injection under a simultaneous bit flip model can be made non-exhaustive and (iv) EDMs can be relocated within a software system using program slicing, reducing the resource costs of experimentation to practicable levels without sacrificing EDM efficiency.},
  archive      = {J_COMJNL},
  author       = {Leeke, Matthew},
  doi          = {10.1093/comjnl/bxz022},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {758-773},
  shortjournal = {Comput. J.},
  title        = {Simultaneous fault models for the generation and location of efficient error detection mechanisms},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ImagIngDev: A new approach for developing automatic
cross-platform mobile applications using image processing techniques.
<em>COMJNL</em>, <em>63</em>(5), 732–757. (<a
href="https://doi.org/10.1093/comjnl/bxz029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The aim of this work is propose and describe ImagIngDev, a new approach for developing automatic cross-platform mobile applications using image processing techniques. As proof of concept, we presented ImagIng Tool and compared its performance with respect to similar cross-platform application development tools. Our main contribution to software development is ImagingDev, a new, intuitive and agile approach aimed at novice developers for automatically developing cross-platform mobile applications. Our proof of concept, ImagIng Tool, can generate mobile applications on four different software platforms: Android™, Windows Phone™, iOS™ and FirefoxOS™. Also, it relies on image processing techniques to recognize user interface design patterns (UIDPs) inside user interfaces, and generates the source code for cross-platform and multi-device applications. As proof of concept, we developed ImagIng Tool for automatic generation of cross-platform mobile applications by using image processing techniques. Results from the evaluation demonstrated that ImagIng Tool has attractive benefits if compared to similar cross-platform application development tools. Such benefits include higher learnability and usability. ImagIngDev and ImagIng Tool can effectively solve current problems in mobile applications development, such as automatic code generation for cross-platform mobile applications, since they allow users to generate cross-platform and multi-device mobile applications in an easy and intuitive way.},
  archive      = {J_COMJNL},
  author       = {Rosales-Morales, Viviana Yarel and Sánchez-Morales, Laura Nely and Alor-Hernández, Giner and Garcia-Alcaraz, Jorge Luis and Sánchez-Cervantes, José Luis and Rodriguez-Mazahua, Lisbeth},
  doi          = {10.1093/comjnl/bxz029},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {732-757},
  shortjournal = {Comput. J.},
  title        = {ImagIngDev: A new approach for developing automatic cross-platform mobile applications using image processing techniques},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dynamic-SoS: An approach for the simulation of
systems-of-systems dynamic architectures. <em>COMJNL</em>,
<em>63</em>(5), 709–731. (<a
href="https://doi.org/10.1093/comjnl/bxz028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Systems-of-Systems (SoS) combine heterogeneous, independent systems to offer complex functionalities for highly dynamic smart applications. Besides their dynamic architecture with continuous changes at runtime, SoS should be reliable and work without interrupting their operation and with no failures that could cause accidents or losses. SoS architectural design should facilitate the prediction of the impact of architectural changes and potential failures due to SoS behavior. However, existing approaches do not support such evaluation. Hence, these systems have been usually built without a proper evaluation of their architecture. This article presents Dynamic-SoS, an approach to predict/anticipate at design time the SoS architectural behavior at runtime to evaluate whether the SoS can sustain their operation. The main contributions of this approach comprise: (i) characterization of the dynamic architecture changes via a set of well-defined operators; (ii) a strategy to automatically include a reconfiguration controller for SoS simulation; and (iii) a means to evaluate architectural configurations that an SoS could assume at runtime, assessing their impact on the viability of the SoS operation. Results of our case study reveal Dynamic-SoS is a promising approach that could contribute to the quality of SoS by enabling prior assessment of its dynamic architecture.},
  archive      = {J_COMJNL},
  author       = {Manzano, Wallace and Graciano Neto, Valdemar Vicente and Nakagawa, Elisa Yumi},
  doi          = {10.1093/comjnl/bxz028},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {709-731},
  shortjournal = {Comput. J.},
  title        = {Dynamic-SoS: An approach for the simulation of systems-of-systems dynamic architectures},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). UnfairDuelMerge: Merging with even fewer moves.
<em>COMJNL</em>, <em>63</em>(5), 701–708. (<a
href="https://doi.org/10.1093/comjnl/bxz015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The merging problem consists of forming an ordered sequence with ; elements from two ordered sequences with ; and ; elements (where ; ). One recent development in in-place merging is a minimum storage algorithm (DuelMerge) that requires half the number of moves in average, when compared to other partition-based algorithms. The reduction is achieved by limiting the exchange to blocks with equal sizes, and using a particularly suited exchange method. The algorithm proposed in this paper (UnfairDuelMerge) reduces the number of moves even further, by limiting the exchange to blocks where the left side is one element larger than the right side, and using the floating hole technique to perform the exchange. As its predecessor, the algorithm is stable, in-place and the merge is asymptotically performed with a ; runtime. A formal description of the method is provided, as well as an analysis of stability, memory cost and time complexity. Experimental results show that the proposed algorithm outperforms other partition-based solutions when applied directly to merge ordered sequences and when used within MergeSort.},
  archive      = {J_COMJNL},
  author       = {Mergen, Sergio L S},
  doi          = {10.1093/comjnl/bxz015},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {701-708},
  shortjournal = {Comput. J.},
  title        = {UnfairDuelMerge: Merging with even fewer moves},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiple-choice hardware/software partitioning for tree
task-graph on MPSoC. <em>COMJNL</em>, <em>63</em>(5), 688–700. (<a
href="https://doi.org/10.1093/comjnl/bxy140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hardware/software (HW/SW) partitioning, that decides which components of an application are implemented in hardware and which ones in software, is a crucial step in embedded system design. On modern heterogeneous embedded system platform, each component of application can typically have multiple feasible configurations/implementations, trading off quality aspects (e.g. energy consumption, completion time) with usage for various types of resources. This provides new opportunities for further improving the overall system performance, but few works explore the potential opportunity by incorporating the multiple choices of hardware implementation in the partitioning process. This paper proposes three algorithms for multiple-choice HW/SW partitioning of tree-shape task graph on multiple processors system on chip (MPSoC) with the objective of minimizing execution time, while meeting area constraint. Firstly, an efficient heuristic algorithm is proposed to rapidly generate an approximate solution. The obtained solution produced by the first algorithm is then further refined by a customized Tabu search algorithm. We also propose a dynamic programming algorithm to calculate the exact solutions for relatively smaller scale instances. Simulation results show that the proposed heuristic algorithm is able to quickly generate good approximate solutions, and the solutions become very close to the exact solutions after refined by the proposed Tabu search algorithm, in comparison to the exact solutions produced by the dynamic programming algorithm.},
  archive      = {J_COMJNL},
  author       = {Shi, Wenjun and Wu, Jigang and Jiang, Guiyuan and Lam, Siew-kei},
  doi          = {10.1093/comjnl/bxy140},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {688-700},
  shortjournal = {Comput. J.},
  title        = {Multiple-choice Hardware/Software partitioning for tree task-graph on MPSoC},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A system for multi-passenger urban ridesharing
recommendations with ordered multiple stops. <em>COMJNL</em>,
<em>63</em>(5), 657–687. (<a
href="https://doi.org/10.1093/comjnl/bxz009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic and air pollution caused by the increasing number of cars have become important issues in nowadays cities. A possible solution is to employ recommender systems for efficient ridesharing among users. These systems, however, typically do not allow specifying ordered stops, thus preventing a large amount of possible users from exploiting ridesharing, e.g. parents leaving kids at school while going to work. Indeed, if a parent desired to share a ride, he/she would need to indicate the following constraint in the path: the stop at school should precede the stop at work. In this paper, we propose a ridesharing recommender, which allows each user to specify an ordered list of stops and suggests efficient ride matches. The ride-matching criterion is based on a dissimilarity between the driver’s path and the ; , computed as the shortest path on a directed acyclic graph with ordering constraints between the stops defined in the single paths. The ; is the detour requested to the driver to visit also the stops of the paths involved in the ride-share, respecting the visiting order of the stops within each path. Results are presented on a case study involving the city of Pisa.},
  archive      = {J_COMJNL},
  author       = {D’Andrea, Eleonora and Lazzerini, Beatrice and Marcelloni, Francesco},
  doi          = {10.1093/comjnl/bxz009},
  journal      = {The Computer Journal},
  number       = {5},
  pages        = {657-687},
  shortjournal = {Comput. J.},
  title        = {A system for multi-passenger urban ridesharing recommendations with ordered multiple stops},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Updatable lossy trapdoor functions under consecutive
leakage. <em>COMJNL</em>, <em>63</em>(4), 648–656. (<a
href="https://doi.org/10.1093/comjnl/bxz119">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lossy trapdoor functions (LTFs), introduced by Peikert and Waters (STOC’08), have already been found to be a very useful tool in constructing complex cryptographic primitives in a black-box manner, such as one-way trapdoor functions, deterministic public-key encryption, CCA-secure public-key encryption, etc. Due to the existence of the side-channel attack, the leakage of trapdoor information in lossy trapdoor function systems can lead to the impossibility of provable security. Recently, Zhang et al. introduced a model of consecutive and continual leakage-resilient and updatable lossy trapdoor functions (ULTFs) and provided a concrete construction to achieve the security. Meanwhile, they proposed a consecutive and continual leakage-resilient public-key encryption scheme. However, in this paper, we demonstrate that the correctness of injective function can not be satisfied. Furthermore, the attacker can easily distinguish the evaluation key of ULTFs generated by the challenger according to the security model. Finally, we show two new constructions based on the continual leakage-resilient public-key encryption scheme of Brakerski et al. (FOCS 2010) and demonstrate the security of our scheme in the consecutive and continual leakage model.},
  archive      = {J_COMJNL},
  author       = {Huang, Meijuan and Yang, Bo and Zhang, Mingwu and Zhang, Lina and Hou, Hongxia},
  doi          = {10.1093/comjnl/bxz119},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {648-656},
  shortjournal = {Comput. J.},
  title        = {Updatable lossy trapdoor functions under consecutive leakage},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A (zero-knowledge) vector commitment with sum binding and
its applications. <em>COMJNL</em>, <em>63</em>(4), 633–647. (<a
href="https://doi.org/10.1093/comjnl/bxz115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vector commitment (VC) schemes allow committing to an ordered sequence of ; values ; in such a way that one can later open the commitment at specific positions. However, the existing VC schemes suffer from two substantial shortcomings that limit their use: (i) the commitments cannot be opened except at some specific positions, and (ii) their security only captures position-binding but offers no privacy: the client may learn additional information about the committed sequence through the proofs and the commitments. To resolve these problems, we first extend VC to a more expressive primitive called VC with sum binding (VCS), in which the commitment can also be opened to the sum of all elements in the committed sequence. VCS additionally satisfies the security of ; , which guarantees that the commitment cannot be opened to different sums. To enhance its privacy, we extend VCS to zero-knowledge VCS (ZKVCS), in which commitments and proofs constructed during the protocol execution leak nothing about the committed sequence. We formalize this new property by a standard real/ideal experiment. Meanwhile, the detailed performance analyses and simulations show that our proposed schemes are more practical. Finally, we introduce a novel notion of (zero-knowledge) verifiable database supporting sum and show how to construct it from our (ZK)VCS scheme.},
  archive      = {J_COMJNL},
  author       = {Wang, Qiang and Zhou, Fucai and Xu, Jian and Xu, Zifeng},
  doi          = {10.1093/comjnl/bxz115},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {633-647},
  shortjournal = {Comput. J.},
  title        = {A (Zero-knowledge) vector commitment with sum binding and its applications},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Server-aided revocable IBE with identity reuse.
<em>COMJNL</em>, <em>63</em>(4), 620–632. (<a
href="https://doi.org/10.1093/comjnl/bxz114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efficient key revocation in Identity-based Encryption (IBE) has been a both fundamental and critical problem when deploying an IBE system in practice. Boneh and Franklin proposed the first revocable IBE (RIBE) scheme where the size of key updates is linear in the number of users. Then, Boldyreva, Goyal and Kumar proposed the first scalable RIBE by using the tree-based approach where the size of key updates is ; and the size of every user’s long-term secret key is ; with ; being the number of users and ; the number of revoked users. Recently, Qin ; . presented the notion of server-aided RIBE where the size of every user’s long-term secret key is ; and users do not need to communicate with Key Generator Center (KGC) during every key updates. However, users must change their identities once their secret keys are revoked as they cannot decrypt ciphertexts by using their revoked secret keys.; To address the above problem, we formalize the notion of RIBE with identity reuse. In our system model, users can obtain a new secret key called the reuse secret key from KGC when their secret keys are revoked. The decryption key can be derived from the reuse secret key and new key updates while it cannot be derived from the revoked secret key and the new key updates. We present a concrete construction that is secure against adaptive-ID chosen plaintext attacks and decryption key exposure attacks under the ; and ; assumptions in the standard model. Furthermore, we extend it to server-aided RIBE scheme with identity reuse property that is more suitable for lightweight devices.},
  archive      = {J_COMJNL},
  author       = {Ma, Xuecheng and Lin, Dongdai},
  doi          = {10.1093/comjnl/bxz114},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {620-632},
  shortjournal = {Comput. J.},
  title        = {Server-aided revocable IBE with identity reuse},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Intrusion detection over encrypted network data.
<em>COMJNL</em>, <em>63</em>(4), 604–619. (<a
href="https://doi.org/10.1093/comjnl/bxz111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effective protection against cyber-attacks requires constant monitoring and analysis of ; in an IT infrastructure, such as log files and network packets, which may contain private and sensitive information. Security operation centers (SOC), which are established to detect, analyze and respond to cyber-security incidents, often utilize ; either for known types of attacks or for anomaly and applies them to the system data for detection. SOC are also motivated to keep their models private to capitalize on the models that are their propriety expertise, and to protect their detection strategies against adversarial machine learning. In this paper, we develop a protocol for privately evaluating detection models on the system data, in which privacy of both the system data and detection models is protected and information leakage is either prevented altogether or quantifiably decreased. Our main approach is to provide an end-to-end encryption for the system data and detection models utilizing lattice-based cryptography that allows homomorphic operations over ciphertext. We employ recent data sets in our experiments which demonstrate that the proposed privacy-preserving intrusion detection system is feasible in terms of execution times and bandwidth requirements and reliable in terms of accuracy.},
  archive      = {J_COMJNL},
  author       = {Karaçay, Leyli and Savaş, Erkay and Alptekin, Halit},
  doi          = {10.1093/comjnl/bxz111},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {604-619},
  shortjournal = {Comput. J.},
  title        = {Intrusion detection over encrypted network data},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). New security risk value estimate method for android
applications. <em>COMJNL</em>, <em>63</em>(4), 593–603. (<a
href="https://doi.org/10.1093/comjnl/bxz109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, mobile applications are the devices’ core, so their security is essential for the platform on which are installed. Therefore, to make this security strong, Android implements permission system and sandboxing mechanism to reduce the attacks contingency. Also, antivirus software is used to detect the already known malware based on their signature, but unfortunately, this kind of program cannot detect cloned or repackaged malware. Thus, using authorizations to estimate the security vulnerability will surely be very useful for avoiding applications that are more susceptible to be dangerous according to the risk values obtained. Consequently, this will provide systematic support that will make it easier for users to make appropriate decisions and therefore greatly improve the Android devices’ security. In this article, additionally to the risk measures that have been already proposed in previous studies based on functionalities such as permissions and function calls, we add a set of mathematical metrics describing the case of susceptible and innocent applications already known. Our risk assessment exploits applications already known as malware and safe samples without any threat.},
  archive      = {J_COMJNL},
  author       = {Er-rajy, Latifa and El Kiram, My Ahmed and El Ghazouani, Mohamed},
  doi          = {10.1093/comjnl/bxz109},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {593-603},
  shortjournal = {Comput. J.},
  title        = {New security risk value estimate method for android applications},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Secure computing resource allocation framework for open fog
computing. <em>COMJNL</em>, <em>63</em>(4), 567–592. (<a
href="https://doi.org/10.1093/comjnl/bxz108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fog computing has become an emerging environment that provides data storage, computing and some other services on the edge of network. It not only can acquire data from terminal devices, but also can provide computing services to users by opening computing resources. Compared with cloud computing, fog devices can collaborate to provide users with powerful computing services through resource allocation. However, as many of fog devices are not monitored, there are some security problems. For example, since fog server processes and maintains user information, device information, task parameters and so on, fog server is easy to perform illegal resource allocation for extra benefits. In this paper, we propose a secure computing resource allocation framework for open fog computing. In our scheme, the fog server is responsible for processing computing requests and resource allocations, and the cloud audit center is responsible for auditing the behaviors of the fog servers and fog nodes. Based on the proposed security framework, our proposed scheme can resist the attack of single malicious node and the collusion attack of fog server and computing devices. Furthermore, the experiments show our proposed scheme is efficient. For example, when the number of initial idle service devices is 40, the rejection rate of allocated tasks is 10\% and the total number of sub-tasks is changed from 150 to 200, the total allocation time of our scheme is only changed from 15 ms to 25 ms; additionally, when the task of 5000 order matrix multiplication is tested on 10 service devices, the total computing time of our scheme is ; 250 s, which is better than that of single computer (where single computer needs more than 1500 s). Therefore, our proposed scheme has obvious advantages when it faces some tasks that require more computational cost, such as complex scientific computing, distributed massive data query, distributed image processing and so on.},
  archive      = {J_COMJNL},
  author       = {Jiang, Jiafu and Tang, Linyu and Gu, Ke and Jia, WeiJia},
  doi          = {10.1093/comjnl/bxz108},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {567-592},
  shortjournal = {Comput. J.},
  title        = {Secure computing resource allocation framework for open fog computing},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Resistance of IID noise in differentially private schemes
for trajectory publishing. <em>COMJNL</em>, <em>63</em>(4), 549–566. (<a
href="https://doi.org/10.1093/comjnl/bxz097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although analyzing and mining user’s trajectory data can provide outstanding benefit, data owners may not be willing to upload their trajectory data because of privacy concerns. Recently, differential privacy technology has achieved a good trade-off between data utility and privacy preserving by publishing noisy outputs, and relevant schemes have been proposed for trajectory release. However, we experimentally find that a relatively accurate estimate of the true data value can still be obtained from the noisy outputs by means of a posterior estimation. But there are no practical mechanisms against current schemes to verify their effectiveness and resistance. To fill this gap, we propose a solution to evaluate the resistance performance of differential privacy on trajectory data release, including a notion of correlation-distinguishability filtering (CDF) and a privacy quantification measurement. Specifically, taking advantage of the principle of filtering that independent noise can be filtered out from correlated sequence, CDF is proposed to sanitize the noise added into the trajectory. To conduct this notion in practice, we attempt to apply a Kalman/particle filter to filter out the corresponding Gaussian/Laplace noise added by differential privacy schemes. Furthermore, to quantify the distortion of privacy strength before and after filtering, an entropy-based privacy quantification metric is proposed, which is used to measure the lost uncertainty of the true locations for an adversary. Experimental results show that the resistance performance of current approaches has a degradation to varying degrees under the filtering attack model in our solution. Moreover, the privacy quantification metric can be regarded as a unified criterion to measure the privacy strength introduced by the noise that does not conform to the form required by differential privacy.},
  archive      = {J_COMJNL},
  author       = {Wang, Hao and Li, Kaiju},
  doi          = {10.1093/comjnl/bxz097},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {549-566},
  shortjournal = {Comput. J.},
  title        = {Resistance of IID noise in differentially private schemes for trajectory publishing},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new construction for linkable secret handshake.
<em>COMJNL</em>, <em>63</em>(4), 536–548. (<a
href="https://doi.org/10.1093/comjnl/bxz095">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a new construction for linkable secret handshake that allows authenticated users to perform handshake anonymously within allowable times. We define formal security models for the new construction, and prove that it can achieve session key security, anonymity, untraceability and linkable affiliation-hiding. In particular, the proposed construction ensures that (i) anyone can trace the real identities of dishonest users who perform handshakes for more than k times; and (ii) an optimal communication cost between authorized users is achieved by exploiting the proof of knowledges.},
  archive      = {J_COMJNL},
  author       = {Tian, Yangguang and Li, Yingjiu and Deng, Robert H and Li, Nan and Yang, Guomin and Yang, Zheng},
  doi          = {10.1093/comjnl/bxz095},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {536-548},
  shortjournal = {Comput. J.},
  title        = {A new construction for linkable secret handshake},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Black-box accountable authority identity-based revocation
system. <em>COMJNL</em>, <em>63</em>(4), 525–535. (<a
href="https://doi.org/10.1093/comjnl/bxz092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identity-based revocation system (IBRS) generates the ciphertext with a revoked identity list such that only the non-revoked identities can use their private keys to decrypt this ciphertext. IBRS can be efficiently applied in some practical applications, such as the pay-TV systems when the number of revoked identities are much less than the non-revoked ones. However, since IBRS is based on identity-based cryptography, it also suffers from the inherent key escrow problem where the private key generator (PKG) has full control of each user’s private key. As a consequence, it is hard to judge whether a pirated private key is generated by the PKG or the suspected user. There is no study on IBRS fulfilling accountability in literature to date. In this paper, we introduce the notion of accountable authority IBRS (A-IBRS), which provides accountability in IBRS schemes. In an A-IBRS, the aforementioned problem can be alleviated and resolved. Furthermore, a full black-box A-IBRS can distinguish the creator of a black box between the PKG and the associated user and the dishonest PKG is allowed to access the decryption results of the user private key. We formalize the definition and security models of the full black-box A-IBRS schemes. Then, we present a concrete full black-box A-IBRS scheme with constant-size master public key and private key. Finally, we prove the security of our scheme under the defined security models without random oracle.},
  archive      = {J_COMJNL},
  author       = {Zhao, Zhen and Wu, Ge and Guo, Fuchun and Susilo, Willy and Mu, Yi and Wang, Baocang and Hu, Yupu},
  doi          = {10.1093/comjnl/bxz092},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {525-535},
  shortjournal = {Comput. J.},
  title        = {Black-box accountable authority identity-based revocation system},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Continuous leakage-resilient certificate-based encryption
scheme without bilinear pairings. <em>COMJNL</em>, <em>63</em>(4),
508–524. (<a href="https://doi.org/10.1093/comjnl/bxz085">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, much attention has been focused on designing provably secure cryptographic primitives in the presence of key leakage, even the continuous leakage attacks. However, several constructions on the (continuous) leakage-resilient certificate-based encryption (CBE) scheme were proposed based on the bilinear pairings, and the corresponding computational efficiency is lower. Also, the leakage on the master secret key is omitted in the previous constructions. In this paper, to further achieve the better performance, a new construction method of continuous leakage-resilient CBE scheme without bilinear pairings is proposed, and the chosen-ciphertext attacks security of designed scheme is proved based on the hardness of the classic decisional Diffie–Hellman assumption. The performance analysis shows that our method not only can obtain higher computational efficiency but also enjoys better security performances, such as the leakage parameter of secret key of user has the constant size, and an adversary cannot obtain any leakage on the secret key of user from the corresponding given ciphertext etc. The advantage is that our proposal allows leakage attacks of multiple keys, i.e. continuous leakage resilience of the secret key of user and bounded leakage resilience of the master secret key. Additionally, to provide the leakage resilience for the cloud computing, a novel data access control scheme for cloud storage service is proposed from our continuous leakage-resilient CBE scheme, which can keep its claimed security in the leakage seting.},
  archive      = {J_COMJNL},
  author       = {Zhou, Yanwei and Yang, Bo and Wang, Tao and Xia, Zhe and Hou, Hongxia},
  doi          = {10.1093/comjnl/bxz085},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {508-524},
  shortjournal = {Comput. J.},
  title        = {Continuous leakage-resilient certificate-based encryption scheme without bilinear pairings},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the security of LWE cryptosystem against subversion
attacks. <em>COMJNL</em>, <em>63</em>(4), 495–507. (<a
href="https://doi.org/10.1093/comjnl/bxz084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Subversion of cryptography has received wide attentions especially after the Snowden Revelations in 2013. Most of the currently proposed subversion attacks essentially rely on the freedom of randomness choosing in the cryptographic protocol to hide backdoors embedded in the cryptosystems. Despite the fact that significant progresses in this line of research have been made, most of them mainly considered the classical setting, while the research gap regarding subversion attacks against post-quantum cryptography remains tremendous. Inspired by this observation, we investigate a subversion attack against existing protocol that is proved post-quantum secure. Particularly, we show an efficient way to undetectably subvert the well-known lattice-based encryption scheme proposed by Regev (STOC 2005). Our subversion enables the subverted algorithm to stealthily leak arbitrary messages to the outsider who knows the backdoor. Through theoretical analysis and experimental observations, we demonstrate that the subversion attack against the LWE encryption scheme is feasible and practical.},
  archive      = {J_COMJNL},
  author       = {Yang, Zhichao and Chen, Rongmao and Li, Chao and Qu, Longjiang and Yang, Guomin},
  doi          = {10.1093/comjnl/bxz084},
  journal      = {The Computer Journal},
  number       = {4},
  pages        = {495-507},
  shortjournal = {Comput. J.},
  title        = {On the security of LWE cryptosystem against subversion attacks},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Combining watermarking and hyper-chaotic map to enhance the
security of stored biometric templates. <em>COMJNL</em>, <em>63</em>(3),
479–493. (<a href="https://doi.org/10.1093/comjnl/bxz047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are a number of issues related to the development of biometric authentication systems, such as privacy breach, consequential security and biometric template storage. Thus, the current paper aims to address these issues through the hybrid approach of watermarking with biometric encryption. A multimodal biometric template protection approach with fusion at score level using fingerprint and face templates is proposed. The proposed approach includes two basic stages, enrollment stage and verification stage. During the enrollment stage, discrete wavelet transform (DWT) is applied on the face images to embed the fingerprint features into different directional sub-bands. Watermark embedding and extraction are done by quantizing the mean values of the wavelet coefficients. Subsequently, the inverse DWT is applied to obtain the watermarked image. Following this, a unique token is assigned for each genuine user and a hyper-chaotic map is used to produce a key stream in order to encrypt a watermarked image using block-cipher. The experimentation results indicate the efficiency of the proposed approach in term of achieving a reasonable error rate of 3.87\%.},
  archive      = {J_COMJNL},
  author       = {Abdul, Wadood and Nafea, Ohoud and Ghouzali, Sanaa},
  doi          = {10.1093/comjnl/bxz047},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {479-493},
  shortjournal = {Comput. J.},
  title        = {Combining watermarking and hyper-chaotic map to enhance the security of stored biometric templates},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Real-time topic detection with dynamic windows.
<em>COMJNL</em>, <em>63</em>(3), 469–478. (<a
href="https://doi.org/10.1093/comjnl/bxz042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microblog is a popular social network in which hot topics propagate online rapidly. Real-time topic detection can not only understand public opinion well but also bring high commercial value. We design a method for real-time microblog data analysis in order to detect popular long lasting events as well as emerging events. Firstly, a mining frequent items algorithm on microblog data stream is proposed to count approximate word frequency. This mining frequent items algorithm can find the frequent words for some time. Secondly, the windows size of the monitored words is adjusted dynamically according to the duration time and the evolution of events. Lastly, new topics and trends of existing topics can be detected by using dynamic clustering algorithm based on vector space model. Experimental results show that the proposed algorithms can improve performance in terms of running time and accuracy.},
  archive      = {J_COMJNL},
  author       = {Su, Na and Ji, Shujuan and Liu, Jimin},
  doi          = {10.1093/comjnl/bxz042},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {469-478},
  shortjournal = {Comput. J.},
  title        = {Real-time topic detection with dynamic windows},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Discriminative correlation filter for long-time tracking.
<em>COMJNL</em>, <em>63</em>(3), 460–468. (<a
href="https://doi.org/10.1093/comjnl/bxz049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object tracking is a very important step in building an intelligent video monitoring system that can protect people’s lives and property. In recent years, although visual tracking has made great progress in terms of speed and accuracy, there are still few real-time high-precision tracking algorithms. Although discriminative correlation filters have excellent performance in tracking speed, there are deficiencies in handling fast motion. This leads to the inability to achieve long-term stable tracking results. The long-time tracking with discriminative correlation filter (LT-DCF) was proposed to solve these deficiencies. We use larger size detection image blocks and smaller size filters to increase the proportion of real samples to solve the boundary effects of fast motion. And we combine the histogram of oriented gradient (HOG) feature detection and scale-invariant feature transform (SIFT) key point detection to solve the obstacles caused by scale variations. The detector with deep feature flow is then incorporated into the tracker to detect key frames to improve tracking accuracy. This method has achieved more than 75\% of the distance accuracy and 70\% of the overlapping success rate on the VOT2015 and VOT2016 datasets, and the stable tracking video length can reach 6895 frames.},
  archive      = {J_COMJNL},
  author       = {Gong, Faming and Yue, Hanbing and Yuan, Xiangbing and Gong, Wenjuan and Song, Tao},
  doi          = {10.1093/comjnl/bxz049},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {460-468},
  shortjournal = {Comput. J.},
  title        = {Discriminative correlation filter for long-time tracking},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new real-time link prediction method based on user
community changes in online social networks. <em>COMJNL</em>,
<em>63</em>(3), 448–459. (<a
href="https://doi.org/10.1093/comjnl/bxz050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The link prediction problem is becoming an important area of online social network (OSN) research. The existing methods that have been developed to address this problem mostly try to predict links based on structural information about the whole of the user lifespan. In addition, most of them do not consider user attributes such as user weight, density of interaction and geo-distance, all of which have an influence on the prediction of future links in OSNs due to the human-centric nature of these networks. Moreover, an OSN is a dynamic environment because users join and leave communities based on their interests over time. Therefore, it is necessary to predict links in real time. Therefore, the current study proposes a new method based on time and user attributes to predict links based on changes in user communities, where the changes in the user communities are indicative of users’ interests. The proposed method is tested on the UKM dataset and its performance is compared with that of 10 well-known methods and another community-based method. The area-under-the-curve results show that the proposed method is more accurate than all of the compared methods.},
  archive      = {J_COMJNL},
  author       = {Mahmoudi, Amin and Yaakub, Mohd Ridzwan and Abu Bakar, Azuraliza},
  doi          = {10.1093/comjnl/bxz050},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {448-459},
  shortjournal = {Comput. J.},
  title        = {A new real-time link prediction method based on user community changes in online social networks},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel data mining on breast cancer survivability using MLP
ensemble learners. <em>COMJNL</em>, <em>63</em>(3), 435–447. (<a
href="https://doi.org/10.1093/comjnl/bxz051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Breast cancer survivability has always been an important and challenging issue for researchers. Different methods have been utilized mostly based on machine learning techniques for prediction of survivability among cancer patients. The most comprehensive available database of cancer incidence is SEER in the United States, which has been frequently used for different research purposes. In this paper, a new data mining has been performed on the SEER database in order to investigate the ability of machine learning techniques for survivability prediction of breast cancer patients. To this end, the data related to breast cancer incidence have been preprocessed to remove unusable records from the dataset. In sequel, two machine learning techniques were developed based on the Multi-Layer Perceptron (MLP) learner machine including MLP stacked generalization and mixture of MLP-experts to make predictions over the database. The machines have been evaluated using K-fold cross-validation technique. The evaluation of the predictors revealed an accuracy of 84.32\% and 83.86\% by the mixture of MLP-experts and MLP stacked generalization methods, respectively. This indicates that the predictors can be significantly used for survivability prediction suggesting time- and cost-effective treatment for breast cancer patients.},
  archive      = {J_COMJNL},
  author       = {Salehi, Mohsen and Razmara, Jafar and Lotfi, Shahriar},
  doi          = {10.1093/comjnl/bxz051},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {435-447},
  shortjournal = {Comput. J.},
  title        = {A novel data mining on breast cancer survivability using MLP ensemble learners},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A comprehensive analysis of 2D&amp;3D video watching of EEG
signals by increasing PLSR and SVM classification results.
<em>COMJNL</em>, <em>63</em>(3), 425–434. (<a
href="https://doi.org/10.1093/comjnl/bxz043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the development of two- and three-dimensional (2D&amp;3D) technology, it has attracted the attention of researchers in recent years. This research is done to reveal the detailed effects of 2D in comparison with 3D technology on the human brain waves. The impact of 2D&amp;3D video watching using electroencephalography (EEG) brain signals is studied. A group of eight healthy volunteers with the average age of 31 ± 3.06 years old participated in this three-stage test. EEG signal recording consisted of three stages: After a bit of relaxation (a), a 2D video was displayed (b), the recording of the signal continued for a short period of time as rest (c), and finally the trial ended. Exactly the same steps were repeated for the 3D video. Power spectrum density (PSD) based on short time Fourier transform (STFT) was used to analyze the brain signals of 2D&amp;3D video viewers. After testing all the EEG frequency bands, delta and theta were extracted as the features. Partial least squares regression (PLSR) and Support vector machine (SVM) classification algorithms were considered in order to classify EEG signals obtained as the result of 2D&amp;3D video watching. Successful classification results were obtained by selecting the correct combinations of effective channels representing the brain regions.},
  archive      = {J_COMJNL},
  author       = {Manshouri, Negin and Kayikcioglu, Temel},
  doi          = {10.1093/comjnl/bxz043},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {425-434},
  shortjournal = {Comput. J.},
  title        = {A comprehensive analysis of 2D&amp;3D video watching of EEG signals by increasing PLSR and SVM classification results},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel aspect-based sentiment analysis network model based
on multilingual hierarchy in online social network. <em>COMJNL</em>,
<em>63</em>(3), 410–424. (<a
href="https://doi.org/10.1093/comjnl/bxz031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, sentiment analysis based on aspects has become one of the research hotspots in the field of natural language processing. Aiming at the fact that the existing network model cannot fully obtain the interrelationship between sentences in the same comment and the long-distance dependence of specific aspects in the whole comment, a multilingual deep hierarchical model combining regional convolutional neural network and bidirectional LSTM network is proposed. The model obtains the time series relationship of different sentences in the comments through the regional CNN, and obtains the local features of the specific aspects in the sentence and the long-distance dependence in the whole comment through the hierarchical attention network. In addition, the model improves the word vector representation based on the gate mechanism to make the model completely independent of the language. Experimental results for different domain datasets in multi-language show that the proposed model achieves better classification results than the traditional deep network model, the network model combining with the attention mechanism and considering the relationship between sentences.},
  archive      = {J_COMJNL},
  author       = {Liu, Guangfeng and Huang, Xianying and Liu, Xiaoyang and Yang, Anzhi},
  doi          = {10.1093/comjnl/bxz031},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {410-424},
  shortjournal = {Comput. J.},
  title        = {A novel aspect-based sentiment analysis network model based on multilingual hierarchy in online social network},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multiple-layer machine learning architecture for improved
accuracy in sentiment analysis. <em>COMJNL</em>, <em>63</em>(3),
395–409. (<a href="https://doi.org/10.1093/comjnl/bxz038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Twitter is an online micro-blogging platform through which one can explore the hidden valuable and delightful information about the current context at any point of time, which also serves as a data source to carry out sentiment analysis. In this paper, the sentiments of large amount of tweets generated from Twitter in the form of big data have been analyzed using machine learning algorithms. A multi-tier architecture for sentiment classification is proposed in this paper, which includes modules such as tokenization, data cleaning, preprocessing, stemming, updated lexicon, stopwords and emoticon dictionaries, feature selection and machine learning classifier. Unigram and bigrams have been used as feature extractors together with ; (Chi-squared) and Singular Value Decomposition for dimensionality reduction together with two model types (Binary and Reg), with four types of scaling methods (No scaling, Standard, Signed and Unsigned) and represented them in three different vector formats (TF-IDF, Binary and Int). Accuracy is considered as the evaluation standard for random forest and bagged trees classification methods. Sentiments were analyzed through tokenization and having several stages of pre-processing and several combinations of feature vectors and classification methods. Through which it was possible to achieve an accuracy of 84.14\%. Obtained results conclude that, the proposed scheme gives a better accuracy when compared with existing schemes in the literature.},
  archive      = {J_COMJNL},
  author       = {Shyamasundar, L B and Jhansi Rani, P},
  doi          = {10.1093/comjnl/bxz038},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {395-409},
  shortjournal = {Comput. J.},
  title        = {A multiple-layer machine learning architecture for improved accuracy in sentiment analysis},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enhanced SVM–KPCA method for brain MR image classification.
<em>COMJNL</em>, <em>63</em>(3), 383–394. (<a
href="https://doi.org/10.1093/comjnl/bxz035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated classification of magnetic resonance brain images (MRIs) is a hot topic in the field of medical and biomedical imaging. Various methods have been suggested recently to improve this technology. In this paper, to reduce the complexity involved in the medical images and to ameliorate the classification of MRIs, a novel 3D magnetic resonance (MR) brain image classifier using kernel principal component analysis (KPCA) and support vector machines (SVMs) is proposed. Experiments are carried out using A deep multiple kernel SVM (DMK-SVM) and a regular SVM. An algorithm entitled SVM–KPCA is put forward. Its main task is to classify a brain MRI as a normal brain image or as a pathological brain image. This algorithm, firstly, adopts the discrete wavelet transform technique to extract features from images. Secondly, KPCA is applied to decrease the dimensionality of features. SVM is then applied to the reduced data. A K-fold cross-validation strategy is used to avoid overfitting and to ameliorate the generalization of the SVM–KPCA algorithm. Three databases are used to validate the suggested SVM–KPCA method. Three conclusions are obtained from this work. First, KPCA is highly efficient in increasing the classifier’s performance compared with similar algorithms working on the proposed database. Second, the SVM–KPCA algorithm performs well in differentiating between two classes of medical images. Third, the approach is robust and might be utilized for other MRIs. This proposes a significant role for computer aided diagnosis analysis systems used for clinical practice.},
  archive      = {J_COMJNL},
  author       = {Neffati, Syrine and Ben Abdellafou, Khaoula and Taouali, Okba and Bouzrara, Kais},
  doi          = {10.1093/comjnl/bxz035},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {383-394},
  shortjournal = {Comput. J.},
  title        = {Enhanced SVM–KPCA method for brain MR image classification},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A two-step knowledge extraction framework for improving
disease diagnosis. <em>COMJNL</em>, <em>63</em>(3), 364–382. (<a
href="https://doi.org/10.1093/comjnl/bxz034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last decades, various methodologies have been proposed by the researchers for developing effective disease diagnosis support systems (DDSSs). The present research proposes a ; framework in which an entropy-based feature-selection approach is introduced in the first step and a ; hybrid model using Perfect Rule Induction by Sequential Method (PRISM) is explored in the subsequent step for effective diagnosis of diseases. The suggested feature-selection technique is validated using five state-of-the-art classifiers namely C4.5 (a decision tree-based classifier), naïve Bayes (NB), Repeated Incremental Pruning to Produce Error Reduction (RIPPER), neural network (NN) and support vector machine (SVM) over fourteen benchmark diseases that are very common and the leading causes of deaths. Next, on the basis of top three performance metrics, viz., ; , ; and ; , the performance of the hybrid model over the datasets is compared with its base learner: PRISM, two other competent learners namely C4.5 and NN, and some specialized models. The empirical outcomes positively demonstrate that the hybrid model with application of feature-selection method is a generic model and effective in diagnosing diseases. More importantly, the model not only is able to produce good results but also to elucidate its knowledge in understandable: IF-THEN form (convenient for medical practitioners).},
  archive      = {J_COMJNL},
  author       = {Sarkar, Bikash Kanti},
  doi          = {10.1093/comjnl/bxz034},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {364-382},
  shortjournal = {Comput. J.},
  title        = {A two-step knowledge extraction framework for improving disease diagnosis},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Emotion recognition by a hybrid system based on the features
of distances and the shapes of the wrinkles. <em>COMJNL</em>,
<em>63</em>(3), 351–363. (<a
href="https://doi.org/10.1093/comjnl/bxz032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion recognition is a key work of research area in brain computer interactions. With the increasing concerns about affective computing, emotion recognition has attracted more and more attention in the past decades. Focusing on geometric positions of key parts of the face and well detecting them is the best way to increase accuracy of emotion recognition systems and reach high classification rates. In this paper, we propose a hybrid system based on wavelet networks using 1D Fast Wavelet Transform. This system combines two approaches: the biometric distances approach where we propose a new technique to locate feature points and the wrinkles approach where we propose a new method to locate the wrinkles regions in the face. The classification rates given by experimental results show the effectiveness of our proposed approach compared to other methods.},
  archive      = {J_COMJNL},
  author       = {Afdhal, Rim and Ejbali, Ridha and Zaied, Mourad},
  doi          = {10.1093/comjnl/bxz032},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {351-363},
  shortjournal = {Comput. J.},
  title        = {Emotion recognition by a hybrid system based on the features of distances and the shapes of the wrinkles},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An improved seed point selection-based unsupervised color
clustering for content-based image retrieval application.
<em>COMJNL</em>, <em>63</em>(3), 337–350. (<a
href="https://doi.org/10.1093/comjnl/bxz017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The images involved in the content-based image retrieval (CBIR) applications are collectively represented by features such as color, texture and shape. The precision of the CBIR application relies on the key features used in image representation and its similarity measure. In CBIR, dominant color feature extraction is affected by the predefined intervals used in color quantization. The proposed work mainly concentrates on extracting the dominant color information of the image using the clustering process. The clustering process is initiated by the proposed seed point’s selection approach. This approach derives the number of seed points using the first order statistical measure and maximum range of the distributed pixel values. Moreover, this work gives equal priority to dominant color and its occurrence information in calculating the similarity between query and database images. Finally, the standard databases such as SIMPLIcity, Corel-10k, OT-scene, Oxford flower and GHIM are taken to investigate the performance of the proposed dominant color based image retrieval application.},
  archive      = {J_COMJNL},
  author       = {Pavithra, L K and Sree Sharmila, T},
  doi          = {10.1093/comjnl/bxz017},
  journal      = {The Computer Journal},
  number       = {3},
  pages        = {337-350},
  shortjournal = {Comput. J.},
  title        = {An improved seed point selection-based unsupervised color clustering for content-based image retrieval application},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). WS-BD-based two-level match: Interesting sequential patterns
and bayesian fuzzy clustering for predicting the web pages from weblogs.
<em>COMJNL</em>, <em>63</em>(2), 322–336. (<a
href="https://doi.org/10.1093/comjnl/bxz132">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid increase in information and technology has led to the increased amount of web pages, which raises the complexity in sticking to relevant web pages, and the visitor suffers due to wastage of time resulting in lack of satisfaction. This paper proposes a web page prediction method using a weighed support and Bhattacharya distance-based (WS-BD) two-level match. The major aim of the proposed method is to attain customer satisfaction. Initially, interesting sequential patterns are obtained using the weighed support that filters the sequential patterns obtained using a PrefixSpan algorithm based on the frequency, duration and recurrence of the web pages. Interesting sequential patterns are clustered using the proposed dice similarity-based Bayesian fuzzy clustering, and the web page is predicted using the two-level match based on Bhattacharya distance. The experimentation is performed using the CTI and MSNBC data which proves the effectiveness of the proposed method. The proposed method shows 9.59, 21.22 and 10.17\% improvement than the existing FCM-KNN in terms of precision, recall and ; measure for the CTI dataset. Also, the proposed method shows 2.58, 22.17 and 7.83\% improvement than the existing FCM-KNN in terms of precision, recall and ; measure for the MSNBC dataset.},
  archive      = {J_COMJNL},
  author       = {Prakash, Pg Om and Jaya, A},
  doi          = {10.1093/comjnl/bxz132},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {322-336},
  shortjournal = {Comput. J.},
  title        = {WS-BD-based two-level match: Interesting sequential patterns and bayesian fuzzy clustering for predicting the web pages from weblogs},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). WHDA-FCM: Wolf hunting-based dragonfly with fuzzy c-mean
clustering for change detection in SAR images. <em>COMJNL</em>,
<em>63</em>(2), 308–321. (<a
href="https://doi.org/10.1093/comjnl/bxz130">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the past few years, the automated addressing of changes in remote sensing images plays a significant role. However, the change detection (CD) model often suffers from the issue of speckle noise. More investigations have been proceeded to overcome this obstacle. This paper also considers the same issue and proposes a new CD model in synthetic aperture radar (SAR) images. Here, two SAR images that are captivated at different times will be considered as the input of the detection process. At first, discrete wavelet transform is incurred for image fusion, where the coefficients are optimally selected through a hybrid model that hybridizes the gray wolf optimization and dragonfly (DA) optimization. At last, the fused images after inverse transform are clustered via the fuzzy c-mean (FCM) clustering approach, and a similarity measure is performed between the segmented image and the ground truth image. The proposed model, wolf hunting-based DA with FCM, compares its performance over other conventional methods in terms of measures like accuracy, specificity, sensitivity, precision, negative predictive value, ; score and Matthews correlation coefficient. Similarly, the negative measures are false positive rate, false negative rate and false discovery rate, and the betterment is proven.},
  archive      = {J_COMJNL},
  author       = {Kumar, J Thrisul and Reddy, Y Mallikarjuna and Rao, B Prabhakara},
  doi          = {10.1093/comjnl/bxz130},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {308-321},
  shortjournal = {Comput. J.},
  title        = {WHDA-FCM: Wolf hunting-based dragonfly with fuzzy C-mean clustering for change detection in SAR images},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Virtual infrastructure orchestration for cloud service
deployment. <em>COMJNL</em>, <em>63</em>(2), 295–307. (<a
href="https://doi.org/10.1093/comjnl/bxz125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud adoption has significantly increased using the infrastructure-as-a-service (IaaS) paradigm, in order to meet the growing demands of computing, storage and networking, in small as well as large enterprises. Different vendors provide their customized solutions for OpenStack deployment on bare metal or virtual infrastructure. Among these many available IaaS solutions, OpenStack stands out as being an agile and open-source platform. However, its deployment procedure is a time-taking and complex process with a learning curve. This paper addresses the lack of basic infrastructure automation in almost all of the OpenStack deployment projects. We propose a flexible framework to automate the process of infrastructure bring up for deployment of several OpenStack distributions, as well as resolving dependencies for a successful deployment. Our experimental results demonstrate the effectiveness of the proposed framework in terms of automation status and deployment time, that is, reducing the time spent in preparing a basic virtual infrastructure by four times, on average.},
  archive      = {J_COMJNL},
  author       = {Qadeer, Arslan and Waqar Malik, Asad and Ur Rahman, Anis and Mian Muhammad, Hamayun and Ahmad, Arsalan},
  doi          = {10.1093/comjnl/bxz125},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {295-307},
  shortjournal = {Comput. J.},
  title        = {Virtual infrastructure orchestration for cloud service deployment},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MNVPCS: Multinode virtual-point–based charging scheme to
prolong the lifetime of wireless rechargeable sensor networks.
<em>COMJNL</em>, <em>63</em>(2), 283–294. (<a
href="https://doi.org/10.1093/comjnl/bxz110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Wireless-charging technology can utilize a mobile wireless charging vehicle (WCV) to rescue dying nodes by supplementing their remaining energy, and using WCVs in this way forms wireless rechargeable sensor networks (WRSNs). However, a WCV in a WRSN encounters several challenges, collectively called the optimized charging problem. This problem involves a set of sensor nodes randomly distributed on the ground for which the WCV must determine an appropriate travel path to charge the sensor nodes. Because these sensor nodes have different workloads, they exhibit different energy consumption profiles over time. Resolving the above-mentioned problem requires the determination of the priority of charging the sensor nodes based on the order in which they are expected to die and subsequently finding the most efficient path to charge the sensor nodes such that sensor death is avoided for as long as possible. Furthermore, the most efficient placement of the charging point needs to be considered when planning the charging path. To address this, the proposed multinode virtual point-based charging scheme (MNVPCS) considers both the planning of an efficient charging and the best location for the charging point. Experimental results show that MNVPCS can improve the lifetime of the entire WRSN and substantially outperform other methods on this measure.},
  archive      = {J_COMJNL},
  author       = {Chang, Hong-Yi and Hang, Zih-Huan and Tzang, Yih-Jou},
  doi          = {10.1093/comjnl/bxz110},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {283-294},
  shortjournal = {Comput. J.},
  title        = {MNVPCS: Multinode virtual-Point–Based charging scheme to prolong the lifetime of wireless rechargeable sensor networks},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stochastic gradient descent–whale optimization
algorithm-based deep convolutional neural network to crowd emotion
understanding. <em>COMJNL</em>, <em>63</em>(2), 267–282. (<a
href="https://doi.org/10.1093/comjnl/bxz103">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd emotion understanding is an interesting research area that assists the security personnel to read the emotion/activity of the crowd in the locality. Most of the traditional methods utilize the low-level visual features to understand the crowd emotions that extend the gap between the low- and the high-level features. With the aim to develop an automatic method for emotion recognition, this paper utilizes the deep convolutional neural network (deep CNN). For the effective emotion recognition, it is essential to select the key frames of the video using the wavelet-based Bhattacharya distance. The key frames are fed to the space-time interest points descriptor that extracts the features and forms the input vector to the classifier. Deep CNN is trained using the proposed Stochastic Gradient Descent–Whale Optimization Algorithm, which is the unification of the standard stochastic gradient descent algorithm with whale optimization algorithm. The proposed classifier recognizes the emotions of the crowd, such as angry, escape, fight, happy, normal, running/walking and violence. The analysis of the method proved that the proposed approaches acquired a maximal accuracy, specificity and sensitivity of 0.9693, 0.9936 and 0.9675, respectively.},
  archive      = {J_COMJNL},
  author       = {Ratre, Avinash},
  doi          = {10.1093/comjnl/bxz103},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {267-282},
  shortjournal = {Comput. J.},
  title        = {Stochastic gradient Descent–Whale optimization algorithm-based deep convolutional neural network to crowd emotion understanding},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Polynomial silent self-stabilizing p-star decomposition.
<em>COMJNL</em>, <em>63</em>(2), 253–266. (<a
href="https://doi.org/10.1093/comjnl/bxz102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a silent self-stabilizing distributed algorithm computing a maximal ; -star decomposition of the underlying communication network. Under the unfair distributed scheduler, the most general scheduler model, the algorithm converges in at most ; moves, where ; is the number of edges, ; is the number of nodes and ; is the maximum node degree. Regarding the time complexity, we obtain the following results: our algorithm outperforms the previously known best algorithm by a factor of ; with respect to the move complexity. While the round complexity for the previous algorithm was unknown, we show a ; bound for our algorithm.},
  archive      = {J_COMJNL},
  author       = {Haddad, Mohammed and Johnen, Colette and Köhler, Sven},
  doi          = {10.1093/comjnl/bxz102},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {253-266},
  shortjournal = {Comput. J.},
  title        = {Polynomial silent self-stabilizing p-star decomposition},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tracy–singh product and genetic whale optimization algorithm
for retrievable data perturbation for privacy preserved data publishing
in cloud computing. <em>COMJNL</em>, <em>63</em>(2), 239–253. (<a
href="https://doi.org/10.1093/comjnl/bxz101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a retrievable data perturbation model for overcoming the challenges in cloud computing. Initially, genetic whale optimization algorithm (genetic WOA) is developed by integrating genetic algorithm (GA) and WOA for generating the optimized secret key. Then, the input data and the optimized secret key are given to the Tracy–Singh product-based model for transforming the original database into perturbed database. Finally, the perturbed database can be retrieved by the client, if and only if the client knows the secret key. The performance of the proposed model is analyzed using three databases, namely, chess, T10I4D100K and retail databases from the FIMI data set based on the performance metrics, privacy and utility. Also, the proposed model is compared with the existing methods, such as Retrievable General Additive Data Perturbation, GA and WOA, for the key values 128 and 256. For the key value 128, the proposed model has the better privacy and utility of 0.18 and 0.83 while using the chess database. For the key value 256, the proposed model has the better privacy and utility of 0.18 and 0.85, using retail database. From the analysis, it can be shown that the proposed model has better privacy and utility values than the existing models.},
  archive      = {J_COMJNL},
  author       = {Revathi S, Thanga and Ramaraj, N and Chithra, S},
  doi          = {10.1093/comjnl/bxz101},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {239-253},
  shortjournal = {Comput. J.},
  title        = {Tracy–Singh product and genetic whale optimization algorithm for retrievable data perturbation for privacy preserved data publishing in cloud computing},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A snap-stabilizing m-wave algorithm for tree networks.
<em>COMJNL</em>, <em>63</em>(2), 220–238. (<a
href="https://doi.org/10.1093/comjnl/bxz100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose the first efficient, reliable and safe snap-stabilizing ; -; (; wave) algorithm implementing concurrent waves started by multiple initiator processes in tree networks. Unlike its self-stabilizing counterpart and broadcast algorithms, the proposed algorithm is a reliable and safe ; -; algorithm, i.e. it implements an ; -; such that after it is started in an arbitrary initial configuration or after faults, the first ; -; started is guaranteed to reach all system processes as per its specification. In addition, the proposed ; -; algorithm increases the efficiency of the broadcast compared to a ; -; algorithm by allowing the broadcast to be initiated by multiple processes. Our proposed algorithm is safe in the sense that after initiating an ; -wave, it completes as per the specifications. Additionally, it is reliable as the algorithm guarantees that each process receives at least one broadcast from an initiator process. Furthermore, the proposed algorithm significantly reduces the broadcast completion time compared to that of its single counterparts. Solutions to global-snapshots, distributed broadcast and various synchronization problems can be solved efficiently using waves with multiple concurrent initiators. In addition, the proposed algorithm has applications in mobile wireless sensor networks, VANETs and military communication networks.},
  archive      = {J_COMJNL},
  author       = {Hakan Karaata, Mehmet and Nais AlMutairi, Anwar},
  doi          = {10.1093/comjnl/bxz100},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {220-238},
  shortjournal = {Comput. J.},
  title        = {A snap-stabilizing m-wave algorithm for tree networks},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Traffic-centric mesoscopic analysis of connectivity in
VANETs. <em>COMJNL</em>, <em>63</em>(2), 203–219. (<a
href="https://doi.org/10.1093/comjnl/bxz094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vehicular ad hoc networks (VANETs) have emerged as an appropriate class of information propagation technology promising to link us even while moving at high speeds. In VANETs, a piece of information propagates through consecutive connections. In the most previous vehicular connectivity analysis, the provided probability density function of intervehicle distance throughout the wide variety of steady-state traffic flow conditions is surprisingly invariant. But, using a constant assumption, generates approximate communication results, prevents us from improving the performance of the current solutions and impedes designing the new applications on VANETs. Hence, in this paper, a mesoscopic vehicular mobility model in a multilane highway with a steady-state traffic flow condition is adopted. To model a traffic-centric distribution for the spatial per-hop progress and the expected spatial per-hop progress, different intervehicle distance distributions are utilized. Moreover, the expected number of hops, distribution of the number of successful multihop forwarding, the expected time delay and the expected connectivity distance are mathematically investigated. Finally, to model the distribution of the connectivity distances, a set of simplistic closed-form traffic-centric equations is proposed. The accuracy of the proposed model is confirmed using an event-based network simulator as well as a road traffic simulator.},
  archive      = {J_COMJNL},
  author       = {Zarei, Mani},
  doi          = {10.1093/comjnl/bxz094},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {203-219},
  shortjournal = {Comput. J.},
  title        = {Traffic-centric mesoscopic analysis of connectivity in VANETs},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Performance evaluation of depth adjustment and void aware
pressure routing (DA-VAPR) protocol for underwater wireless sensor
networks. <em>COMJNL</em>, <em>63</em>(2), 193–202. (<a
href="https://doi.org/10.1093/comjnl/bxz093">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater wireless sensor network (UWSN) has gained its popularity as a powerful technology for monitoring oceans, sea and river. The sensor node drifting along with ocean current offers 4D (space and time) monitoring for real-time underwater application. However, the main challenge arises from the underwater acoustic communication that results in high propagation delay, packet loss and overhead in the network. In order to overcome these issues, a depth adjustment and void aware pressure routing protocol is proposed for UWSN. A greedy forwarding strategy is used to forward the packet. In case a node fails to forward the packet using greedy forwarding strategy, then it immediately switches to the recovery mode. In the recovery mode, the node determines the new depth using particle swarm optimization technique. The global best value gives the new depth with minimum displacement. The void node forwards the packet with minimum displacement without any packet loss and delay.},
  archive      = {J_COMJNL},
  author       = {Ganesh, N},
  doi          = {10.1093/comjnl/bxz093},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {193-202},
  shortjournal = {Comput. J.},
  title        = {Performance evaluation of depth adjustment and void aware pressure routing (DA-VAPR) protocol for underwater wireless sensor networks},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Power saving proxies for web servers. <em>COMJNL</em>,
<em>63</em>(2), 179–192. (<a
href="https://doi.org/10.1093/comjnl/bxz081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electricity is a major cost in running a data centre, and servers are responsible for a significant percentage of the power consumption. Given the widespread use of HTTP, both as a service and a component of other services, it is worthwhile reducing the power consumption of web servers. In this paper we consider how reverse proxies, commonly used to improve the performance of web servers, might be used to improve energy efficiency. We suggest that when demand on a server is low, it may be possible to switch off servers. In their absence, an embedded system with a small energy footprint could act as a reverse proxy serving commonly-requested content. When new content is required, the reverse proxy can power on the servers to meet this new load. Our results indicate that even with a modest server, we can get a 25\% power saving while maintaining acceptable performance.},
  archive      = {J_COMJNL},
  author       = {O’Dwyer, Karl J and Creedon, Eoin and Purcell, Mark and Malone, David},
  doi          = {10.1093/comjnl/bxz081},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {179-192},
  shortjournal = {Comput. J.},
  title        = {Power saving proxies for web servers},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive neuro-fuzzy inference system-based nonlinear
equalizer for CO-OFDM systems. <em>COMJNL</em>, <em>63</em>(2), 169–178.
(<a href="https://doi.org/10.1093/comjnl/bxz072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The principle of orthogonal frequency-division multiplexing (OFDM) is to transmit the data through a large number of multiple orthogonal subcarriers. The coherent optical OFDM (CO-OFDM) is OFDM data that are being modulated to light frequency and being detected in coherent manner. CO-OFDM brings to optical communications the combination of two powerful techniques, coherent optical detection and OFDM. One of the primary challenges in the CO-OFDM system is to remove optical fiber nonlinear effects. This makes nonlinearity compensation a critical task of the CO-OFDM system. So a nonlinear equalizer (NLE) based on adaptive neuro-fuzzy inference system (ANFIS) is presented for CO-OFDM systems to mitigate nonlinearities on long-haul optical communications with high bit rate and bit error rate (BER)of the system. Various performance metrics were analyzed for the proposed ANFIS–NLE, and it is compared with existing techniques such as support vector machine and artificial neural network. From the experimental results, our proposed approach gives better performance in terms of BER and Q-factor on comparing with existing methods.},
  archive      = {J_COMJNL},
  author       = {Raj, Ajay Amrit and Dejey},
  doi          = {10.1093/comjnl/bxz072},
  journal      = {The Computer Journal},
  number       = {2},
  pages        = {169-178},
  shortjournal = {Comput. J.},
  title        = {Adaptive neuro-fuzzy inference system-based nonlinear equalizer for CO-OFDM systems},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimization-based support vector neural network for speaker
recognition. <em>COMJNL</em>, <em>63</em>(1), 151–167. (<a
href="https://doi.org/10.1093/comjnl/bxz012">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speaker recognition is a rapidly emerging area of research. Voice biometrics that is obtained from the speaker’s behavior or physical related features provides a pattern of data that accommodate sensitive information about the speaker. The effectiveness of speaker recognition systems is seen to decrease expeditiously due to the mismatch occurrence, including channel degradations and noise. With the aim to assure security and effective recognition, an adaptive fractional bat-based support vector neural network (AFB-based SVNN classifier) is employed to recognize a speaker for which the frequency-dependent features, such as multiple kernel weighted Mel frequency cepstral coefficient (MKMFCC), spectral kurtosis, spectral skewness and autocorrelation are used. The classification is performed using the SVNN classifier based on the extracted features and the classifier is tuned optimally using the proposed Adaptive Fractional Bat algorithm, which is the modification of fractional BAT optimization using the adaptive concept. The experimental result of the proposed method reveals that the accuracy of 0.95, FAR of 0.05, and FRR of 0.05 is obtained, which proved that the proposed method acquired better accuracy and less error value as compared to existing methods.},
  archive      = {J_COMJNL},
  author       = {Srinivas, Vasamsetti and Santhirani, Ch},
  doi          = {10.1093/comjnl/bxz012},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {151-167},
  shortjournal = {Comput. J.},
  title        = {Optimization-based support vector neural network for speaker recognition},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A graph grammar approach to the design and validation of
floor plans. <em>COMJNL</em>, <em>63</em>(1), 137–150. (<a
href="https://doi.org/10.1093/comjnl/bxz002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers have proposed many approaches to generate floor plans using shape grammars. None of them, however, testifies the semantic relations among rooms. This paper presents a generic approach for grammar specification, grammar induction, validation, and design generation of house floor plans using their path graphs based on the reserved graph grammar (RGG) formalism. In our approach, the connectivity of a floor plan is analyzed by user-specified graph grammar transformation rules, also known as ; . Floor plans of houses in different styles share common attributes while retaining specific features. By identifying these features, our approach validates floor plans in different styles with user-specified graph productions. A graph grammar induction engine is also introduced to assist designers by automatically inferring graph productions from an input graph set. In addition, the derivation process in RGG offers the capability of generating floor plan designs. Two types of constraints, specified as attribute-sets, are introduced to generate floor plans meeting a wide range of requirements. To evaluate this generic approach, we design a set of productions to validate and generate floor plans in the style of Frank Lloyd Wright’s prairie houses. The results are discussed, and further research is suggested.},
  archive      = {J_COMJNL},
  author       = {Wang, Xiao-Yu and Liu, Yu-Feng and Zhang, Kang},
  doi          = {10.1093/comjnl/bxz002},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {137-150},
  shortjournal = {Comput. J.},
  title        = {A graph grammar approach to the design and validation of floor plans},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Algorithm-switching-based last-level cache structure with
hybrid main memory architecture. <em>COMJNL</em>, <em>63</em>(1),
123–136. (<a href="https://doi.org/10.1093/comjnl/bxz004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this research, we designed an algorithm-switching (AS)-based last-level cache (LLC) structure with DRAM-NAND Flash hybrid main memory architecture. In order to take full advantage of previous memory access patterns and achieve high performance in the upper level of memory hierarchy, an AS-based clustering engine that uses k-means, k-medoids and k-center clustering algorithms was applied to LLC. The proposed LLC consists of three major parts, namely a set-divisible cache, and victim and clustering buffers. The victim and clustering buffers efficiently managed the history of cache blocks evicted from the set-divisible cache through the AS-based engine mechanism. The experimental results that were evaluated using Redis application and YCSB benchmark show that compared with conventional LLC structure, the proposed AS-based LLC structure could reduce the total execution time by 19.50\%, power consumption by 16.31\%, and NAND-Flash memory write count by 8.6\%.},
  archive      = {J_COMJNL},
  author       = {Li, Xian-Shu and Yoon, Su-Kyung and Kim, Jeong-Geun and Burgstaller, Bernd and Kim, Shin-Dug},
  doi          = {10.1093/comjnl/bxz004},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {123-136},
  shortjournal = {Comput. J.},
  title        = {Algorithm-switching-based last-level cache structure with hybrid main memory architecture},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Prioritizing methods to accelerate probabilistic model
checking of discrete-time markov models. <em>COMJNL</em>,
<em>63</em>(1), 105–122. (<a
href="https://doi.org/10.1093/comjnl/bxz001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic model checking is an automated technique for the verification of systems that exhibit stochastic behavior. Iterative numerical methods are usually used to solve quantitative verification problems of probabilistic models. In this paper, we consider Markov Decision Processes and propose three techniques to improve the performance of the iterative methods. While several methods have been proposed to improve the performance of the standard iterative methods, their performance depends on the structure of the models, and they are more useful for acyclic models. In contrast, we propose several heuristic methods to improve the performance of the standard iteration methods that are more useful for cyclic models. The first heuristic method prioritizes states according to their impact on the other states. The second prioritizes transitions according to their probability. In these two approaches, low priority states and transitions can be avoided in some iterations while the method reuses related values from the previous iteration. The third method reorders the information of the model to improve the memory access and reduce the impact of cache latency. Experimental results demonstrate that our methods outperform other iterative approaches for most case studies.},
  archive      = {J_COMJNL},
  author       = {Mohagheghi, Mohammadsadegh and Karimpour, Jaber and Isazadeh, Ayaz},
  doi          = {10.1093/comjnl/bxz001},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {105-122},
  shortjournal = {Comput. J.},
  title        = {Prioritizing methods to accelerate probabilistic model checking of discrete-time markov models},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SMT-LH: A new satisfiability modulo theory-based technique
for solving vehicle routing problem with time window constraints.
<em>COMJNL</em>, <em>63</em>(1), 91–104. (<a
href="https://doi.org/10.1093/comjnl/bxy127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The vehicle routing problem (VRP) is the problem of designing a specific number of routes to serve a given number of customers while achieving minimum total travel distance and minimum number of vehicles. VRP with time window constraints (VRPTW) restricts that the service time of each customer should start within a time window. This problem and its variations are known to be NP-hard. This paper proposes a transformation of the NP-hard problem into another. The approach is based on satisfiability modulo theories (SMT) in order to take advantage of its relative CPU time competitiveness. This poses transformability challenges in order to apply SMT solving techniques on the VRPTW. In this work, SMT is successfully applied to solve the VRPTW. The solution quality proves to be competitive to other optimization techniques since the solving time is significantly decreased opening the door to solve much larger problems. In addition, it sets a proof for using SMT efficiently as a main step for solving optimization problems.},
  archive      = {J_COMJNL},
  author       = {Rizkallah, Lydia W and Ahmed, Mona F and Darwish, Nevin M},
  doi          = {10.1093/comjnl/bxy127},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {91-104},
  shortjournal = {Comput. J.},
  title        = {SMT-LH: A new satisfiability modulo theory-based technique for solving vehicle routing problem with time window constraints},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The g-good-neighbor conditional diagnosability of locally
exchanged twisted cubes. <em>COMJNL</em>, <em>63</em>(1), 80–90. (<a
href="https://doi.org/10.1093/comjnl/bxy145">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Connectivity and diagnosability are important parameters in measuring the fault tolerance and reliability of interconnection networks. The ; -vertex-connectivity of a connected graph ; is the minimum cardinality of a faulty set ; such that ; is disconnected and every fault-free vertex has at least ; fault-free neighbors. The ; -good-neighbor conditional diagnosability is defined as the maximum cardinality of a ; -good-neighbor conditional faulty set that the system can guarantee to identify. The interconnection network considered here is the locally exchanged twisted cube ; . For ; and ; , we first determine the ; -vertex-connectivity of ; , then establish the ; -good-neighbor conditional diagnosability of ; under the PMC model and MM; model, respectively.},
  archive      = {J_COMJNL},
  author       = {Liu, Hui-qing and Hu, Xiao-lan and Gao, Shan},
  doi          = {10.1093/comjnl/bxy145},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {80-90},
  shortjournal = {Comput. J.},
  title        = {The g-good-neighbor conditional diagnosability of locally exchanged twisted cubes},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Redundancy-aware and budget-feasible incentive mechanism in
crowd sensing. <em>COMJNL</em>, <em>63</em>(1), 66–79. (<a
href="https://doi.org/10.1093/comjnl/bxy139">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Crowd sensing has emerged as a compelling paradigm for collecting sensing data over a vast area. It is of paramount importance for crowd sensing systems to provide effective incentive mechanisms. This paper studies the critical problem of maximizing the aggregate data utility under a budget constraint in incentive mechanism design in crowd sensing. This problem is particularly challenging given the redundancy in sensing data, self-interested and strategic user behavior, and private cost information of smartphones users. Most of existing mechanisms do not consider the important performance objective—maximizing the redundancy-aware data utility of sensing data collected from smartphones users. Furthermore, they do not consider the practical constraint on budget. In this paper, we propose an incentive mechanism based on a reverse auction framework. It consists of an approximation algorithm for winning user determination and a critical payment scheme. The approximation algorithm guarantees an approximation ratio for the aggregate data utility at polynomial-time complexity. The critical payment scheme guarantees truthful bidding. The rigorous theoretical analysis demonstrates that our mechanism achieves truthfulness, individual rationality, computational efficiency and budget feasibility.},
  archive      = {J_COMJNL},
  author       = {Li, Juan and Zhu, Yanmin and Yu, Jiadi},
  doi          = {10.1093/comjnl/bxy139},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {66-79},
  shortjournal = {Comput. J.},
  title        = {Redundancy-aware and budget-feasible incentive mechanism in crowd sensing},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). String generation for testing regular expressions.
<em>COMJNL</em>, <em>63</em>(1), 41–65. (<a
href="https://doi.org/10.1093/comjnl/bxy137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regular expressions have been widely studied due to their expressiveness and flexibility for various applications. A common yet challenging way to ensure the quality of regular expressions is regular expression testing. In this work, we study coverage criteria-based string generation for testing regular expressions. First, we propose a notion of pairwise coverage criterion for regular expressions and analyze the subsumption relationships with existing coverage criteria for both regular grammars and finite automata. Second, we design an algorithm that given as an input a regular expression, outputs a small set of strings that satisfies the pairwise coverage criterion. Third, we extend the coverage criterion and the generation algorithm to further deal with regular operators counting and interleaving. Fourth, we experimentally demonstrate the effectiveness and efficiency of our algorithms by testing element-type definitions of real-world XML schemas. Finally, we identify more applications of pairwise coverage and its corresponding generation algorithm and show that they can be used to generate characteristic samples for certain regular expression learning algorithms that follow Gold’s learning paradigm of learning (identification) in the limit. These results are not only theoretically meaningful but also useful for practical applications involved with regular expressions.},
  archive      = {J_COMJNL},
  author       = {Zheng, Lixiao and Ma, Shuai and Wang, Yuanyang and Lin, Gang},
  doi          = {10.1093/comjnl/bxy137},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {41-65},
  shortjournal = {Comput. J.},
  title        = {String generation for testing regular expressions},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Effects of edge centrality on random walks on graphs.
<em>COMJNL</em>, <em>63</em>(1), 25–40. (<a
href="https://doi.org/10.1093/comjnl/bxy131">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Random walks are a useful tool to describe and study various dynamical processes on networks. For some particular problems, it is more suitable to formulate them by biased random walks based on the properties of edges, for example, weight and importance. However, in some situations it is not easy to directly observe weight information of a network. In this paper, we show how to extract weight information of edges only from the topological knowledge of a binary network, based on which we develop a framework for biased random walks on the binary network. To this end, we first propose a centrality measure for edges based on line graph. We then present novel biased random walks on a binary network, called edge centrality based random walks (ECBRW), where the walker prefers to jump along edges with high centrality. Furthermore, we develop a series of techniques to derive analytical expressions for relevant quantities of ECBRW, including stationary distribution and hitting times. Finally, we study ECBRW on ; -ary trees, Barabási–Albert networks and some real networks. We find that the behavior for stationary distribution and hitting times for ECBRW on these networks differs significantly from those for unbiased random walks on corresponding networks.},
  archive      = {J_COMJNL},
  author       = {Lin, Yuan and Zhang, Zhongzhi},
  doi          = {10.1093/comjnl/bxy131},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {25-40},
  shortjournal = {Comput. J.},
  title        = {Effects of edge centrality on random walks on graphs},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Intermittent fault diagnosability of some general regular
networks. <em>COMJNL</em>, <em>63</em>(1), 16–24. (<a
href="https://doi.org/10.1093/comjnl/bxy128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault tolerance plays an important role in the interconnection networks, where permanent and intermittent faults are two kinds of fault situations. Permanent fault diagnosabilities of regular networks have been proposed widely while the intermittent fault diagnosabilities are also noteworthy. In this paper, we give a sufficient and necessary condition for ; -regular ; -connected graph ; to be ; -diagnosable without repair in intermittent fault pattern. Detailly, we show that the intermittent fault diagnosability of ; under the PMC model is ; , where ; is the maximum number of common neighbors for any two distinct vertices. As applications, intermittent fault diagnosabilities of many famous networks are explored.},
  archive      = {J_COMJNL},
  author       = {Sun, Xueli and Zhou, Shuming and Lv, Mengjie and Liu, Jiafei and Lian, Guanqin},
  doi          = {10.1093/comjnl/bxy128},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {16-24},
  shortjournal = {Comput. J.},
  title        = {Intermittent fault diagnosability of some general regular networks},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The two-good-neighbor connectivity and diagnosability of the
augmented three-ary n-cubes. <em>COMJNL</em>, <em>63</em>(1), 1–15. (<a
href="https://doi.org/10.1093/comjnl/bxy125">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Connectivity and diagnosability of a multiprocessor system are discussed by many researchers. As a favorable topology structue, the augmented three-ary ; -cubes ; has many good properties. In this paper, we prove that the two-good-neighbor connectivity of ; is ; for ; , and the two-good-neighbor diagnosability of ; is ; for ; under the PMC model and MM; model.},
  archive      = {J_COMJNL},
  author       = {Wang, Shiying and Zhao, Nan},
  doi          = {10.1093/comjnl/bxy125},
  journal      = {The Computer Journal},
  number       = {1},
  pages        = {1-15},
  shortjournal = {Comput. J.},
  title        = {The two-good-neighbor connectivity and diagnosability of the augmented three-ary n-cubes},
  volume       = {63},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
