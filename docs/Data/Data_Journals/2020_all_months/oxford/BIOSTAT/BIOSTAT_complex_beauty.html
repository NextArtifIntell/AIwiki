<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>BIOSTAT_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="biostat---70">BIOSTAT - 70</h2>
<ul>
<li><details>
<summary>
(2020). Estimating menarcheal age distribution from partially
recalled data. <em>BIOSTAT</em>, <em>21</em>(4), 876–894. (<a
href="https://doi.org/10.1093/biostatistics/kxz013">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In a cross-sectional study, adolescent and young adult females were asked to recall the time of menarche, if experienced. Some respondents recalled the date exactly, some recalled only the month or the year of the event, and some were unable to recall anything. We consider estimation of the menarcheal age distribution from this interval-censored data. A complicated interplay between age-at-event and calendar time, together with the evident fact of memory fading with time, makes the censoring informative. We propose a model where the probabilities of various types of recall would depend on the time since menarche. For parametric estimation, we model these probabilities using multinomial regression function. Establishing consistency and asymptotic normality of the parametric maximum likelihood estimator requires a bit of tweaking of the standard asymptotic theory, as the data format varies from case to case. We also provide a non-parametric maximum likelihood estimator, propose a computationally simpler approximation, and establish the consistency of both these estimators under mild conditions. We study the small sample performance of the parametric and non-parametric estimators through Monte Carlo simulations. Moreover, we provide a graphical check of the assumption of the multinomial model for the recall probabilities, which appears to hold for the menarcheal data set. Our analysis shows that the use of the partially recalled part of the data indeed leads to smaller confidence intervals of the survival function.},
  archive      = {J_BIOSTAT},
  author       = {Mirzaei Salehabadi, Sedigheh and Sengupta, Debasis and Ghosal, Rahul},
  doi          = {10.1093/biostatistics/kxz013},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {876-894},
  shortjournal = {Biostatistics},
  title        = {Estimating menarcheal age distribution from partially recalled data},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). G-estimation of structural nested mean models for competing
risks data using pseudo-observations. <em>BIOSTAT</em>, <em>21</em>(4),
860–875. (<a
href="https://doi.org/10.1093/biostatistics/kxz015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article provides methods of causal inference for competing risks data. The methods are formulated as structural nested mean models of causal effects directly related to the cumulative incidence function or subdistribution hazard, which reflect the survival experience of a subject in the presence of competing risks. The effect measures include causal risk differences, causal risk ratios, causal subdistribution hazard ratios, and causal effects of time-varying exposures. Inference is implemented by g-estimation using pseudo-observations, a technique to handle censoring. The finite-sample performance of the proposed estimators in simulated datasets and application to time-varying exposures in a cohort study of type 2 diabetes are also presented.},
  archive      = {J_BIOSTAT},
  author       = {Tanaka, Shiro and Brookhart, M Alan and Fine, Jason P},
  doi          = {10.1093/biostatistics/kxz015},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {860-875},
  shortjournal = {Biostatistics},
  title        = {G-estimation of structural nested mean models for competing risks data using pseudo-observations},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spatially varying age–period–cohort analysis with
application to US mortality, 2002–2016. <em>BIOSTAT</em>,
<em>21</em>(4), 845–859. (<a
href="https://doi.org/10.1093/biostatistics/kxz009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many public health databases index disease counts by age groups and calendar periods within geographic regions (e.g., states, districts, or counties). Issues around relative risk estimation in small areas are well-studied; however, estimating trend parameters that vary across geographic regions has received less attention. Additionally, small counts (e.g., |$\lt10$|⁠ ) in most publicly accessible databases are censored, further complicating age–period–cohort (APC) analysis in small areas. Here, we present a novel APC model with left-censoring and spatially varying intercept and trends, estimated with correlations among contiguous geographic regions. Like traditional models, our model captures population-scale trends, but it can also be used to characterize geographic disparities in relative risk and age-adjusted trends over time. To specify the joint distribution of our three spatially varying parameters, we adapt the generalized multivariate conditional autoregressive prior, previously used for multivariate disease mapping. Specified in this manner, region-specific parameters are correlated spatially, and also to one another. Estimation is performed using the No-U-Turn Hamiltonian Monte Carlo sampler in Stan. We conduct a simulation study to assess the performance of the proposed model relative to the standard model, and conclude with an application to US state-level opioid overdose mortality in men and women aged 15–64 years.},
  archive      = {J_BIOSTAT},
  author       = {Chernyavskiy, Pavel and Little, Mark P and Rosenberg, Philip S},
  doi          = {10.1093/biostatistics/kxz009},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {845-859},
  shortjournal = {Biostatistics},
  title        = {Spatially varying age–period–cohort analysis with application to US mortality, 2002–2016},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Parallel repulsive logic regression with biological
adjacency. <em>BIOSTAT</em>, <em>21</em>(4), 825–844. (<a
href="https://doi.org/10.1093/biostatistics/kxz011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Logic regression, an extension of generalized linear models with Boolean combinations of binary variables as predictors, is a useful tool in exploring interactions among single-nucleotide polymorphisms (SNPs) in genome-wide association studies. However, since the search space defined by all possible combinations of SNPs, their complements, and logical operators in Boolean expressions can be exceedingly large in such studies, objective function optimization is slow and likely to be trapped in many local solutions, resulting in model over-fitting. We introduce a new search algorithm, parallel repulsive logic regression (PRLR), to efficiently estimate parameters of a logic regression to find a best model within the large space of SNP interactions by incorporating: (i) relevant biological adjacency matrix between SNPs to define similarity of estimation paths or trees, which are derived from physical SNP positions on chromosomes and/or memberships in biological gene pathways; and (ii) two repulsive forces to counter the similarity between and within estimation paths considered in parallel, which are introduced as penalty terms in the objective function. We compare our method’s performance for identifying biologically-meaningful SNP interactions through simulations and with real genetic-epidemiological data. PRLR’s detection-accuracy measures outperform existing approaches, especially in terms of positive predictive value and sensitivity for detecting SNP–SNP interactions.},
  archive      = {J_BIOSTAT},
  author       = {Yoneoka, Daisuke and Im, Cindy and Yasui, Yutaka},
  doi          = {10.1093/biostatistics/kxz011},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {825-844},
  shortjournal = {Biostatistics},
  title        = {Parallel repulsive logic regression with biological adjacency},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Time-to-event model-assisted designs for dose-finding trials
with delayed toxicity. <em>BIOSTAT</em>, <em>21</em>(4), 807–824. (<a
href="https://doi.org/10.1093/biostatistics/kxz007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two useful strategies to speed up drug development are to increase the patient accrual rate and use novel adaptive designs. Unfortunately, these two strategies often conflict when the evaluation of the outcome cannot keep pace with the patient accrual rate and thus the interim data cannot be observed in time to make adaptive decisions. A similar logistic difficulty arises when the outcome is late-onset. Based on a novel formulation and approximation of the likelihood of the observed data, we propose a general methodology for model-assisted designs to handle toxicity data that are pending due to fast accrual or late-onset toxicity and facilitate seamless decision making in phase I dose-finding trials. The proposed time-to-event model-assisted designs consider each dose separately and the dose-escalation/de-escalation rules can be tabulated before the trial begins, which greatly simplifies trial conduct in practice compared to that under existing methods. We show that the proposed designs have desirable finite and large-sample properties and yield performance that is comparable to that of more complicated model-based designs. We provide user-friendly software for implementing the designs.},
  archive      = {J_BIOSTAT},
  author       = {Lin, Ruitao and Yuan, Ying},
  doi          = {10.1093/biostatistics/kxz007},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {807-824},
  shortjournal = {Biostatistics},
  title        = {Time-to-event model-assisted designs for dose-finding trials with delayed toxicity},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Critical window variable selection: Estimating the impact of
air pollution on very preterm birth. <em>BIOSTAT</em>, <em>21</em>(4),
790–806. (<a
href="https://doi.org/10.1093/biostatistics/kxz006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding the impact that environmental exposure during different stages of pregnancy has on the risk of adverse birth outcomes is vital for protection of the fetus and for the development of mechanistic explanations of exposure–disease relationships. As a result, statistical models to estimate critical windows of susceptibility have been developed for several different reproductive outcomes and pollutants. However, these current methods fail to adequately address the primary objective of this line of research; how to statistically identify a critical window of susceptibility. In this article, we introduce critical window variable selection (CWVS), a hierarchical Bayesian framework that directly addresses this question while simultaneously providing improved estimation of the risk parameters. Through simulation, we show that CWVS outperforms existing competing techniques in the setting of highly temporally correlated exposures in terms of (i) correctly identifying critical windows and (ii) accurately estimating risk parameters. We apply all competing methods to a case/control analysis of pregnant women in North Carolina, 2005–2008, with respect to the development of very preterm birth and exposure to ambient ozone and particulate matter |$&lt;$| 2.5 |$\mu$| m in aerodynamic diameter, and identify/estimate the critical windows of susceptibility. The newly developed method is implemented in the R package CWVS .},
  archive      = {J_BIOSTAT},
  author       = {Warren, Joshua L and Kong, Wenjing and Luben, Thomas J and Chang, Howard H},
  doi          = {10.1093/biostatistics/kxz006},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {790-806},
  shortjournal = {Biostatistics},
  title        = {Critical window variable selection: Estimating the impact of air pollution on very preterm birth},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Informatively empty clusters with application to
multigenerational studies. <em>BIOSTAT</em>, <em>21</em>(4), 775–789.
(<a href="https://doi.org/10.1093/biostatistics/kxz005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Exposures with multigenerational effects have profound implications for public health, affecting increasingly more people as the exposed population reproduces. Multigenerational studies, however, are susceptible to informative cluster size, occurring when the number of children to a mother (the cluster size) is related to their outcomes, given covariates. A natural question then arises: what if some women bear no children at all? The impact of these potentially informative empty clusters is currently unknown. This article first evaluates the performance of standard methods for informative cluster size when cluster size is permitted to be zero. We find that if the informative cluster size mechanism induces empty clusters, standard methods lead to biased estimates of target parameters. Joint models of outcome and size are capable of valid conditional inference as long as empty clusters are explicitly included in the analysis, but in practice empty clusters regularly go unacknowledged. In contrast, estimating equation approaches necessarily omit empty clusters and therefore yield biased estimates of marginal effects. To resolve this, we propose a joint marginalized approach that readily incorporates empty clusters and even in their absence permits more intuitive interpretations of population-averaged effects than do current methods. Competing methods are compared via simulation and in a study of the impact of in-utero exposure to diethylstilbestrol on the risk of attention-deficit/hyperactivity disorder (ADHD) among 106 198 children to 47 540 nurses from the Nurses Health Study.},
  archive      = {J_BIOSTAT},
  author       = {McGee, Glen and Weisskopf, Marc G and Kioumourtzoglou, Marianthi-Anna and Coull, Brent A and Haneuse, Sebastien},
  doi          = {10.1093/biostatistics/kxz005},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {775-789},
  shortjournal = {Biostatistics},
  title        = {Informatively empty clusters with application to multigenerational studies},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ConvSCCS: Convolutional self-controlled case series model
for lagged adverse event detection. <em>BIOSTAT</em>, <em>21</em>(4),
758–774. (<a
href="https://doi.org/10.1093/biostatistics/kxz003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the increased availability of large electronic health records databases comes the chance of enhancing health risks screening. Most post-marketing detection of adverse drug reaction (ADR) relies on physicians’ spontaneous reports, leading to under-reporting. To take up this challenge, we develop a scalable model to estimate the effect of multiple longitudinal features (drug exposures) on a rare longitudinal outcome. Our procedure is based on a conditional Poisson regression model also known as self-controlled case series (SCCS). To overcome the need of precise risk periods specification, we model the intensity of outcomes using a convolution between exposures and step functions, which are penalized using a combination of group-Lasso and total-variation. Up to our knowledge, this is the first SCCS model with flexible intensity able to handle multiple longitudinal features in a single model. We show that this approach improves the state-of-the-art in terms of mean absolute error and computation time for the estimation of relative risks on simulated data. We apply this method on an ADR detection problem, using a cohort of diabetic patients extracted from the large French national health insurance database (SNIIRAM), a claims database containing medical reimbursements of more than 53 million people. This work has been done in the context of a research partnership between Ecole Polytechnique and CNAMTS (in charge of SNIIRAM).},
  archive      = {J_BIOSTAT},
  author       = {Morel, Maryan and Bacry, Emmanuel and Gaïffas, Stéphane and Guilloux, Agathe and Leroy, Fanny},
  doi          = {10.1093/biostatistics/kxz003},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {758-774},
  shortjournal = {Biostatistics},
  title        = {ConvSCCS: Convolutional self-controlled case series model for lagged adverse event detection},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hidden markov model addressing measurement errors in the
response and replicated covariates for continuous nondecreasing
processes. <em>BIOSTAT</em>, <em>21</em>(4), 743–757. (<a
href="https://doi.org/10.1093/biostatistics/kxz004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by a study tracking the progression of Parkinson’s disease (PD) based on features extracted from voice recordings, an inhomogeneous hidden Markov model with continuous state-space is proposed. The approach addresses the measurement error in the response, the within-subject variability of the replicated covariates and presumed nondecreasing response. A Bayesian framework is described and an efficient Markov chain Monte Carlo method is developed. The model performance is evaluated through a simulation-based example and the analysis of a PD tracking progression dataset is presented. Although the approach was motivated by a PD tracking progression problem, it can be applied to any monotonic nondecreasing process whose continuous response variable is subject to measurement errors and where replicated covariates play a key role.},
  archive      = {J_BIOSTAT},
  author       = {Naranjo, Lizbeth and Pérez, Carlos J and Fuentes-García, Ruth and Martín, Jacinto},
  doi          = {10.1093/biostatistics/kxz004},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {743-757},
  shortjournal = {Biostatistics},
  title        = {A hidden markov model addressing measurement errors in the response and replicated covariates for continuous nondecreasing processes},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A nonparametric test for the association between
longitudinal covariates and censored survival data. <em>BIOSTAT</em>,
<em>21</em>(4), 727–742. (<a
href="https://doi.org/10.1093/biostatistics/kxz002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many biomedical studies focus on the association between a longitudinal measurement and a time-to-event outcome while quantifying this association by means of a longitudinal-survival joint model. In this article we propose using the |$LLR$| test — a longitudinal extension of the log-rank test statistic given by Peto and Peto (1972) — to provide evidence of a plausible association between a time-to-event outcome (right- or interval-censored) and a time-dependent covariate. As joint model methods are complex and hard to interpret, it is wise to conduct a preliminary test such as |$LLR$| for checking the association between both processes. The |$LLR$| statistic can be expressed in the form of a weighted difference of hazards, yielding a broad class of weighted log-rank test statistics known as |$LWLR$|⁠ , which allow a specific emphasis along the time axis of the effects of the time-dependent covariate on the survival. The asymptotic distribution of |$LLR$| is derived by means of a permutation approach under the assumption that the censoring mechanism is independent of the survival time and the longitudinal covariate. A simulation study is conducted to evaluate the performance of the test statistics |$LLR$| and |$LWLR$|⁠ , showing that the empirical size is close to the nominal significance level and that the power of the test depends on the association between the covariates and the survival time. A data set together with a toy example are used to illustrate the |$LLR$| test. The data set explores the study Epidemiology of Diabetes Interventions and Complications ( Sparling and others , 2006 ) which includes interval-censored data. A software implementation of our method is available on github ( https://github.com/RamonOller/LWLRtest ).},
  archive      = {J_BIOSTAT},
  author       = {Oller, Ramon and Gómez Melis, Guadalupe},
  doi          = {10.1093/biostatistics/kxz002},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {727-742},
  shortjournal = {Biostatistics},
  title        = {A nonparametric test for the association between longitudinal covariates and censored survival data},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast nonconvex deconvolution of calcium imaging data.
<em>BIOSTAT</em>, <em>21</em>(4), 709–726. (<a
href="https://doi.org/10.1093/biostatistics/kxy083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Calcium imaging data promises to transform the field of neuroscience by making it possible to record from large populations of neurons simultaneously. However, determining the exact moment in time at which a neuron spikes, from a calcium imaging data set, amounts to a non-trivial deconvolution problem which is of critical importance for downstream analyses. While a number of formulations have been proposed for this task in the recent literature, in this article, we focus on a formulation recently proposed in Jewell and Witten (2018. Exact spike train inference via |$\ell_{0} $| optimization. The Annals of Applied Statistics 12 (4), 2457–2482) that can accurately estimate not just the spike rate, but also the specific times at which the neuron spikes. We develop a much faster algorithm that can be used to deconvolve a fluorescence trace of 100 000 timesteps in less than a second. Furthermore, we present a modification to this algorithm that precludes the possibility of a “negative spike”. We demonstrate the performance of this algorithm for spike deconvolution on calcium imaging datasets that were recently released as part of the |$\texttt{spikefinder}$| challenge ( http://spikefinder.codeneuro.org/ ). The algorithm presented in this article was used in the Allen Institute for Brain Science’s “platform paper” to decode neural activity from the Allen Brain Observatory; this is the main scientific paper in which their data resource is presented. Our |$\texttt{C++}$| implementation, along with |$\texttt{R}$| and |$\texttt{python}$| wrappers, is publicly available. |$\texttt{R}$| code is available on |$\texttt{CRAN}$| and |$\texttt{Github}$|⁠ , and |$\texttt{python}$| wrappers are available on |$\texttt{Github}$|⁠ ; see https://github.com/jewellsean/FastLZeroSpikeInference .},
  archive      = {J_BIOSTAT},
  author       = {Jewell, Sean W and Hocking, Toby Dylan and Fearnhead, Paul and Witten, Daniela M},
  doi          = {10.1093/biostatistics/kxy083},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {709-726},
  shortjournal = {Biostatistics},
  title        = {Fast nonconvex deconvolution of calcium imaging data},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Are clusterings of multiple data views independent?
<em>BIOSTAT</em>, <em>21</em>(4), 692–708. (<a
href="https://doi.org/10.1093/biostatistics/kxz001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the Pioneer 100 (P100) Wellness Project, multiple types of data are collected on a single set of healthy participants at multiple timepoints in order to characterize and optimize wellness. One way to do this is to identify clusters, or subgroups, among the participants, and then to tailor personalized health recommendations to each subgroup. It is tempting to cluster the participants using all of the data types and timepoints, in order to fully exploit the available information. However, clustering the participants based on multiple data views implicitly assumes that a single underlying clustering of the participants is shared across all data views. If this assumption does not hold, then clustering the participants using multiple data views may lead to spurious results. In this article, we seek to evaluate the assumption that there is some underlying relationship among the clusterings from the different data views, by asking the question: are the clusters within each data view dependent or independent ? We develop a new test for answering this question, which we then apply to clinical, proteomic, and metabolomic data, across two distinct timepoints, from the P100 study. We find that while the subgroups of the participants defined with respect to any single data type seem to be dependent across time, the clustering among the participants based on one data type (e.g. proteomic data) appears not to be associated with the clustering based on another data type (e.g. clinical data).},
  archive      = {J_BIOSTAT},
  author       = {Gao, Lucy L and Bien, Jacob and Witten, Daniela},
  doi          = {10.1093/biostatistics/kxz001},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {692-708},
  shortjournal = {Biostatistics},
  title        = {Are clusterings of multiple data views independent?},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A two-stage estimation procedure for non-linear structural
equation models. <em>BIOSTAT</em>, <em>21</em>(4), 676–691. (<a
href="https://doi.org/10.1093/biostatistics/kxy082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Applications of structural equation models (SEMs) are often restricted to linear associations between variables. Maximum likelihood (ML) estimation in non-linear models may be complex and require numerical integration. Furthermore, ML inference is sensitive to distributional assumptions. In this article, we introduce a simple two-stage estimation technique for estimation of non-linear associations between latent variables. Here both steps are based on fitting linear SEMs: first a linear model is fitted to data on the latent predictor and terms describing the non-linear effect are predicted by their conditional means. In the second step, the predictions are included in a linear model for the latent outcome variable. We show that this procedure is consistent and identifies its asymptotic distribution. We also illustrate how this framework easily allows the association between latent variables to be modeled using restricted cubic splines, and we develop a modified estimator which is robust to non-normality of the latent predictor. In a simulation study, we compare the proposed method to MLE and alternative two-stage estimation techniques.},
  archive      = {J_BIOSTAT},
  author       = {Holst, Klaus Kähler and Budtz-Jørgensen, Esben},
  doi          = {10.1093/biostatistics/kxy082},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {676-691},
  shortjournal = {Biostatistics},
  title        = {A two-stage estimation procedure for non-linear structural equation models},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation of high-dimensional directed acyclic graphs with
surrogate intervention. <em>BIOSTAT</em>, <em>21</em>(4), 659–675. (<a
href="https://doi.org/10.1093/biostatistics/kxy080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Directed acyclic graphs (DAGs) have been used to describe causal relationships between variables. The standard method for determining such relations uses interventional data. For complex systems with high-dimensional data, however, such interventional data are often not available. Therefore, it is desirable to estimate causal structure from observational data without subjecting variables to interventions. Observational data can be used to estimate the skeleton of a DAG and the directions of a limited number of edges. We develop a Bayesian framework to estimate a DAG using surrogate interventional data, where the interventions are applied to a set of external variables, and thus such interventions are considered to be surrogate interventions on the variables of interest. Our work is motivated by expression quantitative trait locus (eQTL) studies, where the variables of interest are the expression of genes, the external variables are DNA variations, and interventions are applied to DNA variants during the process of a randomly selected DNA allele being passed to a child from either parent. Our method, surrogate intervention recovery of a DAG ( ⁠|$\texttt{sirDAG}$|⁠ ), first constructs a DAG skeleton using penalized regressions and the subsequent partial correlation tests, and then estimates the posterior probabilities of all the edge directions after incorporating DNA variant data. We demonstrate the utilities of |$\texttt{sirDAG}$| by simulation and an application to an eQTL study for 550 breast cancer patients.},
  archive      = {J_BIOSTAT},
  author       = {Ha, Min Jin and Sun, Wei},
  doi          = {10.1093/biostatistics/kxy080},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {659-675},
  shortjournal = {Biostatistics},
  title        = {Estimation of high-dimensional directed acyclic graphs with surrogate intervention},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating c-level partial correlation graphs with
application to brain imaging. <em>BIOSTAT</em>, <em>21</em>(4), 641–658.
(<a href="https://doi.org/10.1093/biostatistics/kxy076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Alzheimer’s disease (AD) is a chronic neurodegenerative disease that changes the functional connectivity of the brain. The alteration of the strong connections between different brain regions is of particular interest to researchers. In this article, we use partial correlations to model the brain connectivity network and propose a data-driven procedure to recover a |$c$| -level partial correlation graph based on PET data, which is the graph of the absolute partial correlations larger than a pre-specified constant |$c$|⁠ . The proposed procedure is adaptive to the “large p, small n” scenario commonly seen in whole brain studies, and it incorporates the variation of the estimated partial correlations, which results in higher power compared to the existing methods. A case study on the FDG-PET images from AD and normal control (NC) subjects discovers new brain regions, Sup Frontal and Mid Frontal in the frontal lobe, which have different brain functional connectivity between AD and NC.},
  archive      = {J_BIOSTAT},
  author       = {Qiu, Yumou and Zhou, Xiao-Hua},
  doi          = {10.1093/biostatistics/kxy076},
  journal      = {Biostatistics},
  month        = {10},
  number       = {4},
  pages        = {641-658},
  shortjournal = {Biostatistics},
  title        = {Estimating c-level partial correlation graphs with application to brain imaging},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). MACPET: Model-based analysis for ChIA-PET. <em>BIOSTAT</em>,
<em>21</em>(3), 625–639. (<a
href="https://doi.org/10.1093/biostatistics/kxy084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present model-based analysis for ChIA-PET (MACPET), which analyzes paired-end read sequences provided by ChIA-PET for finding binding sites of a protein of interest. MACPET uses information from both tags of each PET and searches for binding sites in a two-dimensional space, while taking into account different noise levels in different genomic regions. MACPET shows favorable results compared with MACS in terms of motif occurrence and spatial resolution. Furthermore, significant binding sites discovered by MACPET are involved in a higher number of significant three-dimensional interactions than those discovered by MACS. MACPET is freely available on Bioconductor. ChIA-PET; MACPET; Model-based clustering; Paired-end tags; Peak-calling algorithm.},
  archive      = {J_BIOSTAT},
  author       = {Vardaxis, Ioannis and Drabløs, Finn and Rye, Morten B and Lindqvist, Bo Henry},
  doi          = {10.1093/biostatistics/kxy084},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {625-639},
  shortjournal = {Biostatistics},
  title        = {MACPET: Model-based analysis for ChIA-PET},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian generalized biclustering analysis via adaptive
structured shrinkage. <em>BIOSTAT</em>, <em>21</em>(3), 610–624. (<a
href="https://doi.org/10.1093/biostatistics/kxy081">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Biclustering techniques can identify local patterns of a data matrix by clustering feature space and sample space at the same time. Various biclustering methods have been proposed and successfully applied to analysis of gene expression data. While existing biclustering methods have many desirable features, most of them are developed for continuous data and few of them can efficiently handle -omics data of various types, for example, binomial data as in single nucleotide polymorphism data or negative binomial data as in RNA-seq data. In addition, none of existing methods can utilize biological information such as those from functional genomics or proteomics. Recent work has shown that incorporating biological information can improve variable selection and prediction performance in analyses such as linear regression and multivariate analysis. In this article, we propose a novel Bayesian biclustering method that can handle multiple data types including Gaussian, Binomial, and Negative Binomial. In addition, our method uses a Bayesian adaptive structured shrinkage prior that enables feature selection guided by existing biological information. Our simulation studies and application to multi-omics datasets demonstrate robust and superior performance of the proposed method, compared to other existing biclustering methods.},
  archive      = {J_BIOSTAT},
  author       = {Li, Ziyi and Chang, Changgee and Kundu, Suprateek and Long, Qi},
  doi          = {10.1093/biostatistics/kxy081},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {610-624},
  shortjournal = {Biostatistics},
  title        = {Bayesian generalized biclustering analysis via adaptive structured shrinkage},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Assessing the incremental value of new biomarkers based on
OR rules. <em>BIOSTAT</em>, <em>21</em>(3), 594–609. (<a
href="https://doi.org/10.1093/biostatistics/kxy070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In early detection of disease, a single biomarker often has inadequate classification performance, making it important to identify new biomarkers to combine with the existing marker for improved performance. A biologically natural method for combining biomarkers is to use logic rules, e.g., the OR/AND rules. In our motivating example of early detection of pancreatic cancer, the established biomarker CA19-9 is only present in a subclass of cancers; it is of interest to identify new biomarkers present in the other subclasses and declare disease when either marker is positive. While there has been research on developing biomarker combinations using the OR/AND rules, inference regarding the incremental value of the new marker within this framework is lacking and challenging due to statistical non-regularity. In this article, we aim to answer the inferential question of whether combining the new biomarker achieves better classification performance than using the existing biomarker alone, based on a nonparametrically estimated OR rule that maximizes the weighted average of sensitivity and specificity. We propose and compare various procedures for testing the incremental value of the new biomarker and constructing its confidence interval, using bootstrap, cross-validation, and a novel fuzzy p -value-based technique. We compare the performance of different methods via extensive simulation studies and apply them to the pancreatic cancer example.},
  archive      = {J_BIOSTAT},
  author       = {Wang, Lu and Luedtke, Alexander R and Huang, Ying},
  doi          = {10.1093/biostatistics/kxy070},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {594-609},
  shortjournal = {Biostatistics},
  title        = {Assessing the incremental value of new biomarkers based on OR rules},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysis in case–control sequencing association studies with
different sequencing depths. <em>BIOSTAT</em>, <em>21</em>(3), 577–593.
(<a href="https://doi.org/10.1093/biostatistics/kxy073">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of next-generation sequencing, investigators have access to higher quality sequencing data. However, to sequence all samples in a study using next generation sequencing can still be prohibitively expensive. One potential remedy could be to combine next generation sequencing data from cases with publicly available sequencing data for controls, but there could be a systematic difference in quality of sequenced data, such as sequencing depths, between sequenced study cases and publicly available controls. We propose a regression calibration (RC)-based method and a maximum-likelihood method for conducting an association study with such a combined sample by accounting for differential sequencing errors between cases and controls. The methods allow for adjusting for covariates, such as population stratification as confounders. Both methods control type I error and have comparable power to analysis conducted using the true genotype with sufficiently high but different sequencing depths. We show that the RC method allows for analysis using naive variance estimate (closely approximates true variance in practice) and standard software under certain circumstances. We evaluate the performance of the proposed methods using simulation studies and apply our methods to a combined data set of exome sequenced acute lung injury cases and healthy controls from the 1000 Genomes project.},
  archive      = {J_BIOSTAT},
  author       = {Chen, Sixing and Lin, Xihong},
  doi          = {10.1093/biostatistics/kxy073},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {577-593},
  shortjournal = {Biostatistics},
  title        = {Analysis in case–control sequencing association studies with different sequencing depths},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian inference of networks across multiple sample groups
and data types. <em>BIOSTAT</em>, <em>21</em>(3), 561–576. (<a
href="https://doi.org/10.1093/biostatistics/kxy078">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we develop a graphical modeling framework for the inference of networks across multiple sample groups and data types. In medical studies, this setting arises whenever a set of subjects, which may be heterogeneous due to differing disease stage or subtype, is profiled across multiple platforms, such as metabolomics, proteomics, or transcriptomics data. Our proposed Bayesian hierarchical model first links the network structures within each platform using a Markov random field prior to relate edge selection across sample groups, and then links the network similarity parameters across platforms. This enables joint estimation in a flexible manner, as we make no assumptions on the directionality of influence across the data types or the extent of network similarity across the sample groups and platforms. In addition, our model formulation allows the number of variables and number of subjects to differ across the data types, and only requires that we have data for the same set of groups. We illustrate the proposed approach through both simulation studies and an application to gene expression levels and metabolite abundances on subjects with varying severity levels of chronic obstructive pulmonary disease. Bayesian inference; Chronic obstructive pulmonary disease (COPD); Data integration; Gaussian graphical model; Markov random field prior; Spike and slab prior.},
  archive      = {J_BIOSTAT},
  author       = {Shaddox, Elin and Peterson, Christine B and Stingo, Francesco C and Hanania, Nicola A and Cruickshank-Quinn, Charmion and Kechris, Katerina and Bowler, Russell and Vannucci, Marina},
  doi          = {10.1093/biostatistics/kxy078},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {561-576},
  shortjournal = {Biostatistics},
  title        = {Bayesian inference of networks across multiple sample groups and data types},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inference on treatment effect modification by biomarker
response in a three-phase sampling design. <em>BIOSTAT</em>,
<em>21</em>(3), 545–560. (<a
href="https://doi.org/10.1093/biostatistics/kxy074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An objective in randomized clinical trials is the evaluation of “principal surrogates,” which consists of analyzing how the treatment effect on a clinical endpoint varies over principal strata subgroups defined by an intermediate response outcome under both or one of the treatment assignments. The latter effect modification estimand has been termed the marginal causal effect predictiveness (mCEP) curve. This objective was addressed in two randomized placebo-controlled Phase 3 dengue vaccine trials for an antibody response biomarker whose sampling design rendered previously developed inferential methods highly inefficient due to a three-phase sampling design. In this design, the biomarker was measured in a case-cohort sample and a key baseline auxiliary strongly associated with the biomarker (the “baseline surrogate measure”) was only measured in a further sub-sample. We propose a novel approach to estimation of the mCEP curve in such three-phase sampling designs that avoids the restrictive “placebo structural risk” modeling assumption common to past methods and that further improves robustness by the use of non-parametric kernel smoothing for biomarker density estimation. Additionally, we develop bootstrap-based procedures for pointwise and simultaneous confidence intervals and testing of four relevant hypotheses about the mCEP curve. We investigate the finite-sample properties of the proposed methods and compare them to those of an alternative method making the placebo structural risk assumption. Finally, we apply the novel and alternative procedures to the two dengue vaccine trial data sets.},
  archive      = {J_BIOSTAT},
  author       = {Juraska, Michal and Huang, Ying and Gilbert, Peter B},
  doi          = {10.1093/biostatistics/kxy074},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {545-560},
  shortjournal = {Biostatistics},
  title        = {Inference on treatment effect modification by biomarker response in a three-phase sampling design},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Non-parametric frailty cox models for hierarchical
time-to-event data. <em>BIOSTAT</em>, <em>21</em>(3), 531–544. (<a
href="https://doi.org/10.1093/biostatistics/kxy071">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a novel model for hierarchical time-to-event data, for example, healthcare data in which patients are grouped by their healthcare provider. The most common model for this kind of data is the Cox proportional hazard model, with frailties that are common to patients in the same group and given a parametric distribution. We relax the parametric frailty assumption in this class of models by using a non-parametric discrete distribution. This improves the flexibility of the model by allowing very general frailty distributions and enables the data to be clustered into groups of healthcare providers with a similar frailty. A tailored Expectation–Maximization algorithm is proposed for estimating the model parameters, methods of model selection are compared, and the code is assessed in simulation studies. This model is particularly useful for administrative data in which there are a limited number of covariates available to explain the heterogeneity associated with the risk of the event. We apply the model to a clinical administrative database recording times to hospital readmission, and related covariates, for patients previously admitted once to hospital for heart failure, and we explore latent clustering structures among healthcare providers.},
  archive      = {J_BIOSTAT},
  author       = {Gasperoni, Francesca and Ieva, Francesca and Paganoni, Anna Maria and Jackson, Christopher H and Sharples, Linda},
  doi          = {10.1093/biostatistics/kxy071},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {531-544},
  shortjournal = {Biostatistics},
  title        = {Non-parametric frailty cox models for hierarchical time-to-event data},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sequence kernel association test for survival outcomes in
the presence of a non-susceptible fraction. <em>BIOSTAT</em>,
<em>21</em>(3), 518–530. (<a
href="https://doi.org/10.1093/biostatistics/kxy075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we propose a single nucleotide polymorphism set association test for survival phenotypes in the presence of a non-susceptible fraction. We consider a mixture model with a logistic regression for the susceptibility indicator and a proportional hazards regression to model survival in the susceptible group. We propose a joint test to assess the significance of the genetic variant in both logistic and survival regressions simultaneously. We adopt the spirit of SKAT and conduct a variance-component test treating the genetic effects of multiple variants as random. We derive score-type test statistics, and we investigate several approaches to compute their |$p$| -values. The finite-sample properties of the proposed tests are assessed and compared to existing approaches by simulations and their use is illustrated through an application to ovarian cancer data from the Consortium of Investigators of Modifiers of BRCA1 and BRCA2.},
  archive      = {J_BIOSTAT},
  author       = {Lakhal-Chaieb, Lajmi and Simard, Jacques and Bull, Shelley},
  doi          = {10.1093/biostatistics/kxy075},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {518-530},
  shortjournal = {Biostatistics},
  title        = {Sequence kernel association test for survival outcomes in the presence of a non-susceptible fraction},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian variable selection for multivariate zero-inflated
models: Application to microbiome count data. <em>BIOSTAT</em>,
<em>21</em>(3), 499–517. (<a
href="https://doi.org/10.1093/biostatistics/kxy067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Microorganisms play critical roles in human health and disease. They live in diverse communities in which they interact synergistically or antagonistically. Thus for estimating microbial associations with clinical covariates, such as treatment effects, joint (multivariate) statistical models are preferred. Multivariate models allow one to estimate and exploit complex interdependencies among multiple taxa, yielding more powerful tests of exposure or treatment effects than application of taxon-specific univariate analyses. Analysis of microbial count data also requires special attention because data commonly exhibit zero inflation, i.e., more zeros than expected from a standard count distribution. To meet these needs, we developed a Bayesian variable selection model for multivariate count data with excess zeros that incorporates information on the covariance structure of the outcomes (counts for multiple taxa), while estimating associations with the mean levels of these outcomes. Though there has been much work on zero-inflated models for longitudinal data, little attention has been given to high-dimensional multivariate zero-inflated data modeled via a general correlation structure. Through simulation, we compared performance of the proposed method to that of existing univariate approaches, for both the binary (“excess zero”) and count parts of the model. When outcomes were correlated the proposed variable selection method maintained type I error while boosting the ability to identify true associations in the binary component of the model. For the count part of the model, in some scenarios the univariate method had higher power than the multivariate approach. This higher power was at a cost of a highly inflated false discovery rate not observed with the proposed multivariate method. We applied the approach to oral microbiome data from the Pediatric HIV/AIDS Cohort Oral Health Study and identified five (of 44) species associated with HIV infection.},
  archive      = {J_BIOSTAT},
  author       = {Lee, Kyu Ha and Coull, Brent A and Moscicki, Anna-Barbara and Paster, Bruce J and Starr, Jacqueline R},
  doi          = {10.1093/biostatistics/kxy067},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {499-517},
  shortjournal = {Biostatistics},
  title        = {Bayesian variable selection for multivariate zero-inflated models: Application to microbiome count data},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Diagnostic methods for uncovering outcome dependent visit
processes. <em>BIOSTAT</em>, <em>21</em>(3), 483–498. (<a
href="https://doi.org/10.1093/biostatistics/kxy068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advent of electronic health records, information collected in the course of regular health care is increasingly being used for clinical research. The hope is that the wealth of clinical data and the realistic setting (compared with information derived from highly controlled experiments like randomized trials) will aid in the investigation of determinants of disease and understanding of which treatments are effective in regular practice and for which patients. The availability of information in such databases is often driven by how a patient feels and may therefore be associated with the health outcomes being considered. We call this an outcome dependent visit process and recent work has shown that ignoring the outcome dependence can produce significant bias in the regression coefficients when fitting longitudinal data models. It is therefore important to have tools to recognize datasets exhibiting outcome dependence. We develop a score statistic to motivate the form of diagnostic test statistics, suggest a variety of approaches for diagnosing such situations, and evaluate their performance. Simple diagnostic tests achieve high power for diagnosing outcome dependent visit processes. This occurs when generalized estimating equations methods begin to be exhibit bias in estimating regression coefficients and before likelihood based methods are substantially biased.},
  archive      = {J_BIOSTAT},
  author       = {McCulloch, Charles E and Neuhaus, John M},
  doi          = {10.1093/biostatistics/kxy068},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {483-498},
  shortjournal = {Biostatistics},
  title        = {Diagnostic methods for uncovering outcome dependent visit processes},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian estimation of a semiparametric recurrent event
model with applications to the penetrance estimation of multiple primary
cancers in li-fraumeni syndrome. <em>BIOSTAT</em>, <em>21</em>(3),
467–482. (<a
href="https://doi.org/10.1093/biostatistics/kxy066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A common phenomenon in cancer syndromes is for an individual to have multiple primary cancers (MPC) at different sites during his/her lifetime. Patients with Li-Fraumeni syndrome (LFS), a rare pediatric cancer syndrome mainly caused by germline TP53 mutations, are known to have a higher probability of developing a second primary cancer than those with other cancer syndromes. In this context, it is desirable to model the development of MPC to enable better clinical management of LFS. Here, we propose a Bayesian recurrent event model based on a non-homogeneous Poisson process in order to obtain penetrance estimates for MPC related to LFS. We employed a familywise likelihood that facilitates using genetic information inherited through the family pedigree and properly adjusted for the ascertainment bias that was inevitable in studies of rare diseases by using an inverse probability weighting scheme. We applied the proposed method to data on LFS, using a family cohort collected through pediatric sarcoma patients at MD Anderson Cancer Center from 1944 to 1982. Both internal and external validation studies showed that the proposed model provides reliable penetrance estimates for MPC in LFS, which, to the best of our knowledge, have not been reported in the LFS literature.},
  archive      = {J_BIOSTAT},
  author       = {Shin, Seung Jun and Li, Jialu and Ning, Jing and Bojadzieva, Jasmina and Strong, Louise C and Wang, Wenyi},
  doi          = {10.1093/biostatistics/kxy066},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {467-482},
  shortjournal = {Biostatistics},
  title        = {Bayesian estimation of a semiparametric recurrent event model with applications to the penetrance estimation of multiple primary cancers in li-fraumeni syndrome},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Subdistribution hazard models for competing risks in
discrete time. <em>BIOSTAT</em>, <em>21</em>(3), 449–466. (<a
href="https://doi.org/10.1093/biostatistics/kxy069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A popular modeling approach for competing risks analysis in longitudinal studies is the proportional subdistribution hazards model by Fine and Gray (1999. A proportional hazards model for the subdistribution of a competing risk. Journal of the American Statistical Association 94 , 496–509). This model is widely used for the analysis of continuous event times in clinical and epidemiological studies. However, it does not apply when event times are measured on a discrete time scale, which is a likely scenario when events occur between pairs of consecutive points in time (e.g., between two follow-up visits of an epidemiological study) and when the exact lengths of the continuous time spans are not known. To adapt the Fine and Gray approach to this situation, we propose a technique for modeling subdistribution hazards in discrete time. Our method, which results in consistent and asymptotically normal estimators of the model parameters, is based on a weighted ML estimation scheme for binary regression. We illustrate the modeling approach by an analysis of nosocomial pneumonia in patients treated in hospitals.},
  archive      = {J_BIOSTAT},
  author       = {Berger, Moritz and Schmid, Matthias and Welchowski, Thomas and Schmitz-Valckenberg, Steffen and Beyersmann, Jan},
  doi          = {10.1093/biostatistics/kxy069},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {449-466},
  shortjournal = {Biostatistics},
  title        = {Subdistribution hazard models for competing risks in discrete time},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Power analysis in a SMART design: Sample size estimation for
determining the best embedded dynamic treatment regime.
<em>BIOSTAT</em>, <em>21</em>(3), 432–448. (<a
href="https://doi.org/10.1093/biostatistics/kxy064">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential, multiple assignment, randomized trial (SMART) designs have become increasingly popular in the field of precision medicine by providing a means for comparing more than two sequences of treatments tailored to the individual patient, i.e., dynamic treatment regime (DTR). The construction of evidence-based DTRs promises a replacement to ad hoc one-size-fits-all decisions pervasive in patient care. However, there are substantial statistical challenges in sizing SMART designs due to the correlation structure between the DTRs embedded in the design (EDTR). Since a primary goal of SMARTs is the construction of an optimal EDTR, investigators are interested in sizing SMARTs based on the ability to screen out EDTRs inferior to the optimal EDTR by a given amount which cannot be done using existing methods. In this article, we fill this gap by developing a rigorous power analysis framework that leverages the multiple comparisons with the best methodology. Our method employs Monte Carlo simulation to compute the number of individuals to enroll in an arbitrary SMART. We evaluate our method through extensive simulation studies. We illustrate our method by retrospectively computing the power in the Extending Treatment Effectiveness of Naltrexone (EXTEND) trial. An R package implementing our methodology is available to download from the Comprehensive R Archive Network.},
  archive      = {J_BIOSTAT},
  author       = {Artman, William J and Nahum-Shani, Inbal and Wu, Tianshuang and Mckay, James R and Ertefaie, Ashkan},
  doi          = {10.1093/biostatistics/kxy064},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {432-448},
  shortjournal = {Biostatistics},
  title        = {Power analysis in a SMART design: Sample size estimation for determining the best embedded dynamic treatment regime},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Array testing for multiplex assays. <em>BIOSTAT</em>,
<em>21</em>(3), 417–431. (<a
href="https://doi.org/10.1093/biostatistics/kxy058">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Group testing involves pooling individual specimens (e.g., blood, urine, swabs, etc.) and testing the pools for the presence of disease. When the proportion of diseased individuals is small, group testing can greatly reduce the number of tests needed to screen a population. Statistical research in group testing has traditionally focused on applications for a single disease. However, blood service organizations and large-scale disease surveillance programs are increasingly moving towards the use of multiplex assays, which measure multiple disease biomarkers at once. Tebbs and others (2013, Two-stage hierarchical group testing for multiple infections with application to the Infertility Prevention Project. Biometrics 69 , 1064–1073) and Hou and others (2017, Hierarchical group testing for multiple infections. Biometrics 73 , 656–665) were the first to examine hierarchical group testing case identification procedures for multiple diseases. In this article, we propose new non-hierarchical procedures which utilize two-dimensional arrays. We derive closed-form expressions for the expected number of tests per individual and classification accuracy probabilities and show that array testing can be more efficient than hierarchical procedures when screening individuals for multiple diseases at once. We illustrate the potential of using array testing in the detection of chlamydia and gonorrhea for a statewide screening program in Iowa. Finally, we describe an R/Shiny application that will help practitioners identify the best multiple-disease case identification algorithm.},
  archive      = {J_BIOSTAT},
  author       = {Hou, Peijie and Tebbs, Joshua M and Wang, Dewei and McMahan, Christopher S and Bilder, Christopher R},
  doi          = {10.1093/biostatistics/kxy058},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {417-431},
  shortjournal = {Biostatistics},
  title        = {Array testing for multiplex assays},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model selection and parameter estimation for dynamic
epidemic models via iterated filtering: Application to rotavirus in
germany. <em>BIOSTAT</em>, <em>21</em>(3), 400–416. (<a
href="https://doi.org/10.1093/biostatistics/kxy057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the wide application of dynamic models in infectious disease epidemiology, the particular modeling of variability in the different model components is often subjective rather than the result of a thorough model selection process. This is in part because inference for a stochastic transmission model can be difficult since the likelihood is often intractable due to partial observability. In this work, we address the question of adequate inclusion of variability by demonstrating a systematic approach for model selection and parameter inference for dynamic epidemic models. For this, we perform inference for six partially observed Markov process models, which assume the same underlying transmission dynamics, but differ with respect to the amount of variability they allow for. The inference framework for the stochastic transmission models is provided by iterated filtering methods, which are readily implemented in the R package pomp by King and others (2016, Statistical inference for partially observed Markov processes via the R package pomp. Journal of Statistical Software 69 , 1–43). We illustrate our approach on German rotavirus surveillance data from 2001 to 2008, discuss practical difficulties of the methods used and calculate a model based estimate for the basic reproduction number |$R_0$| using these data.},
  archive      = {J_BIOSTAT},
  author       = {Stocks, Theresa and Britton, Tom and Höhle, Michael},
  doi          = {10.1093/biostatistics/kxy057},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {400-416},
  shortjournal = {Biostatistics},
  title        = {Model selection and parameter estimation for dynamic epidemic models via iterated filtering: Application to rotavirus in germany},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Combining planned and discovered comparisons in
observational studies. <em>BIOSTAT</em>, <em>21</em>(3), 384–399. (<a
href="https://doi.org/10.1093/biostatistics/kxy055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In observational studies of treatment effects, it is common to have several outcomes, perhaps of uncertain quality and relevance, each purporting to measure the effect of the treatment. A single planned combination of several outcomes may increase both power and insensitivity to unmeasured bias when the plan is wisely chosen, but it may miss opportunities in other cases. A method is proposed that uses one planned combination with only a mild correction for multiple testing and exhaustive consideration of all possible combinations fully correcting for multiple testing. The method works with the joint distribution of |$\kappa^{T}\left( \mathbf{T}-\boldsymbol{\mu}\right) /\sqrt {\boldsymbol{\kappa}^{T}\boldsymbol{\Sigma\boldsymbol{\kappa}}}$| and |$max_{\boldsymbol{\lambda}\neq\mathbf{0}}$| |$\,\lambda^{T}\left( \mathbf{T} -\boldsymbol{\mu}\right) /$| |$\sqrt{\boldsymbol{\lambda}^{T}\boldsymbol{\Sigma \lambda}}$| where |$\kappa$| is chosen a priori and the test statistic |$\mathbf{T}$| is asymptotically |$N_{L}\left( \boldsymbol{\mu},\boldsymbol{\Sigma}\right) $|⁠ . The correction for multiple testing has a smaller effect on the power of |$\kappa^{T}\left( \mathbf{T}-\boldsymbol{\mu }\right) /\sqrt{\boldsymbol{\kappa}^{T}\boldsymbol{\Sigma\boldsymbol{\kappa} }}$| than does switching to a two-tailed test, even though the opposite tail does receive consideration when |$\lambda=-\kappa$|⁠ . In the application, there are three measures of cognitive decline, and the a priori comparison |$\kappa$| is their first principal component, computed without reference to treatment assignments. The method is implemented in an R package sensitivitymult.},
  archive      = {J_BIOSTAT},
  author       = {Rosenbaum, Paul R},
  doi          = {10.1093/biostatistics/kxy055},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {384-399},
  shortjournal = {Biostatistics},
  title        = {Combining planned and discovered comparisons in observational studies},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bayesian mixture modeling approach for public health
surveillance. <em>BIOSTAT</em>, <em>21</em>(3), 369–383. (<a
href="https://doi.org/10.1093/biostatistics/kxy038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spatial monitoring of trends in health data plays an important part of public health surveillance. Most commonly, it is used to understand the etiology of a public health issue, to assess the impact of an intervention, or to provide detection of unusual behavior. In this article, we present a Bayesian mixture model for public health surveillance, which is able to provide estimates of the disease risk in space and time, and also to detect areas with unusual behavior. The model is designed to deal with a range of spatial and temporal patterns in the data, and with time series of different lengths. We carry out a simulation study to assess the performance of the model under different scenarios, and we compare it against a recently proposed Bayesian model for short time series. Finally, the proposed model is used for surveillance of road traffic accidents data in England over the years 2005–2015.},
  archive      = {J_BIOSTAT},
  author       = {Boulieri, Areti and Bennett, James E and Blangiardo, Marta},
  doi          = {10.1093/biostatistics/kxy038},
  journal      = {Biostatistics},
  month        = {7},
  number       = {3},
  pages        = {369-383},
  shortjournal = {Biostatistics},
  title        = {A bayesian mixture modeling approach for public health surveillance},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The upstrap. <em>BIOSTAT</em>, <em>21</em>(2), e164–e166.
(<a href="https://doi.org/10.1093/biostatistics/kxy054">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The bootstrap, introduced in Efron (1979 . Bootstrap methods: another look at the jackknife. The Annals of Statistics 7 , 1–26), is a landmark method for quantifying variability. It uses sampling with replacement with a sample size equal to that of the original data. We propose the upstrap, which samples with replacement either more or fewer samples than the original sample size. We illustrate the upstrap by solving a hard, but common, sample size calculation problem. The data and code used for the analysis in this article are available on GitHub (2018 . https://github.com/ccrainic/upstrap ).},
  archive      = {J_BIOSTAT},
  author       = {Crainiceanu, Ciprian M and Crainiceanu, Adina},
  doi          = {10.1093/biostatistics/kxy054},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {e164-e166},
  shortjournal = {Biostatistics},
  title        = {The upstrap},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A novel calibration framework for survival analysis when a
binary covariate is measured at sparse time points. <em>BIOSTAT</em>,
<em>21</em>(2), e148–e163. (<a
href="https://doi.org/10.1093/biostatistics/kxy063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goals in clinical and cohort studies often include evaluation of the association of a time-dependent binary treatment or exposure with a survival outcome. Recently, several impactful studies targeted the association between initiation of aspirin and survival following colorectal cancer (CRC) diagnosis. The value of this exposure is zero at baseline and may change its value to one at some time point. Estimating this association is complicated by having only intermittent measurements on aspirin-taking. Commonly used methods can lead to substantial bias. We present a class of calibration models for the distribution of the time of status change of the binary covariate. Estimates obtained from these models are then incorporated into the proportional hazard partial likelihood in a natural way. We develop non-parametric, semiparametric, and parametric calibration models, and derive asymptotic theory for the methods that we implement in the aspirin and CRC study. We further develop a risk-set calibration approach that is more useful in settings in which the association between the binary covariate and survival is strong.},
  archive      = {J_BIOSTAT},
  author       = {Nevo, Daniel and Hamada, Tsuyoshi and Ogino, Shuji and Wang, Molin},
  doi          = {10.1093/biostatistics/kxy063},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {e148-e163},
  shortjournal = {Biostatistics},
  title        = {A novel calibration framework for survival analysis when a binary covariate is measured at sparse time points},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sparse relative risk regression models. <em>BIOSTAT</em>,
<em>21</em>(2), e131–e147. (<a
href="https://doi.org/10.1093/biostatistics/kxy060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical studies where patients are routinely screened for many genomic features are becoming more routine. In principle, this holds the promise of being able to find genomic signatures for a particular disease. In particular, cancer survival is thought to be closely linked to the genomic constitution of the tumor. Discovering such signatures will be useful in the diagnosis of the patient, may be used for treatment decisions and, perhaps, even the development of new treatments. However, genomic data are typically noisy and high-dimensional, not rarely outstripping the number of patients included in the study. Regularized survival models have been proposed to deal with such scenarios. These methods typically induce sparsity by means of a coincidental match of the geometry of the convex likelihood and a (near) non-convex regularizer. The disadvantages of such methods are that they are typically non-invariant to scale changes of the covariates, they struggle with highly correlated covariates, and they have a practical problem of determining the amount of regularization. In this article, we propose an extension of the differential geometric least angle regression method for sparse inference in relative risk regression models. A software implementation of our method is available on github ( https://github.com/LuigiAugugliaro/dgcox ).},
  archive      = {J_BIOSTAT},
  author       = {Wit, Ernst C and Augugliaro, Luigi and Pazira, Hassan and González, Javier and Abegaz, Fentaw},
  doi          = {10.1093/biostatistics/kxy060},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {e131-e147},
  shortjournal = {Biostatistics},
  title        = {Sparse relative risk regression models},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Cumulative incidence regression for dynamic treatment
regimens. <em>BIOSTAT</em>, <em>21</em>(2), e113–e130. (<a
href="https://doi.org/10.1093/biostatistics/kxy062">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently dynamic treatment regimens (DTRs) have drawn considerable attention, as an effective tool for personalizing medicine. Sequential Multiple Assignment Randomized Trials (SMARTs) are often used to gather data for making inference on DTRs. In this article, we focus on regression analysis of DTRs from a two-stage SMART for competing risk outcomes based on cumulative incidence functions (CIFs). Even though there are extensive works on the regression problem for DTRs, no research has been done on modeling the CIF for SMART trials. We extend existing CIF regression models to handle covariate effects for DTRs. Asymptotic properties are established for our proposed estimators. The models can be implemented using existing software by an augmented-data approximation. We show the improvement provided by our proposed methods by simulation and illustrate its practical utility through an analysis of a SMART neuroblastoma study, where disease progression cannot be observed after death.},
  archive      = {J_BIOSTAT},
  author       = {Chen, Ling-Wan and Yavuz, Idil and Cheng, Yu and Wahed, Abdus S},
  doi          = {10.1093/biostatistics/kxy062},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {e113-e130},
  shortjournal = {Biostatistics},
  title        = {Cumulative incidence regression for dynamic treatment regimens},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inferring mobility measures from GPS traces with missing
data. <em>BIOSTAT</em>, <em>21</em>(2), e98–e112. (<a
href="https://doi.org/10.1093/biostatistics/kxy059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With increasing availability of smartphones with Global Positioning System (GPS) capabilities, large-scale studies relating individual-level mobility patterns to a wide variety of patient-centered outcomes, from mood disorders to surgical recovery, are becoming a reality. Similar past studies have been small in scale and have provided wearable GPS devices to subjects. These devices typically collect mobility traces continuously without significant gaps in the data, and consequently the problem of data missingness has been safely ignored. Leveraging subjects’ own smartphones makes it possible to scale up and extend the duration of these types of studies, but at the same time introduces a substantial challenge: to preserve a smartphone’s battery, GPS can be active only for a small portion of the time, frequently less than |$10\%$|⁠ , leading to a tremendous missing data problem. We introduce a principled statistical approach, based on weighted resampling of the observed data, to impute the missing mobility traces, which we then summarize using different mobility measures. We compare the strengths of our approach to linear interpolation (LI), a popular approach for dealing with missing data, both analytically and through simulation of missingness for empirical data. We conclude that our imputation approach better mirrors human mobility both theoretically and over a sample of GPS mobility traces from 182 individuals in the Geolife data set, where, relative to LI, imputation resulted in a 10-fold reduction in the error averaged across all mobility features.},
  archive      = {J_BIOSTAT},
  author       = {Barnett, Ian and Onnela, Jukka-Pekka},
  doi          = {10.1093/biostatistics/kxy059},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {e98-e112},
  shortjournal = {Biostatistics},
  title        = {Inferring mobility measures from GPS traces with missing data},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A graphical model for skewed matrix-variate non-randomly
missing data. <em>BIOSTAT</em>, <em>21</em>(2), e80–e97. (<a
href="https://doi.org/10.1093/biostatistics/kxy056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Epidemiological studies on periodontal disease (PD) collect relevant bio-markers, such as the clinical attachment level (CAL) and the probed pocket depth (PPD), at pre-specified tooth sites clustered within a subject’s mouth, along with various other demographic and biological risk factors. Routine cross-sectional evaluation are conducted under a linear mixed model (LMM) framework with underlying normality assumptions on the random terms. However, a careful investigation reveals considerable non-normality manifested in those random terms, in the form of skewness and tail behavior. In addition, PD progression is hypothesized to be spatially-referenced, i.e. disease status at proximal tooth-sites may be different from distally located sites, and tooth missingness is non-random (or informative), given that the number and location of missing teeth informs about the periodontal health in that region. To mitigate these complexities, we consider a matrix-variate skew- |$t$| formulation of the LMM with a Markov graphical embedding to handle the site-level spatial associations of the bivariate (PPD and CAL) responses. Within the same framework, the non-randomly missing responses are imputed via a latent probit regression of the missingness indicator over the responses. Our hierarchical Bayesian framework powered by relevant Markov chain Monte Carlo steps addresses the aforementioned complexities within an unified paradigm, and estimates model parameters with seamless sharing of information across various stages of the hierarchy. Using both synthetic and real clinical data assessing PD status, we demonstrate a significantly improved fit of our proposition over various other alternative models.},
  archive      = {J_BIOSTAT},
  author       = {Zhang, Lin and Bandyopadhyay, Dipankar},
  doi          = {10.1093/biostatistics/kxy056},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {e80-e97},
  shortjournal = {Biostatistics},
  title        = {A graphical model for skewed matrix-variate non-randomly missing data},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Gaining power in multiple testing of interval hypotheses via
conditionalization. <em>BIOSTAT</em>, <em>21</em>(2), e65–e79. (<a
href="https://doi.org/10.1093/biostatistics/kxy042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we introduce a novel procedure for improving power of multiple testing procedures (MTPs) of interval hypotheses. When testing interval hypotheses the null hypothesis |$P$| -values tend to be stochastically larger than standard uniform if the true parameter is in the interior of the null hypothesis. The new procedure starts with a set of |$P$| -values and discards those with values above a certain pre-selected threshold, while the rest are corrected (scaled-up) by the value of the threshold. Subsequently, a chosen family-wise error rate (FWER) or false discovery rate MTP is applied to the set of corrected |$P$| -values only. We prove the general validity of this procedure under independence of |$P$| -values, and for the special case of the Bonferroni method, we formulate several sufficient conditions for the control of the FWER. It is demonstrated that this “filtering” of |$P$| -values can yield considerable gains of power.},
  archive      = {J_BIOSTAT},
  author       = {Ellis, Jules L and Pecanka, Jakub and Goeman, Jelle J},
  doi          = {10.1093/biostatistics/kxy042},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {e65-e79},
  shortjournal = {Biostatistics},
  title        = {Gaining power in multiple testing of interval hypotheses via conditionalization},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Incorporating historical models with adaptive bayesian
updates. <em>BIOSTAT</em>, <em>21</em>(2), e47–e64. (<a
href="https://doi.org/10.1093/biostatistics/kxy053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article considers Bayesian approaches for incorporating information from a historical model into a current analysis when the historical model includes only a subset of covariates currently of interest. The statistical challenge is 2-fold. First, the parameters in the nested historical model are not generally equal to their counterparts in the larger current model, neither in value nor interpretation. Second, because the historical information will not be equally informative for all parameters in the current analysis, additional regularization may be required beyond that provided by the historical information. We propose several novel extensions of the so-called power prior that adaptively combine a prior based upon the historical information with a variance-reducing prior that shrinks parameter values toward zero. The ideas are directly motivated by our work building mortality risk prediction models for pediatric patients receiving extracorporeal membrane oxygenation (ECMO). We have developed a model on a registry-based cohort of ECMO patients and now seek to expand this model with additional biometric measurements, not available in the registry, collected on a small auxiliary cohort. Our adaptive priors are able to use the information in the original model and identify novel mortality risk factors. We support this with a simulation study, which demonstrates the potential for efficiency gains in estimation under a variety of scenarios.},
  archive      = {J_BIOSTAT},
  author       = {Boonstra, Philip S and Barbaro, Ryan P},
  doi          = {10.1093/biostatistics/kxy053},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {e47-e64},
  shortjournal = {Biostatistics},
  title        = {Incorporating historical models with adaptive bayesian updates},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Predictive cluster level surrogacy in the presence of
interference. <em>BIOSTAT</em>, <em>21</em>(2), e33–e46. (<a
href="https://doi.org/10.1093/biostatistics/kxy050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Surrogate evaluation is a difficult problem that is made more so by the presence of interference. Our proposed procedure can allow for relatively easy evaluation of surrogates for indirect or spill-over clinical effects at the cluster level. Our definition of surrogacy is based on the causal-association paradigm ( Joffe and Greene, 2009 . Related causal frameworks for surrogate outcomes. Biometrics 65 , 530–538), under which surrogates are evaluated by the strength of the association between a causal treatment effect on the clinical outcome and a causal treatment effect on the candidate surrogate. Hudgens and Halloran (2008 , Toward causal inference with interference. Journal of the American Statistical Association 103 , 832–842) introduced estimators that can be used for many of the marginal causal estimands of interest in the presence of interference. We extend these to consider surrogates for not just direct effects, but indirect and total effects at the cluster level. We suggest existing estimators that can be used to evaluate biomarkers under our proposed definition of surrogacy. In our motivating setting of a transmission blocking malaria vaccine, there is expected to be no direct protection to those vaccinated and predictive surrogates are urgently needed. We use a set of simulated data examples based on the proposed Phase IIb/III trial design of transmission blocking malaria vaccine to demonstrate how our definition, proposed criteria and procedure can be used to identify biomarkers as predictive cluster level surrogates in the presence of interference on the clinical outcome.},
  archive      = {J_BIOSTAT},
  author       = {Gabriel, Erin E and Follmann, Dean A},
  doi          = {10.1093/biostatistics/kxy050},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {e33-e46},
  shortjournal = {Biostatistics},
  title        = {Predictive cluster level surrogacy in the presence of interference},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pointless spatial modeling. <em>BIOSTAT</em>,
<em>21</em>(2), e17–e32. (<a
href="https://doi.org/10.1093/biostatistics/kxy041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The analysis of area-level aggregated summary data is common in many disciplines including epidemiology and the social sciences. Typically, Markov random field spatial models have been employed to acknowledge spatial dependence and allow data-driven smoothing. In the context of an irregular set of areas, these models always have an ad hoc element with respect to the definition of a neighborhood scheme. In this article, we exploit recent theoretical and computational advances to carry out modeling at the continuous spatial level, which induces a spatial model for the discrete areas. This approach also allows reconstruction of the continuous underlying surface, but the interpretation of such surfaces is delicate since it depends on the quality, extent and configuration of the observed data. We focus on models based on stochastic partial differential equations. We also consider the interesting case in which the aggregate data are supplemented with point data. We carry out Bayesian inference and, in the language of generalized linear mixed models, if the link is linear, an efficient implementation of the model is available via integrated nested Laplace approximations. For nonlinear links, we present two approaches: a fully Bayesian implementation using a Hamiltonian Monte Carlo algorithm and an empirical Bayes implementation, that is much faster and is based on Laplace approximations. We examine the properties of the approach using simulation, and then apply the model to the classic Scottish lip cancer data.},
  archive      = {J_BIOSTAT},
  author       = {Wilson, Katie and Wakefield, Jon},
  doi          = {10.1093/biostatistics/kxy041},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {e17-e32},
  shortjournal = {Biostatistics},
  title        = {Pointless spatial modeling},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ℓ1-penalized censored gaussian graphical model.
<em>BIOSTAT</em>, <em>21</em>(2), e1–e16. (<a
href="https://doi.org/10.1093/biostatistics/kxy043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graphical lasso is one of the most used estimators for inferring genetic networks. Despite its diffusion, there are several fields in applied research where the limits of detection of modern measurement technologies make the use of this estimator theoretically unfounded, even when the assumption of a multivariate Gaussian distribution is satisfied. Typical examples are data generated by polymerase chain reactions and flow cytometer. The combination of censoring and high-dimensionality make inference of the underlying genetic networks from these data very challenging. In this article, we propose an |$\ell_1$| -penalized Gaussian graphical model for censored data and derive two EM-like algorithms for inference. We evaluate the computational efficiency of the proposed algorithms by an extensive simulation study and show that, when censored data are available, our proposal is superior to existing competitors both in terms of network recovery and parameter estimation. We apply the proposed method to gene expression data generated by microfluidic Reverse Transcription quantitative Polymerase Chain Reaction technology in order to make inference on the regulatory mechanisms of blood development. A software implementation of our method is available on github ( https://github.com/LuigiAugugliaro/cglasso ).},
  archive      = {J_BIOSTAT},
  author       = {Augugliaro, Luigi and Abbruzzo, Antonino and Vinciotti, Veronica},
  doi          = {10.1093/biostatistics/kxy043},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {e1-e16},
  shortjournal = {Biostatistics},
  title        = {ℓ1-penalized censored gaussian graphical model},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Regulatory oversight, causal inference, and safe and
effective health care machine learning. <em>BIOSTAT</em>,
<em>21</em>(2), 363–367. (<a
href="https://doi.org/10.1093/biostatistics/kxz044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the applications of Machine Learning (ML) in the health care delivery setting have grown to become both abundant and compelling. Regulators have taken notice of these developments and the U.S. Food and Drug Administration (FDA) has been engaging actively in thinking about how best to facilitate safe and effective use. Although the scope of its oversight for software-driven products is limited, if FDA takes the lead in promoting and facilitating appropriate applications of causal inference as a part of ML development, that leadership is likely to have implications well beyond regulated products.},
  archive      = {J_BIOSTAT},
  author       = {Stern, Ariel Dora and Price, W Nicholson},
  doi          = {10.1093/biostatistics/kxz044},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {363-367},
  shortjournal = {Biostatistics},
  title        = {Regulatory oversight, causal inference, and safe and effective health care machine learning},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Can we learn individual-level treatment policies from
clinical data? <em>BIOSTAT</em>, <em>21</em>(2), 359–362. (<a
href="https://doi.org/10.1093/biostatistics/kxz043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOSTAT},
  author       = {Shalit, Uri},
  doi          = {10.1093/biostatistics/kxz043},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {359-362},
  shortjournal = {Biostatistics},
  title        = {Can we learn individual-level treatment policies from clinical data?},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Machine learning in the estimation of causal effects:
Targeted minimum loss-based estimation and double/debiased machine
learning. <em>BIOSTAT</em>, <em>21</em>(2), 353–358. (<a
href="https://doi.org/10.1093/biostatistics/kxz042">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent decades, the fields of statistical and machine learning have seen a revolution in the development of data-adaptive regression methods that have optimal performance under flexible, sometimes minimal, assumptions on the true regression functions. These developments have impacted all areas of applied and theoretical statistics and have allowed data analysts to avoid the biases incurred under the pervasive practice of parametric model misspecification. In this commentary, I discuss issues around the use of data-adaptive regression in estimation of causal inference parameters. To ground ideas, I focus on two estimation approaches with roots in semi-parametric estimation theory: targeted minimum loss-based estimation (TMLE; van der Laan and Rubin, 2006 ) and double/debiased machine learning (DML; Chernozhukov and others , 2018 ). This commentary is not comprehensive, the literature on these topics is rich, and there are many subtleties and developments which I do not address. These two frameworks represent only a small fraction of an increasingly large number of methods for causal inference using machine learning. To my knowledge, they are the only methods grounded in statistical semi-parametric theory that also allow unrestricted use of data-adaptive regression techniques.},
  archive      = {J_BIOSTAT},
  author       = {Díaz, Iván},
  doi          = {10.1093/biostatistics/kxz042},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {353-358},
  shortjournal = {Biostatistics},
  title        = {Machine learning in the estimation of causal effects: Targeted minimum loss-based estimation and double/debiased machine learning},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). From development to deployment: Dataset shift, causality,
and shift-stable models in health AI. <em>BIOSTAT</em>, <em>21</em>(2),
345–352. (<a
href="https://doi.org/10.1093/biostatistics/kxz041">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOSTAT},
  author       = {Subbaswamy, Adarsh and Saria, Suchi},
  doi          = {10.1093/biostatistics/kxz041},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {345-352},
  shortjournal = {Biostatistics},
  title        = {From development to deployment: Dataset shift, causality, and shift-stable models in health AI},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Teaching yourself about structural racism will improve your
machine learning. <em>BIOSTAT</em>, <em>21</em>(2), 339–344. (<a
href="https://doi.org/10.1093/biostatistics/kxz040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this commentary, we put forth the following argument: Anyone conducting machine learning in a health-related domain should educate themselves about structural racism. We argue that structural racism is a critical body of knowledge needed for generalizability in almost all domains of health research.},
  archive      = {J_BIOSTAT},
  author       = {Robinson, Whitney R and Renson, Audrey and Naimi, Ashley I},
  doi          = {10.1093/biostatistics/kxz040},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {339-344},
  shortjournal = {Biostatistics},
  title        = {Teaching yourself about structural racism will improve your machine learning},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Machine learning for causal inference in biostatistics.
<em>BIOSTAT</em>, <em>21</em>(2), 336–338. (<a
href="https://doi.org/10.1093/biostatistics/kxz045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOSTAT},
  author       = {Rose, Sherri and Rizopoulos, Dimitris},
  doi          = {10.1093/biostatistics/kxz045},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {336-338},
  shortjournal = {Biostatistics},
  title        = {Machine learning for causal inference in biostatistics},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian model averaging for the x-chromosome inactivation
dilemma in genetic association study. <em>BIOSTAT</em>, <em>21</em>(2),
319–335. (<a
href="https://doi.org/10.1093/biostatistics/kxy049">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {X-chromosome is often excluded from the so called “whole-genome” association studies due to the differences it exhibits between males and females. One particular analytical challenge is the unknown status of X-inactivation, where one of the two X-chromosome variants in females may be randomly selected to be silenced. In the absence of biological evidence in favor of one specific model, we consider a Bayesian model averaging framework that offers a principled way to account for the inherent model uncertainty, providing model averaging-based posterior density intervals and Bayes factors. We examine the inferential properties of the proposed methods via extensive simulation studies, and we apply the methods to a genetic association study of an intestinal disease occurring in about 20% of cystic fibrosis patients. Compared with the results previously reported assuming the presence of inactivation, we show that the proposed Bayesian methods provide more feature-rich quantities that are useful in practice.},
  archive      = {J_BIOSTAT},
  author       = {Chen, Bo and Craiu, Radu V and Sun, Lei},
  doi          = {10.1093/biostatistics/kxy049},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {319-335},
  shortjournal = {Biostatistics},
  title        = {Bayesian model averaging for the X-chromosome inactivation dilemma in genetic association study},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generalized integrative principal component analysis for
multi-type data with block-wise missing structure. <em>BIOSTAT</em>,
<em>21</em>(2), 302–318. (<a
href="https://doi.org/10.1093/biostatistics/kxy052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High-dimensional multi-source data are encountered in many fields. Despite recent developments on the integrative dimension reduction of such data, most existing methods cannot easily accommodate data of multiple types (e.g. binary or count-valued). Moreover, multi-source data often have block-wise missing structure, i.e. data in one or more sources may be completely unobserved for a sample. The heterogeneous data types and presence of block-wise missing data pose significant challenges to the integration of multi-source data and further statistical analyses. In this article, we develop a low-rank method, called generalized integrative principal component analysis (GIPCA), for the simultaneous dimension reduction and imputation of multi-source block-wise missing data, where different sources may have different data types. We also devise an adapted Bayesian information criterion (BIC) criterion for rank estimation. Comprehensive simulation studies demonstrate the efficacy of the proposed method in terms of rank estimation, signal recovery, and missing data imputation. We apply GIPCA to a mortality study. We achieve accurate block-wise missing data imputation and identify intriguing latent mortality rate patterns with sociological relevance.},
  archive      = {J_BIOSTAT},
  author       = {Zhu, Huichen and Li, Gen and Lock, Eric F},
  doi          = {10.1093/biostatistics/kxy052},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {302-318},
  shortjournal = {Biostatistics},
  title        = {Generalized integrative principal component analysis for multi-type data with block-wise missing structure},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sequential monte carlo for response adaptive randomized
trials. <em>BIOSTAT</em>, <em>21</em>(2), 287–301. (<a
href="https://doi.org/10.1093/biostatistics/kxy048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Response adaptive randomized clinical trials have gained popularity due to their flexibility for adjusting design components, including arm allocation probabilities, at any point in the trial according to the intermediate results. In the Bayesian framework, allocation probabilities to different treatment arms are commonly defined as functionals of the posterior distributions of parameters of the outcome distribution for each treatment. In a non-conjugate model, however, repeated updates of the posterior distribution can be computationally intensive. In this article, we propose an adaptation of sequential Monte Carlo for efficiently updating the posterior distribution of parameters as new outcomes are observed in a general adaptive trial design. An efficient computational tool facilitates implementation of more flexible designs with more frequent interim looks that can in turn reduce the required sample size and expected number of failures in clinical trials. Moreover, more complex statistical models that reflect realistic modeling assumptions can be used for analysis of trial results.},
  archive      = {J_BIOSTAT},
  author       = {Golchi, Shirin and Thorlund, Kristian},
  doi          = {10.1093/biostatistics/kxy048},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {287-301},
  shortjournal = {Biostatistics},
  title        = {Sequential monte carlo for response adaptive randomized trials},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian modeling of dependence in brain connectivity data.
<em>BIOSTAT</em>, <em>21</em>(2), 269–286. (<a
href="https://doi.org/10.1093/biostatistics/kxy046">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain connectivity studies often refer to brain areas as graph nodes and connections between nodes as edges, and aim to identify neuropsychiatric phenotype-related connectivity patterns. When performing group-level brain connectivity alternation analyses, it is critical to model the dependence structure between multivariate connectivity edges to achieve accurate and efficient estimates of model parameters. However, specifying and estimating dependencies between connectivity edges presents formidable challenges because (i) the dimensionality of parameters in the covariance matrix is high (of the order of the fourth power of the number of nodes); (ii) the covariance between a pair of edges involves four nodes with spatial location information; and (iii) the dependence structure between edges can be related to unknown network topological structures. Existing methods for large covariance/precision matrix regularization and spatial closeness-based dependence structure specification/estimation models may not fully address the complexity and challenges. We develop a new Bayesian nonparametric model that unifies information from brain network areas (nodes), connectivity (edges), and covariance between edges by constructing the function of covariance matrix based on the underlying network topological structure. We perform parameter estimation using an efficient Markov chain Monte Carlo algorithm. We apply our method to resting-state functional magnetic resonance imaging data from 60 subjects of a schizophrenia study and simulated data to demonstrate the performance of our method.},
  archive      = {J_BIOSTAT},
  author       = {Chen, Shuo and Xing, Yishi and Kang, Jian and Kochunov, Peter and Hong, L Elliot},
  doi          = {10.1093/biostatistics/kxy046},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {269-286},
  shortjournal = {Biostatistics},
  title        = {Bayesian modeling of dependence in brain connectivity data},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The impact of different sources of heterogeneity on loss of
accuracy from genomic prediction models. <em>BIOSTAT</em>,
<em>21</em>(2), 253–268. (<a
href="https://doi.org/10.1093/biostatistics/kxy044">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cross-study validation (CSV) of prediction models is an alternative to traditional cross-validation (CV) in domains where multiple comparable datasets are available. Although many studies have noted potential sources of heterogeneity in genomic studies, to our knowledge none have systematically investigated their intertwined impacts on prediction accuracy across studies. We employ a hybrid parametric/non-parametric bootstrap method to realistically simulate publicly available compendia of microarray, RNA-seq, and whole metagenome shotgun microbiome studies of health outcomes. Three types of heterogeneity between studies are manipulated and studied: (i) imbalances in the prevalence of clinical and pathological covariates, (ii) differences in gene covariance that could be caused by batch, platform, or tumor purity effects, and (iii) differences in the “true” model that associates gene expression and clinical factors to outcome. We assess model accuracy, while altering these factors. Lower accuracy is seen in CSV than in CV. Surprisingly, heterogeneity in known clinical covariates and differences in gene covariance structure have very limited contributions in the loss of accuracy when validating in new studies. However, forcing identical generative models greatly reduces the within/across study difference. These results, observed consistently for multiple disease outcomes and omics platforms, suggest that the most easily identifiable sources of study heterogeneity are not necessarily the primary ones that undermine the ability to accurately replicate the accuracy of omics prediction models in new studies. Unidentified heterogeneity, such as could arise from unmeasured confounding, may be more important.},
  archive      = {J_BIOSTAT},
  author       = {Zhang, Yuqing and Bernau, Christoph and Parmigiani, Giovanni and Waldron, Levi},
  doi          = {10.1093/biostatistics/kxy044},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {253-268},
  shortjournal = {Biostatistics},
  title        = {The impact of different sources of heterogeneity on loss of accuracy from genomic prediction models},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Missing data and prediction: The pattern submodel.
<em>BIOSTAT</em>, <em>21</em>(2), 236–252. (<a
href="https://doi.org/10.1093/biostatistics/kxy040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missing data are a common problem for both the construction and implementation of a prediction algorithm. Pattern submodels (PS)—a set of submodels for every missing data pattern that are fit using only data from that pattern—are a computationally efficient remedy for handling missing data at both stages. Here, we show that PS (i) retain their predictive accuracy even when the missing data mechanism is not missing at random (MAR) and (ii) yield an algorithm that is the most predictive among all standard missing data strategies. Specifically, we show that the expected loss of a forecasting algorithm is minimized when each pattern-specific loss is minimized. Simulations and a re-analysis of the SUPPORT study confirms that PS generally outperforms zero-imputation, mean-imputation, complete-case analysis, complete-case submodels, and even multiple imputation (MI). The degree of improvement is highly dependent on the missingness mechanism and the effect size of missing predictors. When the data are MAR, MI can yield comparable forecasting performance but generally requires a larger computational cost. We also show that predictions from the PS approach are equivalent to the limiting predictions for a MI procedure that is dependent on missingness indicators (the MIMI model). The focus of this article is on out-of-sample prediction; implications for model inference are only briefly explored.},
  archive      = {J_BIOSTAT},
  author       = {Fletcher Mercaldo, Sarah and Blume, Jeffrey D},
  doi          = {10.1093/biostatistics/kxy040},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {236-252},
  shortjournal = {Biostatistics},
  title        = {Missing data and prediction: The pattern submodel},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The joint lasso: High-dimensional regression for group
structured data. <em>BIOSTAT</em>, <em>21</em>(2), 219–235. (<a
href="https://doi.org/10.1093/biostatistics/kxy035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider high-dimensional regression over subgroups of observations. Our work is motivated by biomedical problems, where subsets of samples, representing for example disease subtypes, may differ with respect to underlying regression models. In the high-dimensional setting, estimating a different model for each subgroup is challenging due to limited sample sizes. Focusing on the case in which subgroup-specific models may be expected to be similar but not necessarily identical, we treat subgroups as related problem instances and jointly estimate subgroup-specific regression coefficients. This is done in a penalized framework, combining an |$\ell_1$| term with an additional term that penalizes differences between subgroup-specific coefficients. This gives solutions that are globally sparse but that allow information-sharing between the subgroups. We present algorithms for estimation and empirical results on simulated data and using Alzheimer’s disease, amyotrophic lateral sclerosis, and cancer datasets. These examples demonstrate the gains joint estimation can offer in prediction as well as in providing subgroup-specific sparsity patterns.},
  archive      = {J_BIOSTAT},
  author       = {Dondelinger, Frank and Mukherjee, Sach and The Alzheimer’s Disease Neuroimaging Initiative},
  doi          = {10.1093/biostatistics/kxy035},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {219-235},
  shortjournal = {Biostatistics},
  title        = {The joint lasso: High-dimensional regression for group structured data},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the estimation of the incidence and prevalence in
two-phase longitudinal sampling design. <em>BIOSTAT</em>,
<em>21</em>(2), 202–218. (<a
href="https://doi.org/10.1093/biostatistics/kxy033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Two-phase sampling design is a common practice in many medical studies. Generally, the first-phase classification is fallible but relatively cheap, while the accurate second phase state-of-the-art medical diagnosis is complex and rather expensive to perform. When constructed efficiently it offers great potential for higher true case detection as well as for higher precision at a limited cost. In this article, we consider epidemiological studies with two-phase sampling design. However, instead of a single two-phase study, we consider a scenario where a series of two-phase studies are done in a longitudinal fashion on a cohort of interest. Another major design issue is non-curable pattern of certain disease (e.g. Dementia, Alzheimer’s etc.). Thus often the identified disease positive subjects are removed from the original population under observation, as they require clinical attention, which is quite different from the yet unidentified group. In this article, we motivated our methodology development from two real-life studies. We consider efficient and simultaneous estimation of prevalence as well incidence at multiple time points from a sampling design-based approach. We have explicitly shown the benefit of our developed methodology for an elderly population with significant burden of home-health care usage and at the high risk of major depressive disorder.},
  archive      = {J_BIOSTAT},
  author       = {Banerjee, Prithish and Ghosh, Samiran},
  doi          = {10.1093/biostatistics/kxy033},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {202-218},
  shortjournal = {Biostatistics},
  title        = {On the estimation of the incidence and prevalence in two-phase longitudinal sampling design},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A benchmark for dose finding studies with continuous
outcomes. <em>BIOSTAT</em>, <em>21</em>(2), 189–201. (<a
href="https://doi.org/10.1093/biostatistics/kxy045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An important tool to evaluate the performance of any design is an optimal benchmark proposed by O’Quigley and others (2002. Non-parametric optimal design in dose finding studies. Biostatistics 3 , 51–56) that provides an upper bound on the performance of a design under a given scenario. The original benchmark can only be applied to dose finding studies with a binary endpoint. However, there is a growing interest in dose finding studies involving continuous outcomes, but no benchmark for such studies has been developed. We show that the original benchmark and its extension by Cheung (2014. Simple benchmark for complex dose finding studies. Biometrics 70 , 389–397), when looked at from a different perspective, can be generalized to various settings with several discrete and continuous outcomes. We illustrate and compare the benchmark’s performance in the setting of a dose finding Phase I clinical trial with a continuous toxicity endpoint and a Phase I/II trial with binary toxicity and continuous efficacy endpoints. We show that the proposed benchmark provides an accurate upper bound in these contexts and serves as a powerful tool for evaluating designs.},
  archive      = {J_BIOSTAT},
  author       = {Mozgunov, Pavel and Jaki, Thomas and Paoletti, Xavier},
  doi          = {10.1093/biostatistics/kxy045},
  journal      = {Biostatistics},
  month        = {4},
  number       = {2},
  pages        = {189-201},
  shortjournal = {Biostatistics},
  title        = {A benchmark for dose finding studies with continuous outcomes},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Biostatistics reviewer list 2019. <em>BIOSTAT</em>,
<em>21</em>(1), 186–187. (<a
href="https://doi.org/10.1093/biostatistics/kxz055">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_BIOSTAT},
  doi          = {10.1093/biostatistics/kxz055},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {186-187},
  shortjournal = {Biostatistics},
  title        = {Biostatistics reviewer list 2019},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Causal inference in continuous time: An example on prostate
cancer therapy. <em>BIOSTAT</em>, <em>21</em>(1), 172–185. (<a
href="https://doi.org/10.1093/biostatistics/kxy036">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In marginal structural models (MSMs), time is traditionally treated as a discrete parameter. In survival analysis on the other hand, we study processes that develop in continuous time. Therefore, Røysland (2011. A martingale approach to continuous-time marginal structural models. Bernoulli 17, 895–915) developed the continuous-time MSMs, along with continuous-time weights. The continuous-time weights are conceptually similar to the inverse probability weights that are used in discrete time MSMs. Here, we demonstrate that continuous-time MSMs may be used in practice. First, we briefly describe the causal model assumptions using counting process notation, and we suggest how causal effect estimates can be derived by calculating continuous-time weights. Then, we describe how additive hazard models can be used to find such effect estimates. Finally, we apply this strategy to compare medium to long-term differences between the two prostate cancer treatments radical prostatectomy and radiation therapy, using data from the Norwegian Cancer Registry. In contrast to the results of a naive analysis, we find that the marginal cumulative incidence of treatment failure is similar between the strategies, accounting for the competing risk of other death.},
  archive      = {J_BIOSTAT},
  author       = {Ryalen, Pål Christie and Stensrud, Mats Julius and Fosså, Sophie and Røysland, Kjetil},
  doi          = {10.1093/biostatistics/kxy036},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {172-185},
  shortjournal = {Biostatistics},
  title        = {Causal inference in continuous time: An example on prostate cancer therapy},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Instrumental variables estimation with competing risk data.
<em>BIOSTAT</em>, <em>21</em>(1), 158–171. (<a
href="https://doi.org/10.1093/biostatistics/kxy039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time-to-event analyses are often plagued by both—possibly unmeasured—confounding and competing risks. To deal with the former, the use of instrumental variables (IVs) for effect estimation is rapidly gaining ground. We show how to make use of such variables in competing risk analyses. In particular, we show how to infer the effect of an arbitrary exposure on cause-specific hazard functions under a semi-parametric model that imposes relatively weak restrictions on the observed data distribution. The proposed approach is flexible accommodating exposures and IVs of arbitrary type, and enabling covariate adjustment. It makes use of closed-form estimators that can be recursively calculated, and is shown to perform well in simulation studies. We also demonstrate its use in an application on the effect of mammography screening on the risk of dying from breast cancer.},
  archive      = {J_BIOSTAT},
  author       = {Martinussen, Torben and Vansteelandt, Stijn},
  doi          = {10.1093/biostatistics/kxy039},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {158-171},
  shortjournal = {Biostatistics},
  title        = {Instrumental variables estimation with competing risk data},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybrid principal components analysis for region-referenced
longitudinal functional EEG data. <em>BIOSTAT</em>, <em>21</em>(1),
139–157. (<a
href="https://doi.org/10.1093/biostatistics/kxy034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Electroencephalography (EEG) data possess a complex structure that includes regional, functional, and longitudinal dimensions. Our motivating example is a word segmentation paradigm in which typically developing (TD) children, and children with autism spectrum disorder (ASD) were exposed to a continuous speech stream. For each subject, continuous EEG signals recorded at each electrode were divided into one-second segments and projected into the frequency domain via fast Fourier transform. Following a spectral principal components analysis, the resulting data consist of region-referenced principal power indexed regionally by scalp location, functionally across frequencies, and longitudinally by one-second segments. Standard EEG power analyses often collapse information across the longitudinal and functional dimensions by averaging power across segments and concentrating on specific frequency bands. We propose a hybrid principal components analysis for region-referenced longitudinal functional EEG data, which utilizes both vector and functional principal components analyses and does not collapse information along any of the three dimensions of the data. The proposed decomposition only assumes weak separability of the higher-dimensional covariance process and utilizes a product of one dimensional eigenvectors and eigenfunctions, obtained from the regional, functional, and longitudinal marginal covariances, to represent the observed data, providing a computationally feasible non-parametric approach. A mixed effects framework is proposed to estimate the model components coupled with a bootstrap test for group level inference, both geared towards sparse data applications. Analysis of the data from the word segmentation paradigm leads to valuable insights about group-region differences among the TD and verbal and minimally verbal children with ASD. Finite sample properties of the proposed estimation framework and bootstrap inference procedure are further studied via extensive simulations.},
  archive      = {J_BIOSTAT},
  author       = {Scheffler, Aaron and Telesca, Donatello and Li, Qian and Sugar, Catherine A and Distefano, Charlotte and Jeste, Shafali and Şentürk, Damla},
  doi          = {10.1093/biostatistics/kxy034},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {139-157},
  shortjournal = {Biostatistics},
  title        = {Hybrid principal components analysis for region-referenced longitudinal functional EEG data},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Constructing disease onset signatures using
multi-dimensional network-structured biomarkers. <em>BIOSTAT</em>,
<em>21</em>(1), 122–138. (<a
href="https://doi.org/10.1093/biostatistics/kxy037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Potential disease-modifying therapies for neurodegenerative disorders need to be introduced prior to the symptomatic stage in order to be effective. However, current diagnosis of neurological disorders mostly rely on measurements of clinical symptoms and thus only identify symptomatic subjects in their late disease course. Thus, it is of interest to select and integrate biomarkers that may reflect early disease-related pathological changes for earlier diagnosis and recruiting pre-sypmtomatic subjects in a prevention clinical trial. Two sources of biological information are relevant to the construction of biomarker signatures for time to disease onset that is subject to right censoring. First, biomarkers’ effects on disease onset may vary with a subject’s baseline disease stage indicated by a particular marker. Second, biomarkers may be connected through networks, and their effects on disease may be informed by this network structure. To leverage these information, we propose a varying-coefficient hazards model to induce double smoothness over the dimension of the disease stage and over the space of network-structured biomarkers. The distinctive feature of the model is a non-parametric effect that captures non-linear change according to the disease stage and similarity among the effects of linked biomarkers. For estimation and feature selection, we use kernel smoothing of a regularized local partial likelihood and derive an efficient algorithm. Numeric simulations demonstrate significant improvements over existing methods in performance and computational efficiency. Finally, the methods are applied to our motivating study, a recently completed study of Huntington’s disease (HD), where structural brain imaging measures are used to inform age-at-onset of HD and assist clinical trial design. The analysis offers new insights on the structural network signatures for premanifest HD subjects.},
  archive      = {J_BIOSTAT},
  author       = {Li, Xiang and Zeng, Donglin and Marder, Karen and Wang, Yuanjia},
  doi          = {10.1093/biostatistics/kxy037},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {122-138},
  shortjournal = {Biostatistics},
  title        = {Constructing disease onset signatures using multi-dimensional network-structured biomarkers},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A maximum likelihood approach to power calculations for
stepped wedge designs of binary outcomes. <em>BIOSTAT</em>,
<em>21</em>(1), 102–121. (<a
href="https://doi.org/10.1093/biostatistics/kxy031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In stepped wedge designs (SWD), clusters are randomized to the time period during which new patients will receive the intervention under study in a sequential rollout over time. By the study’s end, patients at all clusters receive the intervention, eliminating ethical concerns related to withholding potentially efficacious treatments. This is a practical option in many large-scale public health implementation settings. Little statistical theory for these designs exists for binary outcomes. To address this, we utilized a maximum likelihood approach and developed numerical methods to determine the asymptotic power of the SWD for binary outcomes. We studied how the power of a SWD for detecting risk differences varies as a function of the number of clusters, cluster size, the baseline risk, the intervention effect, the intra-cluster correlation coefficient, and the time effect. We studied the robustness of power to the assumed form of the distribution of the cluster random effects, as well as how power is affected by variable cluster size. % SWD power is sensitive to neither, in contrast to the parallel cluster randomized design which is highly sensitive to variable cluster size. We also found that the approximate weighted least square approach of Hussey and Hughes (2007 , Design and analysis of stepped wedge cluster randomized trials. Contemporary Clinical Trials 28, 182–191) for binary outcomes under-estimates the power in some regions of the parameter spaces, and over-estimates it in others. The new method was applied to the design of a large-scale intervention program on post-partum intra-uterine device insertion services for preventing unintended pregnancy in the first 1.5 years following childbirth in Tanzania, where it was found that the previously available method under-estimated the power.},
  archive      = {J_BIOSTAT},
  author       = {Zhou, Xin and Liao, Xiaomei and Kunz, Lauren M and Normand, Sharon-Lise T and Wang, Molin and Spiegelman, Donna},
  doi          = {10.1093/biostatistics/kxy031},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {102-121},
  shortjournal = {Biostatistics},
  title        = {A maximum likelihood approach to power calculations for stepped wedge designs of binary outcomes},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bayesian approach to mendelian randomization with multiple
pleiotropic variants. <em>BIOSTAT</em>, <em>21</em>(1), 86–101. (<a
href="https://doi.org/10.1093/biostatistics/kxy027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a Bayesian approach to Mendelian randomization (MR), where instruments are allowed to exert pleiotropic (i.e. not mediated by the exposure) effects on the outcome. By having these effects represented in the model by unknown parameters, and by imposing a shrinkage prior distribution that assumes an unspecified subset of the effects to be zero, we obtain a proper posterior distribution for the causal effect of interest. This posterior can be sampled via Markov chain Monte Carlo methods of inference to obtain point and interval estimates. The model priors require a minimal input from the user. We explore the performance of our method by means of a simulation experiment. Our results show that the method is reasonably robust to the presence of directional pleiotropy and moderate correlation between the instruments. One section of the article elaborates the model to deal with two exposures, and illustrates the possibility of using MR to estimate direct and indirect effects in this situation. A main objective of the article is to create a basis for developments in MR that exploit the potential offered by a Bayesian approach to the problem, in relation with the possibility of incorporating external information in the prior, handling multiple sources of uncertainty, and flexibly elaborating the basic model.},
  archive      = {J_BIOSTAT},
  author       = {Berzuini, Carlo and Guo, Hui and Burgess, Stephen and Bernardinelli, Luisa},
  doi          = {10.1093/biostatistics/kxy027},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {86-101},
  shortjournal = {Biostatistics},
  title        = {A bayesian approach to mendelian randomization with multiple pleiotropic variants},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Non-parametric recurrent events analysis with BART and an
application to the hospital admissions of patients with diabetes.
<em>BIOSTAT</em>, <em>21</em>(1), 69–85. (<a
href="https://doi.org/10.1093/biostatistics/kxy032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Much of survival analysis is concerned with absorbing events, i.e., subjects can only experience a single event such as mortality. This article is focused on non-absorbing or recurrent events, i.e., subjects are capable of experiencing multiple events. Recurrent events have been studied by many; however, most rely on the restrictive assumptions of linearity and proportionality. We propose a new method for analyzing recurrent events with Bayesian Additive Regression Trees (BART) avoiding such restrictive assumptions. We explore this new method via a motivating example of hospital admissions for diabetes patients and simulated data sets.},
  archive      = {J_BIOSTAT},
  author       = {Sparapani, Rodney A and Rein, Lisa E and Tarima, Sergey S and Jackson, Tourette A and Meurer, John R},
  doi          = {10.1093/biostatistics/kxy032},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {69-85},
  shortjournal = {Biostatistics},
  title        = {Non-parametric recurrent events analysis with BART and an application to the hospital admissions of patients with diabetes},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Individualized treatment effects with censored data via
fully nonparametric bayesian accelerated failure time models.
<em>BIOSTAT</em>, <em>21</em>(1), 50–68. (<a
href="https://doi.org/10.1093/biostatistics/kxy028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Individuals often respond differently to identical treatments, and characterizing such variability in treatment response is an important aim in the practice of personalized medicine. In this article, we describe a nonparametric accelerated failure time model that can be used to analyze heterogeneous treatment effects (HTE) when patient outcomes are time-to-event. By utilizing Bayesian additive regression trees and a mean-constrained Dirichlet process mixture model, our approach offers a flexible model for the regression function while placing few restrictions on the baseline hazard. Our nonparametric method leads to natural estimates of individual treatment effect and has the flexibility to address many major goals of HTE assessment. Moreover, our method requires little user input in terms of model specification for treatment covariate interactions or for tuning parameter selection. Our procedure shows strong predictive performance while also exhibiting good frequentist properties in terms of parameter coverage and mitigation of spurious findings of HTE. We illustrate the merits of our proposed approach with a detailed analysis of two large clinical trials (N = 6769) for the prevention and treatment of congestive heart failure using an angiotensin-converting enzyme inhibitor. The analysis revealed considerable evidence for the presence of HTE in both trials as demonstrated by substantial estimated variation in treatment effect and by high proportions of patients exhibiting strong evidence of having treatment effects which differ from the overall treatment effect.},
  archive      = {J_BIOSTAT},
  author       = {Henderson, Nicholas C and Louis, Thomas A and Rosner, Gary L and Varadhan, Ravi},
  doi          = {10.1093/biostatistics/kxy028},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {50-68},
  shortjournal = {Biostatistics},
  title        = {Individualized treatment effects with censored data via fully nonparametric bayesian accelerated failure time models},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). STEPS: An efficient prospective likelihood approach to
genetic association analyses of secondary traits in extreme phenotype
sequencing. <em>BIOSTAT</em>, <em>21</em>(1), 33–49. (<a
href="https://doi.org/10.1093/biostatistics/kxy030">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been well acknowledged that methods for secondary trait (ST) association analyses under a case–control design (ST |$_{\text{CC}}$|⁠ ) should carefully consider the sampling process to avoid biased risk estimates. A similar situation also exists in the extreme phenotype sequencing (EPS) designs, which is to select subjects with extreme values of continuous primary phenotype for sequencing. EPS designs are commonly used in modern epidemiological and clinical studies such as the well-known National Heart, Lung, and Blood Institute Exome Sequencing Project. Although naïve generalized regression or ST |$_{\text{CC}}$| method could be applied, their validity is questionable due to difference in statistical designs. Herein, we propose a general prospective likelihood framework to perform association testing for binary and continuous STs under EPS designs (STEPS), which can also incorporate covariates and interaction terms. We provide a computationally efficient and robust algorithm to obtain the maximum likelihood estimates. We also present two empirical mathematical formulas for power/sample size calculations to facilitate planning of binary/continuous STs association analyses under EPS designs. Extensive simulations and application to a genome-wide association study of benign ethnic neutropenia under an EPS design demonstrate the superiority of STEPS over all its alternatives above.},
  archive      = {J_BIOSTAT},
  author       = {Bi, Wenjian and Li, Yun and Smeltzer, Matthew P and Gao, Guimin and Zhao, Shengli and Kang, Guolian},
  doi          = {10.1093/biostatistics/kxy030},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {33-49},
  shortjournal = {Biostatistics},
  title        = {STEPS: An efficient prospective likelihood approach to genetic association analyses of secondary traits in extreme phenotype sequencing},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Empirical bayes shrinkage and false discovery rate
estimation, allowing for unwanted variation. <em>BIOSTAT</em>,
<em>21</em>(1), 15–32. (<a
href="https://doi.org/10.1093/biostatistics/kxy029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We combine two important ideas in the analysis of large-scale genomics experiments (e.g. experiments that aim to identify genes that are differentially expressed between two conditions). The first is use of Empirical Bayes (EB) methods to handle the large number of potentially-sparse effects, and estimate false discovery rates and related quantities. The second is use of factor analysis methods to deal with sources of unwanted variation such as batch effects and unmeasured confounders. We describe a simple modular fitting procedure that combines key ideas from both these lines of research. This yields new, powerful EB methods for analyzing genomics experiments that account for both sparse effects and unwanted variation. In realistic simulations, these new methods provide significant gains in power and calibration over competing methods. In real data analysis, we find that different methods, while often conceptually similar, can vary widely in their assessments of statistical significance. This highlights the need for care in both choice of methods and interpretation of results.},
  archive      = {J_BIOSTAT},
  author       = {Gerard, David and Stephens, Matthew},
  doi          = {10.1093/biostatistics/kxy029},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {15-32},
  shortjournal = {Biostatistics},
  title        = {Empirical bayes shrinkage and false discovery rate estimation, allowing for unwanted variation},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Joint modeling of recurrent events and survival: A bayesian
non-parametric approach. <em>BIOSTAT</em>, <em>21</em>(1), 1–14. (<a
href="https://doi.org/10.1093/biostatistics/kxy026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Heart failure (HF) is one of the main causes of morbidity, hospitalization, and death in the western world, and the economic burden associated with HF management is relevant and expected to increase in the future. We consider hospitalization data for HF in the most populated Italian Region, Lombardia. Data were extracted from the administrative data warehouse of the regional healthcare system. The main clinical outcome of interest is time to death and research focus is on investigating how recurrent hospitalizations affect the time to event. The main contribution of the article is to develop a joint model for gap times between consecutive rehospitalizations and survival time. The probability models for the gap times and for the survival outcome share a common patient specific frailty term. Using a flexible Dirichlet process model for %Bayesian nonparametric prior as the random-effects distribution accounts for patient heterogeneity in recurrent event trajectories. Moreover, the joint model allows for dependent censoring of gap times by death or administrative reasons and for the correlations between different gap times for the same individual. It is straightforward to include covariates in the survival and/or recurrence process through the specification of appropriate regression terms. The main advantages of the proposed methodology are wide applicability, ease of interpretation, and efficient computations. Posterior inference is implemented through Markov chain Monte Carlo methods.},
  archive      = {J_BIOSTAT},
  author       = {Paulon, Giorgio and De Iorio, Maria and Guglielmi, Alessandra and Ieva, Francesca},
  doi          = {10.1093/biostatistics/kxy026},
  journal      = {Biostatistics},
  month        = {1},
  number       = {1},
  pages        = {1-14},
  shortjournal = {Biostatistics},
  title        = {Joint modeling of recurrent events and survival: A bayesian non-parametric approach},
  volume       = {21},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
