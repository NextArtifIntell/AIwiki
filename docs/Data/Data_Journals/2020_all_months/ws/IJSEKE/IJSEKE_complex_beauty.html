<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>IJSEKE_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="ijseke---77">IJSEKE - 77</h2>
<ul>
<li><details>
<summary>
(2020). An attribute-based cross-domain access control model for a
distributed multiple autonomous network. <em>IJSEKE</em>,
<em>30</em>(11n12), 1851–1865. (<a
href="https://doi.org/10.1142/S0218194020500400">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The distributed multiple autonomous network has become the main trend of modern information systems, such as Cloud, Service-Oriented Architecture (SOA) and Internet of Things (IoT). Access control in such a heterogeneous and dynamic system has become a major information security challenge, which hinders the sharing of resources and information. In this work, we present an Attribute-Based Access Control (ABAC) model for cross-domain access control. The proposed access control model comprises a boundary control server designed to provide cross-domain access control capability. An eXtensible Access Control Markup Language (XACML) policy model-based implementation is introduced. Process and implementation show the proposed model alleviates the security risk on the domain boundary during the cross-domain access control process.},
  archive      = {J_IJSEKE},
  author       = {Yunpeng Zhang and Xin Liu},
  doi          = {10.1142/S0218194020500400},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1851-1865},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {An attribute-based cross-domain access control model for a distributed multiple autonomous network},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling and selecting frameworks in terms of patterns,
tactics and system qualities. <em>IJSEKE</em>, <em>30</em>(11n12),
1819–1850. (<a href="https://doi.org/10.1142/S021819402040032X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Selecting a framework and documenting the rationale for choosing it is an essential task for system architects. Different framework selection approaches have been proposed in the literature. However, none of these connect frameworks to qualities based on their implemented patterns and tactics. In this paper, we propose a way to semi-automatically compare the quality attributes of frameworks by extracting the patterns and tactics from a framework’s source code and documenting them to connect frameworks to requirements upon which a selection can be made. We use a tool called Archie (a tool used to extract tactics from a Java-based system’s code) to extract the patterns/tactics from the implementation code of frameworks. We then document and model these patterns/tactics and their impact on qualities using the Goal-oriented Requirements Language (GRL). After that, we reuse these models of patterns and tactics to model frameworks in terms of their implemented patterns and tactics. The satisfaction level of the quality requirements integrated with other criteria such as the preferences of an architect provide architects with a tool for comparing different frameworks and documenting their rationale for choosing a framework. As a validation of the approach, we apply it to three realistic case studies with promising results.},
  archive      = {J_IJSEKE},
  author       = {Hind Milhem and Michael Weiss and Stephane S. Some},
  doi          = {10.1142/S021819402040032X},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1819-1850},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Modeling and selecting frameworks in terms of patterns, tactics and system qualities},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Conversion-based approach to obtain an SNN construction.
<em>IJSEKE</em>, <em>30</em>(11n12), 1801–1818. (<a
href="https://doi.org/10.1142/S0218194020400318">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spiking Neuron Network (SNN) uses spike sequence for data processing, so it has an excellent characteristic of low power consumption. However, due to the immaturity of learning algorithm, the multiplayer network training has difficulty in convergence. Utilizing the mature learning algorithm and fast training speed of the back-propagation network, this paper proposes a method to converse the Convolutional Neural Network (CNN) to the SNN. First, the adjustment strategy for CNN is introduced. Then after training, the weight parameters in the model are extracted, which is the corresponding synaptic weight in the layer of the SNN. Finally, a new threshold-setting algorithm based on feedback is proposed to solve the critical problem of the threshold setting of neurons in the SNN. We evaluate our method on the CIFAR-10 datasets released by Hinton’s team. The experimental results show that the image classification accuracy of the SNN is more than 98% of that of CNN, and the theoretical value of power consumption per second is 3.9 mW.},
  archive      = {J_IJSEKE},
  author       = {Ying Shang and Yongli Li and Feng You and RuiLian Zhao},
  doi          = {10.1142/S0218194020400318},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1801-1818},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Conversion-based approach to obtain an SNN construction},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Is bug severity in line with bug fixing change complexity?
<em>IJSEKE</em>, <em>30</em>(11n12), 1779–1800. (<a
href="https://doi.org/10.1142/S0218194020400306">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Both complexity of code change for bug fixing and bug severity play an important role in release planning when considering which bugs should be fixed in a specific release under certain constraints. This work investigates whether there are significant differences between bugs of different severity levels regarding the complexity of code change for fixing the bugs. Code change complexity is measured by the number of modified lines of code, source files, and packages, as well as the entropy of code change. We performed a case study on 20 Apache open source software (OSS) projects using commit records and bug reports. The study results show that (1) for bugs of high severity levels (i.e. Blocker, Critical and Major in JIRA), there is no significant difference on the complexity of code change for fixing bugs of different severity levels for most projects, while (2) for bugs of low severity levels (i.e. Major, Minor and Trivial in JIRA), fixing bugs of a higher severity level needs significantly more complex code change than fixing bugs of a lower severity level for most projects. These findings provide useful and practical insights for effort estimation and release planning of OSS development.},
  archive      = {J_IJSEKE},
  author       = {Zengyang Li and Peng Liang and Dengwei Li and Ran Mo and Bing Li},
  doi          = {10.1142/S0218194020400306},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1779-1800},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Is bug severity in line with bug fixing change complexity?},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An experimental study of spammer detection on chinese
microblogs. <em>IJSEKE</em>, <em>30</em>(11n12), 1759–1777. (<a
href="https://doi.org/10.1142/S021819402040029X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of Web 2.0, social media such as Twitter and Sina Weibo have become an essential platform for disseminating hot events. Simultaneously, due to the free policy of microblogging services, users can post user-generated content freely on microblogging platforms. Accordingly, more and more hot events on microblogging platforms have been labeled as spammers. Spammers will not only hurt the healthy development of social media but also introduce many economic and social problems. Therefore, the government and enterprises must distinguish whether a hot event on microblogging platforms is a spammer or is a naturally-developing event. In this paper, we focus on the hot event list on Sina Weibo and collect the relevant microblogs of each hot event to study the detecting methods of spammers. Notably, we develop an integral feature set consisting of user profile, user behavior, and user relationships to reflect various factors affecting the detection of spammers. Then, we employ typical machine learning methods to conduct extensive experiments on detecting spammers. We use a real data set crawled from the most prominent Chinese microblogging platform, Sina Weibo, and evaluate the performance of 10 machine learning models with five sampling methods. The results in terms of various metrics show that the Random Forest model and the over-sampling method achieve the best accuracy in detecting spammers and non-spammers.},
  archive      = {J_IJSEKE},
  author       = {Jialing Liang and Peiquan Jin and Lin Mu and Jie Zhao},
  doi          = {10.1142/S021819402040029X},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1759-1777},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {An experimental study of spammer detection on chinese microblogs},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Identifying similar users based on their check-in data: A
graph embedding approach. <em>IJSEKE</em>, <em>30</em>(11n12),
1735–1757. (<a href="https://doi.org/10.1142/S0218194020400288">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, the amount of user check-in data has significantly increased on social network platforms. Such data is an ideal source for characterizing user behaviors and identifying similar users, contributing to many research areas (e.g. user-based collaborative filtering). However, existing trajectory-based user similarity analysis approaches do not distinguish the effects of geographical factors at a fine-grained level, and thus are not able to unleash the full power of semantic information that is hidden in the trajectory. In this paper, we have proposed an effective graph embedding approach to identify similar users based on their check-in data. Specifically, we firstly identify meaningful concepts of user check-in data, based on which we design two metagraphs for representing features of similar user behaviors. Then we characterize each user with a sequence of nodes that are derived through a metagraph-guided random walk strategy. Such sequences are embedded to generate meaningful user vectors for measuring user similarity and eventually identifying similar users. We have evaluated our proposal on three public datasets, the results of which show that our approach is 4% higher than the best existing approach in terms of F 1-measure.},
  archive      = {J_IJSEKE},
  author       = {Rui Song and Tong Li and Xin Dong and Zhiming Ding},
  doi          = {10.1142/S0218194020400288},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1735-1757},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Identifying similar users based on their check-in data: A graph embedding approach},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automatic voter recommendation method for closing questions
in stack overflow. <em>IJSEKE</em>, <em>30</em>(11n12), 1707–1733. (<a
href="https://doi.org/10.1142/S0218194020400276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stack Overflow is the most popular programming question and answer community that continuously receives a large number of questions every day. To ensure the quality of questions, the community grants privileges for the moderators and a group of experienced users to review the quality of questions and close the low-quality ones (e.g. duplicate or irrelevant questions). The review process is a typical crowdsourcing job that relies on users’ volunteer participation, and the current practices of closing questions in Stack Overflow face two aspects of challenges: (1) an obvious increase in both the absolute number and the percentage of “closed” questions; (2) a considerable decrease in participation willingness of experienced users to close questions. In order to solve the problem, we present a novel model of user willingness for reviewing and voting questions by incorporating four types of user activity history, including questions, answers, comments and votes of closing questions. Then we propose an automatic recommendation method based on the model to assign experienced users proper questions, to utilize the forces of them to close questions. The evaluation shows that the successful recommendation probability in the top 5, top 10, top 20, top 30, top 40, top 50 users are 48.23%, 58.93%, 68.83%, 74.27%, 78.13% and 81%, respectively.},
  archive      = {J_IJSEKE},
  author       = {Zhang Zhang and Xinjun Mao and Yao Lu and Jinyu Lu and Yue Yu and Zhixing Li},
  doi          = {10.1142/S0218194020400276},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1707-1733},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Automatic voter recommendation method for closing questions in stack overflow},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Prediction of regional commercial activeness and entity
condition based on online reviews. <em>IJSEKE</em>, <em>30</em>(11n12),
1689–1705. (<a href="https://doi.org/10.1142/S0218194020400264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The activeness of regional business entities, like restaurants, cinemas and shopping malls, represents the evolvement of their corresponding commercial districts, whose prediction helps practitioners grasp the trend of commercial development and provides support for urban layout. On the other hand, online social network services, such as Yelp, are generating massive online reviews toward business entities every day, which provide a solid data source for the prediction of regional commercial activeness and entity condition through big data technology rather than applying business data with limited access and poor time efficiency. Inspired by the outstanding performance of deep learning in the field of image and video processing, this paper proposes a deep spatio-temporal residual network (DSTRN) model for regional commercial activeness prediction using online reviews and check-in records of commercial entities. Furthermore, aiming at predicting business trend of entities, we also propose a novel multi-view entity condition prediction model (SBCE) based on online views, along with business attributes and regional commercial activeness. The experiments on the public Yelp datasets demonstrate that both DSTRN and SBCE outperform the compared approaches.},
  archive      = {J_IJSEKE},
  author       = {Dongjin Yu and Xinfeng Wang and Xiaoxiao Sun},
  doi          = {10.1142/S0218194020400264},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1689-1705},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Prediction of regional commercial activeness and entity condition based on online reviews},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Plagiarism detection of multi-threaded programs using
frequent behavioral pattern mining. <em>IJSEKE</em>, <em>30</em>(11n12),
1667–1688. (<a href="https://doi.org/10.1142/S0218194020400252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software dynamic birthmark techniques construct birthmarks using the captured execution traces from running the programs, which serve as one of the most promising methods for obfuscation-resilient software plagiarism detection. However, due to the perturbation caused by non-deterministic thread scheduling in multi-threaded programs, such dynamic approaches optimized for sequential programs may suffer from the randomness in multi-threaded program plagiarism detection. In this paper, we propose a new dynamic thread-aware birthmark FPBirth to facilitate multi-threaded program plagiarism detection. We first explore dynamic monitoring to capture multiple execution traces with respect to system calls for each multi-threaded program under a specified input, and then leverage the Apriori algorithm to mine frequent patterns to formulate our dynamic birthmark, which can not only depict the program’s behavioral semantics, but also resist the changes and perturbations over execution traces caused by the thread scheduling in multi-threaded programs. Using FPBirth, we design a multi-threaded program plagiarism detection system. The experimental results based on a public software plagiarism sample set demonstrate that the developed system integrating our proposed birthmark FPBirth copes better with multi-threaded plagiarism detection than alternative approaches. Compared against the dynamic birthmark System Call Short Sequence Birthmark (SCSSB), FPBirth achieves 12.4%, 4.1% and 7.9% performance improvements with respect to union of resilience and credibility (URC), F-Measure and matthews correlation coefficient (MCC) metric, respectively.},
  archive      = {J_IJSEKE},
  author       = {Zhenzhou Tian and Qing Wang and Cong Gao and Lingwei Chen and Dinghao Wu},
  doi          = {10.1142/S0218194020400252},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1667-1688},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Plagiarism detection of multi-threaded programs using frequent behavioral pattern mining},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Enabling reliability-driven optimization selection with gate
graph attention neural network. <em>IJSEKE</em>, <em>30</em>(11n12),
1641–1665. (<a href="https://doi.org/10.1142/S0218194020400240">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern compilers provide a huge number of optional compilation optimization options. It is necessary to select the appropriate compilation optimization options for different programs or applications. To mitigate this problem, machine learning is widely used as an efficient technology. How to ensure the integrity and effectiveness of program information is the key to problem mitigation. In addition, when selecting the best compilation optimization option, the optimization goals are often execution speed, code size, and CPU consumption. There is not much research on program reliability. This paper proposes a Gate Graph Attention Neural Network (GGANN)-based compilation optimization option selection model. The data flow and function-call information are integrated into the abstract syntax tree as the program graph-based features. We extend the deep neural network based on GGANN and build a learning model that learns the heuristics method for program reliability. The experiment is performed under the Clang compiler framework. Compared with the traditional machine learning method, our model improves the average accuracy by 5–11% in the optimization option selection for program reliability. At the same time, experiments show that our model has strong scalability.},
  archive      = {J_IJSEKE},
  author       = {Jiang Wu and Jianjun Xu and Xiankai Meng and Haoyu Zhang and Zhuo Zhang},
  doi          = {10.1142/S0218194020400240},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1641-1665},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Enabling reliability-driven optimization selection with gate graph attention neural network},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analyzing the stationarity process in software effort
estimation datasets. <em>IJSEKE</em>, <em>30</em>(11n12), 1607–1640. (<a
href="https://doi.org/10.1142/S0218194020400239">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software effort estimation models are typically developed based on an underlying assumption that all data points are equally relevant to the prediction of effort for future projects. The dynamic nature of several aspects of the software engineering process could mean that this assumption does not hold in at least some cases. This study employs three kernel estimator functions to test the stationarity assumption in five software engineering datasets that have been used in the construction of software effort estimation models. The kernel estimators are used in the generation of nonuniform weights which are subsequently employed in weighted linear regression modeling. In each model, older projects are assigned smaller weights while the more recently completed projects are assigned larger weights, to reflect their potentially greater relevance to present or future projects that need to be estimated. Prediction errors are compared to those obtained from uniform models. Our results indicate that, for the datasets that exhibit underlying nonstationary processes, uniform models are more accurate than the nonuniform models; that is, models based on kernel estimator functions are worse than the models where no weighting was applied. In contrast, the accuracies of uniform and nonuniform models for datasets that exhibited stationary processes were essentially equivalent. Our analysis indicates that as the heterogeneity of a dataset increases, the effect of stationarity is overridden. The results of our study also confirm prior findings that the accuracy of effort estimation models is independent of the type of kernel estimator function used in model development.},
  archive      = {J_IJSEKE},
  author       = {Michael Franklin Bosu and Stephen G. MacDonell and Peter A. Whigham},
  doi          = {10.1142/S0218194020400239},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1607-1640},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Analyzing the stationarity process in software effort estimation datasets},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Guidelines for quality assurance of machine learning-based
artificial intelligence. <em>IJSEKE</em>, <em>30</em>(11n12), 1589–1606.
(<a href="https://doi.org/10.1142/S0218194020400227">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Significant effort is being put into developing industrial applications for artificial intelligence (AI), especially those using machine learning (ML) techniques. Despite the intensive support for building ML applications, there are still challenges when it comes to evaluating, assuring, and improving the quality or dependability. The difficulty stems from the unique nature of ML, namely, system behavior is derived from training data not from logical design by human engineers. This leads to black-box and intrinsically imperfect implementations that invalidate many principles and techniques in traditional software engineering. In light of this situation, the Japanese industry has jointly worked on a set of guidelines for the quality assurance of AI systems (in the Consortium of Quality Assurance for AI-based Products and Services) from the viewpoint of traditional quality-assurance engineers and test engineers. We report on the second version of these guidelines, which cover a list of quality evaluation aspects, catalogue of current state-of-the-art techniques, and domain-specific discussions in five representative domains. The guidelines provide significant insights for engineers in terms of methodologies and designs for tests driven by application-specific requirements.},
  archive      = {J_IJSEKE},
  author       = {Gaku Fujii and Koichi Hamada and Fuyuki Ishikawa and Satoshi Masuda and Mineo Matsuya and Tomoyuki Myojin and Yasuharu Nishi and Hideto Ogawa and Takahiro Toku and Susumu Tokumoto and Kazunori Tsuchiya and Yasuhiro Ujita},
  doi          = {10.1142/S0218194020400227},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1589-1606},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Guidelines for quality assurance of machine learning-based artificial intelligence},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020b). Guest editor’s introduction. <em>IJSEKE</em>,
<em>30</em>(11n12), 1587. (<a
href="https://doi.org/10.1142/S0218194020020052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJSEKE},
  author       = {Shi-Kuo Chang},
  doi          = {10.1142/S0218194020020052},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {11n12},
  pages        = {1587},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Guest editor’s introduction},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Method for predicting mobile service evolution from user
reviews and update logs. <em>IJSEKE</em>, <em>30</em>(10), 1551–1586.
(<a href="https://doi.org/10.1142/S0218194020500394">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because of rapid growth in mobile application markets, competition between companies that provide similar applications has become fierce. To improve user satisfaction for keeping existing users and attracting new users, application developers need to quickly respond to customer feedback regarding functionality and performance defects. In software engineering, specifying an accurate evolution plan according to user feedback is useful but quite difficult. Hence, we propose an approach for predicting and recommending evolution plans to application developers that includes: (1) when a new version of an App should be released; (2) which features should be updated in the next version and (3) if a new version is released, to what degree users would like or dislike it. This approach is based on an elaborate text analysis of massive numbers of user reviews and App update histories. A collocation-based mRAKE method is presented to extract requested and updated features from user reviews and update logs, and the intensity and sentiment scores of each feature are calculated to quantitatively represent time-series histories of App updates and user requests. Machine learning algorithms including linear support vector, Gaussian naïve Bayes and logistic regression are employed to discover the underlying correlation between user opinions embedded in their reviews and the App update behaviors of developers, and rich experiments were conducted on real data to validate the effectiveness of the proposed approach. Overall, our approach can achieve an average accuracy of 72.8% and 93.7% in release time recommendation and content updates of successive versions, respectively, and it can predict user reactions to a planned version with an average accuracy of above 89.0%.},
  archive      = {J_IJSEKE},
  author       = {Jiafei Song and Zhongjie Wang and Zhiying Tu and Xiaofei Xu},
  doi          = {10.1142/S0218194020500394},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {10},
  pages        = {1551-1586},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Method for predicting mobile service evolution from user reviews and update logs},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design and implementation of phylotastic, a service
architecture for evolutionary biology. <em>IJSEKE</em>, <em>30</em>(10),
1525–1550. (<a href="https://doi.org/10.1142/S0218194020500382">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Access and reuse of authoritative phylogenetic knowledge have been a longstanding challenges in the evolutionary biology community — leading to a number of research efforts (e.g. focused on interoperation, standardization of formats, and development of minimum reporting requirements). The Phylotastic project was launched to provide an answer to such challenges — as an architectural concept collaboratively designed by evolutionary biologists and computer scientists. This paper describes the first comprehensive implementation of the Phylotastic architecture, based on an open platform for Web services composition. The implementation provides a portal, which composes Web services along a fixed collection of workflows, as well as an interface to allow users to develop novel workflows. The Web services composition is guided by automated planning algorithms and built on a Web services registry and an execution monitoring engine. The platform provides resilience through seamless automated recovery from failed services.},
  archive      = {J_IJSEKE},
  author       = {Abu Saleh Md. Tayeen and Thanh Hai Nguyen and Van Duc Nguyen and Enrico Pontelli},
  doi          = {10.1142/S0218194020500382},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {10},
  pages        = {1525-1550},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Design and implementation of phylotastic, a service architecture for evolutionary biology},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A generic approach on how to formally specify and model
check path finding algorithms: Dijkstra, a* and LPA*. <em>IJSEKE</em>,
<em>30</em>(10), 1481–1523. (<a
href="https://doi.org/10.1142/S0218194020400215">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper describes how to formally specify three path finding algorithms in Maude, a rewriting logic-based programming/specification language, and how to model check if they enjoy desired properties with the Maude LTL model checker. The three algorithms are Dijkstra Shortest Path Finding Algorithm (DA), A* Algorithm and LPA* Algorithm. One desired property is that the algorithms always find the shortest path. To this end, we use a path finding algorithm (BFS) based on breadth-first search. BFS finds all paths from a start node to a goal node and the set of all shortest paths is extracted. We check if the path found by each algorithm is included in the set of all shortest paths for the property. A* is an extension of DA in that for each node n an estimation h ( n ) of the distance to the goal node from n is used and LPA* is an incremental version of A*. It is known that if h is admissible, A* always finds the shortest path. We have found a possible relaxed sufficient condition. The relaxed condition is that there exists the shortest path such that for each node n except for the start node on the path h ( n ) plus the cost to n from the start node is less than the cost of any non-shortest path to the goal from the start. We informally justify the relaxed condition. For LPA*, if the relaxed condition holds in each updated version of a graph concerned including the initial graph, the shortest path is constructed. Based on the three case studies for DA, A* and LPA*, we summarize the formal specification and model checking techniques used as a generic approach to formal specification and model checking of path finding algorithms.},
  archive      = {J_IJSEKE},
  author       = {Kazuhiro Ogata},
  doi          = {10.1142/S0218194020400215},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {10},
  pages        = {1481-1523},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A generic approach on how to formally specify and model check path finding algorithms: Dijkstra, a* and LPA*},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Regular expression learning from positive examples based on
integer programming. <em>IJSEKE</em>, <em>30</em>(10), 1443–1479. (<a
href="https://doi.org/10.1142/S0218194020400203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a novel method to infer regular expressions from positive examples. The method consists of a candidate’s construction phase and an optimization phase. We first propose multiscaling sample augmentation to capture the cycle patterns from single examples during the candidate’s construction phase. We then use common substrings to build regular expressions that capture patterns across multiple examples, and we show this algorithm is more general than those based on common prefixes or suffixes. Furthermore, we propose a pruning mechanism to improve the efficiency of useful common substring mining, which is an important part of common substring-based expression building algorithm. Finally, in the optimization phase, we model the problem of choosing a set of regular expressions with the lowest cost as an integer linear program, which can be solved to obtain the optimal solution. The experimental results on synthetic and real-life samples demonstrate the effectiveness of our approach in inferring concise and semantically meaningful regular expressions for string datasets.},
  archive      = {J_IJSEKE},
  author       = {Juntao Gao and Yingqian Zhang},
  doi          = {10.1142/S0218194020400203},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {10},
  pages        = {1443-1479},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Regular expression learning from positive examples based on integer programming},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semantic recovery of traceability links between system
artifacts. <em>IJSEKE</em>, <em>30</em>(10), 1415–1442. (<a
href="https://doi.org/10.1142/S0218194020400197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a mechanism to recover traceability links between the requirements and logical models in the context of critical systems development. Currently, lifecycle processes are covered by a good number of tools that are used to generate different types of artifacts. One of the cornerstone capabilities in the development of critical systems lies in the possibility of automatically recovery traceability links between system artifacts generated in different lifecycle stages. To do so, it is necessary to establish to what extent two or more of these work products are similar, dependent or should be explicitly linked together. However, the different types of artifacts and their internal representation depict a major challenge to unify how system artifacts are represented and, then, linked together. That is why, in this work, a concept-based representation is introduced to provide a semantic and unified description of any system artifact. Furthermore, a traceability function is defined and implemented to exploit this new semantic representation and to support the recovery of traceability links between different types of system artifacts. In order to evaluate the traceability function, a case study in the railway domain is conducted to compare the precision and recall of recovery traceability links between text-based requirements and logical model elements. As the main outcome of this work, the use of a concept-based paradigm to represent that system artifacts are demonstrated as a building block to automatically recover traceability links within the development lifecycle of critical systems.},
  archive      = {J_IJSEKE},
  author       = {Jose María Alvarez-Rodríguez and Roy Mendieta and Valentín Moreno and Miguel Sánchez-Puebla and Juan Llorens},
  doi          = {10.1142/S0218194020400197},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {10},
  pages        = {1415-1442},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Semantic recovery of traceability links between system artifacts},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Linear software models: An occam’s razor set of algebraic
connectors integrates modules into a whole software system.
<em>IJSEKE</em>, <em>30</em>(10), 1375–1413. (<a
href="https://doi.org/10.1142/S0218194020400185">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Well-designed software systems, with providers only modules, have been rigorously obtained by algebraic procedures from the software Laplacian Matrices or their respective Modularity Matrices. However, a complete view of the whole software system should display, besides provider relationships, also consumer relationships. Consumers may have two different roles in a system: either internal or external to modules. Composite modules, including both providers and internal consumers, are obtained from the joint providers and consumers Laplacian matrix, by the same spectral method which obtained providers only modules. The composite modules are integrated into a whole Software System by algebraic connectors. These algebraic connectors are a minimal Occam’s razor set of consumers external to composite modules, revealed through iterative splitting of the Laplacian matrix by Fiedler eigenvectors. The composite modules, of the respective standard Modularity Matrix for the whole software system, also obey linear independence of their constituent vectors, and display block-diagonality. The spectral method leading to composite modules and their algebraic connectors is illustrated by case studies. The essential novelty of this work resides in the minimal Occam’s razor set of algebraic connectors — another facet of Brooks’ Propriety principle leading to Conceptual Integrity of the whole Software System — within Linear Software Models, the unified algebraic theory of software modularity.},
  archive      = {J_IJSEKE},
  author       = {Iaakov Exman and Harel Wallach},
  doi          = {10.1142/S0218194020400185},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {10},
  pages        = {1375-1413},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Linear software models: An occam’s razor set of algebraic connectors integrates modules into a whole software system},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Guest editor’s introduction: Software mathematical models
for human understanding. <em>IJSEKE</em>, <em>30</em>(10), 1371–1374.
(<a href="https://doi.org/10.1142/S0218194020020040">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The unrelenting trend of larger and larger sizes of Software Systems and data has made software comprehensibility an increasingly difficult problem. However, a tacit consensus that human understanding of software is essential for most software related activities, stimulated software developers to embed comprehensibility in their systems’ design. On the other hand, recent empirical successes of Deep Learning neural networks, in several application areas, seem to challenge the tacit consensus: is software comprehensibility a necessity, or just superfluous? This introductory paper, to the 2020 special issue on Theoretical Software Engineering, offers reasons justifying our standpoint on the referred controversy. This paper also points out to specific techniques enabling Human Understanding of software systems relevant to this issue’s papers.},
  archive      = {J_IJSEKE},
  author       = {Iaakov Exman},
  doi          = {10.1142/S0218194020020040},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {10},
  pages        = {1371-1374},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Guest editor’s introduction: Software mathematical models for human understanding},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Agent-oriented methodologies evaluation frameworks: A
review. <em>IJSEKE</em>, <em>30</em>(9), 1337–1370. (<a
href="https://doi.org/10.1142/S0218194020500370">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-agent systems and agent-oriented methodologies support analysis, characterization and development of complex software systems. These methodologies introduce different definitions for the essential components of multi-agent systems and cover different phases of the system development life cycle. Therefore, appropriate frameworks for evaluation and comparison of different methodologies would support developers to adopt the best methodology, or a combination of different methodologies, based on the project requirements. This review covers the system development phases and the main conceptual components in the context of multi-agent systems. Then, the evaluation frameworks proposed in the literature for comparison and evaluation of agent-oriented methodologies are reviewed. Evaluation frameworks proposed in the literature are categorized into three categories: methodology-based, phase-based and feature-based evaluation frameworks. The paper concludes with the agent-oriented methodologies’ usage challenges, their current limitations and potential future directions.},
  archive      = {J_IJSEKE},
  author       = {Ali Jazayeri and Ellen J. Bass},
  doi          = {10.1142/S0218194020500370},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {9},
  pages        = {1337-1370},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Agent-oriented methodologies evaluation frameworks: A review},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). AutoWPR: An automatic web page recoloring method.
<em>IJSEKE</em>, <em>30</em>(9), 1309–1336. (<a
href="https://doi.org/10.1142/S0218194020500369">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The color design is one of the important parts of GUI development. To gain an attractive color scheme, designers often seek inspiration from examples. However, transferring an example’s colors to a target web page is time-consuming and tedious. In this paper, we propose a method named AutoWPR to reuse the example web page’s colors for recoloring a web page. To preserve the semantic relations of web elements, we propose a clustering algorithm to group the related elements into a cluster. In order to make the recoloring result have similar color distributions to the example, we use the Random–Forest regression to learn human’s mappings and propose a top-down matching algorithm to generate a mapping between two web pages’ clusters. Then AutoWPR recolors the element with the matching element’s colors. We designed several experiments to evaluate the correctness of the clustering and matching algorithm. We also conducted some qualitative and quantitative experiments to evaluate the effectiveness of our results in helping recoloring. The results show that our method can generate a human-like recoloring result and help novice developers reuse the reference web page’s colors conveniently.},
  archive      = {J_IJSEKE},
  author       = {Yonghao Long and Xiangping Chen and Fan Zhou},
  doi          = {10.1142/S0218194020500369},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {9},
  pages        = {1309-1336},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {AutoWPR: An automatic web page recoloring method},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ABMMRS eradicator: Improving accuracy in recommending move
methods for web-based MVC projects and libraries using method’s external
dependencies. <em>IJSEKE</em>, <em>30</em>(9), 1289–1307. (<a
href="https://doi.org/10.1142/S0218194020500357">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Move Method Refactoring (MMR) is used to place highly coupled methods in appropriate classes for making source code more cohesive. Like other refactoring techniques, it is mandatory that applying MMR will preserve applications’ behaviors. However, traditional MMR techniques failed to meet this essential precondition for Action methods in web-based application and API methods in libraries projects. The reason is that applying MMR on these methods changes the behaviors of the projects by raising Application-breaking issues, for instance, failure of browser requests and compilation errors in client projects. To resolve this problem, developers are suggested to manually check Action and API methods while applying MMR. However, manually inspecting thousands of lines of code for these issues is a time-consuming and hectic task. In this paper, an advanced MMR technique is proposed which automatically identifies Application-breaking MMR suggestions. This technique first takes the initial move method suggestions from the existing prominent MMR techniques e.g. JDeodorant. For each of the suggestions, it parses the source code and construct Abstract Syntax Tree to examine two types of usage. One is whether a suggestion has not been used in any unit test and Regular Class, and another is whether the suggestion has been used in unit test classes only. If any MMR suggestion is found having one of these two types of usage or both, the respective suggestion is marked as Application-breaking. In order to evaluate the proposed technique, several experiments have been conducted on open source projects. The experimental results show that the proposed technique achieved 96.4% Precision, 90% Recall and 93.1% F -score in detecting Application-breaking MMR suggestions, because of considering external dependencies of the MMR suggestions.},
  archive      = {J_IJSEKE},
  author       = {Atish Kumar Dipongkor and Iftekhar Ahmed and Rayhanul Islam and Nadia Nahar and Abdus Satter and Md. Saeed Siddik},
  doi          = {10.1142/S0218194020500357},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {9},
  pages        = {1289-1307},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {ABMMRS eradicator: Improving accuracy in recommending move methods for web-based MVC projects and libraries using method’s external dependencies},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Algorithmic transparency of large-scale *AIDA programs.
<em>IJSEKE</em>, <em>30</em>(9), 1263–1288. (<a
href="https://doi.org/10.1142/S0218194020500345">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Programming in pictures is an approach where pictures and moving pictures are used as super-characters to represent the features of computational algorithms and data structures, as well as for explaining the models and application methods involved. *AIDA is a computer language that supports programming in pictures. This language and its environment have been developed and promoted as a testbed for various innovations in information technology (IT) research and implementation, including exploring the compactness of the programs and their adaptive software systems, and obtaining better understanding of information resources. In this paper, new features of the environment and methods of their implementation are presented. They are considered within a case study of a large-scale module of a nuclear safety analysis system to demonstrate that *AIDA language is appropriate for developing efficient codes of serious applications and for providing support, based on folding/unfolding techniques, enhancing the readability, maintainability and algorithmic transparency of programs. Features of this support and the code efficiency are presented through the results of a computational comparison with a FORTRAN equivalent.},
  archive      = {J_IJSEKE},
  author       = {Yutaka Watanobe and Nikolay Mirenkov},
  doi          = {10.1142/S0218194020500345},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {9},
  pages        = {1263-1288},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Algorithmic transparency of large-scale *AIDA programs},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An effective approach for context driven testing in practice
— a case study. <em>IJSEKE</em>, <em>30</em>(9), 1245–1262. (<a
href="https://doi.org/10.1142/S0218194020500333">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software testing is a continuous process during the software development stages to ensure quality software products. Researchers, experts and software engineers keep going on studying new techniques, methods and approaches of testing to accommodate changes in software development because of the flexible requirement along with the changing of technology. So, developers and testers need to have effective methods, tools and approaches to create a high-quality product at an efficient cost. This paper provides an effective approach for context-driven testing (CDT) in an agile software development process. CDT is a testing approach that supports the tester to choose their testing techniques and test objectives based on specific contexts. The aim of this paper is to propose an effective approach for implementing the CDT in practice, called CDTiP. Through an analysis of two case studies using an agile development process with different contexts, we validate the effectiveness of the approach in terms of test coverage, detect errors, test effort. The empirical results show that CDTiP is suitable for the agile development process that can help the tester to detect defects faster at minimum cost. The results of this method have been applied at Enclave, an ODC Software Engineering company, on real projects.},
  archive      = {J_IJSEKE},
  author       = {Quyet-Thang Huynh and Le-Trinh Pham and Nhu-Hang Ha and Duc-Man Nguyen},
  doi          = {10.1142/S0218194020500333},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {9},
  pages        = {1245-1262},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {An effective approach for context driven testing in practice — a case study},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Split attention pointer network for source code language
modeling. <em>IJSEKE</em>, <em>30</em>(9), 1221–1244. (<a
href="https://doi.org/10.1142/S0218194020500321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There is a growing interest in leveraging Deep Learning (DL) for automating Software Engineering tasks such as program completion. In this paper, we leverage Recurrent Neural Networks (RNNs) for Abstract Syntax Tree (AST)-based code completion. Our approach converts source code into AST nodes and a language model predicts the type and value attributes of next tokens. Our work demonstrates that the attention augmented RNN-based language models are able to understand local context and copy from recent past tokens which have never appeared in the training data set. We observed a drop of performances of both type and value predictions when using a traditional pointer network architecture for out-of-vocabulary (OoV) copying and context understanding, which we call multi-task conflict . To address this challenge, we have devised a new structure of self-attention called Split Attention , where two separate dot-product layers are applied to different parts of the history cache. Based on this structure, we propose a new network called Split Attention Pointer Network (SAPN), which is efficient and flexible in both learning local context and copying OoV tokens from history. The empirical results suggest that our model is superior in syntax-aware generation and OoV token prediction by demonstrating attention behavior similar to human programmers. The results also indicate that our model out performs previous state-of-the-art approaches by more than 6% on widely recognized program completion benchmarks.},
  archive      = {J_IJSEKE},
  author       = {Zhimin Zhou and Zhongwen Chen},
  doi          = {10.1142/S0218194020500321},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {9},
  pages        = {1221-1244},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Split attention pointer network for source code language modeling},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Context-dependent feature values in text categorization.
<em>IJSEKE</em>, <em>30</em>(9), 1199–1219. (<a
href="https://doi.org/10.1142/S021819402050031X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feature engineering is one aspect of knowledge engineering. Besides feature selection, the appropriate assignment of feature values is also crucial to the performance of many software applications, such as text categorization (TC) and speech recognition. In this work, we develop a general method to enhance TC performance by the use of context-dependent feature values (aka term weights), which are obtained by a novel adaptation of a context-dependent adjustment procedure previously shown to be effective in information retrieval. The motivation of our approach is that the general method can be used with different text representations and in combination of other TC techniques. Experiments on several test collections show that our context-dependent feature values can improve TC over traditional context-independent unigram feature values, using a strong classifier like Support Vector Machine (SVM), which past works have found to be hard to improve. We also show that the relative performance improvement of our method over the context-independent baseline is comparable to the levels attained by recent word embedding methods in the literature, while an advantage of our approach is that it does not require the substantial training needed to learn word embedding representations.},
  archive      = {J_IJSEKE},
  author       = {Edward Kai Fung Dang and Robert Wing Pong Luk and James Allan},
  doi          = {10.1142/S021819402050031X},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {9},
  pages        = {1199-1219},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Context-dependent feature values in text categorization},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A proactive approach to test case selection — an efficient
implementation of adaptive random testing. <em>IJSEKE</em>,
<em>30</em>(8), 1169–1198. (<a
href="https://doi.org/10.1142/S0218194020500308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fixed Sized Candidate Set (FSCS) is the first of a series of methods proposed to enhance the effectiveness of random testing (RT) referred to as Adaptive Random Testing methods or ARTs. Since its inception, test case generation overheads have been a major drawback to the success of ART. In FSCS, the bulk of this cost is embedded in distance computations between a set of randomly generated candidate test cases and previously executed but unsuccessful test cases. Consequently, FSCS is caught in a logical trap of probing the distances between every candidate and all executed test cases before the best candidate is determined. Using data mining, however, we discovered that about 50% of all valid test cases are encountered much earlier in the distance computations process but without any benefit of a hindsight, FSCS is unable to validate them; a wild goose chase. This paper then uses this information to propose a new strategy that predictively and proactively selects valid candidates anywhere during the distance computation process without vetting every candidate. Theoretical analysis, simulations and experimental studies conducted led to a similar conclusion: 25% of the distance computations are wasteful and can be discarded without any repercussion on effectiveness.},
  archive      = {J_IJSEKE},
  author       = {Michael Omari and Jinfu Chen and Robert French-Baidoo and Yunting Sun},
  doi          = {10.1142/S0218194020500308},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {8},
  pages        = {1169-1198},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A proactive approach to test case selection — an efficient implementation of adaptive random testing},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A revisit of metrics for test case prioritization problems.
<em>IJSEKE</em>, <em>30</em>(8), 1139–1167. (<a
href="https://doi.org/10.1142/S0218194020500291">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the test case prioritization problems, the average percent of faults detected (APFD) and its variant versions are widely used as metrics to evaluate prioritized test suite’s efficiency of fault detection. By a revisit of metrics for test case prioritization, we observe that APFD is only available for the scenarios where all test suites under evaluation contain the same number of test cases. Such a limitation is often overlooked, and lead to incorrect results when comparing fault detection efficiency of test suites with different sizes. Moreover, APFD cannot precisely illustrate the process of fault detection in the real world. Besides the APFD, most of its variants, including the NAPFD and the APFD C W , have similar problems. This paper points out these limitations in detail by analyzing the physical explanation of APFD series metrics formally. In order to eliminate these limitations, we propose a series of improved metrics, including the relative average percent of faults detected (RAPFD) and the relative cost-cognizant weighted average percent of faults detected (RAPFD C W ), to evaluate the efficiency of the test suite. Furthermore, for the scenario of parallel testing, a series of metrics including the relative average percent of faults detected in parallel testing ( P -RAPFD) and the relative cost-cognizant weighted average percent of faults detected in parallel testing ( P -RAPFD C W ) are proposed too. All the proposed metrics refer to both the speed of fault detection and the constraint of the testing resource. A formal analysis and some examples show that all the proposed metrics provide much more precise illustrations of the fault detection process.},
  archive      = {J_IJSEKE},
  author       = {Ziyuan Wang and Chunrong Fang and Lin Chen and Zhiyi Zhang},
  doi          = {10.1142/S0218194020500291},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {8},
  pages        = {1139-1167},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A revisit of metrics for test case prioritization problems},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Investigating the relationship between mutants and real
faults with respect to mutated code. <em>IJSEKE</em>, <em>30</em>(8),
1119–1137. (<a href="https://doi.org/10.1142/S021819402050028X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mutation testing aims to evaluate the fault detection capability of a test suite. This evaluation substitutes faults with mutants by transforming program code to be defective. Evidences of the relationship between the detection rates of mutants and real faults have supported the use of mutants. It has also been known that the test suite size was a significant factor affecting the relationship. Our study revealed that the selection of the mutated code was another factor affecting the relationship. We generated mutants by transforming the code modified to fix defects, while the modified code was located at three granularity levels. The experiments conducted on the defects4j dataset demonstrated that the granularity level caused a significant difference in the relationship; the detection rate of mutants was more strongly correlated with and more indicative of the fault detection capability at a fine level than at a coarse level. Moreover, the influence of the test suite size was different at each granularity level. These findings implied a strong correlation between the detection rates of mutants and real faults, independently of test suite size, when the error-prone code was located precisely.},
  archive      = {J_IJSEKE},
  author       = {Mingwan Kim and Neunghoe Kim and Hoh Peter In},
  doi          = {10.1142/S021819402050028X},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {8},
  pages        = {1119-1137},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Investigating the relationship between mutants and real faults with respect to mutated code},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two improved topic word detection algorithms.
<em>IJSEKE</em>, <em>30</em>(8), 1097–1118. (<a
href="https://doi.org/10.1142/S0218194020400173">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Topic word extraction is the task of identifying single or multi-word expressions that represent the main topics of a document. In this paper, two improved algorithms for extracting and discovering topic words are proposed in the Rapid Topic word Detection (RTD) Algorithm and CategoryTextRank (CTextRank) Algorithm, which can effectively obtain information by extracting and filtering the topic words in the text. The algorithms overcome the shortcomings of traditional topic words discovering algorithms that require deep linguistic knowledge, domain or language specific annotated corpora. The two algorithms we proposed can process both short and long text. The biggest advantage of the algorithms is that they are unsupervised machine learning algorithms. They need not be trained to process text directly to get topic words. The Accuracy rate, recall rate and F-measure index have been greatly improved when using the two algorithms which show that the results obtained compare favorably with previously published results on datasets Inspec and SemEval. The first algorithm Rapid Topicword Detection improves the metrics compared to PositionRank and TextRank, the second algorithm CategoryTextRank improves the metrics compared to TextRank, SingleRank and TF-IDF.},
  archive      = {J_IJSEKE},
  author       = {Zehao Yu},
  doi          = {10.1142/S0218194020400173},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {8},
  pages        = {1097-1118},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Two improved topic word detection algorithms},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scalable influence maximization meets efficiency and
effectiveness in large-scale social networks. <em>IJSEKE</em>,
<em>30</em>(8), 1079–1096. (<a
href="https://doi.org/10.1142/S0218194020400161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Influence maximization is a problem that aims to select top k influential nodes to maximize the spread of influence in social networks. The classical greedy-based algorithms and their improvements are relatively slow or not scalable. The efficiency of heuristic algorithms is fast but their accuracy is unacceptable. Some algorithms improve the accuracy and efficiency by consuming a large amount of memory usage. To overcome the above shortcoming, this paper proposes a fast and scalable algorithm for influence maximization, called K-paths, which utilizes the influence tree to estimate the influence spread. Additionally, extensive experiments demonstrate that the K-paths algorithm outperforms the comparison algorithms in terms of efficiency while keeping competitive accuracy.},
  archive      = {J_IJSEKE},
  author       = {Liqing Qiu and Shuang Zhang and Chunmei Gu and Xiangbo Tian},
  doi          = {10.1142/S0218194020400161},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {8},
  pages        = {1079-1096},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Scalable influence maximization meets efficiency and effectiveness in large-scale social networks},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A new semantic-based multi-level classification approach for
activity recognition using smartphones. <em>IJSEKE</em>, <em>30</em>(8),
1051–1078. (<a href="https://doi.org/10.1142/S021819402040015X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the problem of recognizing the semantic human activities through the analysis of large dataset collected from users’ sensor-based smartphones. Our approach is unique in terms of covering a large number of activities that users could possibly engage in, and considering the multi-level-based classification model. Our model has three properties that never seemed to be addressed by existing approaches dealing with the same problem. These are: (1) comprehensiveness — in terms of the activity set, (2) accuracy — in terms of the activity classification, and (3) applicability — in terms of flexibility in being applied in real-life settings. Current approaches do not tackle all these properties. When tested on realistic dataset, our multi-level-based model achieved promising results despite the large number of activities being considered. When compared to similar approaches, our approach achieved comparable results in terms of accuracy and outperformed them in terms of the activity types, environment and settings covered, comprehensiveness, and applicability.},
  archive      = {J_IJSEKE},
  author       = {Ghassen Ben Brahim and Wassim El-Hajj and Cynthia El-Hayek and Hazem Hajj},
  doi          = {10.1142/S021819402040015X},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {8},
  pages        = {1051-1078},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A new semantic-based multi-level classification approach for activity recognition using smartphones},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020a). Guest editor’s introduction. <em>IJSEKE</em>,
<em>30</em>(8), 1049–1050. (<a
href="https://doi.org/10.1142/S0218194020020039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJSEKE},
  author       = {S. K. Chang},
  doi          = {10.1142/S0218194020020039},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {8},
  pages        = {1049-1050},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Guest editor’s introduction},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An automated hybrid approach for generating requirements
trace links. <em>IJSEKE</em>, <em>30</em>(7), 1005–1048. (<a
href="https://doi.org/10.1142/S0218194020500278">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Trace links between requirements and software artifacts provide available traceability information and in-depth insights for different stakeholders. Unfortunately, establishing requirements trace links is a tedious, labor-intensive and fallible task. To alleviate this problem, Information Retrieval (IR) methods, such as Vector Space Model (VSM), Latent Semantic Indexing (LSI), and their variants, have been widely used to establish trace links automatically. But with the widespread use of agile development methodology, artifacts that can be used to generate automatic tracing links are getting shorter and shorter, which decreases the effects of traditional IR-based trace link generation methods. In this paper, Biterm Topic Model–Genetic Algorithm (BTM–GA), which is effective in managing short-text artifacts and configuring initial parameters, is introduced. A hybrid method VSM + BTM–GA is proposed to generate requirements trace links. Empirical experiments conducted on five real and frequently-used datasets indicate that (1) the hybrid method VSM+BTM − GA outperforms the others, and its results can achieve the “Good” level, where recall and precision are no less than 70% and 30%, respectively; (2) the performance of the hybrid method is stable and (3) BTM–GA can provide a number of “hard-to-find” trace links that complement the candidate trace links of VSM.},
  archive      = {J_IJSEKE},
  author       = {Bangchao Wang and Rong Peng and Zhuo Wang and Xiaomin Wang and Yuanbang Li},
  doi          = {10.1142/S0218194020500278},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {7},
  pages        = {1005-1048},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {An automated hybrid approach for generating requirements trace links},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Impact analysis about response time considering deployment
change of SaaS software. <em>IJSEKE</em>, <em>30</em>(7), 977–1004. (<a
href="https://doi.org/10.1142/S0218194020500266">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deployment change of SaaS (Software as a Service) software will influence its response time, which is an important performance metric. Therefore, studying the impact of deployment change on the response time of SaaS software could contribute to performance improvement of the software. However, there are few performance analysis methods which can directly analyze the relationship between deployment change and response time of SaaS software. In this paper, we propose an approach which provides the impact analysis of specific deployment change operations on response time of SaaS software explicitly. Specifically, we present an evaluation method for the response time of SaaS software in specific deployment scheme by leveraging queueing theory. With mathematical derivation based on the proposed evaluation method, we qualitatively analyze the variation trend of response time with respect to deployment change. Furthermore, we study the relationship between two specific types of deployment change operations and response time variation of SaaS software, which is used to propose a response time improvement method based on deployment change. Finally, the effectiveness of the analysis conclusions and the proposed method in this paper is validated by practical cases, which indicates that adjusting deployment scheme according to the conclusions obtained in this paper can be helpful in improving the response time of SaaS software.},
  archive      = {J_IJSEKE},
  author       = {Bo Dong and Shi Ying and Lin Li and Hui Luo and Zhe Yang},
  doi          = {10.1142/S0218194020500266},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {7},
  pages        = {977-1004},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Impact analysis about response time considering deployment change of SaaS software},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The turnout abnormality diagnosis based on semi-supervised
learning method. <em>IJSEKE</em>, <em>30</em>(7), 961–976. (<a
href="https://doi.org/10.1142/S0218194020400148">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In China, the turnout abnormality very easily causes traffic accident or affects the efficiency due to the operating environment of railway transportation. The existing monitoring means are relatively backward, and mature automatic diagnosis method is lacking. In this study, a method based on semi-supervised learning algorithm for abnormal state diagnosis of turnout action curve is proposed, which is used to analyze and extract the electrical characteristics of the turnout by using the turnout action curve and the static and dynamic properties collected by the railway centralized monitoring system. The support vector machine model is used to construct the initial classifier with a small number of labeled samples, and the labeled samples are expanded from a large number of unlabeled samples. The switch curve is analyzed and diagnosed by using unlabeled data with a small amount of labeled data. The experimental results show that the method can automatically diagnose turnout electrical characteristics with high accuracy. While the cost of this method is relatively low compared with supervised learning, it can achieve higher accuracy and improve the practicability of fault diagnosis of turnout.},
  archive      = {J_IJSEKE},
  author       = {Zeng Shu Shi and Yiman Du and Tao Du and Guochao Shan},
  doi          = {10.1142/S0218194020400148},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {7},
  pages        = {961-976},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {The turnout abnormality diagnosis based on semi-supervised learning method},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An improved faster r-CNN for UAV-based catenary support
device inspection. <em>IJSEKE</em>, <em>30</em>(7), 941–959. (<a
href="https://doi.org/10.1142/S0218194020400136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The catenary support device inspection is of crucial importance for ensuring safety and reliability of railway systems. At present, visual detection tasks of catenary support devices defect are performed by trained personnel based on the images taken periodically by industrial cameras installed on inspection vehicle in a limited period of time at midnight. However, the inspection mean is inappropriate for low efficiency and high cost. This paper presents a novel network based on unmanned aerial vehicle (UAV) images for catenary support device inspection and focuses on small object detection and the imbalanced dataset. With regards to the first aspect, based on a pyramid network structure, the improved Faster R-CNN consists of a top-down-top feature pyramid fusion structure, which heavily fuses high-level semantic information and low-level detail information. The feature map fusions of three different pooling scales are employed for improving detection accuracy of predicted bounding boxes. With regards to the second, we copy and paste the small proportion objects of dataset for avoiding category imbalance. Finally, quantitative and qualitative evaluations illustrate that the improved Faster-RCNN achieves better performance over the classic methods, yet remains convenient and efficient.},
  archive      = {J_IJSEKE},
  author       = {Jiahao Liu and Zhipeng Wang and Yunpeng Wu and Yong Qin and Xianbin Cao and Yonghui Huang},
  doi          = {10.1142/S0218194020400136},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {7},
  pages        = {941-959},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {An improved faster R-CNN for UAV-based catenary support device inspection},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A data-driven two-stage prediction model for train
primary-delay recovery time. <em>IJSEKE</em>, <em>30</em>(7), 921–940.
(<a href="https://doi.org/10.1142/S0218194020400124">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurate prediction of train delay recovery is critical for railway incident management and providing passengers with accurate journey time. In this paper, a two-stage prediction model is proposed to predict the recovery time of train primary-delay based on the real records from High-Speed Railway (HSR). In Stage 1, two models are built to study the influence of feature space and model framework on the prediction accuracy of buffer time in each section or station. It is found that explicitly inputting the attribute features of stations and sections to the model, instead of implicit simulation, will improve the prediction accuracy effectively. For validation purpose, the proposed model has been compared with several alternative models, namely, Logistic Regression (LR), Artificial Neutral Network (ANN), Support Vector Machine (SVM) and Gradient Boosting Tree (GBT). The results show that its remarkable performance is better than other schemes. Specifically, when the error is extended to 3 min, the proposed model can achieve up to the accuracy of 94.63%. It proves that our method has high value in practical engineering application. Considering the delay propagation of trains is a complex process, our future study will focus on building delay propagation knowledge base and dispatcher experience knowledge base.},
  archive      = {J_IJSEKE},
  author       = {Bowen Gao and Dongxiu Ou and Decun Dong and Yusen Wu},
  doi          = {10.1142/S0218194020400124},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {7},
  pages        = {921-940},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A data-driven two-stage prediction model for train primary-delay recovery time},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An integration of train timetabling, platforming and
routing-based cooperative adjustment methodology for dealing with train
delay. <em>IJSEKE</em>, <em>30</em>(7), 901–919. (<a
href="https://doi.org/10.1142/S0218194020400112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Train delay is a serious issue that can spread rapidly in the railway network leading to further delay of other trains and detention of passengers in stations. However, the current practice in the event of the trail delay usually depends on train dispatcher’s experience, which cannot manage train operation effectively and may have safety risks. The application of intelligent railway monitor and control system can improve train operation management while increasing railway safety. This paper presents a methodology in which train timetabling, platforming and routing models are combined by studying the real-time adjustment and optimization of high-speed railway in the case of the train delay in order to produce a cooperative adjustment algorithm so that the train operation adjustment plan can be obtained. MATLAB computer programs have been developed based on the proposed methodology and adjustment criteria have been established from knowledge data bases in order to calculate optimized solutions. A case study is used to demonstrate the proposed methodology. The results show that the proposed method can quickly adjust the train operation plan in the case of the train delay, restore the normal train operation order, and reduce the impact of train delay on railway network effectively and efficiently.},
  archive      = {J_IJSEKE},
  author       = {Yinggui Zhang and Zengru Chen and Min An and Aliyu Mani Umar},
  doi          = {10.1142/S0218194020400112},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {7},
  pages        = {901-919},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {An integration of train timetabling, platforming and routing-based cooperative adjustment methodology for dealing with train delay},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Software agent-centric semantic social network for
cyber-physical interaction and collaboration. <em>IJSEKE</em>,
<em>30</em>(6), 859–893. (<a
href="https://doi.org/10.1142/S0218194020400100">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Considerable research has recently focused on integrating cyber-physical systems in a social context. However, several challenges remain concerning appropriate methodologies, frameworks and techniques for supporting socio-cyber-physical collaboration. Existing systems do not recognize how cyber-physical resources can be socially connected so that they interact in collaborative decision-making like humans. Furthermore, the lack of semantic representations for heterogeneous cyber-social-collaborative networks limits integration, interoperability and knowledge discovery from their underlying data sources. Semantic Web ontology models can help to overcome this limitation by semantically describing and interconnecting cyber-physical objects and human participants in a social space. This research addresses the establishment of both cyber-physical and human relationships and their interactions within a social-collaborative network. We discuss how nonhuman resources can be represented as socially connected nodes and utilized by software agents. A software agent-centric Semantic Social-Collaborative Network (SSCN) is then presented that provides functionality to represent and manage cyber-physical resources in a social network. It is supported by an extended ontology model for semantically describing human and nonhuman resources and their social interactions. A software agent has been implemented to perform some actions on behalf of the nonhuman resources to achieve cyber-physical collaboration. It is demonstrated within a real-world decision support system, GRiST (www.egrist.org), used by mental-health services in the UK.},
  archive      = {J_IJSEKE},
  author       = {Nazmul Hussain and Hai H. Wang and Christopher D. Buckingham and Xiaoyuan Zhang},
  doi          = {10.1142/S0218194020400100},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {6},
  pages        = {859-893},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Software agent-centric semantic social network for cyber-physical interaction and collaboration},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semantic restful service composition using task
specification. <em>IJSEKE</em>, <em>30</em>(6), 835–857. (<a
href="https://doi.org/10.1142/S0218194020400094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing Web API search engines allow only category-based browsing and keyword- or tag-based searches for RESTful services. In other words, they do not enable the discovery or composition of real-world RESTful services by application developers. This paper outlines a novel scheme, called Transformation–Annotation–Discovery (TAD), which transforms OpenAPI (Swagger) documents related to RESTful services into a graph structure and then automatically annotates the semantic concepts on graph nodes using Latent Dirichlet Allocation (LDA) and WordNet. TAD can then be used for service composition based on the user requirements specified in two modules: a service discovery chain and logical-operation-based composition. The service discovery chain uses the Hungarian algorithm to assess service interface compatibility in order to facilitate the retrieval of services capable of bridging the gap between specified user requirements and the discovered services. The logical-operation-based composition module identifies services that semantically fit the user requirements, based on the structure of the service flow. Those candidate services are then sent to service discovery chains to enable the simultaneous search for potential composition solutions. System prototype and experiment results demonstrate the feasibility and efficacy of the proposed scheme.},
  archive      = {J_IJSEKE},
  author       = {Shang-Pin Ma and Hsuan-Ju Lin and Ming-Jen Hsu},
  doi          = {10.1142/S0218194020400094},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {6},
  pages        = {835-857},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Semantic restful service composition using task specification},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semantics-driven programming of self-adaptive reactive
systems. <em>IJSEKE</em>, <em>30</em>(6), 805–834. (<a
href="https://doi.org/10.1142/S0218194020400082">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, new classes of highly dynamic, complex systems are gaining momentum. These classes include, but are not limited to IoT, smart cities, cyber-physical systems and sensor networks. These systems are characterized by the need to express behaviors driven by external and/or internal changes, i.e. they are reactive and context-aware. A desirable design feature of these systems is the ability of adapting their behavior to environment changes. In this paper, we propose an approach to support adaptive, reactive systems based on semantic runtime representations of their context, enabling the selection of equivalent behaviors, i.e. behaviors that have the same effect on the environment. The context representation and the related knowledge are managed by an engine designed according to a reference architecture and programmable through a declarative definition of sensors and actuators. The knowledge base of sensors and actuators (hosted by an RDF triplestore) is bound to the real world by grounding semantic elements to physical devices via REST APIs. The proposed architecture along with the defined ontology tries to address the main problems of dynamically re-configurable systems by exploiting a declarative, queryable approach to enable runtime reconfiguration with the help of (a) semantics to support discovery in heterogeneous environment, (b) composition logic to define alternative behaviors for variation points, (c) bi-causal connection life-cycle to avoid dangling links with the external environment. The proposal is validated in a case study aimed at designing an edge node for smart buildings dedicated to cultural heritage preservation.},
  archive      = {J_IJSEKE},
  author       = {Ester Giallonardo and Francesco Poggi and Davide Rossi and Eugenio Zimeo},
  doi          = {10.1142/S0218194020400082},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {6},
  pages        = {805-834},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Semantics-driven programming of self-adaptive reactive systems},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Exploiting declarative mapping rules for generating GraphQL
servers with morph-GraphQL. <em>IJSEKE</em>, <em>30</em>(6), 785–803.
(<a href="https://doi.org/10.1142/S0218194020400070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the last decade, REST has become the most common approach to provide web services, yet it was not originally designed to handle typical modern applications (e.g. mobile apps). GraphQL was proposed to reduce the number of queries and data exchanged in comparison with REST. Since its release in 2015, it has gained momentum as an alternative approach to REST. However, generating and maintaining GraphQL resolvers is not a simple task. First, a domain expert has to analyze a dataset, design the corresponding GraphQL schema and map the dataset to the schema. Then, a software engineer (e.g. GraphQL developer) implements the corresponding GraphQL resolvers in a specific programming language. In this paper, we present an approach to exploit the information from mappings rules (relation between target and source schema) and generate a GraphQL server. These mapping rules construct a virtual knowledge graph which is accessed by the generated GraphQL resolvers. These resolvers translate the input GraphQL queries into the queries supported by the underlying dataset. Domain experts or software developers may benefit from our approach: a domain expert does not need to involve software developers to implement the resolvers, and software developers can generate the initial version of the resolvers to be implemented. We implemented our approach in the Morph-GraphQL framework and evaluated it using the LinGBM benchmark.},
  archive      = {J_IJSEKE},
  author       = {David Chaves-Fraga and Freddy Priyatna and Ahmad Alobaid and Oscar Corcho},
  doi          = {10.1142/S0218194020400070},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {6},
  pages        = {785-803},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Exploiting declarative mapping rules for generating GraphQL servers with morph-GraphQL},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semantic service search in IT crowdsourcing platform: A
knowledge graph-based approach. <em>IJSEKE</em>, <em>30</em>(6),
765–783. (<a href="https://doi.org/10.1142/S0218194020400069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding user’s search intent in vertical websites like IT service crowdsourcing platform relies heavily on domain knowledge. Meanwhile, searching for services accurately on crowdsourcing platforms is still difficult, because these platforms do not contain enough information to support high-performance search. To solve these problems, we build and leverage a knowledge graph named ITServiceKG to enhance search performance of crowdsourcing IT services. The main ideas are to (1) build an IT service knowledge graph from Wikipedia, Baidupedia, CN-DBpedia, StuQ and data in IT service crowdsourcing platforms, (2) use properties and relations of entities in the knowledge graph to expand user query and service information, and (3) apply a listwise approach with relevance features and topic features to re-rank the search results. The results of our experiments indicate that our approach outperforms the traditional search approaches.},
  archive      = {J_IJSEKE},
  author       = {Qinyue Wu and Duankang Fu and Beijun Shen and Yuting Chen},
  doi          = {10.1142/S0218194020400069},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {6},
  pages        = {765-783},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Semantic service search in IT crowdsourcing platform: A knowledge graph-based approach},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Integrated formal tools for software architecture smell
detection. <em>IJSEKE</em>, <em>30</em>(6), 723–763. (<a
href="https://doi.org/10.1142/S0218194020400057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The architecture smells are the poor design practices applied to the software architecture design. The smells in software architecture design can be cascaded to cause the issues in the system implementation and significantly affect the maintainability and reliability attribute of the software system. The prevention of architecture smells at the design phase can therefore improve the overall quality of the software system. This paper presents a framework that supports the detection of architecture smells based on the formalization of architecture design. Our modeling specification supports representing both structural and behavioral aspect of software architecture design; it allows the smells to be analyzed and detected with the provided tools. Our framework has been applied to seven architecture smells that violate different design principles. The evaluation has been conducted and the result shows that our detection approach gives accurate results and performs well on different size of models. With the proposed framework, other architecture smells can be defined and detected using the process and tools presented in this paper.},
  archive      = {J_IJSEKE},
  author       = {Nacha Chondamrongkul and Jing Sun and Ian Warren and Scott Uk-Jin Lee},
  doi          = {10.1142/S0218194020400057},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {6},
  pages        = {723-763},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Integrated formal tools for software architecture smell detection},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Guest editor’s introduction. <em>IJSEKE</em>,
<em>30</em>(6), 721. (<a
href="https://doi.org/10.1142/S0218194020020027">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJSEKE},
  author       = {Jing Sun},
  doi          = {10.1142/S0218194020020027},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {6},
  pages        = {721},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Guest editor’s introduction},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Software crucial functions ranking and detection in dynamic
execution sequence patterns. <em>IJSEKE</em>, <em>30</em>(5), 695–719.
(<a href="https://doi.org/10.1142/S0218194020500254">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Because of the sequence and number of calls of functions, software network cannot reflect the real execution of software. Thus, to detect crucial functions (DCF) based on software network is controversial. To address this issue, from the viewpoint of software dynamic execution, a novel approach to DCF is proposed in this paper. It firstly models, the dynamic execution process as an execution sequence by taking functions as nodes and tracing the stack changes occurring. Second, an algorithm for deleting repetitive patterns is designed to simplify execution sequence and construct software sequence pattern sets. Third, the crucial function detection algorithm is presented to identify the distribution law of the numbers of patterns at different levels and rank those functions so as to generate a decision-function-ranking-list (DFRL) by occurrence times. Finally, top-k discriminative functions in DFRL are chosen as crucial functions, and similarity the index of decision function sets is set up. Comparing with the results from Degree Centrality Ranking and Betweenness Centrality Ranking approaches, our approach can increase the node coverage to 80%, which is proven to be an effective and accurate one by combining advantages of the two classic algorithms in the experiments of different test cases on four open source software. The monitoring and protection on crucial functions can help increase the efficiency of software testing, strength software reliability and reduce software costs.},
  archive      = {J_IJSEKE},
  author       = {Bing Zhang and Chun Shan and Munawar Hussain and Jiadong Ren and Guoyan Huang},
  doi          = {10.1142/S0218194020500254},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {5},
  pages        = {695-719},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Software crucial functions ranking and detection in dynamic execution sequence patterns},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Concurrent bug finding based on bounded model checking.
<em>IJSEKE</em>, <em>30</em>(5), 669–694. (<a
href="https://doi.org/10.1142/S0218194020500242">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Automated and reliable software verification is of crucial importance for development of high-quality software. Formal methods can be used for finding different kinds of bugs without executing the software, for example, for finding possible run-time errors. The methods like model checking and symbolic execution offer very precise static analysis but on real world programs do not always scale well. One way to tackle the scalability problem is to apply new concurrent and sequential approaches to complex algorithms used in these kinds of software analysis. In this paper, we compare different variants of bounded model checking and propose two concurrent approaches: concurrency of intra-procedural analysis and concurrency of inter-procedural analysis. We implemented these approaches in a software verification tool LAV, a tool that is based on bounded model checking and symbolic execution. For assessing the improvements gained, we experimentally compared the concurrent approaches with the standard bounded model checking approach (where all correctness conditions are put into a single compound formula) and with a sequential approach (where correctness conditions are checked separately, one after the other). The results show that, in many cases, the proposed concurrent approaches give significant improvements.},
  archive      = {J_IJSEKE},
  author       = {Milena Vujošević Janičić},
  doi          = {10.1142/S0218194020500242},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {5},
  pages        = {669-694},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Concurrent bug finding based on bounded model checking},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep transfer learning for source code modeling.
<em>IJSEKE</em>, <em>30</em>(5), 649–668. (<a
href="https://doi.org/10.1142/S0218194020500230">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In recent years, deep learning models have shown great potential in source code modeling and analysis. Generally, deep learning-based approaches are problem-specific and data-hungry. A challenging issue of these approaches is that they require training from scratch for a different related problem. In this work, we propose a transfer learning-based approach that significantly improves the performance of deep learning-based source code models. In contrast to traditional learning paradigms, transfer learning can transfer the knowledge learned in solving one problem into another related problem. First, we present two recurrent neural network-based models RNN and GRU for the purpose of transfer learning in the domain of source code modeling. Next, via transfer learning, these pre-trained (RNN and GRU) models are used as feature extractors. Then, these extracted features are combined into attention learner for different downstream tasks. The attention learner leverages from the learned knowledge of pre-trained models and fine-tunes them for a specific downstream task. We evaluate the performance of the proposed approach with extensive experiments with the source code suggestion task. The results indicate that the proposed approach outperforms the state-of-the-art models in terms of accuracy, precision, recall and F -measure without training the models from scratch.},
  archive      = {J_IJSEKE},
  author       = {Yasir Hussain and Zhiqiu Huang and Yu Zhou and Senzhang Wang},
  doi          = {10.1142/S0218194020500230},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {5},
  pages        = {649-668},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Deep transfer learning for source code modeling},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Categorization of multiple documents using fuzzy overlapping
clustering based on formal concept analysis. <em>IJSEKE</em>,
<em>30</em>(5), 631–647. (<a
href="https://doi.org/10.1142/S0218194020500229">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most clustering algorithms build disjoint clusters. However, clusters might be overlapped because documents may belong to two or more categories in the real world. For example, a paper discussing the Apple Watch may be categorized into either 3C, Fashion, or even Clothing and Shoes. Therefore, overlapping clustering algorithms have been studied such that a resource can be assigned to one or more clusters. Formal Concept Analysis (FCA), which has many practical applications in information science, has been used in disjoin clustering, but has not been studied in overlapping clustering. To make overlapping clustering possible by using FCA, we propose an approach, including two types of transformation. From the experimental results, it shows that the proposed fuzzy overlapping clustering performed more efficiently than existing overlapping clustering methods. The positive results confirm the feasibility of the proposed scheme used in overlapping clustering. Also, it can be used in applications such as recommendation systems.},
  archive      = {J_IJSEKE},
  author       = {Yi-Hui Chen and Eric Jui-Lin Lu and Ya-Wen Cheng},
  doi          = {10.1142/S0218194020500229},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {5},
  pages        = {631-647},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Categorization of multiple documents using fuzzy overlapping clustering based on formal concept analysis},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An approach to software maintenance: A case study in small
and medium-sized businesses IT organizations. <em>IJSEKE</em>,
<em>30</em>(5), 603–630. (<a
href="https://doi.org/10.1142/S0218194020500217">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software maintenance is the task of modifying a running product previously delivered to the client, in order to correct defects, improve performance or adapt it to the environment. This task is a crucial activity for enterprises. Without it, existing systems would become rapidly out-of-date and inefficient. The purpose of this paper is to present a software maintenance approach used in small and medium-sized business (SMB) organizations in Brazil. Currently, these organizations represent 95.5% of the software companies in the country. The approach presented here indicates how SMB IT companies have improved their software maintenance processes. Multiple case studies were performed to validate this approach. The outcomes showed that strategies associated with managing users’ knowledge and development/maintenance teams are relevant to increase the maintenance process effectiveness. This approach involves three aspects: users’ knowledge management, maintenance team knowledge and the management and maintenance process. This improvement includes reducing time and also minimizing the number of tickets. The response time for tickets resolution to the end user has been reduced. In addition, IT organizations have minimized the effects associated with both staff and client turnovers.},
  archive      = {J_IJSEKE},
  author       = {Alexandre L’Erario and Hellen Christine Seródio Thomazinho and José Augusto Fabri},
  doi          = {10.1142/S0218194020500217},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {5},
  pages        = {603-630},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {An approach to software maintenance: A case study in small and medium-sized businesses IT organizations},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An automated approach for constructing framework
instantiation documentation. <em>IJSEKE</em>, <em>30</em>(4), 575–601.
(<a href="https://doi.org/10.1142/S0218194020500205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A substantial effort, in general, is required for understanding APIs of application frameworks. High-quality API documentation may alleviate the effort, but the production of such documentation still poses a major challenge for modern frameworks. To facilitate the production of framework instantiation documentation, we hypothesize that the framework code itself and the code of existing instantiations provide useful information. However, given the size and complexity of existent code, automated approaches are required to assist the documentation production. Our goal is to assess an automated approach for constructing relevant documentation for framework instantiation based on source code analysis of the framework itself and of existing instantiations. The criterion for defining whether documentation is relevant would be to compare the documentation with an traditional framework documentation, considering the time spent and correctness during instantiation activities, information usefulness, complexity of the activity, navigation, satisfaction, information localization and clarity. We propose an automated approach for constructing relevant documentation for framework instantiation based on source code analysis of the framework itself and of existing instantiations. The proposed approach generates documentation in a cookbook style, where the recipes are programming activities using the necessary API elements driven by the framework features. We performed an empirical study, consisting of three experiments with 44 human subjects executing real framework instantiations aimed at comparing the use of the proposed cookbooks to traditional manual framework documentation (baseline). Our empirical assessment shows that the generated cookbooks performed better or, at least, with non-significant difference when compared to the traditional documentation, evidencing the effectiveness of the approach.},
  archive      = {J_IJSEKE},
  author       = {Raquel Fialho de Queiroz Lafetá and Thiago Fialho de Queiroz Lafetá and Marcelo de Almeida Maia},
  doi          = {10.1142/S0218194020500205},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {575-601},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {An automated approach for constructing framework instantiation documentation},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Winning strategy tree construction for BDD-based ATL model
checkers. <em>IJSEKE</em>, <em>30</em>(4), 555–573. (<a
href="https://doi.org/10.1142/S0218194020500199">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Alternating-time Temporal Logic (ATL) is a temporal logic for formal verification of open systems, which supports selective quantification over paths. The open system can be represented as a game where the system and the environment pick their moves in turn or simultaneously. The ATL model checker Mocha has been developed by using Binary Decision Diagrams (BDDs), and successfully employed for open system verification. However, since Mocha symbolically manipulates a set of states by a BDD, it is hard to generate a winning strategy as a witness or a counter-example. In this paper, we propose a novel algorithm to efficiently construct a winning strategy tree and report that our technique has been successfully implemented on Mocha.},
  archive      = {J_IJSEKE},
  author       = {Wonhong Nam and Haejin Yang and Hyunyoung Kil},
  doi          = {10.1142/S0218194020500199},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {555-573},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Winning strategy tree construction for BDD-based ATL model checkers},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Smart street litter detection and classification based on
faster r-CNN and edge computing. <em>IJSEKE</em>, <em>30</em>(4),
537–553. (<a href="https://doi.org/10.1142/S0218194020400045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cleanliness of city streets has an important impact on city environment and public health. Conventional street cleaning methods involve street sweepers going to many spots and manually confirming if the street needs to be clean. However, this method takes a substantial amount of manual operations for detection and assessment of street’s cleanliness which leads to a high cost for cities. Using pervasive mobile devices and AI technology, it is now possible to develop smart edge-based service system for monitoring and detecting the cleanliness of streets at scale. This paper explores an important aspect of cities — how to automatically analyze street imagery to understand the level of street litter. A vehicle (i.e. trash truck) equipped with smart edge station and cameras is used to collect and process street images in real time. A deep learning model is developed to detect, classify and analyze the diverse types of street litters such as tree branches, leaves, bottles and so on. In addition, two case studies are reported to show its strong potential and effectiveness in smart city systems.},
  archive      = {J_IJSEKE},
  author       = {Ping Ping and Effendy Kumala and Jerry Gao and Guoyan Xu},
  doi          = {10.1142/S0218194020400045},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {537-553},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Smart street litter detection and classification based on faster R-CNN and edge computing},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An intelligent service middleware based on sensors in IoT
environments. <em>IJSEKE</em>, <em>30</em>(4), 523–536. (<a
href="https://doi.org/10.1142/S0218194020400033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Internet of Things (IoT) environment, a number of sensors and devices exist and they have various and heterogeneous characteristics. Applications which provide a variety of services based on the sensors also have different service requirements. Therefore, a middleware that is located between sensors and application systems is needed for integrating two layers. This paper proposes a general purpose middleware for providing intelligent services based on heterogeneous sensors existing in IoT environment. The proposed middleware acquires and manages sensing data in real time. The middleware stores and manages heterogeneous sensors, node, and network metadata. Especially, the middleware in this paper includes a component for providing intelligent services based on inferencing the situation based on ontologies and rules.},
  archive      = {J_IJSEKE},
  author       = {Jong-Hyun Park},
  doi          = {10.1142/S0218194020400033},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {523-536},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {An intelligent service middleware based on sensors in IoT environments},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Mutual inference model for user roles and urban functional
zones. <em>IJSEKE</em>, <em>30</em>(4), 503–521. (<a
href="https://doi.org/10.1142/S0218194020400021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Users engage in various activities in different regions of the city, how to infer their roles through the area of the user’s activity, and how to infer the possible geographical structure of the user’s role through the user’s role, it is the case of the challenging issues in the user’s mobility field. The necessary context elements are extracted from the user’s microblogging, such as user ID, check-ins and GPS coordinates, the mutual inference model of the user’s role and the urban functional zones is established for the user’s movement law. The experimental results show that the model has achieved good performance in terms of using the user role to infer the urban geographical structure, or in the use of the urban functional zones to infer the user roles.},
  archive      = {J_IJSEKE},
  author       = {Haitao Wang and Yanli Chen and Juan Wang and Hao Peng and Hongfang Liu},
  doi          = {10.1142/S0218194020400021},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {503-521},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Mutual inference model for user roles and urban functional zones},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A computation model for senior citizen health self-care.
<em>IJSEKE</em>, <em>30</em>(4), 483–501. (<a
href="https://doi.org/10.1142/S021819402040001X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A computation model is proposed for senior citizen health self-care through adaptive multi-level computation cycles according to the principles of slow intelligence. An experimental system, called the TDR system, is being implemented on a smart phone to serve as the test bed for the proposed approach. In this paper, we describe the computation model, the basic concepts and the prototype experimental system. The main theoretical concept is centered on adaptive multi-level computation cycles, events and event icons. Further experimental as well as theoretical investigations of the proposed computation model are discussed.},
  archive      = {J_IJSEKE},
  author       = {Shi-Kuo Chang and Han Zhong Zheng and Tian Yi Cui and Nannan Wen and Hao Zhai and Lu Zhang},
  doi          = {10.1142/S021819402040001X},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {483-501},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A computation model for senior citizen health self-care},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Guest editor’s introduction. <em>IJSEKE</em>,
<em>30</em>(4), 481–482. (<a
href="https://doi.org/10.1142/S0218194020020015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_IJSEKE},
  author       = {Jerry Gao},
  doi          = {10.1142/S0218194020020015},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {4},
  pages        = {481-482},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Guest editor’s introduction},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Towards generating realistic and high coverage test data for
constraint-based fault injection. <em>IJSEKE</em>, <em>30</em>(3),
451–479. (<a href="https://doi.org/10.1142/S0218194020500187">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Generating faulty data is a key issue in fault injection. The faulty data include not only the ones of extreme values or bad formats, but also the ones which are logically unreasonable. Constraint-based fault injection which negates interface constraints to solve faulty data is effective for logically unreasonable data generation. However, the existing constraint-based approaches just solve brand new data for testing. Such brand new data may easily violate some hidden environment constraints on the test inputs and hence be nonrealistic. Besides, there can be different strategies to negate a constraint in order to derive the constraint-unsatisfied faulty data. What are the possible negation strategies and which strategies are better for high coverage fault injection are still unclear. To these ends, this paper presents a new constraint-based fault injection approach. The approach introduces 10 different strategies for constraint negation and relaxes constraint variables to generate faulty data instead of solving brand new data for fault injection. It can produce faulty data which are closer to the original non-faulty ones and hence likely to be more realistic. We experimentally investigated the effectiveness and cost of the introduced constraint negation strategies. The results provide insights for the application of these strategies in fault injection.},
  archive      = {J_IJSEKE},
  author       = {Ju Qian and Yan Wang and Fusheng Lin and Changjian Li and Zhiyi Zhang and Xuefeng Yan},
  doi          = {10.1142/S0218194020500187},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {3},
  pages        = {451-479},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Towards generating realistic and high coverage test data for constraint-based fault injection},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). AppPerm analyzer: Malware detection system based on android
permissions and permission groups. <em>IJSEKE</em>, <em>30</em>(3),
427–450. (<a href="https://doi.org/10.1142/S0218194020500175">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Besides the applications aimed at increasing the efficiency of the Android mobile devices, also many malicious applications, millions of Android malware according to various security company reports, are being developed and uploaded into the application stores. In order to detect those applications, a malicious Android application detection system based on permission and permission groups namely, AppPerm Analyzer has been developed. The AppPerm Analyzer software extracts the manifest and code permissions of analyzed applications, creates duple and triple permission groups from them, calculates risk scores of these permissions and permission groups according to their usage rates in malicious and benign applications and calculates the total risk score of the analyzed application. After training the software with 7776 applications in total, it is tested with 1664 benign and 1664 malicious applications. In the tests, AppPerm Analyzer detected malicious applications with an accuracy of 96.19% at most. At this point, sensitivity (true-positive ratio) is 95.50% and specificity (true-negative ratio) is 96.88%. If a false-positive ratio up to 10% is accepted, the sensitivity increases to 99.04%.},
  archive      = {J_IJSEKE},
  author       = {İbrahim Alper Doğru and Murat Önder},
  doi          = {10.1142/S0218194020500175},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {3},
  pages        = {427-450},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {AppPerm analyzer: Malware detection system based on android permissions and permission groups},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Automated test input generation via model inference based on
user story and acceptance criteria for mobile application development.
<em>IJSEKE</em>, <em>30</em>(3), 399–425. (<a
href="https://doi.org/10.1142/S0218194020500163">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There has been observed explosive growth in the development of mobile applications (apps) for Android and iOS operating systems, which has led to the direct impact towards mobile app development. In order to design and propose quality-oriented apps, it is the primary responsibility of developers to devote time and sufficient efforts towards testing to make the apps bug-free and operational in the hands of end-users without any hiccup. Manual testing procedures take a prolonged amount of time in writing test cases, and in some cases, the full testing requirements are not met. Besides, the insufficient knowledge of tester also impacts the overall quality and bug-free apps. To overcome the obstacles of testing, we propose a new testing methodology cum tool called “AgileUATM” which works primarily towards white-box and black-box testing. To evaluate the validity of the proposed tool, we put the tool in a real-time operational environment concerning mobile test apps. By using this tool, all the acceptance criteria are determined via user stories. The testers/developers specify requirements with formal specifications based on programs properties, predicates, invariants, and constraints. The results show that the proposed tool generated effective and accurate test cases, test input. Meanwhile, expected output was also generated in a unified fashion from the user stories to meet acceptance criteria. The proposed solution also reduced the development time to identify test data as compared to manual Behavior-Driven Development (BDD) methodologies. This tool can support the developers to get a better idea about the required tests and able to translate the customer’s natural languages to computer languages as well. This paper fulfills an approach to suitably test mobile application development.},
  archive      = {J_IJSEKE},
  author       = {Duc-Man Nguyen and Quyet-Thang Huynh and Nhu-Hang Ha and Thanh-Hung Nguyen},
  doi          = {10.1142/S0218194020500163},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {3},
  pages        = {399-425},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Automated test input generation via model inference based on user story and acceptance criteria for mobile application development},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generation and application of constrained interaction test
suites using base forbidden tuples with a mixed neighborhood tabu
search. <em>IJSEKE</em>, <em>30</em>(3), 363–398. (<a
href="https://doi.org/10.1142/S0218194020500151">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To ensure the quality of current highly configurable software systems, intensive testing is needed to test all the configuration combinations and detect all the possible faults. This task becomes more challenging for most modern software systems when constraints are given for the configurations. Here, intensive testing is almost impossible, especially considering the additional computation required to resolve the constraints during the test generation process. In addition, this testing process is exhaustive and time-consuming. Combinatorial interaction strategies can systematically reduce the number of test cases to construct a minimal test suite without affecting the effectiveness of the tests. This paper presents a new efficient search-based strategy to generate constrained interaction test suites to cover all possible combinations. The paper also shows a new application of constrained interaction testing in software fault searches. The proposed strategy initially generates the set of all possible t - t u p l e combinations; then, it filters out the set by removing the forbidden t - t u p l e s using the Base Forbidden Tuple (BFT) approach. The strategy also utilizes a mixed neighborhood tabu search (TS) to construct optimal or near-optimal constrained test suites. The efficiency of the proposed method is evaluated through a comparison against two well-known state-of-the-art tools. The evaluation consists of three sets of experiments for 35 standard benchmarks. Additionally, the effectiveness and quality of the results are assessed using a real-world case study. Experimental results show that the proposed strategy outperforms one of the competitive strategies, ACTS, for approximately 83% of the benchmarks and achieves similar results to CASA for 65% of the benchmarks when the interaction strength is 2. For an interaction strength of 3, the proposed method outperforms other competitive strategies for approximately 60% and 42% of the benchmarks. The proposed strategy can also generate constrained interaction test suites for an interaction strength of 4, which is not possible for many strategies. The real-world case study shows that the generated test suites can effectively detect injected faults using mutation testing.},
  archive      = {J_IJSEKE},
  author       = {Imad H. Hasan and Bestoun S. Ahmed and Moayad Y. Potrus and Kamal Z. Zamli},
  doi          = {10.1142/S0218194020500151},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {3},
  pages        = {363-398},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Generation and application of constrained interaction test suites using base forbidden tuples with a mixed neighborhood tabu search},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Social network analysis in software development projects: A
systematic literature review. <em>IJSEKE</em>, <em>30</em>(3), 321–362.
(<a href="https://doi.org/10.1142/S021819402050014X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software development in project teams has become more and more complex, with increasing demands for information and decision making. Software development in projects also hugely depends on effective interaction between people, and human factors have been identified as key to successful software projects. Especially in this context, managing and analyzing social networks is highly important. The instrument of social network analysis (SNA) provides fine-grained methods for analyzing social networks in project teams, going beyond the traditional tools and techniques of project management. This paper examines the importance of the application of SNA in software development projects. We conducted a systematic literature review (SLR) of research on software development projects and social network data published between 1980 and 2019. We identified and analyzed 86 relevant studies, finding that research on software development projects spans the topics of project organization, communication management, knowledge management, version and configuration management, requirement management, and risk management. Further, we show that most studies focus on project organization and that the most common method used to gather social data relies on automated extraction from various software development repositories in the SNA context. Our paper contributes to the software development literature by providing a broad overview of published studies on the use of social networks in helping software development projects. Finally, we identify research opportunities and make suggestions for addressing existing research gaps.},
  archive      = {J_IJSEKE},
  author       = {Roland Robert Schreiber and Matthäus Paul Zylka},
  doi          = {10.1142/S021819402050014X},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {3},
  pages        = {321-362},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Social network analysis in software development projects: A systematic literature review},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Regression test case prioritization based on fixed size
candidate set ART algorithm. <em>IJSEKE</em>, <em>30</em>(3), 291–320.
(<a href="https://doi.org/10.1142/S0218194020500138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Regression testing is a very time-consuming and expensive testing activity. Many test case prioritization techniques have been proposed to speed up regression testing. Previous studies show that no one technique is always best. Random strategy, as the simplest strategy, is not always so bad. Particularly, when a test suite has higher fault detection capability, the strategy can generate a better result. Nevertheless, due to the randomness, the strategy is not always as satisfactory as expected. In this context, we present a test case prioritization approach using fixed size candidate set adaptive random testing algorithm to reduce the effect of randomness and improve fault detection effectiveness. The distance between pair-wise test cases is assessed by exclusive OR. We designed and conducted empirical studies on eight C programs to validate the effectiveness of the proposed approach. The experimental results, confirmed by a statistical analysis, indicate that the approach we proposed is more effective than random and the total greedy prioritization techniques in terms of fault detection effectiveness. Although the presented approach has comparable fault detection effectiveness to ART-based and the additional greedy techniques, the time cost is much lower. Consequently, the proposed approach is much more cost-effective.},
  archive      = {J_IJSEKE},
  author       = {Rongcun Wang and Zhengmin Li and Shujuan Jiang and Chuanqi Tao},
  doi          = {10.1142/S0218194020500138},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {3},
  pages        = {291-320},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Regression test case prioritization based on fixed size candidate set ART algorithm},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A model-based test case prioritization approach based on
fault urgency and severity. <em>IJSEKE</em>, <em>30</em>(2), 263–290.
(<a href="https://doi.org/10.1142/S0218194020500126">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the aggrandizement scale of software system, the number of test cases has grown explosively. Test case prioritization (TCP) has been widely used in software testing to effectively improve testing efficiency. However, traditional TCP methods are mostly based on software code and they are difficult to apply to model-based testing. Moreover, existing model-based TCP techniques often do not take the likely distribution of faults into consideration, yet software faults are not often equally distributed in the system, and test cases that cover more fault prone modules are more likely to reveal faults so that they should be run with a higher priority. Therefore, in this paper, we provide a TCP approach based on Hidden Markov Model (HMM), to detect faults as earlier as possible and reduce the cost of modification. This approach consists of the following main parts: (1) transforming the Unified Modeling Language (UML) sequence diagram to HMM; (2) estimating the fault urgency according to fault priority and probability; (3) estimating the fault severity by analyzing the weight of the state in the HMM; (4) generating test case priority from fault urgency and fault severity, then prioritizing test case. The proposed approach is implemented on unmanned aerial vehicles (UAV) flight control system to perform TCP. The experimental results show that our proposed TCP approach can effectively enhance the probability of earlier fault detection and improve the efficiency and stability as compared to other prioritization techniques, such as original prioritization, random prioritization, additional prioritization and EPS-UML.},
  archive      = {J_IJSEKE},
  author       = {Dan Wei and Qingying Sun and Xingqi Wang and Tianning Zhang and Bin Chen},
  doi          = {10.1142/S0218194020500126},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {2},
  pages        = {263-290},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A model-based test case prioritization approach based on fault urgency and severity},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Log-based anomaly detection with the improved k-nearest
neighbor. <em>IJSEKE</em>, <em>30</em>(2), 239–262. (<a
href="https://doi.org/10.1142/S0218194020500114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Logs play an important role in the maintenance of large-scale systems. The number of logs which indicate normal (normal logs) differs greatly from the number of logs that indicate anomalies (abnormal logs), and the two types of logs have certain differences. To automatically obtain faults by K-Nearest Neighbor (KNN) algorithm, an outlier detection method with high accuracy, is an effective way to detect anomalies from logs. However, logs have the characteristics of large scale and very uneven samples, which will affect the results of KNN algorithm on log-based anomaly detection. Thus, we propose an improved KNN algorithm-based method which uses the existing mean-shift clustering algorithm to efficiently select the training set from massive logs. Then we assign different weights to samples with different distances, which reduces the negative effect of unbalanced distribution of the log samples on the accuracy of KNN algorithm. By comparing experiments on log sets from five supercomputers, the results show that the method we proposed can be effectively applied to log-based anomaly detection, and the accuracy, recall rate and F measure with our method are higher than those of traditional keyword search method.},
  archive      = {J_IJSEKE},
  author       = {Bingming Wang and Shi Ying and Guoli Cheng and Rui Wang and Zhe Yang and Bo Dong},
  doi          = {10.1142/S0218194020500114},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {2},
  pages        = {239-262},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Log-based anomaly detection with the improved K-nearest neighbor},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Knowledge deduction and reuse application to the products’
design process. <em>IJSEKE</em>, <em>30</em>(2), 217–237. (<a
href="https://doi.org/10.1142/S0218194020500102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a framework for knowledge reuse and deduction in mechanical products design and development. The proposed system effectively exploits the capitalized and inferred knowledge. To this end, we settled up an ontology dealing with the design process of mechanical products such as “the car”. The ontology-based framework is supported by a software tool that brings an automatic and personalized assistance to correspondent actors using the deduction process. Indeed, the systems provides the relevant knowledge to the suitable users in order to facilitate their professional tasks considering their roles and collaboration. Experimental results have demonstrated the effectiveness of reusing knowledge during product development lifecycle.},
  archive      = {J_IJSEKE},
  author       = {Achraf Ben Miled and Rahma Dhaouadi and Romany F. Mansour},
  doi          = {10.1142/S0218194020500102},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {2},
  pages        = {217-237},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Knowledge deduction and reuse application to the products’ design process},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Case study investigation of the fault detection and error
locating effects of architecture-based software testing.
<em>IJSEKE</em>, <em>30</em>(2), 191–216. (<a
href="https://doi.org/10.1142/S0218194020500096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For software testing, it is well known that the architecture of a software system can be utilized to enhance testability, fault detection and error locating. However, how much and what effects architecture-based software testing has on software testing have been rarely studied. Thus, this paper undertakes case study investigation of the effects of architecture-based software testing specifically with respect to fault detection and error locating. Through comparing the outcomes with the conventional testing approaches that are not based on test architectures, we confirm the effectiveness of architecture-based software testing with respect to fault detection and error locating. The case studies show that using test architecture can improve fault detection rate by 44.1%–88.5% and reduce error locating time by 3%–65.2%, compared to the conventional testing that does not rely on test architecture. With regard to error locating, the scope of relevant components or statements was narrowed by leveraging test architecture for approximately 77% of the detected faults. We also show that architecture-based testing could provide a means of defining an exact oracle or oracles with range values. This study shows by way of case studies the extent to which architecture-based software testing can facilitate detecting certain types of faults and locating the errors that cause such faults. In addition, we discuss the contributing factors of architecture-based software testing which enable such enhancement in fault detection and error locating.},
  archive      = {J_IJSEKE},
  author       = {Jihyun Lee and Sungwon Kang},
  doi          = {10.1142/S0218194020500096},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {2},
  pages        = {191-216},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Case study investigation of the fault detection and error locating effects of architecture-based software testing},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Usability requirements extraction method from software
document. <em>IJSEKE</em>, <em>30</em>(2), 171–189. (<a
href="https://doi.org/10.1142/S0218194020500084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting the usability requirements are crucial during requirements review and requirements validation for different purposes. Usability requirements are hard to be determined until the real user has experienced the software. It is even more challenging when these usability requirements are documented in natural language, which has an inconsistent and unrestricted structure. Automated requirements extraction has been widely studied to facilitate the process of requirements checking. Nevertheless, the accuracy of requirements extraction method still can be improved. Thus, this paper has presented the usability keywords repository that followed the ISO 9126 and ISO 25010 usability category and has gone through the expert validation process. The usability requirement extraction method is moreover enhanced with extra procedures in conforming the usability requirement statement. First, each statement in the requirement document is checked if there is a keyword usability, whereby the keywords used must match with the usability keyword repository. In order to ensure that the selected statement is a usability requirement, the corresponding usability keyword position should be after the fixed auxiliary verbs. The performance of the improved usability requirements extraction method is then evaluated using precision, recall and accuracy.},
  archive      = {J_IJSEKE},
  author       = {Rohayanti Hassan and Noor Atikah Amira Fauzi and Shahreen Kasim and Herman Khalid Omer},
  doi          = {10.1142/S0218194020500084},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {2},
  pages        = {171-189},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Usability requirements extraction method from software document},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Building a credit scoring model based on data mining
approaches. <em>IJSEKE</em>, <em>30</em>(2), 147–169. (<a
href="https://doi.org/10.1142/S0218194020500072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, one of the biggest challenges in banking sector, certainly, is assessment of the client’s creditworthiness. In order to improve the decision-making process and risk management, banks resort to using data mining techniques for hidden patterns recognition within a wide data. The main objective of this study is to build a high-performance customized credit scoring model. The model named Reliable client is based on Bank’s real dataset and originally built by applying four different classification algorithms: decision tree (DT), naive Bayes (NB), generalized linear model (GLM) and support vector machine (SVM). Since it showed the greatest results, but also seemed as the most appropriate algorithm, the adopted model is based on GLM algorithm. The results of this model are presented based on many performance measures that showed great predictive confidence and accuracy, but we also demonstrated significant impact of data pre-processing on model performance. Statistical analysis of the model identified the most significant parameters on the model outcome. In the end, created credit scoring model was evaluated using another set of real data of the same Bank.},
  archive      = {J_IJSEKE},
  author       = {Jasmina Nalić and Goran Martinovic},
  doi          = {10.1142/S0218194020500072},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {2},
  pages        = {147-169},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Building a credit scoring model based on data mining approaches},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A case representation and similarity measurement model with
experience-grounded semantics. <em>IJSEKE</em>, <em>30</em>(1), 119–146.
(<a href="https://doi.org/10.1142/S0218194020500060">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Case-based reasoning heavily depends on the structure and content of the cases, and semantics is essential to effectively represent cases. In the field of structured case representation, most of the works regarding case representation and measurement of semantic similarity between cases are based on model-theoretic semantics and their extensions. The purpose of this study is to explore the potential of experienced-grounded semantics in case representation and semantic similarity measurement. The main contents in this study are as follows: (i) a case representation model based on experience-grounded semantic is proposed, (ii) a novel semantic similarity measurement method with multi-strategy reasoning is introduced, and (iii) a case-based reasoning software for urban firefighting field based on the proposed model is designed and implemented. Theoretically, compared with traditional structured case representation methods, the proposed model not only represents case in a fully formalized way, but also provides a novel metric for computing the strength of the semantic relationship between cases. The proposed model has been applied in an intelligent decision-support software for urban firefighting.},
  archive      = {J_IJSEKE},
  author       = {Nady Slam and Wushour Slamu and Pei Wang},
  doi          = {10.1142/S0218194020500060},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {1},
  pages        = {119-146},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {A case representation and similarity measurement model with experience-grounded semantics},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Thread scheduling sequence generation based on all
synchronization pair coverage criteria. <em>IJSEKE</em>, <em>30</em>(1),
97–118. (<a href="https://doi.org/10.1142/S0218194020500059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Testing multi-thread programs becomes extremely difficult because thread interleavings are uncertain, which may cause a program getting different results in each execution. Thus, Thread Scheduling Sequence (TSS) is a crucial factor in multi-thread program testing. A good TSS can obtain better testing efficiency and save the testing cost especially with the increase of thread numbers. Focusing on the above problem, in this paper, we discuss a kind of approach that can efficiently generate TSS based on the concurrent coverage criteria. First, we give a definition of Synchronization Pair (SP) as well as all Synchronization Pairs Coverage (ASPC) criterion. Then, we introduce the Synchronization Pair Thread Graph (SPTG) to describe the relationships between SPs and threads. Moreover, this paper presents a TSS generation method based on the ASPC according to SPTG. Finally, TSSs automatic generation experiments are conducted on six multi-thread programs in Java Library with the help of Java Path Finder (JPF) tool. The experimental results illustrate that our method not only generates TSSs to cover all SPs but also requires less state number, transition number as well as TSS number when satisfying ASPC, compared with other three widely used TSS generation methods. As a result, it is clear that the efficiency of TSS generation is obviously improved.},
  archive      = {J_IJSEKE},
  author       = {JunXia Guo and Zheng Li and CunFeng Shi and RuiLian Zhao},
  doi          = {10.1142/S0218194020500059},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {1},
  pages        = {97-118},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Thread scheduling sequence generation based on all synchronization pair coverage criteria},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Software analysis method for assessing software
sustainability. <em>IJSEKE</em>, <em>30</em>(1), 67–95. (<a
href="https://doi.org/10.1142/S0218194020500047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software sustainability evaluation has become an essential component of software engineering (SE) owing to sustainability considerations that must be incorporated into software development. Several studies have been performed to address the issues associated with sustainability concerns in the SE process. However, current practices extensively rely on participant experiences to evaluate sustainability achievement. Moreover, there exist limited quantifiable methods for supporting software sustainability evaluation. Our primary objective is to present a methodology that can assist software engineers in evaluating a software system based on well-defined sustainability metrics and measurements. We propose a novel approach that combines machine learning (ML) and software analysis methods. To simplify the application of the proposed approach, we present a semi-automated tool that supports engineers in assessing the sustainability achievement of a software system. The results of our study demonstrate that the proposed approach determines sustainability criteria and defines sustainability achievement in terms of a traceable matrix. Our theoretical evaluation and empirical study demonstrate that the proposed support tool can help engineers identify sustainability limitations in a particular feature of a software system. Our semi-automated tool can identify features that must be revised to enhance sustainability achievement.},
  archive      = {J_IJSEKE},
  author       = {Theresia Ratih Dewi Saputri and Seok-Won Lee},
  doi          = {10.1142/S0218194020500047},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {1},
  pages        = {67-95},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Software analysis method for assessing software sustainability},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Story point-based effort estimation model with machine
learning techniques. <em>IJSEKE</em>, <em>30</em>(1), 43–66. (<a
href="https://doi.org/10.1142/S0218194020500035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Until now, numerous effort estimation models for software projects have been developed, most of them producing accurate results but not providing the flexibility to decision makers during the software development process. The main objective of this study is to objectively and accurately estimate the effort when using the Scrum methodology. A dynamic effort estimation model is developed by using regression-based machine learning algorithms. Story point as a unit of measure is used for estimating the effort involved in an issue. Projects are divided into phases and the phases are respectively divided into iterations and issues. Effort estimation is performed for each issue, then the total effort is calculated with aggregate functions respectively for iteration, phase and project. This architecture of our model provides flexibility to decision makers in any case of deviation from the project plan. An empirical evaluation demonstrates that the error rate of our story point-based estimation model is better than others.},
  archive      = {J_IJSEKE},
  author       = {Muaz Gultekin and Oya Kalipsiz},
  doi          = {10.1142/S0218194020500035},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {1},
  pages        = {43-66},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Story point-based effort estimation model with machine learning techniques},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Applicability of machine learning methods on mobile app
effort estimation: Validation and performance evaluation.
<em>IJSEKE</em>, <em>30</em>(1), 23–41. (<a
href="https://doi.org/10.1142/S0218194020500023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software cost estimation is one of the most crucial tasks in a software development life cycle. Some well-proven methods and techniques have been developed for effort estimation in case of classical software. Mobile applications (apps) are different from conventional software by their nature, size and operational environment; therefore, the established estimation models for traditional desktop or web applications may not be suitable for mobile app development. The objective of this paper is to propose a framework for mobile app project estimation. The research methodology adopted in this work is based on selecting different features of mobile apps from the SAMOA dataset. These features are later used as input vectors to the selected machine learning (ML) techniques. The results of this research experiment are measured in mean absolute residual (MAR). The experimental outcomes are then followed by the proposition of a framework to recommend an ML algorithm as the best match for superior effort estimation of a project in question. This framework uses the Mamdani-type fuzzy inference method to address the ambiguities in the decision-making process. The outcome of this work will particularly help mobile app estimators, development professionals, and industry at large to determine the required efforts in the projects accurately.},
  archive      = {J_IJSEKE},
  author       = {Mamta Pandey and Ratnesh Litoriya and Prateek Pandey},
  doi          = {10.1142/S0218194020500023},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {1},
  pages        = {23-41},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Applicability of machine learning methods on mobile app effort estimation: Validation and performance evaluation},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Feedback2Code: A deep learning approach to identifying
user-feedback-related source code files. <em>IJSEKE</em>,
<em>30</em>(1), 1–22. (<a
href="https://doi.org/10.1142/S0218194020500011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Users frequently raise feedback when using software products. Feedback from users regarding their experiences and expectations and software defects they found adds values to software maintenance and evolution — software managers collect user feedback and then dispatch feedback issues that developers (and/or maintainers) need to track and process. Feedback tracking is often supported by open source platforms and collaborative software systems. Meanwhile, there still exists a gap between feedback issues and source code: since user feedback is usually informal and arbitrary, engineers have to spend much effort on comprehending issues and identifying which source code files need to be improved or fixed. This paper introduces a deep learning approach, Feedback2Code , which facilitates identification of user-feedback-related source code files. The core idea is to (1) explore latent semantics of user feedback and source code using several deep learning techniques such as Multi-Layer Perceptron (MLP), Convolutional Neutral Network (CNN) and skip-gram and (2) establish a multi-correlation model to explore linkages between feedback issues and source code files. Given a feedback issue, the linkages then allow engineers to identify source code files that are highly relevant to the issue. We have implemented Feedback2Code and evaluated it against ChangeAdvisor (a state-of-the-art approach) on 24 open source projects. The evaluation results clearly show the strength of Feedback2Code : for 103793 feedback issues, Feedback2Code successfully established 101190 feedback-code linkages and achieved a precision that is 3 . 1 1 × 3 . 1 1 × 3.11× higher than that of ChangeAdvisor . Feedback2Code also achieved an MRR and an MAP that are 2 . 8 3 × 2 . 8 3 × 2.83× and 2 . 7 9 × 2 . 7 9 × 2.79× higher than those of ChangeAdvisor , respectively. Furthermore, we also found that a Feedback2Code -trained model can be easily transferred, allowing feedback-code linkages to be established in new projects with a little history data.},
  archive      = {J_IJSEKE},
  author       = {Shuhan Yan and Tianjiao Du and Beijun Shen and Yuting Chen and Zhilei Ren},
  doi          = {10.1142/S0218194020500011},
  journal      = {International Journal of Software Engineering and Knowledge Engineering},
  number       = {1},
  pages        = {1-22},
  shortjournal = {Int. J. Soft. Eng. Knowl. Eng.},
  title        = {Feedback2Code: A deep learning approach to identifying user-feedback-related source code files},
  volume       = {30},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
