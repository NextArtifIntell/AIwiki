<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SICOMP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sicomp---65">SICOMP - 65</h2>
<ul>
<li><details>
<summary>
(2020). Erratum: Three-player entangled XOR games are NP-hard to
approximate. <em>SICOMP</em>, <em>49</em>(6), 1423–1427. (<a
href="https://doi.org/10.1137/20M1368598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This note indicates an error in the proof of Theorem 3.1 in [T. Vidick, SIAM J. Comput., 45 (2016), pp. 1007--1063]. Due to an induction step in the soundness analysis not being carried out correctly, the analysis fails to prove the claimed result. The error invalidates the proofs of the main computational hardness results claimed in the paper. We discuss implications for subsequent works. In some cases results can be partially recovered by applying a weakened version of Theorem 3.1 shown in [Z. Ji et al., Quantum Soundness of the Classical Low Individual Degree Test, arXiv:2009.1298, 2020] subsequently to the discovery of the error. The validity of Theorem 3.1 as stated in the paper remains an open question.},
  archive      = {J_SICOMP},
  author       = {Thomas Vidick},
  doi          = {10.1137/20M1368598},
  journal      = {SIAM Journal on Computing},
  number       = {6},
  pages        = {1423-1427},
  shortjournal = {SIAM J. Comput.},
  title        = {Erratum: Three-player entangled XOR games are NP-hard to approximate},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bidimensionality and kernels. <em>SICOMP</em>,
<em>49</em>(6), 1397–1422. (<a
href="https://doi.org/10.1137/16M1080264">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bidimensionality theory was introduced by [E. D. Demaine et al., J. ACM, 52 (2005), pp. 866--893] as a tool to obtain subexponential time parameterized algorithms on H-minor-free graphs. In [E. D. Demaine and M. Hajiaghayi, Bidimensionality: New connections between FPT algorithms and PTASs, in Proceedings of the 16th Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), SIAM, Philadelphia, 2005, pp. 590--601] this theory was extended in order to obtain polynomial time approximation schemes (PTASs) for bidimensional problems. In this work, we establish a third meta-algorithmic direction for bidimensionality theory by relating it to the existence of linear kernels for parameterized problems. In particular, we prove that every minor (resp., contraction) bidimensional problem that satisfies a separation property and is expressible in Countable Monadic Second Order Logic (CMSO) admits a linear kernel for classes of graphs that exclude a fixed graph (resp., an apex graph) H as a minor. Our results imply that a multitude of bidimensional problems admit linear kernels on the corresponding graph classes. For most of these problems no polynomial kernels on H-minor-free graphs were known prior to our work.},
  archive      = {J_SICOMP},
  author       = {Fedor V. Fomin and Daniel Lokshtanov and Saket Saurabh and Dimitrios M. Thilikos},
  doi          = {10.1137/16M1080264},
  journal      = {SIAM Journal on Computing},
  number       = {6},
  pages        = {1397-1422},
  shortjournal = {SIAM J. Comput.},
  title        = {Bidimensionality and kernels},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Connectivity oracles for graphs subject to vertex failures.
<em>SICOMP</em>, <em>49</em>(6), 1363–1396. (<a
href="https://doi.org/10.1137/17M1146610">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce new data structures for answering connectivity queries in graphs subject to batched vertex failures. A deterministic structure processes a batch of $d\leq d_\star$ failed vertices in $\tilde{O}(d^3)$ time and thereafter answers connectivity queries in $O(d)$ time. It occupies space $O(d_\star m\log n)$. We develop a randomized Monte Carlo version of our data structure with update time $\tilde{O}(d^2)$, query time $O(d)$, and space $\tilde{O}(m)$ for any failure bound $d\le n$. This is the first connectivity oracle for general graphs that can efficiently deal with an unbounded number of vertex failures. We also develop a more efficient Monte Carlo edge failure connectivity oracle. Using space $O(n\log^2 n)$, $d$ edge failures are processed in $O(d\log d\log\log n)$ time, and thereafter, connectivity queries are answered in $O(\log\log n)$ time, which are correct with high probability. Our data structures are based on a new decomposition theorem for an undirected graph $G=(V,E)$, which is of independent interest. It states that for any terminal set $U\subseteq V$ we can remove a set $B$ of $|U|/(s-2)$ vertices such that the remaining graph contains a Steiner forest for $U-B$ with maximum degree $s$.},
  archive      = {J_SICOMP},
  author       = {Ran Duan and Seth Pettie},
  doi          = {10.1137/17M1146610},
  journal      = {SIAM Journal on Computing},
  number       = {6},
  pages        = {1363-1396},
  shortjournal = {SIAM J. Comput.},
  title        = {Connectivity oracles for graphs subject to vertex failures},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hardness and ease of curing the sign problem for two-local
qubit hamiltonians. <em>SICOMP</em>, <em>49</em>(6), 1332–1362. (<a
href="https://doi.org/10.1137/19M1287511">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine the problem of determining whether a multiqubit two-local Hamiltonian can be made stoquastic by single-qubit unitary transformations. We prove that when such a Hamiltonian contains one-local terms, then this task can be NP-hard. This is shown by constructing a class of Hamiltonians for which performing this task is equivalent to deciding 3-SAT. In contrast, we show that when such a Hamiltonian contains no one-local terms then this task is easy; namely, we present an algorithm which decides, in a number of arithmetic operations over $\mathbb{R}$ which is polynomial in the number of qubits, whether the sign problem of the Hamiltonian can be cured by single-qubit rotations.},
  archive      = {J_SICOMP},
  author       = {Joel Klassen and Milad Marvian and Stephen Piddock and Marios Ioannou and Itay Hen and Barbara M. Terhal},
  doi          = {10.1137/19M1287511},
  journal      = {SIAM Journal on Computing},
  number       = {6},
  pages        = {1332-1362},
  shortjournal = {SIAM J. Comput.},
  title        = {Hardness and ease of curing the sign problem for two-local qubit hamiltonians},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A framework for exponential-time-hypothesis–tight algorithms
and lower bounds in geometric intersection graphs. <em>SICOMP</em>,
<em>49</em>(6), 1291–1331. (<a
href="https://doi.org/10.1137/20M1320870">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We give an algorithmic and lower bound framework that facilitates the construction of subexponential algorithms and matching conditional complexity bounds. It can be applied to intersection graphs of similarly-sized fat objects, yielding algorithms with running time $2^{O(n^{1-1/d})}$ for any fixed dimension $d\ge 2$ for many well-known graph problems, including Independent Set, $r$-Dominating Set for constant $r$, and Steiner Tree. For most problems, we get improved running times compared to prior work; in some cases, we give the first known subexponential algorithm in geometric intersection graphs. Additionally, most of the obtained algorithms are representation-agnostic, i.e., they work on the graph itself and do not require the geometric representation. Our algorithmic framework is based on a weighted separator theorem and various treewidth techniques. The lower bound framework is based on a constructive embedding of graphs into $d$-dimensional grids, and it allows us to derive matching $2^{\Omega(n^{1-1/d})}$ lower bounds under the exponential time hypothesis even in the much more restricted class of $d$-dimensional induced grid graphs.},
  archive      = {J_SICOMP},
  author       = {Mark de Berg and Hans L. Bodlaender and Sándor Kisfaludi-Bak and Dániel Marx and Tom C. van der Zanden},
  doi          = {10.1137/20M1320870},
  journal      = {SIAM Journal on Computing},
  number       = {6},
  pages        = {1291-1331},
  shortjournal = {SIAM J. Comput.},
  title        = {A framework for exponential-time-hypothesis--tight algorithms and lower bounds in geometric intersection graphs},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Resolving SINR queries in a dynamic setting.
<em>SICOMP</em>, <em>49</em>(6), 1271–1290. (<a
href="https://doi.org/10.1137/19M128733X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a set of transmitters broadcasting simultaneously on the same frequency under the signal to interference plus noise ratio (SINR) model. Transmission power may vary from one transmitter to another, and a transmitter&#39;s signal strength at a given point is modeled by the transmitter&#39;s power divided by some constant power $\alpha$ of the distance it traveled. Roughly, a receiver at a given location can hear a specific transmitter only if the transmitter&#39;s signal is stronger by a specified ratio than the signals of all other transmitters combined. An SINR query is to determine whether a receiver at a given location can hear any transmitter, and if yes, which one. An approximate answer to an SINR query is such that one gets a definite yes or definite no, when the ratio between the strongest signal and all other signals combined is well above or well below the reception threshold, while the answer in the intermediate range is allowed to be either yes or no. We describe compact data structures that support approximate SINR queries in the plane in a dynamic context, i.e., where transmitters may be inserted and deleted over time. We distinguish between two main variants---uniform power and nonuniform power. In both variants the preprocessing time is $O(n\,{polylog} n)$ and the amortized update time is $O({\rm polylog} n)$, while the query time is $O({polylog} n)$ for uniform power, and randomized time $O(\sqrt{n}\,{polylog} n)$ with high probability for nonuniform power. Finally, we observe that in the static context the latter data structure can be implemented differently, so that the query time is also $O({polylog} n)$, thus significantly improving all previous results for this problem.},
  archive      = {J_SICOMP},
  author       = {Boris Aronov and Gali Bar-On and Matthew J. Katz},
  doi          = {10.1137/19M128733X},
  journal      = {SIAM Journal on Computing},
  number       = {6},
  pages        = {1271-1290},
  shortjournal = {SIAM J. Comput.},
  title        = {Resolving SINR queries in a dynamic setting},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Subdeterminant maximization via nonconvex relaxations and
anti-concentration. <em>SICOMP</em>, <em>49</em>(6), 1249–1270. (<a
href="https://doi.org/10.1137/19M1309523">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Several fundamental problems that arise in optimization and computer science can be cast as follows: Given vectors $v_1,\ldots,v_m \in \mathbb{R}^d$ and a constraint family $\mathcal{B} \subseteq 2^{[m]}$, find a set $S \in \mathcal{B}$ that maximizes the squared volume of the simplex spanned by the vectors in $S$. A motivating example is the ubiquitous data-summarization problem in machine learning and information retrieval where one is given a collection of feature vectors that represent data such as documents or images. The volume of a collection of vectors is used as a measure of their diversity, and partition or matroid constraints over $[m]$ are imposed in order to ensure resource or fairness constraints. Even with a simple cardinality constraint ($\mathcal{B}=(\begin{smallmatrix}{[m]} \\ {r}\end{smallmatrix})$, the problem becomes NP-hard and has received much attention starting with a result by Khachiyan [J. Complexity, 11 (1995) pp. 138--153] who gave an $r^{O(r)}$ approximation algorithm for a special case of this problem. Recently, Nikolov and Singh [Proceedings of the Forty-eighth Annual ACM Symposium on Theory of Computing, 2016, pp. 192--201] presented a convex program and showed how it can be used to estimate the value of the most diverse set when there are multiple cardinality constraints (i.e., when $\mathcal{B}$ corresponds to a partition matroid). Their proof of the integrality gap of the convex program relied on an inequality by Gurvits [Proceedings of the Thirty-eighth Annual ACM Symposium on Theory of Computing, ACM, 2006, pp. 417--426], and was recently extended to regular matroids [Straszak and Vishnoi, Proceedings of the Forty-ninth ACM SIGACT Symposium on Theory of Computing, 2017, pp. 370--383] and [Anaris and Gharan, Proceedings of the Forty-ninth Annual SIGACT Symposium on Theory of Computing, 2017, pp. 384--396] and general matroids [Anari, Gharan, and Vinzant, Proceedings of the Fifty-ninth IEEE Annual Symposium on Foundations of Computer Science, 2018, pp. 35--46]. The question of whether these estimation algorithms can be converted into the more useful approximation algorithms---that also output a set---remained open. The main contribution of this paper is to give the first approximation algorithms for both partition and regular matroids. We present novel formulations for the subdeterminant maximization problem, for these matroids; this reduces them to the problem of finding a point that maximizes the absolute value of a nonconvex function over a Cartesian product of probability simplices. The technical core of our results is a new anti-concentration inequality for dependent random variables that arise from these functions, which allows us to relate the optimal value of these nonconvex functions to their value at a random point. Unlike prior work on the constrained subdeterminant maximization problem, our proofs do not rely on real-stability or convexity and could be of independent interest both in algorithms and complexity where anti-concentration phenomena have recently been deployed.},
  archive      = {J_SICOMP},
  author       = {Javad Ebrahimi and Damian Straszak and Nisheeth Vishnoi},
  doi          = {10.1137/19M1309523},
  journal      = {SIAM Journal on Computing},
  number       = {6},
  pages        = {1249-1270},
  shortjournal = {SIAM J. Comput.},
  title        = {Subdeterminant maximization via nonconvex relaxations and anti-concentration},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The power of the combined basic linear programming and
affine relaxation for promise constraint satisfaction problems.
<em>SICOMP</em>, <em>49</em>(6), 1232–1248. (<a
href="https://doi.org/10.1137/20M1312745">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the field of constraint satisfaction problems (CSPs), promise CSPs are an exciting new direction of study. In a promise CSP, each constraint comes in two forms: “strict” and “weak,” and in the associated decision problem one must distinguish between being able to satisfy all the strict constraints versus not being able to satisfy all the weak constraints. The most commonly cited example of a promise CSP is the approximate graph coloring problem-which has recently seen exciting progress [Bulín, Krokhin, and Oprs̆al, Proceedings of the Symposium on Theory of Computing, 2019, pp. 602--613 and Wrochna and Živný, Proceedings of the Symposium on Discrete Algorithms, 2020, pp. 1426--1435] benefiting from a systematic algebraic approach to promise CSPs based on “polymorphisms,” operations that map tuples in the strict form of each constraint to tuples in the corresponding weak form. In this work, we present a simple algorithm which in polynomial time solves the decision problem for all promise CSPs that admit infinitely many symmetric polymorphisms, which are invariant under arbitrary coordinate permutations. This generalizes previous work of the first two authors [Brakensiek and Guruswami, Proceedings of the Symposium on Discrete Algorithms, 2019, pp. 436--455]. We also extend this algorithm to a more general class of block-symmetric polymorphisms. As a corollary, this single algorithm solves all polynomial-time tractable Boolean CSPs simultaneously. These results give a new perspective on Schaefer&#39;s classic dichotomy theorem and shed further light on how symmetries of polymorphisms enable algorithms. Finally, we show that block symmetric polymorphisms are not only sufficient but also necessary for this algorithm to work, thus establishing its precise power.},
  archive      = {J_SICOMP},
  author       = {Joshua Brakensiek and Venkatesan Guruswami and Marcin Wrochna and Stanislav Živný},
  doi          = {10.1137/20M1312745},
  journal      = {SIAM Journal on Computing},
  number       = {6},
  pages        = {1232-1248},
  shortjournal = {SIAM J. Comput.},
  title        = {The power of the combined basic linear programming and affine relaxation for promise constraint satisfaction problems},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Algorithmic polynomials. <em>SICOMP</em>, <em>49</em>(6),
1173–1231. (<a href="https://doi.org/10.1137/19M1278831">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The approximate degree of a Boolean function $f(x_{1},x_{2},\ldots,x_{n})$ is the minimum degree of a real polynomial that approximates $f$ pointwise within $1/3$. Upper bounds on approximate degree have a variety of applications in learning theory, differential privacy, and algorithm design in general. Nearly all known upper bounds on approximate degree arise in an existential manner from bounds on quantum query complexity. We develop a novel, first-principles approach to the polynomial approximation of Boolean functions. We use it to give the first constructive upper bounds on the approximate degree of several fundamental problems: $O(n^{\frac{3}{4}-\frac{1}{4(2^{k}-1)}})$ for the $k$-element distinctness problem; $O(n^{1-\frac{1}{k+1}})$ for the $k$-subset sum problem; $O(n^{1-\frac{1}{k+1}})$ for any $k$-DNF or $k$-CNF formula; and $O(n^{3/4})$ for the surjectivity problem. In all cases, we obtain explicit, closed-form approximating polynomials that are unrelated to the quantum arguments from previous work. Our first three results match the bounds from quantum query complexity. Our fourth result improves polynomially on the $\Theta(n)$ quantum query complexity of the problem and refutes the conjecture by several experts that surjectivity has approximate degree $\Omega(n)$. In particular, we exhibit the first natural problem with a polynomial gap between approximate degree and quantum query complexity.},
  archive      = {J_SICOMP},
  author       = {Alexander A. Sherstov},
  doi          = {10.1137/19M1278831},
  journal      = {SIAM Journal on Computing},
  number       = {6},
  pages        = {1173-1231},
  shortjournal = {SIAM J. Comput.},
  title        = {Algorithmic polynomials},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hardness of continuous local search: Query complexity and
cryptographic lower bounds. <em>SICOMP</em>, <em>49</em>(6), 1128–1172.
(<a href="https://doi.org/10.1137/17M1118014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Local search proved to be an extremely useful tool when facing hard optimization problems (e.g., via the simplex algorithm, simulated annealing, or genetic algorithms). Although powerful, it has its limitations: there are functions for which exponentially many queries are needed to find a local optimum. In many contexts, the optimization problem is defined by a continuous function which might offer an advantage when performing the local search. This leads us to study the following natural question: How hard is continuous local search? The computational complexity of such search problems is captured by the complexity class ${CLS}$ [C. Daskalakis and C. H. Papadimitriou, Proceedings of SODA&#39;11, 2011], which is contained in the intersection of ${PLS}$ and ${PPAD}$, two important subclasses of ${TFNP}$ (the class of ${NP}$ search problems with a guaranteed solution). In this work, we show the first hardness results for ${CLS}$ (the smallest nontrivial class among the currently defined subclasses of $\mathbf{TFNP}$). Our hardness results are in terms of black-box (where only oracle access to the function is given) and white-box (where the function is represented succinctly by a circuit). In the black-box case, we show instances for which any (computationally unbounded) randomized algorithm must perform exponentially many queries in order to find a local optimum. In the white-box case, we show hardness for computationally bounded algorithms under cryptographic assumptions. Our results demonstrate a strong conceptual barrier precluding design of efficient algorithms for solving local search problems even over continuous domains. As our main technical contribution we introduce a new total search problem which we call End-of-Metered-Line. The special structure of End-of-Metered-Line enables us to (1) show that it is contained in ${CLS}$, (2) prove hardness for it in both the black-box and the white-box setting, and (3) extend to ${CLS}$ a variety of results previously known only for ${PPAD}$.},
  archive      = {J_SICOMP},
  author       = {Pavel Hubáček and Eylon Yogev},
  doi          = {10.1137/17M1118014},
  journal      = {SIAM Journal on Computing},
  number       = {6},
  pages        = {1128-1172},
  shortjournal = {SIAM J. Comput.},
  title        = {Hardness of continuous local search: Query complexity and cryptographic lower bounds},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Constructive polynomial partitioning for algebraic curves in
<span class="math inline">ℝ<sup>3</sup></span> with applications.
<em>SICOMP</em>, <em>49</em>(6), 1109–1127. (<a
href="https://doi.org/10.1137/19M1257548">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In 2015, Guth [Math. Proc. Cambridge Philos. Soc., 159 (2015), pp. 459--469] proved that for any set of $k$-dimensional bounded complexity varieties in ${\mathbb R}^d$ and for any positive integer $D$, there exists a polynomial of degree at most $D$ whose zero set divides ${\mathbb R}^d$ into open connected sets so that only a small fraction of the given varieties intersect each of these sets. Guth&#39;s result generalized an earlier result of Guth and Katz [Ann. Math., 181 (2015), pp. 155--190] for points. Guth&#39;s proof relies on a variant of the Borsuk--Ulam theorem, and for $k&gt;0$, it is unknown how to obtain an explicit representation of such a partitioning polynomial and how to construct it efficiently. In particular, it is unknown how to effectively construct such a polynomial for bounded-degree algebraic curves (or even lines) in ${{\mathbb R}}^3$. We present an efficient algorithmic construction for this setting. Given a set of $n$ input algebraic curves and a positive integer $D$, we efficiently construct a decomposition of space into $O(D^3\log^3{D})$ open “cells,” each of which meets $O(n/D^2)$ curves from the input. The construction time is $O(n^2)$. For the case of lines in 3-space, we present an improved implementation whose running time is $O(n^{4/3} { polylog }{n})$. The constant of proportionality in both time bounds depends on $D$ and the maximum degree of the polynomials defining the input curves. As an application, we revisit the problem of eliminating depth cycles among nonvertical lines in 3-space, recently studied by Aronov and Sharir [Discrete Comput. Geom., 59 (2018), pp. 725--741] and show an algorithm that cuts $n$ such lines into $O(n^{3/2+\varepsilon})$ pieces that are depth-cycle free for any $\varepsilon &gt; 0$. The algorithm runs in $O(n^{3/2+\varepsilon})$ time, which is a considerable improvement over the previously known algorithms.},
  archive      = {J_SICOMP},
  author       = {Boris Aronov and Esther Ezra and Joshua Zahl},
  doi          = {10.1137/19M1257548},
  journal      = {SIAM Journal on Computing},
  number       = {6},
  pages        = {1109-1127},
  shortjournal = {SIAM J. Comput.},
  title        = {Constructive polynomial partitioning for algebraic curves in $\mathbb{R}^3$ with applications},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A quasi-polynomial approximation for the restricted
assignment problem. <em>SICOMP</em>, <em>49</em>(6), 1083–1108. (<a
href="https://doi.org/10.1137/19M128257X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Restricted Assignment problem is a prominent special case of Scheduling on Unrelated Parallel Machines. For the strongest known linear programming relaxation, the configuration LP, we improve the nonconstructive bound on its integrality gap from 1.9412 to 1.8334 and significantly simplify the proof. Then we give a constructive variant, yielding a 1.8334-approximation in quasi-polynomial time. This is the first quasi-polynomial algorithm for this problem improving on the long-standing approximation rate of 2.},
  archive      = {J_SICOMP},
  author       = {Klaus Jansen and Lars Rohwedder},
  doi          = {10.1137/19M128257X},
  journal      = {SIAM Journal on Computing},
  number       = {6},
  pages        = {1083-1108},
  shortjournal = {SIAM J. Comput.},
  title        = {A quasi-polynomial approximation for the restricted assignment problem},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computational two-party correlation: A dichotomy for
key-agreement protocols. <em>SICOMP</em>, <em>49</em>(6), 1041–1082. (<a
href="https://doi.org/10.1137/19M1236837">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Let $\pi$ be an efficient two-party protocol that, given security parameter $\kappa$, both parties output single bits $X_\kappa$ and $Y_\kappa$, respectively. We are interested in how $(X_\kappa,Y_\kappa)$ “appears” to an efficient adversary that only views the transcript $T_\kappa$. We make the following contributions: (a) We develop new tools to argue about this loose notion and show (modulo some caveats) that for every such protocol $\pi$, there exists an efficient simulator such that the following holds: on input $T_\kappa$, the simulator outputs a pair $(X&#39;_\kappa,Y&#39;_\kappa)$ such that $(X&#39;_\kappa,Y&#39;_\kappa,T_\kappa)$ is (somewhat) computationally indistinguishable from $(X_\kappa,Y_\kappa,T_\kappa)$. (b) We use these tools to prove the following dichotomy theorem: every such protocol $\pi$ is either uncorrelated---it is (somewhat) indistinguishable from an efficient protocol whose parties interact to produce $T_\kappa$, but then choose their outputs independently from some product distribution (that is determined in poly-time from $T_\kappa$), or the protocol implies a key-agreement protocol (for infinitely many $\kappa$&#39;s). Uncorrelated protocols are uninteresting from a cryptographic viewpoint, as the correlation between outputs is (computationally) trivial. Our dichotomy shows that every protocol is either completely uninteresting or implies key-agreement. (c) We use the above dichotomy to make progress on open problems on minimal cryptographic assumptions required for differentially private mechanisms for the XOR function. (d) A subsequent work [I. Haitner, N. Makriyannis, and E. Omri, in Theory of Cryptography Conference, Springer, Cham, Switzerland, 2018, pp. 539--562] uses the above dichotomy to makes progress on a long-standing open question regarding the complexity of fair two-party coin-flipping protocols. We also highlight the following two ideas regarding our technique: (a) The simulator algorithm is obtained by a carefully designed “competition” between efficient algorithms attempting to forecast $(X_\kappa,Y_\kappa)|_{T_\kappa=t}$. The winner is used to simulate the outputs of the protocol. (b) Our key-agreement protocol uses the simulation to reduce to an information theoretic setup and is, in some sense, a non-black-box.},
  archive      = {J_SICOMP},
  author       = {Iftach Haitner and Kobbi Nissim and Eran Omri and Ronen Shaltiel and Jad Silbak},
  doi          = {10.1137/19M1236837},
  journal      = {SIAM Journal on Computing},
  number       = {6},
  pages        = {1041-1082},
  shortjournal = {SIAM J. Comput.},
  title        = {Computational two-party correlation: A dichotomy for key-agreement protocols},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Shadow tomography of quantum states. <em>SICOMP</em>,
<em>49</em>(5), STOC18-368-394. (<a
href="https://doi.org/10.1137/18M120275X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce the problem of shadow tomography: given an unknown $D$-dimensional quantum mixed state $\rho$, as well as known two-outcome measurements $E_{1},\ldots,E_{M}$, estimate the probability that $E_{i}$ accepts $\rho$, to within additive error $\varepsilon$, for each of the $M$ measurements. How many copies of $\rho$ are needed to achieve this, with high probability? Surprisingly, we give a procedure that solves the problem by measuring only $\widetilde{O}\left(\varepsilon^{-4}\cdot\log^{4}M\cdot\log D\right)$ copies. This means, for example, that we can learn the behavior of an arbitrary $n$-qubit state, on all accepting/rejecting circuits of some fixed polynomial size, by measuring only $n^{O(1)}$ copies of the state. This resolves an open problem of the author, which arose from his work on private-key quantum money schemes, but which also has applications to quantum copy-protected software, quantum advice, and quantum one-way communication. Recently, building on this work, Branda͂o et al. have given a different approach to shadow tomography using semidefinite programming, which achieves a savings in computation time.},
  archive      = {J_SICOMP},
  author       = {Scott Aaronson},
  doi          = {10.1137/18M120275X},
  journal      = {SIAM Journal on Computing},
  number       = {5},
  pages        = {STOC18-368-394},
  shortjournal = {SIAM J. Comput.},
  title        = {Shadow tomography of quantum states},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Inapproximability of the independent set polynomial in the
complex plane. <em>SICOMP</em>, <em>49</em>(5), STOC18-395-448. (<a
href="https://doi.org/10.1137/18M1184485">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the complexity of approximating the value of the independent set polynomial $Z_G(\lambda)$ of a graph $G$ with maximum degree $\Delta$ when the activity $\lambda$ is a complex number. When $\lambda$ is real, the complexity picture is well understood, and is captured by two real-valued thresholds $\lambda^*$ and $\lambda_c$, which depend on $\Delta$ and satisfy $00$, resolving in the affirmative a conjecture of Harvey, Srivastava, and Vondrák. Our proof techniques are based around tools from complex analysis---specifically the study of iterative multivariate rational maps.},
  archive      = {J_SICOMP},
  author       = {Ivona Bezáková and Andreas Galanis and Leslie Ann Goldberg and Daniel Štefankovič},
  doi          = {10.1137/18M1184485},
  journal      = {SIAM Journal on Computing},
  number       = {5},
  pages        = {STOC18-395-448},
  shortjournal = {SIAM J. Comput.},
  title        = {Inapproximability of the independent set polynomial in the complex plane},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Smooth heaps and a dual view of self-adjusting data
structures. <em>SICOMP</em>, <em>49</em>(5), STOC18-45-93. (<a
href="https://doi.org/10.1137/18M1195188">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new connection between self-adjusting binary search trees (BSTs) and heaps, two fundamental, extensively studied, and practically relevant families of data structures [B. Allen and I. Munro, J. ACM, 25 (1978), pp. 526--535; D. D. Sleator and R. E. Tarjan, J. ACM, 32 (1985), pp. 652--686; M. L. Fredman et al., Algorithmica, 1 (1986), pp. 111--129; R. Wilber, SIAM J. Comput., 18 (1989), pp. 56--67; M. L. Fredman, in WAE 1999, Springer, Berlin, 1999, pp. 244--258; J. Iacono and Ö. Özkan, in ICALP 2014, Springer, Berlin, 2014, pp. 637--649]. Roughly speaking, we map an arbitrary heap algorithm within a natural model, to a corresponding BST algorithm with the same cost on a dual sequence of operations (i.e., the same sequence with the roles of time and key-space switched). This is the first general transformation between the two families of data structures. There is a rich theory of dynamic optimality for BSTs (i.e., the theory of competitiveness between BST algorithms). The lack of an analogous theory for heaps has been noted in the literature (e.g., [S. Pettie, in FOCS 2005, IEEE, Washington, DC, 2005, pp. 174--183; S. Pettie, in SODA 2008, ACM, New York, SIAM, Philadelphia, 2008, pp. 1115--1124]). Through our connection, we transfer all instance-specific lower bounds known for BSTs to a general model of heaps, initiating a theory of dynamic optimality for heaps. On the algorithmic side, we obtain a new, simple, and efficient heap algorithm, which we call the smooth heap. We show the smooth heap to be the heap-counterpart of Greedy, the BST algorithm with the strongest proven and conjectured properties from the literature, widely believed to be instance-optimal [J. M. Lucas, Canonical Forms for Competitive Binary Search Tree Algorithms, Tech. rep. DCS-TR-250, Rutgers University, New Brunswick, NJ, 1988; J. Munro, in Algorithms---ESA 2000, Lecture Notes in Comput. Sci. 1879, Springer, Berlin, Heidelberg, 2000, pp. 338--345; E. D. Demaine et al., in SODA 2009, AMC, New York, SIAM, Philadelphia, 2009, pp. 496--505]. Assuming the optimality of Greedy, the smooth heap is also optimal within our model of heap algorithms. As corollaries of results known for Greedy, we obtain instance-specific upper bounds for the smooth heap, with applications in adaptive sorting. Intriguingly, the smooth heap, although derived from a non-practical BST algorithm, is simple and easy to implement (e.g., it stores no auxiliary data besides the keys and tree pointers). It can be seen as a variation on the popular pairing heap data structure, extending it with a “power-of-two-choices” type of heuristic.},
  archive      = {J_SICOMP},
  author       = {László Kozma and Thatchaphol Saranurak},
  doi          = {10.1137/18M1195188},
  journal      = {SIAM Journal on Computing},
  number       = {5},
  pages        = {STOC18-45-93},
  shortjournal = {SIAM J. Comput.},
  title        = {Smooth heaps and a dual view of self-adjusting data structures},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Circuit lower bounds for nondeterministic quasi-polytime
from a new easy witness lemma. <em>SICOMP</em>, <em>49</em>(5),
STOC18-300-322. (<a href="https://doi.org/10.1137/18M1195887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We prove that if every problem in ${NP}$ has $n^k$-size circuits for a fixed constant $k$, then for every ${NP}$-verifier and every yes-instance $x$ of length $n$ for that verifier, the verifier&#39;s search space has an $n^{O(k^3)}$-size witness circuit: A witness for $x$ that can be encoded with a circuit of only $n^{O(k^3)}$ size. An analogous statement is proved for nondeterministic quasi-polynomial time, i.e., ${NQP} = {NTIME}[n^{\log^{O(1)} n}]$. This significantly extends the Easy Witness Lemma of Impagliazzo, Kabanets, and Wigderson [J. Comput. System Sci., 65 (2002), pp. 672--694] which only held for larger nondeterministic classes such as ${NEXP}$. As a consequence, the connections between circuit-analysis algorithms and circuit lower bounds can be considerably sharpened: Algorithms for approximately counting satisfying assignments for given circuits which improve over exhaustive search can imply circuit lower bounds for functions in ${NQP}$, or even ${NP}$. To illustrate, applying known algorithms for satisfiability of ${ACC} \circ {THR}$ circuits [R. Williams, New algorithms and lower bounds for circuits with linear threshold gates, in Proceedings of the 46th Annual ACM Symposium on Theory of Computing, ACM, New York, 2014, pp. 194--202] we conclude that for every fixed $k$, ${NQP}$ does not have $n^{\log^k n}$-size ${ACC} \circ {THR}$ circuits.},
  archive      = {J_SICOMP},
  author       = {Cody D. Murray and R. Ryan Williams},
  doi          = {10.1137/18M1195887},
  journal      = {SIAM Journal on Computing},
  number       = {5},
  pages        = {STOC18-300-322},
  shortjournal = {SIAM J. Comput.},
  title        = {Circuit lower bounds for nondeterministic quasi-polytime from a new easy witness lemma},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A friendly smoothed analysis of the simplex method.
<em>SICOMP</em>, <em>49</em>(5), STOC18-449-499. (<a
href="https://doi.org/10.1137/18M1197205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Explaining the excellent practical performance of the simplex method for linear programming has been a major topic of research for over 50 years. One of the most successful frameworks for understanding the simplex method was given by Spielman and Teng [J. ACM, 51 (2004), pp. 385--463] who developed the notion of smoothed analysis. Starting from an arbitrary linear program (LP) with $d$ variables and $n$ constraints, Spielman and Teng analyzed the expected runtime over random perturbations of the LP, known as the smoothed LP, where variance $\sigma^2$ Gaussian noise is added to the LP data. In particular, they gave a two-stage shadow vertex simplex algorithm which uses an expected $\widetilde{O}(d^{55} n^{86} \sigma^{-30} + d^{70}n^{86})$ number of simplex pivots to solve the smoothed LP. Their analysis and runtime was substantially improved by Deshpande and Spielman [FOCS `05, 2005, pp. 349--356] and later Vershynin [SIAM J. Comput., 39 (2009), pp. 646--678]. The fastest current algorithm, due to Vershynin, solves the smoothed LP using an expected $O\big(\log^2 n \cdot \log\log n \cdot (d^3\sigma^{-4} + d^5\log^2 n + d^9\log^4 d)\big)$ number of pivots, improving the dependence on $n$ from polynomial to polylogarithmic. While the original proof of Spielman and Teng has now been substantially simplified, the resulting analyses are still quite long and complex and the parameter dependencies far from optimal. In this work, we make substantial progress on this front, providing an improved and simpler analysis of shadow simplex methods, where our algorithm requires an expected $O(d^2 \sqrt{\log n} ~ \sigma^{-2} + d^3 \log^{3/2} n)$ number of simplex pivots. We obtain our results via an improved shadow bound, key to earlier analyses as well, combined with improvements on algorithmic techniques of Vershynin. As an added bonus, our analysis is completely modular and applies to a range of perturbations, which, aside from Gaussians, also includes Laplace perturbations.},
  archive      = {J_SICOMP},
  author       = {Daniel Dadush and Sophie Huiberts},
  doi          = {10.1137/18M1197205},
  journal      = {SIAM Journal on Computing},
  number       = {5},
  pages        = {STOC18-449-499},
  shortjournal = {SIAM J. Comput.},
  title        = {A friendly smoothed analysis of the simplex method},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Round compression for parallel matching algorithms.
<em>SICOMP</em>, <em>49</em>(5), STOC18-1-44. (<a
href="https://doi.org/10.1137/18M1197655">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For over a decade now we have been witnessing the success of massive parallel computation frameworks, such as MapReduce, Hadoop, Dryad, or Spark. Compared to the classic distributed algorithms or PRAM models, these frameworks allow for much more local computation. The fundamental question that arises however in this context is can we leverage this additional power to obtain even faster parallel algorithms? A prominent example here is the maximum matching problem. It is well known that in the PRAM model one can compute a 2-approximate maximum matching in $O(\log{n})$ rounds. Lattanzi et al. [SPAA, ACM, New York, 2011, pp. 85--94] showed that if each machine has $n^{1+\Omega(1)}$ memory, this problem can also be solved 2-approximately in a constant number of rounds. These techniques, as well as the approaches developed in the follow-up work, seem though to get stuck in a fundamental way at roughly $O(\log{n})$ rounds once we enter the (at most) near-linear memory regime. In this paper, we break the above $O(\log n)$ round complexity bound even in the case of slightly sublinear memory per machine. In fact, our improvement here is almost exponential: we are able to deliver a $(1+\epsilon)$-approximate maximum matching for any fixed constant $\epsilon&gt;0$ in $O((\log \log n)^2)$ rounds. To establish our result we need to deviate from the previous work in two important ways. First, we use vertex-based graph partitioning, instead of the edge-based approaches that were utilized so far. Second, we develop a technique of round compression.},
  archive      = {J_SICOMP},
  author       = {Artur Czumaj and Jakub Ła̧cki and Aleksander Ma̧dry and Slobodan Mitrović and Krzysztof Onak and Piotr Sankowski},
  doi          = {10.1137/18M1197655},
  journal      = {SIAM Journal on Computing},
  number       = {5},
  pages        = {STOC18-1-44},
  shortjournal = {SIAM J. Comput.},
  title        = {Round compression for parallel matching algorithms},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Pseudorandom pseudo-distributions with near-optimal error
for read-once branching programs. <em>SICOMP</em>, <em>49</em>(5),
STOC18-242-299. (<a href="https://doi.org/10.1137/18M1197734">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nisan [Combinatorica, 12 (1992), pp. 449--461] constructed a pseudorandom generator for length $n$, width $n$ read-once branching programs (ROBPs) with error $\varepsilon$ and seed length $O(\log^2{n} + \log{n} \cdot \log(1/\varepsilon))$. A major goal in complexity theory is to reduce the seed length, hopefully, to the optimal $O(\log{n}+\log(1/\varepsilon))$, or to construct improved hitting sets, as these would yield stronger derandomization of ${BPL}$ and ${RL}$, respectively. In contrast to a successful line of work in restricted settings, no progress has been made for general, unrestricted, ROBPs. Indeed, Nisan&#39;s construction is the best pseudorandom generator and, prior to this work, also the best hitting set for unrestricted ROBPs. In this work, we make the first improvement for the general case by constructing a hitting set with seed length $\widetilde{O}(\log^2{n}+\log(1/\varepsilon))$. That is, we decouple $\varepsilon$ and $n$, and obtain near-optimal dependence on the former. The regime of parameters in which our construction strictly improves upon prior works, namely, $\log(1/\varepsilon) \gg \log{n}$, is also motivated by the work of Saks and Zhou [J. Comput. System Sci., 58 (1999), pp. 376--403], who use pseudorandom generators with error $\varepsilon$, for length $n$, width $w$ ROBPs, such that $w,1/\varepsilon = 2^{(\log{n})^{2}}$ in their proof for ${BPL} \subseteq \mathbf{L}^{3/2}$. In fact, we introduce and construct a new type of primitive we call pseudorandom pseudo-distributions. Informally, this is a generalization of pseudorandom generators in which one may assign negative and unbounded weights to paths, as opposed to working with probability distributions. We show that such a primitive yields hitting sets and, for derandomization purposes, can be used to derandomize two-sided error algorithms.},
  archive      = {J_SICOMP},
  author       = {Mark Braverman and Gil Cohen and Sumegha Garg},
  doi          = {10.1137/18M1197734},
  journal      = {SIAM Journal on Computing},
  number       = {5},
  pages        = {STOC18-242-299},
  shortjournal = {SIAM J. Comput.},
  title        = {Pseudorandom pseudo-distributions with near-optimal error for read-once branching programs},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Collusion resistant traitor tracing from learning with
errors. <em>SICOMP</em>, <em>49</em>(5), STOC18-94-241. (<a
href="https://doi.org/10.1137/18M1197825">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we provide a traitor tracing construction with ciphertexts that grow polynomially in $\log(n)$, where $n$ is the number of users, and prove it secure under the learning with errors (LWE) assumption. This is the first traitor tracing scheme with such parameters provably secure from a standard assumption. In addition to achieving new traitor tracing results, we believe our techniques push forward the broader area of computing on encrypted data under standard assumptions. Notably, traitor tracing is a substantially different problem from other cryptography primitives that have seen recent progress in LWE solutions. We achieve our results by first conceiving a novel approach to building traitor tracing that starts with a new form of functional encryption that we call Mixed FE. In a Mixed FE system the encryption algorithm is bimodal and works with either a public key or master secret key. Ciphertexts encrypted using the public key can only encrypt one type of functionality. On the other hand, the secret key encryption can be used to encode many different types of programs, but is only secure as long as the attacker sees a bounded number of such ciphertexts. We first show how to combine mixed FE with attribute-based encryption to achieve traitor tracing. Second, we build Mixed FE systems for polynomial-sized branching programs (which corresponds to the complexity class logspace) by relying on the polynomial hardness of the LWE assumption with superpolynomial modulus-to-noise ratio.},
  archive      = {J_SICOMP},
  author       = {Rishab Goyal and Venkata Koppula and Brent Waters},
  doi          = {10.1137/18M1197825},
  journal      = {SIAM Journal on Computing},
  number       = {5},
  pages        = {STOC18-94-241},
  shortjournal = {SIAM J. Comput.},
  title        = {Collusion resistant traitor tracing from learning with errors},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nearly work-efficient parallel algorithm for digraph
reachability. <em>SICOMP</em>, <em>49</em>(5), STOC18-500-539. (<a
href="https://doi.org/10.1137/18M1197850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One of the simplest problems on directed graphs is that of identifying the set of vertices reachable from a designated source vertex. This problem can be solved easily sequentially by performing a graph search, but efficient parallel algorithms have eluded researchers for decades. For sparse high-diameter graphs in particular, there is no known work-efficient parallel algorithm with nontrivial parallelism. This amounts to one of the most fundamental open questions in parallel graph algorithms: Is there a parallel algorithm for digraph reachability with nearly linear work? This article shows that the answer is yes, presenting a randomized parallel algorithm for digraph reachability and related problems with expected work $\tilde{O}(m)$ and span $\tilde{O}(n^{2/3})$, and hence parallelism $\tilde{\Omega}(m/n^{2/3}) = \tilde{\Omega}(n^{1/3})$, on any graph with $n$ vertices and $m$ arcs. This is the first parallel algorithm having both nearly linear work and strongly sublinear span, i.e., span $\tilde{O}(n^{1-\epsilon})$ for any constant $\epsilon&gt;0$. The algorithm can be extended to produce a directed spanning tree, determine whether the graph is acyclic, topologically sort the strongly connected components of the graph, or produce a directed ear decomposition, all with work $\tilde{O}(m)$ and span $\tilde{O}(n^{2/3})$. The main technical contribution is an efficient Monte Carlo algorithm that, through the addition of $\tilde{O}(n)$ shortcuts, reduces the diameter of the graph to $\tilde{O}(n^{2/3})$ with high probability. While both sequential and parallel algorithms are known with those combinatorial properties, even the sequential algorithms are not efficient, having sequential runtime $\Omega(mn^{\Omega(1)})$. This article presents a surprisingly simple sequential algorithm that achieves the stated diameter reduction and runs in $\tilde{O}(m)$ time. Parallelizing that algorithm yields the main result, but doing so involves overcoming several other challenges.},
  archive      = {J_SICOMP},
  author       = {Jeremy T. Fineman},
  doi          = {10.1137/18M1197850},
  journal      = {SIAM Journal on Computing},
  number       = {5},
  pages        = {STOC18-500-539},
  shortjournal = {SIAM J. Comput.},
  title        = {Nearly work-efficient parallel algorithm for digraph reachability},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Crossing the logarithmic barrier for dynamic boolean data
structure lower bounds. <em>SICOMP</em>, <em>49</em>(5), STOC18-323-367.
(<a href="https://doi.org/10.1137/18M1198429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proves the first superlogarithmic lower bounds on the cell probe complexity of dynamic Boolean (also known as decision) data structure problems, a long-standing milestone in data structure lower bounds. We introduce a new method for proving dynamic cell probe lower bounds and use it to prove an $\tilde{\Omega}({lg}^{1.5} \ n)$ lower bound on the operational time of a wide range of Boolean data structure problems, most notably, on the query time of dynamic range counting over $\mathbb{F}_2$ [M. Patrascu, Lower bounds for $2$-dimensional range counting, in STOC 2007, ACM, New York, 2007, pp. 40--46]. Proving an $\omega({\rm lg} \ n)$ lower bound for this problem was explicitly posed as one of five important open problems in the late Mihai Pǎtraşcu&#39;s obituary [M. Thorup, Bull. Eur. Assoc. Theor. Comput. Sci., 109 (2013), pp. 7--13]. This result also implies the first $\omega({lg} \ n)$ lower bound for the classical 2-dimensional (2D) range counting problem, one of the most fundamental data structure problems in computational geometry and spatial databases. We derive similar lower bounds for Boolean versions of dynamic polynomial evaluation and 2D rectangle stabbing, and for the (non-Boolean) problems of range selection and range median. Our technical centerpiece is a new way of “weakly” simulating dynamic data structures using efficient one-way communication protocols with small advantage over random guessing. This simulation involves a surprising excursion to low-degree (Chebyshev) polynomials which may be of independent interest, and offers an entirely new algorithmic angle on the “cell sampling” method of Panigrahy, Talwar, and Wieder [Lower bounds on near neighbor search via metric expansion, FOCS 2010, IEEE Computer Society, Los Alamitos, CA, 2010, pp. 805--814].},
  archive      = {J_SICOMP},
  author       = {Kasper Green Larsen and Omri Weinstein and Huacheng Yu},
  doi          = {10.1137/18M1198429},
  journal      = {SIAM Journal on Computing},
  number       = {5},
  pages        = {STOC18-323-367},
  shortjournal = {SIAM J. Comput.},
  title        = {Crossing the logarithmic barrier for dynamic boolean data structure lower bounds},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Special section on the fiftieth annual ACM symposium on
theory of computing (STOC 2018). <em>SICOMP</em>, <em>49</em>(5),
STOC18-i-ii. (<a href="https://doi.org/10.1137/20N975154">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This issue of SICOMP contains 10 specially selected papers from the Fiftieth Annual ACM Symposium on Theory of Computing, otherwise known as STOC 2018, held June 25 to 29 in Los Angeles, California. The papers here were chosen to represent both the excellence and the broad range of the STOC program. The papers have been revised and extended by the authors and subjected to the standard thorough reviewing process of SICOMP. The program committee consisted of Dimitris Achlioptas (University of California, Santa Cruz), Dorit Aharonov (Hebrew University), Susanne Albers (Technical University Munich), Eric Allender (Rutgers University), Sayan Bhattacharya (University of Warwick), Richard Cole (New York University), Vitaly Feldman (Google Research), Uriel Feige (Weizmann Institute), Sanjam Garg (University of California, Berkeley), Ashish Goel (Stanford University), Parikshit Gopalan (VMware), Monika Henzinger, chair (University of Vienna), Giuseppe Italiano (Luiss University), Robert Kleinberg (Cornell University), Claire Matthieu (École Normale Supérieure, CNRS), Ankur Moitra (Massachusetts Institute of Technology), Danupon Nanongkai (KTH Royal Institute of Technology, Stockholm), Michał Pilipczuk (University of Warsaw), Krzysztof Pietrzak (Institute of Science and Technology, Austria), Aaron Sidford (Stanford University), Christian Sohler (Universität zu Köln), Prasad Tetali (Georgia Institute of Technology), Kunal Talwar (Apple), Luca Trevisan (Bocconi University), Thomas Vidick (California Institute of Technology), Emo Welzl (ETH Zurich), Philipp Woelfel (University of Calgary), David Woodruff (Carnegie Mellon University), and Mary Wootters (Stanford University). They selected 112 papers out of 416 submissions. We briefly describe the papers that appear here. In “Round Compression for Parallel Matching Algorithms,” Artur Czumaj, Jakub Ła̧cki, Aleksander Ma̧dry, Slobodan Mitrović, Krzysztof Onak, and Piotr Sankowski break the $O(\log n)$ round complexity bound for 2-approximating the maximum matching in near-linear memory regime of the massively parallel computation model. In “Smooth Heaps and a Dual View of Self-Adjusting Data Structures,” László Kozma and Thatchaphol Saranurak show a new correspondence between self-adjusting binary search trees (BSTs) and heaps. Using this connection they are able to transfer known lower bounds on BSTs to a general model of heaps as well as obtain a new, simple, and efficient heap algorithm called the “smooth heap.” In “Collusion Resistant Traitor Tracing from Learning with Errors,&quot; Rishab Goyal, Venkata Koppula, and Brent Waters introduce a new approach to the traitor tracing problem. Informally, in traitor tracing one aims to devise an encryption scheme such that decryption can be performed using $n$ different private keys and such that moreover any decryption can be “traced back&quot; to the key(s) that was or were used for it. In this paper the authors obtain the first scheme with ciphertext size that grows polynomially in $\log(n)$ and the security parameter $\lambda$ and whose security is based on the learning with errors assumption. In “Pseudorandom Pseudo-distributions with Near-Optimal Error for Read-Once Branching Programs,” Mark Braverman, Gil Cohen, and Sumegha Garg construct a hitting set for unrestricted read-once branching programs with seed length $O(\log^2n + \log(1/\varepsilon))$. This is the first improvement since Nisan&#39;s pseudorandom generator with seed length $O(\log^2n + \log n \log(1/\varepsilon)$. In “Circuit Lower Bounds for Nondeterministic Quasi-Polytime from a New Easy Witness Lemma,” Cody Murray and Ryan Williams show that if every problem in NP has polynomial-size circuits for a fixed polynomial, then every problem in NP also has a fixed polynomial-size witness. A specific consequence of this result is that for every fixed $k$, NQP does not have $n^{\log^k n}$-size ACC$\circ$THR circuits. In “Crossing the Logarithmic Barrier for Dynamic Boolean Data Structure Lower Bounds,” Kasper Green Larsen, Omri Weinstein, and Huacheng Yu prove the first superlogarithmic lower bounds on the cell probe complexity of dynamic Boolean data structure problems, a long-standing milestone in data structure lower bounds. In “Shadow Tomography of Quantum States,” Scott Aaronson asks: Given an unknown $D$-dimensional quantum mixed state $\rho$ and two-outcome measurements $E_1, \ldots, E_M$, how many copies of $\rho$ are needed to estimate the probability that $E_i$ accepts $\rho$ to within additive error $\varepsilon$, for each of the $M$ measurements? He shows that $O(\varepsilon^{-4} \log^4 M \log D)$ copies of $\rho$ suffice, implying, for example, that we can learn the behavior of an arbitrary $n$-qubit state, on all accepting/rejecting circuits of some fixed polynomial size, by measuring only $n^{O(1)}$ copies of the state. In “Inapproximability of the Independent Set Polynomial in the Complex Plane,” Ivona Bezáková, Andreas Galanis, Leslie Ann Goldberg, and Daniel Štefankovič study the complexity of approximating the independent set polynomial of a graph with maximum degree $\Delta$ when the activity $\lambda$ is a complex number. They prove that outside a cardioid-shaped region in the complex plane identified by Peters and Regts, wherein the occupation ratios of $\Delta$-regular trees converge, approximation is $\#$P-hard (unless $\lambda$ is a positive real number, in which case it is NP-hard). In “A Friendly Smoothed Analysis of the Simplex Method,” Daniel Dadush and Sophie Huiberts consider linear programs with $d$ variables and $n$ constraints, smoothed by the addition of Gaussian noise with variance $\sigma^2$. They provide an improved and greatly simplified analysis of shadow simplex methods by combining an improved shadow bound with improvements on algorithmic techniques of Vershynin and show that in expectation $O(d^2 \sqrt{\log n} \, \sigma^{-2} + d^3 \log^{3/2}n)$ pivots suffice. In “Nearly Work-Efficient Parallel Algorithm for Digraph Reachability,” Jeremy T. Fineman presents a randomized parallel algorithm for digraph reachability and related problems with expected work $\tilde{O}(m)$ and span $\tilde{O}(n^{2/3})$. This is the first parallel algorithm having both nearly linear work and strongly sublinear span.},
  archive      = {J_SICOMP},
  author       = {Thomas Vidick and Danupon Nanongkai and Dimitris Achlioptas},
  doi          = {10.1137/20N975154},
  journal      = {SIAM Journal on Computing},
  number       = {5},
  pages        = {STOC18-i-ii},
  shortjournal = {SIAM J. Comput.},
  title        = {Special section on the fiftieth annual ACM symposium on theory of computing (STOC 2018)},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonmalleable extractors and codes, with their many tampered
extensions. <em>SICOMP</em>, <em>49</em>(5), 999–1040. (<a
href="https://doi.org/10.1137/18M1176622">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Randomness extractors and error correcting codes are fundamental objects in computer science. Recently, there have been several natural generalizations of these objects, in the context and study of tamper-resilient cryptography. These are seeded nonmalleable extractors, introduced by Dodis and Wichs (STOC 2009); seedless nonmalleable extractors, introduced by Cheraghchi and Guruswami (TCC 2014); and nonmalleable codes, introduced by Dziembowski, Pietrzak, and Wichs (J. ACM, 2018). Besides being interesting on their own, they also have important applications in cryptography, e.g., privacy amplification with an active adversary, explicit nonmalleable codes, etc., and often have unexpected connections to their nontampered analogues. However, the known constructions are far behind their nontampered counterparts. Indeed, the best known seeded nonmalleable extractor requires min-entropy rate at least 0.49 [X. Li, in Proceedings of the 53rd Annual IEEE Symposium on Foundations of Computer Science, 2012, pp. 688--697], while explicit construction of nonmalleable two-source extractors was not known even if both sources have full min-entropy and was left as an open problem in [M. Cheraghchi and V. Guruswami, J. Cryptology, 30 (2017), pp. 191--241]. In this paper we make progress towards solving the above problems and other related generalizations. Our contributions are as follows: (i) We construct an explicit seeded nonmalleable extractor for min-entropy $k \geq \log^2 n$. This dramatically improves all previous results and gives a simpler two-round privacy amplification protocol with optimal entropy loss, matching the best known result in [X. Li, in Theory of Cryptography (TCC 2015), Springer, 2015, pp. 502--531]. In fact, we construct more general seeded nonmalleable extractors (that can handle multiple adversaries) which were used in the recent construction of explicit two-source extractors for polylogarithmic min-entropy [E. Chattopadhyay and D. Zuckerman, Ann. of Math. (2), 189 (2019), pp. 653--705]. (ii) We construct the first explicit nonmalleable two-source extractor for min-entropy $k \geq n-n^{\Omega(1)}$, with output size $n^{\Omega(1)}$ and error $2^{-n^{\Omega(1)}}$, thus resolving the open question in [M. Cheraghchi and V. Guruswami, J. Cryptology, 30 (2017), pp. 191--241]. (iii) We motivate and initiate the study of two natural generalizations of seedless nonmalleable extractors and nonmalleable codes, where the sources or the codeword may be tampered many times. For this, we construct the first explicit nonmalleable two-source extractor with tampering degree $t$ up to $n^{\Omega(1)}$. By using the connection in [M. Cheraghchi and V. Guruswami, J. Cryptology, 30 (2017), pp. 191--241] and providing efficient sampling algorithms, we obtain the first explicit nonmalleable codes with tampering degree $t$ up to $n^{\Omega(1)}$. We call these stronger notions one-many and many-many nonmalleable codes. This provides a stronger information theoretic analogue of a primitive known as continuous nonmalleable codes. Our basic technique used in all of our constructions can be seen as inspired, in part, by the techniques previously used to construct cryptographic nonmalleable commitments.},
  archive      = {J_SICOMP},
  author       = {Eshan Chattopadhyay and Vipul Goyal and Xin Li},
  doi          = {10.1137/18M1176622},
  journal      = {SIAM Journal on Computing},
  number       = {5},
  pages        = {999-1040},
  shortjournal = {SIAM J. Comput.},
  title        = {Nonmalleable extractors and codes, with their many tampered extensions},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Amplification and derandomization without slowdown.
<em>SICOMP</em>, <em>49</em>(5), 959–998. (<a
href="https://doi.org/10.1137/17M1110596">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present techniques for decreasing the error probability of randomized algorithms and for converting randomized algorithms to deterministic (nonuniform) algorithms. Unlike most existing techniques that involve repetition of the randomized algorithm and hence a slowdown, our techniques produce algorithms with similar run-time to the original randomized algorithms. The amplification technique applies when there is a quick, probabilistic test of the randomness. In this case, we show how to efficiently find one randomness string for the algorithm that works with very high probability and apply the algorithm only on that randomness (in contrast, standard approaches suggest a number of possible randomness strings and run the algorithm on all of them). The search of good randomness turns out to be a natural stochastic multiarmed bandit problem, which we define (``the biased coin problem&#39;&#39;) and analyze. The derandomization technique applies when there is a verifier that can test the randomness of the algorithm while only inspecting a sublinear size sketch of the input (the sketch may be hard to compute; the verifier may be inefficient and is allowed to reject a small portion of the good randomness strings). In this case, we show how to apply Adleman&#39;s derandomization (from the proof of $BPP\subseteq P/poly$) more efficiently. We demonstrate the techniques by showing applications for dense max-cut, approximate clique, free games, and going from list decoding to unique decoding for Reed--Muller codes.},
  archive      = {J_SICOMP},
  author       = {Ofer Grossman and Dana Moshkovitz},
  doi          = {10.1137/17M1110596},
  journal      = {SIAM Journal on Computing},
  number       = {5},
  pages        = {959-998},
  shortjournal = {SIAM J. Comput.},
  title        = {Amplification and derandomization without slowdown},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tight revenue gaps among simple mechanisms. <em>SICOMP</em>,
<em>49</em>(5), 927–958. (<a
href="https://doi.org/10.1137/19M126178X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a fundamental problem in microeconomics: selling a single item to a number of potential buyers, whose values are drawn from known independent and regular (not necessarily identical) distributions. There are four widely used and widely studied mechanisms in the literature: Myerson Auction (OPT), Sequential Posted-Pricing (SPM), Second-Price Auction with Anonymous Reserve (AR), and Anonymous Pricing (AP). OPT is revenue-optimal but complicated and also experiences several issues in practice such as fairness; AP is the simplest mechanism but also generates the lowest revenue among these four mechanisms; SPM and AR are of intermediate complexity and revenue. We explore revenue gaps among these mechanisms, each of which is defined as the largest ratio between revenues from a pair of mechanisms. We establish two tight bounds and one tighter bound: 1. SPM vs. AP: this ratio studies the power of discrimination in pricing schemes. We obtain the tight ratio of constant ${\cal{C}}^* \approx {2.62}$, closing the gap between $[\frac{e}{e - 1}, e]$ left before. 2. AR vs. AP: this ratio measures the relative power of auction scheme vs. pricing scheme, when no discrimination is allowed. We attain the tight ratio of $\frac{\pi^2}{6} \approx 1.64$, closing the previously known bounds $[\frac{e}{e - 1}, e]$. 3. OPT vs. AR: this ratio quantifies the power of discrimination in auction schemes and is previously known to be somewhere between [2, e]. The lower bound of 2 was conjectured to be tight by Hartline and Roughgarden [Proceedings of the 10th ACM Conference on Electronic Commerce, 2009, pp. 225--234] and Alaei et al. [Games Econom. Behav., 118 (2019), pp. 494--510]. We acquire a better lower bound of 2.15 and thus disprove this conjecture.},
  archive      = {J_SICOMP},
  author       = {Yaonan Jin and Pinyan Lu and Zhihao Gavin Tang and Tao Xiao},
  doi          = {10.1137/19M126178X},
  journal      = {SIAM Journal on Computing},
  number       = {5},
  pages        = {927-958},
  shortjournal = {SIAM J. Comput.},
  title        = {Tight revenue gaps among simple mechanisms},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Strong connectivity in directed graphs under failures, with
applications. <em>SICOMP</em>, <em>49</em>(5), 865–926. (<a
href="https://doi.org/10.1137/19M1258530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we investigate some basic connectivity problems in directed graphs (digraphs). Let $G$ be a digraph with $m$ edges and $n$ vertices, and let $G\setminus e$ (resp., $G\setminus v$) be the digraph obtained after deleting edge $e$ (resp., vertex $v$) from $G$. As a first result, we show how to compute in $O(m+n)$ worst-case time: the total number of strongly connected components in $G\setminus e$ (resp., $G\setminus v$) for all edges $e$ (resp., for all vertices $v$) in $G$. Let $G$ be strongly connected. We say that edge $e$ (resp., vertex $v$) separates two vertices $x$ and $y$ if $x$ and $y$ are no longer strongly connected in $G\setminus e$ (resp., $G\setminus v$). As a second set of results, we show how to build in $O(m+n)$ time $O(n)$-space data structures that can answer in optimal time the following basic connectivity queries on digraphs: report in $O(n)$ worst-case time all the strongly connected components of $G\setminus e$ (resp., $G\setminus v$) for a query edge $e$ (resp., vertex $v$); test whether an edge or a vertex separates two query vertices in $O(1)$ worst-case time; report all edges (resp., vertices) that separate two query vertices in optimal worst-case time, i.e., in time $O(k)$, where $k$ is the number of separating edges (resp., separating vertices). (For $k=0$, the time is $O(1).$) All our bounds are tight and are obtained with a common algorithmic framework, based on a novel compact representation of the decompositions induced by the 1-connectivity (i.e., 1-edge and 1-vertex) cuts in digraphs, which might be of independent interest. With the help of our data structures we can design efficient algorithms for several other connectivity problems on digraphs and we can also obtain in linear time a strongly connected spanning subgraph of $G$ with $O(n)$ edges that maintains the 1-connectivity cuts of $G$ and the decompositions induced by those cuts.},
  archive      = {J_SICOMP},
  author       = {Loukas Georgiadis and Giuseppe F. Italiano and Nikos Parotsidis},
  doi          = {10.1137/19M1258530},
  journal      = {SIAM Journal on Computing},
  number       = {5},
  pages        = {865-926},
  shortjournal = {SIAM J. Comput.},
  title        = {Strong connectivity in directed graphs under failures, with applications},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Query-to-communication lifting for BPP. <em>SICOMP</em>,
<em>49</em>(4), FOCS17-441-461. (<a
href="https://doi.org/10.1137/17M115339X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For any $n$-bit boolean function $f$, we show that the randomized communication complexity of the composed function $f\circ g^n$, where $g$ is an index gadget, is characterized by the randomized decision tree complexity of $f$. In particular, this means that many query complexity separations involving randomized models (e.g., classical vs. quantum) automatically imply analogous separations in communication complexity.},
  archive      = {J_SICOMP},
  author       = {Mika Göös and Toniann Pitassi and Thomas Watson},
  doi          = {10.1137/17M115339X},
  journal      = {SIAM Journal on Computing},
  number       = {4},
  pages        = {FOCS17-441-461},
  shortjournal = {SIAM J. Comput.},
  title        = {Query-to-communication lifting for BPP},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local list recovery of high-rate tensor codes and
applications. <em>SICOMP</em>, <em>49</em>(4), FOCS17-157-195. (<a
href="https://doi.org/10.1137/17M116149X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that the tensor product of a high-rate globally list recoverable code is (approximately) locally list recoverable. List recovery has been a useful building block in the design of list decodable codes, and our motivation is to use the tensor construction as such a building block. In particular, instantiating this construction with known constructions of high-rate globally list recoverable codes, and using appropriate transformations, we obtain the first capacity-achieving locally list decodable codes (over a large constant size alphabet), and the first capacity-achieving globally list decodable codes with nearly linear time list decoding algorithms. Our techniques are inspired by an approach of Gopalan, Guruswami, and Raghavendra [SIAM J. Comput., 40 (2011), pp. 1432--1462] for list decoding tensor codes.},
  archive      = {J_SICOMP},
  author       = {Brett Hemenway and Noga Ron-Zewi and Mary Wootters},
  doi          = {10.1137/17M116149X},
  journal      = {SIAM Journal on Computing},
  number       = {4},
  pages        = {FOCS17-157-195},
  shortjournal = {SIAM J. Comput.},
  title        = {Local list recovery of high-rate tensor codes and applications},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Scheduling to minimize total weighted completion time via
time-indexed linear programming relaxations. <em>SICOMP</em>,
<em>49</em>(4), FOCS17-409-440. (<a
href="https://doi.org/10.1137/17M1156332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study approximation algorithms for problems of scheduling precedence constrained jobs with the objective of minimizing total weighted completion time, in identical and related machine models. We give algorithms that improve upon many previous 15- to 20-year-old state-of-the-art results. A major theme in these results is the use of time-indexed linear programming relaxations, which are quite natural for their respective problems. We also consider the scheduling problem of minimizing total weighted completion time on unrelated machines. The recent breakthrough result of [N. Bansal, A. Srinivasan, and O. Svensson, in Proceedings of the 48th Annual ACM Symposium on Theory of Computing, ACM, 2016, pp. 156--167] gave a (1.5-c)-approximation for the problem, based on a two-round lift-and-project of the SDP relaxation for the problem. Our main result is that a (1.5 - c)-approximation can also be achieved using a natural and considerably simpler time-indexed linear programming relaxation for the problem. We hope this relaxation can provide new insights into the problem.},
  archive      = {J_SICOMP},
  author       = {Shi Li},
  doi          = {10.1137/17M1156332},
  journal      = {SIAM Journal on Computing},
  number       = {4},
  pages        = {FOCS17-409-440},
  shortjournal = {SIAM J. Comput.},
  title        = {Scheduling to minimize total weighted completion time via time-indexed linear programming relaxations},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A nearly optimal lower bound on the approximate degree of
AC<span class="math inline"><sup>0</sup></span>. <em>SICOMP</em>,
<em>49</em>(4), FOCS17-59-96. (<a
href="https://doi.org/10.1137/17M1161737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The approximate degree of a Boolean function $f \colon {-1, 1}^n \rightarrow {-1, 1}$ is the least degree of a real polynomial that approximates $f$ pointwise to error at most 1/3. We introduce a generic method for increasing the approximate degree of a given function, while preserving its computability by constant-depth circuits. Specifically, we show how to transform any Boolean function $f$ with approximate degree $d$ into a function $F$ on $O(n \cdot \mathrm{polylog}(n))$ variables with approximate degree at least $D = \Omega(n^{1/3} \cdot d^{2/3})$. In particular, if $d= n^{1-\Omega(1)}$, then $D$ is polynomially larger than $d$. Moreover, if $f$ is computed by a polynomial-size Boolean circuit of constant depth, then so is $F$. By recursively applying our transformation, for any constant $\delta &gt; 0$ we exhibit an AC$^0$ function of approximate degree $\Omega(n^{1-\delta})$. This improves upon the best previous lower bound of $\Omega(n^{2/3})$ due to Aaronson and Shi [J. ACM, 51 (2004), pp. 595--605] and nearly matches the trivial upper bound of $n$ that holds for any function. Our lower bounds also apply to (quasipolynomial-size) disjunctive normal forms of polylogarithmic width. We describe several applications of these results and provide the following: (i) for any constant $\delta &gt; 0$, an $\Omega(n^{1-\delta})$ lower bound on the quantum communication complexity of a function in AC$^0$; (ii) a Boolean function $f$ with approximate degree at least $C(f)^{2-o(1)}$, where $C(f)$ is the certificate complexity of $f$; this separation is optimal up to the $o(1)$ term in the exponent; (iii) improved secret sharing schemes with reconstruction procedures in AC$^0$.},
  archive      = {J_SICOMP},
  author       = {Mark Bun and Justin Thaler},
  doi          = {10.1137/17M1161737},
  journal      = {SIAM Journal on Computing},
  number       = {4},
  pages        = {FOCS17-59-96},
  shortjournal = {SIAM J. Comput.},
  title        = {A nearly optimal lower bound on the approximate degree of AC$^0$},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the power of statistical zero knowledge. <em>SICOMP</em>,
<em>49</em>(4), FOCS17-1-58. (<a
href="https://doi.org/10.1137/17M1161749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We examine the power of statistical zero knowledge proofs (captured by the complexity class $\mathsf{SZK}$) and their variants. First, we give the strongest known relativized evidence that $\mathsf{SZK}$ contains hard problems, by exhibiting an oracle relative to which $\mathsf{SZK}$ (indeed, even $\mathsf{NISZK}$) is not contained in the class $\mathsf{UPP}$, containing those problems solvable by randomized algorithms with unbounded error. This answers an open question of Watrous from 2002 [S. Aaronson, private communication, 2015]. Second, we “lift” this oracle separation to the setting of communication complexity, thereby answering a question of Göös, Pitassi, and Watson [Proceedings of the 43rd International Colloquium on Automata, Languages, and Programming, Part I, 2016, pp. 86:1--86:15]. Third, we give relativized evidence that perfect zero knowledge proofs (captured by the class $\mathsf{PZK}$) are weaker than general zero knowledge proofs. Specifically, we exhibit oracles relative to which $\mathsf{SZK} \not \subseteq \mathsf{PZK}$, $\mathsf{NISZK} \not \subseteq \mathsf{NIPZK}$, and $\mathsf{PZK} \neq \mathsf{coPZK}$. The first of these results answers a question raised in 1991 by Aiello and H\aa stad [Inform. and Comput., 93 (1991), pp. 223--240], and the second answers a question of Lovett and Zhang [Proceedings of the 15th International Conference of Theory of Cryptography, Part 1, Baltimore, MD, 2017, pp. 31--55]. We also describe additional applications of these results outside of structural complexity. The technical core of our results is a stronger hardness amplification theorem for approximate degree, which roughly says that composing the gapped-majority function with any function of high approximate degree yields a function with high threshold degree.},
  archive      = {J_SICOMP},
  author       = {Adam Bouland and Lijie Chen and Dhiraj Holden and Justin Thaler and Prashant Nalini Vasudevan},
  doi          = {10.1137/17M1161749},
  journal      = {SIAM Journal on Computing},
  number       = {4},
  pages        = {FOCS17-1-58},
  shortjournal = {SIAM J. Comput.},
  title        = {On the power of statistical zero knowledge},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hardness results for structured linear systems.
<em>SICOMP</em>, <em>49</em>(4), FOCS17-280-349. (<a
href="https://doi.org/10.1137/17M1161774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that if the nearly linear time solvers for Laplacian matrices and their generalizations can be extended to solve just slightly larger families of linear systems, then they can be used to quickly solve all systems of linear equations over the reals. This result can be viewed either positively or negatively: either nearly linear time algorithms can be developed for solving all systems of linear equations over the reals, or progress on the families that can be solved in nearly linear time will soon halt.},
  archive      = {J_SICOMP},
  author       = {Rasmus Kyng and Peng Zhang},
  doi          = {10.1137/17M1161774},
  journal      = {SIAM Journal on Computing},
  number       = {4},
  pages        = {FOCS17-280-349},
  shortjournal = {SIAM J. Comput.},
  title        = {Hardness results for structured linear systems},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two-round and non-interactive concurrent non-malleable
commitments from time-lock puzzles. <em>SICOMP</em>, <em>49</em>(4),
FOCS17-196-279. (<a href="https://doi.org/10.1137/17M1163177">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-malleable commitments are a fundamental cryptographic tool for preventing (concurrent) man-in-the-middle attacks. Since their invention by Dolev, Dwork, and Naor in 1991, their round-complexity has been extensively studied, leading up to constant-round protocols based on one-way functions (OWFs), and three-round protocols based on sub-exponential OWFs, and standard polynomial-time hardness assumptions such as decisional Diffie--Hellman (DDH) and ZAPs (i.e., two-round witness-indistinguishable proofs). But constructions of two-round, or non-interactive, non-malleable commitments have so far remained elusive; the only known construction relied on a strong and non-falsifiable assumption with a non-malleability flavor. Additionally, a recent result by Pass shows the impossibility of basing two-round non-malleable commitments on falsifiable assumptions using a polynomial-time black-box security reduction. In this work, we show how to overcome this impossibility using super-polynomial-time hardness assumptions. Our main result demonstrates the existence of two-round concurrent non-malleable commitments based on the following four primitives (all with sub-exponential security): (1) non-interactive commitments, (2) ZAPs (i.e., 2-round witness indistinguishable proofs), (3) collision-resistant hash functions, and (4) a “weak” time-lock puzzle. Primitives (1), (2), and (3) can be based on, e.g., the discrete log and the RSA assumption. Time-lock puzzles---puzzles that can be solved by “brute-force” in time $2^t$, but cannot be solved significantly faster even using parallel computers---were proposed by Rivest, Shamir, and Wagner in 1996 and have been extensively studied since. We additionally obtain a non-interactive (i.e., one-message) version of our protocol satisfying concurrent non-malleability w.r.t. uniform attackers and show that our non-malleable commitments satisfy an even stronger notion of chosen commitment attack security.},
  archive      = {J_SICOMP},
  author       = {Huijia Lin and Rafael Pass and Pratik Soni},
  doi          = {10.1137/17M1163177},
  journal      = {SIAM Journal on Computing},
  number       = {4},
  pages        = {FOCS17-196-279},
  shortjournal = {SIAM J. Comput.},
  title        = {Two-round and non-interactive concurrent non-malleable commitments from time-lock puzzles},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Determinant-preserving sparsification of SDDM matrices.
<em>SICOMP</em>, <em>49</em>(4), FOCS17-350-408. (<a
href="https://doi.org/10.1137/18M1165979">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that variants of spectral sparsification routines can preserve the total spanning tree counts of graphs. By Kirchhoff&#39;s matrix-tree theorem, this is equivalent to preserving the determinant of a graph Laplacian minor or, equivalently, of any symmetric diagonally dominant matrix (SDDM). Our analyses utilize this combinatorial connection to bridge the gap between statistical leverage scores/effective resistances and the analysis of random graphs by Janson [Combin. Probab. Comput., 3 (1994), pp. 97--126]. This leads to a routine that, in quadratic time, sparsifies a graph down to about $n^{1.5}$ edges in a way that preserves both the determinant and the distribution of spanning trees (provided the sparsified graph is viewed as a random object). Extending this algorithm to work with Schur complements and approximate Choleksy factorizations leads to algorithms for counting and sampling spanning trees which are nearly optimal for dense graphs. We give an algorithm that computes a $(1 \pm \delta)$ approximation to the determinant of any SDDM matrix with constant probability in about $n^2 \delta^{-2}$ time. This is the first routine for graphs that outperforms general-purpose routines for computing determinants of arbitrary matrices. We also give an algorithm that generates, in about $n^2 \delta^{-2}$ time, a spanning tree of a weighted undirected graph from a distribution with a total variation distance of $\delta$ from the $\boldsymbol{\mathit{w}}$-uniform distribution.},
  archive      = {J_SICOMP},
  author       = {David Durfee and John Peebles and Richard Peng and Anup B. Rao},
  doi          = {10.1137/18M1165979},
  journal      = {SIAM Journal on Computing},
  number       = {4},
  pages        = {FOCS17-350-408},
  shortjournal = {SIAM J. Comput.},
  title        = {Determinant-preserving sparsification of SDDM matrices},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Better guarantees for <span
class="math inline"><em>k</em></span>-means and euclidean <span
class="math inline"><em>k</em></span>-median by primal-dual algorithms.
<em>SICOMP</em>, <em>49</em>(4), FOCS17-97-156. (<a
href="https://doi.org/10.1137/18M1171321">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clustering is a classic topic in optimization with $k$-means being one of the most fundamental such problems. In the absence of any restrictions on the input, the best-known algorithm for $k$-means in Euclidean space with a provable guarantee is a simple local search heuristic yielding an approximation guarantee of $9+\epsilon$, a ratio that is known to be tight with respect to such methods. We overcome this barrier by presenting a new primal-dual approach that allows us to (1) exploit the geometric structure of $k$-means and (2) satisfy the hard constraint that at most $k$ clusters are selected without deteriorating the approximation guarantee. Our main result is a 6.357-approximation algorithm with respect to the standard linear programming (LP) relaxation. Our techniques are quite general, and we also show improved guarantees for $k$-median in Euclidean metrics and for a generalization of $k$-means in which the underlying metric is not required to be Euclidean.},
  archive      = {J_SICOMP},
  author       = {Sara Ahmadian and Ashkan Norouzi-Fard and Ola Svensson and Justin Ward},
  doi          = {10.1137/18M1171321},
  journal      = {SIAM Journal on Computing},
  number       = {4},
  pages        = {FOCS17-97-156},
  shortjournal = {SIAM J. Comput.},
  title        = {Better guarantees for $k$-means and euclidean $k$-median by primal-dual algorithms},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Special section on the fifty-eighth annual IEEE symposium on
foundations of computer science (FOCS 2017). <em>SICOMP</em>,
<em>49</em>(4), FOCS17–i. (<a
href="https://doi.org/10.1137/20N975099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This special section comprises nine fully refereed papers whose extended abstracts were presented at the 58th Annual IEEE Symposium on Foundations of Computer Science (FOCS 2017) in Berkeley, California, on October 15--17, 2017. The preliminary conference versions of these papers were published by in the FOCS 2017 proceedings. The regular conference program consisted of 90 papers chosen from among 323 submissions. They were selected by a program committee consisting of Aditya Bhaskara, Andrej Bogdanov, Vladimir Braverman, Shiri Chechik, Gil Cohen, Anindya De, Ankit Garg, Josh Grochow, Sean Hallgren, Valentine Kabanets, Gillat Kol, Ravi Kumar, Chris Peikert, Sofya Raskhodnikova, Rahul Santhanam, Yaron Singer, Chaitanya Swamy, Amnon Ta-Shma, Chris Umans (chair), Vinod Vaikuntanathan, Emanuele Viola, Omri Weinstein, and Amir Yehudayoff. The papers invited to this special section were also chosen with the input of the program committee. The nine papers in this section span a broad range of topics, including cryptography, approximation algorithms, hardness of approximation, complexity theory, communication complexity, graph sparsification, and error-correcting codes. Each paper underwent an extensive refereeing process. We thank the authors and the anonymous referees for their efforts. In addition, we would like to thank SICOMP Editors-in-Chief Leonard Schulman and Robert Krauthgamer and SIAM Senior Publications Coordinator Heather Blythe for their help in preparing this special section.},
  archive      = {J_SICOMP},
  author       = {Valentine Kabanets and Sofya Raskhodnikova and Chaitanya Swamy},
  doi          = {10.1137/20N975099},
  journal      = {SIAM Journal on Computing},
  number       = {4},
  pages        = {FOCS17-i},
  shortjournal = {SIAM J. Comput.},
  title        = {Special section on the fifty-eighth annual IEEE symposium on foundations of computer science (FOCS 2017)},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Find your place: Simple distributed algorithms for community
detection. <em>SICOMP</em>, <em>49</em>(4), 821–864. (<a
href="https://doi.org/10.1137/19M1243026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given an underlying graph, we consider the following dynamics: Initially, each node locally chooses a value in ${-1,1}$, uniformly at random and independently of other nodes. Then, in each consecutive round, every node updates its local value to the average of the values held by its neighbors, at the same time applying an elementary, local clustering rule that only depends on the current and the previous values held by the node. We prove that the process resulting from this dynamics produces a clustering that exactly or approximately (depending on the graph) reflects the underlying cut in logarithmic time, under various graph models that exhibit a sparse balanced cut, including the stochastic block model. We also prove that a natural extension of this dynamics performs community detection on a regularized version of the stochastic block model with multiple communities. Rather surprisingly, our results provide rigorous evidence for the ability of an extremely simple and natural dynamics to perform community detection, a computational problem which is nontrivial even in a centralized setting.},
  archive      = {J_SICOMP},
  author       = {Luca Becchetti and Andrea E. Clementi and Emanuele Natale and Francesco Pasquale and Luca Trevisan},
  doi          = {10.1137/19M1243026},
  journal      = {SIAM Journal on Computing},
  number       = {4},
  pages        = {821-864},
  shortjournal = {SIAM J. Comput.},
  title        = {Find your place: Simple distributed algorithms for community detection},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simple optimal hitting sets for small-success RL.
<em>SICOMP</em>, <em>49</em>(4), 811–820. (<a
href="https://doi.org/10.1137/19M1268707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We give a simple explicit hitting set generator for read-once branching programs of width $w$ and length $r$ with known variable order and acceptance probability at least $\epsilon$. When $r = w$, our generator has seed length $O(\log^2 r + \log(1/\epsilon))$. When $r = \text{polylog } w$, our generator has optimal seed length $O(\log w + \log(1/\epsilon))$. For intermediate values of $r$, our generator&#39;s seed length smoothly interpolates between these two extremes. Our generator&#39;s seed length improves on recent work by Braverman, Cohen, and Garg [SIAM J. Comput., (2020), doi:10.1137/18M1197734]. In addition, our generator and its analysis are dramatically simpler than the work by Braverman et al. When $\epsilon$ is small, our generator&#39;s seed length improves on all the classic generators for space-bounded computation [N. Nisan, Combinatorica, 12 (1992), pp. 449--461; R. Impagliazzo, N. Nisan, and A. Wigderson, in Proceedings of the 26th Annual ACM Symposium on Theory of Computing, ACM, 1994, pp. 356--364; N. Nisan and D. Zuckerman, J. Comput. System Sci., 52 (1996), pp. 43--52]. However, all of these other works construct more general objects than we do. As a corollary of our construction, we show that every ${RL}$ algorithm that uses $r$ random bits can be simulated by an ${NL}$ algorithm that uses only $O(r/\log^c n)$ nondeterministic bits, where $c$ is an arbitrarily large constant. Finally, we show that any ${RL}$ algorithm with small success probability $\epsilon$ can be simulated deterministically in space $O(\log^{3/2} n + \log n \log \log(1/\epsilon))$. This space bound improves on work by Saks and Zhou [J. Comput. System Sci., 58 (1999), pp. 376--403], who gave an algorithm for the more general “two-sided” problem that runs in space $O(\log^{3/2} n + \sqrt{\log n} \log(1/\epsilon))$.},
  archive      = {J_SICOMP},
  author       = {William M. Hoza and David Zuckerman},
  doi          = {10.1137/19M1268707},
  journal      = {SIAM Journal on Computing},
  number       = {4},
  pages        = {811-820},
  shortjournal = {SIAM J. Comput.},
  title        = {Simple optimal hitting sets for small-success RL},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). From gap-exponential time hypothesis to fixed parameter
tractable inapproximability: Clique, dominating set, and more.
<em>SICOMP</em>, <em>49</em>(4), 772–810. (<a
href="https://doi.org/10.1137/18M1166869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider questions that arise from the intersection between the areas of polynomial-time approximation algorithms, subexponential-time algorithms, and fixed-parameter tractable (FPT) algorithms. The questions, which have been asked several times, are whether there is a nontrivial FPT-approximation algorithm for the Maximum Clique $({\sf Clique})$ and Minimum Dominating Set $({\sf DomSet})$ problems parameterized by the size of the optimal solution. In particular, letting ${\sf OPT}$ be the optimum and $N$ be the size of the input, is there an algorithm that runs in $t({\sf OPT}){\operatorname{poly}}(N)$ time and outputs a solution of size $f({\sf OPT})$ for any computable functions $t$ and $f$ that are independent of $N$ (for ${\sf Clique}$, we want $f({\sf OPT})=\omega(1)$)? In this paper, we show that both ${\sf Clique}$ and ${\sf DomSet}$ admit no nontrivial FPT-approximation algorithm, i.e., there is no $o({\sf OPT})$-FPT-approximation algorithm for ${\sf Clique}$ and no $f({\sf OPT})$-FPT-approximation algorithm for ${\sf DomSet}$ for any function $f$. In fact, our results imply something even stronger: The best way to solve ${\sf Clique}$ and ${\sf DomSet}$, even approximately, is to essentially enumerate all possibilities. Our results hold under the Gap Exponential Time Hypothesis [I. Dinur. ECCC, TR16-128, 2016; P. Manurangsi and P. Raghavendra, preprint, arXiv:1607.02986, 2016], which states that no $2^{o(n)}$-time algorithm can distinguish between a satisfiable 3 \sf SAT formula and one which is not even $(1 - \varepsilon)$-satisfiable for some constant $\varepsilon &gt; 0$. Besides ${\sf Clique}$ and ${\sf DomSet}$, we also rule out nontrivial FPT-approximation for the Maximum Biclique problem, the problem of finding maximum subgraphs with hereditary properties (e.g., Maximum Induced Planar Subgraph), and Maximum Induced Matching in bipartite graphs, and we rule out the $k^{o(1)}$-FPT-approximation algorithm for the Densest $k$-Subgraph problem.},
  archive      = {J_SICOMP},
  author       = {Parinya Chalermsook and Marek Cygan and Guy Kortsarz and Bundit Laekhanukit and Pasin Manurangsi and Danupon Nanongkai and Luca Trevisan},
  doi          = {10.1137/18M1166869},
  journal      = {SIAM Journal on Computing},
  number       = {4},
  pages        = {772-810},
  shortjournal = {SIAM J. Comput.},
  title        = {From gap-exponential time hypothesis to fixed parameter tractable inapproximability: Clique, dominating set, and more},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On approximating the number of <span
class="math inline"><em>k</em></span>-cliques in sublinear time.
<em>SICOMP</em>, <em>49</em>(4), 747–771. (<a
href="https://doi.org/10.1137/18M1176701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of approximating the number of $k$-cliques in a graph when given query access to the graph. We consider the standard query model for general graphs via (1) degree queries, (2) neighbor queries, and (3) pair queries. Let $n$ denote the number of vertices in the graph, $m$ the number of edges, and $C_k$ the number of $k$-cliques. We design an algorithm that outputs a $(1+\varepsilon)$-approximation (with high probability) for $C_k$, whose expected query complexity and running time are $O(\frac{n}{C_k^{1/k}}+\frac{m^{k/2}}{C_k} ){poly}(\log n, 1/\varepsilon,k)$. Hence, the complexity of the algorithm is sublinear in the size of the graph for $C_k = \omega(m^{k/2-1})$. Furthermore, we prove a lower bound showing that the query complexity of our algorithm is essentially optimal (up to the dependence on $\log n$, $1/\varepsilon$, and $k$). The previous results in this vein are by Feige [SIAM J. Comput., 35 (2006), pp. 964--984] and by Goldreich and Ron [Random Structures Algorithms, 32 (2008), pp. 473--493] for edge counting ($k=2$) and by Eden, Levi, Ron, and Seshadhri [SIAM J. Comput., 46 (2017), pp. 1603--1646] for triangle counting ($k=3$). Our result matches the complexities of these results. The previous result by Eden et al. hinges on a certain amortization technique that works only for triangle counting and does not generalize for larger cliques. We obtain a general algorithm that works for any $k\geq 3$ by designing a procedure that samples each $k$-clique incident to one of the vertices of a given set $S$ of vertices with approximately equal probability.},
  archive      = {J_SICOMP},
  author       = {Talya Eden and Dana Ron and C. Seshadhri},
  doi          = {10.1137/18M1176701},
  journal      = {SIAM Journal on Computing},
  number       = {4},
  pages        = {747-771},
  shortjournal = {SIAM J. Comput.},
  title        = {On approximating the number of $k$-cliques in sublinear time},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distributed local approximation algorithms for maximum
matching in graphs and hypergraphs. <em>SICOMP</em>, <em>49</em>(4),
711–746. (<a href="https://doi.org/10.1137/19M1279241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe approximation algorithms in Linial&#39;s classic LOCAL model of distributed computing to find maximum-weight matchings in a hypergraph of rank $r$. Our main result is a deterministic algorithm to generate a matching which is an $O(r)$-approximation to the maximum- weight matching, running in $\tilde O(r \log \Delta + \log^2 \Delta + \log^* n)$ rounds. (Here, the $\tilde O$ notation hides polyloglog $\Delta$ and polylog $r$ factors). This is based on a number of new derandomization techniques extending methods of Ghaffari, Harris, and Kuhn [On derandomizing local distributed algorithms, in Proceedings of the $59$th Annual IEEE Symposium on Foundations of Computer Science, 2018, pp. 662--673]. The first main application is to nearly optimal algorithms for the long-studied problem of maximum-weight graph matching. Specifically, we get a $(1+\epsilon)$-approximation algorithm using $\tilde O(\log \Delta / \epsilon^3 +$ polylog$(1/\epsilon, \log \log n))$ randomized time and $\tilde O(\log^2 \Delta / \epsilon^4 + \log^*n / \epsilon)$ deterministic time. The second application is a faster algorithm for hypergraph maximal matching, a versatile subroutine introduced in Ghaffari, Harris, and Kuhn [On derandomizing local distributed algorithms, in Proceedings of the $59$th Annual IEEE Symposium on Foundations of Computer Science, 2018, pp. 662--673] for a variety of local graph algorithms. This gives an algorithm for $(2 \Delta - 1)$-edge-list-coloring in $\tilde O(\log^2 \Delta \log n)$ rounds deterministically or $\tilde O( (\log \log n)^3 )$ rounds randomly. Another consequence (with additional optimizations) is an algorithm which generates an edge-orientation with out-degree at most $\lceil (1+\epsilon) \lambda \rceil$ for a graph of arboricity $\lambda$; for fixed $\epsilon$ this runs in $\tilde O(\log^6 n)$ rounds deterministically or $\tilde O(\log^3 n )$ rounds randomly.},
  archive      = {J_SICOMP},
  author       = {David G. Harris},
  doi          = {10.1137/19M1279241},
  journal      = {SIAM Journal on Computing},
  number       = {4},
  pages        = {711-746},
  shortjournal = {SIAM J. Comput.},
  title        = {Distributed local approximation algorithms for maximum matching in graphs and hypergraphs},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Algorithms for #BIS-hard problems on expander graphs.
<em>SICOMP</em>, <em>49</em>(4), 681–710. (<a
href="https://doi.org/10.1137/19M1286669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We give a fully polynomial-time approximation scheme (FPTAS) and an efficient sampling algorithm for the high-fugacity hard-core model on bounded-degree bipartite expander graphs and the low-temperature ferromagnetic Potts model on bounded-degree expander graphs. The results apply, for example, to random (bipartite) $\Delta$-regular graphs, for which no efficient algorithms were known for these problems (with the exception of the Ising model) in the nonuniqueness regime of the infinite $\Delta$-regular tree. We also find efficient counting and sampling algorithms for proper $q$-colorings of random $\Delta$-regular bipartite graphs when $q$ is sufficiently small as a function of $\Delta$.},
  archive      = {J_SICOMP},
  author       = {Matthew Jenssen and Peter Keevash and Will Perkins},
  doi          = {10.1137/19M1286669},
  journal      = {SIAM Journal on Computing},
  number       = {4},
  pages        = {681-710},
  shortjournal = {SIAM J. Comput.},
  title        = {Algorithms for #BIS-hard problems on expander graphs},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fair scheduling via iterative quasi-uniform sampling.
<em>SICOMP</em>, <em>49</em>(3), 658–680. (<a
href="https://doi.org/10.1137/18M1202451">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers minimizing the $\ell_k$-norms of flow time on a single machine offline using a preemptive scheduler for $k\geq 1$. The objective is ideal for optimizing jobs&#39; overall waiting times while simultaneously being fair to individual jobs. This work gives the first $O(1)$-approximation for the problem, improving upon the previous best $O( \log \log P)$-approximation by Bansal and Pruhs (FOCS 09 and SICOMP 14) where $P$ is the ratio of the maximum job size to the minimum. The main technical ingredient used in this work is a novel combination of quasi-uniform sampling and iterative rounding, which is of interest in its own right.},
  archive      = {J_SICOMP},
  author       = {Sungjin Im and Benjamin Moseley},
  doi          = {10.1137/18M1202451},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {658-680},
  shortjournal = {SIAM J. Comput.},
  title        = {Fair scheduling via iterative quasi-uniform sampling},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Turning big data into tiny data: Constant-size coresets for
<span class="math inline"><em>k</em></span>-means, PCA, and projective
clustering. <em>SICOMP</em>, <em>49</em>(3), 601–657. (<a
href="https://doi.org/10.1137/18M1209854">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop and analyze a method to reduce the size of a very large set of data points in a high-dimensional Euclidean space $\mathbb{R}^d$ to a small set of weighted points such that the result of a predetermined data analysis task on the reduced set is approximately the same as that for the original point set. For example, computing the first $k$ principal components of the reduced set will return approximately the first $k$ principal components of the original set or computing the centers of a $k$-means clustering on the reduced set will return an approximation for the original set. Such a reduced set is also known as a coreset. The main new feature of our construction is that the cardinality of the reduced set is independent of the dimension $d$ of the input space and that the sets are mergeable [P. K. Agarwal et al., Proceedings of the 31st ACM SIGMOD-SIGACT-SIGAI Symposium on Principals of Database Systems, 2012, pp. 23--34]. The latter property means that the union of two reduced sets is a reduced set for the union of the two original sets. It allows us to turn our methods into streaming or distributed algorithms using standard approaches. For problems such as $k$-means and subspace approximation the coreset sizes are also independent of the number of input points. Our method is based on data-dependently projecting the points on a low-dimensional subspace and reducing the cardinality of the points inside this subspace using known methods. The proposed approach works for a wide range of data analysis techniques including $k$-means clustering, principal component analysis, and subspace clustering. The main conceptual contribution is a new coreset definition that allows charging costs that appear for every solution to an additive constant.},
  archive      = {J_SICOMP},
  author       = {Dan Feldman and Melanie Schmidt and Christian Sohler},
  doi          = {10.1137/18M1209854},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {601-657},
  shortjournal = {SIAM J. Comput.},
  title        = {Turning big data into tiny data: Constant-size coresets for $k$-means, PCA, and projective clustering},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On locality-sensitive orderings and their applications.
<em>SICOMP</em>, <em>49</em>(3), 583–600. (<a
href="https://doi.org/10.1137/19M1246493">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For any constant $d$ and parameter $\varepsilon \in (0,1/2]$, we show the existence of (roughly) $1/\varepsilon^d$ orderings on the unit cube $[0,1)^d$ such that for any two points $p, q\in [0,1)^d$ close together under the Euclidean metric, there is a linear ordering in which all points between $p$ and $q$ in the ordering are “close” to $p$ or $q$. More precisely, the only points that could lie between $p$ and $q$ in the ordering are points with Euclidean distance at most $\varepsilon\left\| {p} - {q} \right\|$ from either $p$ or $q$. These orderings are extensions of the Z-order, and they can be efficiently computed. Functionally, the orderings can be thought of as a replacement to quadtrees and related structures (like well-separated pair decompositions). We use such orderings to obtain surprisingly simple algorithms for a number of basic problems in low-dimensional computational geometry, including (i) dynamic approximate bichromatic closest pair, (ii) dynamic spanners, (iii) dynamic approximate minimum spanning trees, (iv) static and dynamic fault-tolerant spanners, and (v) approximate nearest neighbor search.},
  archive      = {J_SICOMP},
  author       = {Timothy M. Chan and Sariel Har-Peled and Mitchell Jones},
  doi          = {10.1137/19M1246493},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {583-600},
  shortjournal = {SIAM J. Comput.},
  title        = {On locality-sensitive orderings and their applications},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Prophet inequalities made easy: Stochastic optimization by
pricing nonstochastic inputs. <em>SICOMP</em>, <em>49</em>(3), 540–582.
(<a href="https://doi.org/10.1137/20M1323850">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a general framework for stochastic online maximization problems with combinatorial feasibility constraints. The framework establishes prophet inequalities by constructing price-based online approximation algorithms, a natural extension of threshold algorithms for settings beyond binary selection. Our analysis takes the form of an extension theorem: we derive sufficient conditions on prices when all weights are known in advance, then prove that the resulting approximation guarantees extend directly to stochastic settings. Our framework unifies and simplifies much of the existing literature on prophet inequalities and posted price mechanisms and is used to derive new and improved results for combinatorial markets (with and without complements), multidimensional matroids, and sparse packing problems. Finally, we highlight a surprising connection between the smoothness framework for bounding the price of anarchy of mechanisms and our framework, and show that many smooth mechanisms can be recast as posted price mechanisms with comparable performance guarantees.},
  archive      = {J_SICOMP},
  author       = {Paul Dütting and Michal Feldman and Thomas Kesselheim and Brendan Lucier},
  doi          = {10.1137/20M1323850},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {540-582},
  shortjournal = {SIAM J. Comput.},
  title        = {Prophet inequalities made easy: Stochastic optimization by pricing nonstochastic inputs},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Distributed <span
class="math inline">(<em>Δ</em> + 1)</span>-coloring via ultrafast graph
shattering. <em>SICOMP</em>, <em>49</em>(3), 497–539. (<a
href="https://doi.org/10.1137/19M1249527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vertex coloring is one of the classic symmetry breaking problems studied in distributed computing. In this paper, we present a new algorithm for $(\Delta+1)$-list coloring in the randomized ${LOCAL}$ model running in $O({Det}_{\scriptscriptstyle d}(\operatorname{poly} \log n))=O(\operatorname{poly}(\log\log n))$ time, where ${Det}_{\scriptscriptstyle d}(n&#39;)$ is the deterministic complexity of $(\deg+1)$-list coloring on $n&#39;$-vertex graphs. (In this problem, each $v$ has a palette of size $\deg(v)+1$.) This improves upon a previous randomized algorithm of Harris, Schneider, and Su [J. ACM, 65 (2018), 19] with complexity $O(\sqrt{\log \Delta} + \log\log n + {Det}_{\scriptscriptstyle d}(\operatorname{poly}\log n)) = O(\sqrt{\log n})$. Unless $\Delta$ is small, it is also faster than the best known deterministic algorithm of Fraigniaud, Heinrich, and Kosowski [Proceedings of the 57th Annual IEEE Symposium on Foundations of Computer Science (FOCS), 2016] and Barenboim, Elkin, and Goldenberg [Proceedings of the 38th Annual ACM Symposium on Principles of Distributed Computing (PODC), 2018], with complexity $O(\sqrt{\Delta\log \Delta}\log^\ast \Delta + \log^* n)$. Our algorithm&#39;s running time is syntactically very similar to the $\Omega({Det}(\operatorname{poly}\log n))$ lower bound of Chang, Kopelowitz, and Pettie [SIAM J. Comput., 48 (2019), pp. 122--143], where ${Det}(n&#39;)$ is the deterministic complexity of $(\Delta+1)$-list coloring on $n&#39;$-vertex graphs. Although distributed coloring has been actively investigated for 30 years, the best deterministic algorithms for $(\deg+1)$- and $(\Delta+1)$-list coloring (that depend on $n&#39;$ but not $\Delta$) use a black-box application of network decompositions. The recent deterministic network decomposition algorithm of Rozhoň and Ghaffari [Proceedings of the 52nd Annual ACM SIGACT Symposium on Theory of Computing (STOC), 2020] implies that ${Det}_{\scriptscriptstyle d}(n&#39;)$ and ${Det}(n&#39;)$ are both $\operatorname{poly}(\log n&#39;)$. Whether they are asymptotically equal is an open problem.},
  archive      = {J_SICOMP},
  author       = {Yi-Jun Chang and Wenzheng Li and Seth Pettie},
  doi          = {10.1137/19M1249527},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {497-539},
  shortjournal = {SIAM J. Comput.},
  title        = {Distributed $(\Delta+1)$-coloring via ultrafast graph shattering},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spanoids—an abstraction of spanning structures, and a
barrier for LCCs. <em>SICOMP</em>, <em>49</em>(3), 465–496. (<a
href="https://doi.org/10.1137/19M124647X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a simple logical inference structure we call a “spanoid&quot; (generalizing the notion of a matroid), which captures well-studied problems in several areas. These include combinatorial geometry (point-line incidences), algebra (arrangements of hypersurfaces and ideals), statistical physics (bootstrap percolation), network theory (gossip/infection processes) and coding theory. We initiate a thorough investigation of spanoids, from computational and structural viewpoints, focusing on parameters relevant to the applications areas above and, in particular, to questions regarding locally correctable codes (LCCs). One central parameter we study is the “rank&quot; of a spanoid, extending the rank of a matroid and related to the dimension of codes. This leads to one main application of our work, establishing the first known barrier to improving the nearly 20-year old bound of Katz--Trevisan (KT) on the dimension of LCCs. On the one hand, we prove that the KT bound (and its more recent refinements) holds for the much more general setting of spanoid rank. On the other hand we show that there exist (random) spanoids whose rank matches these bounds. Thus, to significantly improve the known bounds one must step out of the spanoid framework. Another parameter we explore is the “functional rank&quot; of a spanoid, which captures the possibility of turning a given spanoid into an actual code. The question of the relationship between rank and functional rank is one of the main questions we raise as it may reveal new avenues for constructing new LCCs (perhaps even matching the KT bound). As a first step, we develop an entropy relaxation of functional rank to create a small constant gap and amplify it by tensoring to construct a spanoid whose functional rank is smaller than rank by a polynomial factor. This is evidence that the entropy method we develop can prove polynomially better bounds than KT-type methods on the dimension of LCCs. To facilitate the above results we also develop some basic structural results on spanoids including an equivalent formulation of spanoids as set systems and properties of spanoid products. We feel that given these initial findings and their motivations, the abstract study of spanoids merits further investigation. We leave plenty of concrete open problems and directions.},
  archive      = {J_SICOMP},
  author       = {Zeev Dvir and Sivakanth Gopi and Yuzhou Gu and Avi Wigderson},
  doi          = {10.1137/19M124647X},
  journal      = {SIAM Journal on Computing},
  number       = {3},
  pages        = {465-496},
  shortjournal = {SIAM J. Comput.},
  title        = {Spanoids---an abstraction of spanning structures, and a barrier for LCCs},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Finding cliques in social networks: A new distribution-free
model. <em>SICOMP</em>, <em>49</em>(2), 448–464. (<a
href="https://doi.org/10.1137/18M1210459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new distribution-free model of social networks. Our definitions are motivated by one of the most universal signatures of social networks, triadic closure---the property that pairs of vertices with common neighbors tend to be adjacent. Our most basic definition is that of a $c$-closed graph, where for every pair of vertices $u,v$ with at least $c$ common neighbors, $u$ and $v$ are adjacent. We study the classic problem of enumerating all maximal cliques, an important task in social network analysis. We prove that this problem is fixed-parameter tractable with respect to $c$ on $c$-closed graphs. Our results carry over to weakly $c$-closed graphs, which only require a vertex deletion ordering that avoids pairs of nonadjacent vertices with $c$ common neighbors. Numerical experiments show that well-studied social networks with thousands of vertices tend to be weakly $c$-closed for modest values of $c$.},
  archive      = {J_SICOMP},
  author       = {Jacob Fox and Tim Roughgarden and C. Seshadhri and Fan Wei and Nicole Wein},
  doi          = {10.1137/18M1210459},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {448-464},
  shortjournal = {SIAM J. Comput.},
  title        = {Finding cliques in social networks: A new distribution-free model},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The greedy spanner is existentially optimal.
<em>SICOMP</em>, <em>49</em>(2), 429–447. (<a
href="https://doi.org/10.1137/18M1210678">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The greedy spanner is arguably the simplest and most well-studied spanner construction. Experimental results demonstrate that it is at least as good as any other spanner construction in terms of both the size and weight parameters. However, a rigorous proof for this statement has remained elusive. In this work we fill in the theoretical gap via a surprisingly simple observation: The greedy spanner is existentially optimal (or existentially near-optimal) for several important graph families in terms of both size and weight. Roughly speaking, the greedy spanner is said to be existentially optimal (or near-optimal) for a graph family $\mathcal G$ if the worst performance of the greedy spanner over all graphs in $\mathcal G$ is just as good (or nearly as good) as the worst performance of an optimal spanner over all graphs in $\mathcal G$. Focusing on the weight parameter, the state-of-the-art spanner constructions for both general graphs (due to Chechik and Wulff-Nilsen [ACM Trans. Algorithms, 14 (2018), 33]) and doubling metrics (due to Gottlieb [Proceedings of the $56$th Annual IEEE Symposium on Foundations of Computer Science, 2015, pp. 759--772]) are complex. Plugging our observation into these results, we conclude that the greedy spanner achieves near-optimal weight guarantees for both general graphs and doubling metrics, thus resolving two longstanding conjectures in the area. Further, we observe that approximate-greedy spanners are existentially near-optimal as well. Consequently, we provide an $O(n \log n)$-time construction of $(1+\epsilon)$-spanners for doubling metrics with constant lightness and degree. Our construction improves Gottlieb&#39;s construction, whose runtime is $O(n \log^2 n)$ and whose number of edges and degree are unbounded, and, remarkably, it matches the state-of-the-art Euclidean result (due to Gudmundsson, Levcopoulos, and Narasimhan [SIAM J. Comput., 31 (2002), pp. 1479--1500]) in all of the involved parameters (up to dependencies on $\epsilon$ and the dimension).},
  archive      = {J_SICOMP},
  author       = {Arnold Filtser and Shay Solomon},
  doi          = {10.1137/18M1210678},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {429-447},
  shortjournal = {SIAM J. Comput.},
  title        = {The greedy spanner is existentially optimal},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An algorithmic proof of the lovász local lemma via
resampling oracles. <em>SICOMP</em>, <em>49</em>(2), 394–428. (<a
href="https://doi.org/10.1137/18M1167176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Lovász local lemma is a seminal result in probabilistic combinatorics. It gives a sufficient condition on a probability space and a collection of events for the existence of an outcome that simultaneously avoids all of those events. Finding such an outcome by an efficient algorithm has been an active research topic for decades. The breakthrough work of Moser [A constructive proof of the Lovász local lemma, in Proceedings of the ACM International Symposium on Theory of Computing, 2009, pp. 343--350] and Moser and Tardos [J. ACM, 57 (2010), 11] presented an efficient algorithm for a general setting primarily characterized by a product structure on the probability space. In this work we present an efficient algorithm for a much more general setting. Our main assumption is that there exist certain functions, called resampling oracles, that can be invoked to address the undesired occurrence of the events. We show that, in all scenarios to which the original Lovász local lemma applies, there exist resampling oracles, although they are not necessarily efficient. Nevertheless, for essentially all known applications of the Lovász local lemma and its generalizations, we have designed efficient resampling oracles. As an application of these techniques, we present a new result on packings of rainbow spanning trees.},
  archive      = {J_SICOMP},
  author       = {Nicholas J. A. Harvey and Jan Vondrák},
  doi          = {10.1137/18M1167176},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {394-428},
  shortjournal = {SIAM J. Comput.},
  title        = {An algorithmic proof of the lovász local lemma via resampling oracles},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Topology is irrelevant (in a dichotomy conjecture for
infinite domain constraint satisfaction problems). <em>SICOMP</em>,
<em>49</em>(2), 365–393. (<a
href="https://doi.org/10.1137/18M1216213">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tractability conjecture for finite domain constraint satisfaction problems (CSPs) stated that such CSPs are solvable in polynomial time whenever there is no natural reduction, in some precise technical sense, from the 3-SAT problem; otherwise, they are NP-complete. Its recent resolution draws on an algebraic characterization of the conjectured borderline: the CSP of a finite structure permits no natural reduction from 3-SAT if and only if the stabilizer of the polymorphism clone of the core of the structure satisfies some nontrivial system of identities, and such satisfaction is always witnessed by several specific nontrivial systems of identities which do not depend on the structure. The tractability conjecture has been generalized in the above formulation to a certain class of infinite domain CSPs, namely, CSPs of reducts of finitely bounded homogeneous structures. It was subsequently shown that the conjectured borderline between hardness and tractability, i.e., a natural reduction from 3-SAT, can be characterized for this class by a combination of algebraic and topological properties. However, it was not known whether the topological component is essential in this characterization. We provide a negative answer to this question by proving that the borderline is characterized by one specific algebraic identity, namely, the pseudo-Siggers identity $\alpha s(x,y,x,z,y,z) \approx \beta s(y,x,z,x,z,y)$. This accomplishes one of the steps of a proposed strategy for reducing the infinite domain CSP dichotomy conjecture to the finite case. Our main theorem is also of independent mathematical interest, characterizing a topological property of any $\omega$-categorical core structure (the existence of a continuous homomorphism of a stabilizer of its polymorphism clone to the projections) in purely algebraic terms (the failure of an identity as above).},
  archive      = {J_SICOMP},
  author       = {Libor Barto and Michael Pinsker},
  doi          = {10.1137/18M1216213},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {365-393},
  shortjournal = {SIAM J. Comput.},
  title        = {Topology is irrelevant (In a dichotomy conjecture for infinite domain constraint satisfaction problems)},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tight bounds for planar strongly connected steiner subgraph
with fixed number of terminals (and extensions). <em>SICOMP</em>,
<em>49</em>(2), 318–364. (<a
href="https://doi.org/10.1137/18M122371X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a vertex-weighted directed graph $G=(V,E)$ and a set $T={t_1, t_2, \ldots, t_k}$ of $k$ terminals, the objective of the Strongly Connected Steiner Subgraph (SCSS) problem is to find a vertex set $H\subseteq V$ of minimum weight such that $G[H]$ contains a $t_{i}\rightarrow t_j$ path for each $i\neq j$. The problem is NP-hard, but Feldman and Ruhl [SIAM J. Comput., 36 (2006), pp. 543--561] gave a novel $n^{O(k)}$ algorithm for the SCSS problem, where $n$ is the number of vertices in the graph and $k$ is the number of terminals. We explore how much easier the problem becomes on planar directed graphs. Our main algorithmic result is a $2^{O(k)}\cdot n^{O(\sqrt{k})}$ algorithm for planar SCSS, which is an improvement of a factor of $O(\sqrt{k})$ in the exponent over the algorithm of Feldman and Ruhl. Our main hardness result is a matching lower bound for our algorithm: we show that planar SCSS does not have an $f(k)\cdot n^{o(\sqrt{k})}$ algorithm for any computable function $f$, unless the exponential time hypothesis (ETH) fails. To obtain our algorithm, we first show combinatorially that there is a minimal solution whose treewidth is $O(\sqrt{k})$, and then use the dynamic-programming based algorithm for finding bounded-treewidth solutions due to Feldmann and Marx [The Complexity Landscape of Fixed-Parameter Directed Steiner Network Problems, preprint, ŭlhttps://arxiv.org/abs/1707.06808]. To obtain the lower bound matching the algorithm, we need a delicate construction of gadgets arranged in a gridlike fashion to tightly control the number of terminals in the created instance. The following additional results put our upper and lower bounds in context: our $2^{O(k)}\cdot n^{O(\sqrt{k})}$ algorithm for planar directed graphs can be generalized to graphs excluding a fixed minor. Additionally, we can obtain this running time for the problem of finding an optimal planar solution even if the input graph is not planar. In general graphs, we cannot hope for such a dramatic improvement over the $n^{O(k)}$ algorithm of Feldman and Ruhl: assuming ETH, SCSS in general graphs does not have an $f(k)\cdot n^{o(k/\log k)}$ algorithm for any computable function $f$. Feldman and Ruhl generalized their $n^{O(k)}$ algorithm to the more general Directed Steiner Network (DSN) problem; here the task is to find a subgraph of minimum weight such that for every source $s_i$ there is a path to the corresponding terminal $t_i$. We show that, assuming ETH, there is no $f(k)\cdot n^{o(k)}$ time algorithm for DSN on acyclic planar graphs. All our lower bounds hold for the integer weighted edge version, while the algorithm works for the more general unweighted vertex version.},
  archive      = {J_SICOMP},
  author       = {Rajesh H. Chitnis and Andreas E. Feldmann and MohammadTaghi HajiAghayi and Daniel Marx},
  doi          = {10.1137/18M122371X},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {318-364},
  shortjournal = {SIAM J. Comput.},
  title        = {Tight bounds for planar strongly connected steiner subgraph with fixed number of terminals (and extensions)},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust positioning patterns with low redundancy.
<em>SICOMP</em>, <em>49</em>(2), 284–317. (<a
href="https://doi.org/10.1137/19M1253472">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A robust positioning pattern is a large array that allows a mobile device to locate its position by reading a possibly corrupted small window around it. In this paper, we provide constructions of binary positioning patterns, equipped with efficient locating algorithms, that are robust to a constant number of errors and have redundancy within a constant factor of optimality. Furthermore, we modify our constructions to correct rank errors and obtain binary positioning patterns robust to any errors of rank less than a constant number. Additionally, we construct $q$-ary robust positioning sequences robust to a large number of errors, some of which have length attaining the upper bound. Our construction of binary positioning sequences that are robust to a constant number of errors has the least known redundancy among those explicit constructions with efficient locating algorithms. On the other hand, for binary robust positioning arrays, our construction is the first explicit construction whose redundancy is within a constant factor of optimality. The locating algorithms accompanying both constructions run in time cubic in sequence length or array dimension.},
  archive      = {J_SICOMP},
  author       = {Yeow Meng Chee and Duc Tu Dao and Han Mao Kiah and San Ling and Hengjia Wei},
  doi          = {10.1137/19M1253472},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {284-317},
  shortjournal = {SIAM J. Comput.},
  title        = {Robust positioning patterns with low redundancy},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Zero-knowledge proof systems for QMA. <em>SICOMP</em>,
<em>49</em>(2), 245–283. (<a
href="https://doi.org/10.1137/18M1193530">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prior work has established that all problems in NP admit classical zero-knowledge proof systems, and under reasonable hardness assumptions for quantum computations, these proof systems can be made secure against quantum attacks. We prove a result representing a further quantum generalization of this fact, which is that every problem in the complexity class QMA has a quantum zero-knowledge proof system. More specifically, assuming the existence of an unconditionally binding and quantum computationally concealing commitment scheme, we prove that every problem in the complexity class QMA has a quantum interactive proof system that is zero-knowledge with respect to efficient quantum computations. Our QMA proof system is sound against arbitrary quantum provers, but only requires an honest prover to perform polynomial-time quantum computations, provided that it holds a quantum witness for a given instance of the QMA problem under consideration. The proof system relies on a new variant of the QMA-complete local Hamiltonian problem in which the local terms are described by Clifford operations and standard basis measurements. We believe that the QMA-completeness of this problem may have other uses in quantum complexity.},
  archive      = {J_SICOMP},
  author       = {Anne Broadbent and Zhengfeng Ji and Fang Song and John Watrous},
  doi          = {10.1137/18M1193530},
  journal      = {SIAM Journal on Computing},
  number       = {2},
  pages        = {245-283},
  shortjournal = {SIAM J. Comput.},
  title        = {Zero-knowledge proof systems for QMA},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Communication complexity of discrete fair division.
<em>SICOMP</em>, <em>49</em>(1), 206–243. (<a
href="https://doi.org/10.1137/19M1244305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We initiate the study of the communication complexity of fair division with indivisible goods. We focus on some of the most well studied fairness notions (envy-freeness, proportionality, and approximations thereof) and valuation classes (submodular, subadditive, and unrestricted). We show that for more than two players (and any combination of other parameters), determining whether a fair allocation exists requires exponential communication (in the number of goods). For two players, tractability depends heavily on the specific combination of parameters, and most of the paper is focused on the two-player setting. Taken together, our results completely resolve whether the communication complexity of computing a fair allocation (or determining that none exists) is polynomial or exponential, for every combination of fairness notion, valuation class, and number of players, for both deterministic and randomized protocols.},
  archive      = {J_SICOMP},
  author       = {Benjamin Plaut and Tim Roughgarden},
  doi          = {10.1137/19M1244305},
  journal      = {SIAM Journal on Computing},
  number       = {1},
  pages        = {206-243},
  shortjournal = {SIAM J. Comput.},
  title        = {Communication complexity of discrete fair division},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Approximation limitations of pure dynamic programming.
<em>SICOMP</em>, <em>49</em>(1), 170–205. (<a
href="https://doi.org/10.1137/18M1196339">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We prove the first, even superpolynomial, lower bounds on the size of tropical (min,+) and (max,+) circuits approximating given optimization problems. Many classical dynamic programming (DP) algorithms for optimization problems are pure in that they only use the basic $\min$, $\max$, $+$ operations in their recursion equations. Tropical circuits constitute a rigorous mathematical model for this class of algorithms. An algorithmic consequence of our lower bounds for tropical circuits is that the approximation powers of pure DP algorithms and greedy algorithms are incomparable. That pure DP algorithms can hardly beat greedy in approximation is long known. New in this consequence is that the converse also holds.},
  archive      = {J_SICOMP},
  author       = {Stasys Jukna and Hannes Seiwert},
  doi          = {10.1137/18M1196339},
  journal      = {SIAM Journal on Computing},
  number       = {1},
  pages        = {170-205},
  shortjournal = {SIAM J. Comput.},
  title        = {Approximation limitations of pure dynamic programming},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The minimum euclidean-norm point in a convex polytope:
Wolfe’s combinatorial algorithm is exponential. <em>SICOMP</em>,
<em>49</em>(1), 138–169. (<a
href="https://doi.org/10.1137/18M1221072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The complexity of Philip Wolfe&#39;s method for the minimum Euclidean-norm point problem over a convex polytope has remained unknown since he proposed the method in 1974. The method is important because it is used as a subroutine for one of the most practical algorithms for submodular function minimization. We present the first example that Wolfe&#39;s method takes exponential time. Additionally, we improve previous results to show that linear programming reduces in strongly polynomial time to the minimum norm point problem over a simplex.},
  archive      = {J_SICOMP},
  author       = {Jesús A. De Loera and Jamie Haddock and Luis Rademacher},
  doi          = {10.1137/18M1221072},
  journal      = {SIAM Journal on Computing},
  number       = {1},
  pages        = {138-169},
  shortjournal = {SIAM J. Comput.},
  title        = {The minimum euclidean-norm point in a convex polytope: Wolfe&#39;s combinatorial algorithm is exponential},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sampling lower bounds: Boolean average-case and
permutations. <em>SICOMP</em>, <em>49</em>(1), 119–137. (<a
href="https://doi.org/10.1137/18M1198405">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that for every small AC$^{0}$ circuit $C:{0,1}^{\ell}\to{0,1}^{m}$ there exists a multiset $S$ of $2^{m-m^{\Omega(1)}}$ restrictions that preserve the output distribution of $C$ and, moreover, polarize min-entropy: the restriction of $C$ to any $r\in S$ either is constant or has polynomial min-entropy. This structural result is then applied to exhibit an explicit boolean function $h:{0,1}^{n}\to{0,1}$ such that for every small AC$^{0}$ circuit $C:{0,1}^{\ell}\to{0,1}^{n+1}$ the output distribution of $C$ for a uniform input has statistical distance exponentially close to $1/2$ from the distribution $(U,h(U))$ for $U$ uniform in ${0,1}^{n}$. Previous such “sampling lower bounds” either gave exponentially small statistical distance or applied to functions $h$ with large output length. We also show that the output distribution of a $d$-local map $f:[n]^{\ell}\to[n]^{n}$ for a uniform input has statistical distance at least $1-2\cdot\exp(-n/\log^{\exp(O(d))}n)$ from a uniform permutation of $[n]$. Here $d$-local means that each output symbol in $[n]={1,2,\ldots,n}$ depends only on $d$ of the $\ell$ input symbols in $[n]$. This separates AC$^{0}$ sampling from local, because small AC$^{0}$ circuits can sample almost uniform permutations. As an application, we prove that any cell-probe data structure for storing permutations $\pi$ of $n$ elements such that $\pi(i)$ can be retrieved with $d$ nonadaptive probes must use space $\ge\log_{2}n!+n/\log^{\exp(O(d))}n$.},
  archive      = {J_SICOMP},
  author       = {Emanuele Viola},
  doi          = {10.1137/18M1198405},
  journal      = {SIAM Journal on Computing},
  number       = {1},
  pages        = {119-137},
  shortjournal = {SIAM J. Comput.},
  title        = {Sampling lower bounds: Boolean average-case and permutations},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Supercritical space-width trade-offs for resolution.
<em>SICOMP</em>, <em>49</em>(1), 98–118. (<a
href="https://doi.org/10.1137/16M1109072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that there are CNF formulas which can be refuted in resolution in both small space and small width, but for which any small-width proof must have space exceeding by far the linear worst-case upper bound. This significantly strengthens the space-width trade-offs in [E. Ben-Sasson, SIAM J. Comput., 38 (2009), pp. 2511--2525], and provides one more example of trade-offs in the &quot;supercritical&quot; regime above worst case recently identified by [A.A. Razborov, J. ACM, 63 (2016), 16]. We obtain our results by using Razborov&#39;s new hardness condensation technique and combining it with the space lower bounds in [E. Ben-Sasson and J. Nordström, Short proofs may be spacious: An optimal separation of space and length in resolution, in Proceedings of the 49th Annual IEEE Symposium on Foundations of Computer Science (FOCS &#39;08), 2008, pp. 709--718].},
  archive      = {J_SICOMP},
  author       = {Christoph Berkholz and Jakob Nordström},
  doi          = {10.1137/16M1109072},
  journal      = {SIAM Journal on Computing},
  number       = {1},
  pages        = {98-118},
  shortjournal = {SIAM J. Comput.},
  title        = {Supercritical space-width trade-offs for resolution},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Approximate modularity revisited. <em>SICOMP</em>,
<em>49</em>(1), 67–97. (<a
href="https://doi.org/10.1137/18M1173873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Set functions with convenient properties (such as submodularity) appear in application areas of current interest, such as algorithmic game theory, and allow for improved optimization algorithms. It is natural to ask (e.g., in the context of data driven optimization) how robust such properties are, and whether small deviations from them can be tolerated. We consider two such questions in the important special case of linear set functions. One question that we address is whether any set function that approximately satisfies the modularity equation (linear functions satisfy the modularity equation exactly) is close to a linear function. The answer to this is positive (in a precise formal sense) as shown by Kalton and Roberts [Trans. Amer. Math. Soc., 278 (1983), pp. 803--816] (and further improved by Bondarenko, Prymak, and Radchenko [J. Math. Anal. Appl., 402 (2013), pp. 234--241]). We revisit their proof idea that is based on expander graphs and provide significantly stronger upper bounds by combining it with new techniques. Furthermore, we provide improved lower bounds for this problem. Another question that we address is that of how to learn a linear function $h$ that is close to an approximately linear function $f$, while querying the value of $f$ on only a small number of sets. We present a deterministic algorithm that makes only linearly many (in the number of items) nonadaptive queries, and thus improve upon a previous algorithm of Chierichetti, Das, Dasgupta, and Kumar [Proceedings of the 56th Symposium on Foundations of Computer Science, 2015, pp. 1143--1162] that is randomized and makes more than a quadratic number of queries. Our learning algorithm is based on the Hadamard transform.},
  archive      = {J_SICOMP},
  author       = {Uriel Feige and Michal Feldman and Inbal Talgam-Cohen},
  doi          = {10.1137/18M1173873},
  journal      = {SIAM Journal on Computing},
  number       = {1},
  pages        = {67-97},
  shortjournal = {SIAM J. Comput.},
  title        = {Approximate modularity revisited},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hallucination helps: Energy efficient virtual circuit
routing. <em>SICOMP</em>, <em>49</em>(1), 37–66. (<a
href="https://doi.org/10.1137/18M1228591">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider virtual circuit routing protocols with an objective of minimizing energy in a network of components that are speed scalable, and that may be shut down when idle. We assume the standard model for component power: the power consumed by a component with load (speed) $s$ is $\sigma+ s^\alpha$, where $\sigma$ is the static power and the exponent $\alpha&gt;1$. We obtain a very simple $O(\log^\alpha k)$-approximation algorithm for multicommodity routing, where $k$ is the number of demand pairs. This improves upon previous results by several logarithmic factors. The key step in our algorithm is a random sampling technique that we call hallucination, which is reminiscent of the sample-augment framework for buy-at-bulk problems, and sampling in cut-sparsification algorithms. We also consider the online setting of the problem, where demand pairs arrive over time. We show that our offline algorithm naturally extends to the online setting, and obtain a randomized competitive ratio of $\tilde{O}( \log^{3\alpha + 1} k)$, which is the first nontrivial bound. The analysis of this algorithm involves the study of priority multicommodity flows, where edges and demand-pairs have priorities and each demand-pair must route its flow only on edges of lower priority. We establish a polylogarithmic flow-cut gap for these priority flows, which we believe is of independent interest. Finally, we show how our technique can be used to achieve a randomized $( O(\log m), O(\log^2 m))$ bicriteria competitive algorithm for the uniform capacitated network design problem, where $m$ is the number of edges. Here, every edge has a cost $c_e$ and uniform capacity $q$, and the goal is to choose the minimum cost subgraph that can support the given multicommodity demand. This is the first online algorithm for this problem. In fact, our approach also improves prior results in the offline setting by several logarithmic factors.},
  archive      = {J_SICOMP},
  author       = {Antonios Antoniadis and Sungjin Im and Ravishankar Krishnaswamy and Benjamin Moseley and Viswanath Nagarajan and Kirk Pruhs and Clifford Stein},
  doi          = {10.1137/18M1228591},
  journal      = {SIAM Journal on Computing},
  number       = {1},
  pages        = {37-66},
  shortjournal = {SIAM J. Comput.},
  title        = {Hallucination helps: Energy efficient virtual circuit routing},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Local flow partitioning for faster edge connectivity.
<em>SICOMP</em>, <em>49</em>(1), 1–36. (<a
href="https://doi.org/10.1137/18M1180335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the problem of computing a minimum cut in a simple, undirected graph and give a deterministic $O(m \log^2 n \log\log^2 n)$ time algorithm. This improves on both the best previously known deterministic running time of $O(m \log^{12} n)$ (Kawarabayashi and Thorup [J. ACM, 66 (2018), 4]) and the best previously known randomized running time of $O(m \log^{3} n)$ (Karger [J. ACM, 47 (2000), pp. 46--76]) for this problem, though Karger&#39;s algorithm can be further applied to weighted graphs. Moreover, our result extends to balanced directed graphs, where the balance of a directed graph captures how close the graph is to being Eulerian. Our approach is using the Kawarabayashi and Thorup graph compression technique, which repeatedly finds low conductance cuts. To find these cuts they use a diffusion-based local algorithm. We use instead a flow-based local algorithm and suitably adjust their framework to work with our flow-based subroutine. Both flow- and diffusion-based methods have a long history of being applied to finding low conductance cuts. Diffusion algorithms have several variants that are naturally local, while it is more complicated to make flow methods local. Some prior work has proven nice properties for local flow-based algorithms with respect to improving or cleaning up low conductance cuts. Our flow subroutine, however, is the first that both is local and produces low conductance cuts. Thus, it may be of independent interest.},
  archive      = {J_SICOMP},
  author       = {Monika Henzinger and Satish Rao and Di Wang},
  doi          = {10.1137/18M1180335},
  journal      = {SIAM Journal on Computing},
  number       = {1},
  pages        = {1-36},
  shortjournal = {SIAM J. Comput.},
  title        = {Local flow partitioning for faster edge connectivity},
  volume       = {49},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
