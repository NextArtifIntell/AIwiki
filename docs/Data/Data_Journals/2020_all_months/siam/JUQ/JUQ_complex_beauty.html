<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>JUQ_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="juq---52">JUQ - 52</h2>
<ul>
<li><details>
<summary>
(2020). An information criterion for choosing observation locations
in data assimilation and prediction. <em>JUQ</em>, <em>8</em>(4),
1548–1573. (<a href="https://doi.org/10.1137/19M1278235">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An information criterion is proposed for determining the observation locations based on maximizing the information gain in the posterior distribution from data assimilation. It is applied to developing an off-line strategy using the long-term statistics from Eulerian observations and an online ensemble strategy for determining the initial locations of Lagrangian tracers. Decompose the total information gain into a signal and a dispersion part, accounting for the posterior mean and posterior uncertainty, respectively. Despite the information criterion being a nonlinear function of the posterior estimates and the intrinsic nonlinearity in the Lagrangian data assimilation, the total information gain can be solved via closed analytic formulae. The signal part is given by the solution of a set of Sylvester equations, and the dispersion part is associated with a Riccati equation. Numerical experiments based on a multiscale compressible rotating shallow water equation show that the information gain using the optimal strategy and that using the random assignment increase as a linear and a logarithm function of the number of the Eulerian observations $L$, respectively, until $L$ approaches the model degree of freedom, at which time the difference between the two information gains reaches the maximum. Afterwards, both the information gains are dominated by the dispersion part and increase as a function of $\ln{L}$. On the other hand, the optimal initial locations of the Lagrangian tracers resulting from the ensemble based strategy also succeed in improving the skill of recovering complex flow patterns and extreme events associated with single random realizations of the model.},
  archive      = {J_JUQ},
  author       = {Nan Chen},
  doi          = {10.1137/19M1278235},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1548-1573},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {An information criterion for choosing observation locations in data assimilation and prediction},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the improved rates of convergence for matérn-type kernel
ridge regression with application to calibration of computer models.
<em>JUQ</em>, <em>8</em>(4), 1522–1547. (<a
href="https://doi.org/10.1137/19M1304222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kernel ridge regression is an important nonparametric method for estimating smooth functions. We introduce a new set of conditions under which the actual rates of convergence of the kernel ridge regression estimator under both the $L_2$ norm and the norm of the reproducing kernel Hilbert space exceed the standard minimax rates. An application of this theory leads to a new understanding of the Kennedy--O&#39;Hagan approach [J. R. Stat. Soc. Ser. B. Stat. Methodol., 63 (2001), pp. 425--464] for calibrating model parameters of computer simulation. We prove that, under certain conditions, the Kennedy--O&#39;Hagan calibration estimator with a known covariance function converges to the minimizer of the norm of the residual function in the reproducing kernel Hilbert space.},
  archive      = {J_JUQ},
  author       = {Rui Tuo and Yan Wang and C. F. Jeff Wu},
  doi          = {10.1137/19M1304222},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1522-1547},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {On the improved rates of convergence for matérn-type kernel ridge regression with application to calibration of computer models},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sensitivity analysis of burgers’ equation with shocks.
<em>JUQ</em>, <em>8</em>(4), 1493–1521. (<a
href="https://doi.org/10.1137/18M1211763">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The generalized polynomial chaos (gPC) method has been extensively used in uncertainty quantification problems where equations contain random variables. For gPC to achieve high accuracy, PDE solutions need to have high regularity in the random space, but this is what hyperbolic type problems cannot provide. We provide a counterargument in this paper and show that even though the solution profile develops singularities in the random space, which destroys the spectral accuracy of gPC, the physical quantities (such as the shock emergence time, the shock location, and the shock strength) are all smooth functions of the uncertainties coming from both initial data and the wave speed. With proper shifting, the solution&#39;s polynomial interpolation approximates the real solution accurately, and the error decays as the order of the polynomial increases. Therefore this work provides a new perspective to “quantify uncertainties&quot; and significantly improves the accuracy of the gPC method with a slight reformulation. We use the Burgers&#39; equation as an example for thorough analysis, and the analysis could be extended to general conservation laws with convex fluxes.},
  archive      = {J_JUQ},
  author       = {Qin Li and Jian-Guo Liu and Ruiwen Shu},
  doi          = {10.1137/18M1211763},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1493-1521},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Sensitivity analysis of burgers&#39; equation with shocks},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A particle filter for stochastic advection by lie transport:
A case study for the damped and forced incompressible two-dimensional
euler equation. <em>JUQ</em>, <em>8</em>(4), 1446–1492. (<a
href="https://doi.org/10.1137/19M1277606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we combine a stochastic model reduction with a particle filter augmented with tempering and jittering, and apply the combined algorithm to a damped and forced incompressible two-dimensional Euler dynamics defined on a simply connected bounded domain. We show that using the combined algorithm, we are able to assimilate data from a reference system state (the “truth&#39;&#39;) modeled by a highly resolved numerical solution of the flow that has roughly $3.1 \times 10^6$ degrees of freedom, into a stochastic system having two orders of magnitude less degrees of freedom, which is able to approximate the true state reasonably accurately for five large-scale eddy turnover times, using modest computational hardware. The model reduction is performed through the introduction of a stochastic advection by Lie transport (SALT) model as the signal on a coarser resolution. The SALT approach was introduced as a general theory using a geometric mechanics framework from Holm [Proc. A, 471 (2015)]. This work follows on the numerical implementation for SALT presented by Cotter et al. [SIAM Multiscale Model. Simul., 17 (2019), pp. 192--232] for the flow in consideration. The model reduction is substantial: the reduced SALT model has $4.9 \times 10^4$ degrees of freedom. Results from reliability tests on the assimilated system are also presented.},
  archive      = {J_JUQ},
  author       = {Colin Cotter and Dan Crisan and Darryl D. Holm and Wei Pan and Igor Shevchenko},
  doi          = {10.1137/19M1277606},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1446-1492},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A particle filter for stochastic advection by lie transport: A case study for the damped and forced incompressible two-dimensional euler equation},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Kernel methods for bayesian elliptic inverse problems on
manifolds. <em>JUQ</em>, <em>8</em>(4), 1414–1445. (<a
href="https://doi.org/10.1137/19M1295222">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the formulation and implementation of Bayesian inverse problems to learn input parameters of partial differential equations (PDEs) defined on manifolds. Specifically, we study the inverse problem of determining the diffusion coefficient of a second-order elliptic PDE on a closed manifold from noisy measurements of the solution. Inspired by manifold learning techniques, we approximate the elliptic differential operator with a kernel-based integral operator that can be discretized via Monte Carlo without reference to the Riemannian metric. The resulting computational method is mesh-free and easy to implement, and can be applied without full knowledge of the underlying manifold, provided that a point cloud of manifold samples is available. We adopt a Bayesian perspective to the inverse problem, and establish an upper bound on the total variation distance between the true posterior and an approximate posterior defined with the kernel forward map. Supporting numerical results show the effectiveness of the proposed methodology.},
  archive      = {J_JUQ},
  author       = {John Harlim and Daniel Sanz-Alonso and Ruiyi Yang},
  doi          = {10.1137/19M1295222},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1414-1445},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Kernel methods for bayesian elliptic inverse problems on manifolds},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Transport map accelerated adaptive importance sampling, and
application to inverse problems arising from multiscale stochastic
reaction networks. <em>JUQ</em>, <em>8</em>(4), 1383–1413. (<a
href="https://doi.org/10.1137/19M1239416">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many applications, Bayesian inverse problems can give rise to probability distributions which contain complexities due to the Hessian varying greatly across parameter space. This complexity often manifests itself as lower-dimensional manifolds on which the likelihood function is invariant, or varies very little. This can be due to trying to infer unobservable parameters, or due to sloppiness in the model which is being used to describe the data. In such a situation, standard sampling methods for characterizing the posterior distribution, which do not incorporate information about this structure, will be highly inefficient. In this paper, we seek to develop an approach to tackle this problem when using adaptive importance sampling methods by employing optimal transport maps to simplify posterior distributions which are concentrated on lower-dimensional manifolds. This approach is applicable to a whole range of problems for which Monte Carlo Markov chain methods mix slowly. We demonstrate the approach by considering inverse problems arising from partially observed stochastic reaction networks. In particular, we consider systems which exhibit multiscale behavior, but for which only the slow variables in the system are observable. We demonstrate that certain multiscale approximations lead to more consistent approximations of the posterior than others. The use of optimal transport maps stabilizes the ensemble transform adaptive importance sampling method and allows for efficient sampling with smaller ensemble sizes. This approach allows us to take advantage of the large increases of efficiency when using adaptive importance sampling methods for previously intractable Bayesian inverse problems with complex posterior structure.},
  archive      = {J_JUQ},
  author       = {Simon L. Cotter and Ioannis G. Kevrekidis and Paul T. Russell},
  doi          = {10.1137/19M1239416},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1383-1413},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Transport map accelerated adaptive importance sampling, and application to inverse problems arising from multiscale stochastic reaction networks},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Objective bayesian analysis of a cokriging model for
hierarchical multifidelity codes. <em>JUQ</em>, <em>8</em>(4),
1358–1382. (<a href="https://doi.org/10.1137/19M1289893">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autoregressive cokriging models have been widely used to emulate multiple computer models with different levels of fidelity. The dependence structures are modeled via Gaussian processes at each level of fidelity, where covariance structures are often parameterized up to a few parameters. The predictive distributions typically required intensive Monte Carlo approximations in previous works. This article derives new closed-form formulas to compute the means and variances of predictive distributions in autoregressive cokriging models that only depend on correlation parameters. For parameter estimation, we consider objective Bayesian analysis of such autoregressive cokriging models. We show that common choices of prior distributions, such as the constant prior and inverse correlation prior, typically lead to improper posteriors. We also develop several objective priors such as the independent reference prior and the independent Jeffreys prior that are shown to yield proper posterior distributions. This development is illustrated with a borehole function in an eight-dimensional input space and applied to an engineering application in a six-dimensional input space.},
  archive      = {J_JUQ},
  author       = {Pulong Ma},
  doi          = {10.1137/19M1289893},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1358-1382},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Objective bayesian analysis of a cokriging model for hierarchical multifidelity codes},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Including a nugget effect in lifted brownian covariance
models. <em>JUQ</em>, <em>8</em>(4), 1338–1357. (<a
href="https://doi.org/10.1137/19M1255252">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Kriging is widely used for modeling the response surface in computer simulations in science and engineering. A Brownian integrated (BI) covariance model and its variant, the lifted Brownian (LB) covariance model, were recently proposed as the underlying random field models for kriging, and they were shown to have attractive properties for modeling deterministic computer experiment data. With no nugget effect, kriging models will perfectly interpolate the response data, which is usually desirable for deterministic simulations. However, it is necessary to include a nugget effect when modeling stochastic simulations or when combining simulation with physical experimental data, and inclusion of a nugget effect is often beneficial, even with deterministic simulations, to avoid numerical problems. This is challenging for the LB covariance, which requires translation based on a perfectly observed response value. In this paper, we introduce a novel approach for including a nugget effect in LB covariance models in a manner that preserves their desirable properties. We also derive a number of important theoretical results, including invariance to the choice of translation point and Bayesian connections.},
  archive      = {J_JUQ},
  author       = {Yang Yu and Ning Zhang and Daniel W. Apley and Wenxin Jiang},
  doi          = {10.1137/19M1255252},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1338-1357},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Including a nugget effect in lifted brownian covariance models},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Convergence of gaussian process regression with estimated
hyper-parameters and applications in bayesian inverse problems.
<em>JUQ</em>, <em>8</em>(4), 1310–1337. (<a
href="https://doi.org/10.1137/19M1284816">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work is concerned with the convergence of Gaussian process regression. A particular focus is on hierarchical Gaussian process regression, where hyper-parameters appearing in the mean and covariance structure of the Gaussian process emulator are a-priori unknown and are learned from the data, along with the posterior mean and covariance. We work in the framework of empirical Bayes, where a point estimate of the hyper-parameters is computed, using the data, and then used within the standard Gaussian process prior to posterior update. We provide a convergence analysis that (i) holds for a given, deterministic function $f$ to be emulated, and (ii) shows that convergence of Gaussian process regression is unaffected by the additional learning of hyper-parameters from data and is guaranteed in a wide range of scenarios. As the primary motivation for the work is the use of Gaussian process regression to approximate the data likelihood in Bayesian inverse problems, we provide a bound on the error introduced in the Bayesian posterior distribution in this context.},
  archive      = {J_JUQ},
  author       = {Aretha L. Teckentrup},
  doi          = {10.1137/19M1284816},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1310-1337},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Convergence of gaussian process regression with estimated hyper-parameters and applications in bayesian inverse problems},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Dealing with measurement uncertainties as nuisance
parameters in bayesian model calibration. <em>JUQ</em>, <em>8</em>(4),
1287–1309. (<a href="https://doi.org/10.1137/19M1283707">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the presence of model discrepancy, the calibration of physics-based models for physical parameter inference is a challenging problem. Lack of identifiability between calibration parameters and model discrepancy requires additional identifiability constraints to be placed on the model discrepancy to obtain unique physical parameter estimates. If these assumptions are violated, the inference for the calibration parameters can be systematically biased. In many applications, such as in dynamic material property experiments, many of the calibration inputs refer to measurement uncertainties. In this setting, we develop a metric for identifying overfitting of these measurement uncertainties, propose a prior capable of reducing this overfitting, and show how this leads to a diagnostic tool for validation of physical parameter inference. The approach is demonstrated for a benchmark example and applied for a material property application to perform inference on the equation of state parameters of tantalum.},
  archive      = {J_JUQ},
  author       = {Kellin Rumsey and Gabriel Huerta and Justin Brown and Lauren Hund},
  doi          = {10.1137/19M1283707},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1287-1309},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Dealing with measurement uncertainties as nuisance parameters in bayesian model calibration},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Compressed principal component analysis of non-gaussian
vectors. <em>JUQ</em>, <em>8</em>(4), 1261–1286. (<a
href="https://doi.org/10.1137/20M1322029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A novel approximate representation of non-Gaussian random vectors is introduced and validated, which can be viewed as a compressed principal component analysis (CPCA). This representation relies on the eigenvectors of the covariance matrix obtained as in a principal component analysis (PCA) but expresses the random vector as a linear combination of a random sample of $N$ of these eigenvectors. In this model, the indices of these eigenvectors are independent discrete random variables with probabilities proportional to the corresponding eigenvalues. Moreover, the coefficients of the linear combination are zero mean unit variance random variables. Under these conditions, it is first shown that the covariance matrix of this CPCA matches exactly its PCA counterpart independently of the value of $N$. Next, it is also shown that the distribution of the random coefficients can be selected, without loss of generality, to be a symmetric function. Then, to represent the vector of these coefficients, a novel set of symmetric vector-valued multidimensional polynomials of the canonical Gaussian random vector is derived. Interestingly, it is noted that the number of such polynomials is only slowly growing with the maximum polynomial order, thereby providing a framework for a compact approximation of the target random vector. The identification of the deterministic parameters of the expansion of the random coefficients on these symmetric vector-valued multidimensional polynomials is addressed next. Finally, an example of application is provided that demonstrates the good matching of the distributions of the elements of the target random vector and its approximation with only a very limited number of parameters.},
  archive      = {J_JUQ},
  author       = {Marc Mignolet and Christian Soize},
  doi          = {10.1137/20M1322029},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {4},
  pages        = {1261-1286},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Compressed principal component analysis of non-gaussian vectors},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multilevel monte carlo estimation of the expected value of
sample information. <em>JUQ</em>, <em>8</em>(3), 1236–1259. (<a
href="https://doi.org/10.1137/19M1284981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study Monte Carlo estimation of the expected value of sample information (EVSI), which measures the expected benefit of gaining additional information for decision making under uncertainty. EVSI is defined as a nested expectation in which an outer expectation is taken with respect to one random variable $Y$ and an inner conditional expectation with respect to the other random variable $\theta$. Although the nested (Markov chain) Monte Carlo estimator has been often used in this context, a root-mean-square accuracy of $\varepsilon$ is achieved notoriously at a cost of $O(\varepsilon^{-2-1/\alpha})$, where $\alpha$ denotes the order of convergence of the bias and is typically between $1/2$ and $1$. In this article we propose a novel efficient Monte Carlo estimator of EVSI by applying a multilevel Monte Carlo (MLMC) method. Instead of fixing the number of inner samples for $\theta$ as done in the nested Monte Carlo estimator, we consider a geometric progression on the number of inner samples, which yields a hierarchy of estimators on the inner conditional expectation with increasing approximation levels. Based on an elementary telescoping sum, our MLMC estimator is given by a sum of the Monte Carlo estimates of the differences between successive approximation levels on the inner conditional expectation. We show, under a set of assumptions on decision and information models, that successive approximation levels are tightly coupled, which directly proves that our MLMC estimator improves the necessary computational cost to optimal $O(\varepsilon^{-2})$. Numerical experiments confirm the considerable computational savings as compared to the nested Monte Carlo estimator.},
  archive      = {J_JUQ},
  author       = {Tomohiko Hironaka and Michael B. Giles and Takashi Goda and Howard Thom},
  doi          = {10.1137/19M1284981},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {1236-1259},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Multilevel monte carlo estimation of the expected value of sample information},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A defensive marginal particle filtering method for data
assimilation. <em>JUQ</em>, <em>8</em>(3), 1215–1235. (<a
href="https://doi.org/10.1137/19M1237430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Particle filtering (PF) is an often used method to estimate the states of dynamical systems. A major limitation of the standard PF method is that the dimensionality of the state space increases as the time proceeds and eventually may cause degeneracy of the algorithm. A possible approach to alleviate the degeneracy issue is to compute the marginal posterior distribution at each time step, which leads to the so-called marginal PF method. A key issue in the marginal PF method is to construct a good sampling distribution in the marginal space. When the posterior distribution is close to Gaussian, the ensemble Kalman filter (EnKF) method can usually provide a good sampling distribution; however the EnKF approximation may fail completely when the posterior is strongly non-Gaussian. In this work we propose for modest dimensional filtering problems a defensive marginal PF algorithm which constructs a sampling distribution in the marginal space by combining the standard PF and the EnKF approximation using a multiple importance sampling (MIS) scheme. An important feature of the proposed algorithm is that it can automatically adjust the relative weight of the PF and the EnKF components in the MIS scheme in each step, according to how non-Gaussian the posterior is. With numerical examples we demonstrate that the proposed method can perform well regardless of whether the posteriors can be well approximated by Gaussian.},
  archive      = {J_JUQ},
  author       = {Linjie Wen and Jiangqi Wu and Linjun Lu and Jinglai Li},
  doi          = {10.1137/19M1237430},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {1215-1235},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A defensive marginal particle filtering method for data assimilation},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An adaptive stochastic galerkin tensor train discretization
for randomly perturbed domains. <em>JUQ</em>, <em>8</em>(3), 1189–1214.
(<a href="https://doi.org/10.1137/19M1246080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A linear PDE problem for randomly perturbed domains is considered in an adaptive Galerkin framework. The perturbation of the domain&#39;s boundary is described by a vector valued random field depending on a countable number of random variables in an affine way. The corresponding Karhunen--Loève expansion is approximated by the pivoted Cholesky decomposition based on a prescribed covariance function. The examined high dimensional Galerkin system follows from the domain mapping approach, transferring the randomness from the domain to the diffusion coefficient and the forcing. In order to make this computationally feasible, the representation makes use of the modern tensor train format for the implicit compression of the problem. Moreover, an a posteriori error estimator is presented, which allows for the problem-dependent iterative refinement of all discretization parameters and the assessment of the achieved error reduction. The proposed approach is demonstrated in numerical benchmark problems.},
  archive      = {J_JUQ},
  author       = {Martin Eigel and Manuel Marschall and Michael Multerer},
  doi          = {10.1137/19M1246080},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {1189-1214},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {An adaptive stochastic galerkin tensor train discretization for randomly perturbed domains},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stratification as a general variance reduction method for
markov chain monte carlo. <em>JUQ</em>, <em>8</em>(3), 1139–1188. (<a
href="https://doi.org/10.1137/18M122964X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The eigenvector method for umbrella sampling (EMUS) [E. H. Thiede et al., J. Chem. Phys., 145 (2016), 084115] belongs to a popular class of methods in statistical mechanics which adapt the principle of stratified survey sampling to the computation of free energies. We develop a detailed theoretical analysis of EMUS. Based on this analysis, we show that EMUS is an efficient general method for computing averages over arbitrary target distributions. In particular, we show that EMUS can be dramatically more efficient than direct MCMC when the target distribution is multimodal or when the goal is to compute tail probabilities. To illustrate these theoretical results, we present a tutorial application of the method to a problem from Bayesian statistics.},
  archive      = {J_JUQ},
  author       = {Aaron R. Dinner and Erik H. Thiede and Brian Van Koten and Jonathan Weare},
  doi          = {10.1137/18M122964X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {1139-1188},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Stratification as a general variance reduction method for markov chain monte carlo},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Resource-constrained model selection for uncertainty
propagation and data assimilation. <em>JUQ</em>, <em>8</em>(3),
1118–1138. (<a href="https://doi.org/10.1137/19M1263376">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {All observable phenomena can be described by alternative mathematical models, which vary in their fidelity and computational cost. Selection of an appropriate model involves a tradeoff between computational cost and representational accuracy. Ubiquitous uncertainty (randomness) in model parameters and forcings, and assimilation of observations of the system states into predictions, complicate the model selection problem. We present a framework for analysis of the impact of data assimilation on cost-constrained model selection. The framework relies on the definitions of cost and accuracy functions in the context of data assimilation for multifidelity models with uncertain (random) coefficients. It contains an estimate of error bounds for a system&#39;s state prediction obtained by assimilating data into a model via an ensemble Kalman filter. This estimate is given in terms of model error, sampling error, and data error. Two examples illustrating the applicability of our model selection method are provided. The first example deals with an ordinary differential equation, for which a sequence of lower-fidelity models is constructed by progressively increasing the time step used in its discretization. The second example comprises the viscous Burgers equation as the high-fidelity model and a linear advection-diffusion equation as its low-fidelity counterpart.},
  archive      = {J_JUQ},
  author       = {Lun Yang and Peng Wang and Daniel M. Tartakovsky},
  doi          = {10.1137/19M1263376},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {1118-1138},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Resource-constrained model selection for uncertainty propagation and data assimilation},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Diffusion map-based algorithm for gain function
approximation in the feedback particle filter. <em>JUQ</em>,
<em>8</em>(3), 1090–1117. (<a
href="https://doi.org/10.1137/19M124513X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Feedback particle filter (FPF) is a numerical algorithm to approximate the solution of the nonlinear filtering problem in continuous-time settings. In any numerical implementation of the FPF algorithm, the main challenge is to numerically approximate the so-called gain function. A numerical algorithm for gain function approximation is the subject of this paper. The exact gain function is the solution of a Poisson equation involving a probability-weighted Laplacian $\Delta_\rho$. The numerical problem is to approximate this solution using only finitely many particles sampled from the probability distribution $\rho$. A diffusion map-based algorithm was proposed by the authors in prior works [A. Taghvaei and P. G. Mehta, Gain function approximation in the feedback particle filter, in 2016 IEEE 55th Conference on Decision and Control (CDC), IEEE, 2016, pp. 5446--5452], [A. Taghvaei, P. G. Mehta, and S. P. Meyn, Error estimates for the kernel gain function approximation in the feedback particle filter, in American Control Conference (ACC), IEEE, 2017, pp. 4576--4582] to solve this problem. The algorithm is named as such because it involves, as an intermediate step, a diffusion map approximation of the exact semigroup $e^{\Delta_\rho}$. The original contribution of this paper is to carry out a rigorous error analysis of the diffusion map-based algorithm. The error is shown to include two components: bias and variance. The bias results from the diffusion map approximation of the exact semigroup. The variance arises because of finite sample size. Scalings and upper bounds are derived for bias and variance. These bounds are then illustrated with numerical experiments that serve to emphasize the effects of problem dimension and sample size. The proposed algorithm is applied to two filtering examples and comparisons provided with the sequential importance resampling (SIR) particle filter.},
  archive      = {J_JUQ},
  author       = {Amirhossein Taghvaei and Prashant G. Mehta and Sean P. Meyn},
  doi          = {10.1137/19M124513X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {1090-1117},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Diffusion map-based algorithm for gain function approximation in the feedback particle filter},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Uncertainty quantification for stochastic approximation
limits using chaos expansion. <em>JUQ</em>, <em>8</em>(3), 1061–1089.
(<a href="https://doi.org/10.1137/18M1178517">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The uncertainty quantification for the limit of a stochastic approximation (SA) algorithm is analyzed. In our setup, this limit $\phi^{\star}$ is defined as a zero of an intractable function and is modeled as uncertain through a parameter $\theta$. We aim at deriving the function $\phi^{\star}$, as well as the probabilistic distribution of $\phi^{\star}(\theta)$ given a probability distribution $\pi$ for $\theta$. We introduce the so-called uncertainty quantification for SA (UQSA) algorithm, an SA algorithm in increasing dimension for computing the basis coefficients of a chaos expansion of $\theta \mapsto \phi^{\star}(\theta)$ on an orthogonal basis of a suitable Hilbert space. UQSA, run with a finite number of iterations $K$, returns a finite set of coefficients, providing an approximation $\widehat{\phi^{\star}_K}(\cdot)$ of $\phi^{\star}(\cdot)$. We establish the almost-sure and $L^p$-convergences in the Hilbert space of the sequence of functions $\widehat{\phi^{\star}_K}(\cdot)$ when the number of iterations $K$ tends to infinity. This is done under mild, tractable conditions, uncovered by the existing literature for convergence analysis of infinite dimensional SA algorithms. For a suitable choice of the Hilbert basis, the algorithm also provides an approximation of the expectation, of the variance-covariance matrix, and of higher order moments of the quantity $\widehat{\phi^{\star}_K}(\theta)$ when $\theta$ is random with distribution $\pi$. UQSA is illustrated and the role of its design parameters is discussed numerically.},
  archive      = {J_JUQ},
  author       = {S. Crépey and G. Fort and E. Gobet and U. Stazhynski},
  doi          = {10.1137/18M1178517},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {1061-1089},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Uncertainty quantification for stochastic approximation limits using chaos expansion},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bayesian approach to estimating background flows from a
passive scalar. <em>JUQ</em>, <em>8</em>(3), 1036–1060. (<a
href="https://doi.org/10.1137/19M1267544">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the statistical inverse problem of estimating a background flow field (e.g., of air or water) from the partial and noisy observation of a passive scalar (e.g., the concentration of a solute), a common experimental approach to visualizing complex fluid flows. Here the unknown is a vector field that is specified by a large or infinite number of degrees of freedom. Since the inverse problem is ill-posed, i.e., there may be many or no background flows that match a given set of observations, we regularize it by laying out a functional analytic and Bayesian framework for approaching this problem. In doing so, we leverage substantial recent advances in statistical inference and adjoint methods for infinite-dimensional problems. We then identify interesting example problems that exhibit posterior measures with simple and complex structure. We use these examples to conduct a large-scale benchmark of Markov chain Monte Carlo methods developed in recent years for infinite-dimensional settings. Our results indicate that these methods are capable of resolving complex multimodal posteriors in high dimensions.},
  archive      = {J_JUQ},
  author       = {Jeff Borggaard and Nathan Glatt-Holtz and Justin Krometis},
  doi          = {10.1137/19M1267544},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {1036-1060},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A bayesian approach to estimating background flows from a passive scalar},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A geometric approach to the transport of discontinuous
densities. <em>JUQ</em>, <em>8</em>(3), 1012–1035. (<a
href="https://doi.org/10.1137/19M1275760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Different observations of a relation between inputs (``sources&quot;) and outputs (``targets&quot;) are often reported in terms of histograms (discretizations of the source and the target densities). Transporting these densities to each other provides insight regarding the underlying relation. In (forward) uncertainty quantification, one typically studies how the distribution of inputs to a system affects the distribution of the system responses. Here we focus on the identification of the system (the transport map) itself, once the input and output distributions are determined, and suggest a modification of current practice by including data from what we call “an observation process.&quot; We hypothesize that there exists a smooth manifold underlying the relation; the sources and the targets are then partial observations (possibly projections) of this manifold. Knowledge of such a manifold implies knowledge of the relation, and thus of “the right&quot; transport between source and target observations. When the source-target observations are not bijective (when the manifold is not the graph of a function over both observation spaces either because folds over them give rise to density singularities or because it marginalizes over several observables), recovery of the manifold is obscured. Using ideas from attractor reconstruction in dynamical systems, we demonstrate how additional information in the form of short histories of an observation process can help us recover the underlying manifold. The types of additional information employed and the relation to optimal transport based solely on density observations are illustrated and discussed, along with limitations in the recovery of the true underlying relation.},
  archive      = {J_JUQ},
  author       = {Caroline Moosmüller and Felix Dietrich and Ioannis G. Kevrekidis},
  doi          = {10.1137/19M1275760},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {1012-1035},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A geometric approach to the transport of discontinuous densities},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian quadrature, energy minimization, and space-filling
design. <em>JUQ</em>, <em>8</em>(3), 959–1011. (<a
href="https://doi.org/10.1137/18M1210332">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A standard objective in computer experiments is to approximate the behavior of an unknown function on a compact domain from a few evaluations inside the domain. When little is known about the function, space-filling design is advisable: typically, points of evaluation spread out across the available space are obtained by minimizing a geometrical (for instance, covering radius) or a discrepancy criterion measuring distance to uniformity. The paper investigates connections between design for integration (quadrature design), construction of the (continuous) best linear unbiased estimator (BLUE) for the location model, space-filling design, and minimization of energy (kernel discrepancy) for signed measures. Integrally strictly positive definite kernels define strictly convex energy functionals, with an equivalence between the notions of potential and directional derivative, showing the strong relation between discrepancy minimization and more traditional design of optimal experiments. In particular, kernel herding algorithms, which are special instances of vertex-direction methods used in optimal design, can be applied to the construction of point sequences with suitable space-filling properties.},
  archive      = {J_JUQ},
  author       = {Luc Pronzato and Anatoly Zhigljavsky},
  doi          = {10.1137/18M1210332},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {959-1011},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Bayesian quadrature, energy minimization, and space-filling design},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Maximum likelihood estimation and uncertainty quantification
for gaussian process approximation of deterministic functions.
<em>JUQ</em>, <em>8</em>(3), 926–958. (<a
href="https://doi.org/10.1137/20M1315968">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Despite the ubiquity of the Gaussian process regression model, few theoretical results are available that account for the fact that parameters of the covariance kernel typically need to be estimated from the data set. This article provides one of the first theoretical analyses in the context of Gaussian process regression with a noiseless data set. Specifically, we consider the scenario where the scale parameter of a Sobolev kernel (such as a Matérn kernel) is estimated by maximum likelihood. We show that the maximum likelihood estimation of the scale parameter alone provides significant adaptation against misspecification of the Gaussian process model in the sense that the model can become “slowly” overconfident at worst, regardless of the difference between the smoothness of the data-generating function and that expected by the model. The analysis is based on a combination of techniques from nonparametric regression and scattered data interpolation. Empirical results are provided in support of the theoretical findings.},
  archive      = {J_JUQ},
  author       = {Toni Karvonen and George Wynne and Filip Tronarp and Chris Oates and Simo Särkkä},
  doi          = {10.1137/20M1315968},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {926-958},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Maximum likelihood estimation and uncertainty quantification for gaussian process approximation of deterministic functions},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian optimization of expected quadratic loss for
multiresponse computer experiments with internal noise. <em>JUQ</em>,
<em>8</em>(3), 891–925. (<a
href="https://doi.org/10.1137/19M1272676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Design of systems based on computer simulations is prevalent. An important idea to improve design quality, called robust parameter design (RPD), is to optimize control factors based on the expectation of a loss function so that the design is robust to noise factor variations. When computer simulations are time consuming, optimizing the simulator based on a Gaussian process (GP) emulator for the response is a computationally efficient approach. For this purpose, acquisition functions (AFs) are used to sequentially determine the next design point so that the GP emulator can more accurately locate the optimal setting. Despite this, few articles consider AFs for positive definite quadratic forms such as the expected quadratic loss (EQL) function, which is the standard expected loss function for RPD with nominally-the-best responses. This paper proposes new AFs for optimizing the EQL, analyzes their convergence, and develops quick and accurate methods based on the characteristic function of the EQL to compute them. We apply the AFs to RPD problems with internal noise factors based on a GP model and an initial design tailored for such problems. Numerical results indicate that all four AFs considered have similar performance, and they outperform an optimization approach based on modeling the quadratic loss as a GP and maximin Latin hypercube designs. MATLAB codes for reproducing reported results are given in the online supplement.},
  archive      = {J_JUQ},
  author       = {Matthias H. Y. Tan},
  doi          = {10.1137/19M1272676},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {891-925},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Bayesian optimization of expected quadratic loss for multiresponse computer experiments with internal noise},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multifidelity quantile-based approach for confidence sets
of random excursion sets with application to ice-sheet dynamics.
<em>JUQ</em>, <em>8</em>(3), 860–890. (<a
href="https://doi.org/10.1137/19M1280466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address uncertainty quantification of physics-based computational models when the quantity of interest concerns geometrical characteristics of their spatial response. Within the probabilistic context of the random set theory, we develop the concept of confidence sets that either contain or are contained within an excursion set of the spatial response with a specified probability level. We seek such confidence sets in a parametric family of nested candidate sets defined as a parametric family of sublevel or superlevel sets of a membership function. We show that the problem of identifying a confidence set with a given probability level in such a parametric family is equivalent to a problem of estimating a quantile of a random variable obtained as a global extremum of the membership function over the complement of the excursion set. To construct such confidence sets, we propose a computationally efficient bifidelity method that exploits a spectral representation of this random variable to reduce the required number of evaluations of the computational model. We show the interest of this concept of confidence sets and the efficiency gain of the proposed bifidelity method in an illustration relevant to the retreat of the grounded portion of the Antarctic ice sheet.},
  archive      = {J_JUQ},
  author       = {Kevin Bulthuis and Frank Pattyn and Maarten Arnst},
  doi          = {10.1137/19M1280466},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {860-890},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A multifidelity quantile-based approach for confidence sets of random excursion sets with application to ice-sheet dynamics},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Overlapping clustering based technique for scalable
uncertainty quantification in physical systems. <em>JUQ</em>,
<em>8</em>(3), 827–859. (<a
href="https://doi.org/10.1137/18M1200567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Forward propagation of uncertainty in physics-based model is nontrivial and a necessary undertaking. This paper provides a methodology for decomposing the state space of scalable dynamical systems with strong interstate coupling. The outlined approach intends to make rigorous Uncertainty Quantification (UQ) of the high-dimension problem feasible by partitioning the overall high-dimensional state space problem into multiple lower-dimensional state space problems. This approach will work quicker with a lesser memory space requirement than existing methods. To enable accelerated and scalable UQ in high-dimensional complex physical system models, the proposed decomposition process leverages an overlapping community detection to detect state variables participating in more than one subsystems (clusters). The final UQ solution is obtained by using the concept of Hadamard product of the state variables in a subsystem (cluster) and their association in the cluster. The developed approach has been tested to detect connected subsystems in coupled dynamical systems. The results analyzing spatio-temporal flow equation are also presented. It is also shown that proposed framework approach is faster and works with a lesser memory requirement to carry out UQ of high-dimensional physical system models.},
  archive      = {J_JUQ},
  author       = {Arpan Mukherjee and Rahul Rai and Puneet Singla and Tarunraj Singh and Abani Patra},
  doi          = {10.1137/18M1200567},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {3},
  pages        = {827-859},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Overlapping clustering based technique for scalable uncertainty quantification in physical systems},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Uncertainty quantification of unsteady flows generated by
line-sources through heterogeneous geological formations. <em>JUQ</em>,
<em>8</em>(2), 807–825. (<a
href="https://doi.org/10.1137/19M1288966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider an unsteady flow generated by a line-source within a geological formation where, owing to the erratic spatial variations of the conductivity $K$, the log-transform $Y \equiv {ln } K$ is modeled as a random space function. As a consequence, flow variables become random fields, and we aim at quantifying their uncertainty. Under ergodic conditions, simple (closed form) expressions for the second order statistical moments of the flow variables are derived by regarding the variance of $Y$ as a small (perturbation) parameter. Then, we focus the discussion by dealing with a formation of stratified heterogeneity structure. In particular, we investigate (in view of their use in practical applications) moments&#39; behavior in the large time regime.},
  archive      = {J_JUQ},
  author       = {Gerardo Severino and Salvatore Cuomo},
  doi          = {10.1137/19M1288966},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {807-825},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Uncertainty quantification of unsteady flows generated by line-sources through heterogeneous geological formations},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Group kernels for gaussian process metamodels with
categorical inputs. <em>JUQ</em>, <em>8</em>(2), 775–806. (<a
href="https://doi.org/10.1137/18M1209386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gaussian processes (GPs) are widely used as a metamodel for emulating time-consuming computer codes. We focus on problems involving categorical inputs, with a potentially large number $L$ of levels (typically several tens), partitioned in $G \ll L$ groups of various sizes. Parsimonious covariance functions, or kernels, can then be defined by block covariance matrices $T$ with constant covariances between pairs of blocks and within blocks. We study the positive definiteness of such matrices to encourage their practical use. The hierarchical group/level structure, equivalent to a nested Bayesian linear model, provides a parameterization of valid block matrices $T$. The same model can then be used when the assumption within blocks is relaxed, giving a flexible parametric family of valid covariance matrices with constant covariances between pairs of blocks. The positive definiteness of ${T}$ is equivalent to the positive definiteness of a smaller matrix of size $G$, obtained by averaging each block. The model is applied to a problem in nuclear waste analysis, where one of the categorical inputs is atomic number, which has more than 90 levels.},
  archive      = {J_JUQ},
  author       = {Olivier Roustant and Espéran Padonou and Yves Deville and Aloïs Clément and Guillaume Perrin and Jean Giorla and Henry Wynn},
  doi          = {10.1137/18M1209386},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {775-806},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Group kernels for gaussian process metamodels with categorical inputs},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A higher order perturbation approach for electromagnetic
scattering problems on random domains. <em>JUQ</em>, <em>8</em>(2),
748–774. (<a href="https://doi.org/10.1137/19M1274365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider time-harmonic electromagnetic scattering problems on perfectly conducting scatterers with uncertain shape. Thus, the scattered field will also be uncertain. Based on the knowledge of the two-point correlation of the domain boundary variations around a reference domain, we derive a perturbation analysis for the mean of the scattered field. Therefore, we compute the second shape derivative of the scattering problem for a single perturbation. Taking the mean, this leads to an at least third order accurate approximation with respect to the perturbation amplitude of the domain variations. To compute the required second order correction term, a tensor product equation on the domain boundary has to be solved. We discuss its discretization and efficient solution using boundary integral equations. Numerical experiments in three dimensions are presented.},
  archive      = {J_JUQ},
  author       = {Jürgen Dölz},
  doi          = {10.1137/19M1274365},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {748-774},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A higher order perturbation approach for electromagnetic scattering problems on random domains},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Locating infinite discontinuities in computer experiments.
<em>JUQ</em>, <em>8</em>(2), 717–747. (<a
href="https://doi.org/10.1137/18M1209076">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Identification of input configurations so as to meet a prespecified output target under a limited experimental budget has been an important task for computer experiments. Such a task often involves the development of response models and design of experimental trials that rely on the models exhibiting continuity and differentiability properties. Motivated by two canonical examples in systems and manufacturing engineering, we propose a strategy for locating the boundary of the response surface in computer experiments, wherein on one side the response is finite, whereas on the other side it is infinite, leveraging ideas from active learning and quasi-Monte Carlo methods. The strategy is illustrated on an example from computer networks engineering and one from precision manufacturing and shown to allocate experimental trials in a fairly effective manner. We conclude by discussing extensions of the proposed strategy to characterize other types of output discontinuity or nondifferentiability in high-cost experiments, including jump discontinuities in the target output response or pathological structures such as kinks and cusps.},
  archive      = {J_JUQ},
  author       = {Ying-Chao Hung and George Michailidis and Horace PakHai Lok},
  doi          = {10.1137/18M1209076},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {717-747},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Locating infinite discontinuities in computer experiments},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Variance reduction for estimation of shapley effects and
adaptation to unknown input distribution. <em>JUQ</em>, <em>8</em>(2),
693–716. (<a href="https://doi.org/10.1137/18M1234631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Shapley effects are global sensitivity indices: they quantify the impact of each input variable on the output variable in a model. In this work, we suggest new estimators of these sensitivity indices. When the input distribution is known, we investigate the already existing estimator defined in [E. Song, B. L. Nelson, and J. Staum, SIAM/ASA J. Uncertain. Quantif., 4 (2016), pp. 1060--1083] and suggest a new one with a lower variance. Then, when the distribution of the inputs is unknown, we extend these estimators. We provide asymptotic properties of the estimators studied in this article. We also apply one of these estimators to a real data set.},
  archive      = {J_JUQ},
  author       = {Baptiste Broto and François Bachoc and Marine Depecker},
  doi          = {10.1137/18M1234631},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {693-716},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Variance reduction for estimation of shapley effects and adaptation to unknown input distribution},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive reduced-order model construction for conditional
value-at-risk estimation. <em>JUQ</em>, <em>8</em>(2), 668–692. (<a
href="https://doi.org/10.1137/19M1257433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper shows how to systematically and efficiently improve a reduced-order model (ROM) to obtain a better ROM-based estimate of the Conditional Value-at-Risk (CVaR) of a computationally expensive quantity of interest (QoI). Efficiency is gained by exploiting the structure of CVaR, which implies that a ROM used for CVaR estimation only needs to be accurate in a small region of the parameter space, called the $\epsilon$-risk region. Hence, any full-order model (FOM) queries needed to improve the ROM can be restricted to this small region of the parameter space, thereby substantially reducing the computational cost of ROM construction. However, an example is presented which shows that simply constructing a new ROM that has a smaller error with the FOM is in general not sufficient to yield a better CVaR estimate. Instead a combination of previous ROMs is proposed that achieves a guaranteed improvement, as well as $\epsilon$-risk regions that converge monotonically to the FOM risk region with decreasing ROM error. Error estimates for the ROM-based CVaR estimates are presented. The gains in efficiency obtained by improving a ROM only in the small $\epsilon$-risk region over a traditional greedy procedure on the entire parameter space are illustrated numerically.},
  archive      = {J_JUQ},
  author       = {Matthias Heinkenschloss and Boris Kramer and Timur Takhtaganov},
  doi          = {10.1137/19M1257433},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {668-692},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Adaptive reduced-order model construction for conditional value-at-risk estimation},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Characterizing impacts of model uncertainties in
quantitative photoacoustics. <em>JUQ</em>, <em>8</em>(2), 636–667. (<a
href="https://doi.org/10.1137/18M1231341">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work is concerned with uncertainty quantification problems for image reconstructions in quantitative photoacoustic imaging (PAT), a recent hybrid imaging modality that utilizes the photoacoustic effect to achieve high-resolution imaging of optical properties of tissue-like heterogeneous media. We quantify mathematically and computationally the impact of uncertainties in various model parameters of PAT on the accuracy of reconstructed optical properties. We derive, via sensitivity analysis, analytical bounds on error in image reconstructions in some simplified settings and develop a computational procedure, based on the method of polynomial chaos expansion, for such error characterization in more general settings. Numerical simulations based on synthetic data are presented to illustrate the main ideas.},
  archive      = {J_JUQ},
  author       = {Kui Ren and Sarah Vallélian},
  doi          = {10.1137/18M1231341},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {636-667},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Characterizing impacts of model uncertainties in quantitative photoacoustics},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On multilevel best linear unbiased estimators. <em>JUQ</em>,
<em>8</em>(2), 601–635. (<a
href="https://doi.org/10.1137/19M1263534">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a general variance reduction technique for the estimation of the expectation of a scalar-valued quantity of interest associated with a family of model evaluations. The key idea is to reformulate the estimation as a linear regression problem. We then show that the associated estimators are variance minimal within the class of linear unbiased estimators. By solving a sample allocation problem we further construct a variance minimal, linear, and unbiased estimator for a given computational budget. We compare our proposed estimator to other multilevel estimators, such as multilevel Monte Carlo, multifidelity Monte Carlo, and approximate control variates. In addition, we provide a sharp lower bound for the variance of any linear unbiased multilevel estimator and show that our estimator approaches this bound in the infinite low fidelity data limit. The results are illustrated by numerical experiments where the underlying output quantity of interest is generated by an elliptic partial differential equation.},
  archive      = {J_JUQ},
  author       = {Daniel Schaden and Elisabeth Ullmann},
  doi          = {10.1137/19M1263534},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {601-635},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {On multilevel best linear unbiased estimators},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A probabilistic clustering approach for identifying primary
subnetworks of discrete fracture networks with quantified uncertainty.
<em>JUQ</em>, <em>8</em>(2), 573–600. (<a
href="https://doi.org/10.1137/19M1279265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fractures form the main pathways for flow of fluids and transport of constituents carried by these fluids in fractured subsurface media. The majority of flow and transport occurs in the primary subnetwork or backbone, which is a subset of the fracture network. Understanding characteristics of the fracture network flow and transport backbones is thus of great importance towards improving efficiency of subsurface applications, such as aquifer management, hydrocarbon extraction, and long- term storage of spent nuclear fuel. We propose a method for identifying these backbones that also quantifies uncertainty over all possible backbones. Our approach treats the backbone identification problem as a probabilistic clustering problem. We develop a probabilistic generative model to simulate backbones that are induced acyclic flow networks. A user-controlled parameter determines the size of the identified backbones, with small backbones having good agreement with early particle arrival times and larger backbones having good agreement with early and late particle arrival times. We demonstrate our method on a representative fracture network with 264 fractures. Our method discovers flow channelization in backbones roughly 10\%--50\% the size of the original network, supporting previous experiments and discrete fracture network simulations. To the best of our knowledge, our method is the first backbone identification method to quantify uncertainty over backbones.},
  archive      = {J_JUQ},
  author       = {Dave Osthus and Jeffrey D. Hyman and Satish Karra and Nishant Panda and Gowri Srinivasan},
  doi          = {10.1137/19M1279265},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {573-600},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A probabilistic clustering approach for identifying primary subnetworks of discrete fracture networks with quantified uncertainty},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Uncertainty quantification for markov processes via
variational principles and functional inequalities. <em>JUQ</em>,
<em>8</em>(2), 539–572. (<a
href="https://doi.org/10.1137/19M1237429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Information-theory based variational principles have proven effective at providing scalable uncertainty quantification (i.e., robustness) bounds for quantities of interest in the presence of nonparametric model-form uncertainty. In this work, we combine such variational formulas with functional inequalities (Poincaré, $\log$-Sobolev, Liapunov functions) to derive explicit uncertainty quantification bounds for time-averaged observables, comparing a Markov process to a second (not necessarily Markov) process. These bounds are well behaved in the infinite-time limit and apply to steady-states of both discrete and continuous-time Markov processes.},
  archive      = {J_JUQ},
  author       = {Jeremiah Birrell and Luc Rey-Bellet},
  doi          = {10.1137/19M1237429},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {539-572},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Uncertainty quantification for markov processes via variational principles and functional inequalities},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Selecting reduced models in the cross-entropy method.
<em>JUQ</em>, <em>8</em>(2), 511–538. (<a
href="https://doi.org/10.1137/18M1192500">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper deals with the estimation of rare event probabilities using importance sampling (IS), where an optimal proposal distribution is computed with the cross-entropy (CE) method. Although IS optimized with the CE method leads to an efficient reduction of the estimator variance, this approach remains unaffordable for problems where the repeated evaluation of the score function represents a too intensive computational effort. This is often the case for score functions related to the solution of a partial differential equation (PDE) with random inputs. This work proposes to alleviate computation by the parsimonious use of a hierarchy of score function approximations in the CE optimization process. The score function approximation is obtained by selecting the surrogate of lowest dimensionality, whose accuracy guarantees to pass the current CE optimization stage. The selection relies on certified upper bounds on the error norm. An asymptotic analysis provides some theoretical guarantees on the efficiency and convergence of the proposed algorithm. Numerical results demonstrate the gain brought by the method in the context of pollution alerts and a system modeled by a PDE.},
  archive      = {J_JUQ},
  author       = {P. Héas},
  doi          = {10.1137/18M1192500},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {2},
  pages        = {511-538},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Selecting reduced models in the cross-entropy method},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stable approximation schemes for optimal filters.
<em>JUQ</em>, <em>8</em>(1), 483–509. (<a
href="https://doi.org/10.1137/19M1255410">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A stable filter has the property that it asymptotically “forgets&quot; initial perturbations. As a result of this property, it is possible to construct approximations of such filters whose errors remain small in time, in other words approximations that are uniformly convergent in the time variable. As uniform approximations are ideal from a practical perspective, finding criteria for filter stability has been the subject of many papers. In this paper, we seek to construct approximate filters that stay close to a given (possibly) unstable filter. Such filters are obtained through a general truncation scheme and, under certain constraints, are stable. The construction enables us to give a characterization of the topological properties of the set of optimal filters. In particular, we introduce a natural topology on this set, under which the subset of stable filters is dense.},
  archive      = {J_JUQ},
  author       = {Dan Crisan and Alberto López-Yela and Joaquin Miguez},
  doi          = {10.1137/19M1255410},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {483-509},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Stable approximation schemes for optimal filters},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On the well-posedness of bayesian inverse problems.
<em>JUQ</em>, <em>8</em>(1), 451–482. (<a
href="https://doi.org/10.1137/19M1247176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The subject of this article is the introduction of a new concept of the well-posedness of Bayesian inverse problems. The conventional concept of (Lipschitz, Hellinger) well-posedness in [A. M. Stuart, Acta Numer., 19 (2010), pp. 451--559] is difficult to verify in practice and may be inappropriate in some contexts. Our concept simply replaces the Lipschitz continuity of the posterior measure in the Hellinger distance by continuity in an appropriate distance between probability measures. Aside from the Hellinger distance, we investigate well-posedness with respect to weak convergence, the total variation distance, the Wasserstein distance, and also the Kullback--Leibler divergence. We demonstrate that the weakening to continuity is tolerable and that the generalization to other distances is important. The main results of this article are proofs of well-posedness with respect to some of the aforementioned distances for large classes of Bayesian inverse problems. Here, little or no information about the underlying model is necessary, making these results particularly interesting for practitioners using black-box models. We illustrate our findings with numerical examples motivated from machine learning and image processing.},
  archive      = {J_JUQ},
  author       = {Jonas Latz},
  doi          = {10.1137/19M1247176},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {451-482},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {On the well-posedness of bayesian inverse problems},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bayesian numerical homogenization method for elliptic
multiscale inverse problems. <em>JUQ</em>, <em>8</em>(1), 414–450. (<a
href="https://doi.org/10.1137/18M1187891">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A new strategy based on numerical homogenization and Bayesian techniques for solving multiscale inverse problems is introduced. We consider a class of elliptic problems which vary at a microscopic scale, and we aim at recovering the highly oscillatory tensor from measurements of the fine scale solution at the boundary, using a coarse model based on numerical homogenization and model order reduction. Assuming a known micro structure, our aim is to recover a macroscopic scalar parameterization of the microscale tensor. We provide a rigorous Bayesian formulation of the problem, taking into account different possibilities for the choice of the prior measure. We prove well-posedness of the effective posterior measure and, by means of G-convergence, we establish a link between the effective posterior and the fine scale model. Several numerical experiments illustrate the efficiency of the proposed scheme and confirm the theoretical findings.},
  archive      = {J_JUQ},
  author       = {Assyr Abdulle and Andrea Di Blasio},
  doi          = {10.1137/18M1187891},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {414-450},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A bayesian numerical homogenization method for elliptic multiscale inverse problems},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Convergence rates for penalized least squares estimators in
PDE constrained regression problems. <em>JUQ</em>, <em>8</em>(1),
374–413. (<a href="https://doi.org/10.1137/18M1236137">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider PDE constrained nonparametric regression problems in which the parameter $f$ is the unknown coefficient function of a second order elliptic partial differential operator $L_f$, and the unique solution $u_f$ of the boundary value problem $L_fu=g_1$ on $\mathcal O, u=g_2$ on $\partial \mathcal O,$ is observed corrupted by additive Gaussian white noise. Here $\mathcal O$ is a bounded domain in $\mathbb R^d$ with smooth boundary $\partial \mathcal O$, and $g_1, g_2$ are given functions defined on $\mathcal O, \partial \mathcal O$, respectively. Concrete examples include $L_fu=\Delta u-2fu$ (Schrödinger equation with attenuation potential $f$) and $L_fu=\text{div} (f\nabla u)$ (divergence form equation with conductivity $f$). In both cases, the parameter space $\mathcal F={f\in H^\alpha(\mathcal O)| f &gt; 0}, \alpha&gt;0$, where $H^\alpha(\mathcal O)$ is the usual order $\alpha$ Sobolev space, induces a set of nonlinearly constrained regression functions ${u_f: f \in \mathcal F}$. We study Tikhonov-type penalized least squares estimators $\hat f$ for $f$. The penalty functionals are of squared Sobolev-norm type and thus $\hat f$ can also be interpreted as a Bayesian “maximum a posteriori” estimator corresponding to some Gaussian process prior. We derive rates of convergence of $\hat f$ and of $u_{\hat f}$, to $f, u_f$, respectively. We prove that the rates obtained are minimax-optimal in prediction loss. Our bounds are derived from a general convergence rate result for nonlinear inverse problems whose forward map satisfies a modulus of continuity condition, a result of independent interest that is applicable also to linear inverse problems, illustrated in an example with the Radon transform.},
  archive      = {J_JUQ},
  author       = {Richard Nickl and Sara van de Geer and Sven Wang},
  doi          = {10.1137/18M1236137},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {374-413},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Convergence rates for penalized least squares estimators in PDE constrained regression problems},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bernstein–von mises theorems and uncertainty quantification
for linear inverse problems. <em>JUQ</em>, <em>8</em>(1), 342–373. (<a
href="https://doi.org/10.1137/18M1226269">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the statistical inverse problem of recovering an unknown function $f$ from a linear measurement corrupted by additive Gaussian white noise. We employ a nonparametric Bayesian approach with standard Gaussian priors, for which the posterior-based reconstruction of $f$ corresponds to a Tikhonov regularizer $\bar f$ with a reproducing kernel Hilbert space norm penalty. We prove a semiparametric Bernstein--von Mises theorem for a large collection of linear functionals of $f$, implying that semiparametric posterior estimation and uncertainty quantification are valid and optimal from a frequentist point of view. The result is applied to study three concrete examples that cover both the mildly and severely ill-posed cases: specifically, an elliptic inverse problem, an elliptic boundary value problem, and the heat equation. For the elliptic boundary value problem, we also obtain a nonparametric version of the theorem that entails the convergence of the posterior distribution to a prior-independent infinite-dimensional Gaussian probability measure with minimal covariance. As a consequence, it follows that the Tikhonov regularizer $\bar f$ is an efficient estimator of $f$, and we derive frequentist guarantees for certain credible balls centered at $\bar{f}$.},
  archive      = {J_JUQ},
  author       = {Matteo Giordano and Hanne Kekkonen},
  doi          = {10.1137/18M1226269},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {342-373},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Bernstein--von mises theorems and uncertainty quantification for linear inverse problems},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Domain uncertainty quantification in computational
electromagnetics. <em>JUQ</em>, <em>8</em>(1), 301–341. (<a
href="https://doi.org/10.1137/19M1239374">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the numerical approximation of time-harmonic, electromagnetic fields inside a lossy cavity of uncertain geometry. Key assumptions are a possibly high-dimensional parametrization of the uncertain geometry along with a suitable transformation to a fixed, nominal domain. This uncertainty parametrization results in families of countably parametric, Maxwell-like cavity problems that are posed in a single domain, with inhomogeneous coefficients that possess finite, possibly low spatial regularity, but exhibit holomorphic parametric dependence in the differential operator. Our computational scheme is composed of a sparse grid interpolation in the high-dimensional parameter domain and an ${H}{curl}$ -conforming edge element discretization of the parametric problem in the nominal domain. As a stepping-stone in the analysis, we derive a novel Strang-type lemma for Maxwell-like problems in the nominal domain, which is of independent interest. Moreover, we accommodate arbitrary small Sobolev regularity of the electric field and also cover uncertain isotropic constitutive or material laws. The shape holomorphy and edge-element consistency error analysis for the nominal problem are shown to imply convergence rates for multilevel Monte Carlo and for quasi-Monte Carlo integration, as well as sparse grid approximations, in uncertainty quantification for computational electromagnetics. They also imply expression rate estimates for deep ReLU networks of shape-to-solution maps in this setting. Finally, our computational experiments confirm the presented theoretical results.},
  archive      = {J_JUQ},
  author       = {Ruben Aylwin and Carlos Jerez-Hanckes and Christoph Schwab and Jakob Zech},
  doi          = {10.1137/19M1239374},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {301-341},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Domain uncertainty quantification in computational electromagnetics},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Density estimation in uncertainty propagation problems using
a surrogate model. <em>JUQ</em>, <em>8</em>(1), 261–300. (<a
href="https://doi.org/10.1137/18M1205959">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The effect of uncertainties and noise on a quantity of interest (model output) is often better described by its probability density function (PDF) than by its moments. Although density estimation is a common task, the adequacy of approximation methods (surrogate models) for density estimation has not been analyzed before in the uncertainty-quantification literature. In this paper, we first show that standard surrogate models (such as generalized polynomial chaos), which are highly accurate for moment estimation, might completely fail to approximate the PDF, even for one-dimensional noise. This is because density estimation requires that the surrogate model accurately approximate the gradient of the quantity of interest and not just the quantity of interest itself. Hence, we develop a novel spline-based algorithm for density estimation whose convergence rate in $L^q$ is polynomial in the sampling resolution. This convergence rate is better than that of standard statistical density estimation methods (such as histograms and kernel density estimators) at dimensions $1 \leq d\leq \frac{5}{2}m$, where $m$ is the spline order. Furthermore, we obtain the convergence rate for density estimation with any surrogate model that approximates the quantity of interest and its gradient in $L^{\infty}$. Finally, we demonstrate our algorithm for problems in nonlinear optics and fluid dynamics.},
  archive      = {J_JUQ},
  author       = {Adi Ditkowski and Gadi Fibich and Amir Sagiv},
  doi          = {10.1137/18M1205959},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {261-300},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Density estimation in uncertainty propagation problems using a surrogate model},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Information geometry for approximate bayesian computation.
<em>JUQ</em>, <em>8</em>(1), 229–260. (<a
href="https://doi.org/10.1137/18M123284X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of this paper is to explore the basic approximate Bayesian computation (ABC) algorithm via the lens of information theory. ABC is a widely used algorithm in cases where the likelihood of the data is hard to work with or intractable, but one can simulate from it. We use relative entropy ideas to analyze the behavior of the algorithm as a function of the threshold parameter and of the size of the data. Relative entropy here is data driven, as it depends on the values of the observed statistics. Relative entropy also allows us to explore the effect of the distance metric and sets up a mathematical framework for sensitivity analysis allowing us to find important directions which could lead to lower computational cost of the algorithm for the same level of accuracy. In addition, we also investigate the bias of the estimators for generic observables as a function of both the threshold parameters and the size of the data. Our analysis provides error bounds on performance for positive tolerances and finite sample sizes. Simulation studies complement and illustrate the theoretical results.},
  archive      = {J_JUQ},
  author       = {Konstantinos Spiliopoulos},
  doi          = {10.1137/18M123284X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {229-260},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Information geometry for approximate bayesian computation},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An iterative ensemble kalman smoother in presence of
additive model error. <em>JUQ</em>, <em>8</em>(1), 198–228. (<a
href="https://doi.org/10.1137/19M1244147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensemble variational methods are being increasingly used in the field of geophysical data assimilation. Their efficiency comes from the combined use of ensembles, which provide statistics estimates, and a variational analysis, which handles nonlinear operators through iterative optimization techniques. Taking model error into account in four-dimensional ensemble variational algorithms is challenging because the state trajectory over the data assimilation window (DAW) is no longer determined by its sole initial condition. In particular, the control variable dimension scales with the DAW length, which yields a high numerical complexity. This is unfortunate since accuracy improvement is expected with longer DAWs. Building upon the work of [P. Sakov and M. Bocquet, Tellus A, 70 (2018), 1414545], this paper discusses how to algorithmically construct and numerically test an iterative ensemble Kalman smoother with additive model error (IEnKS-Q) which is thought to be the natural weak constraint generalization of the IEnKS [M. Bocquet and P. Sakov, Quart. J. Roy. Meteorol. Soc., 140 (2014), pp. 1521--1535], as well as the generalization of IEnKF-Q [P. Sakov, J. Haussaire, and M. Bocquet, Quart. J. Roy. Meteorol. Soc., 144 (2018), pp. 1297--1309] to general DAWs. The number of model evaluations per cycle of the IEnKS-Q is also examined. Solutions based on perturbation decomposition are proposed to dissociate those numerically costly evaluations from the control variable dimension.},
  archive      = {J_JUQ},
  author       = {Anthony Fillion and Marc Bocquet and Serge Gratton and Selime Gürol and Pavel Sakov},
  doi          = {10.1137/19M1244147},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {198-228},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {An iterative ensemble kalman smoother in presence of additive model error},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Statistical treatment of inverse problems constrained by
differential equations-based models with stochastic terms. <em>JUQ</em>,
<em>8</em>(1), 170–197. (<a
href="https://doi.org/10.1137/18M122073X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a statistical treatment of inverse problems constrained by models with stochastic terms. The solution of the forward problem is given by a distribution represented numerically by an ensemble of simulations. The goal is to formulate the inverse problem, in particular the objective function, to find the closest forward distribution (i.e., the output of the stochastic forward problem) that best explains the distribution of the observations in a certain metric. We use proper scoring rules, a concept employed in statistical forecast verification, namely energy, variogram, and hybrid (i.e., combination of the two) scores. We study the performance of the proposed formulation in the context of two applications: a coefficient field inversion for subsurface flow governed by an elliptic partial differential equation with a stochastic source and a parameter inversion for power grid governed by differential-algebraic equations. In both cases we show that the variogram and the hybrid scores produce better parameter inversion results than does the energy score, whereas the energy score leads to better probabilistic predictions.},
  archive      = {J_JUQ},
  author       = {Emil M. Constantinescu and Noémi Petra and Julie Bessac and Cosmin G. Petra},
  doi          = {10.1137/18M122073X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {170-197},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Statistical treatment of inverse problems constrained by differential equations-based models with stochastic terms},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generating nested quadrature rules with positive weights
based on arbitrary sample sets. <em>JUQ</em>, <em>8</em>(1), 139–169.
(<a href="https://doi.org/10.1137/18M1213373">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the purpose of uncertainty propagation a new quadrature rule technique is proposed that has positive weights, has high degree, and is constructed using only samples that describe the probability distribution of the uncertain parameters. Moreover, nodes can be added to the quadrature rule, resulting in a sequence of nested rules. The rule is constructed by iterating over the samples of the distribution and exploiting the null space of the Vandermonde system that describes the nodes and weights, in order to select which samples will be used as nodes in the quadrature rule. The main novelty of the quadrature rule is that it can be constructed using any number of dimensions, using any basis, in any space, and using any distribution. It is demonstrated both theoretically and numerically that the rule always has positive weights and therefore has high convergence rates for sufficiently smooth functions. The convergence properties are demonstrated by approximating the integral of the Genz test functions. The applicability of the quadrature rule to complex uncertainty propagation cases is demonstrated by determining the statistics of the flow over an airfoil governed by the Euler equations, including the case of dependent uncertain input parameters. The new quadrature rule significantly outperforms classical sparse grid methods.},
  archive      = {J_JUQ},
  author       = {Laurent van den Bos and Benjamin Sanderse and Wim Bierbooms and Gerard van Bussel},
  doi          = {10.1137/18M1213373},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {139-169},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Generating nested quadrature rules with positive weights based on arbitrary sample sets},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multifidelity approximate bayesian computation.
<em>JUQ</em>, <em>8</em>(1), 114–138. (<a
href="https://doi.org/10.1137/18M1229742">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A vital stage in the mathematical modeling of real-world systems is to calibrate a model&#39;s parameters to observed data. Likelihood-free parameter inference methods, such as approximate Bayesian computation (ABC), build Monte Carlo samples of the uncertain parameter distribution by comparing the data with large numbers of model simulations. However, the computational expense of generating these simulations forms a significant bottleneck in the practical application of such methods. We identify how simulations of corresponding cheap, low-fidelity models have been used separately in two complementary ways to reduce the computational expense of building these samples, at the cost of introducing additional variance to the resulting parameter estimates. We explore how these approaches can be unified so that cost and benefit are optimally balanced, and we characterize the optimal choice of how often to simulate from cheap, low-fidelity models in place of expensive, high-fidelity models in Monte Carlo ABC algorithms. The resulting early accept/reject multifidelity ABC algorithm that we propose is shown to give improved performance over existing multifidelity and high-fidelity approaches.},
  archive      = {J_JUQ},
  author       = {Thomas P. Prescott and Ruth E. Baker},
  doi          = {10.1137/18M1229742},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {114-138},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Multifidelity approximate bayesian computation},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Block preconditioning of stochastic galerkin problems: New
two-sided guaranteed spectral bounds. <em>JUQ</em>, <em>8</em>(1),
88–113. (<a href="https://doi.org/10.1137/19M125902X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper focuses on numerical solution of parametrized diffusion equations with scalar parameter-dependent coefficient function by the stochastic (spectral) Galerkin method. We study preconditioning of the related discretized problems using preconditioners obtained by modifying the stochastic part of the partial differential equation. We present a simple but general approach for obtaining two-sided bounds to the spectrum of the resulting matrices, based on a particular splitting of the discretized operator. Using this tool and considering the stochastic approximation space formed by classical orthogonal polynomials, we obtain new spectral bounds depending solely on the properties of the coefficient function and the type of the approximation polynomials for several classes of block-diagonal preconditioners. These bounds are guaranteed and applicable to various distributions of parameters. Moreover, the conditions on the parameter-dependent coefficient function are only local, and therefore less restrictive than those usually assumed in the literature.},
  archive      = {J_JUQ},
  author       = {Marie Kubínová and Ivana Pultarová},
  doi          = {10.1137/19M125902X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {88-113},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Block preconditioning of stochastic galerkin problems: New two-sided guaranteed spectral bounds},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The helmholtz equation in random media: Well-posedness and a
priori bounds. <em>JUQ</em>, <em>8</em>(1), 58–87. (<a
href="https://doi.org/10.1137/18M119327X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We prove well-posedness results and a priori bounds on the solution of the Helmholtz equation $\nabla\cdot(A\nabla u) + k^2 n u = -f$, posed either in $\mathbb{R}^d$ or in the exterior of a star-shaped Lipschitz obstacle, for a class of random $A$ and $n,$ random data $f$, and for all $k&gt;0$. The particular class of $A$ and $n$ and the conditions on the obstacle ensure that the problem is nontrapping almost surely. These are the first well-posedness results and a priori bounds for the stochastic Helmholtz equation for arbitrarily large $k$ and for $A$ and $n$ varying independently of $k$. These results are obtained by combining recent bounds on the Helmholtz equation for deterministic $A$ and $n$ and general arguments (i.e., not specific to the Helmholtz equation) presented in this paper for proving a priori bounds and well-posedness of variational formulations of linear elliptic stochastic PDEs. We emphasize that these general results do not rely on either the Lax--Milgram theorem or Fredholm theory, since neither is applicable to the stochastic variational formulation of the Helmholtz equation.},
  archive      = {J_JUQ},
  author       = {O. R. Pembery and E. A. Spence},
  doi          = {10.1137/18M119327X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {58-87},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {The helmholtz equation in random media: Well-posedness and a priori bounds},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A spline chaos expansion. <em>JUQ</em>, <em>8</em>(1),
27–57. (<a href="https://doi.org/10.1137/19M1239702">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A spline chaos expansion, referred to as SCE, is introduced for uncertainty quantification analysis. The expansion provides a means for representing an output random variable of interest with respect to multivariate orthonormal basis splines (B-splines) in input random variables. The multivariate B-splines are built from a whitening transformation to generate univariate orthonormal B-splines in each coordinate direction, followed by a tensor-product structure to produce the multivariate version. SCE, as it stems from compactly supported B-splines, tackles locally prominent responses more effectively than the polynomial chaos expansion (PCE). The approximation quality of the expansion is demonstrated in terms of the modulus of smoothness of the output function, leading to the mean-square convergence of SCE to the correct limit. Analytical formulae are proposed to calculate the mean and variance of an SCE approximation for a general output variable in terms of the requisite expansion coefficients. Numerical results indicate that a low-order SCE approximation with an adequate mesh is markedly more accurate than a high-order PCE approximation in estimating the output variances and probability distributions of oscillatory, nonsmooth, and nearly discontinuous functions.},
  archive      = {J_JUQ},
  author       = {Sharif Rahman},
  doi          = {10.1137/19M1239702},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {27-57},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {A spline chaos expansion},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Diagnostics-driven nonstationary emulators using kernel
mixtures. <em>JUQ</em>, <em>8</em>(1), 1–26. (<a
href="https://doi.org/10.1137/19M124438X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly stationary Gaussian processes (GPs) are the principal tools in the statistical approaches to the design and analysis of computer experiments (or uncertainty quantification). Such processes are fitted to computer model output using a set of training runs to learn the parameters of the process covariance kernel. The stationarity assumption is often adequate, yet can lead to poor predictive performance when the model response exhibits nonstationarity, for example, if its smoothness varies across the input space. In this paper, we introduce a diagnostic-led approach to fitting nonstationary GP emulators by specifying finite mixtures of region-specific covariance kernels. Our method first fits a stationary GP and, if traditional diagnostics exhibit nonstationarity, those diagnostics are used to fit appropriate mixing functions for a covariance kernel mixture designed to capture the nonstationarity, ensuring an emulator that is continuous in parameter space and readily interpretable. We compare our approach to the principal nonstationary GP models in the literature and illustrate its performance on a number of idealized test cases and in an application to modeling the cloud parameterization of the French climate model.},
  archive      = {J_JUQ},
  author       = {Victoria Volodina and Daniel Williamson},
  doi          = {10.1137/19M124438X},
  journal      = {SIAM/ASA Journal on Uncertainty Quantification},
  number       = {1},
  pages        = {1-26},
  shortjournal = {SIAM/ASA J. Uncertainty Quantification},
  title        = {Diagnostics-driven nonstationary emulators using kernel mixtures},
  volume       = {8},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
