<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SIMODS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="simods---45">SIMODS - 45</h2>
<ul>
<li><details>
<summary>
(2020). Extending the step-size restriction for gradient descent to
avoid strict saddle points. <em>SIMODS</em>, <em>2</em>(4), 1181–1197.
(<a href="https://doi.org/10.1137/19M129509X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide larger step-size restrictions for which gradient descent-based algorithms (almost surely) avoid strict saddle points. In particular, consider a twice differentiable (nonconvex) objective function whose gradient has Lipschitz constant $L$ and that the set of points that obtain the maximum value of the spectral norm of the Hessian is measure zero. We prove that given one uniformly random initialization, the probability that gradient descent with a step-size up to $2/L$ will converge to a strict saddle point is zero. This extends previous results up to the sharp limit imposed by the convex quadratic case (provably converging to local minimizers). In addition, the arguments hold in the case when a learning rate schedule is given, with either a continuous decaying rate or a piecewise constant schedule. We show that the assumptions are robust in the sense that functions which do not satisfy the assumptions are meager with respect to analytic functions.},
  archive      = {J_SIMODS},
  author       = {Hayden Schaeffer and Scott G. McCalla},
  doi          = {10.1137/19M129509X},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1181-1197},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Extending the step-size restriction for gradient descent to avoid strict saddle points},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Two models of double descent for weak features.
<em>SIMODS</em>, <em>2</em>(4), 1167–1180. (<a
href="https://doi.org/10.1137/20M1336072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The “double descent” risk curve was proposed to qualitatively describe the out-of-sample prediction accuracy of variably parameterized machine learning models. This article provides a precise mathematical analysis for the shape of this curve in two simple data models with the least squares/least norm predictor. Specifically, it is shown that the risk peaks when the number of features $p$ is close to the sample size $n$ but also that the risk sometimes decreases toward its minimum as $p$ increases beyond $n$. This behavior parallels some key patterns observed in large models, including modern neural networks, and is contrasted with that of “prescient” models that select features in an a priori optimal order.},
  archive      = {J_SIMODS},
  author       = {Mikhail Belkin and Daniel Hsu and Ji Xu},
  doi          = {10.1137/20M1336072},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1167-1180},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Two models of double descent for weak features},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stochastic tverberg theorems with applications in multiclass
logistic regression, separability, and centerpoints of data.
<em>SIMODS</em>, <em>2</em>(4), 1151–1166. (<a
href="https://doi.org/10.1137/19M1277102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present new stochastic geometry theorems that give bounds on the probability that $m$ random data classes all contain a point in common in their convex hulls. These theorems relate to the existence of maximum likelihood estimators in multinomial logistic regression, to the separability of data, and to the computation of centerpoints of data clouds.},
  archive      = {J_SIMODS},
  author       = {Jesus A. De Loera and Thomas Hogan},
  doi          = {10.1137/19M1277102},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1151-1166},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Stochastic tverberg theorems with applications in multiclass logistic regression, separability, and centerpoints of data},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Low-rank tucker approximation of a tensor from streaming
data. <em>SIMODS</em>, <em>2</em>(4), 1123–1150. (<a
href="https://doi.org/10.1137/19M1257718">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper describes a new algorithm for computing a low-Tucker-rank approximation of a tensor. The method applies a randomized linear map to the tensor to obtain a sketch that captures the important directions within each mode, as well as the interactions among the modes. The sketch can be extracted from streaming or distributed data or with a single pass over the tensor, and it uses storage proportional to the degrees of freedom in the output Tucker approximation. The algorithm does not require a second pass over the tensor, although it can exploit another view to compute a superior approximation. The paper provides a rigorous theoretical guarantee on the approximation error. Extensive numerical experiments show that the algorithm produces useful results that improve on the state-of-the-art for streaming Tucker decomposition.},
  archive      = {J_SIMODS},
  author       = {Yiming Sun and Yang Guo and Charlene Luo and Joel Tropp and Madeleine Udell},
  doi          = {10.1137/19M1257718},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1123-1150},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Low-rank tucker approximation of a tensor from streaming data},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Active subspace of neural networks: Structural analysis and
universal attacks. <em>SIMODS</em>, <em>2</em>(4), 1096–1122. (<a
href="https://doi.org/10.1137/19M1296070">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Active subspace is a model reduction method widely used in the uncertainty quantification community. In this paper, we propose analyzing the internal structure and vulnerability of deep neural networks using active subspace. Firstly, we employ the active subspace to measure the number of “active neurons” at each intermediate layer, which indicates that the number of neurons can be reduced from several thousands to several dozens. This motivates us to change the network structure and to develop a new and more compact network, referred to as ASNet, that has significantly fewer model parameters. Secondly, we propose analyzing the vulnerability of a neural network using active subspace by finding an additive universal adversarial attack vector that can misclassify a dataset with a high probability. Our experiments on CIFAR-10 show that ASNet can achieve 23.98x parameter and 7.30x flops reduction. The universal active subspace attack vector can achieve around 20\% higher attack ratio compared with the existing approaches in our numerical experiments. The PyTorch codes for this paper are available online.},
  archive      = {J_SIMODS},
  author       = {Chunfeng Cui and Kaiqi Zhang and Talgat Daulbaev and Julia Gusak and Ivan Oseledets and Zheng Zhang},
  doi          = {10.1137/19M1296070},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1096-1122},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Active subspace of neural networks: Structural analysis and universal attacks},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stochastic gradients for large-scale tensor decomposition.
<em>SIMODS</em>, <em>2</em>(4), 1066–1095. (<a
href="https://doi.org/10.1137/19M1266265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tensor decomposition is a well-known tool for multiway data analysis. This work proposes using stochastic gradients for efficient generalized canonical polyadic (GCP) tensor decomposition of large-scale tensors. GCP tensor decomposition is a recently proposed version of tensor decomposition that allows for a variety of loss functions such as Bernoulli loss for binary data or Huber loss for robust estimation. The stochastic gradient is formed from randomly sampled elements of the tensor and is efficient because it can be computed using the sparse matricized-tensor times Khatri--Rao product tensor kernel. For dense tensors, we simply use uniform sampling. For sparse tensors, we propose two types of stratified sampling that give precedence to sampling nonzeros. Numerical results demonstrate the advantages of the proposed approach and its scalability to large-scale problems.},
  archive      = {J_SIMODS},
  author       = {Tamara G. Kolda and David Hong},
  doi          = {10.1137/19M1266265},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1066-1095},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Stochastic gradients for large-scale tensor decomposition},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Moments of uniform random multigraphs with fixed degree
sequences. <em>SIMODS</em>, <em>2</em>(4), 1034–1065. (<a
href="https://doi.org/10.1137/19M1288772">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the expected adjacency matrix of a uniformly random multigraph with fixed degree sequence $\mathbf{d} \in \mathbb{Z}_+^n$. This matrix arises in a variety of analyses of networked data sets, including modularity-maximization and mean-field theories of spreading processes. Its structure is well understood for large, sparse, simple graphs: the expected number of edges between nodes $i$ and $j$ is roughly $\frac{d_id_j}{\sum_\ell{d_\ell}}$. Many network data sets are neither large, sparse, nor simple, and in these cases the standard approximation no longer applies. We derive a novel estimator using a dynamical approach: the estimator emerges from the stationarity conditions of a class of Markov Chain Monte Carlo algorithms for graph sampling. We derive error bounds for this estimator and provide an efficient scheme with which to compute it. We test the estimator on synthetic and empirical degree sequences, finding that it enjoys relative error against ground truth a full order of magnitude smaller than the standard approximation. We then compare modularity maximization techniques using both the standard and novel estimators, finding that the qualitative structure of the optimization landscape depends significantly on the estimator choice. Our results emphasize the importance of using carefully specified random graph models in data scientific applications.},
  archive      = {J_SIMODS},
  author       = {Philip S. Chodrow},
  doi          = {10.1137/19M1288772},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1034-1065},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Moments of uniform random multigraphs with fixed degree sequences},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Memory capacity of neural networks with threshold and
rectified linear unit activations. <em>SIMODS</em>, <em>2</em>(4),
1004–1033. (<a href="https://doi.org/10.1137/20M1314884">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Overwhelming theoretical and empirical evidence shows that mildly overparametrized neural networks---those with more connections than the size of the training data---are often able to memorize the training data with 100\% accuracy. This was rigorously proved for networks with sigmoid activation functions [M. Yamasaki, Proceedings of the International Conference on Artificial Neural Networks, 1993, pp. 546--549; G.-B. Huang, IEEE Trans. Neural Netw., 14 (2003), pp. 274--281] and, very recently, for rectified linear unit (ReLU) activations [C. Yun, S. Sra, and A. Jadbabaie, Proceedings of the Conference on Neural Information Processing Systems, 2019, pp. 15532--15543]. Addressing a open question of Baum [J. Complexity, 4 (1988), pp. 193--215], we prove that this phenomenon holds for general multilayered perceptrons, i.e., neural networks with threshold activation functions, or with any mix of threshold and ReLU activations. Our construction is probabilistic and exploits sparsity.},
  archive      = {J_SIMODS},
  author       = {Roman Vershynin},
  doi          = {10.1137/20M1314884},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {1004-1033},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Memory capacity of neural networks with threshold and rectified linear unit activations},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Some limit properties of markov chains induced by recursive
stochastic algorithms. <em>SIMODS</em>, <em>2</em>(4), 967–1003. (<a
href="https://doi.org/10.1137/19M1258104">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recursive stochastic algorithms have gained significant attention in the recent past due to data-driven applications. Examples include stochastic gradient descent for solving large-scale optimization problems and empirical dynamic programming algorithms for solving Markov decision problems. These recursive stochastic algorithms approximate certain contraction operators and can be viewed within the framework of iterated random operators. Accordingly, we consider iterated random operators over a Polish space that simulate an iterated contraction operator over that Polish space. Assume that the iterated random operators are indexed by certain batch sizes such that as batch sizes grow to infinity, each realization of the random operator converges (in some sense) to the contraction operator it is simulating. We show that starting from the same initial condition, the distribution of the random sequence generated by the iterated random operators converges weakly to the trajectory generated by the contraction operator. We further show that under certain conditions, the time average of the random sequence converges to the spatial mean of the invariant distribution. We then apply these results to logistic regression, empirical value iteration, and empirical Q-value iteration for finite state finite action MDPs to illustrate the general theory developed here.},
  archive      = {J_SIMODS},
  author       = {Abhishek Gupta and Hao Chen and Jianzong Pi and Gaurav Tendolkar},
  doi          = {10.1137/19M1258104},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {967-1003},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Some limit properties of markov chains induced by recursive stochastic algorithms},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tensor regression using low-rank and sparse tucker
decompositions. <em>SIMODS</em>, <em>2</em>(4), 944–966. (<a
href="https://doi.org/10.1137/19M1299335">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies a tensor-structured linear regression model with a scalar response variable and tensor-structured predictors, such that the regression parameters form a tensor of order $d$ (i.e., a $d$-fold multiway array) in $\mathbb{R}^{n_1 \times n_2 \times \cdots \times n_d}$. In particular, we focus on the task of estimating the regression tensor from $m$ realizations of the response variable and the predictors where $m\ll n = \prod \nolimits_{i} n_i$. Despite the seeming ill-posedness of this estimation problem, it can still be solved if the parameter tensor belongs to the space of sparse, low Tucker-rank tensors. Accordingly, the estimation procedure is posed as a nonconvex optimization program over the space of sparse, low Tucker-rank tensors, and a tensor variant of projected gradient descent is proposed to solve the resulting nonconvex problem. In addition, mathematical guarantees are provided that establish that the proposed method linearly converges to an appropriate solution under a certain set of conditions. Further, an upper bound on sample complexity of tensor parameter estimation for the model under consideration is characterized for the special case when the individual (scalar) predictors independently draw values from a sub-Gaussian distribution. The sample complexity bound is shown to have a polylogarithmic dependence on $\bar{n} = \max \big{n_i: i\in {1,2,\ldots,d } \big}$; orderwise, it matches the bound one can obtain from a heuristic parameter counting argument. Finally, numerical experiments demonstrate the efficacy of the proposed tensor model and estimation method on a synthetic dataset and a collection of neuroimaging datasets pertaining to attention deficit hyperactivity disorder (ADHD). Specifically, the proposed method exhibits better sample complexities on both synthetic and real datasets, demonstrating the usefulness of the model and the method in settings where $n \gg m$.},
  archive      = {J_SIMODS},
  author       = {Talal Ahmed and Haroon Raja and Waheed U. Bajwa},
  doi          = {10.1137/19M1299335},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {944-966},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Tensor regression using low-rank and sparse tucker decompositions},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Measuring the algorithmic convergence of randomized
ensembles: The regression setting. <em>SIMODS</em>, <em>2</em>(4),
921–943. (<a href="https://doi.org/10.1137/20M1343300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When randomized ensemble methods such as bagging and random forests are implemented, a basic question arises: Is the ensemble large enough? In particular, the practitioner desires a rigorous guarantee that a given ensemble will perform nearly as well as an ideal infinite ensemble (trained on the same data). The purpose of the current paper is to develop a bootstrap method for solving this problem in the context of regression---which complements our companion paper in the context of classification [Lopes, Ann. Statist., 47 (2019), 1088--1112]. In contrast to the classification setting, the current paper shows that theoretical guarantees for the proposed bootstrap can be established under much weaker assumptions. In addition, we illustrate the flexibility of the method by showing how it can be adapted to measure algorithmic convergence for variable selection. Lastly, we provide numerical results demonstrating that the method works well in a range of situations.},
  archive      = {J_SIMODS},
  author       = {Miles E. Lopes and Suofei Wu and Thomas C. M. Lee},
  doi          = {10.1137/20M1343300},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {4},
  pages        = {921-943},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Measuring the algorithmic convergence of randomized ensembles: The regression setting},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Deep neural networks, generic universal interpolation, and
controlled ODEs. <em>SIMODS</em>, <em>2</em>(3), 901–919. (<a
href="https://doi.org/10.1137/19M1284117">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A recent paradigm views deep neural networks as discretizations of certain controlled ordinary differential equations, sometimes called neural ordinary differential equations. We make use of this perspective to link expressiveness of deep networks to the notion of controllability of dynamical systems. Using this connection, we study an expressiveness property that we call universal interpolation and show that it is generic in a certain sense. The universal interpolation property is slightly weaker than universal approximation and disentangles supervised learning on finite training sets from generalization properties. We also show that universal interpolation holds for certain deep neural networks even if large numbers of parameters are left untrained and are instead chosen randomly. This lends theoretical support to the observation that training with random initialization can be successful even when most parameters are largely unchanged through the training. Our results also explore what a minimal amount of trainable parameters in neural ordinary differential equations could be without giving up on expressiveness.},
  archive      = {J_SIMODS},
  author       = {Christa Cuchiero and Martin Larsson and Josef Teichmann},
  doi          = {10.1137/19M1284117},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {901-919},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Deep neural networks, generic universal interpolation, and controlled ODEs},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multilayer modularity belief propagation to assess
detectability of community structure. <em>SIMODS</em>, <em>2</em>(3),
872–900. (<a href="https://doi.org/10.1137/19M1279812">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modularity-based community detection encompasses a number of widely used, efficient heuristics for identification of structure in networks. Recently, a belief propagation approach to modularity optimization provided a useful guide for identifying nontrivial structure in single-layer networks in a way that other optimization heuristics have not. In this paper, we extend modularity belief propagation to multilayer networks. As part of this development, we also directly incorporate a resolution parameter. We show that adjusting the resolution parameter affects the convergence properties of the algorithm and yields different community structures than the baseline. We compare our approach with a widely used community detection tool, GenLouvain, across a range of synthetic, multilayer benchmark networks, demonstrating that our method performs comparably to the state of the art. Finally, we demonstrate the practical advantages of the additional information provided by our tool by way of two real-world network examples. We show how the convergence properties of the algorithm can be used in selecting the appropriate resolution and coupling parameters and how the node-level marginals provide an interpretation for the strength of attachment to the identified communities. We have released our tool as a Python package for convenient use.},
  archive      = {J_SIMODS},
  author       = {William H. Weir and Benjamin Walker and Lenka Zdeborová and Peter J. Mucha},
  doi          = {10.1137/19M1279812},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {872-900},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Multilayer modularity belief propagation to assess detectability of community structure},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysis of asymptotic escape of strict saddle sets in
manifold optimization. <em>SIMODS</em>, <em>2</em>(3), 840–871. (<a
href="https://doi.org/10.1137/19M129437X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we provide some analysis on the asymptotic escape of strict saddles in manifold optimization using the projected gradient descent algorithm (PGD). One of our main contributions is that we extend the current analysis to include nonisolated and possibly continuous saddle sets with complicated geometry. We prove that the PGD is able to escape strict critical submanifolds under certain conditions on the geometry and the distribution of the saddle point sets. We also show that the PGD may fail to escape strict saddles under weaker assumptions even if the saddle point set has zero measure. We apply this saddle analysis to the phase retrieval problem on the low-rank matrix manifold, prove that there are only a finite number of saddles, and that in a specific region, they are strict saddles with high probability. We also show the potential application of our analysis for a broader range of manifold optimization problems.},
  archive      = {J_SIMODS},
  author       = {Thomas Y. Hou and Zhenzhen Li and Ziyun Zhang},
  doi          = {10.1137/19M129437X},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {840-871},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Analysis of asymptotic escape of strict saddle sets in manifold optimization},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Toward a mathematical theory of the crystallographic phase
retrieval problem. <em>SIMODS</em>, <em>2</em>(3), 809–839. (<a
href="https://doi.org/10.1137/20M132136X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the X-ray crystallography technology to determine the atomic structure of biological molecules, we study the crystallographic phase retrieval problem, arguably the leading and hardest phase retrieval setup. This problem entails recovering a $K$-sparse signal of length $N$ from its Fourier magnitude or, equivalently, from its periodic autocorrelation. Specifically, this work focuses on the fundamental question of uniqueness: what is the maximal sparsity level $K/N$ that allows unique mapping between a signal and its Fourier magnitude, up to intrinsic symmetries. We design a systemic computational technique to affirm uniqueness for any specific pair $(K,N)$, and establish the following conjecture: the Fourier magnitude determines a generic signal uniquely, up to intrinsic symmetries, as long as $K/N\leq 1/2$. Based on group-theoretic considerations and an additional computational technique, we formulate a second conjecture: if $K/N &lt; 1/2$, then for any signal the set of solutions to the crystallographic phase retrieval problem has measure zero in the set of all signals with a given Fourier magnitude. Together, these conjectures constitute the first attempt to establish a mathematical theory for the crystallographic phase retrieval problem.},
  archive      = {J_SIMODS},
  author       = {Tamir Bendory and Dan Edidin},
  doi          = {10.1137/20M132136X},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {809-839},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Toward a mathematical theory of the crystallographic phase retrieval problem},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reconstruction of a riemannian manifold from noisy intrinsic
distances. <em>SIMODS</em>, <em>2</em>(3), 770–808. (<a
href="https://doi.org/10.1137/19M126829X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the reconstruction of a manifold (or, invariant manifold learning), where a smooth Riemannian manifold $M$ is determined from the intrinsic distances (that is, geodesic distances) of points in a discrete subset of $M$. In the studied problem, the Riemannian manifold $(M,g)$ is considered as an abstract metric space with intrinsic distances, not as an embedded submanifold of an ambient Euclidean space. Let ${X_1,X_2,\dots,X_N}$ be a set of $N$ sample points sampled randomly from an unknown Riemannian $M$ manifold. We assume that we are given the numbers $D_{jk}=d_M(X_j,X_k)+\eta_{jk}$, where $j,k\in {1,2,\dots,N}$. Here, $d_M(X_j,X_k)$ are geodesic distances, and $\eta_{jk}$ are independent, identically distributed random variables such that the exponential moment $\mathbb E e^{|\eta_{jk}|}$ is finite. We show that when $N\sim C_0\delta^{-3n}(\log (1/\delta))^5\log (1/\theta)$, with the probability $1-\theta$, it is possible to construct a manifold that approximates the Riemannian manifold $(M,g)$ with the error $\delta$. Here, $C_0$ depends on the intrinsic dimension $n$ of $M$ and the bounds for the diameter, sectional curvature, the injectivity radius of $(M,g)$, and the the exponential moment of the noise. This problem is a generalization of the geometric Whitney problem with random measurement errors. We also consider the case when the information on the noisy distance $D_{jk}$ of points $X_j$ and $X_k$ is missing with a certain probability. In particular, we consider the case when we have no information on points that are far away.},
  archive      = {J_SIMODS},
  author       = {Charles Fefferman and Sergei Ivanov and Matti Lassas and Hariharan Narayanan},
  doi          = {10.1137/19M126829X},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {770-808},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Reconstruction of a riemannian manifold from noisy intrinsic distances},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Ergodicity coefficients for higher-order stochastic
processes. <em>SIMODS</em>, <em>2</em>(3), 740–769. (<a
href="https://doi.org/10.1137/19M1285214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The use of higher-order stochastic processes such as nonlinear Markov chains or vertex-reinforced random walks is significantly growing in recent years as they are much better at modeling high dimensional data and nonlinear dynamics in numerous application settings. In many cases of practical interest, these processes are identified with a stochastic tensor, and their stationary distribution is a tensor Z-eigenvector. However, fundamental questions such as the convergence of the process towards a limiting distribution and the uniqueness of such a limit are still not well understood and are the subject of rich recent literature. Ergodicity coefficients for stochastic matrices provide a valuable and widely used tool to analyze the long-term behavior of standard, first-order, Markov processes. In this work, we extend an important class of ergodicity coefficients to the setting of stochastic tensors. We show that the proposed higher-order ergodicity coefficients provide new explicit formulas that (a) guarantee the uniqueness of Perron Z-eigenvectors of stochastic tensors, (b) provide bounds on the sensitivity of such eigenvectors with respect to changes in the tensor, and (c) ensure the convergence of different types of higher-order stochastic processes governed by cubical stochastic tensors. Moreover, we illustrate the advantages of the proposed ergodicity coefficients on several example application settings, including the analysis of PageRank vectors for triangle-based random walks and the convergence of lazy higher-order random walks.},
  archive      = {J_SIMODS},
  author       = {Dario Fasino and Francesco Tudisco},
  doi          = {10.1137/19M1285214},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {740-769},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Ergodicity coefficients for higher-order stochastic processes},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A maximum principle argument for the uniform convergence of
graph laplacian regressors. <em>SIMODS</em>, <em>2</em>(3), 705–739. (<a
href="https://doi.org/10.1137/19M1245372">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper investigates the use of methods from partial differential equations and the calculus of variations to study learning problems that are regularized using graph Laplacians. Graph Laplacians are a powerful, flexible method for capturing local and global geometry in many classes of learning problems, and the techniques developed in this paper help to broaden the methodology of studying such problems. In particular, we develop the use of maximum principle arguments to establish asymptotic consistency guarantees within the context of noise corrupted, nonparametric regression with samples living on an unknown manifold embedded in $\mathbb{R}^d$. The maximum principle arguments provide a new technical tool which informs parameter selection by giving concrete error estimates in terms of various regularization parameters. A review of learning algorithms which utilize graph Laplacians, as well as previous developments in the use of differential equation and variational techniques to study those algorithms, is given. In addition, new connections are drawn between Laplacian methods and other machine learning techniques, such as kernel regression and $k$-nearest neighbor methods.},
  archive      = {J_SIMODS},
  author       = {Nicolas García Trillos and Ryan W. Murray},
  doi          = {10.1137/19M1245372},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {705-739},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {A maximum principle argument for the uniform convergence of graph laplacian regressors},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Random points on an algebraic manifold. <em>SIMODS</em>,
<em>2</em>(3), 683–704. (<a
href="https://doi.org/10.1137/19M1271178">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider the set of solutions to a system of polynomial equations in many variables. An algebraic manifold is an open submanifold of such a set. We introduce a new method for computing integrals and sampling from distributions on algebraic manifolds. This method is based on intersecting with random linear spaces. It produces independent and identically distributed samples, works in the presence of multiple connected components, and is simple to implement. We present applications to computational statistical physics and topological data analysis.},
  archive      = {J_SIMODS},
  author       = {Paul Breiding and Orlando Marigliano},
  doi          = {10.1137/19M1271178},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {683-704},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Random points on an algebraic manifold},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Convex-concave backtracking for inertial bregman proximal
gradient algorithms in nonconvex optimization. <em>SIMODS</em>,
<em>2</em>(3), 658–682. (<a
href="https://doi.org/10.1137/19M1298007">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Backtracking line-search is an old yet powerful strategy for finding better step sizes to be used in proximal gradient algorithms. The main principle is to locally find a simple convex upper bound of the objective function, which in turn controls the step size that is used. In case of inertial proximal gradient algorithms, the situation becomes much more difficult and usually leads to very restrictive rules on the extrapolation parameter. In this paper, we show that the extrapolation parameter can be controlled by also locally finding a simple concave lower bound of the objective function. This gives rise to a double convex-concave backtracking procedure which allows for an adaptive choice of both the step size and extrapolation parameters. We apply this procedure to the class of inertial Bregman proximal gradient methods, and prove that any sequence generated by these algorithms converges globally to a critical point of the function at hand. Numerical experiments on a number of challenging nonconvex problems in image processing and machine learning were conducted and show the power of combining inertial step and double backtracking strategy in achieving improved performances.},
  archive      = {J_SIMODS},
  author       = {Mahesh Chandra Mukkamala and Peter Ochs and Thomas Pock and Shoham Sabach},
  doi          = {10.1137/19M1298007},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {658-682},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Convex-concave backtracking for inertial bregman proximal gradient algorithms in nonconvex optimization},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analysis of the generalization error: Empirical risk
minimization over deep artificial neural networks overcomes the curse of
dimensionality in the numerical approximation of black–scholes partial
differential equations. <em>SIMODS</em>, <em>2</em>(3), 631–657. (<a
href="https://doi.org/10.1137/19M125649X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of new classification and regression algorithms based on empirical risk minimization (ERM) over deep neural network hypothesis classes, coined deep learning, revolutionized the area of artificial intelligence, machine learning, and data analysis. In particular, these methods have been applied to the numerical solution of high-dimensional partial differential equations with great success. Recent simulations indicate that deep learning--based algorithms are capable of overcoming the curse of dimensionality for the numerical solution of Kolmogorov equations, which are widely used in models from engineering, finance, and the natural sciences. The present paper considers under which conditions ERM over a deep neural network hypothesis class approximates the solution of a $d$-dimensional Kolmogorov equation with affine drift and diffusion coefficients and typical initial values arising from problems in computational finance up to error $\varepsilon$. We establish that, with high probability over draws of training samples, such an approximation can be achieved with both the size of the hypothesis class and the number of training samples scaling only polynomially in $d$ and $\varepsilon^{-1}$. It can be concluded that ERM over deep neural network hypothesis classes overcomes the curse of dimensionality for the numerical solution of linear Kolmogorov equations with affine coefficients.},
  archive      = {J_SIMODS},
  author       = {Julius Berner and Philipp Grohs and Arnulf Jentzen},
  doi          = {10.1137/19M125649X},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {631-657},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Analysis of the generalization error: Empirical risk minimization over deep artificial neural networks overcomes the curse of dimensionality in the numerical approximation of black--scholes partial differential equations},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Near-optimal sampling strategies for multivariate function
approximation on general domains. <em>SIMODS</em>, <em>2</em>(3),
607–630. (<a href="https://doi.org/10.1137/19M1279459">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we address the problem of approximating a multivariate function defined on a general domain in $d$ dimensions from sample points. We consider weighted least-squares approximation in an arbitrary finite-dimensional space $P$ from independent random samples taken according to a suitable measure. In general, least-squares approximations can be inaccurate and ill-conditioned when the number of sample points $M$ is close to $N = \dim(P)$. To counteract this, we introduce a novel method for sampling in general domains which leads to provably accurate and well-conditioned approximations. The resulting sampling measure is discrete and therefore straightforward to sample from. Our main result shows near-optimal sample complexity for this procedure; specifically, $M = \mathcal{O}(N \log(N))$ samples suffice for a well-conditioned and accurate approximation. Numerical experiments on polynomial approximation in general domains confirm the benefits of this method over standard sampling.},
  archive      = {J_SIMODS},
  author       = {Ben Adcock and Juan M. Cardenas},
  doi          = {10.1137/19M1279459},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {607-630},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Near-optimal sampling strategies for multivariate function approximation on general domains},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A rigorous theory of conditional mean embeddings.
<em>SIMODS</em>, <em>2</em>(3), 583–606. (<a
href="https://doi.org/10.1137/19M1305069">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conditional mean embeddings (CMEs) have proven themselves to be a powerful tool in many machine learning applications. They allow the efficient conditioning of probability distributions within the corresponding reproducing kernel Hilbert spaces by providing a linear-algebraic relation for the kernel mean embeddings of the respective joint and conditional probability distributions. Both centered and uncentered covariance operators have been used to define CMEs in the existing literature. In this paper, we develop a mathematically rigorous theory for both variants, discuss the merits and problems of each, and significantly weaken the conditions for applicability of CMEs. In the course of this, we demonstrate a beautiful connection to Gaussian conditioning in Hilbert spaces.},
  archive      = {J_SIMODS},
  author       = {Ilja Klebanov and Ingmar Schuster and T. J. Sullivan},
  doi          = {10.1137/19M1305069},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {583-606},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {A rigorous theory of conditional mean embeddings},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). EnResNet: ResNets ensemble via the feynman–kac formalism for
adversarial defense and beyond. <em>SIMODS</em>, <em>2</em>(3), 559–582.
(<a href="https://doi.org/10.1137/19M1265302">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Empirical adversarial risk minimization is a widely used mathematical framework to robustly train deep neural nets that are resistant to adversarial attacks. However, both natural and robust accuracies, in classifying clean and adversarial images, respectively, of the trained robust models are far from satisfactory. In this work, we unify the theory of optimal control of transport equations with the practice of training and testing of ResNets. Based on this unified viewpoint, we propose a simple yet effective ResNets ensemble algorithm to boost the accuracy of the robustly trained model on both clean and adversarial images. The proposed algorithm consists of two components: First, we modify the base ResNets by injecting a variance-specified Gaussian noise to the output of each residual mapping. Second, we average over the production of multiple jointly trained modified ResNets to get the final prediction. These two steps give an approximation to the Feynman--Kac formula for representing the solution of a transport equation with viscosity, or a convection-diffusion equation. For the CIFAR10 benchmark, this simple algorithm leads to a robust model with a natural accuracy of 85.62\% on clean images and a robust accuracy of 57.94\% under the 20 iterations of the iterative fast gradient sign method (IFGSM) attack, which outperforms the current state of the art in defending against attack on the CIFAR10. Both natural and robust accuracies of the proposed ResNets ensemble can be improved dynamically as the building block ResNet advances. The code is available at https://github.com/BaoWangMath/EnResNet.},
  archive      = {J_SIMODS},
  author       = {Bao Wang and Binjie Yuan and Zuoqiang Shi and Stanley J. Osher},
  doi          = {10.1137/19M1265302},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {3},
  pages        = {559-582},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {EnResNet: ResNets ensemble via the feynman--kac formalism for adversarial defense and beyond},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Lipschitz certificates for layered network structures driven
by averaged activation operators. <em>SIMODS</em>, <em>2</em>(2),
529–557. (<a href="https://doi.org/10.1137/19M1272780">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Obtaining sharp Lipschitz constants for feed-forward neural networks is essential to assess their robustness in the face of perturbations of their inputs. We derive such constants in the context of a general layered network model involving compositions of nonexpansive averaged operators and affine operators. By exploiting this architecture, our analysis finely captures the interactions between the layers, yielding tighter Lipschitz constants than those resulting from the product of individual bounds for groups of layers. The proposed framework is shown to cover in particular many practical instances encountered in feed-forward neural networks. Our Lipschitz constant estimates are further improved in the case of structures employing scalar nonlinear functions, which include standard convolutional networks as special cases.},
  archive      = {J_SIMODS},
  author       = {Patrick L. Combettes and Jean-Christophe Pesquet},
  doi          = {10.1137/19M1272780},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {529-557},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Lipschitz certificates for layered network structures driven by averaged activation operators},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Network interpolation. <em>SIMODS</em>, <em>2</em>(2),
505–528. (<a href="https://doi.org/10.1137/19M1268380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a set of snapshots from a temporal network we develop, analyze, and experimentally validate a so-called network interpolation scheme. Our method allows us to build a plausible, albeit random, sequence of graphs that transition between any two given graphs. Importantly, our model is well characterized by a Markov chain, and we leverage this representation to analytically estimate the hitting time (to a predefined distance to the target graph) and long-term behavior of our model. These observations also serve to provide interpretation and justification for a rate parameter in our model. Finally, through a mix of synthetic and real-world data experiments we demonstrate that our model builds reasonable graph trajectories between snapshots, as measured through various graph statistics. In these experiments, we find that our interpolation scheme compares favorably to common network growth models, such as preferential attachment and triadic closure.},
  archive      = {J_SIMODS},
  author       = {Thomas Reeves and Anil Damle and Austin R. Benson},
  doi          = {10.1137/19M1268380},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {505-528},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Network interpolation},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graph-based regularization for regression problems with
alignment and highly correlated designs. <em>SIMODS</em>, <em>2</em>(2),
480–504. (<a href="https://doi.org/10.1137/19M1287365">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse models for high-dimensional linear regression and machine learning have received substantial attention over the past two decades. Model selection, or determining which features or covariates are the best explanatory variables, is critical to the interpretability of a learned model. Much of the current literature assumes that covariates are only mildly correlated. However, in many modern applications covariates are highly correlated and do not exhibit key properties (such as the restricted eigenvalue condition, restricted isometry property, or other related assumptions). This work considers a high-dimensional regression setting in which a graph governs both correlations among the covariates and the similarity among regression coefficients---meaning there is alignment between the covariates and regression coefficients. Using side information about the strength of correlations among features, we form a graph with edge weights corresponding to pairwise covariances. This graph is used to define a graph total variation regularizer that promotes similar weights for correlated features. This work shows how the proposed graph-based regularization yields mean-squared error guarantees for a broad range of covariance graph structures. These guarantees are optimal for many specific covariance graphs, including block and lattice graphs. Our proposed approach outperforms other methods for highly correlated design in a variety of experiments on synthetic data and real biochemistry data.},
  archive      = {J_SIMODS},
  author       = {Yuan Li and Benjamin Mark and Garvesh Raskutti and Rebecca Willett and Hyebin Song and David Neiman},
  doi          = {10.1137/19M1287365},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {480-504},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Graph-based regularization for regression problems with alignment and highly correlated designs},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). ISLET: Fast and optimal low-rank tensor regression via
importance sketching. <em>SIMODS</em>, <em>2</em>(2), 444–479. (<a
href="https://doi.org/10.1137/19M126476X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we develop a novel procedure for low-rank tensor regression, namely Importance Sketching Low-rank Estimation for Tensors (ISLET). The central idea behind ISLET is importance sketching, i.e., carefully designed sketches based on both the responses and low-dimensional structure of the parameter of interest. We show that the proposed method is sharply minimax optimal in terms of the mean-squared error under low-rank Tucker assumptions and under the randomized Gaussian ensemble design. In addition, if a tensor is low-rank with group sparsity, our procedure also achieves minimax optimality. Further, we show through numerical study that ISLET achieves comparable or better mean-squared error performance to existing state-of-the-art methods while having substantial storage and run-time advantages including capabilities for parallel and distributed computing. In particular, our procedure performs reliable estimation with tensors of dimension $p = O(10^8)$ and is 1 or 2 orders of magnitude faster than baseline methods.},
  archive      = {J_SIMODS},
  author       = {Anru R. Zhang and Yuetian Luo and Garvesh Raskutti and Ming Yuan},
  doi          = {10.1137/19M126476X},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {444-479},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {ISLET: Fast and optimal low-rank tensor regression via importance sketching},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Empirical regularized optimal transport: Statistical theory
and applications. <em>SIMODS</em>, <em>2</em>(2), 419–443. (<a
href="https://doi.org/10.1137/19M1278788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We derive limit distributions for various empirical regularized optimal transport quantities between probability distributions supported on a finite metric space and show their bootstrap consistency. In particular, we prove that the empirical regularized transport plan itself asymptotically follows a Gaussian law. The theory includes the Boltzmann--Shannon entropy regularization and hence a limit law for the widely applied Sinkhorn divergence. Our approach is based on parametric optimization techniques for the regularized transport problem in conjunction with a statistical delta method. The asymptotic results are investigated in Monte Carlo simulations. We further discuss computational consequences and statistical applications, e.g., confidence bands for colocalization analysis of protein interaction networks based on regularized optimal transport.},
  archive      = {J_SIMODS},
  author       = {Marcel Klatt and Carla Tameling and Axel Munk},
  doi          = {10.1137/19M1278788},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {419-443},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Empirical regularized optimal transport: Statistical theory and applications},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Persistent cohomology for data with multicomponent
heterogeneous information. <em>SIMODS</em>, <em>2</em>(2), 396–418. (<a
href="https://doi.org/10.1137/19M1272226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Persistent homology is a powerful tool for characterizing the topology of a data set at various geometric scales. When applied to the description of molecular structures, persistent homology can capture the multiscale geometric features and reveal certain interaction patterns in terms of topological invariants. However, in addition to the geometric information, there is a wide variety of nongeometric information of molecular structures, such as element types, atomic partial charges, atomic pairwise interactions, and electrostatic potential functions, that is not described by persistent homology. Although element-specific homology and electrostatic persistent homology can encode some nongeometric information into geometry based topological invariants, it is desirable to have a mathematical paradigm to systematically embed both geometric and nongeometric information, i.e., multicomponent heterogeneous information, into unified topological representations. To this end, we propose a persistent cohomology based framework for the enriched representation of data. In our framework, nongeometric information can either be distributed globally or reside locally on the datasets in the geometric sense and can be properly defined on topological spaces, i.e., simplicial complexes. Using the proposed persistent cohomology based framework, enriched barcodes are extracted from datasets to represent heterogeneous information. We consider a variety of datasets to validate the present formulation and illustrate the usefulness of the proposed method based on persistent cohomology. It is found that the proposed framework outperforms or at least matches the state-of-the-art methods in the protein-ligand binding affinity prediction from massive biomolecular datasets without resorting to any deep learning formulation.},
  archive      = {J_SIMODS},
  author       = {Zixuan Cang and Guo-Wei Wei},
  doi          = {10.1137/19M1272226},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {396-418},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Persistent cohomology for data with multicomponent heterogeneous information},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Compressive sensing for cut improvement and local
clustering. <em>SIMODS</em>, <em>2</em>(2), 368–395. (<a
href="https://doi.org/10.1137/19M1265971">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show how one can phrase the cut improvement problem for graphs as a sparse recovery problem, whence one can use algorithms originally developed for use in compressive sensing (such as SubspacePursuit or CoSaMP) to solve it. We show that this approach to cut improvement is fast, both in theory and practice, and moreover enjoys statistical guarantees of success when applied to graphs drawn from probabilistic models such as the stochastic block model. Using this new cut improvement approach, which we call ClusterPursuit, as an algorithmic primitive, we then propose new methods for local clustering and semisupervised clustering, which enjoy similar guarantees of success and speed. Finally, we verify the promise of our approach with extensive numerical benchmarking.},
  archive      = {J_SIMODS},
  author       = {Ming-Jun Lai and Daniel Mckenzie},
  doi          = {10.1137/19M1265971},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {368-395},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Compressive sensing for cut improvement and local clustering},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Blind identification of stochastic block models from
dynamical observations. <em>SIMODS</em>, <em>2</em>(2), 335–367. (<a
href="https://doi.org/10.1137/19M1263340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a blind identification problem in which we aim to recover a statistical model of a network without knowledge of the network&#39;s edges but based solely on nodal observations of a certain process. More concretely, we focus on observations that consist of single snapshots taken from multiple trajectories of a diffusive process that evolves over the unknown network. We model the network as generated from an independent draw from a latent stochastic block model (SBM), and our goal is to infer both the partition of the nodes into blocks and the parameters of this SBM. We discuss some nonidentifiability issues related to this problem and present simple spectral algorithms that provably solve the partition recovery and parameter estimation problems with high accuracy. Our analysis relies on recent results in random matrix theory and covariance estimation and on associated concentration inequalities. We illustrate our results with several numerical experiments.},
  archive      = {J_SIMODS},
  author       = {Michael T. Schaub and Santiago Segarra and John N. Tsitsiklis},
  doi          = {10.1137/19M1263340},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {335-367},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Blind identification of stochastic block models from dynamical observations},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Matching component analysis for transfer learning.
<em>SIMODS</em>, <em>2</em>(2), 309–334. (<a
href="https://doi.org/10.1137/19M1285123">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a new Procrustes-type method called matching component analysis to isolate components in data for transfer learning. Our theoretical results describe the sample complexity of this method, and we demonstrate through numerical experiments that our approach is indeed well suited for transfer learning.},
  archive      = {J_SIMODS},
  author       = {Charles Clum and Dustin G. Mixon and Theresa Scarnati},
  doi          = {10.1137/19M1285123},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {309-334},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Matching component analysis for transfer learning},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). SCOTT: Shape-location combined tracking with optimal
transport. <em>SIMODS</em>, <em>2</em>(2), 284–308. (<a
href="https://doi.org/10.1137/19M1253976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optimal transport (OT) is a prominent framework for point set registration, that is, to align points in two sets. Point set registration becomes particularly difficult when points are organized into objects and the correspondence among the objects is to be established. The registration of pixels must maintain consistency at the object level despite the possibility of object division, merging, and substantial alteration in size and shape over time. Existing approaches in OT exploit either similarity in shape for the entire set of points or spatial closeness of individual points, but not the two simultaneously. We propose a new weighted Gromov--Wasserstein distance (WGWD) to combine both sources of information. Importantly, we use a bipartite graph partitioning strategy to regularize OT in order to achieve object-level consistency and to enhance computational efficiency. We apply the method to cell tracking, specifically, the task of associating biological cells in consecutive image frames from time-lapse image sequences. We call the system SCOTT Shape-Location COmbined Tracking with Optimal Transport). By establishing a pixel-to-pixel correspondence, our method can effectively detect intricate scenarios including cell division and merging (overlapping). Experiments show that our method achieves high accuracy in tracking the movements of cells and outperforms existing methods in the detection of cell division and merging. Location information is shown to be more useful than shape information, while the combination of the two achieves optimal results.},
  archive      = {J_SIMODS},
  author       = {Xinye Zheng and Jianbo Ye and James Z. Wang and Jia Li},
  doi          = {10.1137/19M1253976},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {284-308},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {SCOTT: Shape-location combined tracking with optimal transport},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Detecting overlapping communities in networks using spectral
methods. <em>SIMODS</em>, <em>2</em>(2), 265–283. (<a
href="https://doi.org/10.1137/19M1272238">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Community detection has been well studied in network analysis, but the more realistic case of overlapping communities remains a challenge. Here we propose a general, flexible, and interpretable generative model for overlapping communities, which can be viewed as generalizing several previous models in different ways. We develop an efficient spectral algorithm for estimating the community memberships, which deals with the overlaps by employing the $K$-medians algorithm rather than the usual $K$-means for clustering in the spectral domain. We show that the algorithm is asymptotically consistent when the network is not too sparse and the overlaps between communities are not too large. Numerical experiments on both simulated networks and many real social networks demonstrate that our method performs well compared to a number of benchmark methods for overlapping community detection.},
  archive      = {J_SIMODS},
  author       = {Yuan Zhang and Elizaveta Levina and Ji Zhu},
  doi          = {10.1137/19M1272238},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {2},
  pages        = {265-283},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Detecting overlapping communities in networks using spectral methods},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Phase retrieval with sparse phase constraint.
<em>SIMODS</em>, <em>2</em>(1), 246–263. (<a
href="https://doi.org/10.1137/19M1266800">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose and investigate the phase retrieval problem with the a priori constraint that the phase is sparse (SPR), which encompasses a number of practical applications, for instance, in characterizing phase-only objects such as microlenses, in phase-contrast microscopy, in optical path difference microscopy, and in Fourier ptychography, where the phase object occupies a small portion of the whole field. The considered problem is strictly more general than the sparse signal recovery problem, which assumes the sparsity of the signal because the sparsity of the signal trivially implies the sparsity of the phase, but the converse is not true. As a result, existing solution algorithms in the literature of sparse signal recovery cannot be applied to SPR and there is an appeal for developing new solution methods for it. In this paper, we propose a new regularization scheme which efficiently captures the sparsity constraint of SPR. The idea behind the proposed approach is to perform a metric projection of the current estimated signal onto the set of all the signals whose phase satisfies the sparsity constraint. The main challenge here is that the latter set is not convex and its associated projector in general does not admit a closed form. One novelty of our analysis is to establish an explicit form of that projector when restricted to those points which are relevant to the solutions of SPR. Note that this result is fundamentally different from the widely known calculation form for projections onto intensity constraint sets. Based on this new result, we propose an efficient solution method, named the sparsity regularization on phase (SROP) algorithm, for the SPR problem in the challenging setting where only one point-spread-function image is given, and we analyze its convergence. The algorithm is the combination of the Gerchberg--Saxton (GS) algorithm with the projection step described above. In view of the GS algorithm being equivalent to the alternating projection for an associated two-set feasibility, the SROP algorithm is shown to be the cyclic projection for an associated three-set feasibility, one of the sets being analyzed in this paper for the first time. Analyzing regularity properties of the involved sets, we obtain convergence results for the SROP algorithm based on our recent convergence theory for the cyclic projection method. Numerical results show clear effectiveness and efficiency of the proposed solution approach for the SPR problem.},
  archive      = {J_SIMODS},
  author       = {Nguyen Hieu Thao and David Russell Luke and Oleg Soloviev and Michel Verhaegen},
  doi          = {10.1137/19M1266800},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {246-263},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Phase retrieval with sparse phase constraint},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Geometry and symmetry in short-and-sparse deconvolution.
<em>SIMODS</em>, <em>2</em>(1), 216–245. (<a
href="https://doi.org/10.1137/19M1237569">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the short-and-sparse (SaS) deconvolution problem of recovering a short signal $\boldsymbol{a}_0$ and a sparse signal $\boldsymbol{x}_0$ from their convolution. We propose a method based on nonconvex optimization, which under certain conditions recovers the target short and sparse signals, up to a signed shift symmetry which is intrinsic to this model. This symmetry plays a central role in shaping the optimization landscape for deconvolution. We give a regional analysis, which characterizes this landscape geometrically, on a union of subspaces. Our geometric characterization holds when the length-$p_0$ short signal $\boldsymbol{a}_0$ has shift coherence $\mu$, and $\boldsymbol{x}_0$ follows a random sparsity model with sparsity rate $\theta \in \big[\frac{c_1}{p_0}, \frac{c_2}{p_0\sqrt\mu + \sqrt{p_0}}\big]\cdot\frac{1}{\log^2p_0}$. Based on this geometry, we give a provable method that successfully solves SaS deconvolution with high probability.},
  archive      = {J_SIMODS},
  author       = {Han-Wen Kuo and Yuqian Zhang and Yenson Lau and John Wright},
  doi          = {10.1137/19M1237569},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {216-245},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Geometry and symmetry in short-and-sparse deconvolution},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Randomized algorithms for low-rank tensor decompositions in
the tucker format. <em>SIMODS</em>, <em>2</em>(1), 189–215. (<a
href="https://doi.org/10.1137/19M1261043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many applications in data science and scientific computing involve large-scale datasets that are expensive to store and manipulate. However, these datasets possess inherent multidimensional structure that can be exploited to compress and store the dataset in an appropriate tensor format. In recent years, randomized matrix methods have been used to efficiently and accurately compute low-rank matrix decompositions. Motivated by this success, we develop randomized algorithms for tensor decompositions in the Tucker representation. Specifically, we present randomized versions of two well-known compression algorithms, namely, HOSVD and STHOSVD, and a detailed probabilistic analysis of the error in using both algorithms. We also develop variants of these algorithms that tackle specific challenges posed by large-scale datasets. The first variant adaptively finds a low-rank representation satisfying a given tolerance, and it is beneficial when the target rank is not known in advance. The second variant preserves the structure of the original tensor and is beneficial for large sparse tensors that are difficult to load in memory. We consider several different datasets for our numerical experiments: synthetic test tensors and realistic applications such as the compression of facial image samples in the Olivetti database and word counts in the Enron email dataset.},
  archive      = {J_SIMODS},
  author       = {Rachel Minster and Arvind K. Saibaba and Misha E. Kilmer},
  doi          = {10.1137/19M1261043},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {189-215},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Randomized algorithms for low-rank tensor decompositions in the tucker format},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast convex pruning of deep neural networks.
<em>SIMODS</em>, <em>2</em>(1), 158–188. (<a
href="https://doi.org/10.1137/19M1246468">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a fast, tractable technique called Net-Trim for simplifying a trained neural network. The method is a convex postprocessing module, which prunes (sparsifies) a trained network layer by layer, while preserving the internal responses. We present a comprehensive analysis of Net-Trim from both the algorithmic and sample complexity standpoints, centered on a fast, scalable convex optimization program. Our analysis includes consistency results between the initial and retrained models before and after Net-Trim application and provides a sample complexity bound on the number of input samples needed to discover a network layer with sparse topology. Specifically, if there is a set of weights that uses at most $s$ terms that can recreate the layer outputs from the layer inputs, we can find these weights from $\mathcal{O}(s\log N/s)$ samples, where $N$ is the input size. These theoretical results are similar to those for sparse regression using the Lasso, and our analysis uses some of the same recently developed tools (namely recent results on the concentration of measure and convex analysis). Finally, we propose an algorithmic framework based on the alternating direction method of multipliers (ADMM), which allows a fast and simple implementation of Net-Trim for network pruning and compression.},
  archive      = {J_SIMODS},
  author       = {Alireza Aghasi and Afshin Abdi and Justin Romberg},
  doi          = {10.1137/19M1246468},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {158-188},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Fast convex pruning of deep neural networks},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Graph powering and spectral robustness. <em>SIMODS</em>,
<em>2</em>(1), 132–157. (<a
href="https://doi.org/10.1137/19M1257135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Spectral algorithms, such as principal component analysis and spectral clustering, rely on the extremal eigenpairs of a matrix $A$. However, these may be uninformative without preprocessing $A$ with a proper transformation. The reason is that the spectrum of $A$ may be contaminated by top eigenvalues resulting from scale variations in the data, such as high-degree nodes. Designing a good $\psi$ and establishing what good means is often challenging and model dependent. This paper proposes a simple and generic construction for sparse graphs, $\psi(A) = \mathbb{1}((I+A)^r \ge 1)$, where $A$ denotes the adjacency matrix, $r$ is an integer, and the indicator function is applied entrywise. We support this “graph powering” construction with the following regularization properties: (i) if the graph is drawn from the sparse Erdös--Rényi ensemble, which has no spectral gap, then graph powering produces a “maximal” spectral gap, comparable to that obtained when powering a random regular graph; (ii) if the graph is drawn from the sparse stochastic block model, graph powering achieves the fundamental limit for weak recovery (the Kesten--Stigum threshold), settling at the same time a related conjecture by Massoulié in 2013; (iii) we also demonstrate that graph powering is significantly more robust to tangles and cliques than previous spectral algorithms based on self-avoiding or nonbacktracking walk counts, using a geometric block model as our benchmark and introducing new conjectures for this model.},
  archive      = {J_SIMODS},
  author       = {Emmanuel Abbe and Enric Boix-Adserà and Peter Ralli and Colin Sandon},
  doi          = {10.1137/19M1257135},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {132-157},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Graph powering and spectral robustness},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On gradient-based learning in continuous games.
<em>SIMODS</em>, <em>2</em>(1), 103–131. (<a
href="https://doi.org/10.1137/18M1231298">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce a general framework for competitive gradient-based learning that encompasses a wide breadth of multiagent learning algorithms, and analyze the limiting behavior of competitive gradient-based learning algorithms using dynamical systems theory. For both general-sum and potential games, we characterize a nonnegligible subset of the local Nash equilibria that will be avoided if each agent employs a gradient-based learning algorithm. We also shed light on the issue of convergence to non-Nash strategies in general- and zero-sum games, which may have no relevance to the underlying game, and arise solely due to the choice of algorithm. The existence and frequency of such strategies may explain some of the difficulties encountered when using gradient descent in zero-sum games as, e.g., in the training of generative adversarial networks. To reinforce the theoretical contributions, we provide empirical results that highlight the frequency of linear quadratic dynamic games (a benchmark for multiagent reinforcement learning) that admit global Nash equilibria that are almost surely avoided by policy gradient.},
  archive      = {J_SIMODS},
  author       = {Eric Mazumdar and Lillian J. Ratliff and S. Shankar Sastry},
  doi          = {10.1137/18M1231298},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {103-131},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {On gradient-based learning in continuous games},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On a minimum distance procedure for threshold selection in
tail analysis. <em>SIMODS</em>, <em>2</em>(1), 75–102. (<a
href="https://doi.org/10.1137/19M1260463">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Power-law distributions have been widely observed in different areas of scientific research. Practical estimation issues include selecting a threshold above which observations follow a power-law distribution and then estimating the power-law tail index. A minimum distance selection procedure (MDSP) proposed by Clauset, Shalizi, and Newman [SIAM Rev., 51 (2009), pp. 661--703] has been widely adopted in practice for the analyses of social networks. However, theoretical justifications for this selection procedure remain scant. In this paper, we study the asymptotic behavior of the selected threshold and the corresponding power-law index given by the MDSP. For independent and identically distributed (iid) observations with Pareto-like tails, we derive the limiting distribution of the chosen threshold and the power-law index estimator, where the latter estimator is not asymptotically normal. We deduce that in this iid setting MDSP tends to choose too high a threshold level and show with asymptotic analysis and simulations how the variance increases compared to Hill estimators based on a nonrandom threshold. We also provide simulation results for dependent preferential attachment network data and find that the performance of the MDSP procedure is highly dependent on the chosen model parameters.},
  archive      = {J_SIMODS},
  author       = {Holger Drees and Anja Janßen and Sidney I. Resnick and Tiandong Wang},
  doi          = {10.1137/19M1260463},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {75-102},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {On a minimum distance procedure for threshold selection in tail analysis},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A bayesian framework for persistent homology.
<em>SIMODS</em>, <em>2</em>(1), 48–74. (<a
href="https://doi.org/10.1137/19M1268719">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Persistence diagrams offer a way to summarize topological and geometric properties latent in datasets. While several methods have been developed that use persistence diagrams in statistical inference, a full Bayesian treatment remains absent. This paper, relying on the theory of point processes, presents a generalized Bayesian framework for inference with persistence diagrams relying on a substitution likelihood argument. In essence, we model persistence diagrams as Poisson point processes with prior intensities and compute posterior intensities by adopting techniques from the theory of marked point processes. We then propose a family of conjugate prior intensities via Gaussian mixtures to obtain a closed form of the posterior intensity. Finally, we demonstrate the utility of this generalized Bayesian framework with a classification problem in materials science using Bayes factors.},
  archive      = {J_SIMODS},
  author       = {Vasileios Maroulas and Farzana Nasrin and Christopher Oballe},
  doi          = {10.1137/19M1268719},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {48-74},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {A bayesian framework for persistent homology},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Accelerated gossip in networks of given dimension using
jacobi polynomial iterations. <em>SIMODS</em>, <em>2</em>(1), 24–47. (<a
href="https://doi.org/10.1137/19M1244822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Consider a network of agents connected by communication links, where each agent holds a real value. The gossip problem consists in estimating the average of the values diffused in the network in a distributed manner. We develop a method for solving the gossip problem that depends only on the spectral dimension of the network, that is, in the communication network set-up, the dimension of the space in which the agents live. This contrasts with previous work that required the spectral gap of the network as a parameter, or suffered from slow mixing. Our method shows an important improvement over existing algorithms in the nonasymptotic regime, i.e., when the values are far from being fully mixed in the network. Our approach stems from a polynomial-based point of view on gossip algorithms, as well as an approximation of the spectral measure of the graphs with a Jacobi measure. We show the power of the approach with simulations on various graphs, and with performance guarantees on graphs of known spectral dimension, such as grids and random percolation bonds. An extension of this work to distributed Laplacian solvers is discussed. As a side result, we also use the polynomial-based point of view to show the convergence of the message passing algorithm for gossip of Moallemi and Van Roy on regular graphs. The explicit computation of the rate of the convergence shows that message passing has a slow rate of convergence on graphs with small spectral gap.},
  archive      = {J_SIMODS},
  author       = {Raphaël Berthier and Francis Bach and Pierre Gaillard},
  doi          = {10.1137/19M1244822},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {24-47},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Accelerated gossip in networks of given dimension using jacobi polynomial iterations},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Layer-parallel training of deep residual neural networks.
<em>SIMODS</em>, <em>2</em>(1), 1–23. (<a
href="https://doi.org/10.1137/19M1247620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Residual neural networks (ResNets) are a promising class of deep neural networks that have shown excellent performance for a number of learning tasks, e.g., image classification and recognition. Mathematically, ResNet architectures can be interpreted as forward Euler discretizations of a nonlinear initial value problem whose time-dependent control variables represent the weights of the neural network. Hence, training a ResNet can be cast as an optimal control problem of the associated dynamical system. For similar time-dependent optimal control problems arising in engineering applications, parallel-in-time methods have shown notable improvements in scalability. This paper demonstrates the use of those techniques for efficient and effective training of ResNets. The proposed algorithms replace the classical (sequential) forward and backward propagation through the network layers with a parallel nonlinear multigrid iteration applied to the layer domain. This adds a new dimension of parallelism across layers that is attractive when training very deep networks. From this basic idea, we derive multiple layer-parallel methods. The most efficient version employs a simultaneous optimization approach where updates to the network parameters are based on inexact gradient information in order to speed up the training process. Using numerical examples from supervised classification, we demonstrate that the new approach achieves a training performance similar to that of traditional methods, but enables layer-parallelism and thus provides speedup over layer-serial methods through greater concurrency.},
  archive      = {J_SIMODS},
  author       = {Stefanie Günther and Lars Ruthotto and Jacob B. Schroder and Eric C. Cyr and Nicolas R. Gauger},
  doi          = {10.1137/19M1247620},
  journal      = {SIAM Journal on Mathematics of Data Science},
  number       = {1},
  pages        = {1-23},
  shortjournal = {SIAM J. Math. Data Sci.},
  title        = {Layer-parallel training of deep residual neural networks},
  volume       = {2},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
