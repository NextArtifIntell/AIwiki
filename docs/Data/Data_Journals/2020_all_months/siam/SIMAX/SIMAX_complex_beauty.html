<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SIMAX_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="simax---76">SIMAX - 76</h2>
<ul>
<li><details>
<summary>
(2020). Computing the kreiss constant of a matrix. <em>SIMAX</em>,
<em>41</em>(4), 1944–1975. (<a
href="https://doi.org/10.1137/19M1275127">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We establish the first globally convergent algorithms for computing the Kreiss constant of a matrix to arbitrary accuracy. We propose three different iterations for continuous-time Kreiss constants and analogues for discrete-time Kreiss constants. With standard eigensolvers, the methods do $\mathcal{O}(n^6)$ work, but we show how this theoretical work complexity can be lowered to $\mathcal{O}(n^4)$ on average and $\mathcal{O}(n^5)$ in the worst case via divide-and-conquer variants. Finally, locally optimal Kreiss constant approximations can be efficiently obtained for large-scale matrices via optimization.},
  archive      = {J_SIMAX},
  author       = {Tim Mitchell},
  doi          = {10.1137/19M1275127},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1944-1975},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Computing the kreiss constant of a matrix},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A fast block <span
class="math inline"><em>α</em></span>-circulant preconditoner for
all-at-once systems from wave equations. <em>SIMAX</em>, <em>41</em>(4),
1912–1943. (<a href="https://doi.org/10.1137/19M1309869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a fast block $\alpha$-circulant preconditioner for solving the nonsymmetric linear system arising from an all-at-once implicit discretization scheme in time for the wave equation. As a generalization of the well-known block circulant preconditioning technique, the proposed block $\alpha$-circulant preconditioner can also be efficiently inverted in a parallel-in-time manner. The complex eigenvalues of the preconditioned matrix are fully derived in explicit expression and its diagonalizability is also shown. Furthermore, a mesh-independent convergence rate of the preconditioned GMRES method is derived under certain conditions. Building upon the proposed preconditioner, two stationary iterative methods with uniform asymptotic convergence rates were also presented. The extension of our preconditioner within a simplified Newton iteration to the nonlinear wave equation is also discussed. Both linear and nonlinear numerical examples are given to illustrate the promising performance of our proposed block $\alpha$-circulant preconditioner and also validate our theoretical results on convergence analysis.},
  archive      = {J_SIMAX},
  author       = {Jun Liu and Shu-Lin Wu},
  doi          = {10.1137/19M1309869},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1912-1943},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A fast block $\alpha$-circulant preconditoner for all-at-once systems from wave equations},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Rank one perturbations of matrix pencils. <em>SIMAX</em>,
<em>41</em>(4), 1889–1911. (<a
href="https://doi.org/10.1137/19M1279411">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For the first time in the literature we completely resolve the open problem of describing the possible Kronecker invariants of an arbitrary matrix pencil under rank one perturbations. The solution is explicit and constructive, and it is valid for arbitrary pencils.},
  archive      = {J_SIMAX},
  author       = {Marija Dodig and Marko Stošić},
  doi          = {10.1137/19M1279411},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1889-1911},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Rank one perturbations of matrix pencils},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient preconditioning for time fractional diffusion
inverse source problems. <em>SIMAX</em>, <em>41</em>(4), 1857–1888. (<a
href="https://doi.org/10.1137/20M1320304">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider an inverse problem with quasi-boundary value regularization for recovering a source term of the time fractional diffusion equations from the final observation data. In particular, a two-by-two block linear system arising from the problem is studied. We propose a fast preconditioning technique by approximating the Schur complement in the system using a product of some factors, motivated by an approximate diagonalization of one of the blocks. The eigenvalues of the preconditioned system are shown to be clustered around 1, and the fast convergence of the methods is guaranteed theoretically. We also present an approach for selecting the regularization parameter of the quasi-boundary value method. Numerical experiments are carried out to demonstrate the effectiveness of our method.},
  archive      = {J_SIMAX},
  author       = {Rihuan Ke and Michael K. Ng and Ting Wei},
  doi          = {10.1137/20M1320304},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1857-1888},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Efficient preconditioning for time fractional diffusion inverse source problems},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The GSVD: Where are the ellipses?, matrix trigonometry, and
more. <em>SIMAX</em>, <em>41</em>(4), 1826–1856. (<a
href="https://doi.org/10.1137/18M1234412">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper provides an advanced mathematical theory of the generalized singular value decomposition (GSVD) and its applications. We explore the geometry of the GSVD providing a long-sought-for picture which includes a horizontal and a vertical multiaxis. We further propose that the GSVD provides natural coordinates for the Grassmann manifold. This paper proves a theorem showing how the finite generalized singular values do or do not relate to the singular values of $AB^\dagger$. We then turn to applications, arguing that this geometrical theory is natural for understanding existing applications and recognizing opportunities for new applications. In particular the generalized singular vectors play a direct and as natural a mathematical role for certain applications as the singular vectors do for the SVD. In the same way that experts on the SVD often prefer not to cast SVD problems as eigenproblems, we propose that the GSVD, often cast as a generalized eigenproblem, is perhaps best cast in its natural setting. We illustrate this theoretical approach and the natural multiaxes (with labels from technical domains) in the context of applications where the GSVD arises: Tikhonov regularization (unregularized versus regularized), genome reconstruction (humans versus yeast), signal processing (signal versus noise), and statistical analysis such as analysis of variance and discriminant analysis (between clusters versus within clusters.) With the aid of our ellipse figure, we encourage the labeling of the natural multiaxes in any GSVD problem.},
  archive      = {J_SIMAX},
  author       = {Alan Edelman and Yuyang Wang},
  doi          = {10.1137/18M1234412},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1826-1856},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {The GSVD: Where are the ellipses?, matrix trigonometry, and more},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The epsilon-alternating least squares for orthogonal
low-rank tensor approximation and its global convergence.
<em>SIMAX</em>, <em>41</em>(4), 1797–1825. (<a
href="https://doi.org/10.1137/19M1303113">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The epsilon alternating least squares ($\epsilon$-ALS) is developed and analyzed for canonical polyadic decomposition (approximation) of a higher-order tensor where one or more of the factor matrices are assumed to be columnwise orthonormal. It is shown that the algorithm globally converges to a KKT point for all tensors without any assumption. For the original ALS, by further studying the properties of the polar decomposition, we also establish its global convergence under a reality assumption not stronger than those in the literature. These results completely address a question concerning the global convergence raised in [L. Wang, M. T. Chu, and B. Yu, SIAM J. Matrix Anal. Appl., 36 (2015), pp. 1--19]. In addition, an initialization procedure is proposed, which possesses a provable lower bound when the number of columnwise orthonormal factors is one. Preliminary numerical experiments show that the proposed initialization procedure can help in improving the efficiency and effectiveness of $\epsilon$-ALS.},
  archive      = {J_SIMAX},
  author       = {Yuning Yang},
  doi          = {10.1137/19M1303113},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1797-1825},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {The epsilon-alternating least squares for orthogonal low-rank tensor approximation and its global convergence},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonlinear eigenvector methods for convex minimization over
the numerical range. <em>SIMAX</em>, <em>41</em>(4), 1771–1796. (<a
href="https://doi.org/10.1137/18M1234473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the optimization problem in which a continuous convex function is to be minimized over the joint numerical range of two Hermitian matrices. When those matrices are of large size, solving such problems by convex optimization can be computationally expensive. The goal of this paper is to present a novel nonlinear eigenvector method to accelerate the computation. We will show that the global minimizer of the optimization problem corresponds to a solution of a nonlinear eigenvalue problem with eigenvector nonlinearity (NEPv). The special structure of this NEPv allows for an efficient sequential subspace search algorithm, which is a nonlinear analogue to the NEPv of the commonly applied locally optimal conjugate gradient descent methods for Hermitian linear eigenvalue problems. Our new algorithm can be proven globally convergent to an eigenvector of the NEPv. Implementation details such as block iteration and preconditioning will be discussed. Numerical examples, with applications in computing the coercivity constant of boundary integral operators and solving multicast beamforming problems, show the effectiveness of our approach.},
  archive      = {J_SIMAX},
  author       = {Ding Lu},
  doi          = {10.1137/18M1234473},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1771-1796},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Nonlinear eigenvector methods for convex minimization over the numerical range},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Structured random sketching for PDE inverse problems.
<em>SIMAX</em>, <em>41</em>(4), 1742–1770. (<a
href="https://doi.org/10.1137/20M1310497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For an overdetermined system ${A}{x} \approx {b}$ with ${A}$ and ${b}$ given, the least-squares (LS) formulation $\min_x \, \|{A}{x}-{b}\|_2$ is often used to find an acceptable solution ${x}$. The cost of solving this problem depends on the dimensions of ${A}$, which are large in many practical instances. This cost can be reduced by the use of random sketching, in which we choose a matrix ${S}$ with many fewer rows than ${A}$ and ${b}$ and solve the sketched LS problem $\min_x \, \|{S}({A} {x}-{b})\|_2$ to obtain an approximate solution to the original LS problem. Significant theoretical and practical progress has been made in the last decade in designing the appropriate structure and distribution for the sketching matrix ${S}$. When ${A}$ and ${b}$ arise from discretizations of a PDE-based inverse problem, tensor structure is often present in ${A}$ and ${b}$. For reasons of practical efficiency, ${S}$ should be designed to have a structure consistent with that of ${A}$. Can we claim similar approximation properties for the solution of the sketched LS problem with structured ${S}$ as for fully random ${S}$? We give estimates that relate the quality of the solution of the sketched LS problem to the size of the structured sketching matrices for two different structures. Our results are among the first known for random sketching matrices whose structure is suitable for use in PDE inverse problems.},
  archive      = {J_SIMAX},
  author       = {Ke Chen and Qin Li and Kit Newton and Stephen J. Wright},
  doi          = {10.1137/20M1310497},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1742-1770},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Structured random sketching for PDE inverse problems},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Probabilistic error analysis for inner products.
<em>SIMAX</em>, <em>41</em>(4), 1726–1741. (<a
href="https://doi.org/10.1137/19M1270434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic models are proposed for bounding the forward error in the numerically computed inner product (dot product, scalar product) between two real $n$-vectors. We derive probabilistic perturbation bounds as well as probabilistic roundoff error bounds for the sequential accumulation of the inner product. These bounds are nonasymptotic, explicit, with minimal assumptions, and with a clear relationship between failure probability and relative error. The roundoffs are represented as bounded, zero-mean random variables that are independent or have conditionally independent means. Our probabilistic bounds are based on Azuma&#39;s inequality and its associated martingale, which mirrors the sequential order of computations. The derivation of forward error bounds “from first principles” has the advantage of producing condition numbers that are customized for the probabilistic bounds. Numerical experiments confirm that our bounds are more informative, often by several orders of magnitude, than traditional deterministic bounds---even for small vector dimensions $n$ and very stringent success probabilities. In particular the probabilistic roundoff error bounds are functions of $\sqrt{n}$ rather than $n$, thus giving a quantitative confirmation of Wilkinson&#39;s intuition. The paper concludes with a critical assessment of the probabilistic approach.},
  archive      = {J_SIMAX},
  author       = {Ilse C. F. Ipsen and Hua Zhou},
  doi          = {10.1137/19M1270434},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1726-1741},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Probabilistic error analysis for inner products},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hybrid matrix compression for high-frequency problems.
<em>SIMAX</em>, <em>41</em>(4), 1704–1725. (<a
href="https://doi.org/10.1137/19M124280X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Boundary element methods for the Helmholtz equation lead to large dense matrices that can only be handled if efficient compression techniques are used. Directional compression techniques can reach good compression rates even for high-frequency problems. Currently there are two approaches to directional compression: Analytic methods approximate the kernel function, while algebraic methods approximate submatrices. Analytic methods are quite fast and proven to be robust, while algebraic methods yield significantly better compression rates. We present a hybrid method that combines the speed and reliability of analytic methods with the good compression rates of algebraic methods.},
  archive      = {J_SIMAX},
  author       = {Steffen Börm and Christina Börst},
  doi          = {10.1137/19M124280X},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1704-1725},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Hybrid matrix compression for high-frequency problems},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computing enclosures for the matrix exponential.
<em>SIMAX</em>, <em>41</em>(4), 1674–1703. (<a
href="https://doi.org/10.1137/19M1263431">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a review of old interval arithmetic techniques, and develop new ones, for computing enclosures for all entries of the exact exponential of a matrix. This means that all the rounding and truncation errors committed in the course of computation are rigorously taken into account, and the result is mathematically guaranteed to contain the correct matrix exponential. We consider algorithms relying on verified spectral decomposition, two variants relying on Taylor series expansion, a Padé approximation, and a contour integration approach, together with a Chebyshev approximation--based method which is designed for Hermitian matrices. Most of our methods use the scaling and squaring framework and are examined when applied to both the original matrix and an approximate diagonalization. In addition to a comparative study of algorithms, several illustrative numerical examples are given.},
  archive      = {J_SIMAX},
  author       = {Andreas Frommer and Behnam Hashemi},
  doi          = {10.1137/19M1263431},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1674-1703},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Computing enclosures for the matrix exponential},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Low-rank approximation in the frobenius norm by column and
row subset selection. <em>SIMAX</em>, <em>41</em>(4), 1651–1673. (<a
href="https://doi.org/10.1137/19M1281848">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A CUR approximation of a matrix $A$ is a particular type of low-rank approximation $A\approx CUR$, where $C$ and $R$ consist of columns and rows of $A$, respectively. One way to obtain such an approximation is to apply column subset selection to $A$ and $A^T$. In this work, we describe a numerically robust and much faster variant of the column subset selection algorithm proposed by Deshpande and Rademacher, which guarantees an error close to the best approximation error in the Frobenius norm. For cross approximation, in which $U$ is required to be the inverse of a submatrix of $A$ described by the intersection of $C$ and $R$, we obtain a new algorithm with an error bound that stays within a factor $k+1$ of the best rank-$k$ approximation error in the Frobenius norm. To the best of our knowledge, this is the first deterministic polynomial-time algorithm for which this factor is bounded by a polynomial in $k$. Our derivation and analysis of the algorithm is based on derandomizing a recent existence result by Zamarashkin and Osinsky. To illustrate the versatility of our new column subset selection algorithm, an extension to low multilinear rank approximations of tensors is provided as well.},
  archive      = {J_SIMAX},
  author       = {Alice Cortinovis and Daniel Kressner},
  doi          = {10.1137/19M1281848},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1651-1673},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Low-rank approximation in the frobenius norm by column and row subset selection},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Unit triangular factorization of the matrix symplectic
group. <em>SIMAX</em>, <em>41</em>(4), 1630–1650. (<a
href="https://doi.org/10.1137/19M1308839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we prove that any symplectic matrix can be factored into no more than 9 unit triangular symplectic matrices. This structure-preserving factorization of the symplectic matrices immediately reveals two well-known features that (i) the determinant of any symplectic matrix is one and (ii) the matrix symplectic group is path connected, as well as a new feature that (iii) all the unit triangular symplectic matrices form a set of generators of the matrix symplectic group. Furthermore, this factorization yields effective methods for the unconstrained parametrization of the matrix symplectic group as well as its structured subsets. The unconstrained parametrization enables us to apply faster and more efficient unconstrained optimization algorithms to the problems with symplectic constraints under certain circumstances.},
  archive      = {J_SIMAX},
  author       = {Pengzhan Jin and Yifa Tang and Aiqing Zhu},
  doi          = {10.1137/19M1308839},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1630-1650},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Unit triangular factorization of the matrix symplectic group},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Solving the problem of simultaneous diagonalization of
complex symmetric matrices via congruence. <em>SIMAX</em>,
<em>41</em>(4), 1616–1629. (<a
href="https://doi.org/10.1137/19M1280430">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We provide a solution to the problem of simultaneous diagonalization via congruence of a given set of $m$ complex symmetric $n\times n$ matrices ${A_{1},\ldots,A_{m}}$, by showing that it can be reduced to a possibly lower-dimensional problem where the question is rephrased in terms of the classical problem of simultaneous diagonalization via similarity of a new related set of matrices. We provide a procedure to determine in a finite number of steps whether or not a set of matrices is simultaneously diagonalizable by congruence. This solves a long-standing problem in the complex case.},
  archive      = {J_SIMAX},
  author       = {Miguel D. Bustamante and Pauline Mellon and M. Victoria Velasco},
  doi          = {10.1137/19M1280430},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1616-1629},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Solving the problem of simultaneous diagonalization of complex symmetric matrices via congruence},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Robust preconditioners for multiple saddle point problems
and applications to optimal control problems. <em>SIMAX</em>,
<em>41</em>(4), 1590–1615. (<a
href="https://doi.org/10.1137/19M1308426">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we consider multiple saddle point problems with block tridiagonal Hessian in a Hilbert space setting. Well-posedness and the related issue of preconditioning are discussed. We give a characterization of all block structured norms which ensure well-posedness of multiple saddle point problems as a helpful tool for constructing block diagonal preconditioners. We subsequently apply our findings to a general class of PDE-constrained optimal control problems containing a regularization parameter $\alpha$ and derive $\alpha$-robust preconditioners for the corresponding optimality systems. Finally, we demonstrate the generality of our approach with two optimal control problems related to the heat and the wave equation, respectively. Preliminary numerical experiments support the feasibility of our method.},
  archive      = {J_SIMAX},
  author       = {Alexander Beigl and Jarle Sogn and Walter Zulehner},
  doi          = {10.1137/19M1308426},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1590-1615},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Robust preconditioners for multiple saddle point problems and applications to optimal control problems},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On iterative solution of the extended normal equations.
<em>SIMAX</em>, <em>41</em>(4), 1571–1589. (<a
href="https://doi.org/10.1137/19M1288644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Given a full-rank matrix $A \in \mathbb{R}^{m\times n}$ ($m\geq n$), we consider a special class of linear systems $A^T\! Ax=A^T\! b+c$ with $x, c \in \mathbb{R}^{n}$ and $b \in \mathbb{R}^{m}$, which we refer to as the extended normal equations. The occurrence of $c$ gives rise to a problem with a different conditioning from the standard normal equations and prevents direct application of standard methods for least squares. Hence, we seek more insights on theoretical and practical aspects of the solution of such problems. We propose an explicit formula for the structured condition number, which allows us to compute a more accurate estimate of the forward error than the standard one used for generic linear systems, which does not take into account the structure of the perturbations. The relevance of our estimate is shown on a set of synthetic test problems. Then, we propose a new iterative solution method that, as in the case of normal equations, takes advantage of the structure of the system to avoid unstable computations such as forming $A^T\! A$ explicitly. Numerical experiments highlight the increased robustness and accuracy of the proposed method compared to standard iterative methods. It is also found that the new method can compare to standard direct methods in terms of solution accuracy.},
  archive      = {J_SIMAX},
  author       = {Henri Calandra and Serge Gratton and Elisa Riccietti and Xavier Vasseur},
  doi          = {10.1137/19M1288644},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1571-1589},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {On iterative solution of the extended normal equations},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A multigrid method for nonlocal problems: Non–diagonally
dominant or toeplitz-plus-tridiagonal systems. <em>SIMAX</em>,
<em>41</em>(4), 1546–1570. (<a
href="https://doi.org/10.1137/18M1210460">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nonlocal problems have been used to model very different applied scientific phenomena which involve the fractional Laplacian when one looks at the Lévy processes and stochastic interfaces. This paper deals with the nonlocal problems on a bounded domain where the stiffness matrices of the resulting systems are Toeplitz plus tridiagonal or far from being diagonally dominant as occurs when dealing with linear finite element approximations. By exploiting a weakly diagonally dominant Toeplitz property of the stiffness matrices, the optimal convergence of the two-grid method is well established in [Fiorentino and Serra-Capizzano, SIAM J. Sci. Comput., 17 (1996), pp. 1068--1081; Chen and Deng, SIAM J. Matrix Anal. Appl., 38 (2017), pp. 869--890], and there are still questions about the best ways to define the coarsening and interpolation operators when the stiffness matrix is far from being weakly diagonally dominant [Stüben, J. Comput. Appl. Math., 128 (2001), pp. 281--309]. In this work, using spectral indications from our analysis of the involved matrices, the simple (traditional) restriction operator and prolongation operator are employed in order to handle general algebraic systems which are neither Toeplitz nor weakly diagonally dominant corresponding to the fractional Laplacian kernel and the constant kernel, respectively. We focus our efforts on providing the detailed proof of the convergence of the two-grid method for such situations. Moreover, the convergence of the full multigrid is also discussed with the constant kernel. The numerical experiments are performed to verify the convergence with only $\mathcal{O}(N {log} N)$ complexity by the fast Fourier transform, where $N$ is the number of the grid points.},
  archive      = {J_SIMAX},
  author       = {Minghua Chen and Sven-Erik Ekström and Stefano Serra-Capizzano},
  doi          = {10.1137/18M1210460},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1546-1570},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A multigrid method for nonlocal problems: Non--diagonally dominant or toeplitz-plus-tridiagonal systems},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Coalescing eigenvalues and crossing eigencurves of
1-parameter matrix flows. <em>SIMAX</em>, <em>41</em>(4), 1528–1545. (<a
href="https://doi.org/10.1137/19M1286141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We investigate the eigenvalue curves of 1-parameter Hermitian and general complex or real matrix flows $A(t)$ in light of their geometry and the uniform decomposability of $A(t)$ for all parameters $t$. The results by Hund and by von Neumann and Wigner in the 1920s for eigencurve crossings of “generic” Hermitian matrix flows $A(t) = (A(t))^*$ are clarified. A conjecture on extending these results to general nonnormal or non-Hermitian 1-parameter matrix flows is formulated and investigated. An algorithm to compute the block dimensions of uniformly decomposable Hermitian matrix flows is described and tested. The algorithm uses the Zhang neural network method to compute the time-varying matrix eigenvalue curves of $A(t)$ for $t_o \leq t\leq t_f$. Similar efforts for general complex matrix flows are described. This extension leads to many new and open problems. Specifically, we point to the difficult relationship between the geometry of eigencurves for general complex matrix flows $A(t)$ and a general flow&#39;s decomposability into block-diagonal form via one fixed unitary or general matrix similarity for all parameters $t$.},
  archive      = {J_SIMAX},
  author       = {Frank Uhlig},
  doi          = {10.1137/19M1286141},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1528-1545},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Coalescing eigenvalues and crossing eigencurves of 1-parameter matrix flows},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nearest matrix polynomials with a specified elementary
divisor. <em>SIMAX</em>, <em>41</em>(4), 1505–1527. (<a
href="https://doi.org/10.1137/19M1286505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of finding the distance from a given $n \times n$ matrix polynomial of degree $k$ to the set of matrix polynomials having the elementary divisor $(\lambda-\lambda_0)^j, \, j \geqslant r,$ for fixed scalar $\lambda_0 $ and $2 \leqslant r \leqslant kn$ is considered. For regular matrix polynomials the problem is shown to be equivalent to finding minimal structure-preserving perturbations such that a certain block Toeplitz matrix becomes suitably rank deficient. This is then used to characterize the distance via two different optimizations. The first one shows that if $\lambda_0$ is not already an eigenvalue of the matrix polynomial, then the problem is equivalent to computing a generalized notion of a structured singular value. The distance is computed via algorithms like BFGS and MATLAB&#39;s globalsearch algorithm from the second optimization. Upper and lower bounds of the distance are also derived, and numerical experiments are performed to compare them with the computed values of the distance.},
  archive      = {J_SIMAX},
  author       = {Biswajit Das and Shreemayee Bora},
  doi          = {10.1137/19M1286505},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1505-1527},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Nearest matrix polynomials with a specified elementary divisor},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Krylov methods for low-rank regularization. <em>SIMAX</em>,
<em>41</em>(4), 1477–1504. (<a
href="https://doi.org/10.1137/19M1302727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces new solvers for the computation of low-rank approximate solutions to large-scale linear problems, with a particular focus on the regularization of linear inverse problems. Although Krylov methods incorporating explicit projections onto low-rank subspaces are already used for well-posed systems that arise from discretizing stochastic or time-dependent PDEs, we are mainly concerned with algorithms that solve the so-called nuclear norm regularized problem, where a suitable nuclear norm penalization on the solution is imposed alongside a fit-to-data term expressed in the 2-norm: this has the effect of implicitly enforcing low-rank solutions. By adopting an iteratively reweighted norm approach, the nuclear norm regularized problem is reformulated as a sequence of quadratic problems, which can then be efficiently solved using Krylov methods, giving rise to an inner-outer iteration scheme. Our approach differs from the other solvers available in the literature in that (a) Kronecker product properties are exploited to define the reweighted 2-norm penalization terms; (b) efficient preconditioned Krylov methods replace gradient (projection) methods; (c) the regularization parameter can be efficiently and adaptively set along the iterations. Furthermore, we reformulate within the framework of flexible Krylov methods both the new inner-outer methods for nuclear norm regularization and some of the existing Krylov methods incorporating low-rank projections. This results in an even more computationally efficient (but heuristic) strategy that does not rely on an inner-outer iteration scheme. Numerical experiments including image deblurring, computed tomography, and inpainting show that our new solvers are competitive with other state-of-the-art solvers for low-rank problems and deliver reconstructions of increased quality with respect to other classical Krylov methods.},
  archive      = {J_SIMAX},
  author       = {Silvia Gazzola and Chang Meng and James G. Nagy},
  doi          = {10.1137/19M1302727},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {4},
  pages        = {1477-1504},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Krylov methods for low-rank regularization},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fast solution methods for convex quadratic optimization of
fractional differential equations. <em>SIMAX</em>, <em>41</em>(3),
1443–1476. (<a href="https://doi.org/10.1137/19M128288X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we present numerical methods suitable for solving convex quadratic fractional differential equation (FDE) constrained optimization problems, with box constraints on the state and/or control variables. We develop an alternating direction method of multipliers (ADMM) framework, which uses preconditioned Krylov subspace solvers for the resulting subproblems. The latter allows us to tackle a range of partial differential equation (PDE) optimization problems with box constraints, posed on space-time domains, that were previously out of the reach of state-of-the-art preconditioners. In particular, by making use of the powerful generalized locally Toeplitz (GLT) sequences theory, we show that any existing GLT structure present in the problem matrices is preserved by ADMM, and we propose some preconditioning methodologies that could be used within the solver, to demonstrate the generality of the approach. Focusing on convex quadratic programs with time-dependent 2-dimensional FDE constraints, we derive multilevel circulant preconditioners, which may be embedded within Krylov subspace methods, for solving the ADMM subproblems. Discretized versions of FDEs involve large dense linear systems. In order to overcome this difficulty, we design a recursive linear algebra, which is based on the fast Fourier transform (FFT). We manage to keep the storage requirements linear, with respect to the grid size $N$, while ensuring an order $N \log N$ computational complexity per iteration of the Krylov solver. We implement the proposed method and demonstrate its scalability, generality, and efficiency through a series of experiments over different setups of the FDE optimization problem.},
  archive      = {J_SIMAX},
  author       = {Spyridon Pougkakiotis and John W. Pearson and Santolo Leveque and Jacek Gondzio},
  doi          = {10.1137/19M128288X},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1443-1476},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Fast solution methods for convex quadratic optimization of fractional differential equations},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Tensor ring decomposition: Optimization landscape and
one-loop convergence of alternating least squares. <em>SIMAX</em>,
<em>41</em>(3), 1416–1442. (<a
href="https://doi.org/10.1137/19M1270689">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we study the tensor ring decomposition and its associated numerical algorithms. We establish a sharp transition of algorithmic difficulty of the optimization problem as the bond dimension increases: On one hand, we show the existence of spurious local minima for the optimization landscape even when the tensor ring format is much overparameterized, i.e., with bond dimension much larger than that of the true target tensor. On the other hand, when the bond dimension is further increased, we establish one-loop convergence for the alternating least squares algorithm for the tensor ring decomposition. The theoretical results are complemented by numerical experiments for both local minima and the one-loop convergence for the alternating least squares algorithm.},
  archive      = {J_SIMAX},
  author       = {Ziang Chen and Yingzhou Li and Jianfeng Lu},
  doi          = {10.1137/19M1270689},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1416-1442},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Tensor ring decomposition: Optimization landscape and one-loop convergence of alternating least squares},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Eigenpairs of wilkinson matrices. <em>SIMAX</em>,
<em>41</em>(3), 1388–1415. (<a
href="https://doi.org/10.1137/20M1327343">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the 1950s J. H. Wilkinson introduced two families of symmetric tridiagonal integer matrices. Most of the eigenvalues are close to diagonal entries. We develop the structure of their eigenvectors in a natural way which reveals that these eigenvectors all look the same to the naked eye. The shape of the envelopes is like twin peaks (or a badly dented bell curve). We also analyze the eigenvectors of the remaining noninteger eigenvalues.},
  archive      = {J_SIMAX},
  author       = {Carla Ferreira and Beresford Parlett},
  doi          = {10.1137/20M1327343},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1388-1415},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Eigenpairs of wilkinson matrices},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating higher-order moments using symmetric tensor
decomposition. <em>SIMAX</em>, <em>41</em>(3), 1369–1387. (<a
href="https://doi.org/10.1137/19M1299633">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the problem of decomposing higher-order moment tensors, i.e., the sum of symmetric outer products of data vectors. Such a decomposition can be used to estimate the means in a Gaussian mixture model and for other applications in machine learning. The $d$th-order empirical moment tensor of a set of $p$ observations of $n$ variables is a symmetric $d$-way tensor. Our goal is to find a low-rank tensor approximation comprising $r\ll p$ symmetric outer products. The challenge is that forming the empirical moment tensors costs $O(pn^d)$ operations and $O(n^d)$ storage, which may be prohibitively expensive; additionally, the algorithm to compute the low-rank approximation costs $O(n^d)$ per iteration. Our contribution is avoiding formation of the moment tensor, computing the low-rank tensor approximation of the moment tensor implicitly using $O(pnr)$ operations per iteration and no extra memory. This advance opens the door to more applications of higher-order moments since they can now be efficiently computed. We present numerical evidence of the computational savings and show an example of estimating the means for higher-order moments.},
  archive      = {J_SIMAX},
  author       = {Samantha Sherman and Tamara G. Kolda},
  doi          = {10.1137/19M1299633},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1369-1387},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Estimating higher-order moments using symmetric tensor decomposition},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Geometric matrix midranges. <em>SIMAX</em>, <em>41</em>(3),
1347–1368. (<a href="https://doi.org/10.1137/19M1273475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We define geometric matrix midranges for positive definite Hermitian matrices and study the midrange problem from a number of perspectives. Special attention is given to the midrange of two positive definite matrices before considering the extension of the problem to more than two matrices. We compare matrix midrange statistics with the scalar and vector midrange problem and note the special significance of the matrix problem from a computational standpoint. We also study various aspects of geometric matrix midrange statistics from the viewpoint of linear algebra, differential geometry, and convex optimization. A solution to the $N$-point problem is offered via convex optimization.},
  archive      = {J_SIMAX},
  author       = {Cyrus Mostajeran and Christian Grussler and Rodolphe Sepulchre},
  doi          = {10.1137/19M1273475},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1347-1368},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Geometric matrix midranges},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The lanczos algorithm under few iterations: Concentration
and location of the output. <em>SIMAX</em>, <em>41</em>(3), 1312–1346.
(<a href="https://doi.org/10.1137/19M1275231">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study the Lanczos algorithm where the initial vector is sampled uniformly from $\mathbb{S}^{n-1}$. Let $A$ be an $n \times n$ Hermitian matrix. We show that when run for few iterations, the output of Lanczos on $A$ is almost deterministic. More precisely, we show that for any $ \varepsilon \in (0, 1)$ there exists $c &gt;0$ depending only on $\varepsilon$ and a certain global property of the spectrum of $A$ (in particular, not depending on $n$) such that when Lanczos is run for at most $c \log n$ iterations, the output Jacobi coefficients deviate from their medians by $t$ with probability at most $\exp(-n^\varepsilon t^2)$ for $t 0$. Then we show that for large enough $n$, and for $k=O(\sqrt{\log n})$, the Jacobi coefficients output after $k$ iterations concentrate around those for $\mu$. The asymptotic setting is relevant since Lanczos is often used to approximate the spectral density of an infinite-dimensional operator by way of the Jacobi coefficients; our result provides some theoretical justification for this approach. In a different direction, we show that Lanczos fails with high probability to identify outliers of the spectrum when run for at most $c&#39; \log n$ iterations, where again $c&#39;$ depends only on the same global property of the spectrum of $A$. Classical results imply that the bound $c&#39; \log n$ is tight up to a constant factor.},
  archive      = {J_SIMAX},
  author       = {Jorge Garza-Vargas and Archit Kulkarni},
  doi          = {10.1137/19M1275231},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1312-1346},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {The lanczos algorithm under few iterations: Concentration and location of the output},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Spectrum-based stability analysis and stabilization of
time-periodic time-delay systems. <em>SIMAX</em>, <em>41</em>(3),
1284–1311. (<a href="https://doi.org/10.1137/19M1275851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop an eigenvalue-based approach for the stability assessment and stabilization of linear systems with multiple delays and periodic coefficient matrices. Delays and period are assumed commensurate numbers, such that the Floquet multipliers can be characterized as eigenvalues of the monodromy operator and by the solutions of a finite-dimensional nonlinear eigenvalue problem, where the evaluation of the characteristic matrix involves solving an initial value problem. We demonstrate that such a dual interpretation can be exploited in a two-stage approach for computing dominant Floquet multipliers, where global approximation is combined with local corrections. Correspondingly, we also propose two novel characterizations of left eigenvectors. Finally, from the nonlinear eigenvalue problem formulation, we derive computationally tractable expressions for derivatives of Floquet multipliers with respect to parameters, which are beneficial in the context of stability optimization. Several numerical examples show the efficacy and applicability of the presented results.},
  archive      = {J_SIMAX},
  author       = {Wim Michiels and Luca Fenzi},
  doi          = {10.1137/19M1275851},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1284-1311},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Spectrum-based stability analysis and stabilization of time-periodic time-delay systems},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A hybrid penalty method for a class of optimization problems
with multiple rank constraints. <em>SIMAX</em>, <em>41</em>(3),
1260–1283. (<a href="https://doi.org/10.1137/19M1269919">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider the problem of minimizing a smooth objective over multiple rank constraints on Hankel structured matrices. These kinds of problems arise in system identification, system theory, and signal processing, where the rank constraints are typically “hard constraints.” To solve these problems, we propose a hybrid penalty method that combines a penalty method with a postprocessing scheme. Specifically, we solve the penalty subproblems until the penalty parameter reaches a given threshold, and then switch to a local alternating “pseudoprojection” method to further reduce constraint violation. Pseudoprojection is a generalization of the concept of projection. We show that a pseudoprojection onto a single low-rank Hankel structured matrix constraint can be computed efficiently by existing software such as SLRA [I. Markovsky and K. Usevich, J. Comput. Appl. Math., 256 (2014), pp. 278--292], under mild assumptions. We also demonstrate how the penalty subproblems in the hybrid penalty method can be solved by pseudoprojection-based optimization methods, and then present some convergence results for our hybrid penalty method. Finally, the efficiency of our method is illustrated by numerical examples.},
  archive      = {J_SIMAX},
  author       = {Tianxiang Liu and Ivan Markovsky and Ting Kei Pong and Akiko Takeda},
  doi          = {10.1137/19M1269919},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1260-1283},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A hybrid penalty method for a class of optimization problems with multiple rank constraints},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The AZ algorithm for least squares systems with a known
incomplete generalized inverse. <em>SIMAX</em>, <em>41</em>(3),
1237–1259. (<a href="https://doi.org/10.1137/19M1306385">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce an algorithm for the least squares solution of a rectangular linear system $Ax=b$, in which $A$ may be arbitrarily ill-conditioned. We assume that a complementary matrix $Z$ is known such that $A - AZ^*A$ is numerically low rank. Loosely speaking, $Z^*$ acts like a generalized inverse of $A$ up to a numerically low-rank error. We give several examples of $(A,Z)$ combinations in function approximation, where we can achieve high-order approximations in a number of non-standard settings: the approximation of functions on domains with irregular shapes, weighted least squares problems with highly skewed weights, and the spectral approximation of functions with localized singularities. The algorithm is most efficient when $A$ and $Z^*$ have fast matrix-vector multiplication and when the numerical rank of $A - AZ^*A$ is small.},
  archive      = {J_SIMAX},
  author       = {Vincent Coppé and Daan Huybrechs and Roel Matthysen and Marcus Webb},
  doi          = {10.1137/19M1306385},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1237-1259},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {The AZ algorithm for least squares systems with a known incomplete generalized inverse},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Uniform bounds for invariant subspace perturbations.
<em>SIMAX</em>, <em>41</em>(3), 1208–1236. (<a
href="https://doi.org/10.1137/19M1262760">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For a fixed symmetric matrix $A$ and symmetric perturbation $E$ we develop purely deterministic bounds on how invariant subspaces of $A$ and $A+E$ can differ when measured by a suitable “rowwise” metric rather than via traditional measures of subspace distance. Understanding perturbations of invariant subspaces with respect to such metrics is becoming increasingly important across a wide variety of applications and therefore necessitates new theoretical developments. Under minimal assumptions we develop new bounds on subspace perturbations under the two-to-infinity matrix norm and show in what settings these rowwise differences in the invariant subspaces can be significantly smaller than the analogous two or Frobenius norm differences. We also demonstrate that the constitutive pieces of our bounds are necessary absent additional assumptions and, therefore, our results provide a natural starting point for further analysis of specific problems. Lastly, we briefly discuss extensions of our bounds to scenarios where $A$ and/or $E$ are nonnormal matrices.},
  archive      = {J_SIMAX},
  author       = {Anil Damle and Yuekai Sun},
  doi          = {10.1137/19M1262760},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1208-1236},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Uniform bounds for invariant subspace perturbations},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sharp 2-norm error bounds for LSQR and the conjugate
gradient method. <em>SIMAX</em>, <em>41</em>(3), 1183–1207. (<a
href="https://doi.org/10.1137/19M1272822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the iterative method LSQR for solving $\min_x |Ax-b|_2$. LSQR is based on the Golub--Kahan bidiagonalization process and at every step produces an iterate that minimizes the norm of the residual vector over a Krylov subspace $\mathcal{K}_k$. The 2-norm of the error is known to decrease monotonically, although it is not minimized over $\mathcal{K}_k$. Given a lower bound on the smallest singular value of $A$, we show that in exact arithmetic the solution lies in the interior of a certain ellipsoid and that the LSQR iterate lies on the boundary of this ellipsoid. We use this result to derive new 2-norm error bounds for LSQR. Although our bounds are not much smaller than the existing ones, we show that they are sharp in the following sense: if the only information we use is our lower bound on $\sigma_{\min}(A)$ plus the information gained by running $k$ steps of LSQR, then our bounds cannot be improved. We also show how to choose a point with an error bound smaller than our corresponding bound for the LSQR error, although its true error is not necessarily smaller than the true LSQR error. As LSQR is formally equivalent to the conjugate gradient (CG) method applied to the normal equations $A{^T} Ax = A{^T} b$, we derive analogous error bounds for CG. Our bounds for CG apply to any system $Ax=b$ where $A$ is symmetric positive definite.},
  archive      = {J_SIMAX},
  author       = {Eric Hallman},
  doi          = {10.1137/19M1272822},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1183-1207},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Sharp 2-norm error bounds for LSQR and the conjugate gradient method},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Maximal acyclic subgraphs and closest stable matrices.
<em>SIMAX</em>, <em>41</em>(3), 1167–1182. (<a
href="https://doi.org/10.1137/19M1305422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a matrix approach to the maximal acyclic subgraph (MAS) problem by reducing it to find the closest nilpotent matrix to the matrix of the graph. Using recent results on the closest Schur stable systems and on minimizing the spectral radius over special sets of nonnegative matrices, we obtain an algorithm for finding an approximate solution of the MAS problem. Numerical results for graphs from 50 to 1500 vertices demonstrate its fast convergence and give a rate of approximation larger than 0.6 in most cases. This method also gives the precise solution for the following weakened version of MAS: find the minimal $r$ such that the graph can be made acyclic by cutting at most $r$ incoming edges from each vertex. We also consider several modifications in the case when each vertex is assigned its own maximal number $r_i$ of cut edges, and some of the edges are “untouchable.” Some applications are discussed.},
  archive      = {J_SIMAX},
  author       = {Aleksandar Cvetković and Vladimir Yu. Protasov},
  doi          = {10.1137/19M1305422},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1167-1182},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Maximal acyclic subgraphs and closest stable matrices},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). BiLQ: An iterative method for nonsymmetric linear systems
with a quasi-minimum error property. <em>SIMAX</em>, <em>41</em>(3),
1145–1166. (<a href="https://doi.org/10.1137/19M1290991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce an iterative method named BiLQ for solving general square linear systems $Ax=b$ that is based on the Lanczos biorthogonalization process defined by least-norm subproblems and that is a natural companion to BiCG and Qmr. Whereas the iterates of BiCG, Cgs, and BiCGStab may not exist when the tridiagonal projection of $A$ is singular, BiLQ is reliable on compatible systems even if $A$ is ill-conditioned or singular. As in the symmetric case, the BiCG residual is often smaller than the BiLQ residual, and when the BiCG iterate exists, an inexpensive transfer from the BiLQ iterate is possible. Although the Euclidean norm of the BiLQ error is usually not monotonic, it is monotonic in a different norm that depends on the Lanczos vectors. We establish a similar property for the Qmr residual. BiLQ combines with Qmr to take advantage of two initial vectors and solve a system and an adjoint system simultaneously at a cost similar to that of applying either method. We derive an analogous combination of Usymlq and Usymlqr based on the orthogonal tridiagonalization process. The resulting combinations, named BiLQR and TriLQR, may be used to estimate integral functionals involving the solution of a primal and an adjoint system. We compare BiLQR and TriLQR with Minres-qlp on a related augmented system, which performs a comparable amount of work and requires comparable storage. In our experiments, BiLQR terminates earlier than TriLQR and Minres-qlp in terms of residual and error of the primal and adjoint systems.},
  archive      = {J_SIMAX},
  author       = {Alexis Montoison and Dominique Orban},
  doi          = {10.1137/19M1290991},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1145-1166},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {BiLQ: An iterative method for nonsymmetric linear systems with a quasi-minimum error property},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hermitian tensor decompositions. <em>SIMAX</em>,
<em>41</em>(3), 1115–1144. (<a
href="https://doi.org/10.1137/19M1306889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hermitian tensors are generalizations of Hermitian matrices, but they have very different properties. Every complex Hermitian tensor is a sum of complex Hermitian rank-1 tensors. However, this is not true for the real case. We study basic properties for Hermitian tensors, such as Hermitian decompositions and Hermitian ranks. For canonical basis tensors, we determine their Hermitian ranks and decompositions. For real Hermitian tensors, we give a full characterization for them to have Hermitian decompositions over the real field. In addition to traditional flattening, Hermitian tensors have also Hermitian and Kronecker flattenings, which may give different lower bounds for Hermitian ranks. We also study other topics, such as eigenvalues, positive semidefiniteness, sum-of-squares representations, and separability.},
  archive      = {J_SIMAX},
  author       = {Jiawang Nie and Zi Yang},
  doi          = {10.1137/19M1306889},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1115-1144},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Hermitian tensor decompositions},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multiresolution low-rank tensor formats. <em>SIMAX</em>,
<em>41</em>(3), 1086–1114. (<a
href="https://doi.org/10.1137/19M1284579">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We describe a simple, black-box compression format for tensors with a multiscale structure. By representing the tensor as a sum of compressed tensors defined on increasingly coarse grids, we capture low-rank structures on each grid-scale, and we show how this leads to an increase in compression for a fixed accuracy. We devise an alternating algorithm to represent a given tensor in the multiresolution format and prove local convergence guarantees. In two dimensions, we provide examples that show that this approach can beat the Eckart--Young theorem, and for dimensions higher than two, we achieve higher compression than the tensor-train format on six real-world datasets. We also provide results on the closedness and stability of the tensor format and discuss how to perform common linear algebra operations on the level of the compressed tensors.},
  archive      = {J_SIMAX},
  author       = {Oscar Mickelin and Sertac Karaman},
  doi          = {10.1137/19M1284579},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1086-1114},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Multiresolution low-rank tensor formats},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Analytical low-rank compression via proxy point selection.
<em>SIMAX</em>, <em>41</em>(3), 1059–1085. (<a
href="https://doi.org/10.1137/19M1247838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It has been known in potential theory that, for some kernel matrices corresponding to well-separated point sets, fast analytical low-rank approximation can be achieved via the use of proxy points. This proxy point method gives a surprisingly convenient way of explicitly writing out approximate basis matrices for a kernel matrix. However, this elegant strategy is rarely known or used in the numerical linear algebra community. It still needs clear algebraic understanding of the theoretical background. Moreover, rigorous quantifications of the approximation errors and reliable criteria for the selection of the proxy points are still missing. In this work, we use contour integration to clearly justify the idea in terms of a class of important kernels. We further provide comprehensive accuracy analysis for the analytical compression and show how to choose nearly optimal proxy points. The analytical compression is then combined with fast rank-revealing factorizations to get compact low-rank approximations and also to select certain representative points. We provide the error bounds for the resulting overall low-rank approximation. This work thus gives a fast and reliable strategy for compressing those kernel matrices. Furthermore, it provides an intuitive way of understanding the proxy point method and bridges the gap between this useful analytical strategy and practical low-rank approximations. Some numerical examples help to further illustrate the ideas.},
  archive      = {J_SIMAX},
  author       = {Xin Ye and Jianlin Xia and Lexing Ying},
  doi          = {10.1137/19M1247838},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1059-1085},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Analytical low-rank compression via proxy point selection},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Generic symmetric matrix polynomials with bounded rank and
fixed odd grade. <em>SIMAX</em>, <em>41</em>(3), 1033–1058. (<a
href="https://doi.org/10.1137/19M1294964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We determine the generic complete eigenstructures for $n \times n$ complex symmetric matrix polynomials of odd grade $d$ and rank at most $r$. More precisely, we show that the set of $n \times n$ complex symmetric matrix polynomials of odd grade $d$, i.e., of degree at most $d$, and rank at most $r$ is the union of the closures of the $\lfloor rd/2\rfloor+1$ sets of symmetric matrix polynomials having certain, explicitly described, complete eigenstructures. Then we prove that these sets are open in the set of $n \times n$ complex symmetric matrix polynomials of odd grade $d$ and rank at most $r$. In order to prove the previous results, we need to derive necessary and sufficient conditions for the existence of symmetric matrix polynomials with prescribed grade, rank, and complete eigenstructure in the case where all their elementary divisors are different from each other and of degree $1$. An important remark on the results of this paper is that the generic eigenstructures identified in this work are completely different from the ones identified in previous works for unstructured and skew-symmetric matrix polynomials with bounded rank and fixed grade larger than 1, because the symmetric ones include eigenvalues while the others not. This difference requires using new techniques.},
  archive      = {J_SIMAX},
  author       = {Fernando De Terán and Andrii Dmytryshyn and Froilán M. Dopico},
  doi          = {10.1137/19M1294964},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1033-1058},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Generic symmetric matrix polynomials with bounded rank and fixed odd grade},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Partial smoothness of the numerical radius at matrices whose
fields of values are disks. <em>SIMAX</em>, <em>41</em>(3), 1004–1032.
(<a href="https://doi.org/10.1137/18M1236289">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Solutions to optimization problems involving the numerical radius often belong to the class of “disk matrices”: those whose field of values is a circular disk in the complex plane centered at zero. We investigate this phenomenon using the variational-analytic idea of partial smoothness. We give conditions under which the set of disk matrices is locally a manifold $\mathcal M$, with respect to which the numerical radius $r$ is partly smooth, implying that $r$ is smooth when restricted to $\mathcal M$ but strictly nonsmooth when restricted to lines transversal to $\mathcal M$. Consequently, minimizers of the numerical radius of a parametrized matrix often lie in $\mathcal M$. Partial smoothness holds, in particular, at $n\times n$ matrices with exactly $n-1$ nonzeros, all on the superdiagonal. On the other hand, in the real 18-dimensional vector space of complex $3\times 3$ matrices, the disk matrices comprise the closure of a semialgebraic manifold $\mathcal L$ with dimension 12, and the numerical radius is partly smooth with respect to $\mathcal L$.},
  archive      = {J_SIMAX},
  author       = {A. S. Lewis and M. L. Overton},
  doi          = {10.1137/18M1236289},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {1004-1032},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Partial smoothness of the numerical radius at matrices whose fields of values are disks},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Efficient reduction of compressed unitary plus low rank
matrices to hessenberg form. <em>SIMAX</em>, <em>41</em>(3), 984–1003.
(<a href="https://doi.org/10.1137/19M1280363">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present fast numerical methods for computing the Hessenberg reduction of a unitary plus low rank matrix $A=G+U V^H$, where $G\in \mathbb C^{n\times n}$ is a unitary matrix represented in some compressed format using $O(nk)$ parameters and $U$ and $V$ are $n\times k$ matrices with $k&lt; n$. At the core of these methods is a certain structured decomposition, referred to as an LFR decomposition, of $A$ as a product of three possibly perturbed unitary $k$ Hessenberg matrices of size $n$. It is shown that in most interesting cases an initial LFR decomposition of $A$ can be computed very cheaply. Then we prove structural properties of LFR decompositions by giving conditions under which the LFR decomposition of $A$ implies its Hessenberg shape. Finally, we describe a bulge chasing scheme for converting the initial LFR decomposition of $A$ into the LFR decomposition of a Hessenberg matrix by means of unitary transformations. The reduction can be performed at the overall computational cost of $O(n^2 k)$ arithmetic operations using $O(nk)$ storage. The computed LFR decomposition of the Hessenberg reduction of $A$ can be processed by the fast QR algorithm presented in [R. Bevilacqua, G. M. Del Corso, and L. Gemignani, Numer. Math., 144 (2020), pp. 23--53] in order to compute the eigenvalues of $A$ within the same costs.},
  archive      = {J_SIMAX},
  author       = {Roberto Bevilacqua and Gianna M. Del Corso and Luca Gemignani},
  doi          = {10.1137/19M1280363},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {984-1003},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Efficient reduction of compressed unitary plus low rank matrices to hessenberg form},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An eigenvalue-based method for the unbalanced procrustes
problem. <em>SIMAX</em>, <em>41</em>(3), 957–983. (<a
href="https://doi.org/10.1137/19M1270872">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel eigenvalue-based approach to solving the unbalanced orthogonal Procrustes problem. By making effective use of the necessary condition for the global minimizer and the orthogonal constraint, we shall first show that the unbalanced Procrustes problem can be equivalently transformed into an eigenvalue minimization whose solution can be computed by solving a related eigenvector-dependent nonlinear eigenvalue problem. Through the exploitation of certain techniques in the nonlinear eigenvalue computations, we adapt the standard self-consistent field (SCF) iteration to solve the resulting nonlinear eigenvalue problem. Theoretical convergence analysis of this customized SCF iteration is performed, and practical strategies for a more efficient numerical implementation are discussed. Our numerical experience on preliminary tests indicates that the proposed eigenvalue-based SCF iteration is a promising method for the unbalanced orthogonal Procrustes problem.},
  archive      = {J_SIMAX},
  author       = {Lei-Hong Zhang and Wei Hong Yang and Chungen Shen and Jiaqi Ying},
  doi          = {10.1137/19M1270872},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {3},
  pages        = {957-983},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {An eigenvalue-based method for the unbalanced procrustes problem},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A subspace framework for <span
class="math inline">𝒽<sub>∞</sub></span>-norm minimization.
<em>SIMAX</em>, <em>41</em>(2), 928–956. (<a
href="https://doi.org/10.1137/19M125892X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We deal with the minimization of the ${\mathcal H}_\infty$-norm of the transfer function of a parameter-dependent descriptor system over the set of admissible parameter values. Subspace frameworks are proposed for such minimization problems where the involved systems are of large order. The proposed algorithms are greedy interpolatary approaches inspired by our recent work [Aliyev et al., SIAM J. Matrix Anal. Appl., 38 (2017), pp. 1496--1516] for the computation of the ${\mathcal H}_\infty$-norm. In this work, we minimize the ${\mathcal H}_\infty$-norm of a reduced-order parameter-dependent system obtained by two-sided restrictions onto certain subspaces. Then we expand the subspaces so that Hermite interpolation properties hold between the full and reduced-order system at the optimal parameter value for the reduced-order system. We formally establish the superlinear convergence of the subspace frameworks under some smoothness and nondegeneracy assumptions. The fast convergence of the proposed frameworks in practice is illustrated by several large-scale systems.},
  archive      = {J_SIMAX},
  author       = {Nicat Aliyev and Peter Benner and Emre Mengi and Matthias Voigt},
  doi          = {10.1137/19M125892X},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {928-956},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A subspace framework for ${\mathcal h}_\infty$-norm minimization},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A core-chasing symplectic QR algorithm. <em>SIMAX</em>,
<em>41</em>(2), 901–927. (<a
href="https://doi.org/10.1137/19M1261079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A structure preserving QR algorithm is developed for a set of symplectic matrices with a certain rank condition. The algorithm is constructed based on a factorization presentation of a symplectic Hessenberg matrix and core-chasing type QR iterations. It preserves the symplectic structure in the sense that only unitary symplectic matrices are used for similarity transformations. The proposed algorithm is theoretically equivalent to Mehrmann&#39;s algorithm. In practice, it is more numerically stable. Two economic versions of the algorithm are also provided that use the symplectic structure more efficiently meanwhile at the risk of losing numerical stability. The same idea applies to the symplectic pairs and a core-chasing symplectic QZ algorithm can be developed.},
  archive      = {J_SIMAX},
  author       = {Hongguo Xu},
  doi          = {10.1137/19M1261079},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {901-927},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A core-chasing symplectic QR algorithm},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On fixed-point, krylov, and <span
class="math inline">2 × 2</span> block preconditioners for nonsymmetric
problems. <em>SIMAX</em>, <em>41</em>(2), 871–900. (<a
href="https://doi.org/10.1137/19M1298317">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The solution of matrices with a $2\times 2$ block structure arises in numerous areas of computational mathematics, such as PDE discretizations based on mixed-finite element methods, constrained optimization problems, or the implicit or steady state treatment of any system of PDEs with multiple dependent variables. Often, these systems are solved iteratively using Krylov methods and some form of block preconditioner. Under the assumption that one diagonal block is inverted exactly, this paper proves a direct equivalence between convergence of $2\times2$ block preconditioned Krylov or fixed-point iterations to a given tolerance, with convergence of the underlying preconditioned Schur-complement problem. In particular, results indicate that an effective Schur-complement preconditioner is a necessary and sufficient condition for rapid convergence of $2\times 2$ block-preconditioned GMRES, for arbitrary relative-residual stopping tolerances. A number of corollaries and related results give new insight into block preconditioning, such as the fact that approximate block-LDU or symmetric block-triangular preconditioners offer minimal reduction in iteration over block-triangular preconditioners, despite the additional computational cost. Theoretical results are verified numerically on a nonsymmetric steady linearized Navier--Stokes discretization, which also demonstrate that theory based on the assumption of an exact inverse of one diagonal block extends well to the more practical setting of inexact inverses.},
  archive      = {J_SIMAX},
  author       = {Ben S. Southworth and Abdullah A. Sivas and Sander Rhebergen},
  doi          = {10.1137/19M1298317},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {871-900},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {On fixed-point, krylov, and $2\times 2$ block preconditioners for nonsymmetric problems},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computationally efficient decompositions of oblique
projection matrices. <em>SIMAX</em>, <em>41</em>(2), 852–870. (<a
href="https://doi.org/10.1137/19M1288115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Oblique projection matrices arise in problems in weighted least squares, signal processing, and optimization. While these matrices can be potentially very large, their low-rank structure can be exploited for efficient computation. We propose fast and scalable algorithms for computing their eigendecomposition and singular value decomposition (SVD). Numerical experiments that compare our proposed approaches to existing methods, including randomized SVD, are presented. In addition, we test their accuracy on linear systems from equality constrained optimization problems.},
  archive      = {J_SIMAX},
  author       = {Johannes J. Brust and Roummel F. Marcia and Cosmin G. Petra},
  doi          = {10.1137/19M1288115},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {852-870},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Computationally efficient decompositions of oblique projection matrices},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Restrictively preconditioned conjugate gradient method for a
series of constantly augmented least squares problems. <em>SIMAX</em>,
<em>41</em>(2), 838–851. (<a
href="https://doi.org/10.1137/19M1284853">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this study, we analyze the real-time solution of a series of augmented least squares problems, which are generated by adding information to an original least squares model repetitively. Instead of solving the least squares problems directly, we transform them into a batch of saddle point linear systems and subsequently solve the linear systems using restrictively preconditioned conjugate gradient (RPCG) methods. Approximation of the new Schur complement is generated effectively based on a previously approximated Schur complement. Owing to the variations of the preconditioned conjugate gradient method, the proposed methods generate convergence results similar to the conjugate gradient method and achieve a very fast convergent iterative sequence when the coefficient matrix is well preconditioned. Numerical tests show that the new methods are more effective than some standard Krylov subspace methods. Updated RPCG methods meet the requirement of real-time computing successfully for multifactor models.},
  archive      = {J_SIMAX},
  author       = {Rui Li and Zeng-Qi Wang},
  doi          = {10.1137/19M1284853},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {838-851},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Restrictively preconditioned conjugate gradient method for a series of constantly augmented least squares problems},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Block krylov subspace methods for functions of matrices II:
Modified block FOM. <em>SIMAX</em>, <em>41</em>(2), 804–837. (<a
href="https://doi.org/10.1137/19M1255847">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We analyze an expansion of the generalized block Krylov subspace framework of [Electron. Trans. Numer. Anal., 47 (2017), pp. 100--126]. This expansion allows the use of low-rank modifications of the matrix projected onto the block Krylov subspace and contains, as special cases, the block GMRES method and the new block Radau--Arnoldi method. Within this general setting, we present results that extend the interpolation property from the nonblock case to a matrix polynomial interpolation property for the block case, and we relate the eigenvalues of the projected matrix to the latent roots of these matrix polynomials. Some error bounds for these modified block FOM methods for solving linear systems are presented. We then show how cospatial residuals can be preserved in the case of families of shifted linear block systems. This result is used to derive computationally practical restarted algorithms for block Krylov approximations that compute the action of a matrix function on a set of several vectors simultaneously. We prove some error bounds and present numerical results showing that two modifications of FOM, the block harmonic and the block Radau--Arnoldi methods for matrix functions, can significantly improve the convergence behavior.},
  archive      = {J_SIMAX},
  author       = {Andreas Frommer and Kathryn Lund and Daniel B. Szyld},
  doi          = {10.1137/19M1255847},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {804-837},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Block krylov subspace methods for functions of matrices II: Modified block FOM},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On uniqueness and computation of the decomposition of a
tensor into multilinear rank-<span
class="math inline">(1, <em>L</em><sub><em>r</em></sub>, <em>L</em><sub><em>r</em></sub>)</span>
terms. <em>SIMAX</em>, <em>41</em>(2), 747–803. (<a
href="https://doi.org/10.1137/18M1206849">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Canonical Polyadic Decomposition (CPD) represents a third-order tensor as the minimal sum of rank-1 terms. Because of its uniqueness properties the CPD has found many concrete applications in telecommunication, array processing, machine learning, etc. On the other hand, in several applications the rank-$1$ constraint on the terms is too restrictive. A multilinear rank-$(M,N,L)$ constraint (where a rank-$1$ term is the special case for which $M=N=L=1$) could be more realistic, while it still yields a decomposition with attractive uniqueness properties. In this paper we focus on the decomposition of a tensor $\mathcal T$ into a sum of multilinear rank-$(1,L_r,L_r)$ terms, $r=1,\dots,R$. This particular decomposition type has already found applications in wireless communication, chemometrics, and the blind signal separation of signals that can be modeled as exponential polynomials and rational functions. We find conditions on the terms which guarantee that the decomposition is unique and can be computed by means of the eigenvalue decomposition of a matrix even in the cases where none of the factor matrices has full column rank. We consider both the case where the decomposition is exact and the case where the decomposition holds only approximately. We show that in both cases the number of the terms $R$ and their “sizes&quot; $L_1,\dots,L_R$ do not have to be known a priori and can be estimated as well. The conditions for uniqueness are easy to verify, especially for terms that can be considered “generic.&quot; In particular, we obtain the following two generalizations of a well-known result on generic uniqueness of the CPD (i.e., the case $L_1=\dots=L_R=1$): we show that the multilinear rank-$(1,L_r,L_r)$ decomposition of an $I\times J\times K$ tensor is generically unique if (i) $L_1=\dots=L_R=:L$ and $R\leq \min((J-L)(K-L),I)$ or if (ii) $\sum L_R\leq \min((I-1)(J-1),K)$ and $J\geq \max(L_i+L_j)$.},
  archive      = {J_SIMAX},
  author       = {Ignat Domanov and Lieven De Lathauwer},
  doi          = {10.1137/18M1206849},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {747-803},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {On uniqueness and computation of the decomposition of a tensor into multilinear rank-$(1,L_r,L_r)$ terms},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). An algebraic sparsified nested dissection algorithm using
low-rank approximations. <em>SIMAX</em>, <em>41</em>(2), 715–746. (<a
href="https://doi.org/10.1137/19M123806X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a new algorithm for the fast solution of large, sparse, symmetric positive-definite linear systems, spaND (sparsified Nested Dissection). It is based on nested dissection, sparsification, and low-rank compression. After eliminating all interiors at a given level of the elimination tree, the algorithm sparsifies all separators corresponding to the interiors. This operation reduces the size of the separators by eliminating some degrees of freedom but without introducing any fill-in. This is done at the expense of a small and controllable approximation error. The result is an approximate factorization that can be used as an efficient preconditioner. We then perform several numerical experiments to evaluate this algorithm. We demonstrate that a version using orthogonal factorization and block-diagonal scaling takes fewer CG iterations to converge than previous similar algorithms on various kinds of problems. Furthermore, this algorithm is provably guaranteed to never break down and the matrix stays symmetric positive-definite throughout the process. We evaluate the algorithm on some large problems show it exhibits near-linear scaling. The factorization time is roughly $\mathcal{O}$(N), and the number of iterations grows slowly with N.},
  archive      = {J_SIMAX},
  author       = {Léopold Cambier and Chao Chen and Erik G. Boman and Sivasankaran Rajamanickam and Raymond S. Tuminaro and Eric Darve},
  doi          = {10.1137/19M123806X},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {715-746},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {An algebraic sparsified nested dissection algorithm using low-rank approximations},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Solving quadratic matrix equations arising in random walks
in the quarter plane. <em>SIMAX</em>, <em>41</em>(2), 691–714. (<a
href="https://doi.org/10.1137/19M1276960">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Quadratic matrix equations of the kind $A_1X^2+A_0X+A_{-1}=X$ are encountered in the analysis of Quasi--Birth-Death stochastic processes where the solution of interest is the minimal nonnegative solution $G$. In many queueing models, described by random walks in the quarter plane, the coefficients $A_1,A_0,A_{-1}$ are infinite tridiagonal matrices with an almost Toeplitz structure. Here, we analyze some fixed point iterations, including Newton&#39;s iteration, for the computation of $G$ and introduce effective algorithms and acceleration strategies which fully exploit the Toeplitz structure of the matrix coefficients and of the current approximation. Moreover, we provide a structured perturbation analysis for the solution $G$. The results of some numerical experiments which demonstrate the effectiveness of our approach are reported.},
  archive      = {J_SIMAX},
  author       = {Dario A. Bini and Beatrice Meini and Jie Meng},
  doi          = {10.1137/19M1276960},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {691-714},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Solving quadratic matrix equations arising in random walks in the quarter plane},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). First order structure-preserving perturbation theory for
eigenvalues of symplectic matrices. <em>SIMAX</em>, <em>41</em>(2),
657–690. (<a href="https://doi.org/10.1137/17M1124723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A first order perturbation theory for eigenvalues of real or complex $J$-symplectic matrices under structure-preserving perturbations is developed. The main tools in the analysis are structured canonical forms, together with Lidskii-like formulas for eigenvalues of multiplicative perturbations. Explicit formulas, depending only on appropriately normalized left and right eigenvectors, are obtained for the leading terms of asymptotic expansions describing the perturbed eigenvalues. Special attention is given to eigenvalues on the unit circle, especially to the exceptional eigenvalues $\pm 1$, whose behavior under structure-preserving perturbations is known to differ significantly from the behavior under general perturbations. Several numerical examples are used to illustrate the asymptotic expansions.},
  archive      = {J_SIMAX},
  author       = {Fredy Sosa and Julio Moro and Christian Mehl},
  doi          = {10.1137/17M1124723},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {657-690},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {First order structure-preserving perturbation theory for eigenvalues of symplectic matrices},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Simultaneous hollowization, joint numerical range, and
stabilization by noise. <em>SIMAX</em>, <em>41</em>(2), 637–656. (<a
href="https://doi.org/10.1137/19M1294265">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider orthogonal transformations of arbitrary square matrices to a form where all diagonal entries are equal. In our main results we treat the simultaneous transformation of two matrices and the symplectic orthogonal transformation of one matrix. A relation to the joint real numerical range is worked out, efficient numerical algorithms are developed, and applications to stabilization by rotation and by noise are presented.},
  archive      = {J_SIMAX},
  author       = {Tobias Damm and Heike Faßbender},
  doi          = {10.1137/19M1294265},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {637-656},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Simultaneous hollowization, joint numerical range, and stabilization by noise},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The computation of low multilinear rank approximations of
tensors via power scheme and random projection. <em>SIMAX</em>,
<em>41</em>(2), 605–636. (<a
href="https://doi.org/10.1137/19M1237016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper is devoted to the computation of low multilinear rank approximations of tensors. Combining the stretegy of power scheme, random projection, and singular value decomposition, we derive a three-stage randomized algorithm for the low multilinear rank approximation. Based on the singular values of sub-Gaussian matrices, we derive the error bound of the proposed algorithm with high probability. We illustrate the proposed algorithms via several numerical examples.},
  archive      = {J_SIMAX},
  author       = {Maolin Che and Yimin Wei and Hong Yan},
  doi          = {10.1137/19M1237016},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {605-636},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {The computation of low multilinear rank approximations of tensors via power scheme and random projection},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The rank of trifocal grassmann tensors. <em>SIMAX</em>,
<em>41</em>(2), 591–604. (<a
href="https://doi.org/10.1137/19M1277205">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Grassmann tensors arise from classical problems of scene reconstruction in computer vision. Trifocal Grassmann tensors, related to three projections from a projective space of dimension $k$ onto view spaces of varying dimensions, are studied in this work. A canonical form for the combined projection matrices is obtained. When the centers of projections satisfy a natural generality assumption, such canonical form gives a closed formula for the rank of trifocal Grassmann tensors. The same approach is also applied to the case of two projections, confirming a previous result obtained with different methods in [M. Bertolini, G. Besana, and C. Turrini, Ann. Mat. Pura Appl. (4), 196 (2016), pp. 539--553]. The rank of sequences of tensors converging to tensors associated with degenerate configurations of projection centers is also considered, giving concrete examples of a wide spectrum of phenomena that can happen.},
  archive      = {J_SIMAX},
  author       = {Marina Bertolini and Gianmario Besana and Gilberto Bini and Cristina Turrini},
  doi          = {10.1137/19M1277205},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {591-604},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {The rank of trifocal grassmann tensors},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Design and implementation of a parallel markowitz threshold
algorithm. <em>SIMAX</em>, <em>41</em>(2), 573–590. (<a
href="https://doi.org/10.1137/19M1245815">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a novel algorithm for the parallel factorization of an unsymmetric sparse matrix using a Markowitz threshold algorithm. Our algorithm uses a significant extension of a maximum independent set algorithm of Luby to select a block of independent pivots in parallel. We then use this block to update the matrix again in parallel. This algorithm does not require any symmetry in the matrix and is particularly efficacious when the matrix is structurally very unsymmetric using metrics defined in the paper. We implement this algorithm using OpenMP and show its performance against other state-of-the-art codes on a standard set of sparse matrix test problems.},
  archive      = {J_SIMAX},
  author       = {Timothy A. Davis and Iain S. Duff and Stojce Nakov},
  doi          = {10.1137/19M1245815},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {573-590},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Design and implementation of a parallel markowitz threshold algorithm},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Majorization bounds for ritz values of self-adjoint
matrices. <em>SIMAX</em>, <em>41</em>(2), 554–572. (<a
href="https://doi.org/10.1137/19M1263996">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A priori, a posteriori, and mixed type upper bounds for the absolute change in Ritz values of self-adjoint matrices in terms of submajorization relations are obtained. Some of our results prove recent conjectures by Knyazev, Argentati, and Zhu, which extend several known results for one dimensional subspaces to arbitrary subspaces. In addition, we improve Nakatsukasa&#39;s version of the $\tan \Theta$ theorem of Davis and Kahan. As a consequence, we obtain new quadratic a posteriori bounds for the absolute change in Ritz values.},
  archive      = {J_SIMAX},
  author       = {Pedro G. Massey and Demetrio Stojanoff and Sebastian Zarate},
  doi          = {10.1137/19M1263996},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {554-572},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Majorization bounds for ritz values of self-adjoint matrices},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bounds on the spectral radius of digraphs from subgraph
counts. <em>SIMAX</em>, <em>41</em>(2), 525–553. (<a
href="https://doi.org/10.1137/19M1256312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The spectral radius of a directed graph is a metric that can only be computed when the structure of the network is completely known. However, in many practical scenarios, it is not possible to exactly retrieve the whole structure of the network; hence, the exact value of the spectral radius is not computable. Even in these scenarios, it is typically possible to extract local structural properties of a network using, for example, graph crawlers. In this paper, we develop a novel measure-theoretic framework to upper and lower bound the spectral radius of a directed graph using local structural information, in particular, using the counts of a collection of small subgraphs or motifs. Our framework is based on recent results relating the multivariate moment problem with semidefinite programming. Using these results, we develop a hierarchy of (small) semidefinite programs whose solutions provide upper and lower bounds on the spectral radius of a directed graph using, solely, subgraph and motif counts. We numerically validate the quality of our bounds using both random and real-world directed graphs.},
  archive      = {J_SIMAX},
  author       = {Ximing Chen and Masaki Ogura and Victor M. Preciado},
  doi          = {10.1137/19M1256312},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {525-553},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Bounds on the spectral radius of digraphs from subgraph counts},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Stochastic reformulations of linear systems: Algorithms and
convergence theory. <em>SIMAX</em>, <em>41</em>(2), 487–524. (<a
href="https://doi.org/10.1137/18M1179249">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We develop a family of reformulations of an arbitrary consistent linear system into a stochastic problem. The reformulations are governed by two user-defined parameters: a positive definite matrix defining a norm, and an arbitrary discrete or continuous distribution over random matrices. Our reformulation has several equivalent interpretations, allowing for researchers from various communities to leverage their domain-specific insights. In particular, our reformulation can be equivalently seen as a stochastic optimization problem, stochastic linear system, stochastic fixed point problem, and a probabilistic intersection problem. We prove sufficient, and necessary and sufficient, conditions for the reformulation to be exact. Further, we propose and analyze three stochastic algorithms for solving the reformulated problem---basic, parallel, and accelerated methods---with global linear convergence rates. The rates can be interpreted as condition numbers of a matrix which depends on the system matrix and on the reformulation parameters. This gives rise to a new phenomenon which we call stochastic preconditioning and which refers to the problem of finding parameters (matrix and distribution) leading to a sufficiently small condition number. Our basic method can be equivalently interpreted as stochastic gradient descent, stochastic Newton method, stochastic proximal point method, stochastic fixed point method, and stochastic projection method, with fixed stepsize (relaxation parameter), applied to the reformulations.},
  archive      = {J_SIMAX},
  author       = {Peter Richtárik and Martin Takáč},
  doi          = {10.1137/18M1179249},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {487-524},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Stochastic reformulations of linear systems: Algorithms and convergence theory},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Admissible and attainable convergence behavior of block
arnoldi and GMRES. <em>SIMAX</em>, <em>41</em>(2), 464–486. (<a
href="https://doi.org/10.1137/19M1272469">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is well-established that any nonincreasing convergence curve is possible for GMRES and a family of pairs $(A,b)$ can be constructed for which GMRES exhibits a given convergence curve with $A$ having arbitrary spectrum. No analogue of this result has been established for block GMRES, wherein multiple right-hand sides are considered. By reframing the problem as a single linear system over a ring of square matrices, we develop convergence results for block Arnoldi and block GMRES. In particular, we show what convergence behavior is admissible for block GMRES and how the matrices and right-hand sides producing any admissible behavior can be constructed. Moreover, we show that the convergence of the block Arnoldi method for eigenvalue approximation can be almost fully independent of the convergence of block GMRES for the same coefficient matrix and the same starting vectors.},
  archive      = {J_SIMAX},
  author       = {Marie Kubínová and Kirk M. Soodhalter},
  doi          = {10.1137/19M1272469},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {464-486},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Admissible and attainable convergence behavior of block arnoldi and GMRES},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Riemannian modified polak–ribière–polyak conjugate gradient
order reduced model by tensor techniques. <em>SIMAX</em>,
<em>41</em>(2), 432–463. (<a
href="https://doi.org/10.1137/19M1257147">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents a new Riemannian modified Polak--Ribière--Polyak conjugate gradient algorithm to construct the reduced systems of quadratic-bilinear systems. We eliminate the orthogonality and homogeneity constraints of the truncated $\mathcal{H}_{2}$ optimal model order reduction problem and turn this constrained minimization problem into an unconstrained Riemannian optimization problem on the Grassmann manifold. Due to the compactness of this manifold, the existence of the minimum solution can be guaranteed. Applying tensor techniques, the Riemannian gradient of the cost function is derived. Additionally, we design a new Riemannian MPRP conjugate gradient scheme using the differentiated retraction and the scaled vector transport. The resulting search direction always provides a descent direction. The global convergence of the proposed algorithm is established. Moreover, our algorithm is also applicable to the minimization problems of linear and bilinear systems. Finally, two numerical tests are reported to illustrate the effectiveness of the proposed algorithm.},
  archive      = {J_SIMAX},
  author       = {Yao-Lin Jiang and Kang-Li Xu},
  doi          = {10.1137/19M1257147},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {432-463},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Riemannian modified polak--ribière--polyak conjugate gradient order reduced model by tensor techniques},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Structured inversion of the bernstein mass matrix.
<em>SIMAX</em>, <em>41</em>(2), 413–431. (<a
href="https://doi.org/10.1137/19M1284166">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bernstein polynomials, long a staple of approximation theory and computational geometry, have also increasingly become of interest in finite element methods. Many fundamental problems in interpolation and approximation give rise to interesting linear algebra questions. In [13], we gave block-structured algorithms for inverting the Bernstein mass matrix on simplicial cells but did not study fast algorithms for the univariate case. Here, we give several approaches to inverting the univariate mass matrix based on exact formulae for the inverse; decompositions of the inverse in terms of Hankel, Toeplitz, and diagonal matrices; and a spectral decomposition. In particular, the eigendecomposition can be explicitly constructed in $\mathcal{O}(n^2)$ operations, while its accuracy for solving linear systems is comparable to that of the Cholesky decomposition. Moreover, we study conditioning and accuracy of these methods from the standpoint of the effect of roundoff error in the $L^2$ norm on polynomials, showing that the conditioning in this case is far less extreme than in the standard 2-norm.},
  archive      = {J_SIMAX},
  author       = {Larry Allen and Robert C. Kirby},
  doi          = {10.1137/19M1284166},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {413-431},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Structured inversion of the bernstein mass matrix},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Smoothing splines and rank structured matrices: Revisiting
the spline kernel. <em>SIMAX</em>, <em>41</em>(2), 389–412. (<a
href="https://doi.org/10.1137/19M1267349">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We show that the spline kernel of order $p$ is a so-called semiseparable function with semiseparability rank $p$. A consequence of this is that kernel matrices generated by the spline kernel are rank structured matrices that can be stored and factorized efficiently. We use this insight to derive new recursive algorithms with linear complexity in the number of knots for various kernel matrix computations. We also discuss applications of these algorithms, including smoothing spline regression, Gaussian process regression, and some related hyperparameter estimation problems.},
  archive      = {J_SIMAX},
  author       = {Martin S. Andersen and Tianshi Chen},
  doi          = {10.1137/19M1267349},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {389-412},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Smoothing splines and rank structured matrices: Revisiting the spline kernel},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). The block rational arnoldi method. <em>SIMAX</em>,
<em>41</em>(2), 365–388. (<a
href="https://doi.org/10.1137/19M1245505">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The block version of the rational Arnoldi method is a widely used procedure for generating an orthonormal basis of a block rational Krylov space. We study block rational Arnoldi decompositions associated with this method and prove an implicit Q theorem. We show how to choose parameters to prevent a premature breakdown of the method and improve its numerical stability by relating the decompositions to nonlinear eigenvalue problems. We explain how rational matrix-valued functions are encoded in block rational Arnoldi decompositions and how they can be evaluated numerically. Two different types of deflation strategies are discussed. Numerical illustrations using the MATLAB Rational Krylov Toolbox are included.},
  archive      = {J_SIMAX},
  author       = {Steven Elsworth and Stefan Güttel},
  doi          = {10.1137/19M1245505},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {2},
  pages        = {365-388},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {The block rational arnoldi method},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Coordinate difference matrices. <em>SIMAX</em>,
<em>41</em>(1), 332–363. (<a
href="https://doi.org/10.1137/18M123428X">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many problems such as phase retrieval, molecular biology, source localization, and sensor array calibration, one can measure vector differences between pairs of points and attempt to recover the position of these points; this class of problems is called vector geometry problems (VGPs). A closely related field studies distance geometry problems (DGPs), where only the Euclidean distance between pairs of points is available. This has been extensively studied in the literature and is often associated with Euclidean distance matrices (EDMs). Although similar to DGPs, VGPs have received little attention in the literature; our goal is to fill in this gap and introduce a framework to solve VGPs. Inspired by EDM-related approaches, we arrange the differences in what we call a coordinate difference matrix (CDM) and introduce a methodology to reconstruct a set of points from CDM entries. We first propose a reconstruction scheme in 1D and then generalize it to higher dimensions. We show that our algorithm is optimal in the least-squares sense, even when we have only access to partial measurements. In addition, we provide necessary and sufficient conditions on the number and structure of measurements needed for a successful reconstruction, as well as a comparison with EDMs. In particular we show that compared to EDMs, CDMs are simpler objects, both from an algorithmic and a theoretical point of view. Therefore, CDMs should be favored over EDMs whenever vector differences are available. In the presence of noise, we provide a statistical analysis of the reconstruction error. Finally, we apply the established knowledge to five practical problems to demonstrate the versatility of this theory and showcase the wide range of applications covered by the CDM framework.},
  archive      = {J_SIMAX},
  author       = {Gilles Baechler and Frederike Dümbgen and Golnoosh Elhami and Miranda Kreković and Martin Vetterli},
  doi          = {10.1137/18M123428X},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {332-363},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Coordinate difference matrices},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Chebyshev polynomials and best rank-one approximation ratio.
<em>SIMAX</em>, <em>41</em>(1), 308–331. (<a
href="https://doi.org/10.1137/19M1269713">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We establish a new extremal property of the classical Chebyshev polynomials in the context of best rank-one approximation of tensors. We also give some necessary conditions for a tensor to be a minimizer of the ratio of spectral and Frobenius norms.},
  archive      = {J_SIMAX},
  author       = {Andrei Agrachev and Khazhgali Kozhasov and André Uschmajew},
  doi          = {10.1137/19M1269713},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {308-331},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Chebyshev polynomials and best rank-one approximation ratio},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Linear positive systems may have a reachable subset from the
origin that is either polyhedral or nonpolyhedral. <em>SIMAX</em>,
<em>41</em>(1), 279–307. (<a
href="https://doi.org/10.1137/19M1268161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Positive systems with positive inputs and positive outputs are used in several branches of engineering, biochemistry, and economics. Both control theory and system theory require the concept of reachability of a time-invariant discrete-time linear positive system. The subset of the state set that is reachable from the origin is therefore of interest. The reachable subset is in general a cone in the positive vector space of the positive real numbers. It is established in this paper that the reachable subset can be either a polyhedral or a nonpolyhedral cone. For a single-input case, a characterization is provided of when the infinite-time and the finite-time reachable subsets are polyhedral. An example is provided for which the reachable subset is nonpolyhedral. Finally, for the case of polyhedral reachable subset(s), a method is provided to verify if a target set can be reached from the origin using positive inputs.},
  archive      = {J_SIMAX},
  author       = {Yashar Zeinaly and Jan H. van Schuppen and Bart De Schutter},
  doi          = {10.1137/19M1268161},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {279-307},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Linear positive systems may have a reachable subset from the origin that is either polyhedral or nonpolyhedral},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive low-nonnegative-rank approximation for state
aggregation of markov chains. <em>SIMAX</em>, <em>41</em>(1), 244–278.
(<a href="https://doi.org/10.1137/18M1220790">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper develops a low-nonnegative-rank approximation method to identify the state aggregation structure of a finite-state Markov chain under an assumption that the state space can be mapped into a handful of metastates. The number of metastates is characterized by the nonnegative rank of the Markov transition matrix. Motivated by the success of the nuclear norm relaxation in low-rank minimization problems, we propose an atomic regularizer as a convex surrogate for the nonnegative rank and formulate a convex optimization problem. Because the atomic regularizer itself is not computationally tractable, we instead solve a sequence of problems involving a nonnegative factorization of the Markov transition matrices by using the proximal alternating linearized minimization method. Two methods for adjusting the rank of factorization are developed so that local minima are escaped. One is to append an additional column to the factorized matrices, which can be interpreted as an approximation of a negative subgradient step. The other is to reduce redundant dimensions by means of linear combinations. Overall, the proposed algorithm very likely converges to the global solution. The efficiency and statistical properties of our approach are illustrated on synthetic data. We also apply our state aggregation algorithm on a Manhattan transportation data set and make extensive comparisons with an existing method.},
  archive      = {J_SIMAX},
  author       = {Yaqi Duan and Mengdi Wang and Zaiwen Wen and Yaxiang Yuan},
  doi          = {10.1137/18M1220790},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {244-278},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Adaptive low-nonnegative-rank approximation for state aggregation of markov chains},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Interpolative decomposition via proxy points for kernel
matrices. <em>SIMAX</em>, <em>41</em>(1), 221–243. (<a
href="https://doi.org/10.1137/19M1258700">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the construction of rank-structured matrix representations of dense kernel matrices, a heuristic compression method, called the proxy point method, has been used in practice to efficiently compute the low-rank approximation of certain kernel matrix blocks in the form of an interpolative decomposition. We present a long overdue error analysis for the proxy point method, rigorously proving the effectiveness of the method under specific conditions. The analysis also generalizes the method, allowing it to be applied to the construction of different types of rank-structured matrices with general kernel functions in low-dimensional spaces. Based on the analysis, a systematic and adaptive scheme for selecting the proxy points used in the method is developed, which can guarantee that the method is effective for any given kernel function under specific conditions.},
  archive      = {J_SIMAX},
  author       = {Xin Xing and Edmond Chow},
  doi          = {10.1137/19M1258700},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {221-243},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Interpolative decomposition via proxy points for kernel matrices},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Conditioning of partial nonuniform fourier matrices with
clustered nodes. <em>SIMAX</em>, <em>41</em>(1), 199–220. (<a
href="https://doi.org/10.1137/18M1212197">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We prove sharp lower bounds for the smallest singular value of a partial Fourier matrix with arbitrary &quot;off the grid&quot; nodes (equivalently, a rectangular Vandermonde matrix with the nodes on the unit circle) in the case when some of the nodes are separated by less than the inverse bandwidth. The bound is polynomial in the reciprocal of the so-called superresolution factor, while the exponent is controlled by the maximal number of nodes which are clustered together. As a corollary, we obtain sharp minimax bounds for the problem of sparse superresolution on a grid under the partial clustering assumptions.},
  archive      = {J_SIMAX},
  author       = {Dmitry Batenkov and Laurent Demanet and Gil Goldman and Yosef Yomdin},
  doi          = {10.1137/18M1212197},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {199-220},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Conditioning of partial nonuniform fourier matrices with clustered nodes},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Quotient geometry with simple geodesics for the manifold of
fixed-rank positive-semidefinite matrices. <em>SIMAX</em>,
<em>41</em>(1), 171–198. (<a
href="https://doi.org/10.1137/18M1231389">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper explores the well-known identification of the manifold of rank $p$ positive-semidefinite matrices of size $n$ with the quotient of the set of full-rank $n$-by-$p$ matrices by the orthogonal group in dimension $p$. The induced metric corresponds to the Wasserstein metric between centered degenerate Gaussian distributions and is a generalization of the Bures--Wasserstein metric on the manifold of positive-definite matrices. We compute the Riemannian logarithm and show that the local injectivity radius at any matrix $C$ is the square root of the $p$th largest eigenvalue of $C$. As a result, the global injectivity radius on this manifold is zero. Finally, this paper also contains a detailed description of this geometry, recovering previously known expressions by applying the standard machinery of Riemannian submersions.},
  archive      = {J_SIMAX},
  author       = {Estelle Massart and P.-A. Absil},
  doi          = {10.1137/18M1231389},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {171-198},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Quotient geometry with simple geodesics for the manifold of fixed-rank positive-semidefinite matrices},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Approximate joint diagonalization with riemannian
optimization on the general linear group. <em>SIMAX</em>,
<em>41</em>(1), 152–170. (<a
href="https://doi.org/10.1137/18M1232838">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider the classical problem of approximate joint diagonalization of matrices, which can be cast as an optimization problem on the general linear group. We propose a versatile Riemannian optimization framework for solving this problem---unifiying existing methods and creating new ones. We use two standard Riemannian metrics (left- and right-invariant metrics) having opposite features regarding the structure of solutions and the model. We introduce the Riemannian optimization tools (gradient, retraction, vector transport) in this context, for the two standard nondegeneracy constraints (oblique and nonholonomic constraints). We also develop tools beyond the classical Riemannian optimization framework to handle the non-Riemannian quotient manifold induced by the nonholonomic constraint with the right-invariant metric. We illustrate our theoretical developments with numerical experiments on both simulated data and a real electroencephalographic recording.},
  archive      = {J_SIMAX},
  author       = {Florent Bouchard and Bijan Afsari and Jérôme Malick and Marco Congedo},
  doi          = {10.1137/18M1232838},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {152-170},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Approximate joint diagonalization with riemannian optimization on the general linear group},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Optimal robustness of port-hamiltonian systems.
<em>SIMAX</em>, <em>41</em>(1), 134–151. (<a
href="https://doi.org/10.1137/19M1259092">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We construct optimally robust port-Hamiltonian realizations of a given rational transfer function that represents a passive system. We show that the realization with a maximal passivity radius is a normalized port-Hamiltonian one. Its computation is linked to a particular solution of a linear matrix inequality that defines passivity of the transfer function, and we provide an algorithm to construct this optimal solution. We also consider the problem of finding the nearest passive system to a given nonpassive one and provide a simple but suboptimal solution.},
  archive      = {J_SIMAX},
  author       = {Volker Mehrmann and Paul M. Van Dooren},
  doi          = {10.1137/19M1259092},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {134-151},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Optimal robustness of port-hamiltonian systems},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On strassen’s rank additivity for small three-way tensors.
<em>SIMAX</em>, <em>41</em>(1), 106–133. (<a
href="https://doi.org/10.1137/19M1243099">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We address the problem of the additivity of the tensor rank. That is, for two independent tensors we study if the rank of their direct sum is equal to the sum of their individual ranks. A positive answer to this problem was previously known as Strassen&#39;s conjecture until recent counterexamples were proposed by Shitov. The latter are not very explicit, and they are only known to exist asymptotically for very large tensor spaces. In this article we prove that for some small three-way tensors the additivity holds. For instance, if the rank of one of the tensors is at most 6, then the additivity holds. Or, if one of the tensors lives in ${\mathbb C}^k\otimes {\mathbb C}^3\otimes {\mathbb C}^3$ for any $k$, then the additivity also holds. More generally, if one of the tensors is concise and its rank is at most 2 more than the dimension of one of the linear spaces, then additivity holds. In addition we also treat some cases of the additivity of the border rank of such tensors. In particular, we show that the additivity of the border rank holds if the direct sum tensor is contained in ${\mathbb C}^4\otimes {\mathbb C}^4\otimes {\mathbb C}^4$. Some of our results are valid over an arbitrary base field.},
  archive      = {J_SIMAX},
  author       = {Jarosław Buczyński and Elisa Postinghel and Filip Rupniewski},
  doi          = {10.1137/19M1243099},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {106-133},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {On strassen&#39;s rank additivity for small three-way tensors},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Projections, deflation, and multigrid for nonsymmetric
matrices. <em>SIMAX</em>, <em>41</em>(1), 83–105. (<a
href="https://doi.org/10.1137/18M1180268">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deflation is a well-known technique to accelerate Krylov subspace methods for solving linear systems of equations. In contrast to preconditioning, in deflation methods singular systems have to be solved. The original system is multiplied by a projection which leads to a singular linear system which can be more favorable for a Krylov subspace method. Deflation methods are also closely related to multigrid or multilevel and domain decomposition methods. Here, we continue the analysis of deflation methods with arbitrary deflation subspaces applied to nonsymmetric matrices. We give some characterizations of the spectrum of the deflated matrix, and we prove an equivalence theorem for two types of deflation methods. New results on projections, established here, allow simple proofs for the spectral relation between deflation and multigrid methods.},
  archive      = {J_SIMAX},
  author       = {Luis García Ramos and René Kehl and Reinhard Nabben},
  doi          = {10.1137/18M1180268},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {83-105},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Projections, deflation, and multigrid for nonsymmetric matrices},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Solving the three-dimensional high-frequency helmholtz
equation using contour integration and polynomial preconditioning.
<em>SIMAX</em>, <em>41</em>(1), 58–82. (<a
href="https://doi.org/10.1137/18M1228128">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose an iterative solution method for the three-dimensional high-frequency Helmholtz equation that exploits a contour integral formulation of spectral projectors. In this framework, the solution in certain invariant subspaces is approximated by solving complex-shifted linear systems, resulting in faster GMRES iterations due to the restricted spectrum. The shifted systems are solved by exploiting a polynomial fixed-point iteration, which is a robust scheme even if the magnitude of the shift is small. Numerical tests in three dimensions indicate that $O(n^{1/3})$ matrix-vector products are needed to solve a high-frequency problem with a matrix size $n$ with high accuracy. The method has a small storage requirement, can be applied to both dense and sparse linear systems, and is highly parallelizable.},
  archive      = {J_SIMAX},
  author       = {Xiao Liu and Yuanzhe Xi and Yousef Saad and Maarten V. de Hoop},
  doi          = {10.1137/18M1228128},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {58-82},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Solving the three-dimensional high-frequency helmholtz equation using contour integration and polynomial preconditioning},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A low-rank technique for computing the quasi-stationary
distribution of subcritical galton–watson processes. <em>SIMAX</em>,
<em>41</em>(1), 29–57. (<a
href="https://doi.org/10.1137/19M1241647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present a new algorithm for computing the quasi-stationary distribution of subcritical Galton--Watson branching processes. This algorithm is based on a particular discretization of a well-known functional equation that characterizes the quasi-stationary distribution of these processes. We provide a theoretical analysis of the approximate low-rank structure that stems from this discretization, and we extend the procedure to multitype branching processes. We use numerical examples to demonstrate that our algorithm is both more accurate and more efficient than other approaches.},
  archive      = {J_SIMAX},
  author       = {Sophie Hautphenne and Stefano Massei},
  doi          = {10.1137/19M1241647},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {29-57},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {A low-rank technique for computing the quasi-stationary distribution of subcritical galton--watson processes},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computing closest stable nonnegative matrix. <em>SIMAX</em>,
<em>41</em>(1), 1–28. (<a
href="https://doi.org/10.1137/17M1144568">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The problem of finding the closest stable matrix for a dynamical system has many applications. It is studied for both continuous and discrete-time systems and the corresponding optimization problems are formulated for various matrix norms. As a rule, nonconvexity of these formulations does not allow finding their global solutions. In this paper, we analyze positive discrete-time systems. They also suffer from nonconvexity of the stability region, and the problem in the Frobenius norm or in the Euclidean norm remains hard for them. However, it turns out that for certain polyhedral norms, the situation is much better. We show that for the distances measured in the max-norm, we can find an exact solution of the corresponding nonconvex projection problems in polynomial time. For the distance measured in the operator $\ell_{\infty}$-norm or $\ell_{1}$-norm, the exact solution is also efficiently found. To this end, we develop a modification of the recently introduced spectral simplex method. On the other hand, for all these three norms, we obtain exact descriptions of the region of stability around a given stable matrix. In the case of the max-norm, this can be seen as an extension onto the class of nonnegative matrices, the Kharitonov theorem, providing a stability criterion for polynomials with interval coefficients [V. L Kharitonov, Differ. Uravn., 14 (1978), pp. 2086--2088; K. Panneerselvam and R. Ayyagari, Internat. J. Control Sci. Engrg., 3 (2013), pp. 81--85]. For practical implementation of our technique, we developed a new method for approximating the maximal eigenvalue of a nonnegative matrix. It combines the local quadratic rate of convergence with polynomial-time global performance guarantees.},
  archive      = {J_SIMAX},
  author       = {Yu. Nesterov and V. Yu. Protasov},
  doi          = {10.1137/17M1144568},
  journal      = {SIAM Journal on Matrix Analysis and Applications},
  number       = {1},
  pages        = {1-28},
  shortjournal = {SIAM J. Matrix Anal. Appl.},
  title        = {Computing closest stable nonnegative matrix},
  volume       = {41},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
