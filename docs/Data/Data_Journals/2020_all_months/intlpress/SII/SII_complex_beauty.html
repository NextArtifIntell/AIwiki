<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>SII_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="sii---42">SII - 42</h2>
<ul>
<li><details>
<summary>
(2020). A CD-based mapping method for combining multiple related
parameters from heterogeneous intervention trials. <em>SII</em>,
<em>13</em>(4), 533–549. (<a
href="https://doi.org/10.4310/SII.2020.v13.n4.a10">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Effect size can differ as a function of the elapsed time since treatment or as a function of other key covariates, such as sex or age. In evidence synthesis, a better understanding of the precise conditions under which treatment does work or does not work well has been highly valued. With increasingly accessible individual patient or participant data (IPD), more precise and informative inference can be within our reach. However, simultaneously combining multiple related parameters across heterogeneous studies is challenging because each parameter from each study has a specific interpretation within the context of the study and other covariates in the model. This paper proposes a novel mapping method to combine study-specific estimates of multiple related parameters across heterogeneous studies, which ensures valid inference at all inference levels by combining sample-dependent functions known as Confidence Distributions (CD). We describe the “CD-based mapping method” and provide a data application example for a multivariate random-effects meta-analysis model. We estimated up to 13 study-specific regression parameters for each of 14 individual studies using IPD in the first step, and subsequently combined the study-specific vectors of parameters, yielding a full vector of hyperparameters in the second step of metaanalysis. Sensitivity analysis indicated that the CD-based mapping method is robust to model misspecification. This novel approach to multi-parameter synthesis provides a reasonable methodological solution when combining complex evidence using IPD.},
  archive      = {J_SII},
  author       = {Jiao, Yang and Mun, Eun-Young and Trikalinos, Thomas A. and Xie, Minge},
  doi          = {10.4310/SII.2020.v13.n4.a10},
  journal      = {Statistics and Its Interface},
  number       = {4},
  pages        = {533-549},
  shortjournal = {Stat. Interface},
  title        = {A CD-based mapping method for combining multiple related parameters from heterogeneous intervention trials},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimating the mean and variance from the five-number
summary of a log-normal distribution. <em>SII</em>, <em>13</em>(4),
519–531. (<a href="https://doi.org/10.4310/SII.2020.v13.n4.a9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the past several decades, meta-analysis has been widely used to pool multiple studies for evidence-based practice. To conduct a meta-analysis, the mean and variance from each study are often required; whereas in certain studies, the five-number summary may instead be reported that consists of the median, the first and third quartiles, and/or the minimum and maximum values. To transform the fivenumber summary back to the mean and variance, several popular methods have emerged in the literature. However, we note that most existing methods are developed under the normality assumption; and when this assumption is violated, these methods may not be able to provide a reliable transformation. In this paper, we propose to estimate the mean and variance from the five-number summary of a $\log$-normal distribution. Specifically, we first make the log-transformation of the reported quantiles. With the existing mean estimators and newly proposed variance estimators under the normality assumption, we construct the estimators of the log-scale mean and variance. Finally, we transform them back to the original scale for the final estimators.We also propose a biascorrected method to further improve the estimation of the mean and variance. Simulation studies demonstrate that our new estimators have smaller biases and smaller relative risks in most settings. A real data example is used to illustrate the practical usefulness of our new estimators.},
  archive      = {J_SII},
  author       = {Shi, Jiandong and Tong, Tiejun and Wang, Yuedong and Genton, Marc G.},
  doi          = {10.4310/SII.2020.v13.n4.a9},
  journal      = {Statistics and Its Interface},
  number       = {4},
  pages        = {519-531},
  shortjournal = {Stat. Interface},
  title        = {Estimating the mean and variance from the five-number summary of a log-normal distribution},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sample size estimation for future studies using bayesian
multivariate network meta-analysis. <em>SII</em>, <em>13</em>(4),
511–517. (<a href="https://doi.org/10.4310/SII.2020.v13.n4.a8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although systematic reviews of randomized clinical trials (RCTs) are considered the pinnacle of evidence-based medicine, RCTs are often designed to reach a desired level of power for a pre-specified effect size, independent of the current body of evidence. Evidence indicates that sample size calculations for a new RCT should be conducted in the context of a systematic review and meta-analysis of the existing body of evidence. This paper presents a framework to estimate sample size and power for a future study, based on a prospective multivariate network metaanalysis (MNMA) of RCTs. The term “multivariate” refers to powering on (potentially) multiple outcomes. Specifically, a Bayesian MNMA is fit to the existing network and 1000 hypothetical trials are designed from the resultant posterior predictive distribution of effect sizes. Thus, the future RCT is designed in the context of the current network of evidence. The approach is applied to a systematic review of pharmacologic treatments for adult acute manic disorder. The analysis suggests that new trials should be designed/ powered within the context of either a multivariate or univariate network meta-analysis, where the former is preferred if researchers are interested in multiple primary outcomes, or the network is subject to extensive missing outcomes.},
  archive      = {J_SII},
  author       = {Desantis, Stacia M. and Hwang, Hyunsoo},
  doi          = {10.4310/SII.2020.v13.n4.a8},
  journal      = {Statistics and Its Interface},
  number       = {4},
  pages        = {511-517},
  shortjournal = {Stat. Interface},
  title        = {Sample size estimation for future studies using bayesian multivariate network meta-analysis},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Model selection between the fixed-effects model and the
random-effects model in meta-analysis. <em>SII</em>, <em>13</em>(4),
501–510. (<a href="https://doi.org/10.4310/SII.2020.v13.n4.a7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The common-effect model and the random-effects model are the two most popular models for meta-analysis in the literature. To choose a proper model between them, the $Q$ statistic and the $I^2$ statistic are commonly used as the criteria. Recently, it is recognized that the fixed-effects model is also essential for meta-analysis, especially when the number of studies is small. With this new model, the existing methods are no longer sufficient for model selection in metaanalysis. In view of the demand, we propose a novel method for model selection between the fixed-effects model and the random-effects model. Specifically, we apply the Akaike information criterion (AIC) to both models and then select the model with a smaller AIC value. A real data example is also presented to illustrate how the new method can be applied. We further propose the generalized AIC (GAIC) to reduce the large variation in the AIC value, and demonstrate its superiority through real data analysis and simulation studies. To the best of our knowledge, this is the first work in meta-analysis for model selection between the fixed-effects model and the random-effects model, and we expect that our new criterion has the potential to be widely applied in meta-analysis and evidence-based medicine.},
  archive      = {J_SII},
  author       = {Yang, Ke and Kwan, Hiu-Yee and Yu, Zhiling and Tong, Tiejun},
  doi          = {10.4310/SII.2020.v13.n4.a7},
  journal      = {Statistics and Its Interface},
  number       = {4},
  pages        = {501-510},
  shortjournal = {Stat. Interface},
  title        = {Model selection between the fixed-effects model and the random-effects model in meta-analysis},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian flexible hierarchical skew heavy-tailed
multivariate meta regression models for individual patient data with
applications. <em>SII</em>, <em>13</em>(4), 485–500. (<a
href="https://doi.org/10.4310/SII.2020.v13.n4.a6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A flexible class of multivariate meta-regression models are proposed for Individual Patient Data (IPD). The methodology is well motivated from 26 pivotal Merck clinical trials that compare statins (cholesterol lowering drugs) in combination with ezetimibe and statins alone on treatment-naïve patients and those continuing on statins at baseline. The research goal is to jointly analyze the multivariate outcomes, Low Density Lipoprotein Cholesterol (LDL-C), High Density Lipoprotein Cholesterol (HDL-C), and Triglycerides (TG). These three continuous outcome measures are correlated and shed much light on a subject’s lipid status. The proposed multivariate meta-regression models allow for different skewness parameters and different degrees of freedom for the multivariate outcomes from different trials under a general class of skew $t$-distributions. The theoretical properties of the proposed models are examined and an efficient Markov chain Monte Carlo (MCMC) sampling algorithm is developed for carrying out Bayesian inference under the proposed multivariate meta-regression model. In addition, the Conditional Predictive Ordinates (CPOs) are computed via an efficient Monte Carlo method. Consequently, the logarithm of the pseudo marginal likelihood and Bayesian residuals are obtained for model comparison and assessment, respectively. A detailed analysis of the IPD meta data from the 26 Merck clinical trials is carried out to demonstrate the usefulness of the proposed methodology.},
  archive      = {J_SII},
  author       = {Kim, Sungduk and Chen, Ming-Hui and Ibrahim, Joseph and Shah, Arvind and Lin, Jianxin},
  doi          = {10.4310/SII.2020.v13.n4.a6},
  journal      = {Statistics and Its Interface},
  number       = {4},
  pages        = {485-500},
  shortjournal = {Stat. Interface},
  title        = {Bayesian flexible hierarchical skew heavy-tailed multivariate meta regression models for individual patient data with applications},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Small-study effects: Current practice and challenges for
future research. <em>SII</em>, <em>13</em>(4), 475–484. (<a
href="https://doi.org/10.4310/SII.2020.v13.n4.a5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-analyses and systematic reviews are highly valued as evidence for clinical decision- and policy-making. However, inference in these settings may be invalid if the studies do not come from the same underlying distribution. Small study effects is one form of heterogeneity that can lead to biased estimates, particularly if it arises due to the selective publishing of studies, a phenomenon known as publication bias. In this paper we discuss landmark methods for diagnosing the presence of small-study effects and correcting for them, as well as the limitations of each method. We also identify ongoing challenges and key areas in need of methodological innovation.},
  archive      = {J_SII},
  author       = {Marks-Anglin, Arielle and Chen, Yong},
  doi          = {10.4310/SII.2020.v13.n4.a5},
  journal      = {Statistics and Its Interface},
  number       = {4},
  pages        = {475-484},
  shortjournal = {Stat. Interface},
  title        = {Small-study effects: Current practice and challenges for future research},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Meta-analysis of peptides to detect protein significance.
<em>SII</em>, <em>13</em>(4), 465–474. (<a
href="https://doi.org/10.4310/SII.2020.v13.n4.a4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shotgun assays are widely used in biotechnologies to characterize large molecules, which are hard to be measured as a whole directly. For instance, in Liquid Chromatography–Mass Spectrometry (LC–MS) shotgun experiments, proteins in biological samples are digested into peptides, and then peptides are separated and measured. However, in proteomics study, investigators are usually interested in the performance of the whole proteins instead of those peptide fragments. In light of meta-analysis, we propose an adaptive thresholding method to select informative peptides, and combine peptide-level models to protein-level analysis. The meta-analysis procedure and modeling rationale can be adapted to data analysis of other types of shotgun assays.},
  archive      = {J_SII},
  author       = {Zhang, Yuping and Ouyang, Zhengqing and Qian, Wei-Jun and Smith, Richard D. and Wong, Wing Hung and Davis, Ronald W.},
  doi          = {10.4310/SII.2020.v13.n4.a4},
  journal      = {Statistics and Its Interface},
  number       = {4},
  pages        = {465-474},
  shortjournal = {Stat. Interface},
  title        = {Meta-analysis of peptides to detect protein significance},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Statistical methods for quantifying between-study
heterogeneity in meta-analysis with focus on rare binary events.
<em>SII</em>, <em>13</em>(4), 449–464. (<a
href="https://doi.org/10.4310/SII.2020.v13.n4.a3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Meta-analysis, the statistical procedure for combining results from multiple independent studies, has been widely used in medical research to evaluate intervention efficacy and drug safety. In many practical situations, treatment effects vary notably among the collected studies, and the variation, often modeled by the between-study variance parameter $\tau^2$, can greatly affect the inference of the overall effect size. In the past, comparative studies have been conducted for both point and interval estimation of $\tau^2$. However, most are incomplete, only including a limited subset of existing methods, and some are outdated. Further, none of the studies covers descriptive measures for assessing the level of heterogeneity. Nor are they focused on rare binary events that require special attention. We summarize by far the most comprehensive set including 11 descriptive measures, 23 estimators, and 16 confidence intervals. In addition to providing synthesized information, we further categorize these methods according to their key features. We then evaluate their performance based on simulation studies that examine various realistic scenarios for rare binary events, with an illustration using a data example of a gestational diabetes meta-analysis. We conclude that there is no uniformly “best” method. However, methods with consistently better performance do exist in the context of rare binary events, and we provide practical guidelines based on numerical evidences.},
  archive      = {J_SII},
  author       = {Zhang, Chiyu and Chen, Min and Wang, Xinlei},
  doi          = {10.4310/SII.2020.v13.n4.a3},
  journal      = {Statistics and Its Interface},
  number       = {4},
  pages        = {449-464},
  shortjournal = {Stat. Interface},
  title        = {Statistical methods for quantifying between-study heterogeneity in meta-analysis with focus on rare binary events},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian meta-regression model using heavy-tailed
random-effects with missing sample sizes for self-thinning meta-data.
<em>SII</em>, <em>13</em>(4), 437–447. (<a
href="https://doi.org/10.4310/SII.2020.v13.n4.a2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motivated by the self-thinning meta-data, a randomeffects meta-analysis model with unknown precision parameters is proposed with a truncated Poisson regression model for missing sample sizes. The random effects are assumed to follow a heavy-tailed distribution to accommodate outlying aggregate values in the response variable. The logarithm of the pseudo-marginal likelihood (LPML) is used for model comparison. In addition, in order to determine which self-thinning law is more supported by the meta-data, a measure called “Plausibility Index (PI)” is developed. A simulation study is conducted to examine empirical performance of the proposed methodology. Finally, the proposed model and the PI measure are applied to analyze a self-thinning meta-data set in details.},
  archive      = {J_SII},
  author       = {Ma, Zhihua and Chen, Ming-Hui and Tang, Yi},
  doi          = {10.4310/SII.2020.v13.n4.a2},
  journal      = {Statistics and Its Interface},
  number       = {4},
  pages        = {437-447},
  shortjournal = {Stat. Interface},
  title        = {Bayesian meta-regression model using heavy-tailed random-effects with missing sample sizes for self-thinning meta-data},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). On evidence cycles in network meta-analysis. <em>SII</em>,
<em>13</em>(4), 425–436. (<a
href="https://doi.org/10.4310/SII.2020.v13.n4.a1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As an extension of pairwise meta-analysis of two treatments, network meta-analysis has recently attracted many researchers in evidence-based medicine because it simultaneously synthesizes both direct and indirect evidence from multiple treatments and thus facilitates better decision making. The Bayesian hierarchical model is a popular method to implement network meta-analysis, and it is generally considered more powerful than conventional pairwise metaanalysis, leading to more precise effect estimates with narrower credible intervals. However, the improvement of effect estimates produced by Bayesian network meta-analysis has never been studied theoretically. This article shows that such improvement depends highly on evidence cycles in the treatment network. When all treatment comparisons are assumed to have different heterogeneity variances, a network meta-analysis produces posterior distributions identical to separate pairwise meta-analyses for treatment comparisons that are not contained in any evidence cycles. However, this equivalence does not hold under the commonly-used assumption of a common heterogeneity variance for all comparisons. Simulations and a case study are used to illustrate the equivalence of the Bayesian network and pairwise metaanalyses in certain networks.},
  archive      = {J_SII},
  author       = {Lin, Lifeng and Chu, Haitao and Hodges, James S.},
  doi          = {10.4310/SII.2020.v13.n4.a1},
  journal      = {Statistics and Its Interface},
  number       = {4},
  pages        = {425-436},
  shortjournal = {Stat. Interface},
  title        = {On evidence cycles in network meta-analysis},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A semi-parametric joint latent class model with longitudinal
and survival data. <em>SII</em>, <em>13</em>(3), 411–422. (<a
href="https://doi.org/10.4310/SII.2020.v13.n3.a10">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In many longitudinal studies, we are interested in both repeated measures of a biomarker and time to an event. When there exist heterogeneous patterns of the longitudinal and survival profiles, we propose a latent class joint model to identify subgroups of subjects and study the association between longitudinal and survival outcomes. The model is estimated by maximizing the full likelihood function. We use B-splines to approximate the baseline hazard function which involves a diverging number of parameters. Asymptotic properties of the estimator for the joint latent class model are investigated. We conduct simulation studies to assess the performance of the developed method. A real data example, Mayo Clinic Primary Biliary Cirrhosis Data, is analyzed using the joint modeling approach.},
  archive      = {J_SII},
  author       = {Liu, Yue and Lin, Ye and Zhou, Jianhui and Liu, Lei},
  doi          = {10.4310/SII.2020.v13.n3.a10},
  journal      = {Statistics and Its Interface},
  number       = {3},
  pages        = {411-422},
  shortjournal = {Stat. Interface},
  title        = {A semi-parametric joint latent class model with longitudinal and survival data},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Estimation of a distribution function using lagrange
polynomials with tchebychev-gauss points. <em>SII</em>, <em>13</em>(3),
399–410. (<a href="https://doi.org/10.4310/SII.2020.v13.n3.a9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The estimation of the distribution function of a real random variable is an intrinsic topic in non parametric estimation. To this end, a distribution estimator based on Lagrange polynomials and Tchebychev–Gauss points, is introduced. Some asymptotic properties of the proposed estimator are investigated, such as its asymptotic bias, variance, mean squared error and Chung–Smirnov propriety. The asymptotic normality and the uniform convergence of the estimator are also established. Lastly, the performance of the proposed estimator is explored through a certain simulation examples.},
  archive      = {J_SII},
  author       = {Helali, Salima and Slaoui, Yousri},
  doi          = {10.4310/SII.2020.v13.n3.a9},
  journal      = {Statistics and Its Interface},
  number       = {3},
  pages        = {399-410},
  shortjournal = {Stat. Interface},
  title        = {Estimation of a distribution function using lagrange polynomials with tchebychev-gauss points},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Asymptotic theory for differentially private generalized
<span class="math inline"><em>β</em></span>-models with parameters
increasing. <em>SII</em>, <em>13</em>(3), 385–398. (<a
href="https://doi.org/10.4310/SII.2020.v13.n3.a8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modelling edge weights play a crucial role in the analysis of network data, which reveals the extent of relationships among individuals. Due to the diversity of weight information, sharing these data has become a complicated challenge in a privacy-preserving way. In this paper, we consider the case of the non-denoising process to achieve the trade-off between privacy and weight information in the generalized β-model. Under the edge differential privacy with a discrete Laplace mechanism, the Z-estimators from estimating equations for the model parameters are shown to be consistent and asymptotically normally distributed. The simulations and a real data example are given to further support the theoretical results.},
  archive      = {J_SII},
  author       = {Fan, Yifan and Zhang, Huiming and Yan, Ting},
  doi          = {10.4310/SII.2020.v13.n3.a8},
  journal      = {Statistics and Its Interface},
  number       = {3},
  pages        = {385-398},
  shortjournal = {Stat. Interface},
  title        = {Asymptotic theory for differentially private generalized $\beta$-models with parameters increasing},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Semiparametric accelerated failure time modeling for
multivariate failure times under multivariate outcome-dependent sampling
designs. <em>SII</em>, <em>13</em>(3), 373–383. (<a
href="https://doi.org/10.4310/SII.2020.v13.n3.a7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Researchers working on large cohort studies are always seeking for cost-effective designs due to a limited budget. An outcome-dependent sampling (ODS) design, a retrospective sampling scheme where one observes covariates with a probability depending on the outcome and selects supplemental samples from more informative segments, improves the study efficiency while effectively controlling for the budget. To take the advantage of the ODS scheme when multivariate failure times are main response variables, relevant study designs and inference procedures need to be studied. In this paper, we consider a general multivariate-ODS design for multivariate failure times under the framework of a semiparametric accelerated failure time model. We develop a weighted estimating equations approach, based on the induced smoothing method, for parameter estimation. Extensive simulation studies show that our proposed design and estimator are more efficient than other competing estimators based on simple random samples. The proposed method is illustrated with a real data set from the Busselton Health Study.},
  archive      = {J_SII},
  author       = {Lu, Tsui-Shan and Kang, Sangwook and Zhou, Haibo},
  doi          = {10.4310/SII.2020.v13.n3.a7},
  journal      = {Statistics and Its Interface},
  number       = {3},
  pages        = {373-383},
  shortjournal = {Stat. Interface},
  title        = {Semiparametric accelerated failure time modeling for multivariate failure times under multivariate outcome-dependent sampling designs},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A sequential naive bayes method for music genre
classification based on transitional information from pitch and beat.
<em>SII</em>, <em>13</em>(3), 361–371. (<a
href="https://doi.org/10.4310/SII.2020.v13.n3.a6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the rapid development of digital music market, online music websites are widely available in our daily life. There is a practical need to develop automatic music genre classification algorithms to manage a huge amount of music. In this regard, the transitional information contained in pitches and beats should be very useful. Particularly, the transition in pitches produces a melody, and the transition in beats produces a rhythm. They both decide the music genre. To take these valuable information into consideration, we propose here a sequential naïve Bayes method for music genre classification. This method can be viewed as an novel extension of the classical naïve Bayes classifier, but takes the transitional information between pitches and beats into consideration. To reduce the number of estimated parameters, we propose a BIC-type criterion and develop a computationally efficient algorithm for model selection. The selection consistency of the BIC method is theoretically proved and numerically investigated. The finite sample performance of the proposed methods are assessed through both simulations and a real music dataset.},
  archive      = {J_SII},
  author       = {Ren, Tunan and Wang, Feifei and Wang, Hansheng},
  doi          = {10.4310/SII.2020.v13.n3.a6},
  journal      = {Statistics and Its Interface},
  number       = {3},
  pages        = {361-371},
  shortjournal = {Stat. Interface},
  title        = {A sequential naive bayes method for music genre classification based on transitional information from pitch and beat},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Multi-dimensional classification with semiparametric mixture
model. <em>SII</em>, <em>13</em>(3), 347–359. (<a
href="https://doi.org/10.4310/SII.2020.v13.n3.a5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compared to non-model based classification methods, the model based classification has the advantage of classification together with regression analysis, and is the interest of our investigation. For robustness, we propose and study a semiparametric mixture model, in which each sub-density is only assumed unimodal. The semiparametric maximum likelihood estimate is used to estimate the parametric and nonparametric components. Then the Bayesian classification rule is used to classify the subjects according to the model. Large sample properties of the estimates are investigated, simulation studies are conducted to evaluate the finite sample performance of the proposed model, and then the method is applied to analyze a real data.},
  archive      = {J_SII},
  author       = {Yin, Anqi and Yuan, Ao},
  doi          = {10.4310/SII.2020.v13.n3.a5},
  journal      = {Statistics and Its Interface},
  number       = {3},
  pages        = {347-359},
  shortjournal = {Stat. Interface},
  title        = {Multi-dimensional classification with semiparametric mixture model},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A log birnbaum-saunders regression model based on the
skew-normal distribution under the centred parameterization.
<em>SII</em>, <em>13</em>(3), 335–346. (<a
href="https://doi.org/10.4310/SII.2020.v13.n3.a4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper we introduce a new regression model for positive and skewed data, a log Birnbaum–Saunders model based on the centred skew-normal distribution, also presenting several inference tools for this model. Initially, we developed a new version of the skew-sinh-normal distribution, describing some of its properties. For the proposed regression model, we carry out, through the expectation conditional maximization (ECM) algorithm, parameter estimation, model fit assessment, model comparison and residual analysis. Finally, our model accommodates more suitably the asymmetry of the data, compared with the usual log Birnbaum–Saunders model, which is illustrated through a real data analysis.},
  archive      = {J_SII},
  author       = {Chaves, Nathalia L. and Azevedo, Caio L. N. and Vilca-Labra, Filidor and Nobre, Juvêncio S.},
  doi          = {10.4310/SII.2020.v13.n3.a4},
  journal      = {Statistics and Its Interface},
  number       = {3},
  pages        = {335-346},
  shortjournal = {Stat. Interface},
  title        = {A log birnbaum-saunders regression model based on the skew-normal distribution under the centred parameterization},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Computerized adaptive test using raw responses for item
selection: Theoretical results and applications for the up-and-down
method. <em>SII</em>, <em>13</em>(3), 317–333. (<a
href="https://doi.org/10.4310/SII.2020.v13.n3.a3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Modern computerized adaptive testing (CAT) is finding applications that contain more intensive assessments, collected over nontraditional devices such as tablets and smartphones. In this paper, we introduce an CAT algorithm that uses raw responses to adaptively select items and does not require updating the ability estimate at every administration of an item. The proposed algorithm is especially useful in adaptive assessment situations in which updating ability estimate at each administration is either not feasible or too costly to implement. Specifically, an $a$-stratified multistage up-and-down method is proposed as an approximation to the commonly used recursive maximum likelihood estimate (R-MLE). Using Markov chain tools, we derive theoretical results for the statistical properties of the up-and-down method. We also report empirical studies for the performance of the proposed method. Both simulation experiments and real data analysis are included. Limitations of the method such as reduced statistical efficiency are also discussed. Overall, despite the limitations, our results show that the up-and-down method is a promising alternative to the classical R-MLE and well-suited for some CAT applications such as ecological momentary assessments.},
  archive      = {J_SII},
  author       = {Fuh, Cheng-Der and Ip, Edward Haksing and Chen, Shyh-Huei},
  doi          = {10.4310/SII.2020.v13.n3.a3},
  journal      = {Statistics and Its Interface},
  number       = {3},
  pages        = {317-333},
  shortjournal = {Stat. Interface},
  title        = {Computerized adaptive test using raw responses for item selection: Theoretical results and applications for the up-and-down method},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Evaluation of bias for outcome adaptive randomization
designs with binary endpoints. <em>SII</em>, <em>13</em>(3), 287–315.
(<a href="https://doi.org/10.4310/SII.2020.v13.n3.a2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Clinical trial designs applying outcome adaptive randomization (OAR) sequentially change randomization probabilities basing on observed outcomes. Compared to the conventional equal randomization procedure, OAR has the feature to assign more patients to the better treatment arm and yield higher overall response rates for patients in the trial. However, the true response rates tend to be underestimated in OAR trials. Although the bias converges to zero asymptotically as the sample size increases, it is nonnegligible in small trials. In this paper, we evaluated the bias of OAR designs with binary endpoints, with the allocation probabilities implemented under two respective randomization procedures, namely, the sequential maximum likelihood procedure (SMLE) and the doubly adaptive biased coin design (DBCD). We found that the patterns of bias are similar between the two adaptive randomization procedures. When the true response rate is less than 10%, we discover that the bias can be as large as 20% of the true response rates if the sample size is less than 30; the absolute value of the bias, however, remains small. To better gauge the magnitude of the bias, we derived some large-sample strategies to approximate the bias for two target allocation proportions and two randomization procedures. In addition, we conducted simulation studies to quantify the magnitude of the bias in finite samples to assess the accuracy of the asymptotic approximations. We also provided an intuitive explanation for the cause of the underestimation under OAR, and discussed remedies to alleviate bias in the OAR design. A deeper understanding of this bias can help us design better OAR trials and provide more accurate estimates.},
  archive      = {J_SII},
  author       = {Wang, Yaping and Zhu, Hongjian and Lee, J. Jack},
  doi          = {10.4310/SII.2020.v13.n3.a2},
  journal      = {Statistics and Its Interface},
  number       = {3},
  pages        = {287-315},
  shortjournal = {Stat. Interface},
  title        = {Evaluation of bias for outcome adaptive randomization designs with binary endpoints},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Fully bayesian <span
class="math inline"><em>L</em><sub>1/2</sub></span>-penalized linear
quantile regression analysis with autoregressive errors. <em>SII</em>,
<em>13</em>(3), 271–286. (<a
href="https://doi.org/10.4310/SII.2020.v13.n3.a1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the quantile regression framework, we incorporate Bayesian $L_{1/2}$ and adaptive $L_{1/2}$ penalties into quantile linear regression models with autoregressive (AR) errors to conduct statistical inference. A Bayesian joint hierarchical model is established using the working likelihood of the asymmetric Laplace distribution (ALD). On the basis of the mixture representations of ALD and the generalized Gaussian distribution priors of regression coefficients and AR parameters, a Markov chain Monte Carlo algorithm is developed to conduct posterior inference. Finally, the proposed Bayesian estimation procedures are demonstrated by simulation studies and applied to a real data application concerning the electricity consumption of residential customers.},
  archive      = {J_SII},
  author       = {Tian, Yuzhu and Song, Xinyuan},
  doi          = {10.4310/SII.2020.v13.n3.a1},
  journal      = {Statistics and Its Interface},
  number       = {3},
  pages        = {271-286},
  shortjournal = {Stat. Interface},
  title        = {Fully bayesian $L_{1/2}$-penalized linear quantile regression analysis with autoregressive errors},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A spatial autoregression model with time-varying
coefficients. <em>SII</em>, <em>13</em>(2), 261–270. (<a
href="https://doi.org/10.4310/SII.2020.v13.n2.a10">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article proposes a spatial autoregression (SAR) model with time-varying coefficients. The model incorporates both spatial dependence and the impacts of explanatory variables, and all the coefficients are allowed to flexibly vary according to time. This article further develops a kernel-smoothed estimator (KSE) to estimate the time-varying coefficients. Compared with the maximum likelihood estimator (MLE) obtained at discrete time points, the KSE utilizes the potentially useful information from time neighborhoods. We have theoretically proved the consistency of the proposed KSE. A number of simulation studies show that the KSE is more accurate and performs substantially better than the MLE. Moreover, a real data analysis for a ride-hailing platform in China also shows that the KSE is more stable and interpretable. The proposed model as well as the KSE can be widely applied to data with a large number of cross-sectional units and regularly spaced time points.},
  archive      = {J_SII},
  author       = {Xu, Ke and Sun, Luping and Liu, Jin and Zhu, Xuening and Wang, Hansheng},
  doi          = {10.4310/SII.2020.v13.n2.a10},
  journal      = {Statistics and Its Interface},
  number       = {2},
  pages        = {261-270},
  shortjournal = {Stat. Interface},
  title        = {A spatial autoregression model with time-varying coefficients},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A variable selection approach to multiple change-points
detection. <em>SII</em>, <em>13</em>(2), 251–260. (<a
href="https://doi.org/10.4310/SII.2020.v13.n2.a9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change-point detection has been studied extensively with continuous data, while much less research has been carried out for categorical data. Focusing on ordinal data, we reframe the change-point detection problem in a Bayesian variable selection context. We propose a latent probit model in conjunction with reversible jump Markov chain Monte Carlo to estimate both the number and locations of change-points with ordinal data. We conduct extensive simulation studies to assess the performance of our method. As an illustration, we apply the new method to detect changes in the ordinal data from the north Atlantic tropical cyclone record, which has an indication of global warming in the past decades.},
  archive      = {J_SII},
  author       = {Lam, Chi Kin and Jin, Huaqing and Jiang, Fei and Yin, Guosheng},
  doi          = {10.4310/SII.2020.v13.n2.a9},
  journal      = {Statistics and Its Interface},
  number       = {2},
  pages        = {251-260},
  shortjournal = {Stat. Interface},
  title        = {A variable selection approach to multiple change-points detection},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sparse signal shrinkage and outlier detection in
high-dimensional quantile regression with variational bayes.
<em>SII</em>, <em>13</em>(2), 237–249. (<a
href="https://doi.org/10.4310/SII.2020.v13.n2.a8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Model misspecification can compromise valid inference in conventional quantile regression models. To address this issue, we consider two flexible model extensions for high-dimensional data. The first is a Bayesian quantile regression approach with variable selection, which uses a sparse signal shrinkage prior on the high-dimensional regression coefficients. The second extension robustifies conventional parametric quantile regression methods by including observation specific mean shift terms. Since the number of outliers is assumed to be small, the vector of mean shifts is sparse, which again motivates the use of a sparse signal shrinkage prior. Specifically, we exploit the horseshoe+ prior distribution for variable selection and outlier detection in the high-dimensional quantile regression models. Computational complexity is alleviated using fast mean field variational Bayes methods, and we compare results obtained by variational methods with those obtained using Markov chain Monte Carlo (MCMC).},
  archive      = {J_SII},
  author       = {Lim, Daeyoung and Park, Beomjo and Nott, David and Wang, Xueou and Choi, Taeryon},
  doi          = {10.4310/SII.2020.v13.n2.a8},
  journal      = {Statistics and Its Interface},
  number       = {2},
  pages        = {237-249},
  shortjournal = {Stat. Interface},
  title        = {Sparse signal shrinkage and outlier detection in high-dimensional quantile regression with variational bayes},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A composite nonparametric product limit approach for
estimating the distribution of survival times under length-biased and
right-censored data. <em>SII</em>, <em>13</em>(2), 221–235. (<a
href="https://doi.org/10.4310/SII.2020.v13.n2.a7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper considers a composite nonparametric product limit estimator for estimating the distribution of survival times when the data are length-biased and right censored. Our method takes into account auxiliary information that frequently arises in survival analysis, and is easier to implement than existing methods for estimating survival functions. We derive a strong representation of the proposed estimator, establish its consistency and asymptotic normality, and derive its convergence rate of approximation. As well, we prove that auxiliary information improves the asymptotic efficiency of the proposed estimator, and provide the values of the composite weights that result in the largest efficiency gain. Our proposed estimator fares well in comparison with other more complex methods in finite samples and offers a clear advantage with respect to computational time.},
  archive      = {J_SII},
  author       = {Fan, Shuqin and Zhao, Wei and Wan, Alan T. K. and Zhou, Yong},
  doi          = {10.4310/SII.2020.v13.n2.a7},
  journal      = {Statistics and Its Interface},
  number       = {2},
  pages        = {221-235},
  shortjournal = {Stat. Interface},
  title        = {A composite nonparametric product limit approach for estimating the distribution of survival times under length-biased and right-censored data},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Non-gaussian stochastic volatility model with jumps via
gibbs sampler. <em>SII</em>, <em>13</em>(2), 209–219. (<a
href="https://doi.org/10.4310/SII.2020.v13.n2.a6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work we propose a model for estimating volatility from financial time series, extending the non-Gaussian family of space-state models with exact marginal likelihood proposed by [6]. On the literature there are models focused on estimating financial assets risk, however, most of them rely on MCMC methods based on Metropolis algorithms, since full conditional posterior distributions are not known. We present an alternative model capable of automatically estimating the volatility, since all full conditional posterior distributions are known, and it is possible to obtain an exact sample of volatility parameters via Gibbs Sampler. The incorporation of jumps in returns allows the model to capture speculative movements of the data so that their influence does not propagate to volatility. We evaluate the performance of the algorithm using synthetic and real data time series and the results are satisfactory.},
  archive      = {J_SII},
  author       = {Rego, Arthur T. and dos Santos, Thiago R.},
  doi          = {10.4310/SII.2020.v13.n2.a6},
  journal      = {Statistics and Its Interface},
  number       = {2},
  pages        = {209-219},
  shortjournal = {Stat. Interface},
  title        = {Non-gaussian stochastic volatility model with jumps via gibbs sampler},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Zero-one-in ated simplex regression models for the analysis
of continuous proportion data. <em>SII</em>, <em>13</em>(2), 193–208.
(<a href="https://doi.org/10.4310/SII.2020.v13.n2.a5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Continuous data restricted in the closed unit interval [0,1] often appear in various fields. Neither the beta distribution nor the simplex distribution provides a satisfactory fitting for such data, since the densities of the two distributions are defined only in the open interval (0,1). To model continuous proportional data with excessive zeros and excessive ones, it is the first time that we propose a zero-one-inflated simplex (ZOIS) distribution, which can be viewed as a mixture of the Bernoulli distribution and the simplex distribution. Besides, we introduce a new minorization–maximization (MM) algorithm to calculate the maximum likelihood estimates (MLEs) of parameters in the simplex distribution without covariates. Likelihood-based inference methods for the ZOIS regression model are also provided. Some simulation studies are performed and the hospital stay data of Barcelona in 1988 and 1990 are analyzed to illustrate the proposed methods. The comparison between the ZOIS model and the zero-one-inflated beta (ZOIB) model is also presented.},
  archive      = {J_SII},
  author       = {Liu, Pengyi and Yuen, Kam Chuen and Wu, Liu-Cang and Tian, Guo-Liang and Li, Tao},
  doi          = {10.4310/SII.2020.v13.n2.a5},
  journal      = {Statistics and Its Interface},
  number       = {2},
  pages        = {193-208},
  shortjournal = {Stat. Interface},
  title        = {Zero-one-in ated simplex regression models for the analysis of continuous proportion data},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Additive hazards regression for case-cohort studies with
interval-censored data. <em>SII</em>, <em>13</em>(2), 181–191. (<a
href="https://doi.org/10.4310/SII.2020.v13.n2.a4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A large literature has been developed for the analysis of case-cohort studies that are often performed with the aim of reducing the cost on the collection of covariate information. In particular, many authors have discussed their regression analysis under the framework of the additive hazards model, which is often preferred when the risk difference is of main interest. However, all of the existing methods assume or are applicable only to right-censored data. In this paper, we consider the case of interval-censored data, which often occur in practice and include right-censored data as a special case, and propose two estimation approaches, an estimating equation-based method and a maximum likelihood method. The resulting estimators of regression parameters are shown to be consistent and asymptotically normal. Also a simulation study is conducted and suggests that the proposed methods works well in practice, and an application is provided.},
  archive      = {J_SII},
  author       = {Du, Mingyue and Li, Huiqiong and Sun, Jianguo},
  doi          = {10.4310/SII.2020.v13.n2.a4},
  journal      = {Statistics and Its Interface},
  number       = {2},
  pages        = {181-191},
  shortjournal = {Stat. Interface},
  title        = {Additive hazards regression for case-cohort studies with interval-censored data},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Hypothesis testing for normal distributions: A unified
framework and new developments. <em>SII</em>, <em>13</em>(2), 167–179.
(<a href="https://doi.org/10.4310/SII.2020.v13.n2.a3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hypothesis testing for normal distributions is one important problem in statistics and related fields including management science, engineering science and medical science. In this paper, from a very unique perspective, we propose a unified framework to comprehensively review the existing literature on the one- and two-sample testing problems of normal distributions. The unified framework has integrated the literature in a way that it includes most commonly used tests as special cases, including the one-sample mean test, the one-sample variance test, the two-sample mean test, the two-sample variance test, and the Behrens–Fisher test. The unified framework has also put forward two new hypothesis tests that are rarely studied in the literature. To complete the puzzle, we propose two likelihood ratio test statistics to solve those new testing problems. Simulation studies and real data examples are also provided to demonstrate that our proposed test statistics are appropriate for practical implementation.},
  archive      = {J_SII},
  author       = {Zhou, Yuejin and Ho, Sze-Yui and Liu, Jiahua and Tong, Tiejun},
  doi          = {10.4310/SII.2020.v13.n2.a3},
  journal      = {Statistics and Its Interface},
  number       = {2},
  pages        = {167-179},
  shortjournal = {Stat. Interface},
  title        = {Hypothesis testing for normal distributions: A unified framework and new developments},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Sampling high dimensional tables with applications to
assessing linkage disequilibrium. <em>SII</em>, <em>13</em>(2), 157–166.
(<a href="https://doi.org/10.4310/SII.2020.v13.n2.a2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a sequential importance sampling strategy to sample high dimensional tables with fixed one way margins. The proposal distribution for the method is constructed by adapting an approximation to the total number of tables available in the literature. We apply the method to estimating the total number of tables and assessing linkage disequilibrium in multimarker genetic data with the table representing haplotype count data. We demonstrate efficient and accurate performance in these practical, real-world examples. The method may be applied in any situation in which uniformly sampling high dimensional tables with fixed one way margins is of interest. Detailed derivations are provided in the appendix.},
  archive      = {J_SII},
  author       = {Eisinger, Robert D. and Su, Xiao and Chen, Yuguo},
  doi          = {10.4310/SII.2020.v13.n2.a2},
  journal      = {Statistics and Its Interface},
  number       = {2},
  pages        = {157-166},
  shortjournal = {Stat. Interface},
  title        = {Sampling high dimensional tables with applications to assessing linkage disequilibrium},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bi-level variable selection in high dimensional tobit
models. <em>SII</em>, <em>13</em>(2), 151–156. (<a
href="https://doi.org/10.4310/SII.2020.v13.n2.a1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To study variable selection for high-dimensional Tobit models, we formulate Tobit models to single-index models. We hybrid group variable selection procedures for single index models and univariate regression methods for Tobit models to achieve variable selection for Tobit models with group structures taken into consideration. The procedure is computationally efficient and easily implemented. Finite sample experiments show its promising performance. We also illustrate its utility by analyzing a dataset from an HIV/AIDS study.},
  archive      = {J_SII},
  author       = {Huang, Hailin and Shangguan, Jizi and Li, Yuanzhang and Liang, Hua},
  doi          = {10.4310/SII.2020.v13.n2.a1},
  journal      = {Statistics and Its Interface},
  number       = {2},
  pages        = {151-156},
  shortjournal = {Stat. Interface},
  title        = {Bi-level variable selection in high dimensional tobit models},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Reference bayesian analysis for the generalized lognormal
distribution with application to survival data. <em>SII</em>,
<em>13</em>(1), 139–149. (<a
href="https://doi.org/10.4310/SII.2020.v13.n1.a12">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a reference Bayesian approach for the estimation of the parameters of the generalized lognormal distribution in the presence of survival data. It is shown that the reference prior leads to a proper posterior distribution while the Jeffreys prior leads to an improper posterior. Simulation studies were performed to analyze the frequentist properties of credible intervals from the reference posterior distribution, considering complete and censored data. The proposed methodology is illustrated using two real datasets.},
  archive      = {J_SII},
  author       = {Tomazella, Vera L. D. and de Jesus, Sandra R. and Louzada, Francisco and Nadarajah, Saralees and Ramos, Pedro L.},
  doi          = {10.4310/SII.2020.v13.n1.a12},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {139-149},
  shortjournal = {Stat. Interface},
  title        = {Reference bayesian analysis for the generalized lognormal distribution with application to survival data},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian kernel adaptive grouping learning for
multi-dimensional datasets. <em>SII</em>, <em>13</em>(1), 127–137. (<a
href="https://doi.org/10.4310/SII.2020.v13.n1.a11">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the development of information technology, a large number of datasets with complex structures, such as multidimensional datasets, need to be processed and analyzed. In this paper we propose a kernel-based statistical learning algorithm, Bayesian Kernel Adaptive Grouping Learning (BKAGL), to provide an innovative solution for the classification problem of multi-dimensional datasets. BKAGL can integrate information from different dimensions adaptively. Meanwhile, we adopt the Bayesian framework which can infer the approximate posterior distributions of parameters. The utilization of grouping features can help find which groups of features have more contributions to the response. Simulation results illustrate that BKAGL outperforms some classical classification methods and the corresponding ungrouped method. The analysis of the electrocardiogram (ECG) and electroencephalography (EEG) datasets shows that BKAGL has the highest classification accuracy and provides explanatory information.},
  archive      = {J_SII},
  author       = {Wang, Xiaozhou and Dong, Fangli},
  doi          = {10.4310/SII.2020.v13.n1.a11},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {127-137},
  shortjournal = {Stat. Interface},
  title        = {Bayesian kernel adaptive grouping learning for multi-dimensional datasets},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). GPU accelerated liquid association GALA. <em>SII</em>,
<em>13</em>(1), 119–125. (<a
href="https://doi.org/10.4310/SII.2020.v13.n1.a10">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {High throughput biological assays have provided numerous data sources for studying complex interactions between multiple variables in a biological system. Many computational tools for exploring the voluminous biological data are based on pair-wise correlation between variables. Liquid Association (LA) is a novel statistical concept for inferring higher order of association between variables in a system. While LA was originally introduced to study gene-gene interaction involving three genes at a time, it can be applied for correlating biological measurements with clinical variables such as drug sensitivity profiling and patient survival time. It is computationally expensive to compute LA scores for all possible triplets in very large datasets. Here we show how to take advantage of Graphic Processing Units (GPUs) for speeding up the LA computing. Our GPU-accelerated version of LA computation (GALA) achieved nearly 200-fold improvement over the traditional CPU-alone version. A companion package in R was developed for facilitating follow-up analysis and improving user experience. An example on Global Health Observatory data is provided to showcase how LA analysis can be applied in other data intensive fields.},
  archive      = {J_SII},
  author       = {Yuan, Shinsheng and Wu, Guani and Li, Yu-Cheng and Lu, Yi-Chang and Li, Ker-Chau},
  doi          = {10.4310/SII.2020.v13.n1.a10},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {119-125},
  shortjournal = {Stat. Interface},
  title        = {GPU accelerated liquid association GALA},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Copula modeling for data with ties. <em>SII</em>,
<em>13</em>(1), 103–117. (<a
href="https://doi.org/10.4310/SII.2020.v13.n1.a9">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Tied observations in copula modeling may cause serious problems to rank-based inference methods that are intended for data with no ties. Simple methods such as breaking the ties at random or using midrank could lead to bias in estimation and invalidity in naive bootstrap inferences. We propose to treat the ranks of tied observations as being interval censored and estimate the copula parameters by maximizing a pseudo-likelihood based on interval censored pseudo-observations. A parametric bootstrap procedure that preserves the tied ranks in the observed data is adapted to do interval estimation and goodness-of-fit test. The proposed approach is shown to be very competitive in comparison to the simple treatments in a large scale simulation study. The utility of the method is illustrated in real data examples.},
  archive      = {J_SII},
  author       = {Li, Yan and Li, Yang and Qin, Yichen and Yan, Jun},
  doi          = {10.4310/SII.2020.v13.n1.a9},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {103-117},
  shortjournal = {Stat. Interface},
  title        = {Copula modeling for data with ties},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Leverage effect in high-frequency data with market
microstructure. <em>SII</em>, <em>13</em>(1), 91–101. (<a
href="https://doi.org/10.4310/SII.2020.v13.n1.a8">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The leverage effect is an important explanation for volatility asymmetry, which has got extensively attention in the recent years. In this paper, we introduces a new estimator of leverage effect. The key feature of the proposed estimator is explored in the setting when the microstructure noise model is the parameter function of trading information. The proposed estimator shows good statistical performances via theorems and simulations study. Specially, the estimator has a convergence rate $n^{1/4}$. The QQ-Plots, Histogram plots and quartiles perform sufficient asymptotical normality compared with the exist estimated methods. An empirical study is carried out to demonstrate that the proposed estimator could present the efficient application value, and confirm that the leverage effect plays an important role in forecasting volatility.},
  archive      = {J_SII},
  author       = {Yuan, Huiling and Mu, Yan and Zhou, Yong},
  doi          = {10.4310/SII.2020.v13.n1.a8},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {91-101},
  shortjournal = {Stat. Interface},
  title        = {Leverage effect in high-frequency data with market microstructure},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Modeling RCOV matrices with a generalized threshold
conditional autoregressive wishart model. <em>SII</em>, <em>13</em>(1),
77–89. (<a href="https://doi.org/10.4310/SII.2020.v13.n1.a7">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose a generalized threshold conditional autoregressive Wishart (GTCAW) model to analyze the dynamics of the realized covariance (RCOV) matrices. This model extends the idea of [29] to a threshold framework. It is believed that, as in many financial time series, the dynamic of RCOV matrices exhibits nonlinearity and may be better explained by a threshold type model. The noncentrality matrix and scale matrix of the Wishart distribution are piecewise linear driven by the lagged values of RCOV matrices and retain two different sources of dynamics. The GTCAW model guarantees the symmetry and positive definiteness of RCOV matrices, some simulation results on the maximum likelihood estimation are also given. Real data examples based on daily RCOV matrices present the nonlinear behavior in these time series and the usefulness of the proposed model.},
  archive      = {J_SII},
  author       = {Cui, Yan and Zhu, Fukang and Li, Wai Keung},
  doi          = {10.4310/SII.2020.v13.n1.a7},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {77-89},
  shortjournal = {Stat. Interface},
  title        = {Modeling RCOV matrices with a generalized threshold conditional autoregressive wishart model},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Adaptive LASSO regression against heteroscedastic
idiosyncratic factors in the covariates. <em>SII</em>, <em>13</em>(1),
65–75. (<a href="https://doi.org/10.4310/SII.2020.v13.n1.a6">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  archive      = {J_SII},
  author       = {Zhang, Kaimeng and Ng, Chi Tim},
  doi          = {10.4310/SII.2020.v13.n1.a6},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {65-75},
  shortjournal = {Stat. Interface},
  title        = {Adaptive LASSO regression against heteroscedastic idiosyncratic factors in the covariates},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian variable selection and estimation in joint
confirmatory factor analysis–cox model. <em>SII</em>, <em>13</em>(1),
49–63. (<a href="https://doi.org/10.4310/SII.2020.v13.n1.a5">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we propose the joint confirmatory factor analysis–Cox model to assess the effects of observed and latent risk factors on survival time. The Bayesian adaptive Lasso procedure is developed to simultaneously conduct estimation and variable selection for the proposed model. Nice features including the empirical performance of the proposed method are demonstrated by simulation studies. The proposed method is applied to analyze the bladder cancer data set obtained from the Surveillance, Epidemiology, and End Results (SEER) Program of the National Cancer Institute.},
  archive      = {J_SII},
  author       = {Liang, Chenyi and Cai, Jingheng},
  doi          = {10.4310/SII.2020.v13.n1.a5},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {49-63},
  shortjournal = {Stat. Interface},
  title        = {Bayesian variable selection and estimation in joint confirmatory factor analysis--cox model},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). High-dimensional two-sample precision matrices test: An
adaptive approach through multiplier bootstrap. <em>SII</em>,
<em>13</em>(1), 37–48. (<a
href="https://doi.org/10.4310/SII.2020.v13.n1.a4">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Precision matrix, which is the inverse of covariance matrix, plays an important role in statistics, as it captures the partial correlation between variables. Testing the equality of two precision matrices in high-dimensional setting is a very challenging but meaningful problem, especially in the differential network modelling. To our best knowledge, existing test is only powerful for sparse alternative patterns where two precision matrices differ in a small number of elements. In this paper we propose a data-adaptive test which is powerful against either dense or sparse alternatives. Multiplier bootstrap approach is utilized to approximate the limiting distribution of the test statistic. Theoretical properties including asymptotic size and power of the test are investigated. Simulation study verifies that the data-adaptive test performs well under various alternative scenarios. The practical usefulness of the test is illustrated by applying it to a gene expression data set associated with lung cancer.},
  archive      = {J_SII},
  author       = {Zhang, Mingjuan and He, Yong and Zhou, Cheng and Zhang, Xinsheng},
  doi          = {10.4310/SII.2020.v13.n1.a4},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {37-48},
  shortjournal = {Stat. Interface},
  title        = {High-dimensional two-sample precision matrices test: An adaptive approach through multiplier bootstrap},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). A test on linear hypothesis of <span
class="math inline"><em>k</em></span>-sample means in high-dimensional
data. <em>SII</em>, <em>13</em>(1), 27–36. (<a
href="https://doi.org/10.4310/SII.2020.v13.n1.a3">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, a new test procedure is proposed to test a linear hypothesis of $k$-sample mean vectors in high-dimensional normal models with heteroskedasticity. The motivation is on the basis of the generalized likelihood ratio method and the Bennett transformation. The asymptotic distributions of the new test are derived under null and local alternative hypotheses under mild conditions. Simulation results show that the new test can control the nominal level reasonably and has greater power than competing tests in some cases. Moreover, numerical studies illustrate that our proposed test can also be applied to non-normal data.},
  archive      = {J_SII},
  author       = {Cao, Mingxiang and Sun, Peng and He, Daojiang and Wang, Rui and Xu, Xingzhong},
  doi          = {10.4310/SII.2020.v13.n1.a3},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {27-36},
  shortjournal = {Stat. Interface},
  title        = {A test on linear hypothesis of $k$-sample means in high-dimensional data},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Nonparametric statistical inference and imputation for
incomplete categorical data. <em>SII</em>, <em>13</em>(1), 17–25. (<a
href="https://doi.org/10.4310/SII.2020.v13.n1.a2">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Missingness in categorical data is a common problem in various real applications. Traditional approaches either utilize only the complete observations or impute the missing data by some ad hoc methods rather than the true conditional distribution of the missing data, thus losing or distorting the rich information in the partial observations. In this paper, we propose the Dirichlet Process Mixture of Collapsed Product-Multinomials (DPMCPM) to model the full data jointly and compute the model efficiently. By fitting an infinite mixture of product-multinomial distributions, DPMCPM is applicable for any categorical data regardless of the true distribution, which may contain complex association among variables. Under the framework of latent class analysis, we show that DPMCPM can model general missing mechanisms by creating an extra category to denote missingness, which implicitly integrates out the missing part with regard to their true conditional distribution. Through simulation studies and a real application, we demonstrate that DPMCPM outperforms existing approaches on statistical inference and imputation for incomplete categorical data of various missing mechanisms. DPMCPM is implemented as the R package $\texttt{MMDai}$, which is available from the Comprehensive R Archive Network.},
  archive      = {J_SII},
  author       = {Wang, Chaojie and Shen, Linghao and Li, Han and Fan, Xiaodan},
  doi          = {10.4310/SII.2020.v13.n1.a2},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {17-25},
  shortjournal = {Stat. Interface},
  title        = {Nonparametric statistical inference and imputation for incomplete categorical data},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
<li><details>
<summary>
(2020). Bayesian longitudinal multilevel item response modeling
approach for studying individual growth differences. <em>SII</em>,
<em>13</em>(1), 1–16. (<a
href="https://doi.org/10.4310/SII.2020.v13.n1.a1">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A longitudinal multilevel item response model is proposed for measuring changes in individual growth over time. To estimate the model parameters, a combined Bayesian procedure is developed. The deviance information criterion (DIC) and the widely applicable information criterion (WAIC) are used to assess the competing models. The simulation results show that the combined Bayesian estimation method performs perfectly in terms of recovering model parameters under various design conditions. Finally, a longitudinal dataset about the development of achievement in mathematics illustrates the significance and implementation of the proposed procedure.},
  archive      = {J_SII},
  author       = {Qu, Shuang and Zhang, Jiwei and Tao, Jian},
  doi          = {10.4310/SII.2020.v13.n1.a1},
  journal      = {Statistics and Its Interface},
  number       = {1},
  pages        = {1-16},
  shortjournal = {Stat. Interface},
  title        = {Bayesian longitudinal multilevel item response modeling approach for studying individual growth differences},
  volume       = {13},
  year         = {2020},
}
</textarea>
</details></li>
</ul>

</body>
</html>
