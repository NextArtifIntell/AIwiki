<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TKDE_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tkde---69">TKDE - 69</h2>
<ul>
<li><details>
<summary>
(2025). Boosting GNN-based link prediction via PU-AUC optimization.
<em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3525490">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Link prediction, which aims to predict the existence of a link between two nodes in a network, has various applications ranging from friend recommendation to protein interaction prediction. Recently, Graph Neural Network (GNN)-based link prediction has demonstrated its advantages and achieved the state-of-the-art performance. Typically, GNN-based link prediction can be formulated as a binary classification problem. However, in link prediction, we only have positive data (observed links) and unlabeled data (unobserved links), but no negative data. Therefore, Positive Unlabeled (PU) learning naturally fits the link prediction scenario. Unfortunately, the unknown class prior and data imbalance of networks impede the use of PU learning in link prediction. To deal with these issues, this paper proposes a novel model-agnostic PU learning algorithm for GNN-based link prediction by means of Positive-Unlabeled Area Under the Receiver Operating Characteristic Curve (PU-AUC) optimization. The proposed method is free of class prior estimation and able to handle the data imbalance. Moreover, we propose an accelerated method to reduce the operational complexity of PU-AUC optimization from quadratic to approximately linear. Extensive experiments back up our theoretical analysis and validate that the proposed method is capable of boosting the performance of the state-of-the-art GNN-based link prediction models.},
  archive      = {J_TKDE},
  author       = {Yuren Mao and Yu Hao and Xin Cao and Yunjun Gao and Chang Yao and Xuemin Lin},
  doi          = {10.1109/TKDE.2025.3525490},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Boosting GNN-based link prediction via PU-AUC optimization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CLEAR: Spatial-temporal traffic data representation learning
for traffic prediction. <em>TKDE</em>, 1–16. (<a
href="https://doi.org/10.1109/TKDE.2025.3536009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the evolving field of urban development, precise traffic prediction is essential for optimizing traffic and mitigating congestion. While traditional graph learning-based models effectively exploit complex spatial-temporal correlations, their reliance on trivially generated graph structures or deeply intertwined adjacency learning without supervised loss significantly impedes their efficiency. This paper presents Contrastive Learning of spatial-tEmporal trAffic data Representations (CLEAR) framework, a comprehensive approach to spatial-temporal traffic data representation learning aimed at enhancing the accuracy of traffic predictions. Employing self-supervised contrastive learning, CLEAR strategically extracts discriminative embeddings from both traffic time-series and graph-structured data. The framework applies weak and strong data augmentations to facilitate subsequent exploitations of intrinsic spatial-temporal correlations that are critical for accurate prediction. Additionally, CLEAR incorporates advanced representation learning models that transmute these dynamics into compact, semantic-rich embeddings, thereby elevating downstream models&#39; prediction accuracy. By integrating with existing traffic predictors, CLEAR boosts predicting performance and accelerates the training process by effectively decoupling adjacency learning from correlation learning. Comprehensive experiments validate that CLEAR can robustly enhance the capabilities of existing graph learning-based traffic predictors and provide superior traffic predictions with a straightforward representation decoder. This investigation highlights the potential of contrastive representation learning in developing robust traffic data representations for traffic prediction.},
  archive      = {J_TKDE},
  author       = {James Jianqiao Yu and Xinwei Fang and Shiyao Zhang and Yuxin Ma},
  doi          = {10.1109/TKDE.2025.3536009},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {CLEAR: Spatial-temporal traffic data representation learning for traffic prediction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Are large language models really good logical reasoners? A
comprehensive evaluation and beyond. <em>TKDE</em>, 1–15. (<a
href="https://doi.org/10.1109/TKDE.2025.3536008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Logical reasoning consistently plays a fundamental and significant role in the domains of knowledge engineering and artificial intelligence. Recently, Large Language Models (LLMs) have emerged as a noteworthy innovation in natural language processing (NLP). However, the question of whether LLMs can effectively address the task of logical reasoning, which requires gradual cognitive inference similar to human intelligence, remains unanswered. To this end, we aim to bridge this gap and provide comprehensive evaluations in this paper. First, to offer systematic evaluations, we select fifteen typical logical reasoning datasets and organize them into deductive, inductive, abductive and mixed-form reasoning settings. Considering the comprehensiveness of evaluations, we include 3 early-era representative LLMs and 4 trending LLMs. Second, different from previous evaluations relying only on simple metrics (e.g., accuracy), we propose fine-level evaluations in objective and subjective manners, covering both answers and explanations, including answer correctness, explain correctness, explain completeness and explain redundancy. Additionally, to uncover the logical flaws of LLMs, problematic cases will be attributed to five error types from two dimensions, i.e., evidence selection process and reasoning process. Third, to avoid the influences of knowledge bias and concentrate purely on benchmarking the logical reasoning capability of LLMs, we propose a new dataset with neutral content. Based on the in-depth evaluations, this paper finally forms a general evaluation scheme of logical reasoning capability from six dimensions (i.e., Correct, Rigorous, Self-aware, Active, Oriented and No hallucination). It reflects the pros and cons of LLMs and gives guiding directions for future works.},
  archive      = {J_TKDE},
  author       = {Fangzhi Xu and Qika Lin and Jiawei Han and Tianzhe Zhao and Jun Liu and Erik Cambria},
  doi          = {10.1109/TKDE.2025.3536008},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Are large language models really good logical reasoners? a comprehensive evaluation and beyond},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Group-aware dynamic graph representation learning for next
POI recommendation. <em>TKDE</em>, 1–12. (<a
href="https://doi.org/10.1109/TKDE.2025.3538005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Next POI recommendation, which has attracted many attentions recently, is a complex study due to the sparsity of check-in data and numerous sequential patterns. The recent methods based on sequential modeling have shown promising applicability for this task. However, most of existing next POI recommendation researches only model users&#39; preferences based on their own sequences and ignore the influence of partners who visit POI with the target user and may change with time. Inspired by dynamic Graph neural networks, we propose a Group-aware Dynamic Graph Representation Learning (GDGRL) method for next POI recommendation. GDGRL connects different user sequences and specific partners via dynamic graph structure, which contains interactions between users and POIs as well as influence of partners. The users&#39; dynamic preferences are learned from group-aware dynamic graph and context-aware dynamic graph through dynamic graph neural networks. Finally, the next POI recommendation task is transformed into a link prediction between user node and POI node in the dynamic graph. Extensive experiments on two real-world datasets show that GDGRL outperforms several state-of-the-art approaches.},
  archive      = {J_TKDE},
  author       = {Ruichang Li and Xiangwu Meng and Yujie Zhang},
  doi          = {10.1109/TKDE.2025.3538005},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Group-aware dynamic graph representation learning for next POI recommendation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partitioned dynamic hub labeling for large road networks.
<em>TKDE</em>, 1–18. (<a
href="https://doi.org/10.1109/TKDE.2025.3538694">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shortest path computation is ubiquitous in various applications in road networks and the index-based algorithms, especially hub labeling, can boost the query performance dramatically. However, traffic conditions keep changing in real life, making the precomputed index unable to answer the query correctly. In this work, we adopt the state-of-the-art tree decomposition-based hub labeling (TDHL) as the underlying index and design efficient algorithms to incrementally maintain the index. Specifically, we first analyze the structural stability of the index in dynamic road networks which enables us to concentrate on label value maintenance. We then introduce the minimum weight property and minimum distance property to guarantee index correctness without graph traversal. Moreover, we propose the star-centric paradigm for tracing index change and design various pruning techniques to further accelerate index maintenance. We also extend our algorithms to batch mode for shared computation, to structural maintenance for full types of updates, and generalize to all kinds of TDHL. Finally, we further improve the index maintenance efficiency and scalability of our algorithms by leveraging graph partition. Our experimental results validate the superiority of our proposals over existing solutions on both index maintenance and query processing.},
  archive      = {J_TKDE},
  author       = {Mengxuan Zhang and Xinjie Zhou and Lei Li and Xiaofang Zhou},
  doi          = {10.1109/TKDE.2025.3538694},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Partitioned dynamic hub labeling for large road networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CogLign: Interpretable text sentiment determination by
aligning cognition between EEG-derived brain graph and text-derived
knowledge graph. <em>TKDE</em>, 1–17. (<a
href="https://doi.org/10.1109/TKDE.2025.3538618">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays, detecting sentiment or emotion from user generated texts has been intensively studied in natural language understanding, especially via neural-based models based on text representation. However, the interpretability on how could the final text sentiment be determined by neural-based text representation has not been thoroughly unfolded yet. Consequently, in this paper, we propose CogLign which injects the neural-cognition derived from Electroencephalogram (EEG)-signal into the neural-based text sentiment analysis model, aimed at learning the activation of brain regions stimulated by different sentiments, so as to guide our proposed CogLign to make proper determination on text sentiment in brain-like way. Specifically, on the one hand, the given videos in different sentiments have been watched by subjects, during which the EEG-signals are monitored to construct brain connectivity pattern as brain graph (BG), attaining more obvious sentiment response on brain region activation for neural-cognition. On the other hand, we interpret the video-plots (or video-semantics) along timeline into text, where the entire video-interpreted-text will be strictly bound with the whole EEG-signal-sequence by segment via the fixed size of time-window. Then, entities and relations are extracted from the video-interpreted-text to construct knowledge graph (KG), depicting text semantics. Next, mapping from entities (or nodes) in KG to EEG-Electrodes (or nodes) in BG, further dated back to different brain regions, has been learned via cognition alignment between the EEG-derived BG and text-derived KG. In this way, by aligning neural cognition from brain graph with the semantic cognition from knowledge graph, our proposed framework CogLign can not only achieve the overall best sentiment analysis performance on the video-interpreted-text, but can also detect brain connectivity patterns in different sentiments more consistent with the prior conclusion of brain region sentiment preference, revealing competitive interpretability on text sentiment determination},
  archive      = {J_TKDE},
  author       = {Huan Rong and Wenxuan Ji and Tinghuai Ma and Weiyi Ding and Victor S. Sheng},
  doi          = {10.1109/TKDE.2025.3538618},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {CogLign: Interpretable text sentiment determination by aligning cognition between EEG-derived brain graph and text-derived knowledge graph},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards stable and explainable attention mechanisms.
<em>TKDE</em>, 1–15. (<a
href="https://doi.org/10.1109/TKDE.2025.3538583">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, attention mechanism has become a standard fixture in most state-of-the-art natural language processing (NLP) models, not only due to the outstanding performance it could gain but also due to plausible innate explanations for the behaviors of neural architectures it provides, which is notoriously difficult to analyze. However, recent studies show that attention is unstable against randomness and perturbations during training or testing, such as random seeds and slight perturbation of embedding vectors, which impedes it from becoming a faithful explanation tool. Thus, a natural question is whether we can find some substitute for the current attention that is more stable and could keep the most important characteristics of explanation and prediction of attention. In this paper, to resolve the problem, we provide a rigorous definition of such alternate namely SEAT (Stable and Explainable Attention). Specifically, a SEAT should have the following three properties: (1) Its prediction distribution is enforced to be close to the distribution based on the vanilla attention; (2) Its top-$k$ indices have large overlaps with those of the vanilla attention; (3) It is robust w.r.t perturbations, i.e., any slight perturbation on SEAT will not change the prediction distribution too much, which implicitly indicates that it is stable to randomness and perturbations. To further improve the interpretability stability against perturbations, based on SEAT we provide another definition called SEAT++. Then we propose a method to get a SEAT++, which could be considered an ad hoc modification for canonical attention. Finally, through intensive experiments on various datasets, we compare our SEAT and SEAT++ with other baseline methods using RNN, BiLSTM, and BERT architectures via six different evaluation metrics for model interpretation, stability, and accuracy. Results show that SEAT and SEAT++ are more stable against different perturbations and randomness while also keeping the explainability of attention, which indicates they provide more faithful explanations. Moreover, compared with vanilla attention, there is almost no utility (accuracy) degradation for SEAT and SEAT++.},
  archive      = {J_TKDE},
  author       = {Lijie Hu and Xinhai Wang and Yixin Liu and Ninghao Liu and Mengdi Huai and Lichao Sun and Di Wang},
  doi          = {10.1109/TKDE.2025.3538583},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Towards stable and explainable attention mechanisms},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TagRec: Temporal-aware graph contrastive learning with
theoretical augmentation for sequential recommendation. <em>TKDE</em>,
1–14. (<a href="https://doi.org/10.1109/TKDE.2025.3538706">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential recommendation systems aim to predict the future behaviors of users based on their historical interactions. Despite the success of neural architectures like Transformer and Graph Neural Networks, these models often struggle with the inherent challenge of sparse data in accurately predicting future user behaviors. To alleviate the data sparsity problem, some methods leverage the contrastive learning to generate contrastive views, assuming the items appear discretely at the same time intervals and focusing on the sequence order. However, these approaches neglect the crucial temporal-aware collaborative patterns hidden within the user-item interactions, leading to a limited variety of contrastive pairs and less informative embeddings. The proposed framework, Temporal-aware graph contrastive learning with theoretical guarantees for sequential Recommendation (TagRec), integrates temporal-aware collaborative patterns with adaptive data augmentation to generate more informative user and item representations. TagRec employs a temporal-aware graph neural network to embed the original graph, then generates augmented graphs through the addition of interactions via latent user interest mining, the dropping of redundant interaction edges, and the perturbation of temporal information. Theoretical guarantees are provided that these augmentations enhance the graph&#39;s utility. Extensive experiments on real-world datasets demonstrate the superiority of the proposed approach over the state-of-the-art recommendation methods.},
  archive      = {J_TKDE},
  author       = {Tianhao Peng and Haitao Yuan and Yongqi Zhang and Yuchen Li and Peihong Dai and Qunbo Wang and Senzhang Wang and Wenjun Wu},
  doi          = {10.1109/TKDE.2025.3538706},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {TagRec: Temporal-aware graph contrastive learning with theoretical augmentation for sequential recommendation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Discovery of temporal network motifs. <em>TKDE</em>, 1–14.
(<a href="https://doi.org/10.1109/TKDE.2025.3538514">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network motifs provide a deep insight into the network functional abilities, and have proven useful in various practical applications. Existing studies reveal that different definitions of motifs may be needed for different temporal networks. In this study, we focus on a class of temporal networks such that the nodes and edges keep fixed, but the edge labels vary regularly with timestamps. First, we propose a proper definition of temporal motifs, which appear continuously within sufficiently large time intervals, to properly reinterpret the recurrent and statistically significant nature of motifs in temporal networks. Second, we develop a low polynomial time solution to find temporal motifs for all possible time intervals with the top to bottom and right to left scheme, based on the analyses of the properties for temporal motifs. Third, we develop a theoretically faster incremental solution to efficiently find temporal motifs to support continuously updates of temporal networks, by identifying unaffected time intervals and unnecessary edges. Finally, we have conducted extensive experiments to verify the efficiency and usefulness of our static and incremental solutions.},
  archive      = {J_TKDE},
  author       = {Hanqing Chen and Shuai Ma and Junfeng Liu and Lizhen Cui},
  doi          = {10.1109/TKDE.2025.3538514},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Discovery of temporal network motifs},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Large-scale hierarchical causal discovery via weak prior
knowledge. <em>TKDE</em>, 1–17. (<a
href="https://doi.org/10.1109/TKDE.2025.3537832">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Causal discovery faces significant challenges as the number of hypotheses grows exponentially with the number of variables. This complexity becomes particularly daunting when dealing with large sets of variables. We introduce a novel divide-and-conquer method that uniquely handles this challenge. The existing division strategies often rely on conditional independency (CI) tests or data-driven clustering to split variables, which can suffer from the typical data scarcity in large-scale settings, thus leading to inaccurate division results. The proposed method overcomes this by implementing a data-independent division strategy, which constructs a prior structure, informed by potential causal relationships identified using a Large Language Model (LLM), to guide recursively dividing variables into sub-sets. This approach avoids the impact of data insufficiency and is robust against potential incompleteness in the prior structure. In the merging phase, we adopt a score-based refinement strategy to address fake causal links caused by hidden variables in sub-sets, which eliminates edges in the intersected parts of sub-sets to optimize the score of local structures. While maintaining both correctness and completeness under the faithfulness assumption, this novel merging approach demonstrates enhanced performance than the conventional CI-test based merging strategy in practical scenarios. Empirical evaluations on various large-scale datasets demonstrate the proposed approach&#39;s superior accuracy and efficiency compared to existing causal discovery methods.},
  archive      = {J_TKDE},
  author       = {Xiangyu Wang and Taiyu Ban and Lyuzhou Chen and Derui Lyu and Qinrui Zhu and Huanhuan Chen},
  doi          = {10.1109/TKDE.2025.3537832},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Large-scale hierarchical causal discovery via weak prior knowledge},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TaylorS: A multi-order expansion structure for urban
spatio-temporal forecasting. <em>TKDE</em>, 1–17. (<a
href="https://doi.org/10.1109/TKDE.2025.3538857">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Although a variety of models have been proposed for urban spatio-temporal forecasting, most existing forecasting models are developed manually for specific tasks. By investigating the correlation between multi-order derivative and spatio-temporal data, we propose a generic yet simple plug-in structure, named TaylorS, to improve the performance and generalization of existing forecasting models. The TaylorS converts the non-linear regression problem into a multi-order non-linear approximation problem by plugging a Taylor expansion into the forecasting task. To achieve this, we design a two-step training framework, including a training step and an adjusting step. During training, we train a given forecasting model as a base model to be equipped with prior knowledge. During adjusting, we fine-tune the base model while plugging an adjustment model into the base model. The adjustment model, as a multi-order expansion, takes the multi-order derivative of data to evaluate data uncertainty for further forecasting approximation and adjustment. Extensive experimental results demonstrate that the proposed TaylorS framework can consistently improve the performance of existing state-of-the-art methods and generalize these methods to different forecasting tasks. The source code for the TaylorS framework is published and available at https://github.com/JianyangQin/TaylorS.},
  archive      = {J_TKDE},
  author       = {Jianyang Qin and Yan Jia and Binxing Fang and Qing Liao},
  doi          = {10.1109/TKDE.2025.3538857},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {TaylorS: A multi-order expansion structure for urban spatio-temporal forecasting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable multi-view graph clustering with cross-view
corresponding anchor alignment. <em>TKDE</em>, 1–15. (<a
href="https://doi.org/10.1109/TKDE.2025.3538852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view graph clustering (MVGC) explores pairwise correlations of entire instances and comprehensively aggregates diverse source information with optimal graph structure. One major issue of practical MVGC is the high time and space complexities prohibiting being applied on large-scale applications. As a promising solution of addressing large-scale problems, anchor-based strategy identifies small portion and key landmarks to serve as replacements for the entire dataset. Despite of its efficiency, anchors chosen across views may be semantically unaligned contrasting to naturally-aligned full sample setting, which may lead to the latter inappropriate graph fusion. Limited attention has been focused on the mentioned Multi-View Anchor-Unaligned Problem (MV-AUP) in the existing literature. In this paper, we firstly revisit existing multi-view anchor graph clustering frameworks and present the MV-AUP phenomenon. Then, we propose a novel Multi-view Corresponding Anchor Graph Alignment Fusion framework (MV-CAGAF), which elegantly solves MV-AUP with structural representation matching in multi-dimensional spaces. Further, we theoretically prove our proposed structural matching approach can be regarded as minimizing the EMD distance of the two relative anchor distributions. Based on this, we design the innovative multi-view anchor graph fusion paradigm with correspondence alignment, which inherits the linear sample complexity for scalable cross-view clustering. Our proposed MV-CAGAF achieves significant improvements with the help of the novel fusion framework on comprehensive benchmark datasets. Most importantly, the experimental results on both of the simulated and real-world datasets significantly prove the importance of cross-view alignment for large-scale multi-view clustering.},
  archive      = {J_TKDE},
  author       = {Siwei Wang and Xinwang Liu and Qing Liao and Yi Wen and En Zhu and Kunlun He},
  doi          = {10.1109/TKDE.2025.3538852},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Scalable multi-view graph clustering with cross-view corresponding anchor alignment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical causal discovery from large-scale observed
variables. <em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3539788">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {It is a long-standing question to discover causal relations from observed variables in many empirical sciences. However, current causal discovery methods are inefficient when dealing with large-scale observed variables due to challenges in conditional independence (CI) tests or complex computations of acyclicity, and may even fail altogether. To address the efficiency issue in causal discovery from large-scale observed variables, we propose a Hierarchical Causal Discovery (HCD) framework with a bilevel policy that handles this issue by boosting existing models. Specifically, the high-level policy first finds a causal cut set to partition observed variables into several causal clusters and releases the clusters to the low-level policy. The low-level policy applies any causal discovery method to process these causal clusters in parallel and obtain intra-cluster structures for subsequently inter-cluster structure merging in the high-level policy. To avoid missing inter-cluster edges, we theoretically demonstrate the feasibility of causal cluster cut and inter-cluster structure merging. We also prove the completeness and correctness of HCD for causal discovery. Experiments on both synthetic and real-world datasets demonstrate that HCD consistently and significantly enhances the efficiency and effectiveness of existing advanced methods. The code is available at https://github.com/HITshenrj/HCD.},
  archive      = {J_TKDE},
  author       = {Rujia Shen and Muhan Li and Chao Zhao and Boran Wang and Yi Guan and Jie Liu and Jingchi Jiang},
  doi          = {10.1109/TKDE.2025.3539788},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Hierarchical causal discovery from large-scale observed variables},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hypergraph collaborative filtering with adaptive
augmentation of graph data for recommendation. <em>TKDE</em>, 1–13. (<a
href="https://doi.org/10.1109/TKDE.2025.3539769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Self-supervised tasks show significant advantages for node representation learning in recommender systems. This core idea of self-supervised task-based recommender systems depends on data augmentation to generate multi-view representations. However, there are two key challenges that are not well explored in existing self-supervised tasks: i) Restricted by the structure of the graph-based CF paradigm itself, the classical graph comparison learning architecture ignores the global structural information on the user-item interaction graph. ii) In a key part of existing contrast learning-random graph data enhancement schemes can significantly deteriorate model performance. To address these challenges, we propose a new hypergraph collaborative filtering with adaptive augmentation framework(HCFAA). It captures both local and global collaborative relationships on the user-item graph through a hypergraph-enhanced joint learning architecture. In particular, the designed adaptive structure-guided model ignores the noise introduced on unimportant edges, and thus learns the critical node information on the user-item graph. Comprehensive experimental studies on the Amazon dataset show that the method is effective, which provides an optimization scheme with a new perspective for the problems of key node loss in graph data enhancement and loss of higher-order structural information in GNN. The source code of our model can be available on https://github.com/RSnewbie/RS/tree/master/HCFAA.},
  archive      = {J_TKDE},
  author       = {Jian Wang and Jianrong Wang and Di Jin and Xinglong Chang},
  doi          = {10.1109/TKDE.2025.3539769},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Hypergraph collaborative filtering with adaptive augmentation of graph data for recommendation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing security and privacy in federated learning using
low-dimensional update representation and proximity-based defense.
<em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3539717">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) is a promising privacy-preserving machine learning paradigm that allows data owners to collaboratively train models while keeping their data localized. Despite its potential, FL faces challenges related to the trustworthiness of both clients and servers, particularly against curious or malicious adversaries. In this paper, we introduce a novel framework named Federated Learning with Low-Dimensional Update Representation and Proximity-Based defense (FLURP), designed to address privacy preservation and resistance to Byzantine attacks in distributed learning environments. FLURP employs $\mathsf {LinfSample}$ method, enabling clients to compute the $l_{\infty }$ norm across sliding windows of updates, resulting in a Low-Dimensional Update Representation (LUR). Calculating the shared distance matrix among LURs, rather than updates, significantly reduces the overhead of Secure Multi-Party Computation (SMPC) by three orders of magnitude while effectively distinguishing between benign and poisoned updates. Additionally, FLURP integrates a privacy-preserving proximity-based defense mechanism utilizing optimized SMPC protocols to minimize communication rounds. Our experiments demonstrate FLURP&#39;s effectiveness in countering Byzantine adversaries with low communication and runtime overhead. FLURP offers a scalable framework for secure and reliable FL in distributed environments, facilitating its application in scenarios requiring robust data management and security.},
  archive      = {J_TKDE},
  author       = {Wenjie Li and Kai Fan and Jingyuan Zhang and Hui Li and Wei Yang Bryan Lim and Qiang Yang},
  doi          = {10.1109/TKDE.2025.3539717},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Enhancing security and privacy in federated learning using low-dimensional update representation and proximity-based defense},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GAFExplainer: Global view explanation of graph neural
networks through attribute augmentation and fusion embedding.
<em>TKDE</em>, 1–16. (<a
href="https://doi.org/10.1109/TKDE.2025.3539989">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The excellent performance of graph neural networks (GNNs), which learn node representations by aggregating their neighborhood information, led to their use in various graph tasks. However, GNNs are black box models, the prediction results of which are difficult to understand directly. Although node attributes are vital for making predictions, previous studies have ignored their importance for explanation. This study presents GAFExplainer, a novel GNN explainer that emphasizes node attributes via attribute augmentation and fusion embedding. The former enhances node attribute encoding for more expressive masks, while the latter preserves the discrimination of node representations across different layers. Together, these modules significantly improve explanation performance. By training the explanatory network, a global view explanation of GNN models is obtained, and reasonably explainable subgraphs are available for new graphs, thus rendering the model well-generalizable. Multiple sets of experimental results on real and synthetic datasets demonstrate that the proposed model provides valid and accurate explanations. In the visual analysis, the explanations obtained by the proposed model are more comprehensible than those in existing work. Further, the fidelity evaluation and efficiency comparison reveal that with an average performance improvement of 8.9% compared with representative baselines, GAFExplainer achieves the best fidelity metrics while maintaining computational efficiency. Our code is available at https://github.com/wyhi/GAFExplainer.},
  archive      = {J_TKDE},
  author       = {Wenya Hu and Jia Wu and Quan Qian},
  doi          = {10.1109/TKDE.2025.3539989},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {GAFExplainer: Global view explanation of graph neural networks through attribute augmentation and fusion embedding},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive reliable defense graph for multi-channel robust
GCN. <em>TKDE</em>, 1–13. (<a
href="https://doi.org/10.1109/TKDE.2025.3538645">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Convolutional Networks (GCNs) have demonstrated remarkable success in various graph-related tasks. However, recent studies show that GCNs are vulnerable to adversarial attacks on graph structures. Therefore, how to defend against such attacks has become a popular research topic. The current common defense methods face two main limitations: (1) From the data perspective, it may lead to suboptimal results since the structural information is ignored when distinguishing the perturbed edges. (2) From the model perspective, the defenders rely on the low-pass filter of the GCN, which is vulnerable during message passing. To overcome these limitations, this paper analyzes the characteristics of perturbed edges, and based on this we propose a robust defense framework, REDE, to generate the adaptive Reliable Defense graph for multi-channel robust GCN. REDE first uses feature similarity and structure difference to discriminate perturbed edges and generates the defense graph by pruning them. Then REDE designs a multi-channel GCN, which can separately capture the information of different edges and high-order neighbors utilizing different frequency components. Leveraging this capability, the defense graph is adaptively updated at each layer, enhancing its reliability and improving prediction accuracy. Extensive experiments on four benchmark datasets demonstrate the enhanced performance and robustness of our proposed REDE over the state-of-the-art defense methods.},
  archive      = {J_TKDE},
  author       = {Xiao Zhang and Peng Bao},
  doi          = {10.1109/TKDE.2025.3538645},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Adaptive reliable defense graph for multi-channel robust GCN},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Acceleration algorithms in GNNs: A survey. <em>TKDE</em>,
1–20. (<a href="https://doi.org/10.1109/TKDE.2025.3540787">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Neural Networks have demonstrated remarkable effectiveness in various graph-based tasks, but their inefficiency in training and inference poses significant challenges for scaling to real-world, large-scale applications. To address these challenges, a plethora of algorithms have been developed to accelerate GNN training and inference, garnering substantial interest from the research community. This paper presents a systematic review of these acceleration algorithms, categorizing them into three main topics: training acceleration, inference acceleration, and execution acceleration. For training acceleration, we discuss techniques like graph sampling and GNN simplification. In inference acceleration, we focus on knowledge distillation, GNN quantization, and GNN pruning. For execution acceleration, we explore GNN binarization and graph condensation. Additionally, we review several libraries related to GNN acceleration, including our Scalable Graph Learning library, and propose future research directions. A comprehensive summary is available in our GitHub repository: https://github.com/PKU-DAIR/SGL/blob/main/Awsome-GNN-Acceleration.md.},
  archive      = {J_TKDE},
  author       = {Lu Ma and Zeang Sheng and Xunkai Li and Xinyi Gao and Zhezheng Hao and Ling Yang and Xiaonan Nie and Jiawei Jiang and Wentao Zhang and Bin Cui},
  doi          = {10.1109/TKDE.2025.3540787},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Acceleration algorithms in GNNs: A survey},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CoLLM: Integrating collaborative embeddings into large
language models for recommendation. <em>TKDE</em>, 1–12. (<a
href="https://doi.org/10.1109/TKDE.2025.3540912">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Leveraging Large Language Models as recommenders, referred to as LLMRec, is gaining traction and brings novel dynamics for modeling user preferences, particularly for cold-start users. However, existing LLMRec approaches primarily focus on text semantics and overlook the crucial aspect of incorporating collaborative information from user-item interactions, leading to potentially sub-optimal performance in warm-start scenarios. To ensure superior recommendations across both warm and cold scenarios, we introduce CoLLM, an innovative LLMRec approach that explicitly integrates collaborative information for recommendations. CoLLM treats collaborative information as a distinct modality, directly encoding it from well-established traditional collaborative models, and then tunes a mapping module to align this collaborative information with the LLM&#39;s input text token space for recommendations. By externally integrating traditional models, CoLLM ensures effective collaborative information modeling without modifying the LLM itself, providing the flexibility to adopt diverse collaborative information modeling mechanisms. Extensive experimentation validates that CoLLM adeptly integrates collaborative information into LLMs, resulting in enhanced recommendation performance. Our implementations are available in Github: https://github.com/zyang1580/CoLLM.},
  archive      = {J_TKDE},
  author       = {Yang Zhang and Fuli Feng and Jizhi Zhang and Keqin Bao and Qifan Wang and Xiangnan He},
  doi          = {10.1109/TKDE.2025.3540912},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {CoLLM: Integrating collaborative embeddings into large language models for recommendation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-view riemannian manifolds fusion enhancement for
knowledge graph completion. <em>TKDE</em>, 1–15. (<a
href="https://doi.org/10.1109/TKDE.2025.3538110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As the application of knowledge graphs becomes increasingly widespread, the issue of knowledge graph incompleteness has garnered significant attention. As a classical type of non-euclidean spatial data, knowledge graphs possess various complex structural types. However, most current knowledge graph completion models are developed within a single space, which makes it challenging to capture the inherent knowledge information embedded in the entire knowledge graph. This limitation hinders the representation learning capability of the models. To address this issue, this paper focuses on how to better extend the representation learning from a single space to Riemannian manifolds, which are capable of representing more complex structures. We propose a new knowledge graph completion model called MRME-KGC, based on multi-view Riemannian Manifolds fusion to achieve this. Specifically, MRME-KGC simultaneously considers the fusion of four views: two hyperbolic Riemannian spaces with negative curvature, a Euclidean Riemannian space with zero curvature, and a spherical Riemannian space with positive curvature to enhance knowledge graph modeling. Additionally, this paper proposes a contrastive learning method for Riemannian spaces to mitigate the noise and representation issues arising from Multi-view Riemannian Manifolds Fusion. This paper presents extensive experiments on MRME-KGC across multiple datasets. The results consistently demonstrate that MRME-KGC significantly outperforms current state-of-the-art models, achieving highly competitive performance even with low-dimensional embeddings.},
  archive      = {J_TKDE},
  author       = {Linyu Li and Zhi Jin and Xuan Zhang and Haoran Duan and Jishu Wang and Zhengwei Tao and Haiyan Zhao and Xiaofeng Zhu},
  doi          = {10.1109/TKDE.2025.3538110},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multi-view riemannian manifolds fusion enhancement for knowledge graph completion},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RAGIC: Risk-aware generative framework for stock interval
construction. <em>TKDE</em>, 1–12. (<a
href="https://doi.org/10.1109/TKDE.2025.3533492">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Efforts to predict stock market outcomes have yielded limited success due to the inherently stochastic nature of the market, influenced by numerous unpredictable factors. Many existing prediction approaches focus on single-point predictions, lacking the depth needed for effective decision-making and often overlooking market risk. To bridge this gap, we propose RAGIC, a novel risk-aware framework for stock interval prediction to quantify uncertainty. Our approach leverages a Generative Adversarial Network (GAN) to produce future price sequences infused with randomness inherent in financial markets. RAGIC&#39;s generator detects the risk perception of informed investors and captures historical price trends globally and locally. Then the risk-sensitive intervals is built upon the simulated future prices from sequence generation through statistical inference, incorporating horizon-wise insights. The interval&#39;s width is adaptively adjusted to reflect market volatility. Importantly, our approach relies solely on publicly available data and incurs only low computational overhead. RAGIC&#39;s evaluation across globally recognized broad-based indices demonstrates its balanced performance, offering both accuracy and informativeness. Achieving a consistent 95% coverage, RAGIC maintains a narrow interval width. This promising outcome suggests that our approach effectively addresses the challenges of stock market prediction while incorporating vital risk considerations.},
  archive      = {J_TKDE},
  author       = {Jingyi Gu and Wenlu Du and Guiling Wang},
  doi          = {10.1109/TKDE.2025.3533492},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {RAGIC: Risk-aware generative framework for stock interval construction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-state personalized knowledge tracing with emotional
incorporation. <em>TKDE</em>, 1–16. (<a
href="https://doi.org/10.1109/TKDE.2025.3538121">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge tracing has been widely used in online learning systems to guide the students&#39; future learning. However, most existing KT models primarily focus on extracting abundant information from the question sets and explore the relationships between them, but ignore the personalized student behavioral information in the learning process. This will limit the model&#39;s ability to accurately capture the personalized knowledge states of students and reasonably predict their performances. To alleviate this limitation, we explicitly models the personalized learning process by incorporating the emotions, a representative personalized behavior in the learning process, into KT framework. Specifically, we present a novel Dual-State Personalized Knowledge Tracing with Emotional Incorporation model to achieve this goal: Firstly, we incorporate emotional information into the modeling process of knowledge state, resulting in the Knowledge State Boosting Module. Secondly, we design an Emotional State Tracing Module to monitor students&#39; personalized emotional states, and propose an emotion prediction method based on personalized emotional states. Finally, we apply the predicted emotions to enhance students&#39; response prediction. Furthermore, to extend the generalization capability of our model across different datasets, we design a transferred version of DEKT, named Transfer Learning-based Self-loop model (T-DEKT). Extensive experiments show our method achieves the state-of-the-art performance. Our code is available at https://github.com/yfz-cloud/DEKT-project.},
  archive      = {J_TKDE},
  author       = {Shanshan Wang and Fangzheng Yuan and Keyang Wang and Xun Yang and Xingyi Zhang and Meng Wang},
  doi          = {10.1109/TKDE.2025.3538121},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Dual-state personalized knowledge tracing with emotional incorporation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning location-guided time-series shapelets.
<em>TKDE</em>, 1–15. (<a
href="https://doi.org/10.1109/TKDE.2025.3536462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Shapelets are interclass discriminative subsequences that can be used to characterize target classes. Learning shapelets by continuous optimization has recently been studied to improve classification accuracy. However, there are two issues in previous studies. First, since the locations where shapelets appear in the time series are determined by only their shapes, shapelets may appear at incorrect and non-discriminative locations in the time series, degrading the accuracy and interpretability. Second, the theoretical interpretation of learned shapelets has been limited to binary classification. To tackle the first issue, we propose a continuous optimization that learns not only shapelets but also their probable locations in a time series, and we show theoretically that this enhances feature discriminability. To tackle the second issue, we provide a theoretical interpretation of shapelet closeness to the time series for target / off-target classes when learning with softmax loss, which allows for multi-class classification. We demonstrate the effectiveness of the proposed method in terms of accuracy, runtime, and interpretability on the UCR archive},
  archive      = {J_TKDE},
  author       = {Akihiro Yamaguchi and Ken Ueno and Hisashi Kashima},
  doi          = {10.1109/TKDE.2025.3536462},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning location-guided time-series shapelets},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Finding rule-interpretable non-negative data representation.
<em>TKDE</em>, 1–12. (<a
href="https://doi.org/10.1109/TKDE.2025.3538327">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Non-negative Matrix Factorization (NMF) is an intensively used technique for obtaining parts-based, lower dimensional and non-negative representation. Researchers in biology, medicine, pharmacy and other fields often prefer NMF over other dimensionality reduction approaches (such as PCA) because the non-negativity of the approach naturally fits the characteristics of the domain problem and its results are easier to analyze and understand. Despite these advantages, obtaining exact characterization and interpretation of the NMF&#39;s latent factors can still be difficult due to their numerical nature. Rule-based approaches, such as rule mining, conceptual clustering, subgroup discovery and redescription mining, are often considered more interpretable but lack lower-dimensional representation of the data. We present a version of the NMF approach that merges rule-based descriptions with advantages of part-based representation offered by the NMF. Given the numerical input data with non-negative entries and a set of rules with high entity coverage, the approach creates the lower-dimensional non-negative representation of the input data in such a way that its factors are described by the appropriate subset of the input rules. In addition to revealing important attributes for latent factors, their interaction and value ranges, this approach allows performing focused embedding potentially using multiple overlapping target labels},
  archive      = {J_TKDE},
  author       = {Matej Mihelciˇc and Pauli Miettinen},
  doi          = {10.1109/TKDE.2025.3538327},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Finding rule-interpretable non-negative data representation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatio-temporal multivariate probabilistic modeling for
traffic prediction. <em>TKDE</em>, 1–15. (<a
href="https://doi.org/10.1109/TKDE.2025.3539680">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traffic prediction is an essential task in intelligent transportation systems dealing with complex and dynamic spatio-temporal correlations. To date, most work is focused on point estimation models, which only output a single value w.r.t an attribute of traffic data at a time, falling short of depicting diverse situations and uncertainty in future. Besides, most methods are not flexible enough to handle real complex traffic scenarios, involving missing values and non-uniformly sampled data. The interactions among different attributes of traffic data are also rarely explored explicitly. In this paper, we focus on probabilistic estimation in traffic prediction tasks, proposing a spatio-temporal multivariate probabilistic predictive model to estimate the distributions of traffic data. Specifically, we devise a multivariate spatio-temporal fusion graph block to extract spatio-temporal correlations of multiple traffic attributes at different locations. A multi-graph fusion module is designed to capture time-varying spatial relationships. We estimate the joint distributions of missing traffic data using copulas. The proposed model can simultaneously perform traffic forecasting and interpolation tasks with non-uniformly sampled data. Our experiments on two real-world traffic datasets demonstrate the advantages of our model over the state-of-the-art.},
  archive      = {J_TKDE},
  author       = {Yang An and Zhibin Li and Xiaoyu Li and Wei Liu and Xinghao Yang and Haoliang Sun and Meng Chen and Yu Zheng and Yongshun Gong},
  doi          = {10.1109/TKDE.2025.3539680},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Spatio-temporal multivariate probabilistic modeling for traffic prediction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Causal-TSF: A causal intervention approach to mitigate
confounding bias in time series forecasting. <em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3536107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series forecasting, aiming to learn models from historical data and predict future values in time series, is a fundamental research topic in machine learning. However, few efforts have been devoted to addressing the confounding effects in time series data, e.g., the historical data are affected by some hidden surrounding factors (i.e., confounders), leading to biased forecasting models for future data. This paper presents a causal intervention approach to eliminate the bias that is raised by some hidden confounders. By using a causal graph, we illustrate why hidden confounders can bring bias in time series forecasting and how to tackle it. We implement causal intervention by a deep architecture that consists of two modules, a Confounders Estimation module to estimate the hidden confounders and a Debiasing module to eliminate the confounding bias in the forecasting model via sampling on confounders. We conduct comprehensive evaluations on various time series datasets. The experiment results indicate that the proposed method can reduce the negative confounding effects in time series data, and it achieves superior gains over state-of-the-art baselines for time series forecasting.},
  archive      = {J_TKDE},
  author       = {Qinkang Gong and Yan Pan and Hanjiang Lai and Rongbang Qiu and Jian Yin},
  doi          = {10.1109/TKDE.2025.3536107},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Causal-TSF: A causal intervention approach to mitigate confounding bias in time series forecasting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A causal-based attribute selection strategy for
conversational recommender systems. <em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3543112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conversational recommender systems (CRSs) provide personalised recommendations by strategically querying attributes matching users&#39; preferences. However, this process suffers from confounding effects of time and user attributes, as users&#39; preferences naturally evolve over time and differ among similar users due to their unique attributes. These confounding effects distort user behaviors&#39; causal drivers, challenging CRSs in learning users&#39; true preferences and generalizable patterns. Recently, causal inference provides principled tools to clarify cause-effect relations in data, offering a promising way to address such confounding effects. In this context, we introduce Causal Conversational Recommender (CCR), which applies causal inference to model the causality between user behaviors and time/user attribute, enabling deeper understanding of user behaviors&#39; causal drivers. First, CCR employs stratification and matching to ensure attribute asked per round is independent from time and user attributes, mitigating their confounding effects. Following that, we apply the Average Treatment Effect (ATE) to quantify the unbiased causal impact of each unasked attribute on user preferences, identifying the attribute with the highest ATE per round as the causal-based attribute, i.e., causal driver of user behaviour. Finally, CCR iteratively refines user preferences through feedback on causal-based attributes. Extensive experiments verified CCR&#39;s robustness and personalization.},
  archive      = {J_TKDE},
  author       = {Dianer Yu and Qian Li and Xiangmeng Wang and Guandong Xu},
  doi          = {10.1109/TKDE.2025.3543112},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A causal-based attribute selection strategy for conversational recommender systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SCHENO: Measuring schema vs. Noise in graphs. <em>TKDE</em>,
1–12. (<a href="https://doi.org/10.1109/TKDE.2025.3543032">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-world data is typically a noisy manifestation of a core pattern (schema), and the purpose of data mining algorithms is to uncover that pattern, thereby splitting (i.e. decomposing) the data into schema and noise. We introduce SCHENO, a principled evaluation metric for the goodness of a schema-noise decomposition of a graph. SCHENO captures how schematic the schema is, how noisy the noise is, and how well the combination of the two represent the original graph data. We visually demonstrate what this metric prioritizes in small graphs, then show that if SCHENO is used as the fitness function for a simple optimization strategy, we can uncover a wide variety of patterns. Finally, we evaluate several well-known graph mining algorithms with this metric; we find that although they produce patterns, those patterns are not always the best representation of the input data.},
  archive      = {J_TKDE},
  author       = {Justus Isaiah Hibshman and Adnan Hoq and Tim Weninger},
  doi          = {10.1109/TKDE.2025.3543032},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {SCHENO: Measuring schema vs. noise in graphs},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph clustering with harmonic-maxmin cut guidance.
<em>TKDE</em>, 1–15. (<a
href="https://doi.org/10.1109/TKDE.2025.3542839">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph clustering has become a crucial technique for uncovering community structures in complex network data. However, existing approaches often introduce cumbersome regularization or constraints (hyperparameter tuning burden) to obtain balanced clustering results, thereby increasing hyperparameter tuning requirements and intermediate variables. These limitations can lead to suboptimal performance, particularly in scenarios involving imbalanced clusters or large-scale datasets. Besides, most graph cut clustering methods solve two separate discrete problems, resulting in information loss and relying on time-consuming eigen-decomposition. To address these challenges, this paper propose an effective graph cut framework, termed Harmonic MaxMin Cut (HMMC), inspired by worst-case objective optimization and the harmonic mean. Unlike traditional spectral clustering, HMMC produces all cluster assignments in a single step, eliminating the need for additional discretization and notably enhancing robustness to “worst-case cluster” boundaries. this paper further devise a fast coordinate descent (CD) solver that scales linearly complexity with the graph size, offering a computationally efficient alternative to eigen decomposition. Extensive experiments on real-world datasets demonstrate that HMMC is comparable to, or even surpasses, state-of-the-art methods, while also finding more favorable local solutions than non-negative matrix factorization techniques.},
  archive      = {J_TKDE},
  author       = {Jingwei Chen and Zihan Wu and Jingqing Cheng and Xiaohua Xu and Feiping Nie},
  doi          = {10.1109/TKDE.2025.3542839},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Graph clustering with harmonic-maxmin cut guidance},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Intent propagation contrastive collaborative filtering.
<em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3543241">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Disentanglement techniques used in collaborative filtering uncover interaction intents between nodes, improving the interpretability of node representations and enhancing recommendation performance. However, existing disentanglement methods still face the following two problems. (1) They focus on local structural features derived from direct node interactions, overlooking the comprehensive graph structure, which limits disentanglement accuracy. (2) The disentanglement process depends on backpropagation signals derived from recommendation tasks, lacking direct supervision, which may lead to biases and overfitting. To address the issues, we propose the Intent Propagation Contrastive Collaborative Filtering (IPCCF) algorithm. Specifically, we design a double helix message propagation framework to more effectively extract the deep semantic information of nodes, thereby improving the model&#39;s understanding of interactions between nodes. An intent message propagation method is also developed that incorporates graph structure information into the disentanglement process, thereby expanding the consideration scope of disentanglement. In addition, contrastive learning techniques are employed to align node representations derived from the structure and intents, providing direct supervision for the disentanglement process, mitigating biases, and enhancing the model&#39;s robustness to overfitting. The experiments on three real data graphs illustrate the superiority of the proposed approach.},
  archive      = {J_TKDE},
  author       = {Haojie Li and Junwei Du and Guanfeng Liu and Feng Jiang and Yan Wang and Xiaofang Zhou},
  doi          = {10.1109/TKDE.2025.3543241},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Intent propagation contrastive collaborative filtering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). QAEA-DR: A unified text augmentation framework for dense
retrieval. <em>TKDE</em>, 1–15. (<a
href="https://doi.org/10.1109/TKDE.2025.3543203">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In dense retrieval, embedding long texts into dense vectors can result in information loss, leading to inaccurate query-text matching. Additionally, low-quality texts with excessive noise or sparse key information are unlikely to align well with relevant queries. Recent studies mainly focus on improving the sentence embedding model or retrieval process. In this work, we introduce a novel text augmentation framework for dense retrieval. This framework transforms raw documents into information-dense text formats, which supplement the original texts to effectively address the aforementioned issues without modifying embedding or retrieval methodologies. Two text representations are generated via large language models (LLMs) zero-shot prompting: question-answer pairs and element-driven events. We term this approach QAEA-DR: unifying question-answer generation and event extraction in a text augmentation framework for dense retrieval. To further enhance the quality of generated texts, a scoring-based evaluation and regeneration mechanism is introduced in LLM prompting. Our QAEA-DR model has a positive impact on dense retrieval, supported by both theoretical analysis and empirical experiments.},
  archive      = {J_TKDE},
  author       = {Hongming Tan and Shaoxiong Zhan and Hai Lin and Hai-Tao Zheng and Wai Kin Chan},
  doi          = {10.1109/TKDE.2025.3543203},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {QAEA-DR: A unified text augmentation framework for dense retrieval},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Practical equi-join over encrypted database with reduced
leakage. <em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3543168">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Secure join schemes, an important class of queries over encrypted databases, have attracted increasing attention. While efficient querying is paramount, data owners also emphasize the significance of privacy preservation. The state-of-the-art JXT (Jutla and Patranabis ASIACRYPT 2022) enables efficient join queries over encrypted tables with a symmetric-key solution. However, we observe that JXT inadvertently leaks undesirable query results as the number of queries increases. In this paper, we propose a novel equi-join scheme, One-Time Join Cross-Tags (OTJXT), which can avoid additional result leakage in multiple queries and extend to equi-join as opposed to natural join in JXT. Specifically, we design a new data encoding method using nonlinear transformations that reveals only the union of results for each query without extra leakage observed in JXT. Moreover, OTJXT addresses the linear search complexity issue (Shafieinejad et al. ICDE 2022) while preventing multiple query leakage. Finally, we implement OTJXT and compare its performance with JXT and Shafieinejad et al.&#39;s scheme on the TPC-H dataset. The results show that OTJXT outperforms in search and storage efficiency, achieving a $\mathbf {98.5\times }$ (resp., $\mathbf {10^{6}\times }$) speedup in search latency and reducing storage cost by 62.5% (resp., 78.5%), compared to JXT (resp., Shafieinejad et al. &#39;s scheme). Using OTJXT, a TPC-H query on a 40 MB database only takes 21 ms.},
  archive      = {J_TKDE},
  author       = {Qiaoer Xu and Jianfeng Wang and Shi-Feng Sun and Zhipeng Liu and Xiaofeng Chen},
  doi          = {10.1109/TKDE.2025.3543168},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Practical equi-join over encrypted database with reduced leakage},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-graph interaction networks. <em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3543377">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) are recognized as a significant methodology for handling graph-structure data. However, with the increasing prevalence of learning scenarios involving multiple graphs, traditional GNNs mostly overlook the relationships between nodes across different graphs, mainly due to their limitation of traditional message passing within each graph. In this paper, we propose a novel GNN architecture called cross-graph interaction networks (GInterNet) to enable inter-graph message passing. Specifically, we develop a cross-graph topology construction module to uncover and learn the potential topologies between nodes across different graphs. Furthermore, we establish inter-graph message passing based on the learned cross-graph topologies, achieving cross-graph interaction by aggregating information from different graphs. Finally, we employ cross-graph construction functions involving the relationships between contextual information and cross-graph topology structure to iteratively update the cross-graph topologies. Different to existing related approaches, GInterNet is designed as a cross-graph interaction paradigm for inter-graph message passing. It enables multi-graph interaction during the message passing process. Additionally, it is a plug-and-play framework that can be easily embedded into other models. We evaluate its performance in semi-supervised and unsupervised learning scenarios involving multiple graphs. A detailed theoretical analysis and extensive experiment results have shown that GInterNet improves the performance and robustness of the base models. Our codes are available at: https://github.com/goalsetting/GInterNet.},
  archive      = {J_TKDE},
  author       = {Qihang Guo and Xibei Yang and Weiping Ding and Yuhua Qian},
  doi          = {10.1109/TKDE.2025.3543377},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Cross-graph interaction networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PipeOptim: Ensuring effective 1F1B schedule with
optimizer-dependent weight prediction. <em>TKDE</em>, 1–15. (<a
href="https://doi.org/10.1109/TKDE.2025.3543225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Asynchronous pipeline model parallelism with a “1F1B” (one forward, one backward) schedule generates little bubble overhead and always provides quite a high throughput. However, the “1F1B” schedule inevitably leads to weight inconsistency and weight staleness issues due to the cross-training of different mini-batches across GPUs. To simultaneously address these two problems, in this paper, we propose an optimizer-dependent weight prediction strategy (a.k.a PipeOptim) for asynchronous pipeline training. The key insight of our proposal is that we employ a weight prediction strategy in the forward pass to approximately ensure that each mini-batch uses consistent and staleness-free weights to compute the forward pass of the “1F1B” schedule. To be concrete, we first construct the weight prediction scheme based on the update rule of the used optimizer when training the deep neural network models. Then throughout the “1F1B” pipeline training, each mini-batch is mandated to execute weight prediction, subsequently employing the predicted weights to perform the forward pass. As a result, PipeOptim 1) inherits the advantage of the “1F1B” schedule and generates high throughput, and 2) can ensure effective parameter learning regardless of the type of the used optimizer. We conducted extensive experimental evaluations using nine different deep-learning models to verify the effectiveness of our proposal. The experiment results demonstrate that PipeOptim outperforms the other five popular pipeline approaches including GPipe, PipeDream, PipeDream-2BW, SpecTrain, and XPipe.},
  archive      = {J_TKDE},
  author       = {Lei Guan and Dongsheng Li and Yongle Chen and Jiye Liang and Wenjian Wang and Xicheng Lu},
  doi          = {10.1109/TKDE.2025.3543225},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {PipeOptim: Ensuring effective 1F1B schedule with optimizer-dependent weight prediction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Style feature extraction using contrastive conditioned
variational autoencoders with mutual information constraints.
<em>TKDE</em>, 1–16. (<a
href="https://doi.org/10.1109/TKDE.2025.3543383">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Extracting fine-grained features such as styles from unlabeled data is crucial for data analysis. Unsupervised methods such as variational autoencoders (VAEs) can extract styles that are usually mixed with other features. Conditional VAEs (CVAEs) can isolate styles using class labels; however, there are no established methods to extract only styles using unlabeled data. In this paper, we propose a CVAE-based method that extracts style features using only unlabeled data. The proposed model consists of a contrastive learning (CL) part that extracts style-independent features and a CVAE part that extracts style features. The CL model learns representations independent of data augmentation, which can be viewed as a perturbation in styles, in a self-supervised manner. Considering the style-independent features from the pretrained CL model as a condition, the CVAE learns to extract only styles. Additionally, we introduce a constraint based on mutual information between the CL and VAE features to prevent the CVAE from ignoring the condition. Experiments conducted using two simple datasets, MNIST and an original dataset based on Google Fonts, demonstrate that the proposed method can efficiently extract style features. Further experiments using real-world natural image datasets were also conducted to illustrate the method&#39;s extendability.},
  archive      = {J_TKDE},
  author       = {Suguru Yasutomi and Toshihisa Tanaka},
  doi          = {10.1109/TKDE.2025.3543383},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Style feature extraction using contrastive conditioned variational autoencoders with mutual information constraints},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diversity-promoting recommendation with dual-objective
optimization and dual consideration. <em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3543285">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Diversifying recommendations to broaden user horizons and explore potential interests has become a prominent research area in recommender systems. Although numerous efforts have been made to enhance diverse recommendations, the trade-off between diversity and accuracy remains a significant challenge. The primary causes lie in the following two aspects: (i) the inherent goals of diversity-promoting recommendation, which are to simultaneously deliver accurate recommendations and cater to a broader spectrum of users&#39; interests, have not been adequately explored; and (ii) considering diversity only in the model training procedure cannot guarantee the provision of diversification services in recommender systems. In this work, we directly formulate the inherent goals of diversity-promoting recommendation as a dual-objective optimization problem by simultaneously minimizing the recommendation error and maximizing diversity. These proposed objectives are integrated into Generative Adversarial Nets (GANs) to guide the training process toward the orientation of boosting both diversification and accuracy. Additionally, we propose considering diversity in both training and serving phases. Experimental results demonstrate that our model outperforms others in both diversity and relevance. We extend DDPR to state-of-the-art CTR and re-ranking models, which also result in improved performance on these tasks, further demonstrating the applicability of our model in real-world scenarios.},
  archive      = {J_TKDE},
  author       = {Yuli Liu and Yuan Zhang},
  doi          = {10.1109/TKDE.2025.3543285},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Diversity-promoting recommendation with dual-objective optimization and dual consideration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CMVC+: A multi-view clustering framework for open knowledge
base canonicalization via contrastive learning. <em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3543423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open information extraction (OIE) methods extract plenty of OIE triples from unstructured text, which compose large open knowledge bases (OKBs). Noun phrases and relation phrases in such OKBs are not canonicalized, which leads to scattered and redundant facts. It is found that two views of knowledge (i.e., a fact view based on the fact triple and a context view based on the fact triple&#39;s source context) provide complementary information that is vital to the task of OKB canonicalization, which clusters synonymous noun phrases and relation phrases into the same group and assigns them unique identifiers. In order to leverage these two views of knowledge jointly, we propose CMVC+, a novel unsupervised framework for canonicalizing OKBs without the need for manually annotated labels. Specifically, we propose a multi-view CHF K-Means clustering algorithm to mutually reinforce the clustering of view-specific embeddings learned from each view by considering the clustering quality in a fine-grained manner. Furthermore, we propose a novel contrastive learning module to refine the learned view-specific embeddings and further enhance the canonicalization performance. We demonstrate the superiority of our framework through extensive experiments on multiple real-world OKB data sets against state-of-the-art methods},
  archive      = {J_TKDE},
  author       = {Yang Yang and Wei Shen and Junfeng Shu and Yinan Liu and Edward Curry and Guoliang Li},
  doi          = {10.1109/TKDE.2025.3543423},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {CMVC+: A multi-view clustering framework for open knowledge base canonicalization via contrastive learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking variational bayes in community detection from
graph signal perspective. <em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3543378">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Methods based on variational bayes theory are widely used to detect community structures in networks. In recent years, many related methods have emerged that provide valuable insights into variational bayes theory. Remarkably, a fundamental assumption remains incomprehensible. Variational bayes-based methods typically employ a posterior distribution that follows a gaussian distribution to approximate the unknown prior distribution. However, the complexity and irregularity of node distributions in real-world networks prompt us to consider what characteristics of network information are suitable for the posterior distribution. Mathematically, inappropriate low- and high-frequency signals in expectation inference and variance inference can intensify the adverse effects of community distortion and ambiguity. To analysis these two phenomena and propose reasonable countermeasures, we conduct an empirical study. It is found that appropriately compressing low-frequency signals during expectation inference and amplifying high-frequency signals during variance inference are effective strategies. Based on these two strategies, this paper proposes a novel variational bayes plug-in, namely VBPG, to boost the performance of existing variational bayes-based community detection methods. Specifically, we modulate the frequency signals during expectation and variance inference to generate a new gaussian distribution. This strategy improves the fitting accuracy between the posterior distribution and the unknown true distribution without altering the modules of existing methods. The comprehensive experimental results validate that methods using VBPG achieve competitive performance improvements in most cases. The code is available at https://github.com/GDM-SCNU/VBPG.},
  archive      = {J_TKDE},
  author       = {Junwei Cheng and Yong Tang and Chaobo He and Pengxing Feng and Kunlin Han and Quanlong Guan},
  doi          = {10.1109/TKDE.2025.3543378},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Rethinking variational bayes in community detection from graph signal perspective},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Build a good human-free prompt tuning: Jointly pre-trained
template and verbalizer for few-shot classification. <em>TKDE</em>,
1–12. (<a href="https://doi.org/10.1109/TKDE.2025.3543422">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prompt tuning for pre-trained language models (PLMs) has been an effective approach for few-shot text classification. To make a prediction, a typical prompt tuning method employs a template wrapping the input text into a cloze question, and a verbalizer mapping the output embedding to labels. However, current methods typically depend on handcrafted templates and verbalizers, which require much domain-specific prior knowledge by human efforts. In this work, we investigate how to build a good human-free prompt tuning using soft prompt templates and soft verbalizers, which can be learned directly from data. To address the challenge of data scarcity, we integrate a set of trainable bases for sentence representation to transfer the contextual information into a low-dimensional space. By jointly pre-training the soft prompts and the bases using contrastive learning, the projection space can catch critical semantics at the sentence level, which could be transferred to various downstream tasks. To better bridge the gap between downstream tasks and the pre-training procedure, we formulate the few-shot classification tasks as another contrastive learning problem. We name this Jointly Pretrained Template and Verbalizer (JPTV). Extensive experiments show that this human-free prompt tuning can achieve comparable or even better performance than manual prompt tuning.},
  archive      = {J_TKDE},
  author       = {Mouxiang Chen and Han Fu and Chenghao Liu and Xiaoyun Joy Wang and Zhuo Li and Jianling Sun},
  doi          = {10.1109/TKDE.2025.3543422},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Build a good human-free prompt tuning: Jointly pre-trained template and verbalizer for few-shot classification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiscale temporal dynamic learning for time series
classification. <em>TKDE</em>, 1–13. (<a
href="https://doi.org/10.1109/TKDE.2025.3542799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series classification (TSC) is crucial in many applications, yet accurately modeling complex time series patterns remains challenging. Model-based TSC strives to aptly model time series by capturing their intrinsic temporal dynamics, deriving effective dynamic representations for classification. Despite significant progress in this domain, existing works are still constrained by a singular and overly simplistic modeling paradigm, which proves inadequate to handle the multiscale hierarchies inherent in time series. Additionally, the prevailing reliance on manual model configuration fails to address the diverse dynamic characteristics across varying data scenarios. In this paper, we amalgamate multiple recurrent reservoirs to devise a model-based Multiscale Temporal Dynamic Learning (MsDL) approach. These reservoirs are endowed with varied recurrent connection skips, ensuring a comprehensive capture of temporal dynamics across different timescales. We also present a multi-objective optimization algorithm, which adaptively configures the memory length of each reservoir, allowing for more accurate time series modeling. This optimization further encourages time series from the same class to look closer, while separating those from different classes, thereby enhancing the category-discriminability. Extensive experiments on public datasets demonstrate that MsDL outperforms the state-of-the-art methods. Additionally, ablation studies confirm that our multiscale design and optimization algorithm effectively enhance classification accuracy.},
  archive      = {J_TKDE},
  author       = {Shikang Liu and Xiren Zhou and Huanhuan Chen},
  doi          = {10.1109/TKDE.2025.3542799},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Multiscale temporal dynamic learning for time series classification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). One-step adaptive graph learning for incomplete multiview
subspace clustering. <em>TKDE</em>, 1–13. (<a
href="https://doi.org/10.1109/TKDE.2025.3543696">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incomplete multiview clustering (IMVC) optimally integrates complementary information within incomplete multiview data to improve clustering performance. Several one-step graph-based methods show great potential for IMVC. However, the low-rank structures of similarity graphs are neglected at the initialization stage of similarity graph construction. Moreover, further investigation into complementary information integration across incomplete multiple views is needed, particularly when considering the low-rank structures implied in high-dimensional multiview data. In this paper, we present one-step adaptive graph learning (OAGL) that adaptively performs spectral embedding fusion to achieve clustering assignments at the clustering indicator level. We first initiate affinity matrices corresponding to incomplete multiple views using spare representation under two constraints, i.e., the sparsity constraint on each affinity matrix corresponding to an incomplete view and the degree matrix of the affinity matrix approximating an identity matrix. This approach promotes exploring complementary information across incomplete multiple views. Subsequently, we perform an alignment of the spectral block-diagonal matrices among incomplete multiple views using low-rank tensor learning theory. This facilitates consistency information exploration across incomplete multiple views. Furthermore, we present an effective alternating iterative algorithm to solve the resulting optimization problem. Extensive experiments on benchmark datasets demonstrate that the proposed OAGL method outperforms several state-of-the-art approaches.},
  archive      = {J_TKDE},
  author       = {Jie Chen and Hua Mao and Wai Lok Woo and Chuanbin Liu and Zhu Wang and Xi Peng},
  doi          = {10.1109/TKDE.2025.3543696},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {One-step adaptive graph learning for incomplete multiview subspace clustering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). REP: An interpretable robustness enhanced plugin for
differentiable neural architecture search. <em>TKDE</em>, 1–15. (<a
href="https://doi.org/10.1109/TKDE.2025.3543503">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural architecture search (NAS) is widely used to automate the design of high-accuracy deep architectures, which are often vulnerable to adversarial attacks in practice due to the lack of adversarial robustness. Existing methods focus on the direct utilization of regularized optimization process to address this critical issue, which causes the lack of interpretability for the end users to learn how the robust architecture is constructed. In this paper, we introduce a robust enhanced plugin (REP) method for differentiable NAS to search for robust neural architectures. Different from existing peer methods, REP focuses on the robust search primitives in the search space of NAS methods, and naturally has the merit of contributing to understanding how the robust architectures are progressively constructed. Specifically, we first propose an effective sampling strategy to sample robust search primitives in the search space. In addition, we also propose a probabilistic enhancement method to guarantee natural accuracy and adversarial robustness simultaneously during the search process. We conduct experiments on both convolutional neural networks and graph neural networks with widely used benchmarks against state of the arts. The results reveal that REP can achieve superiority in terms of both the adversarial robustness to popular adversarial attacks and the natural accuracy of original data. REP is flexible and can be easily used by any existing differentiable NAS methods to enhance their robustness without much additional effort. The source code is available at https://github.com/fyqsama/REP.},
  archive      = {J_TKDE},
  author       = {Yuqi Feng and Yanan Sun and Gary G. Yen and Kay Chen Tan},
  doi          = {10.1109/TKDE.2025.3543503},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {REP: An interpretable robustness enhanced plugin for differentiable neural architecture search},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). <span class="math inline"><em>k</em></span>-graph: A graph
embedding for interpretable time series clustering. <em>TKDE</em>, 1–14.
(<a href="https://doi.org/10.1109/TKDE.2025.3543946">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series clustering poses a significant challenge with diverse applications across domains. A prominent drawback of existing solutions lies in their limited interpretability, often confined to presenting users with centroids. In addressing this gap, our work presents $k$-Graph, an unsupervised method explicitly crafted to augment interpretability in time series clustering. Leveraging a graph representation of time series subsequences, $k$-Graph constructs multiple graph representations based on different subsequence lengths. This feature accommodates variable-length time series without requiring users to predetermine subsequence lengths. Our experimental results reveal that $k$-Graph outperforms current state-of-the-art time series clustering algorithms in accuracy, while providing users with meaningful explanations and interpretations of the clustering outcomes.},
  archive      = {J_TKDE},
  author       = {Paul Boniol and Donato Tiano and Angela Bonifati and Themis Palpanas},
  doi          = {10.1109/TKDE.2025.3543946},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {$k$-graph: A graph embedding for interpretable time series clustering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing attribute-driven fraud detection with risk-aware
graph representation. <em>TKDE</em>, 1–12. (<a
href="https://doi.org/10.1109/TKDE.2025.3543887">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Credit card fraud is a severe issue that causes significant losses for both cardholders and issuing banks. Existing methods utilize machine learning-based classifiers to identify fraudulent transactions from labeled transaction records. However, labeled data are often scarce compared to the billions of real transactions due to the high cost of annotation, which means that previous methods do not fully utilize the rich features of unlabeled data. What&#39;s more, contemporary methods succumb to a fallacy of unawareness of the local risk structure and the inability to capture certain risk patterns. Therefore, we propose the Risk-aware Gated Temporal Attention Network (RGTAN) for fraud detection in this work. Specifically, we first build a temporal transaction graph based on the transaction records, which consists of temporal transactions (nodes) and their interactions (edges). Then we leverage a Gated Temporal Graph Attention (GTGA) Mechanism to propagate messages among the nodes and learn adaptive representations of transactions. We also model the fraud patterns through risk propagation, taking advantage of the relations among transactions. More importantly, we devise a neighbor risk-aware representation learning layer to enhance our method&#39;s perception of multi-hop risk structures. We conduct extensive experiments on a real-world credit card transaction dataset and two public fraud detection datasets. The results show that our proposed method, RGTAN, outperforms other state-of-the-art methods on three fraud detection datasets. The risk-aware semi-supervised experiments also demonstrate the excellent performance of our model with only a small fraction of manually labeled data. Moreover, RGTAN has been deployed in a world-leading credit card issuer for credit card fraud detection, and the case study results show the effectiveness of our method in uncovering real-world fraud patterns.},
  archive      = {J_TKDE},
  author       = {Sheng Xiang and Guibin Zhang and Dawei Cheng and Ying Zhang},
  doi          = {10.1109/TKDE.2025.3543887},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Enhancing attribute-driven fraud detection with risk-aware graph representation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scalable min-max multi-view spectral clustering.
<em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3543817">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view spectral clustering has attracted considerable attention since it can explore common geometric structures from diverse views. Nevertheless, existing min-min framework-based models adopt internal minimization to find the view combination with the minimized within-cluster variance, which will lead to effectiveness loss since the real clusters often exhibit high within-cluster variance. To address this issue, we provide a novel scalable min-max multi-view spectral clustering (SMMSC) model to improve clustering performance. Besides, anchor graphs, rather than full sample graphs, are utilized to reduce the computational complexity of graph construction and singular value decomposition, thereby enhancing the applicability of SMMSC to large-scale applications. Then, we rewrite the min-max model as a minimized optimal value function, demonstrate its differentiability, and develop an efficient gradient descent-based algorithm to optimize it with linear computational complexity. Moreover, we demonstrate that the resultant solution of the proposed algorithm is the global optimum. Numerous experiments on different real-world datasets, including some large-scale datasets, demonstrate that SMMSC outperforms existing state-of-the-art multi-view clustering methods regarding clustering performance.},
  archive      = {J_TKDE},
  author       = {Ben Yang and Xuetao Zhang and Jinghan Wu and Feiping Nie and Fei Wang and Badong Chen},
  doi          = {10.1109/TKDE.2025.3543817},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Scalable min-max multi-view spectral clustering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Topic videolization: A rumor detection method inspired by
video forgery detection technology. <em>TKDE</em>, 1–13. (<a
href="https://doi.org/10.1109/TKDE.2025.3543852">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study was inspired by video forgery detection techniques. If the topic space at a certain time is considered as a frame image, the consecutive frame images over time could be viewed as a video. Then the rumor topic detection problem is transformed into a topic video forgery detection problem. Thus, a novel rumor detection method was proposed. Firstly, a Topic2RGB algorithm was proposed to convert comment users into pixel points. The algorithm views commenting users as pixel points while using game theory to mine user pro-opposition emotions as RGB information. Secondly, a Topic2Video algorithm was proposed to convert the topic space into video. The algorithm converts the topic space into frame images. Meanwhile, the topic space is time-sliced, then the topic space is transformed into a video. Finally, the volatility of user emotional confrontation during a long time in the topic space is like the change of characteristics of frame images in forgeries videos. Then, a topic video rumor detection method (TVRD) was proposed. The experiments indicate that the method successfully verifies the viability of the topic videolization for rumor detection. Additionally, the method also demonstrates the effectiveness of user emotion confrontation of topic space on detection performance.},
  archive      = {J_TKDE},
  author       = {Yucai Pang and Zhou Yang and Qian Li and Shihong Wei and Yunpeng Xiao},
  doi          = {10.1109/TKDE.2025.3543852},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Topic videolization: A rumor detection method inspired by video forgery detection technology},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Few-shot knowledge graph completion with star and ring
topology information aggregation. <em>TKDE</em>, 1–13. (<a
href="https://doi.org/10.1109/TKDE.2025.3544202">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot knowledge graph completion (FKGC) addresses the long-tail problem of relations by leveraging a few observed support entity pairs to infer unknown facts for tail-located relations. Learning the relation representation of entity pairs and evaluating the match of query and support entity pairs are the two key steps of FKGC. Existing methods learn the representation of entity pairs by either aggregating neighbors of entities or integrating relation representations in the connected paths from head to tail. However, in few-shot scenarios, the limited number of support entity pairs and insufficient structural information with a single neighborhood topology will lead to matching failure. To this end, we consider the star and ring topological information for a given entity pair: (1) Entity neighborhood, which captures multi-hop neighbors of entities; (2) Relational path, which characterizes compound relation forms. Furthermore, to effectively fuse the two kinds of heterogeneous topological information, we design the multi-aggregator and the fine-grained path correlation matching algorithm to obtain more delicate and balanced matching. Based on the proposed relational path correlation matching module, we propose the relation adaptive network to solve the few-shot temporal knowledge graph completion problem. The experimental results show that our method continuously outperforms the state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Jing Zhao and Xinzhu Zhang and Yujia Li and Shiliang Sun},
  doi          = {10.1109/TKDE.2025.3544202},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Few-shot knowledge graph completion with star and ring topology information aggregation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transfer-and-fusion: Integrated link prediction across
knowledge graphs. <em>TKDE</em>, 1–13. (<a
href="https://doi.org/10.1109/TKDE.2025.3544255">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing work on knowledge graph (KG) link prediction has primarily focused on a single KG. However, a single KG is often limited by its incompleteness, encompassing missing facts, entities, and relations. This limitation subsequently restricts the practicality, as it cannot handle the queries that involve missing entities or relations within the single KG. In this paper, we explore an extended link prediction task, cross-KG link prediction, which answers queries using entities or relations integrated from other KGs. The crux of this problem is transferring knowledge across KGs and fusing their embedding spaces, which possess varying schemata. We develop a relation prototype graph to model the interactions among relations from different KGs. Based on this graph, we first propose a dual-view embedding learning module to fuse embedding spaces by training with instance facts and relation prototype edges. We then introduce an attention mechanism to highlight pivotal information for specific queries, recognizing that different KGs often emphasize various domains. Moreover, we devise an augmentation strategy to generate pseudo-cross-KG facts, facilitating knowledge transfer across KGs. Using four widely-used KGs, we construct two cross-KG link prediction datasets. Extensive experimental results demonstrate the superiority of our model and the unique contributions of each module.},
  archive      = {J_TKDE},
  author       = {Yuanning Cui and Zequn Sun and Wei Hu},
  doi          = {10.1109/TKDE.2025.3544255},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Transfer-and-fusion: Integrated link prediction across knowledge graphs},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Probabilistic learning of multivariate time series with
temporal irregularity. <em>TKDE</em>, 1–15. (<a
href="https://doi.org/10.1109/TKDE.2025.3544348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Probabilistic forecasting of multivariate time series is essential for various downstream tasks. Most existing approaches rely on the sequences being uniformly spaced and aligned across all variables. However, real-world multivariate time series often suffer from temporal irregularities, including nonuniform intervals and misaligned variables, which pose significant challenges for accurate forecasting. To address these challenges, we propose an end-to-end framework that models temporal irregularities while capturing the joint distribution of variables at arbitrary continuous-time points. Specifically, we introduce a dynamic conditional continuous normalizing flow to model data distributions in a non-parametric manner, accommodating the complex, non-Gaussian characteristics commonly found in real-world datasets. Then, by leveraging a carefully factorized log-likelihood objective, our approach captures both temporal and cross-sectional dependencies efficiently. Extensive experiments on a range of real-world datasets demonstrate the superiority and adaptability of our method compared to existing approaches. The data and code supporting this work are available at https://github.com/lyjsilence/RFN.},
  archive      = {J_TKDE},
  author       = {Yijun LI and Cheuk Hang Leung and Qi Wu},
  doi          = {10.1109/TKDE.2025.3544348},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Probabilistic learning of multivariate time series with temporal irregularity},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MTD-DS: An SLA-aware decision support benchmark for
multi-tenant parallel DBMSs. <em>TKDE</em>, 1–12. (<a
href="https://doi.org/10.1109/TKDE.2025.3543727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-tenant DBMSs are used by cloud providers for their Database-as-a-Service products. They could be single-node DBMSs installed in virtual machines, SQL-on-Hadoop systems or classic parallel relational DBMSs running on top of a shared-nothing or shared-disk architecture. For a cloud provider, it is interesting to measure these systems&#39; capability of dealing with multi-tenant workloads, i.e., taking advantage of the statistical multiplexing to obtain economic gain while being attractive by providing a good quality of service and a low bill to the tenants. In this paper, we present MTD-DS benchmark (with MTD for Multi-Tenant parallel DBMSs and DS for Decision Support). MTD-DS extends TPC-DS by adding a multi-tenant query workload generator, a performance Service Level Objectives generator, configurable Database-as-a-Service pricing models, and new metrics to measure the potential capability of a multi-tenant parallel DBMS in obtaining the best trade-off between the provider&#39;s benefit and the tenants&#39; satisfaction. Example experimental results have been produced to show the relevance and the feasibility of the MTD-DS benchmark.},
  archive      = {J_TKDE},
  author       = {Shaoyi Yin and Franck Morvan and Jorge Martinez-Gil and Abdelkader Hameurlain},
  doi          = {10.1109/TKDE.2025.3543727},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {MTD-DS: An SLA-aware decision support benchmark for multi-tenant parallel DBMSs},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Model-agnostic dual-side online fairness learning for
dynamic recommendation. <em>TKDE</em>, 1–16. (<a
href="https://doi.org/10.1109/TKDE.2025.3544510">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fairness in recommendation has drawn much attention since it significantly affects how users access information and how information is exposed to users. However, most fairness-aware methods are designed offline with the entire stationary interaction data to handle the global unfairness issue and evaluate their performance in a one-time paradigm. In real-world scenarios, users tend to interact with items continuously over time, leading to a dynamic recommendation environment where unfairness is evolving online. Moreover, previous methods that focus on mitigating the unfairness can hardly bring significant improvements to the recommendation task. Hence, in this paper, we propose a Model-agnostic Dual-side Online Fairness Learning method (MDOFair) for the dynamic recommendation. First, we carefully design dynamic dual-side fairness learning to trace the rapid evolution of unfairness from both the user and item sides. Second, we leverage the fairness and recommendation tasks in one utilized framework to pursue the double-win success. Last, we present an efficient model-agnostic post-ranking method for the dynamic recommendation scenario to mitigate the dynamic unfairness while improving the recommendation performance significantly. Extensive experiments demonstrate the superiority and effectiveness of our proposed MDOFair by incorporating it into existing dynamic models as a post-ranking stage},
  archive      = {J_TKDE},
  author       = {Haoran Tang and Shiqing Wu and Zhihong Cui and Yicong Li and Guandong Xu and Qing Li},
  doi          = {10.1109/TKDE.2025.3544510},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Model-agnostic dual-side online fairness learning for dynamic recommendation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient PMU data compression using enhanced graph
filtering enabled principal component analysis. <em>TKDE</em>, 1–13. (<a
href="https://doi.org/10.1109/TKDE.2025.3544768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Phasor Measurement Units (PMUs) are state-of-the-art measuring devices that capture high-resolution time-synchronized voltage and current phasor measurements in wide area monitoring systems (WAMS). Their usage for various real-time applications demands a huge amount of data collected from multiple PMUs to be transmitted from the local phasor data concentrator (PDC) to the control centre. To optimize the requirements of bandwidth to transmit the data as well as to store the data, an efficient synchrophasor data compression technique is desired. To this end, this paper presents a 3-stage data compression scheme in which Stage-1 performs the accumulation of the data matrix from the optimally placed PMUs in WAMS into the local PDC. The data is then passed through a novel Ramanujan&#39;s sum-based fault window detection algorithm to identify the fault within the PMU data matrix in Stage-2. Finally, Stage-3 proposes an enhanced graph filtering-enabled principal component analysis scheme which expands the notion of conventional PCA techniques into the graph domain to compress the data. The performance of the proposed scheme is verified on the IEEE 14-bus system and New England 39-bus system. Further, practical applicability of the proposed method is validated on field PMU data collected from EPFL campus in Switzerland.},
  archive      = {J_TKDE},
  author       = {Manish Pandit and Ranjana Sodhi},
  doi          = {10.1109/TKDE.2025.3544768},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient PMU data compression using enhanced graph filtering enabled principal component analysis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An amortized o(1) lower bound for dynamic time warping in
motif discovery. <em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3544751">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Motif discovery is a critical operation for analyzing series data in many applications. Recent works demonstrate the importance of finding motifs with Dynamic Time Warping. However, existing algorithms spend most of their time in computing lower bounds of Dynamic Time Warping to filter out the unpromising candidates. Specifically, the time complexity for computing these lower bounds is $O(L)$ for each pair of subsequences, where $L$ is the length of the motif (subsequences). This paper proposes two new lower bounds, called $LB_{f}$ and $LB_{M}$, both of them only cost amortized $O(1)$ time for each pair of subsequences. On real datasets, the proposed lower bounds are at least one magnitude faster than the state-of-the-art lower bounds used in motif discovery while still keeping satisfying effectiveness. Based on these faster lower bounds, this paper designs an efficient motif discovery algorithm that significantly reduces the cost of lower bounds. The experiments conducted on real datasets show the proposed algorithm is 5.6 times faster than the state-of-the-art algorithms on average.},
  archive      = {J_TKDE},
  author       = {Zemin Chao and Hong Gao and Dongjing Miao and Jianzhong Li and Hongzhi Wang},
  doi          = {10.1109/TKDE.2025.3544751},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {An amortized o(1) lower bound for dynamic time warping in motif discovery},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Provenance graph kernel. <em>TKDE</em>, 1–16. (<a
href="https://doi.org/10.1109/TKDE.2025.3543097">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Provenance is a standardised record that describes how entities, activities, and agents have influenced a piece of data; it is commonly represented as graphs with relevant labels on both their nodes and edges. With the growing adoption of provenance in a wide range of application domains, users are increasingly confronted with an abundance of graph data, which may prove challenging to process. Graph kernels, on the other hand, have been successfully used to efficiently analyse graphs. In this paper, we introduce a novel graph kernel called provenance kernel, which is inspired by and tailored for provenance data. We employ provenance kernels to classify provenance graphs from three application domains. Our evaluation shows that they perform well in terms of classification accuracy and yield competitive results when compared against existing graph kernel methods and the provenance network analytics method while more efficient in computing time. Moreover, the provenance types used by provenance kernels are a symbolic representation of a tree pattern which can, in turn, be described using the domain-agnostic vocabulary of provenance. Therefore, provenance types thus allow for the creation of explanations of predictive models built on them.},
  archive      = {J_TKDE},
  author       = {David Kohan Marzagão and Trung Dong Huynh and Ayah Helal and Sean Baccas and Luc Moreau},
  doi          = {10.1109/TKDE.2025.3543097},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Provenance graph kernel},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A novel expandable borderline smote oversampling method for
class imbalance problem. <em>TKDE</em>, 1–19. (<a
href="https://doi.org/10.1109/TKDE.2025.3544284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The class imbalance problem can cause classifiers to be biased toward the majority class and inclined to generate incorrect predictions. While existing studies have proposed numerous oversampling methods to alleviate class imbalance by generating extra minority class samples, these methods still have some inherent weaknesses and make the generated samples less informative. This study proposes a novel over-sampling method named the Expandable Borderline Smote (EB-Smote), which can address the weaknesses of existing over-sampling methods and generate more informative synthetic samples. In EB-Smote, not only minority class but also majority class is oversampled, and the synthetic samples are generated in the area between the selected minority and majority samples, which are close to the borderlines of their respective classes. EB-Smote can generate more informative samples by expanding the borderlines of minority and majority classes toward the actual decision boundary. Based on 27 imbalanced datasets and commonly used machine learning models, the experimental results demonstrate that EB-Smote significantly outperforms the other 8 existing oversampling methods. This study can provide theoretical guidance and practical recommendations to solve the crucial class imbalance problem in classification tasks.},
  archive      = {J_TKDE},
  author       = {Hao Sun and Jianping Li and Xiaoqian Zhu},
  doi          = {10.1109/TKDE.2025.3544284},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-19},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A novel expandable borderline smote oversampling method for class imbalance problem},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Snoopy: Effective and efficient semantic join discovery via
proxy columns. <em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3545176">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic join discovery, which aims to find columns in a table repository with high semantic joinabilities to a query column, is crucial for dataset discovery. Existing methods can be divided into two categories: cell-level methods and column-level methods. However, neither of them ensures both effectiveness and efficiency simultaneously. Cell-level methods, which compute the joinability by counting cell matches between columns, enjoy ideal effectiveness but suffer poor efficiency. In contrast, column-level methods, which determine joinability only by computing the similarity of column embeddings, enjoy proper efficiency but suffer poor effectiveness due to the issues occurring in their column embeddings: (i) semantics-joinability-gap, (ii) size limit, and (iii) permutation sensitivity. To address these issues, this paper proposes to compute column embeddings via proxy columns; furthermore, a novel column-level semantic join discovery framework, ${\sf Snoopy}$, is presented, leveraging proxy-column-based embeddings to bridge effectiveness and efficiency. Specifically, the proposed column embeddings are derived from the implicit column-to-proxy-column relationships, which are captured by the lightweight approximate-graph-matching-based column projection. To acquire good proxy columns for guiding the column projection, we introduce a rank-aware contrastive learning paradigm. Extensive experiments on four real-world datasets demonstrate that ${\sf Snoopy}$ outperforms SOTA column-level methods by 16% in Recall@25 and 10% in NDCG@25, and achieves superior efficiency-being at least 5 orders of magnitude faster than cell-level solutions, and 3.5× faster than existing column-level methods.},
  archive      = {J_TKDE},
  author       = {Yuxiang Guo and Yuren Mao and Zhonghao Hu and Lu Chen and Yunjun Gao},
  doi          = {10.1109/TKDE.2025.3545176},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Snoopy: Effective and efficient semantic join discovery via proxy columns},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SPIN: Sparse portfolio strategy with irregular news in
fluctuating markets. <em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3545115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The sparse portfolio optimization (SPO) problem is increasingly crucial in portfolio management, focusing on selecting a few stocks with the potential for strong market performance. However, sparse portfolio strategies often face significant short-term drawdowns during periods of market volatility. To this end, a news-driven portfolio strategy offers valuable insights to capture sudden market changes. Nevertheless, it encounters two main challenges: how to reasonably map the relationships between news and stocks and how to effectively utilize the irregular timing of news releases. To tackle the SPO problem in fluctuating markets while addressing these challenges, we propose a novel news-driven sparse portfolio strategy, named SPIN. Specifically, SPIN not only leverages industry-specific group structures existing among stocks for a more reasonable news-stock mapping and models news sequential patterns based on our devised novel news-driven forecaster to handle the irregularity of news releases. We rigorously prove that SPIN achieves a sub-linear regret. Extensive experiments on three real-world datasets demonstrate SPIN&#39;s superiority over state-of-the-art portfolio strategies in terms of cumulative wealth and short-term drawdowns.},
  archive      = {J_TKDE},
  author       = {Mengying Zhu and Mengyuan Yang and Yan Wang and Fei Wu and Qianqiao Liang and Chaochao Chen and Hua Wei and Xiaolin Zheng},
  doi          = {10.1109/TKDE.2025.3545115},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {SPIN: Sparse portfolio strategy with irregular news in fluctuating markets},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CGoFed: Constrained gradient optimization strategy for
federated class incremental learning. <em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3544605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Class Incremental Learning (FCIL) has emerged as a new paradigm due to its applicability in real-world scenarios. In FCIL, clients continuously generate new data with unseen class labels and do not share local data due to privacy restrictions, and each client&#39;s class distribution evolves dynamically and independently. However, existing work still faces two significant challenges. Firstly, current methods lack a better balance between maintaining sound anti-forgetting effects over old data (stability) and ensuring good adaptability for new tasks (plasticity). Secondly, some FCIL methods overlook that the incremental data will also have a non-identical label distribution, leading to poor performance. This paper proposes CGoFed, which includes relax-constrained gradient update and cross-task gradient regularization modules. The relax-constrained gradient update prevents forgetting the knowledge about old data while quickly adapting to the new data by constraining the gradient update direction to a gradient space that minimizes interference with historical tasks. The cross-task gradient regularization also finds applicable historical models from other clients and trains a personalized global model to address the non-identical label distribution problem. The results demonstrate that the CGoFed performs well in alleviating catastrophic forgetting and improves model performance by 8%-23% compared with the SOTA comparison method.},
  archive      = {J_TKDE},
  author       = {Jiyuan Feng and Xu Yang and Liwen Liang and Weihong Han and Binxing Fang and Qing Liao},
  doi          = {10.1109/TKDE.2025.3544605},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {CGoFed: Constrained gradient optimization strategy for federated class incremental learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Doing more with less: A survey of data selection methods for
mathematical modeling. <em>TKDE</em>, 1–20. (<a
href="https://doi.org/10.1109/TKDE.2025.3545965">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Big data applications such as Artificial Intelligence (AI) and Internet of Things (IoT) have in recent years been leading to many technological breakthroughs in system modeling. However, these applications are typically data intensive, thus requiring an increasing cost of resources. In this paper, a first-of-its-kind comprehensive review of data selection methods across different engineering disciplines is given in order to analyze the effectiveness of these methods in improving the data efficiency of mathematical modeling algorithms. Eight distinct selection methods have been identified and subsequently analyzed and discussed on the basis of the relevant literature. In addition, the selection methods have been classified according to three dichotomies established by the survey. A comparative analysis of these methods was conducted along with a discussion of potentials, challenges, and future research directions for the research area. Data selection was found to be widely used in many engineering applications and has the potential to play an important role in making more sustainable Big Data applications, especially those in which transmission of data across large distances is required. Furthermore, making resource-aware decisions about the use of data has been shown to be highly effective in reducing energy costs while ensuring high performance of the model},
  archive      = {J_TKDE},
  author       = {Nicolai A. Weinreich and Arman Oshnoei and Remus Teodorescu and Kim G. Larsen},
  doi          = {10.1109/TKDE.2025.3545965},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Doing more with less: A survey of data selection methods for mathematical modeling},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving sequential recommendations via bidirectional
temporal data augmentation with pre-training. <em>TKDE</em>, 1–13. (<a
href="https://doi.org/10.1109/TKDE.2025.3546035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sequential recommendation systems are integral to discerning temporal user preferences. Yet, the task of learning from abbreviated user interaction sequences poses a notable challenge. Data augmentation has been identified as a potent strategy to enhance the informational richness of these sequences. Traditional augmentation techniques, such as item randomization, may disrupt the inherent temporal dynamics. Although recent advancements in reverse chronological pseudo-item generation have shown promise, they can introduce temporal discrepancies when assessed in a natural chronological context. In response, we introduce a sophisticated approach, Bidirectional temporal data Augmentation with pre-training (BARec). Our approach leverages bidirectional temporal augmentation and knowledge-enhanced fine-tuning to synthesize authentic pseudo-prior items that retain user preferences and capture deeper item semantic correlations, thus boosting the model&#39;s expressive power. Our comprehensive experimental analysis on five benchmark datasets confirms the superiority of BARec across both short and elongated sequence contexts. Moreover, theoretical examination and case study offer further insight into the model&#39;s logical processes and interpretability. The source code for our study is publicly available at https://github.com/juyongjiang/BARec.},
  archive      = {J_TKDE},
  author       = {Juyong Jiang and Peiyan Zhang and Yingtao Luo and Chaozhuo Li and Jae Boum Kim and Kai Zhang and Senzhang Wang and Sunghun Kim and Philip S. Yu},
  doi          = {10.1109/TKDE.2025.3546035},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Improving sequential recommendations via bidirectional temporal data augmentation with pre-training},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Next-POI recommendation via spatial-temporal knowledge graph
contrastive learning and trajectory prompt. <em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3545958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Next POI (Point-of-Interest) recommendation aims to forecast users&#39; future movements based on their historical check-in trajectories, holding significant value in location-based services. Existing methods address trajectory data sparsity by integrating rich auxiliary information or using spatial-temporal knowledge graphs (STKGs), showing promising results. Yet, they face two main challenges: i) Due to the difficulty of transforming structured trajectory data into trajectory text describing users&#39; spatial-temporal mobility, the powerful reasoning ability of pre-trained language models is rarely explored to enhance recommendation performance. ii) Methods based on STKG can introduce external knowledge inconsistent with user preferences, leading to the knowledge noise generated hampering the accuracy of recommendations. To this end, we propose a novel approach called STKG-PLM that integrates STKG contrastive learning and prompt pre-trained language model (PLM) to enhance the next POI recommendation. Specifically, we design a spatial-temporal trajectory prompt template that transforms structured trajectories into text corpus based on STKG, serving as the input of PLM to understand the movement pattern of users from coarse-grained and fine-grained perspectives. Additionally, we propose an STKG contrastive learning framework to mitigate the introduced knowledge noise. Extensive experiments on three real-world datasets demonstrate that STKG-PLM exhibits notable performance improvements over the state-of-the-art baseline methods.},
  archive      = {J_TKDE},
  author       = {Wei Chen and Haoyu Huang and Zhiyu Zhang and Tianyi Wang and Youfang Lin and Liang Chang and Huaiyu Wan},
  doi          = {10.1109/TKDE.2025.3545958},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Next-POI recommendation via spatial-temporal knowledge graph contrastive learning and trajectory prompt},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Estimating multi-label expected accuracy using labelset
distributions. <em>TKDE</em>, 1–12. (<a
href="https://doi.org/10.1109/TKDE.2025.3545972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A multi-label classifier estimates the binary label state (relevant/irrelevant) for each of a set of concept labels, for a given instance. Probabilistic multi-label classifiers provide a distribution over all possible labelset combinations of such label states (the powerset of labels), from which we can provide the best estimate by selecting the labelset corresponding to the largest expected accuracy. Providing confidence for predictions is important for real-world application of multi-label models, which provides the practitioner with a sense of the correctness of the prediction. It has been thought that the probability of the chosen labelset is a good measure of the confidence of the prediction, but multi-label accuracy can be measured in many ways and so confidence should align with the expected accuracy of the evaluation method. In this article, we investigate the effectiveness of seven candidate functions for estimating multi-label expected accuracy conditioned on the labelset distribution and the evaluation method. We found most correlate to expected accuracy and have varying levels of robustness. Further, we found that the candidate functions provide high expected accuracy estimates for Hamming similarity, but a combination of the candidates provided an accurate estimate of expected accuracy for Jaccard index and Exact match.},
  archive      = {J_TKDE},
  author       = {Laurence A. F. Park and Jesse Read},
  doi          = {10.1109/TKDE.2025.3545972},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Estimating multi-label expected accuracy using labelset distributions},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A universal pre-training and prompting framework for general
urban spatio-temporal prediction. <em>TKDE</em>, 1–20. (<a
href="https://doi.org/10.1109/TKDE.2025.3545948">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Urban spatio-temporal prediction is crucial for informed decision-making, such as traffic management, resource optimization, and emergency response. Despite remarkable breakthroughs in pretrained natural language models that enable one model to handle diverse tasks, a universal solution for spatio-temporal prediction remains challenging. Existing prediction approaches are typically tailored for specific spatio-temporal scenarios, requiring task-specific model designs and extensive domain-specific training data. In this study, we introduce UniST, a universal model designed for general urban spatio-temporal prediction across a wide range of scenarios. Inspired by large language models, UniST achieves success through: (i) utilizing diverse spatio-temporal data from different scenarios, (ii) effective pre-training to capture complex spatio-temporal dynamics, (iii) knowledge-guided prompts to enhance generalization capabilities. These designs together unlock the potential of building a universal model for various scenarios. Extensive experiments on more than 20 spatio-temporal scenarios, including grid-based data and graph-based data, demonstrate UniST&#39;s efficacy in advancing state-of-the-art performance, especially in few-shot and zero-shot prediction. The datasets and code implementation are released on https://github.com/tsinghua-fib-lab/UniST.},
  archive      = {J_TKDE},
  author       = {Yuan Yuan and Jingtao Ding and Jie Feng and Depeng Jin and Yong Li},
  doi          = {10.1109/TKDE.2025.3545948},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {A universal pre-training and prompting framework for general urban spatio-temporal prediction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PipeFilter: Parallelizable and space-efficient filter for
approximate membership query. <em>TKDE</em>, 1–15. (<a
href="https://doi.org/10.1109/TKDE.2025.3543881">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate membership query data structures (i.e., filters) have ubiquitous applications in database and data mining. Cuckoo filters are emerging as the alternative to Bloom filters because they support deletions and usually have higher operation throughput and space efficiency. However, their designs are confined to a single-threaded execution paradigm and consequently cannot fully exploit the parallel processing capabilities of modern hardware. This paper presents PipeFilter, a faster and more space-efficient filter that harnesses pipeline parallelism for superior performance. PipeFilter re-architects the Cuckoo filter by partitioning its data structure into several sub-filters, each providing a candidate position for every item. This allows the filter operations, including insertion, lookup, and deletion, to be naturally distributed across several pipeline stages, each overseeing one of the sub-filters, which can further be implemented through multi-threaded execution or pipeline stages of programmable hardware to achieve significantly higher throughput. Meanwhile, PipeFilter excels for single-threaded execution thanks to a combination of unique design features, including block design, path prophet, round robin, and SIMD optimization, such that it achieves superior performance than the SOTAs even when running with a single core. PipeFilter also has a competitive advantage in space utilization because it permits each item to explore more candidate positions. We implement and optimize PipeFilter on four platforms (single-core CPU, multi-core CPU, FPGA, and P4 ASIC). Experimental results demonstrate that PipeFilter surpasses all baseline methods on four platforms. When running with a single core, it showcases a notable 15%$\sim$57% improvement in operation throughput and a high load factor exceeding 99%. When parallel processing on other platforms, PipeFilter achieves 7$\times \sim 800\times$ higher throughput than single-threaded execution.},
  archive      = {J_TKDE},
  author       = {Shankui Ji and Yang Du and He Huang and Yu-E Sun and Jia Liu and Yapeng Shu},
  doi          = {10.1109/TKDE.2025.3543881},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {PipeFilter: Parallelizable and space-efficient filter for approximate membership query},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient and accurate spatial queries using lossy
compressed 3D geometry data. <em>TKDE</em>, 1–16. (<a
href="https://doi.org/10.1109/TKDE.2025.3539729">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D spatial data management is increasingly vital across various application scenarios, such as GIS, digital twins, human atlases, and tissue imaging. However, the inherent complexity of 3D spatial data, primarily represented by 3D geometries in real-world applications, hinders the efficient evaluation of spatial relationships through resource-intensive geometric computations. Geometric simplification algorithms have been developed to reduce the complexity of 3D representations, albeit at the cost of querying accuracy. Previous work has aimed to address precision loss by leveraging the spatial relationship between the simplified and original 3D object representations. However, this approach relied on specialized geometric simplification algorithms tailored to regions with specific criteria. In this paper, we introduce a novel approach to achieve highly efficient and accurate 3D spatial queries, incorporating geometric computation and simplification. We present a generalized progressive refinement methodology applicable to general geometric simplification algorithms, involving accurate querying of 3D geometry data using low-resolution representations and simplification extents quantified using Hausdorff distances at the facet level. Additionally, we propose techniques for calculating and storing Hausdorff distances efficiently. Extensive experimental evaluations validate the effectiveness of the proposed method which outperforms state-of-the-art systems by a factor of 4 while minimizing computational and storage overhead.},
  archive      = {J_TKDE},
  author       = {Dejun Teng and Zhaochuan Li and Zhaohui Peng and Shuai Ma and Fusheng Wang},
  doi          = {10.1109/TKDE.2025.3539729},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Efficient and accurate spatial queries using lossy compressed 3D geometry data},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast counting and utilizing induced 6-cycles in bipartite
networks. <em>TKDE</em>, 1–13. (<a
href="https://doi.org/10.1109/TKDE.2025.3546516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Bipartite graphs are a powerful tool for modeling the interactions between two distinct groups. These bipartite relationships often feature small, recurring structural patterns called motifs which are building blocks for community structure. One promising structure is the induced 6-cycle which consists of three nodes on each node set forming a cycle where each node has exactly two edges. In this paper, we study the problem of counting and utilizing induced 6-cycles in large bipartite networks. We first consider two adaptations inspired by previous works for cycle counting in bipartite networks. Then, we introduce a new approach for node triplets which offer a systematic way to count the induced 6-cycles, used in BATCHTRIPLETJOIN. Our experimental evaluation shows that BATCHTRIPLETJOIN is significantly faster than the other algorithms while being scalable to large graph sizes and number of cores. On a network with $ 112M$ edges, BATCHTRIPLETJOIN is able to finish the computation in 78 mins by using 52 threads. In addition, we provide a new way to identify anomalous node triplets by comparing and contrasting the butterfly and induced 6-cycle counts of the nodes. We showcase several case studies on real-world networks from Amazon Kindle ratings, Steam game reviews, and Yelp ratings.},
  archive      = {J_TKDE},
  author       = {Jason Niu and Jaroslaw Zola and Ahmet Erdem Sarıyüce},
  doi          = {10.1109/TKDE.2025.3546516},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Fast counting and utilizing induced 6-cycles in bipartite networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning causal representations based on a GAE embedded
autoencoder. <em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3546607">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Traditional machine-learning approaches face limitations when confronted with insufficient data. Transfer learning addresses this by leveraging knowledge from closely related domains. The key in transfer learning is to find a transferable feature representation to enhance cross-domain classification models. However, in some scenarios, some features correlated with samples in the source domain may not be relevant to those in the target. Causal inference enables us to uncover the underlying patterns and mechanisms within the data, mitigating the impact of confounding factors. Nevertheless, most existing causal inference algorithms have limitations when applied to high-dimensional datasets with nonlinear causal relationships. In this work, a new causal representation method based on a Graph autoencoder embedded AutoEncoder, named GeAE, is introduced to learn invariant representations across domains. The proposed approach employs a causal structure learning module, similar to a graph autoencoder, to account for nonlinear causal relationships present in the data. Moreover, the cross-entropy loss as well as the causal structure learning loss and the reconstruction loss are incorporated in the objective function designed in a united autoencoder. This method allows for the handling of high-dimensional data and can provide effective representations for cross-domain classification tasks. Experimental results on generated and real-world datasets demonstrate the effectiveness of GeAE compared with the state-of-the-art methods.},
  archive      = {J_TKDE},
  author       = {Kuang Zhou and Ming Jiang and Bogdan Gabrys and Yong Xu},
  doi          = {10.1109/TKDE.2025.3546607},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Learning causal representations based on a GAE embedded autoencoder},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Valuing training data via causal inference for in-context
learning. <em>TKDE</em>, 1–16. (<a
href="https://doi.org/10.1109/TKDE.2025.3546761">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-context learning (ICL) empowers large pre-trained language models (PLMs) to predict outcomes for unseen inputs without parameter updates. However, the efficacy of ICL heavily relies on the choice of demonstration examples. Randomly selecting from the training set frequently leads to inconsistent performance. Addressing this challenge, this study takes a novel approach by focusing on training data valuation through causal inference. Specifically, we introduce the concept of average marginal effect (AME) to quantify the contribution of individual training samples to ICL performance, encompassing both its generalization and robustness. Drawing inspiration from multiple treatment effects and randomized experiments, we initially sample diverse training subsets to construct prompts and evaluate the ICL performance based on these prompts. Subsequently, we employ Elastic Net regression to collectively estimate the AME values for all training data, considering subset compositions and inference performance. Ultimately, we prioritize samples with the highest values to prompt the inference of the test data. Across various tasks and with seven PLMs ranging in size from 0.8B to 33B, our approach consistently achieves state-of-the-art performance. Particularly, it outperforms Vanilla ICL and the best-performing baseline by an average of 14.1% and 5.2%, respectively. Moreover, prioritizing the most valuable samples for prompting leads to a significant enhancement in performance stability and robustness across various learning scenarios. Impressively, the valuable samples exhibit transferability across diverse PLMs and generalize well to out-of-distribution tasks.},
  archive      = {J_TKDE},
  author       = {Xiaoling Zhou and Wei Ye and Zhemg Lee and Lei Zou and Shikun Zhang},
  doi          = {10.1109/TKDE.2025.3546761},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Valuing training data via causal inference for in-context learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Do as i can, not as i get: Topology-aware multi-hop
reasoning on multi-modal knowledge graphs. <em>TKDE</em>, 1–14. (<a
href="https://doi.org/10.1109/TKDE.2025.3546686">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A multi-modal knowledge graph (MKG) includes triplets that consist of entities and relations and multi-modal auxiliary data. In recent years, multi-hop multi-modal knowledge graph reasoning (MMKGR) based on reinforcement learning (RL) has received extensive attention because it addresses the intrinsic incompleteness of MKG in an interpretable manner. However, its performance is limited by empirically designed rewards and sparse relations. In addition, this method has been designed for the transductive setting where test entities have been seen during training, and it works poorly in the inductive setting where test entities do not appear in the training set. To overcome these issues, we propose TMR (Topology-aware Multi-hop Reasoning), which can conduct MKG reasoning under inductive and transductive settings. Specifically, TMR mainly consists of two components. (1) The topology-aware inductive representation captures information from the directed relations of unseen entities, and aggregates query-related topology features in an attentive manner to generate the fine-grained entity-independent features. (2) After completing multi-modal feature fusion, the relation-augmented adaptive RL conducts multi-hop reasoning by eliminating manual rewards and dynamically adding actions. Finally, we construct new MKG datasets with different scales for inductive reasoning evaluation. Experimental results demonstrate that TMP outperforms state-of-the-art MKGR methods under both inductive and transductive settings.},
  archive      = {J_TKDE},
  author       = {Shangfei Zheng and Hongzhi Yin and Tong Chen and Quoc Viet Hung Nguyen and Wei Chen and Lei Zhao},
  doi          = {10.1109/TKDE.2025.3546686},
  journal      = {IEEE Transactions on Knowledge and Data Engineering},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Knowl. Data Eng.},
  title        = {Do as i can, not as i get: Topology-aware multi-hop reasoning on multi-modal knowledge graphs},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
