<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TPAMI_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tpami---63">TPAMI - 63</h2>
<ul>
<li><details>
<summary>
(2025). BridgeNet: Comprehensive and effective feature interactions
via bridge feature for multi-task dense predictions. <em>TPAMI</em>,
1–16. (<a href="https://doi.org/10.1109/TPAMI.2025.3535875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-task dense prediction aims at handling multiple pixel-wise prediction tasks within a unified network simultaneously for visual scene understanding. However, cross-task feature interactions of current methods are still suffering from incomplete levels of representations, less discriminative semantics in feature participants, and inefficient pair-wise task interaction processes. To tackle these under-explored issues, we propose a novel BridgeNet framework, which extracts comprehensive and discriminative intermediate Bridge Features, and conducts interactions based on them. Specifically, a Task Pattern Propagation (TPP) module is first applied to ensure highly semantic task-specific feature participants are prepared for subsequent interactions, and a Bridge Feature Extractor (BFE) is specially designed to selectively integrate both high-level and low-level representations to generate the comprehensive bridge features. Then, instead of conducting heavy pair-wise cross-task interactions, a Task-Feature Refiner (TFR) is developed to efficiently take guidance from bridge features and form final task predictions. To the best of our knowledge, this is the first work considering the completeness and quality of feature participants in cross-task interactions. Extensive experiments are conducted on NYUD-v2, Cityscapes and PASCAL Context benchmarks, and the superior performance shows the proposed architecture is effective and powerful in promoting different dense prediction tasks simultaneously.},
  archive      = {J_TPAMI},
  author       = {Jingdong Zhang and Jiayuan Fan and Peng Ye and Bo Zhang and Hancheng Ye and Baopu Li and Yancheng Cai and Tao Chen},
  doi          = {10.1109/TPAMI.2025.3535875},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {BridgeNet: Comprehensive and effective feature interactions via bridge feature for multi-task dense predictions},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pre-training a graph recurrent network for text
understanding. <em>TPAMI</em>, 1–14. (<a
href="https://doi.org/10.1109/TPAMI.2025.3538221">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transformer-based pre-trained models have gained much advance in recent years, Transformer architecture also becomes one of the most important backbones in natural language processing. Recent works show that the attention mechanism inside Transformer may not be necessary, and Transformer alternatives such as convolutional neural networks, multi-layer perceptron, and state space model have also been investigated. Transformer-based models have two main limitations: Firstly, they have quadratic time complexity due to the full attention mechanism, which leads to high computational costs. Secondly, they rely on representation of a special token such as [CLS] to encode entire text, which limits its sentence-level expressiveness. In this paper, we consider a graph recurrent network with linear time complexity for language model pre-training, which builds a graph structure for each sequence with local token-level communications, together with a sentence-level representation detached from other normal tokens. On both English and Chinese text understanding tasks, our model can achieve comparable performance to existing pre-trained models while also achieving higher inference efficiency. Furthermore, we discovered that the representations generated by our model are more diverse and uniform compared to that of Transformer, which alleviates the problems in existing pre-trained models such as representation degradation.},
  archive      = {J_TPAMI},
  author       = {Yile Wang and Linyi Yang and Zhiyang Teng and Ming Zhou and Yue Zhang},
  doi          = {10.1109/TPAMI.2025.3538221},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Pre-training a graph recurrent network for text understanding},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Quasi-metric learning for bilateral person-job fit.
<em>TPAMI</em>, 1–14. (<a
href="https://doi.org/10.1109/TPAMI.2025.3538774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Matching suitable jobs provided by employers with qualified candidates is a crucial task for online recruitment. Typically, candidates and employers have specific expectations in recruitment market, leading them to prefer similar jobs and candidates, respectively. Metric learning provides a promising way to capture the similarity propagation between candidates and jobs. However, existing metric learning technologies rely on symmetric distance measures, which fail to model the asymmetric relationships of bilateral users (i.e., candidates and employers) in the two-way selective process of recruitment scenarios. In addition, the behavior of users (e.g., candidates) is highly affected by the actions and feedback of their counterparts (e.g., employers). These effects can hardly be captured by the existing person-job fit methods which primarily explore homogeneous and undirected graphs. To address these problems, we propose a quasi-metric learning framework to capture the similarity propagation between candidates and jobs while modeling their asymmetric relations for bilateral person-job fit. Specifically, we propose a quasi-metric space that not only satisfies the triangle inequality rule to capture the fine-grained similarity between candidates and jobs, but also incorporates a tailored asymmetric measure to model the two-way selection process of bilateral users in online recruitment. More importantly, the proposed quasi-metric learning framework can theoretically model recruitment rules from similarity and competitiveness perspectives, making it seamlessly align with bilateral person-job fit scenarios. To explore the mutual effects of two-sided users on each other, we first organize candidates, employers, and their different-typed interactions into a heterogeneous relation graph, and then propose a relation-aware graph convolution network to capture the mutual effects of users with their bilateral behaviors. Extensive experiments on several real-world datasets demonstrate the effectiveness of the proposed quasi-metric learning framework and bilateral person-job fit model.},
  archive      = {J_TPAMI},
  author       = {Yingpeng Du and Hongzhi Liu and Hengshu Zhu and Yang Song and Zhi Zheng and Zhonghai Wu},
  doi          = {10.1109/TPAMI.2025.3538774},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Quasi-metric learning for bilateral person-job fit},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Temporally-consistent surface reconstruction using
metrically-consistent atlases. <em>TPAMI</em>, 1–13. (<a
href="https://doi.org/10.1109/TPAMI.2025.3538776">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We propose a method for unsupervised reconstruction of a temporally-consistent sequence of surfaces from a sequence of time-evolving point clouds. It yields dense and semantically meaningful correspondences between frames. We represent the reconstructed surfaces as atlases computed by a neural network, which enables us to establish correspondences between frames. The key to making these correspondences semantically meaningful is to guarantee that the metric tensors computed at corresponding points are as similar as possible. We have devised an optimization strategy that makes our method robust to noise and global motions, without a priori correspondences or pre-alignment steps. As a result, our approach outperforms state-of-the-art ones on several challenging datasets. The code is available at https://github.com/bednarikjan/temporally_coherent_surface_reconstruction.},
  archive      = {J_TPAMI},
  author       = {Jan Bednarik and Noam Aigerman and Vladimir G. Kim and Siddhartha Chaudhuri and Shaifali Parashar and Mathieu Salzmann and Pascal Fua},
  doi          = {10.1109/TPAMI.2025.3538776},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Temporally-consistent surface reconstruction using metrically-consistent atlases},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GaussNav: Gaussian splatting for visual navigation.
<em>TPAMI</em>, 1–14. (<a
href="https://doi.org/10.1109/TPAMI.2025.3538496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In embodied vision, Instance ImageGoal Navigation (IIN) requires an agent to locate a specific object depicted in a goal image within an unexplored environment. The primary challenge of IIN arises from the need to recognize the target object across varying viewpoints while ignoring potential distractors. Existing map-based navigation methods typically use Bird&#39;s Eye View (BEV) maps, which lack detailed texture representation of a scene. Consequently, while BEV maps are effective for semantic-level visual navigation, they are struggling for instancelevel tasks. To this end, we propose a new framework for IIN, Gaussian Splatting for Visual Navigation (GaussNav), which constructs a novel map representation based on 3D Gaussian Splatting (3DGS). The GaussNav framework enables the agent to memorize both the geometry and semantic information of the scene, as well as retain the textural features of objects. By matching renderings of similar objects with the target, the agent can accurately identify, ground, and navigate to the specified object. Our GaussNav framework demonstrates a significant performance improvement, with Success weighted by Path Length (SPL) increasing from 0.347 to 0.578 on the challenging HabitatMatterport 3D (HM3D) dataset. The source code is publicly available at the link: https://github.com/XiaohanLei/GaussNav.},
  archive      = {J_TPAMI},
  author       = {Xiaohan Lei and Min Wang and Wengang Zhou and Houqiang Li},
  doi          = {10.1109/TPAMI.2025.3538496},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {GaussNav: Gaussian splatting for visual navigation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Transferable unintentional action localization with
language-guided intention translation. <em>TPAMI</em>, 1–15. (<a
href="https://doi.org/10.1109/TPAMI.2025.3538675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unintentional action localization (UAL) is a challenging task that requires reasoning about action intention clues to detect the temporal locations of unintentional action occurrences in real-world videos. Previous efforts usually treated this task as a dense binary classification problem and did not fully explore the relationships between intention clues and unintentional actions, resulting in unsatisfactory performance on open-set scenarios during inference. In this paper, we propose a Transferable Unintentional Action Localization framework by introducing language-guided intention translation, which explicitly formulates unintentional action localization as an open-set localization problem. Our framework constructs a transferable reasoning model guided by natural languages to translate the action intention of the entire video, which generates natural and powerful supervision signals for reconstructing complete action intention clues to address the problem of unintentional action localization. Based on the fact that a video with failure action is composed of intentional and unintentional parts connected by a transient action transition. Our transferable reasoning model employs a transformer architecture to transfer knowledge between intentional and unintentional parts for learning complementary semantic representations of these two parts, completing the action intention clue in an implicit supervision manner. We also present a dense voting scheme for detecting the action transition from intentional to unintentional using discriminative representations incorporating action intention clues. Extensive experiments demonstrate that our framework outperforms representative unintentional action localization methods in a wide range of open-set scenarios. In addition, we create a new unintentional sports video dataset, FS-Falls, and extend our framework from in-the-wild scenarios to competitive sports to demonstrate better generalization ability. We hope this work will provide a new perspective on creating powerful representations with complete action intention priors, which will help us better understand human action and capture underlying intention clues in real-world videos.},
  archive      = {J_TPAMI},
  author       = {Jinglin Xu and Yongming Rao and Jie Zhou and Jiwen Lu},
  doi          = {10.1109/TPAMI.2025.3538675},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Transferable unintentional action localization with language-guided intention translation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). YOLO-MS: Rethinking multi-scale representation learning for
real-time object detection. <em>TPAMI</em>, 1–14. (<a
href="https://doi.org/10.1109/TPAMI.2025.3538473">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We aim at providing the object detection community with an efficient and performant object detector, termed YOLO-MS. The core design is based on a series of investigations on how multi-branch features of the basic block and convolutions with different kernel sizes affect the detection performance of objects at different scales. The outcome is a new strategy that can significantly enhance multi-scale feature representations of real-time object detectors. To verify the effectiveness of our work, we train our YOLO-MS on the MS COCO dataset from scratch without relying on any other large-scale datasets, like ImageNet or pre-trained weights. Without bells and whistles, our YOLO-MS outperforms the recent state-of-the-art real-time object detectors, including YOLO-v7, RTMDet, and YOLO-v8. Taking the XS version of YOLO-MS as an example, it can achieve an AP score of 42+% on MS COCO, which is about 2% higher than RTMDet with the same model size. Furthermore, our work can also serve as a plug-and-play module for other YOLO models. Typically, our method significantly advances the APs, APl, and AP of YOLOv8-N from 18%+, 52%+, and 37%+ to 20%+, 55%+, and 40%+, respectively, with even fewer parameters and MACs. Code and trained models are publicly available at https://github.com/FishAndWasabi/YOLO-MS. We also provide the Jittor version at https://github.com/NK-JittorCV/nk-yolo.},
  archive      = {J_TPAMI},
  author       = {Yuming Chen and Xinbin Yuan and Jiabao Wang and Ruiqi Wu and Xiang Li and Qibin Hou and Ming-Ming Cheng},
  doi          = {10.1109/TPAMI.2025.3538473},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {YOLO-MS: Rethinking multi-scale representation learning for real-time object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fair representation learning for continuous sensitive
attributes using expectation of integral probability metrics.
<em>TPAMI</em>, 1–12. (<a
href="https://doi.org/10.1109/TPAMI.2025.3538915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {AI fairness, also known as algorithmic fairness, aims to ensure that algorithms operate without bias or discrimination towards any individual or group. Among various AI algorithms, the Fair Representation Learning (FRL) approach has gained significant interest in recent years. However, existing FRL algorithms have a limitation: they are primarily designed for categorical sensitive attributes and thus cannot be applied to continuous sensitive attributes, such as age or income. In this paper, we propose an FRL algorithm for continuous sensitive attributes. First, we introduce a measure called the Expectation of Integral Probability Metrics (EIPM) to assess the fairness level of representation space for continuous sensitive attributes. We demonstrate that if the distribution of the representation has a low EIPM value, then any prediction head constructed on the top of the representation become fair, regardless of the selection of the prediction head. Furthermore, EIPM possesses a distinguished advantage in that it can be accurately estimated using our proposed estimator with finite samples. Based on these properties, we propose a new FRL algorithm called Fair Representation using EIPM with MMD (FREM). Experimental evidences show that FREM outperforms other baseline methods.},
  archive      = {J_TPAMI},
  author       = {Insung Kong and Kunwoong Kim and Yongdai Kim},
  doi          = {10.1109/TPAMI.2025.3538915},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Fair representation learning for continuous sensitive attributes using expectation of integral probability metrics},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Invertible diffusion models for compressed sensing.
<em>TPAMI</em>, 1–15. (<a
href="https://doi.org/10.1109/TPAMI.2025.3538896">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While deep neural networks (NN) significantly advance image compressed sensing (CS) by improving reconstruction quality, the necessity of training current CS NNs from scratch constrains their effectiveness and hampers rapid deployment. Although recent methods utilize pre-trained diffusion models for image reconstruction, they struggle with slow inference and restricted adaptability to CS. To tackle these challenges, this paper proposes Invertible Diffusion Models (IDM), a novel efficient, end-to-end diffusion-based CS method. IDM repurposes a large-scale diffusion sampling process as a reconstruction model, and fine-tunes it end-to-end to recover original images directly from CS measurements, moving beyond the traditional paradigm of one-step noise estimation learning. To enable such memory-intensive end-to-end fine-tuning, we propose a novel two-level invertible design to transform both (1) multi-step sampling process and (2) noise estimation U-Net in each step into invertible networks. As a result, most intermediate features are cleared during training to reduce up to 93.8% GPU memory. In addition, we develop a set of lightweight modules to inject measurements into noise estimator to further facilitate reconstruction. Experiments demonstrate that IDM outperforms existing state-of-the-art CS networks by up to 2.64dB in PSNR. Compared to the recent diffusion-based approach DDNM, our IDM achieves up to 10.09dB PSNR gain and 14.54 times faster inference. Code is available at https://github.com/Guaishou74851/IDM.},
  archive      = {J_TPAMI},
  author       = {Bin Chen and Zhenyu Zhang and Weiqi Li and Chen Zhao and Jiwen Yu and Shijie Zhao and Jie Chen and Jian Zhang},
  doi          = {10.1109/TPAMI.2025.3538896},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Invertible diffusion models for compressed sensing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting flatness-aware optimization in continual learning
with orthogonal gradient projection. <em>TPAMI</em>, 1–12. (<a
href="https://doi.org/10.1109/TPAMI.2025.3539019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of continual learning (CL) is to learn from a series of continuously arriving new tasks without forgetting previously learned old tasks. To avoid catastrophic forgetting of old tasks, orthogonal gradient projection (OGP) based CL methods constrain the gradients of new tasks to be orthogonal to the space spanned by old tasks. This strict gradient constraint will limit the learning ability of new tasks, resulting in lower performance on new tasks. In this paper, we first establish a unified framework for OGP-based CL methods. We then revisit OGP-based CL methods from a new perspective on the loss landscape, where we find that when relaxing projection constraints to improve performance on new tasks, the unflatness of the loss landscape can lead to catastrophic forgetting of old tasks. Based on our findings, we propose a new Dual Flatness-aware OGD framework that optimizes the flatness of the loss landscape from both data and weight levels. Our framework consists of three modules: data and weight perturbation, flatness-aware optimization, and gradient projection. Specifically, we first perform perturbations on the task&#39;s data and current model weights to make the task&#39;s loss reach the worst-case. Next, we optimize the loss and loss landscape on the original data and the worst-case perturbed data to obtain a flatness-aware gradient. Finally, the flatness-aware gradient will update the network in directions orthogonal to the space spanned by the old tasks. Extensive experiments on four benchmark datasets show that the framework improves the flatness of the loss landscape and performance on new tasks, and achieves state-of-the-art (SOTA) performance on average accuracy across all tasks.},
  archive      = {J_TPAMI},
  author       = {Enneng Yang and Li Shen and Zhenyi Wang and Shiwei Liu and Guibing Guo and Xingwei Wang and Dacheng Tao},
  doi          = {10.1109/TPAMI.2025.3539019},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Revisiting flatness-aware optimization in continual learning with orthogonal gradient projection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A causality-aware paradigm for evaluating creativity of
multimodal large language models. <em>TPAMI</em>, 1–17. (<a
href="https://doi.org/10.1109/TPAMI.2025.3539433">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, numerous benchmarks have been developed to evaluate the logical reasoning abilities of large language models (LLMs). However, assessing the equally important creative capabilities of LLMs is challenging due to the subjective, diverse, and data-scarce nature of creativity, especially in multimodal scenarios. In this paper, we consider the comprehensive pipeline for evaluating the creativity of multimodal LLMs, with a focus on suitable evaluation platforms and methodologies. First, we find the Oogiri game—a creativity-driven task requiring humor, associative thinking, and the ability to produce unexpected responses to text, images, or both. This game aligns well with the input-output structure of modern multimodal LLMs and benefits from a rich repository of high-quality, human-annotated creative responses, making it an ideal platform for studying LLM creativity. Next, beyond using the Oogiri game for standard evaluations like ranking and selection, we propose LoTbench, an interactive, causality-aware evaluation framework, to further address some intrinsic risks in standard evaluations, such as information leakage and limited interpretability. The proposed LoTbench not only quantifies LLM creativity more effectively but also visualizes the underlying creative thought processes. Our results show that while most LLMs exhibit constrained creativity, the performance gap between LLMs and humans is not insurmountable. Furthermore, we observe a strong correlation between results from the multimodal cognition benchmark MMMU and LoTbench, but only a weak connection with traditional creativity metrics. This suggests that LoTbench better aligns with human cognitive theories, highlighting cognition as a critical foundation in the early stages of creativity and enabling the bridging of diverse concepts.},
  archive      = {J_TPAMI},
  author       = {Zhongzhan Huang and Shanshan Zhong and Pan Zhou and Shanghua Gao and Marinka Zitnik and Liang Lin},
  doi          = {10.1109/TPAMI.2025.3539433},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A causality-aware paradigm for evaluating creativity of multimodal large language models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Absorption-based, passive range imaging from hyperspectral
thermal measurements. <em>TPAMI</em>, 1–18. (<a
href="https://doi.org/10.1109/TPAMI.2025.3538711">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Passive hyperspectral longwave infrared measurements are remarkably informative about the surroundings. Remote object material and temperature determine the spectrum of thermal radiance, and range, air temperature, and gas concentrations determine how this spectrum is modified by propagation to the sensor. We introduce a passive range imaging method based on computationally separating these phenomena. Previous methods assume hot and highly emitting objects; ranging is more challenging when objects&#39; temperatures do not deviate greatly from air temperature. Our method jointly estimates range and intrinsic object properties, with explicit consideration of air emission, though reflected light is assumed negligible. Inversion being underdetermined is mitigated by using a parametric model of atmospheric absorption and regularizing for smooth emissivity estimates. To assess where our estimate is likely accurate, we introduce a technique to detect which scene pixels are significantly influenced by reflected downwelling. Monte Carlo simulations demonstrate the importance of regularization, temperature differentials, and availability of many spectral bands. We apply our method to longwave infrared (8–13 $\mu$ m) hyperspectral image data acquired from natural scenes with no active illumination. Range features from 15 m to 150 m are recovered, with good qualitative match to lidar data for pixels classified as having negligible reflected downwelling.},
  archive      = {J_TPAMI},
  author       = {Unay Dorken Gallastegi and Hoover Rueda-Chacón and Martin J. Stevens and Vivek K Goyal},
  doi          = {10.1109/TPAMI.2025.3538711},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Absorption-based, passive range imaging from hyperspectral thermal measurements},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic scene understanding through object-centric
voxelization and neural rendering. <em>TPAMI</em>, 1–16. (<a
href="https://doi.org/10.1109/TPAMI.2025.3539866">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning object-centric representations from unsupervised videos is challenging. Unlike most previous approaches that focus on decomposing 2D images, we present a 3D generative model named DynaVol-S for dynamic scenes that enables object-centric learning within a differentiable volume rendering framework. The key idea is to perform object-centric voxelization to capture the 3D nature of the scene, which infers per-object occupancy probabilities at individual spatial locations. These voxel features evolve through a canonical-space deformation function and are optimized in an inverse rendering pipeline with a compositional NeRF. Additionally, our approach integrates 2D semantic features to create 3D semantic grids, representing the scene through multiple disentangled voxel grids. DynaVol-S significantly outperforms existing models in both novel view synthesis and unsupervised decomposition tasks for dynamic scenes. By jointly considering geometric structures and semantic features, it effectively addresses challenging real-world scenarios involving complex object interactions. Furthermore, once trained, the explicitly meaningful voxel features enable additional capabilities that 2D scene decomposition methods cannot achieve, such as novel scene generation through editing geometric shapes or manipulating the motion trajectories of objects.},
  archive      = {J_TPAMI},
  author       = {Yanpeng Zhao and Yiwei Hao and Siyu Gao and Yunbo Wang and Xiaokang Yang},
  doi          = {10.1109/TPAMI.2025.3539866},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Dynamic scene understanding through object-centric voxelization and neural rendering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Instruct-ReID++: Towards universal purpose
instruction-guided person re-identification. <em>TPAMI</em>, 1–18. (<a
href="https://doi.org/10.1109/TPAMI.2025.3538766">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, person re-identification (ReID) has witnessed fast development due to its broad practical applications and proposed various settings, e.g., traditional ReID, clothes-changing ReID, and visible-infrared ReID. However, current studies primarily focus on single specific tasks, which limits model applicability in real-world scenarios. This paper aims to address this issue by introducing a novel instruct-ReID task that unifies 6 existing ReID tasks in one model and retrieves images based on provided visual or textual instructions. Instruct-ReID is the first exploration of a general ReID setting, where 6 existing ReID tasks can be viewed as special cases by assigning different instructions. To facilitate research in this new instruct-ReID task, we propose a large-scale OmniReID++ benchmark equipped with diverse data and comprehensive evaluation methods, e.g., task-specific and task-free evaluation settings. In the task-specific evaluation setting, gallery sets are categorized according to specific ReID tasks. We propose a novel baseline model, IRM, with an adaptive triplet loss to handle various retrieval tasks within a unified framework. For task-free evaluation setting, where target person images are retrieved from task-agnostic gallery sets, we further propose a new method called IRM++ with novel memory bank-assisted learning. Extensive evaluations of IRM and IRM++ on OmniReID++ benchmark demonstrate the superiority of our proposed methods, achieving state-of-the-art performance on 10 test sets. The datasets, the model, and the code will be available at https://github.com/hwz-zju/Instruct-ReID.},
  archive      = {J_TPAMI},
  author       = {Weizhen He and Yiheng Deng and Yunfeng Yan and Feng Zhu and Yizhou Wang and Lei Bai and Qingsong Xie and Rui Zhao and Donglian Qi and Wanli Ouyang and Shixiang Tang},
  doi          = {10.1109/TPAMI.2025.3538766},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Instruct-ReID++: Towards universal purpose instruction-guided person re-identification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Digging deeper in gradient for unrolling-based accelerated
MRI reconstruction. <em>TPAMI</em>, 1–14. (<a
href="https://doi.org/10.1109/TPAMI.2025.3540218">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {There are two main methods that can be used to accelerate MRI reconstruction: parallel imaging and compressed sensing. To further accelerate the sampling process, the combination of these two methods has been extensively studied in recent years. However, existing MRI reconstruction methods often overlook the exploration of high-frequency information of images, leading to sub-optimal recovery of fine details in the reconstructed results. To address this issue, we conduct an in-depth analysis of image gradients and propose a novel MRI reconstruction model based on Maximum a Posteriori (MAP) estimation. We first establish the Cumulative Deviation from Maximum Gradient magnitude (CDMG) prior for fully sampled MR images through theoretical analysis, then incorporate this explicit CDMG prior along with an implicit deep prior to form the prior probability term. This combination of priors strikes a balance between physically informed constraints and data-driven adaptability, aiding in the recovery of meaningful high-frequency information. Additionally, we introduce a multi-order gradient operator to enhance the observation model, thereby improving the accuracy of the likelihood term. Through MAP estimation, we develop a novel accelerated MRI reconstruction model, the optimization of which is achieved by unrolling it into a convolutional neural network structure, referred to as DDGU-Net. Extensive experimental results demonstrate the effectiveness of our approach in reconstructing high-quality MR images and achieving state-of-the-art (SOTA) results, particularly at higher acceleration factors.},
  archive      = {J_TPAMI},
  author       = {Faming Fang and Tingting Wang and Guixu Zhang and Fang Li},
  doi          = {10.1109/TPAMI.2025.3540218},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Digging deeper in gradient for unrolling-based accelerated MRI reconstruction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). C2P-net: Comprehensive depth map to planar depth conversion
for room layout estimation. <em>TPAMI</em>, 1–18. (<a
href="https://doi.org/10.1109/TPAMI.2025.3540084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Room layout estimation seeks to infer the overall spatial configuration of indoor scenes using perspective or panoramic images. As the layout is determined by the dominant indoor planes, this problem inherently requires the reconstruction of these planes. Some studies reconstruct indoor planes from perspective images by learning pixel-level or instance-level plane parameters. However, directly learning these parameters has the problems of susceptibility to occlusions and position dependency. In this paper, we introduce the Comprehensive depth map to Planar depth (C2P) conversion, which reformulates planar depth reconstruction into the prediction of a comprehensive depth map and planar visibility confidence. Based on the parametric representation of planar depth we propose, the C2P conversion is applicable to both panoramic and perspective images. Accordingly, we present an effective framework for room layout estimation that jointly learns the comprehensive depth map and planar visibility confidence. Due to the differentiability of the C2P conversion, our network autonomously learns planar visibility confidence by constraining the estimated plane parameters and reconstructed planar depth map. We further propose a novel approach for 3D layout generation through sequential planar depth map integration. Experimental results demonstrate the superiority of our method across all evaluated panoramic and perspective datasets.},
  archive      = {J_TPAMI},
  author       = {Weidong Zhang and Mengjie Zhou and Jiyu Cheng and Ying Liu and Wei Zhang},
  doi          = {10.1109/TPAMI.2025.3540084},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {C2P-net: Comprehensive depth map to planar depth conversion for room layout estimation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MS-NeRF: Multi-space neural radiance fields. <em>TPAMI</em>,
1–18. (<a href="https://doi.org/10.1109/TPAMI.2025.3540074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing Neural Radiance Fields (NeRF) methods suffer from the existence of reflective objects, often resulting in blurry or distorted rendering. Instead of calculating a single radiance field, we propose a multi-space neural radiance field (MS-NeRF) that represents the scene using a group of feature fields in parallel sub-spaces, which leads to a better understanding of the neural network toward the existence of reflective and refractive objects. Our multi-space scheme works as an enhancement to existing NeRF methods, with only small computational overheads needed for training and inferring the extra-space outputs. We design different multi-space modules for representative MLP-based and grid-based NeRF methods, which improve Mip-NeRF 360 by 4.15 dB in PSNR with 0.5% extra parameters and further improve TensoRF by 2.71 dB with 0.046% extra parameters on reflective regions without degrading the rendering quality on other regions. We further construct a novel dataset consisting of 33 synthetic scenes and 7 real captured scenes with complex reflection and refraction, where we design complex camera paths to fully benchmark the robustness of NeRF-based methods. Extensive experiments show that our approach significantly outperforms the existing single-space NeRF methods for rendering high-quality scenes concerned with complex light paths through mirror- like objects. The source code, dataset, and results are available via our project page: https://zx-yin.github.io/msnerf/.},
  archive      = {J_TPAMI},
  author       = {Ze-Xin Yin and Peng-Yi Jiao and Jiaxiong Qiu and Ming-Ming Cheng and Bo Ren},
  doi          = {10.1109/TPAMI.2025.3540074},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MS-NeRF: Multi-space neural radiance fields},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Convection-diffusion equation: A theoretically certified
framework for neural networks. <em>TPAMI</em>, 1–14. (<a
href="https://doi.org/10.1109/TPAMI.2025.3540310">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Differential equations have demonstrated intrinsic connections to network structures, linking discrete network layers through continuous equations. Most existing approaches focus on the interaction between ordinary differential equations (ODEs) and feature transformations, primarily working on input signals. In this paper, we study the partial differential equation (PDE) model of neural networks, viewing the neural network as a functional operating on a base model provided by the last layer of the classifier. Inspired by scale-space theory, we theoretically prove that this mapping can be formulated by a convection-diffusion equation, under interpretable and intuitive assumptions from both neural network and PDE perspectives. This theoretically certified framework covers various existing network structures and training techniques, offering a mathematical foundation and new insights into neural networks. Moreover, based on the convection-diffusion equation model, we design a new network structure that incorporates a diffusion mechanism into the network architecture from a PDE perspective. Extensive experiments on benchmark datasets and real-world applications confirm the effectiveness of the proposed model.},
  archive      = {J_TPAMI},
  author       = {Tangjun Wang and Chenglong Bao and Zuoqiang Shi},
  doi          = {10.1109/TPAMI.2025.3540310},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Convection-diffusion equation: A theoretically certified framework for neural networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving adversarial training from the perspective of
class-flipping distribution. <em>TPAMI</em>, 1–15. (<a
href="https://doi.org/10.1109/TPAMI.2025.3540200">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adversarial training has been proposed and widely recognized as a very effective method to defend against adversarial noise. However, the label flipping pattern on different classes still need deeper exploration to identify potential problems and assist in further enhancing robustness. In this work, we model the class-flipping distribution via statistical investigations and find this distribution reveals two shortcomings: the highly misleading category is present in the model&#39;s predictions for data in each class, and the trend in class flipping are significantly different across classes. Based on these observations, we propose a Class-Flipping-aware Adversarial Training (CFAT) method. On the one hand, we obtain the most misleading categories for the data in each class by counting the samples flipped to different wrong categories, and utilize them as the target to construct corresponding targeted adversarial samples, respectively. On the other hand, we take the proportions of samples flipped to the most misleading category as factors to scale the perturbation budgets of adversarial training samples for the data with corresponding classes. Experimental results on datasets with different class number validate the effectiveness of the proposed method.},
  archive      = {J_TPAMI},
  author       = {Dawei Zhou and Nannan Wang and Tongliang Liu and Xinbo Gao},
  doi          = {10.1109/TPAMI.2025.3540200},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Improving adversarial training from the perspective of class-flipping distribution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MIXRTs: Toward interpretable multi-agent reinforcement
learning via mixing recurrent soft decision trees. <em>TPAMI</em>, 1–17.
(<a href="https://doi.org/10.1109/TPAMI.2025.3540467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {While achieving tremendous success in various fields, existing multi-agent reinforcement learning (MARL) with a black-box neural network makes decisions in an opaque manner that hinders humans from understanding the learned knowledge and how input observations influence decisions. In contrast, existing interpretable approaches usually suffer from weak expressivity and low performance. To bridge this gap, we propose MIXing Recurrent soft decision Trees (MIXRTs), a novel interpretable architecture that can represent explicit decision processes via the root-to-leaf path and reflect each agent&#39;s contribution to the team. Specifically, we construct a novel soft decision tree using a recurrent structure and demonstrate which features influence the decision-making process. Then, based on the value decomposition framework, we linearly assign credit to each agent by explicitly mixing individual action values to estimate the joint action value using only local observations, providing new insights into interpreting the cooperation mechanism. Theoretical analysis confirms that MIXRTs guarantee additivity and monotonicity in the factorization of joint action values. Evaluations on complex tasks like Spread and StarCraft II demonstrate that MIXRTs compete with existing methods while providing clear explanations, paving the way for interpretable and high-performing MARL systems.},
  archive      = {J_TPAMI},
  author       = {Zichuan Liu and Yuanyang Zhu and Zhi Wang and Yang Gao and Chunlin Chen},
  doi          = {10.1109/TPAMI.2025.3540467},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MIXRTs: Toward interpretable multi-agent reinforcement learning via mixing recurrent soft decision trees},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Implicit shape and appearance priors for few-shot full head
reconstruction. <em>TPAMI</em>, 1–14. (<a
href="https://doi.org/10.1109/TPAMI.2025.3540542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in learning techniques that employ coordinate-based neural representations have yielded remarkable results in multi-view 3D reconstruction tasks. However, these approaches often require a substantial number of input views (typically several tens) and computationally intensive optimization procedures to achieve their effectiveness. In this paper, we address these limitations specifically for the problem of few-shot full 3D head reconstruction. We accomplish this by incorporating a probabilistic shape and appearance prior into coordinate-based representations, enabling faster convergence and improved generalization when working with only a few input images (even as low as a single image). During testing, we leverage this prior to guiding the fitting process of a signed distance function using a differentiable renderer. By incorporating the statistical prior alongside parallelizable ray tracing and dynamic caching strategies, we achieve an efficient and accurate approach to few-shot full 3D head reconstruction. Moreover, we extend the H3DS dataset, which now comprises 60 high-resolution 3D full-head scans and their corresponding posed images and masks, which we use for evaluation purposes. By leveraging this dataset, we demonstrate the remarkable capabilities of our approach in achieving state-of-the-art results in geometry reconstruction while being an order of magnitude faster than previous approaches.},
  archive      = {J_TPAMI},
  author       = {Pol Caselles and Eduard Ramon and Jaime García and Gil Triginer and Francesc Moreno-Noguer},
  doi          = {10.1109/TPAMI.2025.3540542},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Implicit shape and appearance priors for few-shot full head reconstruction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A new metric based on association rules to assess
feature-attribution explainability techniques for time series
forecasting. <em>TPAMI</em>, 1–18. (<a
href="https://doi.org/10.1109/TPAMI.2025.3540513">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces a new, model-independent, metric, called RExQUAL, for quantifying the quality of explanations provided by attribution-based explainable artificial intelligence techniques and compare them. The underlying idea is based on feature attribution, using a subset of the ranking of the attributes highlighted by a model-agnostic explainable method in a forecasting task. Then, association rules are generated using these key attributes as input data. Novel metrics, including global support and confidence, are proposed to assess the joint quality of generated rules. Finally, the quality of the explanations is calculated based on a wise and comprehensive combination of the association rules global metrics. The proposed method integrates local explanations through attribution-based approaches for evaluation and feature selection with global explanations for the entire dataset. This paper rigorously evaluates the new metric by comparing three explainability techniques: the widely used SHAP and LIME, and the novel methodology RULEx. The experimental design includes predicting time series of different natures, including univariate and multivariate, through deep learning models. The results underscore the efficacy and versatility of the proposed methodology as a quantitative framework for evaluating and comparing explainable techniques.},
  archive      = {J_TPAMI},
  author       = {A. R. Troncoso-García and M. Martínez-Ballesteros and F. Martínez-Álvarez and A. Troncoso},
  doi          = {10.1109/TPAMI.2025.3540513},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A new metric based on association rules to assess feature-attribution explainability techniques for time series forecasting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Homeomorphism prior for false positive and negative problem
in medical image dense contrastive representation learning.
<em>TPAMI</em>, 1–18. (<a
href="https://doi.org/10.1109/TPAMI.2025.3540644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Dense contrastive representation learning (DCRL) has greatly improved the learning efficiency for image dense prediction tasks, showing its great potential to reduce the large costs of medical image collection and dense annotation. However, the properties of medical images make unreliable correspondence discovery, bringing an open problem of large-scale false positive and negative (FP&amp;N) pairs in DCRL. In this paper, we propose GEoMetric vIsual deNse sImilarity (GEMINI) learning which embeds the homeomorphism prior to DCRL and enables a reliable correspondence discovery for effective dense contrast. We proposes a deformable homeomorphism learning (DHL) which models the homeomorphism of medical images and learns to estimate a deformable mapping to predict the pixels&#39; correspondence under the condition of topological preservation. It effectively reduces the searching space of pairing and drives an implicit and soft learning of negative pairs via gradient. We also proposes a geometric semantic similarity (GSS) which extracts semantic information in features to measure the alignment degree for the correspondence learning. It will promote the learning efficiency and performance of deformation, constructing positive pairs reliably. We implement two practical variants on two typical representation learning tasks in our experiments. Our promising results on seven datasets which outperform the existing methods show our great superiority. We will release our code at a companion https://github.com/YutingHe-list/GEMINI.},
  archive      = {J_TPAMI},
  author       = {Yuting He and Boyu Wang and Rongjun Ge and Yang Chen and Guanyu Yang and Shuo Li},
  doi          = {10.1109/TPAMI.2025.3540644},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Homeomorphism prior for false positive and negative problem in medical image dense contrastive representation learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Equivariant diffusion model with a5-group neurons for joint
pose estimation and shape reconstruction. <em>TPAMI</em>, 1–15. (<a
href="https://doi.org/10.1109/TPAMI.2025.3540593">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object pose estimation and shape reconstruction are inherently coupled tasks although they have so far been studied separately in most existing approaches. A few recent works addressed the problem of joint pose estimation and shape reconstruction, but they found difficulties in handling partial observations and shape ambiguities. An open challenge in this area is to design a mechanism that has the two tasks benefit each other and boost the performance and robustness of both. In this work, we advocate the use of diffusion models for joint estimation of category-level object poses and reconstruction of object geometry. Diffusion models formulate shape reconstruction as a generation process conditioned on input observations. It has two main advantages. First, the iterative inference of diffusion models provides a mechanism for iterative optimization for both pose estimation and shape reconstruction. Second, diffusion models allow multiple outputs starting from different input noises, which would address the problem of ambiguity caused by partial observations. To achieve this, we propose equivariant diffusion model for joint pose estimation and shape reconstruction. The approach consists of an equivariant feature extractor to aggregate features of the input point cloud and a ShapePose diffusion model to generate object pose and shape simultaneously. To avoid training the model on all possible shape poses in the SO(3) space, we propose to augment the diffusion model with A5-group neurons where the neurons are converted into 5D vectors and can be rotated with the alternating group A5. Based on the A5-group neurons, we implement SO(3)-equivariant 3D point convolution and SO(3)-equivariant concatenation, making the entire network SO(3)-equivariant. Moreover, to select the most plausible combination of pose and shape from the generated ones, we propose a geometry-based measure of plausibility for an estimated pose along with a reconstructed shape. Extensive experiments demonstrate the effectiveness of the proposed method. Specifically, our method achieves the state-of-the-art on two public datasets and a new dataset with stacked objects, in terms of shape reconstruction and pose estimation. In particular, we show the proposed method could provide multiple plausible outputs under partial observations and shape ambiguities.},
  archive      = {J_TPAMI},
  author       = {Boyan Wan and Yifei Shi and Xiaohong Chen and Kai Xu},
  doi          = {10.1109/TPAMI.2025.3540593},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Equivariant diffusion model with a5-group neurons for joint pose estimation and shape reconstruction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning without forgetting for vision-language models.
<em>TPAMI</em>, 1–16. (<a
href="https://doi.org/10.1109/TPAMI.2025.3540889">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class-Incremental Learning (CIL) or continual learning is a desired capability in the real world, which requires a learning system to adapt to new tasks without forgetting former ones. While traditional CIL methods focus on visual information to grasp core features, recent advances in Vision-Language Models (VLM) have shown promising capabilities in learning generalizable representations with the aid of textual information. However, when continually trained with new classes, VLMs often suffer from catastrophic forgetting of former knowledge. Applying VLMs to CIL poses two major challenges: 1) how to adapt the model without forgetting; and 2) how to make full use of the multi-modal information. To this end, we propose PROjectiOn Fusion (Proof) that enables VLMs to learn without forgetting. To handle the first challenge, we propose training task-specific projections based on the frozen image/text encoders. When facing new tasks, new projections are expanded, and former projections are fixed, alleviating the forgetting of old concepts. For the second challenge, we propose the fusion module to better utilize the cross-modality information. By jointly adjusting visual and textual features, the model can capture better task-specific semantic information that facilitates recognition. Extensive experiments on nine benchmark datasets with various continual learning scenarios and various VLMs validate that Proofachieves state-of-the-art performance. Code is available at https://github.com/zhoudw-zdw/PROOF.},
  archive      = {J_TPAMI},
  author       = {Da-Wei Zhou and Yuanhan Zhang and Yan Wang and Jingyi Ning and Han-Jia Ye and De-Chuan Zhan and Ziwei Liu},
  doi          = {10.1109/TPAMI.2025.3540889},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning without forgetting for vision-language models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual-level matching with outlier filtering for unsupervised
visible-infrared person re-identification. <em>TPAMI</em>, 1–14. (<a
href="https://doi.org/10.1109/TPAMI.2025.3541053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-infrared person re-identification (VI-ReID) is a challenging cross-modality retrieval task due to the large modality gap. While numerous efforts have been devoted to the supervised setting with a large amount of labeled cross-modality correspondences, few studies have tried to mitigate the modality gap by mining cross-modality correspondences in an unsupervised manner. However, existing works failed to capture the intrinsic relations among samples across two modalities, resulting in limited performance outcomes. In this paper, we propose a novel Progressive Graph Matching (PGM) approach to globally model the cross-modality relationships and instance-level affinities. PGM formulates cross-modality correspondence mining as a graph matching procedure, aiming to integrate global information by minimizing global matching costs. Considering that samples in wrong clusters cannot find reliable cross-modality correspondences by PGM, we further introduce a robust Dual-Level Matching (DLM) mechanism, combining the cluster-level PGM and Nearest Instance-Cluster Searching (NICS) with instance-level affinity optimization. Additionally, we design an Outlier Filter Strategy (OFS) to filter out unreliable cross-modality correspondences based on the dual-level relation constraints. To mitigate false accumulation in cross-modal correspondence learning, an Alternate Cross Contrastive Learning (ACCL) module is proposed to alternately adjust the dominated matching, i.e., visible-to-infrared or infrared-to-visible matching. Empirical results demonstrate the superiority of our unsupervised solution, achieving comparable performance with supervised counterparts.},
  archive      = {J_TPAMI},
  author       = {Mang Ye and Zesen Wu and Bo Du},
  doi          = {10.1109/TPAMI.2025.3541053},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Dual-level matching with outlier filtering for unsupervised visible-infrared person re-identification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pixel-inconsistency modeling for image manipulation
localization. <em>TPAMI</em>, 1–18. (<a
href="https://doi.org/10.1109/TPAMI.2025.3541028">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Digital image forensics plays a crucial role in image authentication and manipulation localization. Despite the progress powered by deep neural networks, existing forgery localization methodologies exhibit limitations when deployed to unseen datasets and perturbed images (i.e., lack of generalization and robustness to real-world applications). To circumvent these problems and aid image integrity, this paper presents a generalized and robust manipulation localization model through the analysis of pixel inconsistency artifacts. The rationale is grounded on the observation that most image signal processors (ISP) involve the demosaicing process, which introduces pixel correlations in pristine images. Moreover, manipulating operations, including splicing, copy-move, and inpainting, directly affect such pixel regularity. We, therefore, first split the input image into several blocks and design masked self-attention mechanisms to model the global pixel dependency in input images. Simultaneously, we optimize another local pixel dependency stream to mine local manipulation clues within input forgery images. In addition, we design novel Learning-to-Weight Modules (LWM) to combine features from the two streams, thereby enhancing the final forgery localization performance. To improve the training process, we propose a novel Pixel-Inconsistency Data Augmentation (PIDA) strategy, driving the model to focus on capturing inherent pixel-level artifacts instead of mining semantic forgery traces. This work establishes a comprehensive benchmark integrating 16 representative detection models across 12 datasets. Extensive experiments show that our method successfully extracts inherent pixel-inconsistency forgery fingerprints and achieve state-of-the-art generalization and robustness performances in image manipulation localization.},
  archive      = {J_TPAMI},
  author       = {Chenqi Kong and Anwei Luo and Shiqi Wang and Haoliang Li and Anderson Rocha and Alex C. Kot},
  doi          = {10.1109/TPAMI.2025.3541028},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Pixel-inconsistency modeling for image manipulation localization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Revisiting gradient-based uncertainty for monocular depth
estimation. <em>TPAMI</em>, 1–14. (<a
href="https://doi.org/10.1109/TPAMI.2025.3541964">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Monocular depth estimation, similar to other image-based tasks, is prone to erroneous predictions due to ambiguities in the image, for example, caused by dynamic objects or shadows. For this reason, pixel-wise uncertainty assessment is required for safety-critical applications to highlight the areas where the prediction is unreliable. We address this in a post hoc manner and introduce gradient-based uncertainty estimation for already trained depth estimation models. To extract gradients without depending on the ground truth depth, we introduce an auxiliary loss function based on the consistency of the predicted depth and a reference depth. The reference depth, which acts as pseudo ground truth, is in fact generated using a simple image or feature augmentation, making our approach simple and effective. To obtain the final uncertainty score, the derivatives w.r.t. the feature maps from single or multiple layers are calculated using back-propagation. We demonstrate that our gradient-based approach is effective in determining the uncertainty without re-training using the two standard depth estimation benchmarks KITTI and NYU. In particular, for models trained with monocular sequences and therefore most prone to uncertainty, our method outperforms related approaches. In addition, we publicly provide our code and models: https://github.com/jhornauer/GrUMoDepth.},
  archive      = {J_TPAMI},
  author       = {Julia Hornauer and Amir El-Ghoussani and Vasileios Belagiannis},
  doi          = {10.1109/TPAMI.2025.3541964},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Revisiting gradient-based uncertainty for monocular depth estimation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diffusion model-based image editing: A survey.
<em>TPAMI</em>, 1–27. (<a
href="https://doi.org/10.1109/TPAMI.2025.3541625">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Denoising diffusion models have emerged as a powerful tool for various image generation and editing tasks, facilitating the synthesis of visual content in an unconditional or input-conditional manner. The core idea behind them is learning to reverse the process of gradually adding noise to images, allowing them to generate high-quality samples from a complex distribution. In this survey, we provide an exhaustive overview of existing methods using diffusion models for image editing, covering both theoretical and practical aspects in the field. We delve into a thorough analysis and categorization of these works from multiple perspectives, including learning strategies, user-input conditions, and the array of specific editing tasks that can be accomplished. In addition, we pay special attention to image inpainting and outpainting, and explore both earlier traditional context-driven and current multimodal conditional methods, offering a comprehensive analysis of their methodologies. To further evaluate the performance of text-guided image editing algorithms, we propose a systematic benchmark, EditEval, featuring an innovative metric, LMM Score. Finally, we address current limitations and envision some potential directions for future research. The accompanying repository is released at https://github.com/SiatMMLab/Awesome-Diffusion-Model-Based-Image-Editing-Methods.},
  archive      = {J_TPAMI},
  author       = {Yi Huang and Jiancheng Huang and Yifan Liu and Mingfu Yan and Jiaxi Lv and Jianzhuang Liu and Wei Xiong and He Zhang and Liangliang Cao and Shifeng Chen},
  doi          = {10.1109/TPAMI.2025.3541625},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-27},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Diffusion model-based image editing: A survey},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uni-MoE: Scaling unified multimodal LLMs with mixture of
experts. <em>TPAMI</em>, 1–15. (<a
href="https://doi.org/10.1109/TPAMI.2025.3532688">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in Multimodal Large Language Models (MLLMs) underscore the significance of scalable models and data to boost performance, yet this often incurs substantial computational costs. Although the Mixture of Experts (MoE) architecture has been employed to scale large language or visual-language models efficiently, these efforts typically involve fewer experts and limited modalities. To address this, our work presents the pioneering attempt to develop a unified MLLM with the MoE architecture, named Uni-MoE that can handle a wide array of modalities. Specifically, it features modality-specific encoders with connectors for a unified multimodal representation. We also implement a sparse MoE architecture within the LLMs to enable efficient training and inference through modality-level data parallelism and expert-level model parallelism. To enhance the multi-expert collaboration and generalization, we present a progressive training strategy: 1) Cross-modality alignment using various connectors with different cross-modality data, 2) Training modality-specific experts with cross-modality instruction data to activate experts&#39; preferences, and 3) Tuning the whole Uni-MoE framework utilizing Low-Rank Adaptation (LoRA) on mixed multimodal instruction data. We evaluate the instruction-tuned Uni-MoE on a comprehensive set of multimodal datasets. The extensive experimental results demonstrate Uni-MoE&#39;s principal advantage of significantly reducing performance bias in handling mixed multimodal datasets, alongside improved multi-expert collaboration and generalization. Our findings highlight the substantial potential of MoE frameworks in advancing MLLMs and the code is available at https://github.com/HITsz-TMG/UMOE-Scaling-Unified-Multimodal-LLMs.},
  archive      = {J_TPAMI},
  author       = {Yunxin Li and Shenyuan Jiang and Baotian Hu and Longyue Wang and Wanqi Zhong and Wenhan Luo and Lin Ma and Min Zhang},
  doi          = {10.1109/TPAMI.2025.3532688},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Uni-MoE: Scaling unified multimodal LLMs with mixture of experts},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Generalized conditional similarity learning via semantic
matching. <em>TPAMI</em>, 1–16. (<a
href="https://doi.org/10.1109/TPAMI.2025.3535730">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The inherent complexity of image semantics engenders a fascinating variability in relationships between images. For instance, under a certain condition, two images may demonstrate similarity, while under different circumstances, the same pair could exhibit absolute dissimilarity. A singular feature space is therefore insufficient for capturing the nuanced semantic relationships that exist between samples. Conditional Similarity Learning (CSL) aims to address this gap by learning multiple, distinct feature spaces. Existing approaches in CSL often fail to capture the intricate similarity relationships between samples across different semantic conditions, particularly in weakly-supervised settings where condition labels are absent during training. To address this limitation, we introduce Distance Induced Semantic COndition VERification NETwork (DISCOVERNET), a unified framework designed to cater to a range of CSL scenarios– supervised CSL (sCSL), weakly-supervised CSL (wsCSL), and semi-supervised CSL (ssCSL). In addition to traditional linear projections, we also introduce a prompt learning technique utilizing transformer encoding layer to create diverse embedding spaces. Our framework incorporates a Condition Match Module (CMM) that dynamically matches different training triplets with corresponding embedding spaces, adapting to varying levels of supervision. We also shed light on existing evaluation biases in wsCSL and introduce two novel criteria for a more robust evaluation. Through extensive experiments and visualizations on benchmark datasets such as UT-Zappos-50k and Celeb-A, we substantiate the efficacy and interpretability of DISCOVERNET.},
  archive      = {J_TPAMI},
  author       = {Yi Shi and Rui-Xiang Li and Le Gan and De-Chuan Zhan and Han-Jia Ye},
  doi          = {10.1109/TPAMI.2025.3535730},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Generalized conditional similarity learning via semantic matching},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CCDPlus: Towards accurate character to character
distillation for text recognition. <em>TPAMI</em>, 1–17. (<a
href="https://doi.org/10.1109/TPAMI.2025.3533737">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing scene text recognition methods leverage large-scale labeled synthetic data (LSD) to reduce reliance on labor-intensive annotation tasks and improve recognition capability in real-world scenarios. However, the emergence of a synth-to-real domain gap still limits their efficiency and robustness. Consequently, harvesting the meaningful intrinsic qualities of unlabeled real data (URD) is of great importance, given the prevalence of text-laden images. Toward the target, recent efforts have focused on pre-training on URD through sequence-to-sequence self-supervised learning, followed by fine-tuning on LSD via supervised learning. Nevertheless, they encounter three important issues: coarse representation learning units, inflexible data augmentation, and an emerging real-to-synth domain drift. To overcome these challenges, we propose CCDPlus, an accurate character-to-character distillation method for scene text recognition with a joint supervised and self-supervised learning framework. Specifically, tailored for text images, CCDPlus delineates the fine-grained character structures on URD as representation units by transferring knowledge learned from LSD online. Without requiring extra bounding box or pixel-level annotations, this process allows CCDPlus to enable character-to-character distillation flexibly with versatile data augmentation, which effectively extracts general real-world character-level feature representations. Meanwhile, the unified framework combines self-supervised learning on URD with supervised learning on LSD, effectively solving the domain inconsistency and enhancing the recognition performance. Extensive experiments demonstrate that CCDPlus outperforms previous state-of-the-art (SOTA) supervised, semi-supervised, and self-supervised methods by an average of 1.8%, 0.6%, and 1.1% on standard datasets, respectively. Additionally, it achieves a 6.1% improvement on the more challenging Union14M-L dataset. Code will be available at https://github.com/TongkunGuan/CCD.},
  archive      = {J_TPAMI},
  author       = {Tongkun Guan and Wei Shen and Xiaokang Yang},
  doi          = {10.1109/TPAMI.2025.3533737},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {CCDPlus: Towards accurate character to character distillation for text recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond batch learning: Global awareness enhanced domain
adaptation. <em>TPAMI</em>, 1–16. (<a
href="https://doi.org/10.1109/TPAMI.2025.3541207">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In domain adaptation (DA), the effectiveness of deep learning-based models is often constrained by batch learning strategies that fail to fully apprehend the global statistical and geometric characteristics of data distributions. Addressing this gap, we introduce “Global Awareness Enhanced Domain Adaptation” (GAN-DA), a novel approach that transcends traditional batch-based limitations. GAN-DA integrates a unique predefined feature representation (PFR) to facilitate the alignment of cross-domain distributions, thereby achieving a comprehensive global statistical awareness. This representation is innovatively expanded to encompass orthogonal and common feature aspects, which enhances the unification of global manifold structures and refines decision boundaries for more effective DA. Our extensive experiments, encompassing 27 diverse cross-domain image classification tasks, demonstrate GAN-DA&#39;s remarkable superiority, outperforming 24 established DA methods by a significant margin. Furthermore, our in-depth analyses shed light on the decision-making processes, revealing insights into the adaptability and efficiency of GAN-DA. This approach not only addresses the limitations of existing DA methodologies but also sets a new benchmark in the realm of domain adaptation, offering broad implications for future research and applications in this field.},
  archive      = {J_TPAMI},
  author       = {Lingkun Luo and Shiqiang Hu and Liming Chen},
  doi          = {10.1109/TPAMI.2025.3541207},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Beyond batch learning: Global awareness enhanced domain adaptation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adversarially robust neural architectures. <em>TPAMI</em>,
1–15. (<a href="https://doi.org/10.1109/TPAMI.2025.3542350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks (DNNs) are vulnerable to adversarial attacks. Existing methods are devoted to developing various robust training strategies or regularizations to update the weights of the neural network. But beyond the weights, the overall structure and information flow in the network are explicitly determined by the neural architecture, which remains unexplored. Thus, this paper aims to improve the adversarial robustness of the network from the architectural perspective. We explore the relationship among adversarial robustness, Lipschitz constant, and architecture parameters and show that an appropriate constraint on architecture parameters could reduce the Lipschitz constant to further improve the robustness. The importance of architecture parameters could vary from operation to operation or connection to connection. We approximate the Lipschitz constant of the entire network through a univariate log-normal distribution, whose mean and variance are related to architecture parameters. The confidence can be fulfilled through formulating a constraint on the distribution parameters based on the cumulative function. Compared with adversarially trained neural architectures searched by various NAS algorithms as well as efficient human-designed models, our algorithm empirically achieves the best performance among all the models under various attacks on different datasets.},
  archive      = {J_TPAMI},
  author       = {Minjing Dong and Yanxi Li and Yunhe Wang and Chang Xu},
  doi          = {10.1109/TPAMI.2025.3542350},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Adversarially robust neural architectures},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wholly-WOOD: Wholly leveraging diversified-quality labels
for weakly-supervised oriented object detection. <em>TPAMI</em>, 1–18.
(<a href="https://doi.org/10.1109/TPAMI.2025.3542542">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Accurately estimating the orientation of visual objects with compact rotated bounding boxes (RBoxes) has become a prominent demand, which challenges existing object detection paradigms that only use horizontal bounding boxes (HBoxes). To equip the detectors with orientation awareness, supervised regression/classification modules have been introduced at the high cost of rotation annotation. Meanwhile, some existing datasets with oriented objects are already annotated with horizontal boxes or even single points. It becomes attractive yet remains open for effectively utilizing weaker single point and horizontal annotations to train an oriented object detector (OOD). We develop Wholly-WOOD, a weakly-supervised OOD framework, capable of wholly leveraging various labeling forms (Points, HBoxes, RBoxes, and their combination) in a unified fashion. By only using HBox for training, our Wholly-WOOD achieves performance very close to that of the RBox-trained counterpart on remote sensing and other areas, significantly reducing the tedious efforts on labor-intensive annotation for oriented objects.},
  archive      = {J_TPAMI},
  author       = {Yi Yu and Xue Yang and Yansheng Li and Zhenjun Han and Feipeng Da and Junchi Yan},
  doi          = {10.1109/TPAMI.2025.3542542},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Wholly-WOOD: Wholly leveraging diversified-quality labels for weakly-supervised oriented object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). S-NeRF++: Autonomous driving simulation via neural
reconstruction and generation. <em>TPAMI</em>, 1–19. (<a
href="https://doi.org/10.1109/TPAMI.2025.3543072">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Autonomous driving simulation system plays a crucial role in enhancing self-driving data and simulating complex and rare traffic scenarios, ensuring navigation safety. However, traditional simulation systems, which often heavily rely on manual modeling and 2D image editing, struggled with scaling to extensive scenes and generating realistic simulation data. In this study, we present S-NeRF++, an innovative autonomous driving simulation system based on neural reconstruction. Trained on widely-used self-driving datasets such as nuScenes and Waymo, S-NeRF++ can generate a large number of realistic street scenes and foreground objects with high rendering quality as well as offering considerable flexibility in manipulation and simulation. Specifically, S-NeRF++ is an enhanced neural radiance field for synthesizing large-scale scenes and moving vehicles, with improved scene parameterization and camera pose learning. The system effectively utilizes noisy and sparse LiDAR data to refine training and address depth outliers, ensuring high-quality reconstruction and novel-view rendering. It also provides a diverse foreground asset bank by reconstructing and generating different foreground vehicles to support comprehensive scenario creation. Moreover, we have developed an advanced foreground-background fusion pipeline that skillfully integrates illumination and shadow effects, further enhancing the realism of our simulations. With the high-quality simulated data provided by our S-NeRF++, we found the perception methods enjoy performance boosts on several autonomous driving downstream tasks, further demonstrating our proposed simulator&#39;s effectiveness.},
  archive      = {J_TPAMI},
  author       = {Yurui Chen and Junge Zhang and Ziyang Xie and Wenye Li and Feihu Zhang and Jiachen Lu and Li Zhang},
  doi          = {10.1109/TPAMI.2025.3543072},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-19},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {S-NeRF++: Autonomous driving simulation via neural reconstruction and generation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Shared growth of graph neural networks via prompted
free-direction knowledge distillation. <em>TPAMI</em>, 1–18. (<a
href="https://doi.org/10.1109/TPAMI.2025.3543211">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation (KD) has shown to be effective to boost the performance of graph neural networks (GNNs), where the typical objective is to distill knowledge from a deeper teacher GNN into a shallower student GNN. However, it is often quite challenging to train a satisfactory deeper GNN due to the well-known over-parametrized and over-smoothing issues, leading to invalid knowledge transfer in practical applications. In this paper, we propose the first Free-direction Knowledge Distillation framework via reinforcement learning for GNNs, called FreeKD, which is no longer required to provide a deeper well-optimized teacher GNN. Our core idea is to collaboratively learn two shallower GNNs in an effort to exchange knowledge between them via reinforcement learning in a hierarchical way. As we observe that one typical GNN model often exhibits better and worse performances at different nodes during training, we devise a dynamic and free-direction knowledge transfer strategy that involves two levels of actions: 1) node-level action determines the directions of knowledge transfer between the corresponding nodes of two networks; and then 2) structure-level action determines which of the local structures generated by the node-level actions to be propagated. Additionally, considering that different augmented graphs can potentially capture distinct perspectives or representations of the graph data, we propose FreeKD-Prompt that learns undistorted and diverse augmentations based on prompt learning for exchanging varied knowledge. Furthermore, instead of confining knowledge exchange within two GNNs, we develop FreeKD++ and FreeKD-Prompt++ to enable free-direction knowledge transfer among multiple shallow GNNs. Extensive experiments on five benchmark datasets demonstrate our approaches outperform the base GNNs by a large margin, and show their efficacy to various GNNs. More surprisingly, our FreeKD has comparable or even better performance than traditional KD algorithms that distill knowledge from a deeper and stronger teacher GNN.},
  archive      = {J_TPAMI},
  author       = {Kaituo Feng and Yikun Miao and Changsheng Li and Ye Yuan and Guoren Wang},
  doi          = {10.1109/TPAMI.2025.3543211},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Shared growth of graph neural networks via prompted free-direction knowledge distillation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frozen CLIP-DINO: A strong backbone for weakly supervised
semantic segmentation. <em>TPAMI</em>, 1–17. (<a
href="https://doi.org/10.1109/TPAMI.2025.3543191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised semantic segmentation has witnessed great achievements with image-level labels. Several recent approaches use the CLIP model to generate pseudo labels for training an individual segmentation model, while there is no attempt to apply the CLIP model as the backbone to directly segment objects with image-level labels. In this paper, we propose WeCLIP and its advanced version WeCLIP+, to build the single-stage pipeline for weakly supervised semantic segmentation. For WeCLIP, the frozen CLIP model is applied as the backbone for semantic feature extraction, and a new light decoder is designed to interpret extracted semantic features for final prediction. Meanwhile, we utilize the above frozen backbone to generate pseudo labels for training the decoder. Such labels are fixed during training. We then propose a refinement module (RFM) to optimize them dynamically. For WeCLIP+, we introduce the frozen DINO model to achieve more comprehensive semantic feature extraction. The frozen DINO is combined with the frozen CLIP as the backbone, followed by a shared decoder to make predictions with less training cost. Moreover, a strengthened refinement module (RFM+) is designed to revise online pseudo labels with extra guidance from DINO features. Extensive experiments show that both WeCLIP and WeCLIP+ significantly outperform other approaches with less training cost. Particularly, WeCLIP+ gets mIoU of 83.9% on VOC 2012 test set and 56.3% on COCO val set. Additionally, these two approaches also obtain promising results for fully supervised settings. The code is available at https://github.com/zbf1991/WeCLIP.},
  archive      = {J_TPAMI},
  author       = {Bingfeng Zhang and Siyue Yu and Jimin Xiao and Yunchao Wei and Yao Zhao},
  doi          = {10.1109/TPAMI.2025.3543191},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Frozen CLIP-DINO: A strong backbone for weakly supervised semantic segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A variational bayesian inference theory of elasticity and
its mixed probabilistic finite element method for inverse deformation
solutions in any dimension. <em>TPAMI</em>, 1–12. (<a
href="https://doi.org/10.1109/TPAMI.2025.3542423">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this work, we have developed a variational Bayesian inference theory of elasticity, which is accomplished by using a mixed Variational Bayesian inference Finite Element Method (VBI-FEM) that can be used to solve the inverse deformation problems of continua. In the proposed variational Bayesian inference theory of continuum mechanics, the elastic strain energy is used as a prior in a Bayesian inference network, which can intelligently recover the detailed continuum deformation mappings with only given the information on the deformed and undeformed continuum body shapes without knowing the interior deformation and the precise actual boundary conditions, both traction as well as displacement boundary conditions, and the actual material constitutive relation. Moreover, we have implemented the related finite element formulation in a computational probabilistic mechanics framework. To numerically solve mixed variational problem, we developed an operator splitting or staggered algorithm that consists of the finite element (FE) step and the Bayesian learning (BL) step as an analogue of the well-known the Expectation-Maximization (EM) algorithm. By solving the mixed probabilistic Galerkin variational problem, we demonstrated that the proposed method is able to inversely predict continuum deformation mappings with strong discontinuity or fracture without knowing the external load conditions. The proposed method provides a robust machine intelligent solution for the long-sought-after inverse problem solution, which has been a major challenge in structure failure forensic pattern analysis in past several decades. The proposed method may become a promising artificial intelligence-based inverse method for solving general partial differential equations.},
  archive      = {J_TPAMI},
  author       = {Chao Wang and Shaofan Li},
  doi          = {10.1109/TPAMI.2025.3542423},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A variational bayesian inference theory of elasticity and its mixed probabilistic finite element method for inverse deformation solutions in any dimension},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization of rank losses for image retrieval.
<em>TPAMI</em>, 1–12. (<a
href="https://doi.org/10.1109/TPAMI.2025.3543846">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In image retrieval, standard evaluation metrics rely on score ranking, e.g. average precision (AP), recall at k (R@k), normalized discounted cumulative gain (NDCG). In this work, we introduce a general framework for robust and decomposable rank losses optimization. It addresses two major challenges for end-to-end training of deep neural networks with rank losses: non-differentiability and non-decomposability. Firstly, we propose a general surrogate for ranking operator, SupRank, that is amenable to stochastic gradient descent. It provides an upperbound for rank losses and ensures robust training. Secondly, we use a simple yet effective loss function to reduce the decomposability gap between the averaged batch approximation of ranking losses and their values on the whole training set. We apply our framework to two standard metrics for image retrieval: AP and R@k. Additionally, we apply our framework to hierarchical image retrieval. We introduce an extension of AP, the hierarchical average precision ${\mathcal {H}}$, and optimize it as well as the NDCG. Finally, we create the first hierarchical landmarks retrieval dataset. We use a semi-automatic pipeline to create hierarchical labels, extending the large scale Google Landmarks v2 dataset. The hierarchical dataset is publicly available at github.com/cvdfoundation/google-landmark. Code is available at github.com/elias-ramzi/SupRank.},
  archive      = {J_TPAMI},
  author       = {Elias Ramzi and Nicolas Audebert and Clément Rambour and André Araujo and Xavier Bitot and Nicolas Thome},
  doi          = {10.1109/TPAMI.2025.3543846},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Optimization of rank losses for image retrieval},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025a). S2-transformer for mask-aware hyperspectral image
reconstruction. <em>TPAMI</em>, 1–18. (<a
href="https://doi.org/10.1109/TPAMI.2025.3543842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Snapshot compressive imaging (SCI) surges as a novel way of capturing hyperspectral images. It operates an optical encoder to compress the 3D data into a 2D measurement and adopts a software decoder for the signal reconstruction. Recently, a representative SCI set-up of coded aperture snapshot compressive imager (CASSI) with Transformer reconstruction backend remarks high-fidelity sensing performance. However, dominant spatial and spectral attention designs show limitations in hyperspectral modeling. The spatial attention values describe the inter-pixel correlation but overlook the across-spectra variation within each pixel. The spectral attention size is unscalable to the token spatial size and thus bottlenecks information allocation. Besides, CASSI entangles the spatial and spectral information into a 2D measurement, placing a barrier for information disentanglement and modeling. In addition, CASSI blocks the light with a physical binary mask, yielding the masked data loss. To tackle above challenges, we propose a spatial-spectral (S 2 -) Transformer implemented by a paralleled attention design and a mask-aware learning strategy. Firstly, we systematically explore pros and cons of different spatial (-spectral) attention designs, based on which we find performing both attentions in parallel well disentangles and models the blended information. Secondly, the masked pixels induce higher prediction difficulty and should be treated differently from unmasked ones. We adaptively prioritize the loss penalty attributing to the mask structure by referring to the mask-encoded prediction as an uncertainty estimator. We theoretically discuss the distinct convergence tendencies between masked/unmasked regions of the proposed learning strategy. Extensive experiments demonstrate that on average, the results of the proposed method are superior over the state-of-the-art methods. We empirically visualize and reason the behaviour of spatial and spectral attentions, and comprehensively examine the impact of the mask-aware learning, both of which advances the physics-driven deep network design for the reconstruction with CASSI. Code is available at https://github.com/Jiamian-Wang/S2-transformer-HSI.},
  archive      = {J_TPAMI},
  author       = {Jiamian Wang and Kunpeng Li and Yulun Zhang and Xin Yuan and Zhiqiang Tao},
  doi          = {10.1109/TPAMI.2025.3543842},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {S2-transformer for mask-aware hyperspectral image reconstruction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diffusion models in low-level vision: A survey.
<em>TPAMI</em>, 1–20. (<a
href="https://doi.org/10.1109/TPAMI.2025.3545047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep generative models have gained considerable attention in low-level vision tasks due to their powerful generative capabilities. Among these, diffusion model-based approaches, which employ a forward diffusion process to degrade an image and a reverse denoising process for image generation, have become particularly prominent for producing high-quality, diverse samples with intricate texture details. Despite their widespread success in low-level vision, there remains a lack of a comprehensive, insightful survey that synthesizes and organizes the advances in diffusion model-based techniques. To address this gap, this paper presents the first comprehensive review focused on denoising diffusion models applied to low-level vision tasks, covering both theoretical and practical contributions. We outline three general diffusion modeling frameworks and explore their connections with other popular deep generative models, establishing a solid theoretical foundation for subsequent analysis. We then categorize diffusion models used in low-level vision tasks from multiple perspectives, considering both the underlying framework and the target application. Beyond natural image processing, we also summarize diffusion models applied to other low-level vision domains, including medical imaging, remote sensing, and video processing. Additionally, we provide an overview of widely used benchmarks and evaluation metrics in low-level vision tasks. Our review includes an extensive evaluation of diffusion model-based techniques across six representative tasks, with both quantitative and qualitative analysis. Finally, we highlight the limitations of current diffusion models and propose four promising directions for future research. This comprehensive review aims to foster a deeper understanding of the role of denoising diffusion models in low-level vision. For those interested, a curated list of diffusion model-based techniques, datasets, and related information across over 20 low-level vision tasks is available at https://github.com/ChunmingHe/awesome-diffusion-models-in-low-level-vision.},
  archive      = {J_TPAMI},
  author       = {Chunming He and Yuqi Shen and Chengyu Fang and Fengyang Xiao and Longxiang Tang and Yulun Zhang and Wangmeng Zuo and Zhenhua Guo and Xiu Li},
  doi          = {10.1109/TPAMI.2025.3545047},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Diffusion models in low-level vision: A survey},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Context perception parallel decoder for scene text
recognition. <em>TPAMI</em>, 1–16. (<a
href="https://doi.org/10.1109/TPAMI.2025.3545453">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scene text recognition (STR) methods have struggled to attain high accuracy and fast inference speed. Auto-Regressive (AR)-based models implement the recognition in a character-by-character manner, showing superiority in accuracy but with slow inference speed. Alternatively, Parallel Decoding (PD)-based models infer all characters in a single decoding pass, offering faster inference speed but generally worse accuracy. To realize the dual goals of “AR-level accuracy and PD-level speed”, we propose a Context Perception Parallel Decoder (CPPD) to perceive the related context and predict the character sequence in a PD pass. CPPD devises a character counting module to infer the occurrence count of each character, and a character ordering module to deduce the content-free reading order and positions. Meanwhile, the character prediction task associates the positions with characters. They together build a comprehensive recognition context, which benefits the decoder to focus accurately on characters with the attention mechanism, thereby improving the recognition accuracy. We construct a series of CPPD models and also plug the proposed modules into existing STR decoders. Experiments on both English and Chinese benchmarks demonstrate that the CPPD models achieve highly competitive accuracy while running much faster than existing leading models. Moreover, the plugged models achieve significant accuracy improvements.},
  archive      = {J_TPAMI},
  author       = {Yongkun Du and Zhineng Chen and Caiyan Jia and Xiaoting Yin and Chenxia Li and Yuning Du and Yu-Gang Jiang},
  doi          = {10.1109/TPAMI.2025.3545453},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Context perception parallel decoder for scene text recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning emotion category representation to detect emotion
relations across languages. <em>TPAMI</em>, 1–15. (<a
href="https://doi.org/10.1109/TPAMI.2025.3545447">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Understanding human emotions is crucial for a myriad of applications, from psychological research to advancements in Natural Language Processing (NLP). Traditionally, emotions are categorized into distinct basic groups, which has led to the development of various emotion detection tasks within NLP. However, these tasks typically rely on one-hot vectors to represent emotions, a method that fails to capture the relations between different emotion categories. In this study, we challenge the assumption that emotion categories are mutually exclusive and argue that the connections and boundaries between them are complex and often blurred. To better represent these nuanced interconnections, we introduce an innovative framework as well as two algorithms to learn distributed representations of emotion categories by leveraging soft labels from trained neural network models. For the first time, our approach enables the detection of emotion relations across different languages through an NLP lens, a feat unattainable with traditional one-hot representations. Validation experiments confirm the superior ability of our distributed representation algorithms to articulate these emotional connections. Moreover, application experiments corroborate several interdisciplinary insights into cross-linguistic emotion relations, findings that align with research in psychology and linguistics. This work not only presents a breakthrough in emotion detection but also bridges the gap between computational models and humanistic understanding of emotions},
  archive      = {J_TPAMI},
  author       = {Xiangyu Wang and Chengqing Zong},
  doi          = {10.1109/TPAMI.2025.3545447},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning emotion category representation to detect emotion relations across languages},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards expressive spectral-temporal graph neural networks
for time series forecasting. <em>TPAMI</em>, 1–14. (<a
href="https://doi.org/10.1109/TPAMI.2025.3545671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series forecasting has remained a focal point due to its vital applications in sectors such as energy management and transportation planning. Spectral-temporal graph neural network is a promising abstraction underlying most time series forecasting models that are based on graph neural networks (GNNs). However, more is needed to know about the underpinnings of this branch of methods. In this paper, we establish a theoretical framework that unravels the expressive power of spectral-temporal GNNs. Our results show that linear spectral-temporal GNNs are universal under mild assumptions, and their expressive power is bounded by our extended first-order Weisfeiler-Leman algorithm on discrete-time dynamic graphs. To make our findings useful in practice on valid instantiations, we discuss related constraints in detail and outline a theoretical blueprint for designing spatial and temporal modules in spectral domains. Building on these insights and to demonstrate how powerful spectral-temporal GNNs are based on our framework, we propose a simple instantiation named Temporal Graph Gegenbauer Convolution (TGGC), which significantly outperforms most existing models with only linear components and shows better model efficiency. Our findings pave the way for devising a broader array of provably expressive GNN-based models for time series, and the code is at https://anonymous.4open.science/r/TGGC.},
  archive      = {J_TPAMI},
  author       = {Ming Jin and Guangsi Shi and Yuan-Fang Li and Bo Xiong and Tian Zhou and Flora D. Salim and Liang Zhao and Lingfei Wu and Qingsong Wen and Shirui Pan},
  doi          = {10.1109/TPAMI.2025.3545671},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards expressive spectral-temporal graph neural networks for time series forecasting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Local texture pattern estimation for image detail
super-resolution. <em>TPAMI</em>, 1–18. (<a
href="https://doi.org/10.1109/TPAMI.2025.3545571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the image super-resolution (SR) field, recovering missing high-frequency textures has always been an important goal. However, deep SR networks based on pixel-level constraints tend to focus on stable edge details and cannot effectively restore random high-frequency textures. It was not until the emergence of the generative adversarial network (GAN) that GAN-based SR models achieved realistic texture restoration and quickly became the mainstream method for texture SR. However, GAN-based SR models still have some drawbacks, such as relying on a large number of parameters and generating fake textures that are inconsistent with ground truth. Inspired by traditional texture analysis research, this paper proposes a novel SR network based on local texture pattern estimation (LTPE), which can restore fine high-frequency texture details without GAN. A differentiable local texture operator is firstly designed to extract local texture structures, and a texture enhancement branch is used to predict the high-resolution local texture distribution based on the LTPE. Then, the predicted high-resolution texture structure map can be used as a reference for the texture fusion SR branch to obtain high-quality texture reconstruction. Finally, $L_{1}$ loss and Gram loss are simultaneously used to optimize the network. Experimental results demonstrate that the proposed method can effectively recover high-frequency texture without using GAN structures. In addition, the restored high-frequency details are constrained by local texture distribution, thereby reducing significant errors in texture generation. The proposed algorithm will be open-sourced.},
  archive      = {J_TPAMI},
  author       = {Fan Fan and Yang Zhao and Yuan Chen and Nannan Li and Wei Jia and Ronggang Wang},
  doi          = {10.1109/TPAMI.2025.3545571},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Local texture pattern estimation for image detail super-resolution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PhysMLE: Generalizable and priors-inclusive multi-task
remote physiological measurement. <em>TPAMI</em>, 1–19. (<a
href="https://doi.org/10.1109/TPAMI.2025.3545598">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Remote photoplethysmography (rPPG) has been widely applied to measure heart rate from face videos. To increase the generalizability of the algorithms, domain generalization (DG) attracted increasing attention in rPPG. However, when rPPG is extended to simultaneously measure more vital signs (e.g., respiration and blood oxygen saturation), achieving generalizability brings new challenges. Although partial features shared among different physiological signals can benefit multi-task learning, the sparse and imbalanced target label space brings the seesaw effect over task-specific feature learning. To resolve this problem, we designed an end-to-end Mixture of Low-rank Experts for multi-task remote Physiological measurement (PhysMLE), which is based on multiple low-rank experts with a novel router mechanism, thereby enabling the model to adeptly handle both specifications and correlations within tasks. Additionally, we introduced prior knowledge from physiology among tasks to overcome the imbalance of label space under real-world multi-task physiological measurement. For fair and comprehensive evaluations, this paper proposed a large-scale multi-task generalization benchmark, named Multi-Source Synsemantic Domain Generalization (MSSDG) protocol. Extensive experiments with MSSDG and intra-dataset have shown the effectiveness and efficiency of PhysMLE. In addition, a new dataset was collected and made publicly available to meet the needs of the MSSDG. The code is available at https://github.com/WJULYW/PhysMLE.},
  archive      = {J_TPAMI},
  author       = {Jiyao Wang and Hao Lu and Ange Wang and Xiao Yang and Yingcong Chen and Dengbo He and Kaishun Wu},
  doi          = {10.1109/TPAMI.2025.3545598},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-19},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {PhysMLE: Generalizable and priors-inclusive multi-task remote physiological measurement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gauging-<span class="math inline"><em>δ</em></span>: A
non-parametric hierarchical clustering algorithm. <em>TPAMI</em>, 1–12.
(<a href="https://doi.org/10.1109/TPAMI.2025.3545573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of a nonparametric and versatile clustering algorithm has been a longstanding challenge in unsupervised learning due to the exploratory nature of the clustering problem. This study presents a novel algorithm, named Gauging-$\delta$, which can handle diverse cluster shapes and operate in a nonparametric manner. The algorithm employs a hierarchical merging process that starts from individual data points until no further clusters can be merged. The central component of Gauging-$\delta$ is the adaptive mergeability function, which progressively determines if two clusters are mergeable considering the perceptual statistics of the clusters and their environment. Empirical evaluations on 105 synthetic datasets demonstrate the superiority of the proposed algorithm, particularly in accurately handling well-separated clusters. Experiments on real-world datasets highlight the impact of selecting appropriate data features and distance metrics on clustering results. The source code is available at https://github.com/design-zeng/Gauging-delta.},
  archive      = {J_TPAMI},
  author       = {Jinli Yao and Jie Pan and Yong Zeng},
  doi          = {10.1109/TPAMI.2025.3545573},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Gauging-$\delta$: A non-parametric hierarchical clustering algorithm},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Instance-consistent fair face recognition. <em>TPAMI</em>,
1–17. (<a href="https://doi.org/10.1109/TPAMI.2025.3545781">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The fairness of face recognition (FR) is a challenging issue to numerous FR algorithms in the modern pluralistic and egalitarian society. In this work, we propose an instance-consistent fair face recognition (IC-FFR) method by fulfilling complete instance fairness on false positive rate (FPR) and true positive rate (TPR). In view of the misalignment of testing and training metrics, not yet considered by the current fair FR algorithms, in theory, we inspect the correlation between the testing metrics (FPR and TPR) and the label classification loss, and we derive a high-probability consistency of unfairness penalties from FPR and TPR to the softmax loss. According to the theoretical analysis, we further develop an instance-consistent fairness solution by introducing customized instance margins, which well preserve consistent FPR and TPR of all instances during the label classification in training. To encourage more fine-grained fairness evaluation, we contribute a dataset called national faces in the world (NFW) to measure the fairness of individuals and countries. Extensive experiments on our NFW as well as the RFW and BFW benchmarks demonstrate the effectiveness and superiority of our method compared to those state-of-the-art fair FR methods.},
  archive      = {J_TPAMI},
  author       = {Yong Li and Yufei Sun and Zhen Cui and Pengcheng Shen and Shiguang Shan},
  doi          = {10.1109/TPAMI.2025.3545781},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Instance-consistent fair face recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LoCo: Low-bit communication adaptor for large-scale model
training. <em>TPAMI</em>, 1–13. (<a
href="https://doi.org/10.1109/TPAMI.2025.3544764">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To efficiently train large-scale models, low-bit gradient communication compresses full-precision gradients on local GPU nodes into low-precision ones for higher gradient synchronization efficiency among GPU nodes. However, it often degrades training quality due to compression information loss. To address this, we propose the Low-bit Communication Adaptor (LoCo), which compensates gradients on local GPU nodes before compression, ensuring efficient synchronization without compromising training quality. Specifically, LoCo designs a moving average of historical compensation errors to stably estimate concurrent compression error and then adopts it to compensate for the concurrent gradient compression, yielding a less lossless compression. This mechanism allows it to be compatible with general optimizers like Adam and sharding strategies like FSDP. Theoretical analysis shows that integrating LoCo into full-precision optimizers like Adam and SGD does not impair their convergence speed on non-convex problems. Experimental results show that across large-scale model training frameworks like Megatron-LM and PyTorch&#39;s FSDP, LoCo significantly improves communication efficiency, e.g., improving Adam&#39;s training speed by 14% to 40% without performance degradation on large language models like LLAMAs and MoEs.},
  archive      = {J_TPAMI},
  author       = {Xingyu Xie and Zhijie Lin and Kim-Chuan Toh and Pan Zhou},
  doi          = {10.1109/TPAMI.2025.3544764},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {LoCo: Low-bit communication adaptor for large-scale model training},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NER-net+: Seeing motion at nighttime with an event camera.
<em>TPAMI</em>, 1–18. (<a
href="https://doi.org/10.1109/TPAMI.2025.3545936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We focus on a very challenging task: imaging at nighttime dynamic scenes. Conventional RGB cameras struggle with the trade-off between long exposure for low-light imaging and short exposure for capturing dynamic scenes. Event cameras react to dynamic changes, with their high temporal resolution (microsecond) and dynamic range (120dB), and thus offer a promising alternative. However, existing methods are mostly based on simulated datasets due to the lack of paired event-clean image data for nighttime conditions, where the domain gap leads to performance limitations in real-world scenarios. Moreover, most existing event reconstruction methods are tailored for daytime data, overlooking issues unique to low-light events at night, such as strong noise, temporal trailing, and spatial nonuniformity, resulting in unsatisfactory reconstruction results. To address these challenges, we construct the first real paired lowlight event dataset (RLED) through a co-axial imaging system, comprising 80,400 spatially and temporally aligned image GTs and low-light events, which provides a unified training and evaluation dataset for existing methods. We further conduct a comprehensive analysis of the causes and characteristics of strong noise, temporal trailing, and spatial non-uniformity in nighttime events, and propose a nighttime event reconstruction network (NER-Net+). It includes a learnable event timestamps calibration module (LETC) to correct the temporal trailing events and a non-stationary spatio-temporal information enhancement module (NSIE) to suppress sensor noise and spatial non-uniformity. Extensive experiments demonstrate that the proposed method outperforms state-of-the-art methods in visual quality and generalization on real-world nighttime datasets.},
  archive      = {J_TPAMI},
  author       = {Haoyue Liu and Jinghan Xu and Shihan Peng and Yi Chang and Hanyu Zhou and Yuxing Duan and Lin Zhu and Yonghong Tian and Luxin Yan},
  doi          = {10.1109/TPAMI.2025.3545936},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {NER-net+: Seeing motion at nighttime with an event camera},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards a theoretical understanding of semi-supervised
learning under class distribution mismatch. <em>TPAMI</em>, 1–15. (<a
href="https://doi.org/10.1109/TPAMI.2025.3545930">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semi-supervised learning (SSL) confronts a formidable challenge under class distribution mismatch, wherein unlabeled data contain numerous categories absent in the labeled dataset. Traditional SSL methods undergo performance deterioration in such mismatch scenarios due to the invasion of those instances from unknown categories. Despite some technical efforts to enhance SSL by mitigating the invasion, the profound theoretical analysis of SSL under class distribution mismatch is still under study. Accordingly, in this work, we propose Bi-Objective Optimization Mechanism (https://www.anonymousx.store) to theoretically analyze the excess risk between the empirical optimal solution and the population-level optimal solution. Specifically, BOOM reveals that the SSL error is the essential contributor behind excess risk, resulting from both the pseudo-labeling error and invasion error. Meanwhile, BOOM unveils that the optimization objectives of SSL under mismatch are binary: high-quality pseudo-labels and adaptive weights on the unlabeled instances, which contribute to alleviating the pseudo-labeling error and the invasion error, respectively. Moreover, BOOM explicitly discovers the fundamental factors crucial for optimizing the bi-objectives, guided by which an approach is then proposed as a strong baseline for SSL under mismatch. Extensive experiments on benchmark and real datasets confirm the effectiveness of our proposed algorithm.},
  archive      = {J_TPAMI},
  author       = {Pan Du and Suyun Zhao and Puhui Tan and Zisen Sheng and Zeyu Gan and Hong Chen and Cuiping Li},
  doi          = {10.1109/TPAMI.2025.3545930},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Towards a theoretical understanding of semi-supervised learning under class distribution mismatch},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AUCPro: AUC-oriented provable robustness learning.
<em>TPAMI</em>, 1–18. (<a
href="https://doi.org/10.1109/TPAMI.2025.3545639">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The current studies of provable robustness for deep neural networks (DNNs) usually assume that the class distribution is overall balanced. However, in real-world applications especially for safety-sensitive systems, the class distribution often exhibits a long-tailed property. It is well-known that the Area Under the ROC Curve (AUC) is a more proper metric for long-tailed learning problems. Motivated by this fact, an AUC-oriented provable robustness learning framework (named AUCPro) is first proposed in this paper. The key is to construct a proxy model smoothed by the isotropic Gaussian noise and then consider optimizing the proxy model from the AUC-oriented learning point of view. Theoretically, we provide a certified safety region for AUCPro within which the model would be free from the $\ell _{2}$ adversarial attacks. Most importantly, we propose a novel standard to theoretically study the robustness generalization toward unseen data for provable robustness learning approaches. To the best of our knowledge, such a problem remains barely considered in the machine learning community. To be specific, under a general principle for performance-robustness trade-off, we prove that the generalization ability of the resulting model could be equivalently expressed as the expected adversarial risk of AUC under $\ell _{2}$ perturbation. On top of this, we present two practical settings to explore the excess risk formed by the difference between the empirical risk of AUCPro and the derived generalization performance. Finally, comprehensive experiments speak to the efficacy of our proposed algorithm.},
  archive      = {J_TPAMI},
  author       = {Shilong Bao and Qianqian Xu and Zhiyong Yang and Yuan He and Xiaochun Cao and Qingming Huang},
  doi          = {10.1109/TPAMI.2025.3545639},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {AUCPro: AUC-oriented provable robustness learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Gait recognition in the wild: A large-scale benchmark and
NAS-based baseline. <em>TPAMI</em>, 1–18. (<a
href="https://doi.org/10.1109/TPAMI.2025.3546482">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait benchmarks empower the research community to train and evaluate high-performance gait recognition systems. Even though growing efforts have been devoted to cross-view recognition, academia is restricted by current existing databases captured in the controlled environment. In this paper, we contribute a new benchmark and strong baseline for Gait REcognition in the Wild (GREW). The GREW dataset is constructed from natural videos, which contain hundreds of cameras and thousands of hours of streams in open systems. With tremendous manual annotations, the GREW consists of 26K identities and 128K sequences with rich attributes for unconstrained gait recognition. Moreover, we add a distractor set of over 233K sequences, making it more suitable for real-world applications. Compared with prevailing predefined cross-view datasets, the GREW has diverse and practical view variations, as well as more naturally challenging factors. To the best of our knowledge, this is the first large-scale dataset for gait recognition in the wild. Equipped with this benchmark, we dissect the unconstrained gait recognition problem, where representative appearance-based and model-based methods are explored. The proposed GREW benchmark proves to be essential for both training and evaluating gait recognizers in unconstrained scenarios. In addition, we propose the Single Path One-Shot neural architecture search with uniform sampling for Gait recognition, named SPOSGait, which is the first NAS-based gait recognition model. In experiments, SPOSGait achieves state-of-the-art performance on the CASIA-B, OU-MVLP, Gait3D, and GREW benchmarks, outperforming existing approaches by a large margin. The code will be released at https://github.com/XiandaGuo/SPOSGait.},
  archive      = {J_TPAMI},
  author       = {Xianda Guo and Zheng Zhu and Tian Yang and Beibei Lin and Junjie Huang and Jiankang Deng and Guan Huang and Jie Zhou and Jiwen Lu},
  doi          = {10.1109/TPAMI.2025.3546482},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Gait recognition in the wild: A large-scale benchmark and NAS-based baseline},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Attack as defense: Proactive adversarial multi-modal
learning to evade retrieval. <em>TPAMI</em>, 1–17. (<a
href="https://doi.org/10.1109/TPAMI.2025.3546334">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With growing concerns about information security, protecting the privacy of user-sensitive data has become crucial. The rapid development of multi-modal retrieval technologies poses new threats, making sensitive data more vulnerable to leakage and malicious mining. To address this, we introduce a Proactive Adversarial Multi-modal Learning (PAML) approach that transforms sensitive data into adversarial counterparts, evading malicious multi-modal retrieval and ensuring privacy. Our method starts by sending queries to a knowledge-agnostic retrieval system and analyzing the results to understand the retrieval feedback mechanism. Using a U-Net-based diffusion model, we create a semantic perturbation network that subtly alters the implicit semantics of sensitive data. This, combined with multi-modal retrieved results and random noise, shifts the data&#39;s semantics towards outliers, preventing retrieval as neighbors to relevant queries. Additionally, a discriminator and pre-trained model enhance the visual realism and outlier generalization of protected data. Extensive experiments show that PAML outperforms potential baselines in data privacy protection. Ablation analysis validates each component&#39;s effectiveness, and our approach&#39;s variants are applicable to diverse retrieval systems. Source codes and datasets are available at https://github.com/tswang0116/PAML.},
  archive      = {J_TPAMI},
  author       = {Fengling Li and Tianshi Wang and Lei Zhu and Jingjing Li and Heng Tao Shen},
  doi          = {10.1109/TPAMI.2025.3546334},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Attack as defense: Proactive adversarial multi-modal learning to evade retrieval},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning probabilistic presence-absence evidence for
weakly-supervised audio-visual event perception. <em>TPAMI</em>, 1–15.
(<a href="https://doi.org/10.1109/TPAMI.2025.3546312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With only video-level event labels, this paper targets at the task of weakly-supervised audio-visual event perception (WS-AVEP), which aims to temporally localize and categorize events that belong to each modality. Despite the recent progress, most existing approaches either ignore the unsynchronized property of audio-visual tracks or discount the complementary modality for explicit enhancement. We argue that, a modality should provide ample presence evidence for an event, while the complementary modality offers absence evidence as a reference. However, to learn reliable evidence, we face challenging uncertainties caused by weak supervision and the complicated audio-visual data itself. To this end, we propose to collect Probabilistic Presence-Absence Evidence (PPAE) in a unified framework. Specifically, by leveraging uni-modal and cross-modal representations, a probabilistic presence-absence evidence collector (PAEC) is designed. To learn the evidence in a reliable range, we propose a joint-modal mutual learning (JML) process, which calibrates the evidence of diverse audible, visible, and audi-visible events adaptively and dynamically. Extensive experiments show that our method surpasses state-of-the-arts (e.g., absolute gains of $3.1\%$ and $4.2\%$ in terms of event-level audio and visual metrics on the LLP dataset).},
  archive      = {J_TPAMI},
  author       = {Junyu Gao and Mengyuan Chen and Changsheng Xu},
  doi          = {10.1109/TPAMI.2025.3546312},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Learning probabilistic presence-absence evidence for weakly-supervised audio-visual event perception},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MOVE: Effective and harmless ownership verification via
embedded external features. <em>TPAMI</em>, 1–18. (<a
href="https://doi.org/10.1109/TPAMI.2025.3546223">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, deep neural networks (DNNs) are widely adopted in different applications. Despite its commercial values, training a well-performing DNN is resource-consuming. Accordingly, the well-trained model is valuable intellectual property for its owner. However, recent studies revealed the threats of model stealing, where the adversaries can obtain a function-similar copy of the victim model, even when they can only query the model. In this paper, we propose an effective and harmless model ownership verification (MOVE) to defend against different types of model stealing simultaneously, without introducing new security risks. In general, we conduct the ownership verification by verifying whether a suspicious model contains the knowledge of defender-specified external features. Specifically, we embed the external features by modifying a few training samples with style transfer. We then train a meta-classifier to determine whether a model is stolen from the victim. This approach is inspired by the understanding that the stolen models should contain the knowledge of features learned by the victim model. In particular, we develop our MOVE method under both white-box and black-box settings and analyze its theoretical foundation to provide comprehensive model protection. Extensive experiments on benchmark datasets verify the effectiveness of our method and its resistance to potential adaptive attacks. The codes for reproducing the main experiments of our method are available at https://github.com/THUYimingLi/MOVE.},
  archive      = {J_TPAMI},
  author       = {Yiming Li and Linghui Zhu and Xiaojun Jia and Yang Bai and Yong Jiang and Shu-Tao Xia and Xiaochun Cao and Kui Ren},
  doi          = {10.1109/TPAMI.2025.3546223},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {MOVE: Effective and harmless ownership verification via embedded external features},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reliable representation learning for incomplete multi-view
missing multi-label classification. <em>TPAMI</em>, 1–17. (<a
href="https://doi.org/10.1109/TPAMI.2025.3546356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a cross-topic of multi-view learning and multi-label classification, multi-view multi-label classification has gradually gained traction in recent years. The application of multi-view contrastive learning has further facilitated this process; however, the existing multi-view contrastive learning methods crudely separate the so-called negative pair, which largely results in the separation of samples belonging to the same category or similar ones. Besides, plenty of multi-view multi-label learning methods ignore the possible absence of views and labels. To address these issues, in this paper, we propose an incomplete multi-view missing multi-label classification network named RANK. In this network, a label-driven multi-view contrastive learning strategy is proposed to leverage supervised information to preserve the intra-view structure and perform the cross-view consistency alignment. Furthermore, we break through the view-level weights inherent in existing methods and propose a quality-aware subnetwork to dynamically assign quality scores to each view of each sample. The label correlation information is fully utilized in the final multi-label cross-entropy classification loss, effectively improving the discriminative power. Last but not least, our model is not only able to handle complete multi-view multi-label data, but also works on datasets with missing instances and labels. Extensive experiments confirm that our RANK outperforms existing state-of-the-art methods.},
  archive      = {J_TPAMI},
  author       = {Chengliang Liu and Jie Wen and Yong Xu and Bob Zhang and Liqiang Nie and Min Zhang},
  doi          = {10.1109/TPAMI.2025.3546356},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Reliable representation learning for incomplete multi-view missing multi-label classification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DFedADMM: Dual constraint controlled model inconsistency for
decentralize federated learning. <em>TPAMI</em>, 1–12. (<a
href="https://doi.org/10.1109/TPAMI.2025.3546659">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {To address the communication burden issues associated with Federated Learning (FL), Decentralized Federated Learning (DFL) discards the central server and establishes a decentralized communication network, where each client communicates only with neighboring clients. However, existing DFL methods still suffer from two major challenges: local inconsistency and local heterogeneous overfitting, which existing DFL methods have not fundamentally addressed. To tackle these issues, we propose novel DFL algorithms, DFedADMM and its enhanced version DFedADMM-SAM, to improve the performance for DFL. The DFedADMM algorithm employs primal-dual optimization (ADMM) by utilizing dual variables to control the model inconsistency raised from the decentralized heterogeneous data distributions. The DFedADMM-SAM algorithm further improves on DFedADMM by employing a Sharpness-Aware Minimization (SAM) optimizer, which uses gradient perturbations to generate locally flat models and searches for models with uniformly low loss values to mitigate local heterogeneous overfitting. Theoretically, we derive convergence rates of $\mathcal {O}(\frac{1}{\sqrt{KT}}+\frac{1}{KT(1-\psi )^{2}})$ and $ \mathcal {O}(\frac{1}{\sqrt{KT}}+\frac{1}{KT(1-\psi )^{2}}+ \frac{1}{T^{3/2}K^{1/2}})$ in the non-convex setting for DFedADMM and DFedADMM-SAM, respectively, where $1 - \psi$ represents the spectral gap of the gossip matrix. Empirically, extensive experiments on MNIST, CIFAR10, and CIFAR100 datasets demonstrate that our algorithms exhibit superior performance in terms of generalization, convergence speed, and communication overhead compared to existing state-of-the-art (SOTA) optimizers in DFL.},
  archive      = {J_TPAMI},
  author       = {Qinglun Li and Li Shen and Guanghao Li and Quanjun Yin and Dacheng Tao},
  doi          = {10.1109/TPAMI.2025.3546659},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {DFedADMM: Dual constraint controlled model inconsistency for decentralize federated learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computational imaging for long-term prediction of solar
irradiance. <em>TPAMI</em>, 1–12. (<a
href="https://doi.org/10.1109/TPAMI.2025.3546571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The occlusion of the sun by clouds is one of the primary sources of uncertainties in solar power generation, and is a factor that affects the wide-spread use of solar power as a primary energy source. Real-time forecasting of cloud movement and, as a result, solar irradiance is necessary to schedule and allocate energy across grid-connected photovoltaic systems. Previous works monitored cloud movement using wide-angle field of view imagery of the sky. However, such images have poor resolution for clouds that appear near the horizon, which reduces their effectiveness for long term prediction of solar occlusion. Specifically, to be able to predict occlusion of the sun over long time periods, clouds that are near the horizon need to be detected, and their velocities estimated precisely. To enable such a system, we design and deploy a catadioptric system that delivers wide-angle imagery with uniform spatial resolution of the sky over its field of view. To enable prediction over a longer time horizon, we design an algorithm that uses carefully selected spatio-temporal slices of the imagery using estimated wind direction and velocity as inputs. Using ray-tracing simulations as well as a real testbed deployed outdoors, we show that the system is capable of predicting solar occlusion as well as irradiance for tens of minutes in the future, which is an order of magnitude improvement over prior work.},
  archive      = {J_TPAMI},
  author       = {Leron K. Julian and Haejoon Lee and Soummya Kar and Aswin C. Sankaranarayanan},
  doi          = {10.1109/TPAMI.2025.3546571},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Computational imaging for long-term prediction of solar irradiance},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025b). A generalized tensor formulation for hyperspectral image
super-resolution under general spatial blurring. <em>TPAMI</em>, 1–15.
(<a href="https://doi.org/10.1109/TPAMI.2025.3545605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hyperspectral super-resolution is commonly accomplished by the fusing of a hyperspectral imaging of low spatial resolution with a multispectral image of high spatial resolution, and many tensor-based approaches to this task have been recently proposed. Yet, it is assumed in such tensor-based methods that the spatial-blurring operation that creates the observed hyperspectral image from the desired super-resolved image is separable into independent horizontal and vertical blurring. Recent work has argued that such separable spatial degradation is ill-equipped to model the operation of real sensors which may exhibit, for example, anisotropic blurring. To accommodate this fact, a generalized tensor formulation based on a Kronecker decomposition is proposed to handle any general spatial-degradation matrix, including those that are not separable as previously assumed. Analysis of the generalized formulation reveals conditions under which exact recovery of the desired super-resolved image is guaranteed, and a practical algorithm for such recovery, driven by a blockwise-group-sparsity regularization, is proposed. Extensive experimental results demonstrate that the proposed generalized tensor approach outperforms not only traditional matrix-based techniques but also state-of-the-art tensor-based methods; the gains with respect to the latter are especially significant in cases of anisotropic spatial blurring.},
  archive      = {J_TPAMI},
  author       = {Yinjian Wang and Wei Li and Yuanyuan Gui and Qian Du and James E. Fowler},
  doi          = {10.1109/TPAMI.2025.3545605},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {A generalized tensor formulation for hyperspectral image super-resolution under general spatial blurring},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unified prompt attack against text-to-image generation
models. <em>TPAMI</em>, 1–18. (<a
href="https://doi.org/10.1109/TPAMI.2025.3545652">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Text-to-Image (T2I) models have advanced significantly, but their growing popularity raises security concerns due to their potential to generate harmful images. To address these issues, we propose UPAM, a novel framework to evaluate the robustness of T2I models from an attack perspective. Unlike prior methods that focus solely on textual defenses, UPAM unifies the attack on both textual and visual defenses. Additionally, it enables gradient-based optimization, overcoming reliance on enumeration for improved efficiency and effectiveness. To handle cases where T2I models block image outputs due to defenses, we introduce Sphere-Probing Learning (SPL) to enable optimization even without image results. Following SPL, our model bypasses defenses, inducing the generation of harmful content. To ensure semantic alignment with attacker intent, we propose Semantic-Enhancing Learning (SEL) for precise semantic control. UPAM also prioritizes the naturalness of adversarial prompts using In-context Naturalness Enhancement (INE), making them harder for human examiners to detect. Additionally, we address the issue of iterative queries–common in prior methods and easily detectable by API defenders–by introducing Transferable Attack Learning (TAL), allowing effective attacks with minimal queries. Extensive experiments validate UPAM&#39;s superiority in effectiveness, efficiency, naturalness, and low query detection rates.},
  archive      = {J_TPAMI},
  author       = {Duo Peng and Qiuhong Ke and Mark He Huang and Ping Hu and Jun Liu},
  doi          = {10.1109/TPAMI.2025.3545652},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Unified prompt attack against text-to-image generation models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Replay without saving: Prototype derivation and distribution
rebalance for class-incremental semantic segmentation. <em>TPAMI</em>,
1–18. (<a href="https://doi.org/10.1109/TPAMI.2025.3545966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The research of class-incremental semantic segmentation (CISS) seeks to enhance semantic segmentation methods by enabling the progressive learning of new classes while preserving knowledge of previously learned ones. A significant yet often neglected challenge in this domain is class imbalance. In CISS, each task focuses on different foreground classes, with the training set for each task exclusively comprising images that contain these currently focused classes. This results in an overrepresentation of these classes within the single-task training set, leading to a classification bias towards them. To address this issue, we propose a novel CISS method named STAR, whose core principle is to reintegrate the missing proportions of previous classes into current single-task training samples by replaying their prototypes. Moreover, we develop a prototype deviation technique that enables the deduction of past-class prototypes, integrating the recognition patterns of the classifiers and the extraction patterns of the feature extractor. With this technique, replay can be accomplished without using any storage to save prototypes. Complementing our method, we devise two loss functions to enforce cross-task feature constraints: the Old-Class Features Maintaining (OCFM) loss and the Similarity-Aware Discriminative (SAD) loss. The OCFM loss is designed to stabilize the feature space of old classes, thus preserving previously acquired knowledge without compromising the ability to learn new classes. The SAD loss aims to enhance feature distinctions between similar old and new class pairs, minimizing potential confusion. Our experiments on two public datasets, Pascal VOC 2012 and ADE20 K, demonstrate that our STAR achieves state-of-the-art performance.},
  archive      = {J_TPAMI},
  author       = {Jinpeng Chen and Runmin Cong and Yuxuan Luo and Horace Ho Shing Ip and Sam Kwong},
  doi          = {10.1109/TPAMI.2025.3545966},
  journal      = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Pattern Anal. Mach. Intell.},
  title        = {Replay without saving: Prototype derivation and distribution rebalance for class-incremental semantic segmentation},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
