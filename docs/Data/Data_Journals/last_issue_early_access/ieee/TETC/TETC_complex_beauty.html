<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TETC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tetc---13">TETC - 13</h2>
<ul>
<li><details>
<summary>
(2025). Energy efficient approximate computing framework for DNN
acceleration using a probabilistic-oriented method. <em>TETC</em>, 1–13.
(<a href="https://doi.org/10.1109/TETC.2024.3522307">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Approximate computing (AxC) has recently emerged as a successful approach for optimizing energy consumption in error-tolerant applications, such as deep neural networks (DNNs). The enormous model size and high computation cost of DNNs present significant challenges for deployment in energy-efficient and resource-constrained computing systems. Emerging DNN hardware accelerators based on AxC designs selectively approximate the non-critical segments of computation to address these challenges. However, a systematic and principled approach that incorporates domain knowledge and approximate hardware for optimal approximation is still lacking. In this paper, we propose a probabilistic-oriented AxC (PAxC) framework that provides high energy savings with acceptable quality by considering the overall probability effect of approximation. To achieve aggressive approximate designs, we utilize the minimum likelihood error to determine the AxC synergy profile at both application and circuit levels. This enables effective coordination of the trade-off between energy and accuracy. Compared with a baseline design, the power-delay product (PDP) is significantly reduced by up to 83.66% with an acceptable accuracy reduction. Simulation and a case study of the image process validate the effectiveness of the proposed framework.},
  archive      = {J_TETC},
  author       = {Pengfei Huang and Ke Chen and Chenghua Wang and Weiqiang Liu},
  doi          = {10.1109/TETC.2024.3522307},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Energy efficient approximate computing framework for DNN acceleration using a probabilistic-oriented method},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Improving deep neural network reliability via
transient-fault-aware design and training. <em>TETC</em>, 1–12. (<a
href="https://doi.org/10.1109/TETC.2024.3520672">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep Neural Networks (DNNs) have revolutionized several fields, including safety- and mission-critical applications, such as autonomous driving and space exploration. However, recent studies have highlighted that transient hardware faults can corrupt the model&#39;s output, leading to high misprediction probabilities. Since traditional reliability strategies, based on modular hardware, software replications, or matrix multiplication checksum impose a high overhead, there is a pressing need for efficient and effective hardening solutions tailored for DNNs. In this paper we present several network design choices and a training procedure that increase the robustness of standard deep models and thoroughly evaluate these strategies with experimental analyses on vision classification tasks. We name DieHardNet the specialized DNN obtained by applying all our hardening techniques that combine knowledge from experimental hardware faults characterization and machine learning studies. We conduct extensive ablation studies to quantify the reliability gain of each hardening component in DieHardNet. We perform over 10,000 instruction-level fault injections to validate our approach and expose DieHardNet executed on GPUs to an accelerated neutron beam equivalent to more than 570,000 years of natural radiation. Our evaluation demonstrates that DieHardNet can reduce the critical error rate (i.e., errors that modify the inference) up to 100 times compared to the unprotected baseline model, without causing any increase in inference time.},
  archive      = {J_TETC},
  author       = {Fernando Fernandes dos Santos and Niccolo Cavagnero and Marco Ciccone and Giuseppe Averta and Angeliki Kritikakou and Olivier Sentieys and Paolo Rech and Tatiana Tommasi},
  doi          = {10.1109/TETC.2024.3520672},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Improving deep neural network reliability via transient-fault-aware design and training},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FHEmem: A processing in-memory accelerator for fully
homomorphic encryption. <em>TETC</em>, 1–16. (<a
href="https://doi.org/10.1109/TETC.2025.3528862">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fully Homomorphic Encryption (FHE) is a technique that allows arbitrary computations to be performed on encrypted data without the need for decryption, making it ideal for secure computation outsourcing. However, computation on FHE-encrypted data is significantly slower than that on plain data, primarily due to the explosive increases in data size and computation complexity after encryption. To enable real-world FHE applications, recent research has proposed several custom hardware accelerators that provide orders of magnitude speedup over conventional systems. However, the performance of existing FHE accelerators is severely bounded by memory bandwidth, even with expensive on-chip buffers. Processing In-Memory (PIM) is a promising technology that can accelerate data-intensive workloads with extensive internal bandwidth. Unfortunately, existing PIM accelerators cannot efficiently support FHE due to the limited throughput to support FHE&#39;s complex computing and data movement operations. To tackle such challenges, we propose FHEmem, an FHE accelerator using a novel PIM architecture for high- throughput FHE acceleration. Furthermore, we present an optimized end-to-end processing flow with an automated mapping framework to maximize the hardware utilization of FHEmem. Our evaluation shows that FHEmem achieves at least 4.0× speedup and 6.9× energy-delay-area efficiency improvement over state-of-the-art FHE accelerators on popular FHE applications.},
  archive      = {J_TETC},
  author       = {Minxuan Zhou and Yujin Nam and Pranav Gangwar and Weihong Xu and Arpan Dutta and Chris Wilkerson and Rosario Cammarota and Saransh Gupta and Tajana Rosing},
  doi          = {10.1109/TETC.2025.3528862},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {FHEmem: A processing in-memory accelerator for fully homomorphic encryption},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Scatter-gather DMA performance analysis within an SoC-based
control system for trapped-ion quantum computing. <em>TETC</em>, 1–12.
(<a href="https://doi.org/10.1109/TETC.2025.3528899">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Scatter-gather dynamic-memory-access (SG-DMA) is utilized in applications that require high bandwidth and low latency data transfers between memory and peripherals, where data blocks, described using buffer descriptors (BDs), are distributed throughout the memory system. The data transfer organization and requirements of a Trapped-Ion Quantum Computer (TIQC) possess characteristics similar to those targeted by SGDMA. In particular, the ion qubits in a TIQC are manipulated by applying control sequences consisting primarily of modulated laser pulses. These optical pulses are defined by parameters that are (re)configured by the electrical control system. Variations in the operating environment and equipment make it necessary to create and run a wide range of control sequence permutations, which can be well represented as BD regions distributed across the main memory. In this paper, we experimentally evaluate the latency and throughput of SG-DMA on Xilinx radiofrequency SoC (RFSoC) devices under a variety of BD and payload sizes as a means of determining the benefits and limitations of an RFSoC system architecture for TIQC applications.},
  archive      = {J_TETC},
  author       = {Tiamike Dudley and Jim Plusquellic and Eirini Eleni Tsiropoulou and Joshua Goldberg and Daniel Stick and Daniel Lobser},
  doi          = {10.1109/TETC.2025.3528899},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Scatter-gather DMA performance analysis within an SoC-based control system for trapped-ion quantum computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bi-layer deep learning with neural networks and large
language models for cognitive biometrics. <em>TETC</em>, 1–13. (<a
href="https://doi.org/10.1109/TETC.2025.3528874">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The paper focuses on cognitive biometrics and their application in user authentication. To be more precise, this work aims at investigating if the inference logic of Neural Networks can be combined with the contextual analysis of Large Language Models to create a composite metric, able to accurately reflect a user&#39;s Keystroke Dynamic and text-based Stylometrics in response to an arbitrary visual stimulus. The created artefact showed that this strategy is viable for real-world usage, having tested the solution on real participants and industry-standard datasets. In some cases, the performance testing showed superior accuracy and speed in some performance metrics compared to other contemporary solutions that exist in this area.},
  archive      = {J_TETC},
  author       = {Matthew Swann and Stavros Shiaeles and Nicholas Kolokotronis},
  doi          = {10.1109/TETC.2025.3528874},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Bi-layer deep learning with neural networks and large language models for cognitive biometrics},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A pervasive edge computing model for proactive intelligent
data migration. <em>TETC</em>, 1–12. (<a
href="https://doi.org/10.1109/TETC.2025.3528994">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Currently, there is a great attention of the research community for the intelligent management of data in a context-aware manner at the intersection of the Internet of Things (IoT) and Edge Computing (EC). In this paper, we propose a strategy to be adopted by autonomous edge nodes related to their decision on what data should be migrated to specific locations of the infrastructure and support the desired requests for processing. Our intention is to arm nodes with the ability of learning the access patterns of offloaded data-driven tasks and predict which data should be migrated to the original ‘owners’ of tasks. Naturally, these tasks are linked to the processing of data that are absent at the original hosting nodes indicating the required data assets that need to be accessed directly. To identify these data intervals, we employ an ensemble scheme that combines a statistically oriented model and a machine learning scheme. Hence, we are able not only to detect the density of the requests but also to learn and infer the ‘strong’ data assets. The proposed approach is analyzed in detail by presenting the corresponding formulations being also evaluated and compared against baselines and models found in the respective literature.},
  archive      = {J_TETC},
  author       = {Georgios Boulougaris and Kostas Kolomvatsos},
  doi          = {10.1109/TETC.2025.3528994},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {A pervasive edge computing model for proactive intelligent data migration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BCIM: Constant-time and high-throughput
block-cipher-in-memory with massively-parallel bit-serial execution.
<em>TETC</em>, 1–13. (<a
href="https://doi.org/10.1109/TETC.2025.3529842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In-memory computing (IMC) emerges as one of the most promising computing technologies for data-intensive applications to ameliorate the “memory wall” bottleneck in von Neumann computer systems. Meanwhile, IMC also shows promising prospects towards high- throughput and energy-efficient processing of cryptographic workloads. This paper presents Block-Cipher-In-Memory (BCIM), a constant-time, high- throughput, bit-serial in-memory cryptography scheme to support versatile Substitution-Permutation and Feistel network based block ciphers, such as standard ciphers like Advanced Encryption Standard (AES), and lightweight block ciphers like RECTANGLE and Simon. In addition, BCIM employs a processor-assisted key loading scheme and prudent memory management strategies to minimize the memory footprint needed for cryptographic algorithms to improve the peak operating frequency and energy efficiency. Built upon these, BCIM can also support alternative block cipher modes of operation like counter mode beyond electronic-codebook. Furthermore, the bit-serial operation of BCIM inherently ensures constant-time execution and exploits column- wise single instruction multiple data (SIMD) processing, thereby providing strong resistance to side-channel timing attacks, and achieves high- throughput encryption and decryption via massively-parallel compact round function implementation. Experimental results suggest that BCIM shows substantial performance and energy improvements over state-of-the-art bit-parallel IMC ciphers. Additionally, BCIM show competitive performance and orders of magnitude energy advantages over the bitsliced software implementations on MCU/CPU platforms. BCIM is available at https://github.com/adervay1/BCIM and open source.},
  archive      = {J_TETC},
  author       = {Andrew Dervay and Wenfeng Zhao},
  doi          = {10.1109/TETC.2025.3529842},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {BCIM: Constant-time and high-throughput block-cipher-in-memory with massively-parallel bit-serial execution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Software-defined number formats for high-speed belief
propagation. <em>TETC</em>, 1–14. (<a
href="https://doi.org/10.1109/TETC.2025.3528972">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper presents the design and implementation of Software-Defined Floating-Point (SDF) number formats for high-speed implementation of the Belief Propagation (BP) algorithm. SDF formats are designed specifically to meet the numeric needs of the computation and are more compact representations of the data. They reduce memory footprint and memory bandwidth requirements without sacrificing accuracy, given that BP for loopy graphs inherently involves algorithmic errors. This paper designs several SDF formats for sum-product BP applications by careful analysis of the computation. Our theoretical analysis leads to the design of 16-bit (half-precision) and 8-bit (mini-precision) widths. We moreover present highly efficient software implementation of the proposed SDF formats which is centered around conversion to hardware-supported single-precision arithmetic hardware. Our solution demonstrates negligible conversion overhead on commercially available CPUs. For Ising grids with sizes from 100×100 to 500×500, the 16- and 8-bit SDF formats along with our conversion module produce equivalent accuracy to double-precision floating-point format but with 2.86× speedups on average on an Intel Xeon processor. Particularly, increasing the grid size results in higher speed-up. For example, the proposed half-precision format with 3-bit exponent and 13-bit mantissa achieved the minimum and maximum speedups of 1.30× and 1.39× over single-precision, and 2.55× and 3.40× over double-precision, by increasing grid size from 100×100 to 500×500.},
  archive      = {J_TETC},
  author       = {Amir Sabbagh Molahosseini and JunKyu Lee and Hans Vandierendonck},
  doi          = {10.1109/TETC.2025.3528972},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Software-defined number formats for high-speed belief propagation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continual test-time adaptation with weighted contrastive
learning and pseudo-label correction. <em>TETC</em>, 1–12. (<a
href="https://doi.org/10.1109/TETC.2025.3528985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time adaptability is often required to maintain system accuracy in scenarios involving domain shifts caused by constantly changing environments. While continual test-time adaptation has been proposed to handle such scenarios, existing methods rely on high-accuracy pseudo-labels. Moreover, contrastive learning methods for continuous test-time adaptation consider the aggregation of features from the same class while neglecting the problem of aggregating similar features within the same class. Therefore, we propose “Weighted Contrastive Learning” and apply it to both pre-training and continual test-time adaptation. To address the issue of catastrophic forgetting caused by continual adaptation, previous studies have employed source-domain knowledge to stochastically recover the target-domain model. However, significant domain shifts may cause the source-domain knowledge to behave as noise, thus impacting the model&#39;s adaptability. Therefore, we propose “Domain-aware Pseudo-label Correction” to mitigate catastrophic forgetting and error accumulation without accessing the original source-domain data while minimizing the impact on model adaptability. The thorough evaluations in our experiments have demonstrated the effectiveness of our proposed approach.},
  archive      = {J_TETC},
  author       = {Shih-Chieh Chuang and Ching-Hu Lu},
  doi          = {10.1109/TETC.2025.3528985},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Continual test-time adaptation with weighted contrastive learning and pseudo-label correction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Processing in-memory PUF watermark embedding with cellular
memristor network. <em>TETC</em>, 1–12. (<a
href="https://doi.org/10.1109/TETC.2025.3528336">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The cellular neural network (CNN or CeNN) is known to be useful because of its suitability in real-time processing, parallel processing, robustness, flexibility, and energy efficiency. CeNNs have a large number of interconnected processing elements, which can be programmed to produce a wide range of patterns, including regular and irregular patterns, random patterns, and more. When implemented in memristive hardware, the pattern generator ability and inherent variability of memristive devices can be explored to create Physical Unclonable Functions (PUFs). This work reports a method of using memristive CeNNs to perform image processing tasks along with PUF image generation. The CeNN-PUF has dual mode capability combining data processing and encryption using PUF image watermarking. The proposed method provides unique device-specific image watermarks, following a two-stage process of (1) device-specific secret mask generation and (2) watermark embedding. The system is evaluated using multiple CeNN cloning templates and the robustness of the method is validated against ML attacks. A detailed analysis is presented to evaluate the uniqueness, randomness and reliability against different environmental changes. The experimental validation of the proposed model is done on FPGA Xilinx Zynq-7010 processor and benchmarked the system against quantization noise.},
  archive      = {J_TETC},
  author       = {Alex James and Chithra Reghuvaran and Leon Chua},
  doi          = {10.1109/TETC.2025.3528336},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Processing in-memory PUF watermark embedding with cellular memristor network},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards label-efficient deep learning-based aging-related
bug prediction with spiking convolutional neural networks.
<em>TETC</em>, 1–16. (<a
href="https://doi.org/10.1109/TETC.2025.3531051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advances in Deep Learning (DL) have enhanced Aging-Related Bug (ARB) prediction for mitigating software aging. However, DL-based ARB prediction models face a dual challenge: overcoming overfitting to enhance generalization and managing the high labeling costs associated with extensive data requirements. To address the first issue, we utilize the sparse and binary nature of spiking communication in Spiking Neural Networks (SNNs), which inherently provides brain-inspired regularization to effectively alleviate overfitting. Therefore, we propose a Spiking Convolutional Neural Network (SCNN)-based ARB prediction model along with a training framework that handles the model&#39;s spatial-temporal dynamics and non-differentiable nature. To reduce labeling costs, we introduce a Bio-inspired and Diversity-aware Active Learning framework (BiDAL), which prioritizes highly informative and diverse samples, enabling more efficient usage of the limited labeling budget. This framework incorporates bio-inspired uncertainty to enhance informativeness measurement along with using a diversity-aware selection strategy based on clustering to prevent redundant labeling. Experiments on three ARB datasets show that ARB-SCNN effectively reduces overfitting, improving generalization performance by 6.65% over other DL-based classifiers. Additionally, BiDAL boosts label efficiency for ARB-SCNN training, outperforming four state-of-the-art active learning methods by 4.77% within limited labeling budgets.},
  archive      = {J_TETC},
  author       = {Yunzhe Tian and Yike Li and Kang Chen and Zhenguo Zhang and Endong Tong and Jiqiang Liu and Fangyun Qin and Zheng Zheng and Wenjia Niu},
  doi          = {10.1109/TETC.2025.3531051},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Towards label-efficient deep learning-based aging-related bug prediction with spiking convolutional neural networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SiPT: Signature-based predictive testing of RRAM crossbar
arrays for deep neural networks. <em>TETC</em>, 1–15. (<a
href="https://doi.org/10.1109/TETC.2025.3533895">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Resistive Random-Access Memory (RRAM) crossbar array-based Deep Neural Networks (DNNs) are increasingly attractive for implementing ultra-low-power computing for AI. However, RRAM-based DNNs face inherent challenges from manufacturing process variability, which can compromise their performance (classification accuracy) and functional safety. One way to test these DNNs is to apply the exhaustive set of test images to each DNN to ascertain its performance; however, this is expensive and time-consuming. We propose a signature-based predictive testing (SiPT) in which a small subset of test images is applied to each DNN and the classification accuracy of the DNN is predicted directly from observations of the intermediate and final layer outputs of the network. This saves the test cost while allowing binning of RRAM-based DNNs for performance. To further improve the test efficiency of SiPT, we create the optimized compact set of test images, leveraging image filters and enhancements to synthesize images and develop a cascaded test structure, incorporating multiple sets of SiPT modules trained on compact test subsets of varying sizes. Through experimentation across diverse test cases, we demonstrate the viability of our SiPT framework under the RRAM process variations, showing test efficiency improvements up to 48X over testing with the exhaustive image dataset.},
  archive      = {J_TETC},
  author       = {Kwondo Ma and Anurup Saha and Chandramouli Amarnath and Abhijit Chatterjee},
  doi          = {10.1109/TETC.2025.3533895},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {SiPT: Signature-based predictive testing of RRAM crossbar arrays for deep neural networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploiting entity information for robust prediction over
event knowledge graphs. <em>TETC</em>, 1–12. (<a
href="https://doi.org/10.1109/TETC.2025.3534243">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Script event prediction is the task of predicting the subsequent event given a sequence of events that already took place. It benefits task planning and process scheduling for event-centric systems including enterprise systems, IoT systems, etc. Sequence-based and graph-based learning models have been applied to this task. However, when learning data is limited, especially in a multiple-participant-involved enterprise environment, the performance of such models falls short of expectations as they heavily rely on large-scale training data. To take full advantage of given data, in this paper we propose a new type of knowledge graph (KG) that models not just events but also entities participating in the events, and we design a collaborative event prediction model exploiting such KGs. Our model identifies semantically similar vertices as collaborators to resolve unknown events, applies gated graph neural networks to extract event-wise sequential features, and exploits a heterogeneous attention network to cope with entity-wise influence in event sequences. To verify the effectiveness of our approach, we designed multiple-choice narrative cloze tasks with inadequate knowledge. Our experimental evaluation with three datasets generated from well-known corpora shows our method can successfully defend against such incompleteness of data and outperforms the state-of-the-art approaches for event prediction.},
  archive      = {J_TETC},
  author       = {Han Yu and Hongming Cai and Shengtung Tsai and Mengyao Li and Pan Hu and Jiaoyan Chen and Bingqing Shen},
  doi          = {10.1109/TETC.2025.3534243},
  journal      = {IEEE Transactions on Emerging Topics in Computing},
  month        = {1},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Emerg. Topics Comput.},
  title        = {Exploiting entity information for robust prediction over event knowledge graphs},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
