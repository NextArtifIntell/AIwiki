<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tc---28">TC - 28</h2>
<ul>
<li><details>
<summary>
(2025). PQNTRU: Acceleration of NTRU-based schemes via customized
post-quantum processor. <em>TC</em>, 1–14. (<a
href="https://doi.org/10.1109/TC.2025.3540647">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Post-quantum cryptography (PQC) has rapidly evolved in response to the emergence of quantum computers, with the US National Institute of Standards and Technology (NIST) selecting four finalist algorithms for PQC standardization in 2022, including the Falcon digital signature scheme. Hawk is currently the only lattice-based candidate in NIST Round 2 additional signatures. Falcon and Hawk are based on the NTRU lattice, offering compact signatures, fast generation, and verification suitable for deployment on resource-constrained Internet-of-Things (IoT) devices. Despite the popularity of ML-DSA and ML-KEM, research on NTRU-based schemes has been limited due to their complex algorithms and operations. Falcon and Hawk’s performance remains constrained by the lack of parallel execution in crucial operations like the Number Theoretic Transform (NTT) and Fast Fourier Transform (FFT), with data dependency being a significant bottleneck. This paper enhances NTRU-based schemes Falcon and Hawk through hardware/software codesign on a customized Single-Instruction-Multiple-Data (SIMD) processor, proposing new SIMD hardware units and instructions to expedite these schemes along with software optimizations to boost performance. Our NTT optimization includes a novel layer merging technique for SIMD architecture to reduce memory accesses, and the use of modular algorithms (Signed Montgomery and Improved Plantard) targets various modulus data widths to enhance performance. We explore applying layer merging to accelerate fixed-point FFT at the SIMD instruction level and devise a dual-issue parser to streamline assembly code organization to maximize dual-issue utilization. A System-on-chip (SoC) architecture is devised to improve the practical application of the processor in real-world scenarios. Evaluation on 28 nm technology and FPGA platform shows that our design and optimizations can increase the performance of Hawk signature generation and verification by over 7×.},
  archive      = {J_TC},
  author       = {Zewen Ye and Junhao Huang and Tianshun Huang and Yudan Bai and Jinze Li and Hao Zhang and Guangyan Li and Donglong Chen and Ray C.C. Cheung and Kejie Huang},
  doi          = {10.1109/TC.2025.3540647},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {PQNTRU: Acceleration of NTRU-based schemes via customized post-quantum processor},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Aurora: Leaderless state-machine replication with high
throughput. <em>TC</em>, 1–12. (<a
href="https://doi.org/10.1109/TC.2025.3540656">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {State-machine replication (SMR) allows a deterministic state machine to be replicated across a set of replicas and handle clients’ requests as a single machine. Most existing SMR protocols are leader-based requiring a leader to order requests and coordinate the protocol. This design places a disproportionately high load on the leader, inevitably impairing the scalability. If the leader fails, a complex and bug-prone fail-over protocol is needed to switch to a new leader. An adversary can also exploit the fail-over protocol to slow down the protocol. In this paper, we propose a crash-fault tolerant SMR named Aurora, with the following properties: •Leaderless: it does not require a leader, hence completely get rid of the fail-over protocol. •Scalable: it can scale up to 11 replicas. •Robust: it behaves well even under a poor network connection. We provide a full-fledged implementation of Aurora and systematically evaluate its performance. Our benchmark results show that Aurora achieves a throughput of around two million Transactions Per Second (TPS), up to 8.7× higher than the state-of-the-art leaderless SMR.},
  archive      = {J_TC},
  author       = {Hao Lu and Jian Liu and Kui Ren},
  doi          = {10.1109/TC.2025.3540656},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Aurora: Leaderless state-machine replication with high throughput},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 29-billion atoms molecular dynamics simulation with ab
initio accuracy on 35 million cores of new sunway supercomputer.
<em>TC</em>, 1–14. (<a
href="https://doi.org/10.1109/TC.2025.3540646">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Physical phenomena such as bond breaking and phase transitions require molecular dynamics (MD) with ab initio accuracy, involving up to billions of atoms and over nanosecond timescales. Previous state-of-the-art work has demonstrated that neural network molecular dynamics (NNMD) like deep potential molecular dynamics (DeePMD), can successfully extend the temporal and spatial scales of MD with ab initio accuracy on both ARM and GPU platforms. However, the DeePMD-kit package is currently unable to fully exploit the computational potential of the new Sunway supercomputer due to its unique many-core architecture, memory hierarchy, and low precision capability. In this paper, we re-design the DeePMD-kit to harness the massive computing power of the new Sunway, enabling the MD with over ten billion atoms. We first design a large-scale parallelization scheme to exploit the massive parallelism of the new Sunway. Then we devise specialized optimizations for the time-consuming operators. Finally, we design a novel mixed precision method for DeePMD-kit customized operators to leverage the low precision computing power of the new Sunway. The optimized DeePMD-kit achieves 67.6 / 56.5 × speedup for water / copper systems on the new Sunway. Meanwhile, it can perform 29 billion atoms simulation for the water system on 35 million cores (i.e., 90,000 computing nodes, around 84% of the whole supercomputer) with a peak performance of 57.1 PFLOPs, which is 7.9× bigger and 1.2× faster than state-of-the-art results. This paves the way for investigating more realistic scenarios, such as studying the mechanical properties of metals, semiconductor devices, batteries, and other materials and physical systems.},
  archive      = {J_TC},
  author       = {Xun Wang and Xiangyu Meng and Zhuoqiang Guo and Mingzhen Li and Lijun Liu and Mingfan Li and Qian Xiao and Tong Zhao and Ninghui Sun and Guangming Tan and Weile Jia},
  doi          = {10.1109/TC.2025.3540646},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {29-billion atoms molecular dynamics simulation with ab initio accuracy on 35 million cores of new sunway supercomputer},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Defying multi-model forgetting in one-shot neural
architecture search using orthogonal gradient learning. <em>TC</em>,
1–13. (<a href="https://doi.org/10.1109/TC.2025.3540650">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One-shot neural architecture search (NAS) trains an over-parameterized network (termed as supernet) that assembles all the architectures as its subnets by using weight sharing for computational budget reduction. However, there is an issue of multi-model forgetting during supernet training that some weights of the previously well-trained architecture will be overwritten by that of the newly sampled architecture which has overlapped structures with the old one. To overcome the issue, we propose an orthogonal gradient learning (OGL) guided supernet training paradigm, where the novelty lies in the fact that the weights of the overlapped structures of current architecture are updated in the orthogonal direction to the gradient space of these overlapped structures of all previously trained architectures. Moreover, a new approach of calculating the projection is designed to effectively find the base vectors of the gradient space to acquire the orthogonal direction. We have theoretically and experimentally proved the effectiveness of the proposed paradigm in overcoming the multi-model forgetting. Besides, we apply the proposed paradigm to two one-shot NAS baselines, and experimental results demonstrate that our approach is able to mitigate the multi-model forgetting and enhance the predictive ability of the supernet with remarkable efficiency on popular test datasets.},
  archive      = {J_TC},
  author       = {Lianbo Ma and Yuee Zhou and Ye Ma and Guo Yu and Qing Li and Qiang He and Yan Pei},
  doi          = {10.1109/TC.2025.3540650},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Defying multi-model forgetting in one-shot neural architecture search using orthogonal gradient learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AToM: Adaptive token merging for efficient acceleration of
vision transformer. <em>TC</em>, 1–14. (<a
href="https://doi.org/10.1109/TC.2025.3540638">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, Vision Transformers (ViTs) have set a new standard in computer vision (CV), showing unparalleled image processing performance. However, their substantial computational requirements hinder practical deployment, especially on resource-limited devices common in CV applications. Token merging has emerged as a solution, condensing tokens with similar features to cut computational and memory demands. Yet, existing applications on ViTs often miss the mark in token compression, with rigid merging strategies and a lack of in-depth analysis of ViT merging characteristics. To overcome these issues, this paper introduces Adaptive Token Merging (AToM), a comprehensive algorithm-architecture co-design for accelerating ViTs. The AToM algorithm employs an image-adaptive, fine-grained merging strategy, significantly boosting computational efficiency. We also optimize the merging and unmerging processes to minimize overhead, employing techniques like First-Come-First-Merge mapping and Linear Distance Calculation. On the hardware side, the AToM architecture is tailor-made to exploit the AToM algorithm’s benefits, with specialized engines for efficient merge and unmerge operations. Our pipeline architecture ensures end-to-end ViT processing, minimizing latency and memory overhead from the AToM algorithm. Across various hardware platforms including CPU, EdgeGPU, and GPU, AToM achieves average end-to-end speedups of 10.9×, 7.7×, and 5.4×, alongside energy savings of 24.9×, 1.8×, and 16.7×. Moreover, AToM offers 1.2× 1.9× higher effective throughput compared to existing transformer accelerators.},
  archive      = {J_TC},
  author       = {Jaekang Shin and Myeonggu Kang and Yunki Han and Junyoung Park and Lee-Sup Kim},
  doi          = {10.1109/TC.2025.3540638},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {AToM: Adaptive token merging for efficient acceleration of vision transformer},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Blockchain-based privacy-preserving deduplication and
integrity auditing in cloud storage. <em>TC</em>, 1–13. (<a
href="https://doi.org/10.1109/TC.2025.3540670">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ensuring cloud data security and reducing cloud storage costs have become particularly important. Many schemes expose user file ownership privacy when deduplicating authentication tags and during integrity auditing. Moreover, key management becomes more difficult as the number of files increases. Also, many audit schemes rely on third-party auditors (TPAs), but finding a fully trustworthy TPA is challenging. Therefore, we propose a blockchain-based integrity audit scheme supporting data deduplication. It protects file tag privacy during deduplication of ciphertexts and authentication tags, safeguards audit proof privacy, and effectively protects user file ownership privacy. To reduce key management costs, we introduce identity-based broadcast encryption (IBBE) that does not require interaction with key servers, eliminating additional communication costs. Additionally, we use smart contracts for integrity auditing, eliminating the need for a fully trusted TPA. We evaluate the proposed scheme through security and theoretical analyses and a series of experiments, demonstrating its efficiency and practicality.},
  archive      = {J_TC},
  author       = {Qingyang Zhang and Shuai Qian and Jie Cui and Hong Zhong and Fengqun Wang and Debiao He},
  doi          = {10.1109/TC.2025.3540670},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Blockchain-based privacy-preserving deduplication and integrity auditing in cloud storage},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A low-rate DoS attack mitigation scheme based on port and
traffic state in SDN. <em>TC</em>, 1–13. (<a
href="https://doi.org/10.1109/TC.2025.3541143">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-rate Denial of Service (DoS) attacks can significantly compromise network availability and are difficult to detect and mitigate due to their stealthy exploitation of flaws in congestion control mechanisms. Software-Defined Networking (SDN) is a revolutionary architecture that decouples network control from packet forwarding, emerging as a promising solution for defending against low-rate DoS attacks. In this paper, we propose Trident, a low-rate DoS attack mitigation scheme based on port and traffic state in SDN. Specifically, we design a multi-step strategy to monitor switch states. First, Trident identifies switches suspected of suffering from low-rate DoS attacks through port state detection. Then, it monitors the traffic state of switches with abnormal port states. Once a switch is identified as suffering from an attack, Trident analyzes the flow information to pinpoint the malicious flow. Finally, Trident issues rules to the switch’s flow table to block the malicious flow, effectively mitigating the attack. We prototype Trident on the Mininet platform and conduct experiments using a real-world topology to evaluate its performance. The experiments show that Trident can accurately and robustly detect low-rate DoS attacks, respond quickly to mitigate them, and maintain low overhead.},
  archive      = {J_TC},
  author       = {Dan Tang and Rui Dai and Chenguang Zuo and Jingwen Chen and Keqin Li and Zheng Qin},
  doi          = {10.1109/TC.2025.3541143},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A low-rate DoS attack mitigation scheme based on port and traffic state in SDN},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). KDN-based adaptive computation offloading and resource
allocation strategy optimization: Maximizing user satisfaction.
<em>TC</em>, 1–14. (<a
href="https://doi.org/10.1109/TC.2025.3541142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In large-scale dynamic network environments, optimizing the computation offloading and resource allocation strategy is key to improving resource utilization and meeting the diverse demands of User Equipment (UE). However, traditional strategies for providing personalized computing services face several challenges: dynamic changes in the environment and UE demands, along with the inefficiency and high costs of real-time data collection; the unpredictability of resource status leads to an inability to ensure long-term UE satisfaction. To address these challenges, we propose a Knowledge-Defined Networking (KDN)-based Adaptive Edge Resource Allocation Optimization (KARO) architecture, facilitating real-time data collection and analysis of environmental conditions. Additionally, we implement an environmental resource change perception module in the KARO to assess current and future resource utilization trends. Based on the real-time state and resource urgency, we develop a Deep Reinforcement Learning-based Adaptive Long-term Computation Offloading and Resource Allocation (AL-CORA) strategy optimization algorithm. This algorithm adapts to the environmental resource urgency, autonomously balancing UE satisfaction and task execution cost. Experimental results indicate that AL-CORA effectively improves long-term UE satisfaction and task execution success rates, under the limited computation resource constraints.},
  archive      = {J_TC},
  author       = {Kaiqi Yang and Qiang He and Xingwei Wang and Zhi Liu and Yufei Liu and Min Huang and Liang Zhao},
  doi          = {10.1109/TC.2025.3541142},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {KDN-based adaptive computation offloading and resource allocation strategy optimization: Maximizing user satisfaction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RelHDx: Hyperdimensional computing for learning on graphs
with FeFET acceleration. <em>TC</em>, 1–12. (<a
href="https://doi.org/10.1109/TC.2025.3541141">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) are a powerful machine learning (ML) method to analyze graph data. The training of GNN has compute and memory-intensive phases along with irregular data movements, which makes in-memory acceleration challenging. We present a hyperdimensional computing (HDC)-based graph ML framework called RelHDx that aggregates node features and graph structure, along with representing node and edge information in high-dimensional space. RelHDx enables single-pass training and inference with simple arithmetic operations, resulting in the efficient design of graph-based ML tasks: node classification and link prediction. We accelerate RelHDx using scalable processing in-memory (PIM) architecture based on emerging ferroelectric FET (FeFET) technology. Our accelerator uses a data allocation optimization and operation scheduler to address the irregularity of the graph and maximize the performance. Evaluation results show that RelHDx offers comparable accuracy to popular GNN-based algorithms while achieving up to 63.8× faster speed on GPU. Our FeFET-based accelerator, RelHDx-PIM, is 32× faster for node classification, while for link prediction it is 65.4× faster than when running on GPU. Furthermore, RelHDx-PIM improves energy efficiency by four orders of magnitude over GPU. Compared to the state-of-the-art in-memory processing-based GNN accelerator, PIM-GCN [1], RelHDx-PIM is 10× faster and 986× more energy-efficient on average.},
  archive      = {J_TC},
  author       = {Jaeyoung Kang and Minxuan Zhou and Weihong Xu and Tajana Rosing},
  doi          = {10.1109/TC.2025.3541141},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Comput.},
  title        = {RelHDx: Hyperdimensional computing for learning on graphs with FeFET acceleration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DataFly: A confidentiality-preserving data migration across
heterogeneous blockchains. <em>TC</em>, 1–14. (<a
href="https://doi.org/10.1109/TC.2025.3535830">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Permissioned blockchains play a significant role in various application scenarios. Applications built on heterogeneous permissioned blockchains need to migrate data from one chain to another, aiming to keep their competitiveness and security. Thus, data migration across heterogeneous chains is a building block of permissioned blockchains. However, existing data migration protocols across heterogeneous chains are rarely used in practice since data migration technologies are insecure. To this end, we propose a data migration protocol across permissioned blockchains, named DataFly. We design a peg consensus mechanism, which provides consistent data-migration functionality between any two permissioned blockchains. To preserve the confidentiality of data, we invoke two classical cryptographic methods, i.e., i) ECDSA feature and ii) the integrated signature and public key encryption scheme. Through combining those two methods, data can be securely migrated from one permissioned blockchain to another without exposing the migrated data to anyone except associated parties. To demonstrate the practicality of DataFly, we implement a prototype of DataFly using existing popular permissioned blockchains, i.e., Hyperledger Fabric and private enterprise Ethereum. Measurement results demonstrate that DataFly outperforms related works in terms of transaction latency and gas costs.},
  archive      = {J_TC},
  author       = {Taotao Li and Huawei Huang and Parhat Abla and Zhihong Deng and Qinglin Yang and Anke Xie and Debiao He and Zibin Zheng},
  doi          = {10.1109/TC.2025.3535830},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {DataFly: A confidentiality-preserving data migration across heterogeneous blockchains},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). NDirect2: A high-performance library for direct convolutions
on multi-core CPUs. <em>TC</em>, 1–14. (<a
href="https://doi.org/10.1109/TC.2025.3543677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convolution kernels are widely seen in high-performance computing (HPC) and deep learning (DL) workloads and are often responsible for performance bottlenecks. Prior works have demonstrated that the direct convolution approach can outperform the conventional convolution implementation. Although well-studied, the existing approaches for direct convolution are either incompatible with the mainstream deep learning (DL) data layouts or lead to suboptimal performance. We design nDirect2, a novel direct convolution approach that targets multi-core CPUs commonly found in smartphones and HPC systems. nDirect2 is compatible with the data layout formats used by mainstream DL frameworks and offers new optimizations for the computational kernel, data packing, advanced operator fusion, and parallelization. We evaluate nDirect2 by applying it to representative convolution kernels and demonstrating how well it performs on four distinct ARM-based CPUs and an X86-based CPU. Experimental results show that nDirect2 outperforms four state-of-the-art convolution approaches across most evaluation cases and hardware architectures.},
  archive      = {J_TC},
  author       = {Weiling Yang and Pengyu Wang and Jianbin Fang and Dezun Dong and Zhengbin Pang and Runxi He and Peng Zhang and Tao Tang and Chun Huang and Yonggang Che and Jie Ren},
  doi          = {10.1109/TC.2025.3543677},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {NDirect2: A high-performance library for direct convolutions on multi-core CPUs},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ETBench: Characterizing hybrid vision transformer workloads
across edge devices. <em>TC</em>, 1–14. (<a
href="https://doi.org/10.1109/TC.2025.3543697">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Lightweight Convolution and Vision Transformer hybrid models have increasingly dominated the frontiers of deep learning (DL) on edge devices; however, to the best of our knowledge, no prior work has provided comprehensive evaluation on hybrid models’ performance and analyzed their characteristics by diving deep into the edge ecosystem with diversified modern DL inference engines and heterogeneous hardware. This paper proposes a comprehensive open-source benchmark suite, ETBench, to allow power-efficiency, performance and accuracy assessment for state-of-the-art (SOTA) hybrid models across 11 most widely-used DL engines deployed on diverse edge devices. After building ETBench that satisfies 6 design requirements proposed in our work, we conduct extensive experiments on 14 devices including 19 CPUs, 11 GPUs and 5 NPUs, and obtain benchmark results from all deployment scenarios (combinations of models, quantization formats, software engines, and hardware platforms). Valuable observations and insightful implications are finally summarized. For example, within current DL engines, the INT8 quantization is significantly underperformed in terms of accuracy and speed against FP16 for hybrid models. Overall, ETBench serves as a collaborative platform that assists model architects in better evaluating their models and makes it possible for future co-optimizations of DL engines and hardware accelerators.},
  archive      = {J_TC},
  author       = {Yingkun Zhou and Zhengshuyuan Tian and Wenhao Yang and Tingting Zhang and Jinpeng Ye and Chenji Han and Tianyi Liu and Fuxin Zhang},
  doi          = {10.1109/TC.2025.3543697},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {ETBench: Characterizing hybrid vision transformer workloads across edge devices},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic graph publication with differential privacy
guarantees for decentralized applications. <em>TC</em>, 1–14. (<a
href="https://doi.org/10.1109/TC.2025.3543605">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Decentralized Applications (DApps) have garnered significant attention due to their decentralization, anonymity, and data autonomy. However, these systems face potential privacy challenge. The privacy challenge arises from the necessity for external service providers to collect and process user interaction data. The untrustworthiness of these providers may lead to privacy breaches, compromising the overall security of such DApp environments. To address this challenge, we model the interaction data in the DApp environments as dynamic graphs and propose a dynamic graph publication method named HMG (Hidden Markov Model for Dynamic Graphs). HMG estimates the interaction probabilities between users by extracting the temporal information from historically collected data and constructs an optimized model to generate synthetic graphs. The synthetic graphs can preserve the dynamic topological characteristics of the interaction processes within DApp environments while effectively protecting user privacy, thus assisting external service providers in performing effective analyses. Finally, we evaluate the performance of HMG using real-world datasets and benchmark it against commonly used graph metrics. The results demonstrate that the synthetic graphs preserve essential features, making them suitable for analysis by service providers.},
  archive      = {J_TC},
  author       = {Zhetao Li and Yong Xiao and Haolin Liu and Xiaofei Liao and Ye Yuan and Junzhao Du},
  doi          = {10.1109/TC.2025.3543605},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Dynamic graph publication with differential privacy guarantees for decentralized applications},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Localizing multiple bugs in RTL designs by classifying
hit-statements using neural networks. <em>TC</em>, 1–14. (<a
href="https://doi.org/10.1109/TC.2025.3543609">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nowadays the advanced applications required in our lives have led to a significant increase in the complexity of circuits, which enhances the possibility of occurring design errors. Hence an automated, powerful, and scalable debugging approach is needed. Therefore, this paper proposes a scalable approach for localizing multiple bugs in Register-Transfer level (RTL) designs by using neural networks. The main idea is that hit-statements which are covered by failed test-vectors are more suspicious than those covered by passed test-vectors. We use coverage data as samples of our data set, label these samples, and tune the neural network model. Then we encode hit-statements and give them to the tuned model as new samples. The model classifies hit-statements. Hit-statements that take the failed labels, labels related to the failed test-vectors, are more suspicious of containing bugs. The results demonstrate that the proposed methodology outperforms recent approaches Tarsel and CirFix by localizing 80% of bugs at Top-1. The results also imply that our methodology increases the F1-score metric by 1.13× in comparison with existing RTL debugging techniques, which are prediction-based.},
  archive      = {J_TC},
  author       = {Mahsa Heidari and Bijan Alizadeh},
  doi          = {10.1109/TC.2025.3543609},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Localizing multiple bugs in RTL designs by classifying hit-statements using neural networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient methodology for binary logarithmic computations
of floating-point numbers with normalized output within one ulp of
accuracy. <em>TC</em>, 1–14. (<a
href="https://doi.org/10.1109/TC.2025.3543676">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many studies have focused on the hardware implementation of binary logarithmic computation with fixed-point output. Although their outputs are accurate within 1 ulp (unit in the last place) in fixed-point format, they are far from meeting the accuracy requirement of 1 ulp in floating-point format when the output is close to 0. However, normalized floating-point output that is accurate to within 1-3 ulp is needed in many math libraries (for example, OpenCL, NVIDIA CUDA, and AMD AOCL). To the best of our knowledge, this is the first study to propose a hardware implementation of binary logarithmic computation for floating-point numbers with a normalized output that is accurate to within 1 ulp. Instead of calculating log2(1 + fi) (where fi is the fractional part of the floating-point number) directly, the proposed methodology uses two novel objective functions for the polynomial approximation method. The novel objective functions make the significant bits of the outputs move forward to eliminate the necessity for high precision near zero. Compared with the designs of fixed-point binary logarithmic converters, the proposed hardware implementation achieves greater accuracy to meet the requirement of 1 ulp of floating-point format with a 21% extra area consumption.},
  archive      = {J_TC},
  author       = {Fei Lyu and Yuanyong Luo and Weiqiang Liu},
  doi          = {10.1109/TC.2025.3543676},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {An efficient methodology for binary logarithmic computations of floating-point numbers with normalized output within one ulp of accuracy},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HashScape: Leveraging virtual address dynamics for efficient
hashed page tables. <em>TC</em>, 1–14. (<a
href="https://doi.org/10.1109/TC.2025.3543698">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The evolving memory landscape for larger capacity prompts alternative approaches due to scalability challenges in multi-level page tables, which require multiple serial memory accesses for address translation. Hashed Page Tables (HPTs) have gained attention for ideally facilitating a single memory access per translation. However, current HPTs increase minor page fault latency, thereby impeding its superiority over conventional multi-level page table design. This paper provides a comprehensive analysis of HPTs regarding minor page fault latency concerning memory management subsystems. In particular, we demonstrate how feasibility issues in memory management with HPTs can escalate minor page fault latency. We observe that different page types in HPTs (anon pages and page caches) exhibit distinct behaviors on the occurrence of minor page faults, indicating a significant correlation between page types and minor page faults. To address these challenges, we propose HashScape, a scheme that harmonizes with memory management using tailored HPTs per segment and size-tailored allocation via Virtual Memory Areas. Our evaluation demonstrates that HashScape significantly improves the insertion latency, with average, 95th, and 99th percentiles improving by 1.8×, 1.9×, and 2.2×, respectively, resulting in an overall 10% reduction in minor page fault latency compared to a state-of-the-art HPT design.},
  archive      = {J_TC},
  author       = {Won Hur and Jiwon Lee and Jaewon Kwon and Minjae Kim and Won Woo Ro},
  doi          = {10.1109/TC.2025.3543698},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {HashScape: Leveraging virtual address dynamics for efficient hashed page tables},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Towards effective local search for qubit mapping.
<em>TC</em>, 1–14. (<a
href="https://doi.org/10.1109/TC.2025.3544869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of noisy intermediate-scale quantum (NISQ), a quantum logical circuit must undergo certain compilation before it can be used on a NISQ device, subject to connectivity constraints posed by NISQ device. During compilation, numerous auxiliary quantum gates are inserted, but a circuit with too many is unreliable, necessitating gate minimization. This requirement gives rise to the qubit mapping problem (QMP), an NP-hard optimization problem that is critical in quantum computing. This work proposes a novel and effective local search algorithm dubbed EffectiveQM. First, EffectiveQM proposes a new mode-aware search strategy to alleviate the challenge of being trapped in local optima, where local search typically suffers. Moreover, EffectiveQM introduces a novel potential-guided scoring function, which can thoroughly quantify the actual benefit brought by an operation of inserting auxiliary gates. By incorporating the potential-guided scoring function, EffectiveQM can effectively determine the appropriate operation to be performed. Extensive experiments on a diverse collection of logical circuits and 6 NISQ devices demonstrate that EffectiveQM can generate physical circuits with significantly fewer inserted auxiliary gates than current state-of-the-art QMP algorithms, indicating that EffectiveQM greatly advances the state of the art in QMP solving.},
  archive      = {J_TC},
  author       = {Chuan Luo and Shenghua Cao and Shanyu Guo and Chunming Hu},
  doi          = {10.1109/TC.2025.3544869},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Towards effective local search for qubit mapping},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Redactable blockchain from decentralized chameleon hash
functions, revisited. <em>TC</em>, 1–10. (<a
href="https://doi.org/10.1109/TC.2025.3544878">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, redactable blockchains have attracted attention owing to enabling the contents of blocks to be re-written. The existing redactable blockchain solutions can be classified as two categories, the centralized one and decentralized one. In centralized solutions, a single blockchain node possessing the trapdoor conducts redaction operations. However, they suffer from the issue of single point of failure. In decentralized solutions, redaction operations are performed by numerous blockchain nodes cooperatively. But there also exists the issue of inefficiency or requiring a trusted party in these solutions. Subsequently, Jia et al. proposed a redactable blockchain solution from a decentralized chameleon hash function (DCH) they designed, which supports the threshold redaction, traceability and consistency check. Nevertheless, after carefully analyzing their solution, we find that it fails to achieve the security they claimed by presenting a concrete attack. To resolve this security issue, we propose a novel chameleon hash function scheme that achieves strong collision-resistant security while maintaining simple and efficient as the building block. Based on it, we then present an improved DCH scheme with sufficient security, so that the redactable blockchain from it can resist the presented attack. Theoretical and experimental analyses demonstrate that improved DCH achieves efficiency comparable to DCH.},
  archive      = {J_TC},
  author       = {Cong Li and Qingni Shen and Zhonghai Wu},
  doi          = {10.1109/TC.2025.3544878},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Redactable blockchain from decentralized chameleon hash functions, revisited},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A path-based topology-agnostic fault diagnosis strategy for
multiprocessor systems. <em>TC</em>, 1–12. (<a
href="https://doi.org/10.1109/TC.2025.3543701">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fault diagnosis technology is a method for locating faulty processors in multiprocessor systems, and it plays a crucial role in ensuring system stability, security and reliability. A widely used approach in this technology is the system-level strategy, which determines processor status by interpreting the set of test results between adjacent processors. Among them, the PMC and MM models are two commonly employed methods for generating these results. The diversity and complexity of network topologies in systems constrain existing algorithms to specific topologies, while the limitations of fault diagnosis strategies lead to reduced fault tolerance. In this paper, we present a novel path-based method to tackle the fault diagnosis problems in various networks according to the PMC and MM models. Firstly, we introduce the algorithm for partitioning the path into subpaths based on these models. To ensure that at least one subpath is diagnosed as fault-free, we derive the relationship between the fault bound T and the path length N. Then, building on methods for recognizing the subpath states, we have developed fault diagnosis algorithms for both the PMC and MM models. The simulation results show that our proposed algorithms can quickly and accurately diagnose faults in multiprocessor systems.},
  archive      = {J_TC},
  author       = {Lin Chen and Hao Feng and Jiong Wu},
  doi          = {10.1109/TC.2025.3543701},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Comput.},
  title        = {A path-based topology-agnostic fault diagnosis strategy for multiprocessor systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reliable communication scheme based on completely
independent spanning trees in data center networks. <em>TC</em>, 1–14.
(<a href="https://doi.org/10.1109/TC.2025.3547161">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With technological advancements, real-time applications have permeated various aspects of human life, relying on fast, reliable, and low-latency data transmission for seamless user experiences. The development of data center networks (DCNs) has greatly advanced real-time applications, with network reliability being a key factor in ensuring high-quality network services. As a switch-centric DCN, DPCell has good scalability and the ability to achieve load balancing at different traffic levels. With the increasing demand for high availability, fault tolerance, and efficient data transmission, highly reliable communication for DPCell is essential. Completely independent spanning trees (CISTs) play a significant role in enhancing reliable communication performance in networks. This paper proposes an algorithm for constructing CISTs in DPCell, which has relatively low time and space consumption compared to other CISTs construction algorithms in DCNs, offering an efficiency advantage. Communication simulations validate the effectiveness of using paths provided by CISTs in DPCell for data transmission. Furthermore, experimental results show that a multi-protection routing scheme configured with multiple CISTs significantly enhances fault tolerance in DPCell.},
  archive      = {J_TC},
  author       = {Hui Dong and Huaqun Wang and Mengjie Lv and Weibei Fan},
  doi          = {10.1109/TC.2025.3547161},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Reliable communication scheme based on completely independent spanning trees in data center networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring hyperdimensional computing robustness against
hardware errors. <em>TC</em>, 1–14. (<a
href="https://doi.org/10.1109/TC.2025.3547142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Brain-inspired hyperdimensional computing (HDC) is an emerging machine learning paradigm leveraging high-dimensional spaces for efficient tasks like pattern recognition and medical diagnostics. As a lightweight alternative to deep neural networks, HDC offers smaller model sizes, reduced computation, and memory-centric processing. However, deploying HDC in safety-critical applications, such as healthcare and robotics, is challenged by hardware-induced errors. This paper investigates HDC’s robustness to memory errors via extensive bit-flip injection experiments on item and associative memories. Results reveal that certain bit-flips severely degrade accuracy. To address this, we introduce the Hyperdimensional Bit-Flip Search (HD-BFS), a similarity-guided method for identifying vulnerabilities and crafting efficient attacks, where flipping just 6 critical bits—3.9% of random bit-flips—reduces accuracy to chance levels. We further propose Hyperdimensional Accelerated Bit-Flip Search (HD-ABFS), which narrows the search space by targeting critical dimensions and most significant bits (MSBs), achieving up to 282× speedup over HD-BFS. Finally, we develop an effective protection mechanism to enhance model safety. These insights highlight HDC’s resilience to random errors, offer robust defenses against targeted attacks, and advance the security and reliability of HDC systems.},
  archive      = {J_TC},
  author       = {Sizhe Zhang and Kyle Juretus and Xun Jiao},
  doi          = {10.1109/TC.2025.3547142},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Exploring hyperdimensional computing robustness against hardware errors},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Eliminate data divergence in SpMV via processor and memory
co-computing framework. <em>TC</em>, 1–14. (<a
href="https://doi.org/10.1109/TC.2025.3547162">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse matrix-vector multiplication (SpMV) is a performance-critical kernel in various application domains, including high-performance computing, artificial intelligence, and big data. However, the performance of SpMV on SIMD devices is greatly affected by data divergences. To address this issue, we propose an In-SRAM Computing-based Processor Memory Co-Compute SpMV optimization framework that divides the SpMV kernel into two stages: a compute-intensive stage and a control-intensive stage. For optimizing the first stage, we leverage the parallel random access feature of multi-bank SRAM to eliminate overheads caused by memory divergences and use the Aggregate Table (AT) to reduce bank conflicts. For optimizing the second stage, we convert control divergences into memory divergences and utilize the Accumulate ScratchPad Memory (AccSPM) for executing reduction operations while eliminating overheads caused by memory divergences. Experimental results demonstrate that our solution achieves significant throughput increase over highly optimized vector SpMV kernels under CSR, CSR5, and CVR compression formats with performance speedups up to 4.74×, 5.58×, and 4.83× (3.11×, 3.04×, and 3.07× on average), respectively.},
  archive      = {J_TC},
  author       = {Zhang Dunbo and Shen Li and Lu Kai},
  doi          = {10.1109/TC.2025.3547162},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Eliminate data divergence in SpMV via processor and memory co-computing framework},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CoSpMV: Towards agile software and hardware co-design for
SpMV computation. <em>TC</em>, 1–14. (<a
href="https://doi.org/10.1109/TC.2025.3547136">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sparse Matrix-Vector multiplication (SpMV) is a widely used kernel in scientific or engineering applications and it is commonly implemented in FPGAs for acceleration. Existing works on FPGA usually pre-process the sparse matrix for data compression from the software perspective, and then design a unified architecture from the hardware perspective. However, as different SpMV kernels expose different levels of data parallelism after software processing, a unified architecture may not efficiently tap the underlying parallelism exposed in a specific kernel, leading to poor bandwidth utilization (BU) or poor resource utilization. To this end, this paper proposes an agile software and hardware co-design framework, CoSpMV, that employs design space exploration on both software and hardware for a specific kernel. Specifically, by providing a scalable compressed data format and a highly pipelined hardware template, CoSpMV can select the most suitable software and hardware configurations for different kernels and generate the accelerator instantly. The experimental results show that CoSpMV can achieve 3.91× speedup on GFLOPs, and 1.31× speedup on BU compared to the state-of-the-art work.},
  archive      = {J_TC},
  author       = {Minghao Tian and Yue Liang and Bowen Liu and Dajiang Liu},
  doi          = {10.1109/TC.2025.3547136},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {CoSpMV: Towards agile software and hardware co-design for SpMV computation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PFed-NS: An adaptive personalized federated learning scheme
through neural network segmentation. <em>TC</em>, 1–13. (<a
href="https://doi.org/10.1109/TC.2025.3547138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) is typically deployed in a client-server architecture, which makes the Edge-Cloud architecture an ideal backbone for FL. A significant challenge in this setup arises from the diverse data feature distributions across different edge locations (i.e., non-IID data). In response, Personalized Federated Learning (PFL) approaches have been developed. Network segmentation-based PFL is an important approach to achieving PFL, in which the training network is divided into a global segment for server aggregation and a local segment maintained client-side. Existing methods determine the segmentation before the training, and the segmentation remains fixed throughout the PFL training. However, our investigation reveals that model representations vary as PFL progresses and the fixed segmentation may not deliver best performance across various training settings. To address this, we propose PFed-NS, a PFL framework based on adaptive network segmentation. This adaptive segmentation technique is composed of two elements: a mechanism for assessing divergence of clients’ probability density functions constructed from network layers’ outputs, and a model for dynamically establishing divergence thresholds, beyond which server aggregation is deemed detrimental. Further optimization strategies are proposed to reduce the computation and communication costs incurred by divergence modeling. Moreover, we propose a divergence-based BN strategy to optimize BN performance for network segmentation-based PFL. Extensive experiments have been conducted to compare PFed-NS against recent PFL models. The results demonstrate its superiority in enhancing model accuracy and accelerating convergence.},
  archive      = {J_TC},
  author       = {Yuchen Liu and Ligang He and Zhigao Zheng and Shenyuan Ren},
  doi          = {10.1109/TC.2025.3547138},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Comput.},
  title        = {PFed-NS: An adaptive personalized federated learning scheme through neural network segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep learning-based cloud security: Innovative attack
detection and privacy focused key management. <em>TC</em>, 1–12. (<a
href="https://doi.org/10.1109/TC.2025.3547150">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud Computing (CC) is widely adopted in sectors like education, healthcare, and banking due to its scalability and cost-effectiveness. However, its internet-based nature exposes it to cyber threats, necessitating advanced security frameworks. Traditional models suffer from high false positives and limited adaptability. To address these challenges, VECGLSTM, an attack detection model integrating Variable Long Short-Term Memory (VLSTM), capsule networks, and the Enhanced Gannet Optimization Algorithm (EGOA), is introduced. This hybrid approach enhances accuracy, reduces false positives, and dynamically adapts to evolving threats. EGOA is employed for its superior optimization capability, ensuring faster convergence and resilience. Additionally, Chaotic Cryptographic Pelican Tunicate Swarm Optimization (CCPTSO) is proposed for privacy-preserving key management. This model combines chaotic cryptographic techniques with the Pelican Tunicate Swarm Optimization Algorithm (PTSOA), leveraging the pelican algorithm’s exploration strength and the tunicate swarm’s exploitation ability for optimal encryption security. Performance evaluation demonstrates 99.675% accuracy, 99.5175% recall, 99.7075% precision, and 99.615% F1-score, along with reduced training (1.79s), encryption (0.986s), and decryption (1.029s) times. This research significantly enhances CC security by providing a scalable, adaptive framework that effectively counters evolving cyber threats while ensuring efficient key management.},
  archive      = {J_TC},
  author       = {Shahnawaz Ahmad and Mohd Arif and Shabana Mehfuz and Javed Ahmad and Mohd Nazim},
  doi          = {10.1109/TC.2025.3547150},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Deep learning-based cloud security: Innovative attack detection and privacy focused key management},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SAFA: Handling sparse and scarce data in federated learning
with accumulative learning. <em>TC</em>, 1–13. (<a
href="https://doi.org/10.1109/TC.2025.3543682">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) has emerged as an effective paradigm allowing multiple parties to collaboratively train a global model while protecting their private data. However, it is observed that the performance of FL approaches tends to degrade significantly when data are sparsely distributed across clients with small datasets. This is referred to as the sparse-and-scarce challenge, where data held by each client is both sparse (does not contain examples to all classes) and scarce (small dataset). Sparse-and-scarce data diminishes the generalizability of clients’ data, leading to intensive over-fitting and massive domain shifts in the local models and, ultimately, decreasing the aggregated model’s performance. Interestingly, while this scenario is a specific manifestation of the well-known non-IID11This refers to the generic situation where local data distributions are not identical and independently distributed challenge in FL, it has not been distinctly addressed. Our empirical investigation highlights that generic approaches to the non-IID challenge often prove inadequate in mitigating the sparse-and-scarce issue. To bridge this gap, we develop SAFA, a novel FL algorithm that specifically addresses the sparse-and-scarce challenge via a novel continual model iteration procedure. SAFA maximally exposes local models to the inter-client diversity of data with minimal effects of catastrophic forgetting. Our experiments show that SAFA outperforms existing FL solutions, up to 17.86%, compared to the prominent baseline. The code is accessible via https://github.com/HungNguyen20/SAFA.},
  archive      = {J_TC},
  author       = {Nguyen Nang Hung and Truong Thao Nguyen and Trong Nghia Hoang and Hieu H. Pham and Thanh Hung Nguyen and Nguyen Phi Le},
  doi          = {10.1109/TC.2025.3543682},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Comput.},
  title        = {SAFA: Handling sparse and scarce data in federated learning with accumulative learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lattice-based forward secure multi-user authenticated
searchable encryption for cloud storage systems. <em>TC</em>, 1–14. (<a
href="https://doi.org/10.1109/TC.2025.3540649">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Public key authenticated encryption with keyword search (PAEKS) has been widely studied in cloud storage systems, which allows the cloud server to search encrypted data while safeguarding against insider keyword guessing attacks (IKGAs). Most PAEKS schemes are based on the discrete logarithm (DL) hardness. However, this assumption becomes insecure when it comes to quantum attacks. To address this concern, there have been studies on post-quantum PAEKS based on lattice. But to our best knowledge, current lattice-based PAEKS exhibit limited applicability and security, such as only supporting single user scenarios, or encountering secret key leakage problem. In this paper, we propose FS-MUAEKS, the forward-secure multi-user authenticated searchable encryption, mitigating the secret key exposure problem and further supporting multi-user scenarios in a quantum setting. Additionally, we formalize the security models of FS-MUAEKS and prove its security in the random oracle model (ROM). Ultimately, the comprehensive performance evaluation indicates that our scheme is computationally efficient and surpasses other state-of-the-art PAEKS schemes. The ciphertext generation overhead of our scheme is only 0.27 times of others in the best case. The communication overhead of our FS-MUAEKS algorithm is constant at 1.75MB under different security parameter settings.},
  archive      = {J_TC},
  author       = {Shiyuan Xu and Xue Chen and Yu Guo and Yuer Yang and Shengling Wang and Siu-Ming Yiu and Xiuzhen Cheng},
  doi          = {10.1109/TC.2025.3540649},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Lattice-based forward secure multi-user authenticated searchable encryption for cloud storage systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient and secure storage verification in cloud-assisted
industrial IoT networks. <em>TC</em>, 1–14. (<a
href="https://doi.org/10.1109/TC.2025.3540661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of Industrial IoT (IIoT) has caused the explosion of industrial data, which opens up promising possibilities for data analysis in IIoT networks. Due to the limitation of computation and storage capacity, IIoT devices choose to outsource the collected data to remote cloud servers. Unfortunately, the cloud storage service is not as reliable as it claims, whilst the loss of physical control over the cloud data makes it a significant challenge in ensuring the integrity of the data. Existing schemes are designed to check the data integrity in the cloud. However, it is still an open problem since IIoT devices have to devote lots of computation resources in existing schemes, which are especially not friendly to resource-constrained IIoT devices. In this paper, we propose an efficient storage verification approach for cloud-assisted industrial IoT platform by adopting a homomorphic hash function combined with polynomial commitment. The proposed approach can efficiently generate verification tags and verify the integrity of data in the industrial cloud platform for IIoT devices. Moreover, the proposed scheme can be extended to support privacy-enhanced verification and dynamic updates. We prove the security of the proposed approach under the random oracle model. Extensive experiments demonstrate the superior performance of our approach for resource-constrained devices in comparison with the state-of-the-art.},
  archive      = {J_TC},
  author       = {Haiyang Yu and Hui Zhang and Zhen Yang and Yuwen Chen and Huan Liu},
  doi          = {10.1109/TC.2025.3540661},
  journal      = {IEEE Transactions on Computers},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Comput.},
  title        = {Efficient and secure storage verification in cloud-assisted industrial IoT networks},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
