<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TIP_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tip---18">TIP - 18</h2>
<ul>
<li><details>
<summary>
(2025). Deep face leakage: Inverting high-quality faces from
gradients using residual optimization. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3533210">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Collaborative learning has gained significant traction for training deep learning models without sharing the original data of participants, particularly when dealing with sensitive data such as facial images. However, current gradient inversion attacks are employed to progressively reconstruct private data from gradients, and they have shown successful in extracting private training data. Nonetheless, our observations reveal that these methods exhibit suboptimal performance in face reconstruction and result in the loss of numerous facial details. In this paper, we propose DFLeak, an effective approach to boost face leakage from gradients using residual optimization and thwart the privacy of facial applications in collaborative learning. In particular, we first introduce a superior initialization method to stabilize the inversion process. Second, we propose to integrate blind face restoration results into the gradient inversion optimization process in a residual manner, which enriches facial details. We further design a pixel update schedule to mitigate the adverse effects of image regularization terms and preserve fine facial details. Comprehensive experimentation demonstrates the effectiveness of our approach in achieving more realistic and higher-quality facial image reconstructions, surpassing the performance of state-of-the-art gradient inversion attacks.},
  archive      = {J_TIP},
  author       = {Xu Zhang and Tao Xiang and Shangwei Guo and Fei Yang and Tianwei Zhang},
  doi          = {10.1109/TIP.2025.3533210},
  journal      = {IEEE Transactions on Image Processing},
  month        = {2},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Deep face leakage: Inverting high-quality faces from gradients using residual optimization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hybrid DQN-based low-computational reinforcement learning
object detection with adaptive dynamic reward function and ROI
align-based bounding box regression. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3541564">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep reinforcement learning-based object detection approaches center around a pivotal concept: hierarchically scaling image segments that harbor more intricate details. Compared with the traditional object detection approaches, this approach significantly curbs the quantity of region proposals. This reduction holds paramount significance in curtailing the computational overhead. However, common deep reinforcement learning-based approaches suffer from a significant defect in terms of precision. This issue arises from inadequacies in representing image states appropriately and the unstable learning ability exhibited by the agent. To address these issues, we present the LHAR-RLD. First, we design the Low-dimensional RepVGG(LDR) feature extractor to reduce memory consumption and to reduce the difficulty of fitting downstream networks. Second, we propose the Hybrid DQN(HDQN) to enhance the agent’s ability to determine the state-action of images in complex environments. Then, the Adaptive Dynamic Reward Function(ADR) is crafted to dynamically adjust the reward based on shifts within the agent’s exploration environment. Finally, the ROI Align-based bounding box regression network (RABRNet) is proposed, which aims at further regressing the localization results of reinforcement learning to improve the detection precision. Our method accomplishes 74.4% mAP on the VOC2007, 76.2% mAP on the COCO2017, 75.2% Precision on the SF dataset, with 1.43G FLOPs. The precision outperforms the advanced deep reinforcement learning approaches and the computational cost is far less than them and mainstream object detection methods. This method facilitates highly accurate object localization with minimal computational demands, which means it has notable applications on resource-constrained devices.},
  archive      = {J_TIP},
  author       = {Xun Zhou and Guangjie Han and Guoxiong Zhou and Yongfei Xue and Mingjie Lv and Aibin Chen},
  doi          = {10.1109/TIP.2025.3541564},
  journal      = {IEEE Transactions on Image Processing},
  month        = {2},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Hybrid DQN-based low-computational reinforcement learning object detection with adaptive dynamic reward function and ROI align-based bounding box regression},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). These maps are made by propagation: Adapting deep stereo
networks to road scenarios with decisive disparity diffusion.
<em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3540283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stereo matching has emerged as a cost-effective solution for road surface 3D reconstruction, garnering significant attention towards improving both computational efficiency and accuracy. This article introduces decisive disparity diffusion (D3Stereo), marking the first exploration of dense deep feature matching that adapts pre-trained deep convolutional neural networks (DCNNs) to previously unseen road scenarios. A pyramid of cost volumes is initially created using various levels of learned representations. Subsequently, a novel recursive bilateral filtering algorithm is employed to aggregate these costs. A key innovation of D3Stereo lies in its alternating decisive disparity diffusion strategy, wherein intra-scale diffusion is employed to complete sparse disparity images, while inter-scale inheritance provides valuable prior information for higher resolutions. Extensive experiments conducted on our created UDTIRI-Stereo and Stereo-Road datasets underscore the effectiveness of D3Stereo strategy in adapting pre-trained DCNNs and its superior performance compared to all other explicit programming-based algorithms designed specifically for road surface 3D reconstruction. Additional experiments conducted on the Middlebury dataset with backbone DCNNs pre-trained on the ImageNet database further validate the versatility of D3Stereo strategy in tackling general stereo matching problems. Our source code and supplementary material are publicly available at https://mias.group/D3-Stereo.},
  archive      = {J_TIP},
  author       = {Chuang-Wei Liu and Yikang Zhang and Qijun Chen and Ioannis Pitas and Rui Fan},
  doi          = {10.1109/TIP.2025.3540283},
  journal      = {IEEE Transactions on Image Processing},
  month        = {2},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {These maps are made by propagation: Adapting deep stereo networks to road scenarios with decisive disparity diffusion},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). BGPSeg: Boundary-guided primitive instance segmentation of
point clouds. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3540586">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Point cloud primitive instance segmentation is critical for understanding the geometric shapes of man-made objects. Existing learning-based methods mainly focus on learning high-dimensional feature representations of points and further perform clustering or region growing to obtain corresponding primitive instances. However, these features generally cannot accurately represent the discriminability between instances, especially near the boundaries or in regions with small differences in geometric properties. This limitation often leads to over- or under-segmentation of geometric primitives. On the other hand, the boundaries of different primitives are the direct features that distinguish them and thus utilizing boundary information to guide feature learning and clustering is crucial for this task. In this paper, we propose a novel framework BGPSeg for point cloud primitive instance segmentation that utilizes boundary-guided feature extraction and clustering. Specifically, we first introduce a boundary-guided feature extractor with the additional input of a boundary probability map, which utilizes boundary-guided sampling and a boundary transformer to enhance feature discrimination among points crossing geometric boundaries. Furthermore, we propose a boundary-guided primitive clustering module, which combines boundary clues and geometric feature discrimination for clustering to further improve the segmentation performance. Finally, we demonstrate the effectiveness of our BGPSeg with a series of comparison and ablation experiments while achieving the state-of-the-art primitive instance segmentation. Our code is available at https://github.com/fz-20/BGPSeg.},
  archive      = {J_TIP},
  author       = {Zheng Fang and Chuanqing Zhuang and Zhengda Lu and Yiqun Wang and Lupeng Liu and Jun Xiao},
  doi          = {10.1109/TIP.2025.3540586},
  journal      = {IEEE Transactions on Image Processing},
  month        = {2},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {BGPSeg: Boundary-guided primitive instance segmentation of point clouds},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MaCon: A generic self-supervised framework for unsupervised
multimodal change detection. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3542276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Change detection (CD) is important for Earth observation, emergency response and time-series understanding. Recently, data availability in various modalities has increased rapidly, and multimodal change detection (MCD) is gaining prominence. Given the scarcity of datasets and labels for MCD, unsupervised approaches are more practical for MCD. However, previous methods typically either merely reduce the gap between multimodal data through transformation or feed the original multimodal data directly into the discriminant network for difference extraction. The former faces challenges in extracting precise difference features. The latter contains the pronounced intrinsic distinction between the original multimodal data; direct extraction and comparison of features usually introduce significant noise, thereby compromising the quality of the resultant difference image. In this article, we proposed the MaCon framework to synergistically distill the common and discrepancy representations. The MaCon framework unifies mask reconstruction (MR) and contrastive learning (CL) self-supervised paradigms, where the MR serves the purpose of transformation while CL focuses on discrimination. Moreover, we presented an optimal sampling strategy in the CL architecture, enabling the CL subnetwork to extract more distinguishable discrepancy representations. Furthermore, we developed an effective silent attention mechanism that not only enhances contrast in output representations but stabilizes the training. Experimental results on both multimodal and monomodal datasets demonstrate that the MaCon framework effectively distills the intrinsic common representations between varied modalities and manifests state-of-the-art performance across both multimodal and monomodal CD. Such findings imply that the MaCon possesses the potential to serve as a unified framework in the CD and relevant fields. Source code will be publicly available once the article is accepted.},
  archive      = {J_TIP},
  author       = {Jian Wang and Li Yan and Jianbing Yang and Hong Xie and Qiangqiang Yuan and Pengcheng Wei and Zhao Gao and Ce Zhang and Peter M. Atkinson},
  doi          = {10.1109/TIP.2025.3542276},
  journal      = {IEEE Transactions on Image Processing},
  month        = {2},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {MaCon: A generic self-supervised framework for unsupervised multimodal change detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Who, what and where: Composite-semantics instance search for
story videos. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3542272">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Who, What and Where (3W) are the three core elements of storytelling, and accurately identifying the 3W semantics is critical to understanding the story in a video. This paper studies the 3W composite-semantics video Instance Search (INS) problem, which aims to find video shots about a specific person doing a concrete action in a particular location. The popular Complete-Decomposition (CD) methods divide a composite-semantics query into multiple single-semantics queries, which are likely to yield inaccurate or incomplete retrieval results due to neglecting important semantic correlations. Recent Non-Decomposition (ND) methods utilize Vision Language Model (VLM) to directly measure the similarity between textual query and video content. However, the accuracy is limited by VLM’s immature capability to recognize fine-grained objects. To address the above challenges, we propose a video structure-aware Partial-Decomposition (PD) method. Its core idea is to partially decompose the 3W INS problem into three semantic-correlated 2W INS problems i.e., person-action INS, action-location INS, and location-person INS. Thereafter, we respectively model the correlations between pairs of semantics at frames, shots and scenes of story videos. With the help of the spatial consistency and temporal continuity contained in the unique hierarchical structure of story videos, we can finally obtain identity-matching, logic-consistent, and content-coherent 3W INS results. To validate the effectiveness of the proposed method, we specifically build three large-scale 3W INS datasets based on three TV series Eastenders, Friends and The Big Bang Theory, totally comprising over 670K video shots spanning 700 hours. Extensive experiments show that the proposed PD method surpasses the current state-of-the-art CD and ND methods for 3W INS in story videos.},
  archive      = {J_TIP},
  author       = {Jiahao Guo and Ankang Lu and Zhengqian Wu and Zhongyuan Wang and Chao Liang},
  doi          = {10.1109/TIP.2025.3542272},
  journal      = {IEEE Transactions on Image Processing},
  month        = {2},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Who, what and where: Composite-semantics instance search for story videos},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). When adversarial training meets prompt tuning: Adversarial
dual prompt tuning for unsupervised domain adaptation. <em>TIP</em>, 1.
(<a href="https://doi.org/10.1109/TIP.2025.3541868">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptation (UDA) aims to adapt models learned from a well-annotated source domain to a target domain, where only unlabeled samples are available. To this end, adversarial training is widely used in conventional UDA methods to reduce the discrepancy between source and target domains. Recently, prompt tuning has emerged as an efficient way to adapt large pre-trained vision-language models like CLIP to a variety of downstream tasks. In this paper, we present a novel method named Adversarial DuAl Prompt Tuning (ADAPT) for UDA, which employs text prompts and visual prompts to guide CLIP simultaneously. Rather than simply performing a joint optimization of text prompts and visual prompts, we integrate text prompt tuning and visual prompt tuning into a collaborative framework where they engage in an adversarial game: text prompt tuning focuses on distinguishing between source and target images, whereas visual prompt tuning seeks to align source and target domains. Unlike most existing adversarial training-based UDA approaches, ADAPT does not require explicit domain discriminators for domain alignment. Instead, the objective is effectively achieved at both global and category levels through modeling the joint probability distribution of images on domains and categories. Extensive experiments on four benchmark datasets demonstrate the effectiveness of our ADAPT method for UDA. We have released our code at https://github.com/Liuziyi1999/ADAPT.},
  archive      = {J_TIP},
  author       = {Chaoran Cui and Ziyi Liu and Shuai Gong and Lei Zhu and Chunyun Zhang and Hui Liu},
  doi          = {10.1109/TIP.2025.3541868},
  journal      = {IEEE Transactions on Image Processing},
  month        = {2},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {When adversarial training meets prompt tuning: Adversarial dual prompt tuning for unsupervised domain adaptation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimal graph learning based label propagation for
cross-domain image classification. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3526380">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Label propagation (LP) is a popular semi-supervised learning technique that propagates labels from a training dataset to a test one using a similarity graph, assuming that nearby samples should have similar labels. However, recent cross-domain problem assumes that training (source domain) and test datasets (target domain) follow different distributions, which may unexpectedly degrade the performance of LP due to small similarity weights connecting the two domains. To address this problem, we propose an approach called optimal graph learning based label propagation (OGL2P), which optimizes one cross-domain graph and two intra-domain graphs to connect the two domains and preserve domain-specific structures, respectively. During label propagation, the cross-domain graph draws two labels close if they are nearby in feature space and from different domains, while the intra-domain graph pulls two labels close if they are nearby in feature space and from the same domain. This makes label propagation more insensitive to cross-domain problem. During graph embedding, the three graphs bring two samples close in an embedded subspace if they are nearby and from the same class. This makes feature representations of the two domains in the embedded subspace are domain-invariant and locally discriminative. Moreover, we optimize the three graphs using both features and labels in the embedded subspace to make them locally discriminative and robust to feature noise. Finally, we conduct extensive experiments on five cross-domain image classification datasets to verify that OGL2P outperforms some state-of-the-art cross-domain approaches.},
  archive      = {J_TIP},
  author       = {Wei Wang and Mengzhu Wang and Chao Huang and Cong Wang and Jie Mu and Feiping Nie and Xiaochun Cao},
  doi          = {10.1109/TIP.2025.3526380},
  journal      = {IEEE Transactions on Image Processing},
  month        = {2},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Optimal graph learning based label propagation for cross-domain image classification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RTF: Recursive TransFusion for multi-modal image synthesis.
<em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3541877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-modal image synthesis is crucial for obtaining complete modalities due to the imaging restrictions in reality. Current methods, primarily CNN-based models, find it challenging to extract global representations because of local inductive bias, leading to synthetic structure deformation or color distortion. Despite the significant global representation ability of transformer in capturing long-range dependencies, its huge parameter size requires considerable training data. Multi-modal synthesis solely based on one of the two structures makes it hard to extract comprehensive information from each modality with limited data. To tackle this dilemma, we propose a simple yet effective Recursive TransFusion (RTF) framework for multi-modal image synthesis. Specifically, we develop a TransFusion unit to integrate local knowledge extracted from the individual modality by connecting a CNN-based local representation block (LRB) and a transformer-based global fusion block (GFB) via a feature translating gate (FTG). Considering the numerous parameters introduced by the transformer, we further unfold a TransFusion unit with recursive constraint repeatedly, forming recursive TransFusion (RTF), which progressively extracts multi-modal information at different depths. Our RTF remarkably reduces network parameters while maintaining superior performance. Extensive experiments validate our superiority against the competing methods on multiple benchmarks. The source code will be available at https://github.com/guoliangq/RTF.},
  archive      = {J_TIP},
  author       = {Bing Cao and Guoliang Qi and Jiaming Zhao and Pengfei Zhu and Qinghua Hu and Xinbo Gao},
  doi          = {10.1109/TIP.2025.3541877},
  journal      = {IEEE Transactions on Image Processing},
  month        = {2},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {RTF: Recursive TransFusion for multi-modal image synthesis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization design of projection grating wavelength for
robust 3D imaging. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3541543">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For three-dimensional (3D) imaging based on fringe projection profilometry (FPP), maximum fringe frequency selection and fringe frequencies allocation have a significant impact on the accuracy and robustness of 3D imaging. In this paper, we conduct a detailed analysis of the wrapped phase error, and analyze the phase unwrapping reliability in the three-frequency temporal phase unwrapping (TPU). Since different measurement systems and scenes having different maximum sampling frequencies, we introduce a maximum frequency selection approach in this work. In order to ensure the overall phase unwrapping reliability, we introduce an optimal frequencies allocation approach. Experimental results show the valid of the proposed approach. The research in this paper will help to improve the accuracy and robustness of FPP in practical 3D measurement.},
  archive      = {J_TIP},
  author       = {Jianhua Wang and Yanxi Yang},
  doi          = {10.1109/TIP.2025.3541543},
  journal      = {IEEE Transactions on Image Processing},
  month        = {2},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Optimization design of projection grating wavelength for robust 3D imaging},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Reserve to adapt: Mining inter-class relations for open-set
domain adaptation. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3534023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Open-Set Domain Adaptation (OSDA) aims at adapting a model trained on a labelled source domain, to an unlabeled target domain that is corrupted with unknown classes. The key challenge inherent to this open-set setting is therefore how best to avoid the negative transfer incurred by unknown classes during model adaptation. Most existing works tackle this challenge by simply pushing the entire unknown classes away. In this paper, we take a different stance – instead of addressing these unknown classes as a single entity, we &quot;reserve&quot; in-between spaces for their subsets in the learned embedding. Our key finding is that the inter-class relations learned off the source domain, can help to enforce class separations in the target domain – thereby reserving spaces for unknown classes. More specifically, we first prep the &quot;reservation&quot; by tightening the known-class representations while enlarging their inter-class margin. We then learn soft-label prototypes in the source domain to facilitate the discrimination of known and unknown samples in the target domain. It follows that these two steps are iterated at each epoch in a mutually beneficial manner – better discrimination of unknown samples helps with space reservation, and vice versa. We show state-of-the-art results on four standard OSDA datasets, Office-31, Office-Home, VisDA and ImageCLEF, and conduct further analysis to help understand our method. Codes are available at: https://github.com/PRIS-CV/Reserve_to_Adapt.},
  archive      = {J_TIP},
  author       = {Yujun Tong and Dongliang Chang and Da Li and Xinran Wang and Kongming Liang and Zhongjiang He and Yi-Zhe Song and Zhanyu Ma},
  doi          = {10.1109/TIP.2025.3534023},
  journal      = {IEEE Transactions on Image Processing},
  month        = {2},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Reserve to adapt: Mining inter-class relations for open-set domain adaptation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Image clustering with transition probabilities learning.
<em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3542602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large-scale multi-view clustering for image data has achieved impressive clustering performance and efficiency. However, most methods lack interpretability in clustering and do not fully consider the complementarity of distributions between different views. To address these problems, we introduce Multi-View Clustering with Transition Probabilities Learning (MVCTPL). Specifically, we construct an anchor graph factorization model from the perspective of transition probabilities, while simultaneously learning transition probability matrices from samples to clusters and from anchor points to clusters, serving as soft label matrices for samples and anchor points, respectively. This model enables one-step label acquisition and provides the model with a sound probability interpretation. Moreover, since the clusters of samples and anchor points should be consistent across all views, we employ Schatten p-norm regularization on the two matrices, effectively mining the complementary information distributed among the views, thereby aligning the labels across views more consistently. Comprehensive testing on four small-scale datasets and three large-scale datasets confirms the effectiveness of this model.},
  archive      = {J_TIP},
  author       = {Xingyu Xue and Wenhui Zhao and Quanxue Gao and Ming Yang and Cheng Deng},
  doi          = {10.1109/TIP.2025.3542602},
  journal      = {IEEE Transactions on Image Processing},
  month        = {2},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Image clustering with transition probabilities learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Group visual relation detection. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3543114">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we propose a novel visual relation detection task, named Group Visual Relation Detection (GVRD), for detecting visual relations whose subjects and/or objects are groups (GVRs), inspired by the observation that groups are common in image semantic representation. GVRD can be deemed as an evolution over the existing visual relation detection task that limits both subjects and objects of visual relations as individuals. We propose a Simultaneous Group Relation Prediction (SGRP) method that can simultaneously predict groups and predicates to address GVRD. SGRP contains an Entity Construction (EC) module, a Feature Extraction (FE) module, and a Group Relation Prediction (GRP) module. Specifically, the EC module constructs instances, group candidates, and phrase candidates; the FE module extracts visual, location and semantic features for these entities; and the GRP module simultaneously predicts groups and predicates, and generates the GVRs. Moreover, we construct a new dataset, named COCO-GVR, to facilitate solutions to GVRD task, which consists of 9,570 images from COCO dataset and 31,855 manually labeled GVRs. We test and validate the performance of SGRP by extensive experiments on COCO-GVR dataset. It shows that SGRP outperforms the baselines generated from the state-of-the-art visual relation detection and scene graph generation methods.},
  archive      = {J_TIP},
  author       = {Fan Yu and Beibei Zhang and Tongwei Ren and Jiale Liu and Gangshan Wu and Jinhui Tang},
  doi          = {10.1109/TIP.2025.3543114},
  journal      = {IEEE Transactions on Image Processing},
  month        = {2},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Group visual relation detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking semantic segmentation with multi-grained logical
prototype. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3543052">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The last decade has witnessed significant advances in semantic segmentation brought about by deep learning. However, existing methods only fit the data-label correspondence in a data-driven manner and do not fully conform to the abstraction and structuralization characteristics of the human visual cognition process, which limits the upper bounds of their performance. To this end, a multi-grained logical prototype (MGLP) method is proposed to rethink semantic segmentation based on these two key characteristics. Its novel design can be summarized as follows. (1) For abstraction, prototypes of the same class at different grain levels are established: a label generation method is proposed to automatically generate a multi-grained label space, which can guide the learning of the multi-grained prototypes for each class. (2) For structuralization, the intrinsic logical structure across different semantic levels is explicitly modeled: the horizontal metric relationships are established via metric relation operations on prototypes at the same grain level, to improve the discriminability between classes while taking the vertical semantic hierarchy into account. Moveover, the vertical logical relationships are established as the sub-to-super positive and super-to-sub negative constraints, to strengthen the semantic dependencies among prototypes at different grain levels. (3) MGLP is plug-and-play and can be directly combined with existing segmentation methods. Extensive experimental results indicate that MGLP can significantly improve the segmentation performance of existing methods, which opens up a new avenue for future research.},
  archive      = {J_TIP},
  author       = {Anzhu Yu and Kuiliang Gao and Xiong You and Yanfei Zhong and Yu Su and Bing Liu and Chunping Qiu},
  doi          = {10.1109/TIP.2025.3543052},
  journal      = {IEEE Transactions on Image Processing},
  month        = {2},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Rethinking semantic segmentation with multi-grained logical prototype},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic atomic column detection in transmission electron
microscopy videos via ridge estimation. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3543138">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Ridge detection is a classical tool to extract curvilinear features in image processing. As such, it has great promise in applications to material science problems; specifically, for trend filtering relatively stable atom-shaped objects in image sequences, such as bright-field Transmission Electron Microscopy (TEM) videos. Standard analysis of TEM videos is limited to frame-by-frame object recognition. We instead harness temporal correlation across frames through simultaneous analysis of long image sequences, specified as a spatio-temporal image tensor. We define new ridge detection algorithms to non-parametrically estimate explicit trajectories of atomic-level object locations as a continuous function of time. Our approach is specially tailored to handle temporal analysis of objects that seemingly stochastically disappear and subsequently reappear throughout a sequence. We demonstrate that the proposed method is highly effective in simulation scenarios, and delivers notable performance improvements in TEM experiments compared to other material science benchmarks.},
  archive      = {J_TIP},
  author       = {Yuchen Xu and Andrew M. Thomas and Peter A. Crozier and David S. Matteson},
  doi          = {10.1109/TIP.2025.3543138},
  journal      = {IEEE Transactions on Image Processing},
  month        = {2},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Dynamic atomic column detection in transmission electron microscopy videos via ridge estimation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Resolving symmetry ambiguity in correspondence-based methods
for instance-level object pose estimation. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3544142">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating the 6D pose of an object from a single RGB image is a critical task that becomes additionally challenging when dealing with symmetric objects. Recent approaches typically establish one-to-one correspondences between image pixels and 3D object surface vertices. However, the utilization of one-to-one correspondences introduces ambiguity for symmetric objects. To address this, we propose SymCode, a symmetry-aware surface encoding that encodes the object surface vertices based on one-to-many correspondences, eliminating the problem of one-to-one correspondence ambiguity. We also introduce SymNet, a fast end-to-end network that directly regresses the 6D pose parameters without solving a PnP problem. We demonstrate faster runtime and comparable accuracy achieved by our method on the T-LESS and IC-BIN benchmarks of mostly symmetric objects. The code is available at https://github.com/lyltc1/SymNet.},
  archive      = {J_TIP},
  author       = {Yongliang Lin and Yongzhi Su and Sandeep Inuganti and Yan Di and Naeem Ajilforoushan and Hanqing Yang and Yu Zhang and Jason Rambach},
  doi          = {10.1109/TIP.2025.3544142},
  journal      = {IEEE Transactions on Image Processing},
  month        = {2},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Resolving symmetry ambiguity in correspondence-based methods for instance-level object pose estimation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Recurrent diffusion for 3D point cloud generation from a
single image. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3539935">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Single-image 3D shape reconstruction has attracted significant attention with the advance of generative models. Recent studies have utilized diffusion models to achieve unprecedented shape reconstruction quality. However, these methods, in each sampling step, perform denoising in a single forward pass, leading to cumulative errors that severely impact the geometric consistency of the generated shapes with the input targets and face difficulties in reconstructing rich details of complex 3D shapes. Moreover, the performance of current works suffers significant degradation due to limited information when only a single image is used as input during testing, further affecting the quality of 3D shape generation. In this paper, we present a recurrent diffusion framework, aiming to improve generation performance during single image-to-shape inference. Diverging from denoising in a single forward pass, we recursively refine the noise prediction in a self-rectified manner with the explicit guidance of the input target, thereby markedly suppressing cumulative errors and improving detail modeling. To enhance the geometric perception ability of the network during single-image inference, we further introduce a multi-view training scheme equipped with a view-robust conditional generation mechanism, which effectively promotes generation quality even when only a single image is available during inference. The effectiveness of our method is demonstrated through extensive evaluations on two public 3D shape datasets, where it surpasses state-of-the-art methods both qualitatively and quantitatively.},
  archive      = {J_TIP},
  author       = {Yan Zhou and Dewang Ye and Huaidong Zhang and Xuemiao Xu and Huajie Sun and Yewen Xu and Xiangyu Liu and Yuexia Zhou},
  doi          = {10.1109/TIP.2025.3539935},
  journal      = {IEEE Transactions on Image Processing},
  month        = {2},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Recurrent diffusion for 3D point cloud generation from a single image},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging frequency analysis for image denoising network
pruning. <em>TIP</em>, 1. (<a
href="https://doi.org/10.1109/TIP.2025.3544108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a common model compression technique, network pruning is widely used to reduce storage and computational cost of deep models in the resource-constrained regime. However, most current pruning methods are designed for highlevel vision tasks, with few developed for low-level vision tasks. We observed that the norm-based pruning criterion, originally designed for high-level vision tasks, is highly unsuitable for low-level image denoising networks. This difference arises because image denoising networks pursue distinct feature granularities and goals compared to typical high-level vision tasks. To address this issue, we propose a novel filter evaluation method, termed High-Frequency Components Pruning (HFCP), specifically tailored for image denoising network pruning. HFCP assesses filter importance based on high-frequency components. To the best of our knowledge, this is the first pruning method designed specifically for image denoising tasks, straightforward and applicable to various types of noise. Furthermore, HFCP enhances the pruned model’s high-frequency information content with high reliability and interpretability. This facilitates the network’s ability to distinguish high-frequency signals from noise. We comprehensively analyzed multiple image denoising networks and validated HFCP’s effectiveness across four mainstream networks.},
  archive      = {J_TIP},
  author       = {Dongdong Ren and Wenbin Li and Jing Huo and Lei Wang and Hongbing Pan and Yang Gao},
  doi          = {10.1109/TIP.2025.3544108},
  journal      = {IEEE Transactions on Image Processing},
  month        = {2},
  pages        = {1},
  shortjournal = {IEEE Trans. Image Process.},
  title        = {Leveraging frequency analysis for image denoising network pruning},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
