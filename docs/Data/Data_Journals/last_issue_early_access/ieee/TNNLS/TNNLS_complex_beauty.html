<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TNNLS_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tnnls---40">TNNLS - 40</h2>
<ul>
<li><details>
<summary>
(2025). Double-graph representation with relational enhancement for
emotion–cause pair extraction. <em>TNNLS</em>, 1–15. (<a
href="https://doi.org/10.1109/TNNLS.2025.3527767">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The emotion–cause pair extraction (ECPE) task is to simultaneously extract emotions and causes as pairs (EC-pairs) from documents, which is important for natural language processing. Previous research tackled this task via a two-step approach, which first predicts separately the emotion and cause clauses, and then pairs them up by using a binary classifier. However, such a two-step approach may suffer from the possible propagation of errors, and it neglects the interaction between emotions and causes. In this article, an end-to-end double-graph method with relational enhancement (DGRE) is proposed to stimulate two relationship modes among clauses, i.e., semantic dependence and logical dependence. First, two united graph encoders are established to embed the semantic dependence into the representation of clauses and pairs. The first encoder is built on graph attention networks (GATs) for clause-level representation, the result of which is used by a relational graph convolutional network (RGCN) for the refinement of pair-level representation. Aiming to enhance the fitting ability of logical dependence, the emotion-type classification task is introduced into the multitask learning framework of GATs, which can effectively distinguish the logical relations between clauses according to their emotion types. Moreover, seven types of dependence relations have been designed for the node connections in RGCN, which emphasize the contextual interaction and clustering among neighboring nodes. Experiments on a benchmark Chinese corpus demonstrate that the proposed DGRE approach could effectively establish the communication mechanism between clauses and pairs from multiple perspectives, and comparisons with state-of-the-art (SOTA) models well validate its effectiveness.},
  archive      = {J_TNNLS},
  author       = {Ming Zhang and Zhe Chen and Vasile Palade and Tao Lu and Liya Wang and Junchi Zhang and Yanduo Zhang},
  doi          = {10.1109/TNNLS.2025.3527767},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Double-graph representation with relational enhancement for Emotion–Cause pair extraction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boosting temporal graph learning from perspectives of global
and local structures. <em>TNNLS</em>, 1–15. (<a
href="https://doi.org/10.1109/TNNLS.2025.3526944">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning on temporal graphs has attracted tremendous research interest due to its wide range of applications. Some works intuitively merge graph neural networks (GNNs) and recurrent neural networks (RNNs) to capture structural and temporal information, and recent works propose to aggregate information from neighbor nodes in local subgraphs based on message passing or random walks. These methods produce node embeddings from a global or local perspective and ignore the complementarity between them, thus facing limitations in capturing complex and entangled dynamic patterns when applied to diverse datasets or evaluated by more challenging evaluation protocols. To address the issues, we propose the global and local embedding network (GLEN) for effective and efficient temporal graph representation learning. Specifically, GLEN dynamically generates embeddings for graph nodes by considering both global and local perspectives using specially designed modules. Then, global and local embeddings are combined by a devised cross-perspective fusion module to extract high-order semantic relations of node embeddings. We evaluate GLEN on multiple real-world datasets and apply more stringent evaluation procedures. Extensive experimental results demonstrate that GLEN outperforms other baselines in both link prediction and dynamic node classification tasks. Moreover, with concise and effective modules, GLEN can achieve a better balance between inference precision and training efficiency.},
  archive      = {J_TNNLS},
  author       = {Fengyi Wang and Guanghui Zhu and Hongqing Ding and Pengfei Zhang and Chunfeng Yuan and Yihua Huang},
  doi          = {10.1109/TNNLS.2025.3526944},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Boosting temporal graph learning from perspectives of global and local structures},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weighted contrastive learning with hard negative mining for
positive and unlabeled learning. <em>TNNLS</em>, 1–15. (<a
href="https://doi.org/10.1109/TNNLS.2025.3530427">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Positive and unlabeled (PU) learning aims to train a suitable classifier simply based on a set of positive data and unlabeled data. The state-of-the-art methods usually formulate PU learning as a cost-sensitive learning problem, in which every unlabeled example is treated as negative with modified class weights. However, existing methods fail to generate high-quality data representations, which brings about negative-prediction preference and performance decline. To overcome this problem, this article proposes a novel algorithm dubbed weighted contrastive learning with hard negative mining for positive and unlabeled learning (termed WConPU), which specifically designs a new prototypical contrastive strategy for gaining discriminative representations for PU learning. Specifically, our proposed WConPU consists of a contrastive learning (CL) module and a classifier training module, which can benefit from each other in an iterative manner. Moreover, a novel weighted contrastive objective function equipped with a prototype-based hard negative mining module is proposed to further enhance the representation quality. Theoretically, we show that our WConPU can be justified from the perspective of the expectation-maximization (EM) algorithm. Empirically, we compare our method with state-of-the-art PU algorithms on a wide range of real-world benchmark datasets, and the experimental results firmly demonstrate the advantage of our proposed method over the existing PU learning approaches.},
  archive      = {J_TNNLS},
  author       = {Botai Yuan and Chen Gong and Dacheng Tao and Jie Yang},
  doi          = {10.1109/TNNLS.2025.3530427},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Weighted contrastive learning with hard negative mining for positive and unlabeled learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deep graph multi-view representation learning with
self-augmented view fusion. <em>TNNLS</em>, 1–12. (<a
href="https://doi.org/10.1109/TNNLS.2025.3527928">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Some current researchers attempt to extend the graph neural network (GNN) on multi-view representation learning and learn the latent structure information among the data. Generally, they concatenate the features of each view and employ a single GNN to extract the representations of this concatenated feature. It causes that the within-view information may not be learned and the pivotal view will not be strengthened during the concatenation. Although some GNN models introduce the Siamese structure to extract the within-view information, the learned representation may not be informative since the Siamese GNNs share the same parameters. To overcome these issues, we propose a novel deep graph auto-encoder for multi-view representation learning. Among them, a self-augmented view-weight technique is theoretically devised for cross-view fusion, which can highlight the pivotal views and maintain the rest views. Then, GNNs of different views can learn the informative representation without sharing parameters. Furthermore, by fitting the fusion distribution with a neural layer, the model unifies these two individual procedures and achieve to extract the fusion representation end-to-end. Compared with numerous recently proposed methods, extensive experiments on clustering and recognition tasks demonstrate our superior performance.},
  archive      = {J_TNNLS},
  author       = {Ziheng Jiao and Hongyuan Zhang and Xuelong Li},
  doi          = {10.1109/TNNLS.2025.3527928},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Deep graph multi-view representation learning with self-augmented view fusion},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Extended invariant risk minimization for machine fault
diagnosis with label noise and data shift. <em>TNNLS</em>, 1–14. (<a
href="https://doi.org/10.1109/TNNLS.2025.3531214">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incorrect labels as well as the discrepancy between training and test domain data distributions can significantly affect the effectiveness of supervised data-driven models in machine fault diagnosis applications. Such a challenge can be characterized as the noisy label-domain generalization (NL-DG) problem. In this article, the extended invariant risk minimization (EIRM) is developed, which incorporates flat minima seeking to address the NL-DG challenge. The ability of handling NL-DG is realized by shifting the gradient penalty base from the dummy classifier to the entire model. EIRM is shown to be closely related to locating a flat minimum, which is crucial for label noise (LN) robustness and model generalization. Explorations on function smoothness and algorithm convergence are offered to understand EIRM from the theoretical aspect. An efficient implementation of EIRM is also developed to construct the fault diagnosis model. The EIRM-based fault diagnosis method is compared with strong benchmarks on multiple NL-DG tasks using actuator and gearbox fault datasets. Results indicate that the EIRM-based method on average is more effective than the benchmarks. The code is available at https://github.com/mozhenling/doge-eirm.},
  archive      = {J_TNNLS},
  author       = {Zhenling Mo and Zijun Zhang and Qiang Miao and Kwok-Leung Tsui},
  doi          = {10.1109/TNNLS.2025.3531214},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Extended invariant risk minimization for machine fault diagnosis with label noise and data shift},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interpretable optimization-inspired deep network for
off-grid frequency estimation. <em>TNNLS</em>, 1–13. (<a
href="https://doi.org/10.1109/TNNLS.2025.3534784">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The accuracy of on-grid frequency estimation methods suffers from the quantization error of discrete grids. In this article, a deep unfolded network for off-grid frequency estimation is proposed, dubbed OGFreq. In the OGFreq, there exist two kinds of variables. One is the batch-oriented dictionary for frequency-domain transform, and the other one is the instance-specific on-grid frequency and off-grid bias. As the dictionary is required to be universally applicable among all observed signals, network layers are designed and network weights are updated to approximate the transform bases in a data-driven way. Besides, instance-specific on-grid frequencies and off-grid biases are solved by unfolding the iterative soft-threshold algorithm (ISTA). In addition, the instance-specific hyperparameters for sparsity in ISTA are obtained by an encoder–decoder soft-threshold (EDS) module with the attention mechanism. In this way, the dictionary, on-grid frequency, and off-grid bias are learned in a unified data-driven framework. Numerical experiments show that the OGFreq obtains 4% lower false negative rate (FNR) when the SNR is 20 dB. Moreover, the computational complexity is one order of magnitude lower than the iteration-based off-grid frequency estimation methods. Finally, the robustness of the OGFreq is discussed when extended to the impulse noise and damped signals.},
  archive      = {J_TNNLS},
  author       = {Pingping Pan and Yunjian Zhang and You Li and Yishan Ye and Wei He and Yutao Zhu and Renzhong Guo},
  doi          = {10.1109/TNNLS.2025.3534784},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Interpretable optimization-inspired deep network for off-grid frequency estimation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical AttentionShift for pointly supervised instance
segmentation. <em>TNNLS</em>, 1–14. (<a
href="https://doi.org/10.1109/TNNLS.2025.3526961">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pointly supervised instance segmentation (PSIS) remains a challenging task when appearance variances across object parts cause semantic inconsistency. In this article, we propose a hierarchical AttentionShift approach, to solve the semantic inconsistency issue through exploiting the hierarchical nature of semantics and the flexibility of key-point representation. The estimation of hierarchical attention is defined upon key-point sets. The representative key points are iteratively estimated spatially and in the feature space to capture the fine-grained semantics and cover the full object extent. Hierarchical AttentionShift is performed at instance, part, and fine-grained levels, optimizing object semantics while promoting the conventional self-attention activation to hierarchical activation with local refinement. Experiments on PASCAL VOC 2012 Aug and MS-COCO 2017 benchmarks show that hierarchical AttentionShift improves the state-of-the-art (SOTA) method by 10.4% and 7.0% upon mean average precision (mAP)50, respectively. When applying hierarchical AttentionShift to the segment anything model (SAM), 9.4% AP improvement on the COCO test-dev is achieved. Hierarchical AttentionShift provides a fresh insight to regularize the self-attention mechanism for fine-grained vision tasks. The code is available at github.com/MingXiangL/AttentionShift.},
  archive      = {J_TNNLS},
  author       = {Mingxiang Liao and Fang Wan and Zonghao Guo and Qixiang Ye},
  doi          = {10.1109/TNNLS.2025.3526961},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Hierarchical AttentionShift for pointly supervised instance segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FRCL-MNER: A finer grained rank-based contrastive learning
framework for multimodal NER. <em>TNNLS</em>, 1–15. (<a
href="https://doi.org/10.1109/TNNLS.2025.3528567">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal named entity recognition (MNER) is an emerging field that aims to automatically detect named entities and classify their categories, utilizing input text and auxiliary resources such as images. While previous studies have leveraged object detectors to preprocess images and fuse textual semantics with corresponding image features, these methods often overlook the potential finer grained information within each modality and may exacerbate error propagation due to predetection. To address these issues, we propose a finer grained rank-based contrastive learning (FRCL) framework for MNER. This framework employs a global-level contrastive learning to align multimodal semantic features and a Top-K rank-based mask strategy to construct positive–negative pairs, thereby learning a finer grained multimodal interaction representation. Experimental results from three well-known social media datasets reveal that our approach surpasses existing strong baselines, and achieves up to a 1.54% improvement on the Twitter2015 dataset. Extensive discussions further confirm the effectiveness of our approach. We will release the source code on https://github.com/augusyan/FRCL.},
  archive      = {J_TNNLS},
  author       = {Tianwei Yan and Shan Zhao and Wentao Ma and Shezheng Song and Chengyu Wang and Zhibo Rao and Shizhao Chen and Zhigang Luo and Xinwang Liu},
  doi          = {10.1109/TNNLS.2025.3528567},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {FRCL-MNER: A finer grained rank-based contrastive learning framework for multimodal NER},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Persistent excitation of improved RBF neural networks:
Neuron dynamic-growing strategy. <em>TNNLS</em>, 1–8. (<a
href="https://doi.org/10.1109/TNNLS.2025.3528118">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This brief proposes a novel neuron dynamic-growing (NDG) strategy for radial basis function neural networks (RBF NNs). Only one neuron is selected in advance relying on the system initial states, and other neurons are dynamically generated based on the designed threshold for the distance between the current NN input and the closest neuron. Compared with the RBF NN using neuron fixed evenly spaced strategy (NFES), the improved RBF NN has two major advantages: one is to extremely reduce the number of neurons, especially for the high dimensional NN inputs; and the other is to provide a theoretical criteria for the choice of NN structure parameters including the neuron center and the compact set size. To guarantee the dynamic learning ability of the improved RBF NN, the persistent excitation (PE) is verified strictly by subtly constructing the threshold and the center of newly added neurons. Simulation and experimental results illustrate that the improved RBF NN integrated into the existing dynamic learning control effectively enhances the transient control performance, reduces the computational burden, and saves data storage space.},
  archive      = {J_TNNLS},
  author       = {Min Wang and Mingyu Wang and Chenguang Yang},
  doi          = {10.1109/TNNLS.2025.3528118},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-8},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Persistent excitation of improved RBF neural networks: Neuron dynamic-growing strategy},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Stacked ensemble deep random vector functional link network
with residual learning for medium-scale time-series forecasting.
<em>TNNLS</em>, 1–11. (<a
href="https://doi.org/10.1109/TNNLS.2025.3529219">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The deep random vector functional link (dRVFL) and ensemble dRVFL (edRVFL) succeed in various tasks and achieve state-of-the-art performance compared with other randomized neural networks (NNs). However, existing edRVFL structures need more diversity and error correction ability in an independent network. Our work fills the gap by combining stacked deep blocks and residual learning with the edRVFL. Subsequently, we propose a novel dRVFL combined with residual learning, ResdRVFL, whose deep layers calibrate the wrong estimations from shallow layers. Additionally, we propose incorporating a scaling parameter to control the scaling of residuals from shallow layers, thus mitigating the risk of overfitting. Finally, we present an ensemble deep stacking network, SResdRVFL, based on ResdRVFL. SResdRVFL aggregates multiple blocks into a cohesive network, leveraging the benefits of deep learning and ensemble learning. We evaluate the proposed model on 28 datasets and compare it with the state-of-the-art methods. The comparative study demonstrates that the SResdRVFL is the best-performing approach in terms of average ranking and errors based on 28 datasets.},
  archive      = {J_TNNLS},
  author       = {Ruobin Gao and Minghui Hu and Ruilin Li and Xuewen Luo and Ponnuthurai Nagaratnam Suganthan and M. Tanveer},
  doi          = {10.1109/TNNLS.2025.3529219},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Stacked ensemble deep random vector functional link network with residual learning for medium-scale time-series forecasting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Machine unlearning: Taxonomy, metrics, applications,
challenges, and prospects. <em>TNNLS</em>, 1–21. (<a
href="https://doi.org/10.1109/TNNLS.2025.3530988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Personal digital data is a critical asset, and governments worldwide have enforced laws and regulations to protect data privacy. Data users have been endowed with the “right to be forgotten” (RTBF) of their data. In the course of machine learning (ML), the forgotten right requires a model provider to delete user data and its subsequent impact on ML models upon user requests. Machine unlearning (MU) emerges to address this, which has garnered ever-increasing attention from both industry and academia. Specifically, MU allows model providers to eliminate the influence of unlearned data without retraining the model from scratch, ensuring the model behaves as if it never encountered this data. While the area has developed rapidly, there is a lack of comprehensive surveys to capture the latest advancements. Recognizing this shortage, we conduct an extensive exploration to map the landscape of MU including the (fine-grained) taxonomy of unlearning algorithms under centralized and distributed settings, debate on approximate unlearning, verification and evaluation metrics, and challenges and solutions across various applications. We also focus on the motivations, challenges, and specific methods for deploying unlearning in large language models (LLMs), as well as the potential attacks targeting unlearning processes. The survey concludes by outlining potential directions for future research, hoping to serve as a beacon for interested scholars.},
  archive      = {J_TNNLS},
  author       = {Na Li and Chunyi Zhou and Yansong Gao and Hui Chen and Zhi Zhang and Boyu Kuang and Anmin Fu},
  doi          = {10.1109/TNNLS.2025.3530988},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-21},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Machine unlearning: Taxonomy, metrics, applications, challenges, and prospects},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wavelet transformer: An effective method on multiple
periodic decomposition for time series forecasting. <em>TNNLS</em>,
1–15. (<a href="https://doi.org/10.1109/TNNLS.2025.3525502">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series forecasting has attracted significant interest across various fields in recent years. Notably, Transformers have been extensively investigated for long-term time series forecasting (LTSF) due to their remarkable ability on modeling sequential data. However, the point-wise calculation of its self-attention leads to a challenging task for accurately capturing real-world time series’ local and global characteristics, especially with multiple seasonal periodic components and outliers. In this article, we leverage wavelet analysis to recognize different frequency patterns and design an effective attention mechanism for time series forecasting to address this issue. In detail, we employ the maximal overlap discrete wavelet transform (MODWT) to construct a novel wavelet attention (WA) mechanism and propose the wavelet transformer (Waveformer) prediction technique. This approach effectively extracts multiple periodic features, mitigates the influence of anomalies and improves the precision of time series prediction under seasonal-trend decomposition methods. Experimental evaluations on six real-world datasets from various application fields demonstrate that the multiple periodic decomposition strategy of Waveformer successfully captures time series seasonal patterns and improves forecasting performance in comparison with many state-of-art methods.},
  archive      = {J_TNNLS},
  author       = {Wei Wei and Zi’ang Wang and Bowen Pang and Jiannan Wang and Xue Liu},
  doi          = {10.1109/TNNLS.2025.3525502},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Wavelet transformer: An effective method on multiple periodic decomposition for time series forecasting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A nonlinear noise-resistant zeroing neural network model for
solving time-varying quaternion generalized lyapunov equation and
applications to color image processing. <em>TNNLS</em>, 1–13. (<a
href="https://doi.org/10.1109/TNNLS.2025.3526620">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The time-varying Lyapunov equation (TVLE) plays a crucial role in control design and system stability. However, there has been limited research conducted on the time-varying generalized Lyapunov equation in the quaternion field. To tackle the time-varying quaternion generalized Lyapunov equation, a nonlinear noise-resistant zeroing neural network (NNR-ZNN) model with a novel power activation function (NPAF) is devised. The issue of non-commutativity within quaternion is circumvented by utilizing the real representation. The theoretical analyses provide a sufficient explanation for the global stability, fixed-time convergence, and robustness of the NNR-ZNN model. Under several different kinds of noises, the exceptional robustness of the NNR-ZNN model is highlighted by comparison with other existing models. In the end, the successful applications of the NNR-ZNN model to color image fusion and color image denoising confirm the practical value of the NNR-ZNN model.},
  archive      = {J_TNNLS},
  author       = {Lin Xiao and Xiangru Yan and Yongjun He and Biao Luo and Qiya Song},
  doi          = {10.1109/TNNLS.2025.3526620},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A nonlinear noise-resistant zeroing neural network model for solving time-varying quaternion generalized lyapunov equation and applications to color image processing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multivariable collaborative modeling with knowledge transfer
and its application in soft sensing of iron flotation grade.
<em>TNNLS</em>, 1–12. (<a
href="https://doi.org/10.1109/TNNLS.2025.3538777">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the iron flotation production process, production stages often undergo updates due to equipment upgrades, changes in raw materials, and other reasons. The operating condition prediction model established based on data from previous production stages may not meet the requirements of the new stage, resulting in a significant waste of collected datasets. Data-driven models established using small samples collected during the current stage may lack accuracy due to the limited sample size. This study proposes a method based on knowledge transfer to effectively leverage a large amount of outdated data. It allows for the rapid establishment of a new model that aligns with production requirements while minimizing the need for additional data collection. In previous tailings grade soft sensors, more emphasis was placed on quality parameters such as flotation froth features, often overlooking production process parameters. To enhance model accuracy, we introduce a multivariate collaborative modeling approach. The experimental results and industrial applications validate the effectiveness of this method.},
  archive      = {J_TNNLS},
  author       = {Dingsen Zhang and Yingwei Zhang and Kaicheng Shang and Xianwen Gao},
  doi          = {10.1109/TNNLS.2025.3538777},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Multivariable collaborative modeling with knowledge transfer and its application in soft sensing of iron flotation grade},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PHFS: Progressive hierarchical feature selection based on
adaptive sample weighting. <em>TNNLS</em>, 1–13. (<a
href="https://doi.org/10.1109/TNNLS.2025.3525643">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Hierarchical feature selection is considered an effective technique to reduce the dimensionality of data with complex hierarchical label structures. Incorrect labels are a common and challenging issue in complex hierarchical data. However, the existing hierarchical methods often struggle to dynamically adapt to label noise and lack the flexibility to adjust sample weights. Therefore, their effectiveness in managing complex data with many classes and mitigating label noise is significantly limited. To address these issues, in this article, an adaptive sample weighting-based progressive hierarchical feature selection (PHFS) method was proposed, which dynamically adjusts the sample weights to focus on high-quality data. PHFS integrates progressive sample selection and hierarchical feature selection into a unified framework, thus enhancing its effectiveness in reducing the impact of label noise and achieving optimal performance. The progressive selection process is divided into initial and subsequent stages, focusing on correct and incorrect samples. In the initial stage, PHFS selects valuable and correct samples based on the adaptive weights calculated through hierarchical classification feedback, maximizing the guiding effect of the correctly labeled examples. In the subsequent stages, PHFS uses matrix factorization to preserve the structure of the correctly labeled samples, preventing the forgetting of the early selected samples and minimizing the negative impact of the mislabelled samples. The superiority of PHFS over 13 state-of-the-art methods was demonstrated by performing extensive experiments on eight real-world datasets, highlighting its effectiveness in reducing label noise and achieving optimal performance.},
  archive      = {J_TNNLS},
  author       = {Hong Zhao and Jie Shi and Yang Zhang},
  doi          = {10.1109/TNNLS.2025.3525643},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {PHFS: Progressive hierarchical feature selection based on adaptive sample weighting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CUDA-x: Unsupervised domain-adaptive vehicle-to-everything
collaboration via knowledge transfer and alignment. <em>TNNLS</em>,
1–15. (<a href="https://doi.org/10.1109/TNNLS.2025.3539358">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently emerged vehicle-to-everything (V2X) perception has revealed great potential to overcome the limitation of single-vehicle intelligence aided by vigorous interaction among on-road agents, while prior endeavors are practically developed on parameter-specific simulation or configuration-dynamic real-world setting, overlooking the transferability across various scenarios. In this article, we propose ${\underline{\text{u}}}$nsupervised ${\underline{\text{d}}}$omain-${\underline{\text{a}}}$daptive vehicle-to-everything ${\underline{\text{c}}}$ollaboration framework dubbed CUDA-X, which is built on top of a de facto collective model with key-point information exchange and instance adaptation. Specifically, collaborative knowledge transfer (CKT) is responsible for domain-agnostic feature reconstruction from nearby car or infrastructure by spatial–channel pooling operation in an elementwise manner. To promote the candidate alignment, a brand-new bin-based location correction (BLC) provides an auxiliary supervision for cross-dataset box refinement via residual coordinate encoding (RCE), and category-aware pooling alignment (CPA) is further designed for pulling the category-specific instance closer between source and target samples. We benchmark CUDA-X against the counterparts on four prevalent cooperative perception datasets, i.e., OPV2V, V2X-Sim, V2V4Real, and DAIR-V2X: it establishes the new state-of-the-art vehicle-to-vehicle (V2V) and vehicle-to-infrastructure (V2I) performances regardless of simulation or reality. We expect that this appealing attempt would provide an in-depth insight into domain generalization in the context of multiagent perception, and the code is publicly available soon.},
  archive      = {J_TNNLS},
  author       = {Hongbo Yin and Daxin Tian and Chunmian Lin and Xuting Duan and Jianshan Zhou and Dezong Zhao and Dongpu Cao},
  doi          = {10.1109/TNNLS.2025.3539358},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {CUDA-X: Unsupervised domain-adaptive vehicle-to-everything collaboration via knowledge transfer and alignment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive boundary control for synchronization of
reaction–diffusion neural networks with random time-varying delay.
<em>TNNLS</em>, 1–12. (<a
href="https://doi.org/10.1109/TNNLS.2025.3540449">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article addresses the synchronization problem of reaction–diffusion neural networks (RDNNs) with random time-varying delay (RTVD) via boundary control (BC) (including adaptive BC and BC with constant-valued gain) under distributed measurements or boundary measurements. First, a novel BC strategy with constant-valued gain is designed, which considers three cases of the measurements, that is, distributed measurements, boundary measurements, and both coexist. Subsequently, an adaptive BC scheme under boundary measurements is proposed, where the control gain is regulated effectively. Next, based on the inequality techniques and Lyapunov direct approach, the delay-dependent synchronization conditions are gained and some linear matrix inequalities (LMIs) based theorems are given. Then, the BC design for the delayed RDNNs is transformed into an LMI feasibility problem. Finally, the developed BC approaches are validated by the simulation results.},
  archive      = {J_TNNLS},
  author       = {Xu Zhang and Biao Luo and Zi-Peng Wang and Xiaodong Xu and Chunhua Yang},
  doi          = {10.1109/TNNLS.2025.3540449},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Adaptive boundary control for synchronization of Reaction–Diffusion neural networks with random time-varying delay},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Probabilistic topic modeling with transformer
representations. <em>TNNLS</em>, 1–15. (<a
href="https://doi.org/10.1109/TNNLS.2025.3538262">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The field of topic modelling was mostly dominated by Bayesian graphical models during the last decade. With the rise of transformers in natural language processing, however, several successful models that rely on straightforward clustering approaches in transformer-based embedding spaces have emerged and consolidated the notion of topics as clusters of embedding vectors. We propose the transformer-representation neural topic model (TNTM), which combines the benefits of topic representations in transformer-based embedding spaces and probabilistic modeling. Therefore, this approach unifies the powerful and versatile notion of topics based on transformer embeddings with fully probabilistic modeling, as in models such as latent Dirichlet allocation (LDA). We utilize the variational autoencoder (VAE) framework for improved inference speed and modeling flexibility. Experimental results show that our proposed model achieves results on par with various state-of-the-art approaches in terms of embedding coherence while maintaining almost perfect topic diversity. The corresponding source code is available at: https://github.com/ArikReuter/TNTM.},
  archive      = {J_TNNLS},
  author       = {Arik Reuter and Anton Thielmann and Christoph Weisser and Benjamin Säfken and Thomas Kneib},
  doi          = {10.1109/TNNLS.2025.3538262},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Probabilistic topic modeling with transformer representations},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). S3INet: Semantic-information space sharing interaction
network for arbitrary shape text detection. <em>TNNLS</em>, 1–15. (<a
href="https://doi.org/10.1109/TNNLS.2025.3538806">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detecting arbitrary shape text is a challenging task due to the significant variation in text shape, size, and aspect ratio, as well as the complexity of scene backgrounds. The enhancing feature extraction capabilities is essential for the boosting text detection accuracy. However, traditional text feature extraction methods face several issues, including insufficient multiscale feature fusion, limited information transfer between different feature levels, and constrained receptive field expansion when using asymmetric convolutional kernels for long text detection. To address these challenges, this article introduces an arbitrarily shaped scene text detector called the semantic-information space sharing interaction network (S3INet). The proposed network leverages the semantic-information space sharing module (S3M) to generate a single-level feature map capable of capturing multiscale features with rich semantic information and prominent foreground elements. In addition, we propose the multibranch parallel asymmetric convolutional module (MPACM) group to enhance the representation of text features, thereby further enhancing text detection performance. Extensive experimental evaluations on five publicly available natural scene text datasets (CTW-1500, Total-Text, MSRA-TD500, ICDAR2015, and ICDAR2017-MLT) and two traffic text datasets (CTST-1600 and TPD) demonstrate the superiority of our method. The results indicate that S3INet significantly outperforms most existing state-of-the-art methods in both accuracy and robustness. The code will be released at: https://github.com/runminwang/S3INet.},
  archive      = {J_TNNLS},
  author       = {Runmin Wang and Hua Chen and Yanbin Zhu and Juan Xu and Xiaofei Cao and Zhenlin Zhu and Shengyou Qian and Changxin Gao and Li Liu and Nong Sang},
  doi          = {10.1109/TNNLS.2025.3538806},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {S3INet: Semantic-information space sharing interaction network for arbitrary shape text detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fast graph construction-driven rotating machine fault
diagnosis method using edge predictor. <em>TNNLS</em>, 1–9. (<a
href="https://doi.org/10.1109/TNNLS.2025.3539390">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph-based machine fault diagnosis methods are successfully used in extracting relationship information. However, the heavy computational burden of K-nearest neighbor graph (KNNG) has limited its application. To overcome it, a fast graph construction-driven rotating machine fault diagnosis method using an edge predictor is proposed in this article. The edge predictor, pretrained on an edge connection prediction task, is designed to learn how to get a distance matrix from an initial KNNG (IKNNG). Subsequently, numerous samples are directly input to the edge predictor, obtaining the generated distance matrix and enabling fast KNNG construction. Compared to the traditional KNNG construction method, this approach outputs directly without calculating the distance matrix, significantly reducing the computational burden. The experimental results show that the performance of the proposed method is as well as existing graph data-driven methods. Furthermore, theoretical analysis reveals that the quality of the constructed KNNG is similar to the KNNG obtained by traditional distance matrix calculations, but with a significantly reduced computational load.},
  archive      = {J_TNNLS},
  author       = {Chaoying Yang and Jie Liu and Yue Wang and Shuangye Yang and Tielin Shi},
  doi          = {10.1109/TNNLS.2025.3539390},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-9},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A fast graph construction-driven rotating machine fault diagnosis method using edge predictor},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Temporal-interim pose synthesis and distillation for dynamic
human pose estimation. <em>TNNLS</em>, 1–15. (<a
href="https://doi.org/10.1109/TNNLS.2025.3541936">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the task of dynamic human pose estimation (dynamic HPE), the temporal relationships between human body parts should be captured comprehensively to understand the dynamic human motions, where the correlated motion information eventually helps to recognize body parts. The popular methods are successful in terms of utilizing long-term motion information captured by low-speed cameras. Yet they neglect the underlying intermediate motions between captured frames, which comprise the temporal-interim poses lost in the video. In this article, we introduce a novel framework, temporal-interim pose synthesis and distillation, to produce and leverage the intermediate motion information for dynamic motion establishment. The pose synthesis yields the visual feature maps of the intermediate poses, which appear between the existing video frames. It allows the synthesized and current poses to form richer motion patterns. Next, the pose distillation divides the body parts into several groups, where it learns the specific part-wise relationship within each group. It degrades the complexity of learning useful part-wise relationships from rich motion patterns and extracts more detailed motion information for fine-grained part groups. We extensively evaluate our method on challenging datasets for dynamic pose estimation, achieving state-of-the-artresults.},
  archive      = {J_TNNLS},
  author       = {Renjie Zhang and Di Lin and Xin Wang and Ruonan Liu and Bin Sheng and George Baciu and C. L. Philip Chen and Ping Li},
  doi          = {10.1109/TNNLS.2025.3541936},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Temporal-interim pose synthesis and distillation for dynamic human pose estimation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conditional mutual information constrained deep learning for
classification. <em>TNNLS</em>, 1–13. (<a
href="https://doi.org/10.1109/TNNLS.2025.3540014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The concepts of conditional mutual information (CMI) and normalized CMI (NCMI) are introduced to measure the concentration and separation performance of a classification deep neural network (DNN) in the output probability distribution space of the DNN, where CMI and the ratio between CMI and NCMI represent the intraclass concentration and interclass separation of the DNN, respectively. By using NCMI to evaluate popular DNNs pretrained over CIFAR-100 and ImageNet in the literature, it is shown that their validation accuracies are more or less inversely proportional to their NCMI values. Based on this observation, the standard deep learning (DL) framework is further modified to minimize the standard cross entropy (CE) function subject to an NCMI constraint, yielding CMI constrained DL (CMIC-DL). A novel alternating learning algorithm is proposed to solve such a constrained optimization problem. Extensive experimental results show that DNNs trained within CMIC-DL outperform the state-of-the-art models trained within the standard DL and other loss functions in the literature in terms of both accuracy and robustness against adversarial attacks. In addition, visualizing the evolution of the learning process through the lens of CMI and NCMI is also advocated.},
  archive      = {J_TNNLS},
  author       = {En-Hui Yang and Shayan Mohajer Hamidi and Linfeng Ye and Renhao Tan and Beverly Yang},
  doi          = {10.1109/TNNLS.2025.3540014},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Conditional mutual information constrained deep learning for classification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coupled homogeneous hopfield neural networks: Simplest model
design, synchronization, and multiplierless circuit implementation.
<em>TNNLS</em>, 1–8. (<a
href="https://doi.org/10.1109/TNNLS.2025.3539283">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When using a synapse as a coupler to connect neurons, parameter-based synchronization transitions have been investigated. However, the dependence on initial conditions has not been comprehensively discussed in the literature. This work presents an electrical-synapse-coupled model consisting of two homogeneous Hopfield neural networks (HNNs), which is the simplest network-to-network coupling model known for HNN. The model possesses several fixed points, which are found to be unstable. Simulation results of peak differences, bifurcation diagrams, and normalized mean synchronization errors indicate that complex synchronization transitions occur, depending on both the electrical coupling strength and initial conditions. Particularly, we focus here on mapping the basins of attraction between periodic and chaotic synchronization for bistable patterns. Finally, a multiplierless electrical neuron circuit is developed to validate initial condition-induced synchronization phenomena, which provides a new perspective for the study of collective dynamics of brain-like networks and the development of lightweight neuromorphic circuits.},
  archive      = {J_TNNLS},
  author       = {Fuhong Min and Chengjie Chen and Neil G. R. Broderick},
  doi          = {10.1109/TNNLS.2025.3539283},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-8},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Coupled homogeneous hopfield neural networks: Simplest model design, synchronization, and multiplierless circuit implementation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Disentanglement of prosody representations via diffusion
models and scheduled gradient reversal. <em>TNNLS</em>, 1–12. (<a
href="https://doi.org/10.1109/TNNLS.2025.3534822">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prosody plays a fundamental role in human speech and communication, facilitating intelligibility and conveying emotional and cognitive states. Extracting accurate prosodic information from speech is vital for building assistive technology, such as controllable speech synthesis, speaking style transfer, and speech emotion recognition (SER). However, it is challenging to disentangle speaker-independent prosody representations since prosodic attributes, such as intonation, excessively entangle with speaker-specific attributes, e.g., pitch. In this article, we propose a novel model, called Diffsody, to disentangle and refine prosody representations: 1) to disentangle prosody representations, we leverage the expressive generative ability of a diffusion model by conditioning it on quantified semantic information and pretrained speaker embeddings. Additionally, a prosody encoder automatically learns prosody representations used for spectrogram reconstruction in an unsupervised fashion; and 2) to refine and learn speaker-invariant prosody representations, a scheduled gradient reversal layer (sGRL) is proposed and integrated into the prosody encoder of Diffsody. We extensively evaluate Diffsody through qualitative and quantitative means. t-SNE visualization and speaker verification experiments demonstrate the efficacy of the sGRL method in preventing speaker-specific information leakage. Experimental results on speaker-independent SER and automatic depression detection (ADD) tasks demonstrate that Diffsody can efficiently factorize speaker-independent prosody representations, resulting in a significant boost in SER and ADD. In addition, Diffsody synergistically integrates with the semantic representation model WavLM, which leads to a discernibly elevated performance, outperforming contemporary methods in both SER and ADD tasks. Furthermore, the Diffsody model exhibits promising potential for various practical applications, such as voice or style conversion. Some audio samples can be found on our https://leyuanqu.github.io/Diffsody/demo website.},
  archive      = {J_TNNLS},
  author       = {Leyuan Qu and Cornelius Weber and Wei Wang and Jia Jin and Yingming Gao and Taihao Li and Stefan Wermter},
  doi          = {10.1109/TNNLS.2025.3534822},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Disentanglement of prosody representations via diffusion models and scheduled gradient reversal},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TDSF-net: Tensor decomposition-based subspace fusion network
for multimodal medical image classification. <em>TNNLS</em>, 1–14. (<a
href="https://doi.org/10.1109/TNNLS.2025.3541170">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data from multimodalities bring complementary information for deep learning-based medical image classification models. However, data fusion methods simply concatenating features or images barely consider the correlations or complementarities among different modalities and easily suffer from exponential growth in dimensions and computational complexity when the modality increases. Consequently, this article proposes a subspace fusion network with tensor decomposition (TD) to heighten multimodal medical image classification. We first introduce a Tucker low-rank TD module to map the high-level dimensional tensor to the low-rank subspace, reducing the redundancy caused by multimodal data and high-dimensional features. Then, a cross-tensor attention mechanism is utilized to fuse features from the subspace into a high-dimension tensor, enhancing the representation ability of extracted features and constructing the interaction information among components in the subspace. Extensive comparison experiments with state-of-the-art (SOTA) methods are conducted on one self-established and three public multimodal medical image datasets, verifying the effectiveness and generalization ability of the proposed method. The code is available at https://github.com/1zhang-yi/TDSFNet.},
  archive      = {J_TNNLS},
  author       = {Yi Zhang and Guoxia Xu and Meng Zhao and Hao Wang and Fan Shi and Shengyong Chen},
  doi          = {10.1109/TNNLS.2025.3541170},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {TDSF-net: Tensor decomposition-based subspace fusion network for multimodal medical image classification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic self-organizing neurons. <em>TNNLS</em>, 1–15. (<a
href="https://doi.org/10.1109/TNNLS.2025.3542063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many currently available deep neural network (DNN) accelerators are highly application specific and have focused on supervised learning. In addition, many accelerators have rigid architectures and algorithms that prevent adapting to dynamic environments. In this work, we propose a neuromorphic architecture implementing a self-organizing feature map (SOFM) using ferroelectric field-effect transistors (FeFETs) for in-memory error computation. The neuromorphic architecture takes inspiration from biological networks and is able to grow neurons to adapt to the application. Furthermore, it is able to modulate the distance between neurons to provide more fluidity to its topography. We demonstrate that the ability of the network to adapt to various datasets and even exhibit lifelong learning and self-repair. We further demonstrate the architecture’s efficiency in terms of both power and speed as well as its robustness to device variability.},
  archive      = {J_TNNLS},
  author       = {Siddharth Barve and Rashmi Jha},
  doi          = {10.1109/TNNLS.2025.3542063},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Dynamic self-organizing neurons},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Latency adjustable transformer encoder for language
understanding. <em>TNNLS</em>, 1–14. (<a
href="https://doi.org/10.1109/TNNLS.2025.3539727">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Adjusting the latency, power, and accuracy of natural language understanding models is a desirable objective of an efficient architecture. This article proposes an efficient Transformer architecture that adjusts the inference computational cost adaptively with a desired inference latency speedup. In the fine-tuning phase, the proposed method detects less important hidden sequence elements (word-vectors) and eliminates them in each encoder layer using a proposed attention context contribution (ACC) metric. After the fine-tuning phase, with the novel offline-tuning property, the inference latency of the model can be adjusted in a wide range of inference speedup selections without any further training. Extensive experiments reveal that most word-vectors in higher Transformer layers contribute less to subsequent layers, allowing their removal to improve inference latency. Experimental results on various language understanding, text generation, and instruction tuning tasks and benchmarks demonstrate the approach’s effectiveness across diverse datasets, with minimal impact on the input’s global context. The technique improves the time-to-first-token (TTFT) of Llama3 by up to 2.9×, with a minor performance drop. The suggested approach posits that in large language models (LLMs), although the complete network is necessary for training, it can be truncated during the fine-tuning phase.},
  archive      = {J_TNNLS},
  author       = {Sajjad Kachuee and Mohammad Sharifkhani},
  doi          = {10.1109/TNNLS.2025.3539727},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Latency adjustable transformer encoder for language understanding},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Random orthogonal additive filters: A solution to the
vanishing/exploding gradient of deep neural networks. <em>TNNLS</em>,
1–14. (<a href="https://doi.org/10.1109/TNNLS.2025.3538924">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Since the recognition in the early 1990s of the vanishing/exploding (V/E) gradient issue plaguing the training of neural networks (NNs), significant efforts have been exerted to overcome this obstacle. However, a clear solution to the V/E issue remained elusive so far. The pursuit of approximate dynamical isometry, i.e., parameter configurations where the singular values of the input–output Jacobian (IOJ) are tightly distributed around 1, leads to the derivation of an NN’s architecture that shares common traits with the popular residual network (ResNet) model. Instead of skipping connections between layers, the idea is to filter the previous activations orthogonally and add them to the nonlinear activations of the next layer, realizing a convex combination between them. Remarkably, the impossibility of the gradient updates to either vanish or explode is demonstrated with analytical bounds that hold even in the infinite depth case. The effectiveness of this method is empirically proved by means of training via backpropagation an extremely deep multilayer perceptron (MLP) of 50k layers, and an Elman NN to learn long-term dependencies in the input of 10k time steps in the past. Compared with other architectures specifically devised to deal with the V/E problem, e.g., LSTMs, the proposed model is way simpler yet more effective. Surprisingly, a single-layer vanilla recurrent NN (RNN) can be enhanced to reach state-of-the-art performance, while converging super fast; for instance, on the psMNIST task, it is possible to get test accuracy of over 94% in the first epoch, and over 98% after just ten epochs.},
  archive      = {J_TNNLS},
  author       = {Andrea Ceni},
  doi          = {10.1109/TNNLS.2025.3538924},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Random orthogonal additive filters: A solution to the Vanishing/Exploding gradient of deep neural networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Backdoor attacks and countermeasures in natural language
processing models: A comprehensive security review. <em>TNNLS</em>,
1–21. (<a href="https://doi.org/10.1109/TNNLS.2025.3540303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Language models (LMs) are becoming increasingly popular in real-world applications. Outsourcing model training and data hosting to third-party platforms has become a standard method for reducing costs. In such a situation, the attacker can manipulate the training process or data to inject a backdoor into models. Backdoor attacks are a serious threat where malicious behavior is activated when triggers are present; otherwise, the model operates normally. However, there is still no systematic and comprehensive review of LMs from the attacker’s capabilities and purposes on different backdoor attack surfaces. Moreover, there is a shortage of analysis and comparison of the diverse emerging backdoor countermeasures. Therefore, this work aims to provide the natural language processing (NLP) community with a timely review of backdoor attacks and countermeasures. According to the attackers’ capability and affected stage of the LMs, the attack surfaces are formalized into four categorizations: attacking the pretrained model with fine-tuning (APMF) or parameter-efficient fine-tuning (PEFT), attacking the final model with training (AFMT), and attacking large language model (ALLM). Thus, attacks under each categorization are combed. The countermeasures are categorized into two general classes: sample inspection and model inspection. Thus, we review countermeasures and analyze their advantages and disadvantages. Also, we summarize the benchmark datasets and provide comparable evaluations for representative attacks and defenses. Drawing the insights from the review, we point out the crucial areas for future research on the backdoor, especially soliciting more efficient and practical countermeasures.},
  archive      = {J_TNNLS},
  author       = {Pengzhou Cheng and Zongru Wu and Wei Du and Haodong Zhao and Wei Lu and Gongshen Liu},
  doi          = {10.1109/TNNLS.2025.3540303},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-21},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Backdoor attacks and countermeasures in natural language processing models: A comprehensive security review},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DNRHP: Temporal network representation learning via hawkes
point process. <em>TNNLS</em>, 1–7. (<a
href="https://doi.org/10.1109/TNNLS.2025.3540195">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph neural networks (GNNs) have significantly advanced our ability to mine structured data, playing a central role in areas such as social networks and recommendation systems. However, while most GNN-based methods focus on learning node representations in static graphs, they often ignore the dynamic nature of real-world networks, limiting their applicability. Furthermore, existing dynamic representation learning methods using Hawkes point processes, while capable of modeling event sequences, are inherently transductive and tailored to specific scenarios with dual timescales and mixed event types, thus not fully generalizable. To bridge this gap, we introduce DNRHP, a novel framework for learning temporal network representations. Specifically, DNRHP integrates historical edge (HE) information with the network’s evolutionary properties, using the Hawkes point process to model edge formation. It captures not only the influence of past events on the likelihood of future connections but also the impact of the structural evolution of the network. The novelty of our model lies in its comprehensive consideration of the dynamics of network evolution and historical connectivity, allowing for a more accurate representation of nodes and their interactions over time. Extensive experiments on diverse real-world networks demonstrate the effectiveness of DNRHP, outperforming state-of-the-art baselines in terms of accuracy and efficiency for tasks such as node classification and link prediction.},
  archive      = {J_TNNLS},
  author       = {Changtian Ying and Qi Li and Chen Wang and Donghua Yu},
  doi          = {10.1109/TNNLS.2025.3540195},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-7},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {DNRHP: Temporal network representation learning via hawkes point process},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Meta learning task representation in multiagent
reinforcement learning: From global inference to local inference.
<em>TNNLS</em>, 1–14. (<a
href="https://doi.org/10.1109/TNNLS.2025.3540758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multiagent meta reinforcement learning (MAMRL) enables multiagent systems (MASs) to adapt to multiple tasks. However, partial observability poses a significant challenge by hindering efficient task inference from agents’ limited local experiences. To address this, we propose MG2L, a novel algorithm featuring a global-to-local (G2L) training scheme based on mutual information optimization (MIO). We first extend the centralized training and decentralized execution (CTDE) framework to MAMRL, and introduce a multilevel task encoder for joint global and local task inference. Building on this encoder, the MG2L scheme employs tailored loss functions to optimize task representations. For global inference, the MAS learns a centralized global representation by maximizing the MI between the representation and the task context. For local inference, we formulate conditional MI reduction to quantify the G2L gap. Agents then learn the local representation by minimizing this reduction. The MG2L scheme effectively harmonizes centralized training with decentralized execution, offering a versatile solution for MAMRL challenges. Additionally, we integrate a permutation-invariant attention (PIA) module into the task encoder to reduce sensitivity to behavior policy variations. Extensive experiments—including comparative analyses, ablation studies, meta-test evaluations, and visualizations—demonstrate MG2L’s effectiveness. The implementation of MG2L is publicly available at https://github.com/zhaozijie2022/mg2l.},
  archive      = {J_TNNLS},
  author       = {Zijie Zhao and Yuqian Fu and Jiajun Chai and Yuanheng Zhu and Dongbin Zhao},
  doi          = {10.1109/TNNLS.2025.3540758},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Meta learning task representation in multiagent reinforcement learning: From global inference to local inference},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A memristor-based neural network circuit with retrospective
revaluation effect and application in intelligent household robots.
<em>TNNLS</em>, 1–13. (<a
href="https://doi.org/10.1109/TNNLS.2025.3539842">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The traditional association theory maintains that associations between cues can change only in trials where the cue is actually presented. However, the retrospective revaluation (RR) studies the phenomenon that responses to a cue can change even when the cue is not actually presented. A hardware memristor-based neural network circuit with an RR effect is proposed in this article. The neural network circuit successfully demonstrates various phenomena of RR, including the impact of deflation and inflation of companion cue associations on target cue, higher order RR, and context dependence. The correctness of the circuit design is verified by Pspice simulation. The key feature of this design lies in its ability to learn cue associations even in training trials, where the target cues are absent. This distinctive attribute offers a fresh perspective for the creation of more intricate, brain-inspired information processing systems with enhanced integration capabilities.},
  archive      = {J_TNNLS},
  author       = {Junwei Sun and Yijin Shen and Yingcong Wang and Yanfeng Wang},
  doi          = {10.1109/TNNLS.2025.3539842},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A memristor-based neural network circuit with retrospective revaluation effect and application in intelligent household robots},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A systematic review on long-tailed learning. <em>TNNLS</em>,
1–21. (<a href="https://doi.org/10.1109/TNNLS.2025.3539314">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Long-tailed data are a special type of multiclass imbalanced data with a very large amount of minority/tail classes that have a very significant combined influence. Long-tailed learning (LTL) aims to build high-performance models on datasets with long-tailed distributions that can identify all the classes with high accuracy, in particular the minority/tail classes. It is a cutting-edge research direction that has attracted a remarkable amount of research effort in the past few years. In this article, we present a comprehensive survey of the latest advances in long-tailed visual learning. We first propose a new taxonomy for LTL, which consists of eight different dimensions, including data balancing, neural architecture, feature enrichment, logits adjustment, loss function, bells and whistles, network optimization, and posthoc processing techniques. Based on our proposed taxonomy, we present a systematic review of LTL methods, discussing their commonalities and alignable differences. We also analyze the differences between imbalance learning and LTL. Finally, we discuss prospects and future directions in this field.},
  archive      = {J_TNNLS},
  author       = {Chongsheng Zhang and George Almpanidis and Gaojuan Fan and Binquan Deng and Yanbo Zhang and Ji Liu and Aouaidjia Kamel and Paolo Soda and João Gama},
  doi          = {10.1109/TNNLS.2025.3539314},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-21},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {A systematic review on long-tailed learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). When heterophily meets heterogeneous graphs: Latent graphs
guided unsupervised representation learning. <em>TNNLS</em>, 1–14. (<a
href="https://doi.org/10.1109/TNNLS.2025.3540063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised heterogeneous graph representation learning (UHGRL) has gained increasing attention due to its significance in handling practical graphs without labels. However, heterophily has been largely ignored, despite its ubiquitous presence in real-world heterogeneous graphs. In this article, we define semantic heterophily and propose an innovative framework called latent graphs guided unsupervised representation learning (LatGRL) to handle this problem. First, we develop a similarity mining method that couples global structures and attributes, enabling the construction of fine-grained homophilic and heterophilic latent graphs (LGs) to guide the representation learning. Moreover, we propose an adaptive dual-frequency semantic fusion mechanism to address the problem of node-level semantic heterophily. To cope with the massive scale of real-world data, we further design a scalable implementation. Extensive experiments on benchmark datasets validate the effectiveness and efficiency of our proposed framework. The source code and datasets have been made available at https://github.com/zxlearningdeep/LatGRL.},
  archive      = {J_TNNLS},
  author       = {Zhixiang Shen and Zhao Kang},
  doi          = {10.1109/TNNLS.2025.3540063},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {When heterophily meets heterogeneous graphs: Latent graphs guided unsupervised representation learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). E-3SFC: Communication-efficient federated learning with
double-way features synthesizing. <em>TNNLS</em>, 1–15. (<a
href="https://doi.org/10.1109/TNNLS.2025.3539869">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The exponential growth in model sizes has significantly increased the communication burden in federated learning (FL). Existing methods to alleviate this burden by transmitting compressed gradients often face high compression errors, which slow down the model’s convergence. To simultaneously achieve high compression effectiveness and lower compression errors, we study the gradient compression problem from a novel perspective. Specifically, we propose a systematical algorithm termed extended single-step synthetic features compressing (E-3SFC), which consists of three subcomponents, i.e., the single-step synthetic features compressor (3SFC), a double-way compression (DWC) algorithm, and a communication budget scheduler (BS). First, we regard the process of gradient computation of a model as decompressing gradients from corresponding inputs, while the inverse process is considered as compressing the gradients. Based on this, we introduce a novel gradient compression method termed 3SFC, which utilizes the model itself as a decompressor, leveraging training priors such as model weights and objective functions. The 3SFC compresses raw gradients into tiny synthetic features in a single-step simulation, incorporating error feedback (EF) to minimize overall compression errors. To further reduce communication overhead, 3SFC is extended to E-3SFC, allowing DWC and dynamic communication budget scheduling. Our theoretical analysis under both strongly convex and nonconvex conditions demonstrates that 3SFC achieves linear and sublinear convergence rates with aggregation noise. Extensive experiments across six datasets and six models reveal that 3SFC outperforms the state-of-the-art methods by up to 13.4% while reducing communication costs by 111.6 times. These findings suggest that 3SFC can significantly enhance communication efficiency in FL without compromising model performance.},
  archive      = {J_TNNLS},
  author       = {Yuhao Zhou and Yuxin Tian and Mingjia Shi and Yuanxi Li and Yanan Sun and Qing Ye and Jiancheng Lv},
  doi          = {10.1109/TNNLS.2025.3539869},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {E-3SFC: Communication-efficient federated learning with double-way features synthesizing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cascade fusion and correlation enhancement for knowledge
distillation. <em>TNNLS</em>, 1–14. (<a
href="https://doi.org/10.1109/TNNLS.2025.3539991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Knowledge distillation (KD) improves the performance of a compact student network by transferring learned knowledge from a cumbersome teacher network. In the existing approaches, the multiscale feature knowledge is transferred via densely connected paths, which increases the optimization difficulty. Moreover, correlations among the labels are neglected despite their capability to enhance the intraclass similarity of samples. To solve these issues, we propose cascade fusion and correlation enhancement for KD (CC-KD). The multiscale feature knowledge is transferred via much simpler paths, which are constructed by fusing features of different scales with cross-scale attention (CSA) in a cascade manner, thereby reducing the optimization difficulty. On the other hand, the relational knowledge of teacher logits is further enhanced by correlations of the corresponding labels, so that the student can produce more similar logits for the samples in the same category. Extensive experimental results on five public datasets (i.e., CIFAR100/10, ImageNet, RAF-DB, and FERPlus) indicate superior performance of the proposed method over several state-of-the-arts (SOTAs). More specifically, our method obtains an accuracy of 71.70% on ImageNet and achieves a new record of 90.20% on RAF-DB with fewer calculations and parameters.},
  archive      = {J_TNNLS},
  author       = {Bin Sun and Zuxiang Long and Ziyu Ma and Shutao Li},
  doi          = {10.1109/TNNLS.2025.3539991},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Cascade fusion and correlation enhancement for knowledge distillation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Harmonic fast one-step cut: An efficient strategy for
spectral clustering optimization. <em>TNNLS</em>, 1–14. (<a
href="https://doi.org/10.1109/TNNLS.2025.3539628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the excellent performance of spectral clustering (SC), it has been widely used in many fields of application. However, the high computational complexity and two successive steps have limited SC’s development. In addition, the traditional SC is formulated to maximize the arithmetic mean of trace ratios which is dominated by the larger objectives and may reduce the recognition accuracy in practical applications. In this article, we propose a novel graph cut criterion to minimize the trace ratios of harmonic mean with objectives, which can avoid the worst-cluster issue without imposing any regularization or constraints. Furthermore, an efficient and effective coordinate descent (CD) method is exploited to achieve a one-step solution. Therefore, this article can simultaneously solve three main challenges in a unified framework. Extensive experiments verify that the harmonic fast one-step graph cut (HFOC) achieves superior clustering performance with relatively less time-consuming compared to the other state-of-the-art clustering methods.},
  archive      = {J_TNNLS},
  author       = {Jingwei Chen and Shasha Fu and Hui Yang and Feiping Nie},
  doi          = {10.1109/TNNLS.2025.3539628},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Harmonic fast one-step cut: An efficient strategy for spectral clustering optimization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CPST-GAN: Conditional probabilistic state transition
generative adversarial network with the biomedical large foundation
models. <em>TNNLS</em>, 1–14. (<a
href="https://doi.org/10.1109/TNNLS.2025.3539006">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The risk prediction of Alzheimer’s disease (AD) is crucial for its early prevention and treatment. However, current risk prediction methods face challenges in effectively extracting and fusing multiomics features, particularly overlooking the multilevel evolutionary mechanisms of AD. This article combines biomedical large foundation models with the conditional generative adversarial network (GAN) to mine the evolutionary patterns of AD by considering the regulatory effect of genes on brain lesions. Specifically, we first use biomedical large foundation models to effectively construct high-quality imaging genetic features. Next, a conditional probabilistic state transition mathematical model is constructed to describe AD progression as state transitions of brain regions under genetic regulations. Based on the mathematical model, a conditional probabilistic state transition GAN (CPST-GAN) is proposed. This algorithm can mine the dynamic evolutionary patterns of AD by fusing brain imaging and genetic features to achieve risk prediction of AD. Finally, experiments on the public imaging genetics datasets validate the effectiveness and superiority of CPST-GAN in evolutionary pattern mining and risk prediction of AD. This article not only provides a reliable intelligence algorithm for early intervention of AD but also offers new insights for future research on AD pathogenesis. The code has been published at github.com/fmri123456/CPST-GAN.},
  archive      = {J_TNNLS},
  author       = {Qiong Wang and Luyun Xu and Yinglu Shan and Wenzhuo Shen and Lou Li and Xia-An Bi and Zhonghua Liu},
  doi          = {10.1109/TNNLS.2025.3539006},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {CPST-GAN: Conditional probabilistic state transition generative adversarial network with the biomedical large foundation models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning decision boundaries for multidimensional anomaly
detection. <em>TNNLS</em>, 1–14. (<a
href="https://doi.org/10.1109/TNNLS.2025.3540602">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this article, we attempt to study a problem of identifying outliers from multiple dimensions, terming it as multidimensional anomaly detection. This task assumes that each sample corresponds to heterogeneous discriminative spaces, where each space characterizes distinct semantic information along one dimension. Consequently, the abnormalities exhibited by a sample will vary under these semantically different dimensions. In contrast to traditional anomaly detection, multidimensional anomaly detection offers a more holistic assessment of a sample’s abnormalities. However, the heterogeneity of discriminative spaces leads to incomparability of the outputs from different dimensions which is the major difficulty in designing multidimensional anomaly detection methods. This article introduces a novel model, maximum margin multidimensional anomaly detection (ALOE), specifically tailored for multidimensional anomaly detection. ALOE constructs a convex optimization problem with nonlinear constraints. The primary objective is to simultaneously learn multiple decision boundaries, utilizing the maximum margin principle and covariance regularization, while distinguishing between outliers and normal samples under multiple dimensions by capturing the correlation among multiple dimensions. To obtain the optimal decision boundary under each dimension, we devise an alternating optimization method for this convex optimization problem. To validate the effectiveness of ALOE, we conduct extensive experiments on 12 real-world datasets, comparing its performance against 34 anomaly detection methods. The experimental results demonstrate the superior performance of ALOE.},
  archive      = {J_TNNLS},
  author       = {Xinye Wang and Lei Duan and Lili Guan and Jiaxuan Xu and Chengxin He},
  doi          = {10.1109/TNNLS.2025.3540602},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {Learning decision boundaries for multidimensional anomaly detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PPGF: Probability pattern-guided time series forecasting.
<em>TNNLS</em>, 1–12. (<a
href="https://doi.org/10.1109/TNNLS.2025.3540873">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Time series forecasting (TSF) is an essential branch of machine learning with various applications. Most methods for TSF focus on constructing different networks to extract better information and improve performance. However, practical application data contain different internal mechanisms, resulting in a mixture of multiple patterns. That is, the model’s ability to fit different patterns is different and generates different errors. In order to solve this problem, we propose an end-to-end framework, namely probability pattern-guided time series forecasting (PPGF). PPGF reformulates the TSF problem as a forecasting task guided by probabilistic pattern classification. First, we propose the grouping strategy to approach forecasting problems as classification and alleviate the impact of data imbalance on classification. Second, we predict the corresponding class interval to guarantee the consistency of classification and forecasting. In addition, true class probability (TCP) is introduced to pay more attention to the difficult samples to improve the classification accuracy. Detailedly, PPGF classifies the different patterns to determine which one the target value may belong to and estimates it accurately in the corresponding interval. To demonstrate the effectiveness of the proposed framework, we conduct extensive experiments on real-world datasets, and PPGF achieves significant performance improvements over several baseline methods. Furthermore, the effectiveness of TCP and the necessity of consistency between classification and forecasting are proved in the experiments. All data and codes are available online: https://github.com/syrGitHub/PPGF.},
  archive      = {J_TNNLS},
  author       = {Yanru Sun and Zongxia Xie and Haoyu Xing and Hualong Yu and Qinghua Hu},
  doi          = {10.1109/TNNLS.2025.3540873},
  journal      = {IEEE Transactions on Neural Networks and Learning Systems},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Neural Netw. Learn. Syst.},
  title        = {PPGF: Probability pattern-guided time series forecasting},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
