<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TMM_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tmm---92">TMM - 92</h2>
<ul>
<li><details>
<summary>
(2025). Language-assisted 3D scene understanding. <em>TMM</em>,
1–12. (<a href="https://doi.org/10.1109/TMM.2025.3535305">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The scale and quality of point cloud datasets constrain the advancement of point cloud learning. Recently, with the development of multi-modal learning, the incorporation of domain-agnostic prior knowledge from other modalities, such as images and text, to assist in point cloud feature learning has been considered a promising avenue. Existing methods have demonstrated the effectiveness of multi-modal contrastive training and feature distillation on point clouds. However, challenges remain, including the requirement for paired triplet data, redundancy and ambiguity in supervised features, and the disruption of the original priors. In this paper, we propose a language-assisted approach to point cloud feature learning (LAST-PCL), enriching semantic concepts through large language model-based text enrichment. We achieve de-redundancy and feature dimensionality reduction without compromising textual priors by statistical-based and training-free significant feature selection. Furthermore, we also delve into an in-depth analysis of the impact of text contrastive training on the point cloud. Extensive experiments validate that the proposed method learns semantically meaningful point cloud features and achieves state-of-the-art or comparable performance in 3D semantic segmentation, 3D object detection, and 3D scene classification tasks.},
  archive      = {J_TMM},
  author       = {Yanmin Wu and Qiankun Gao and Renrui Zhang and Haijie Li and Jian Zhang},
  doi          = {10.1109/TMM.2025.3535305},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Language-assisted 3D scene understanding},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RSUIA: Dynamic no-reference underwater image assessment via
reinforcement sequences. <em>TMM</em>, 1–14. (<a
href="https://doi.org/10.1109/TMM.2025.3535308">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Underwater image quality assessment (UIQA) is a challenging task due to the complexities of underwater environments. Traditional UIQA methods primarily rely on fitting mean opinion scores (MOS), which are limited by human visual biases. To address the above limitation, we propose a no-reference underwater image quality assessment paradigm using reinforcement sequences. Our paradigm leverages reinforcement learning to iteratively merge the input image with the corresponding ground truth, generating an optimized sequence of images. A classifier generates probability arrays for the optimized sequence, which are converted into objective scores by a regression model. Unlike existing methods that focus solely on the final quality score, our paradigm emphasizes dynamic quality changes throughout the image-enhancement process. By employing objective mixing ratio labels, our reinforcement sequence dataset reduces subjective bias. The multiscale classifier captures local and global information differences between the input and ground truth images, effectively preserving the contrast and detail in diverse lighting conditions. Our paradigm combines multi-source data classification with support vector regression, optimizing the mapping of feature vectors to quality scores through fine-tuning libsvm kernel parameters. Experimental results on multiple benchmark datasets demonstrate that our paradigm outperforms the state-of-the-art UIQA methods, providing an effective solution for Underwater Image quality Assessment via Reinforcement Sequences (RSUIA). Our code will be available at: https://github.com/zhoujingchun03/RSUIA.},
  archive      = {J_TMM},
  author       = {Jingchun Zhou and Chunjiang Liu and Dehuan Zhang and Zongxin He and Ferdous Sohel and Qiuping Jiang},
  doi          = {10.1109/TMM.2025.3535308},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {RSUIA: Dynamic no-reference underwater image assessment via reinforcement sequences},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ETC: Temporal boundary expand then clarify for weakly
supervised video grounding with multimodal large language model.
<em>TMM</em>, 1–12. (<a
href="https://doi.org/10.1109/TMM.2024.3521758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Early weakly supervised video grounding (WSVG) methods often struggle with incomplete boundary detection due to the absence of temporal boundary annotations. To bridge the gap between video-level and boundary-level annotations, explicit supervision methods (i.e., generating pseudo-temporal boundaries for training) have achieved great success. However, data augmentation in these methods might disrupt critical temporal information, yielding poor pseudo-temporal boundaries. In this paper, we propose a new perspective that maintains the integrity of the original temporal content while introducing more valuable information for expanding the incomplete boundaries. To this end, we propose ETC (Expand then Clarify), first using the additional information to expand the initial incomplete pseudo-temporal boundaries, and subsequently refining these expanded ones to achieve precise boundaries. Motivated by video continuity, i.e., visual similarity across adjacent frames, we use powerful multi-modal large language models (MLLMs) to annotate each frame within the initial pseudo-temporal boundaries, yielding more comprehensive descriptions for expanded boundaries. To further clarify the noise in expanded boundaries, we combine mutual learning with a tailored proposal-level contrastive objective to use a learnable approach to harmonize a balance between incomplete yet clean (initial) and comprehensive yet noisy (expanded) boundaries for more precise ones. Experiments demonstrate the superiority of our method on two challenging WSVG datasets.},
  archive      = {J_TMM},
  author       = {Guozhang Li and Xinpeng Ding and De Cheng and Jie Li and Nannan Wang and Xinbo Gao},
  doi          = {10.1109/TMM.2024.3521758},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {ETC: Temporal boundary expand then clarify for weakly supervised video grounding with multimodal large language model},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Delving into quaternion wavelet transformer for facial
expression recognition in the wild. <em>TMM</em>, 1–14. (<a
href="https://doi.org/10.1109/TMM.2025.3535361">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Facial Expression Recognition (FER) technique has increasingly matured over time. However, recognizing facial expressions in wild environments poses great challenges in achieving promising performance. The main obstacles arise from various factors, such as illumination changes, head pose variations, and occlusions. To overcome interferences from external environments and improve recognition accuracy, we propose a novel Quaternion Wavelet TRansformer (QWTR) model for FER in the wild. Specifically, we present a Quaternion Value Transformer (QVT) network that combines quaternion multi-head attention with quaternion CNN to capture emotional cues from global and local perception. To preserve the color structure while enhancing image contrast and brightness, we introduce a Quaternion Histogram Equalization (QHE) representation to transform color images into quaternion matrices representation. After that, to alleviate the impact of head pose and occlusion together with feature redundancy, a Quaternion Wavelet Feature Selection (QWFS) scheme is designed to decompose quaternion features and select the most correlated signals. Extensive experiments have been conducted on four in-the-wild FER datasets and several specific FER benchmarks under various conditions. The qualitative and quantitative results demonstrate that QWTR outperforms other state-of-the-art methods in FER benchmarks, e.g., 68.37% vs. 66.31% accuracy on the AffectNet dataset. Code is available at https://github.com/zy197997312/QWTR-for-FER.},
  archive      = {J_TMM},
  author       = {Yu Zhou and Jialun Pei and Weixin Si and Jing Qin and Pheng-Ann Heng},
  doi          = {10.1109/TMM.2025.3535361},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Delving into quaternion wavelet transformer for facial expression recognition in the wild},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking the role of panchromatic images in
pan-sharpening. <em>TMM</em>, 1–14. (<a
href="https://doi.org/10.1109/TMM.2025.3535309">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent pan-sharpening methods have predominantly utilized techniques tailored for natural image scenes, often overlooking the unique features arising from non-overlapping spectral responses. In light of this, we have reevaluated the utility of panchromatic (PAN) images and introduced a theory anchored in the spectral response of satellite sensors. This posits that a PAN image is effectively a linear weighted summation of individual bands from its corresponding multi-spectral (MS) image, offset by an error map. We developed a deep unmixing network termed “DUN” that integrates an unmixing network, a fusion mechanism, and a distinctive mutual information contrastive loss function. Notably, the unmixing network is adept at decomposing a PAN image into its MS counterpart and error map. Further, the demixed image alongside the low-resolution MS image is channeled into the fusion network for pan-sharpening. Recognizing the challenges of achieving robust supervised learning directly from the unmixing phase, we have innovated a mutual information contrastive learning loss function, ensuring enhanced separation and minimizing overlap during the unmixing process. Preliminary experiments underscore both the quantitative and qualitative prowess of the proposed method.},
  archive      = {J_TMM},
  author       = {Jiaming Wang and Xitong Chen and Xiao Huang and Ruiqian Zhang and Yu Wang and Tao Lu},
  doi          = {10.1109/TMM.2025.3535309},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Rethinking the role of panchromatic images in pan-sharpening},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Video compressed sensing via wavelet residual sampling and
dual-domain fusion. <em>TMM</em>, 1–16. (<a
href="https://doi.org/10.1109/TMM.2025.3535326">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning-based compressed sensing (CS) technology attracts widespread attention owing to its remarkable reconstruction with only a few sampling measurements and low computational complexity. However, the existing video compressive sampling approaches cannot fully exploit the inherent interframe and intraframe correlations and sparsity of video sequences. To address this limitation, a novel sampling and reconstruction method for video CS (called WRDD) is proposed, which exploits the advantages of wavelet residual sampling and dual-domain fusion optimization.Specifically, in order to capture high-frequency details and achieve efficient and high-quality measurements, we propose a wavelet residual (WR) sampling strategy for the nonkeyframe sampling, which is achieved by the wavelet residuals between nonkeyframes and keyframes. Furthermore, a dual-domain (DD) fusion strategy is proposed, which fully combine intraframe and interframe to improve the reconstruction quality of nonkeyframes both in the pixel domain and multilevel feature domains. Extensive experiments demonstrate that our WRDD surpasses the state-of-the-art video and image CS methods in both subjective and objective evaluations. Besides, it exhibits outstanding antinoise capability and computational efficiency.},
  archive      = {J_TMM},
  author       = {Zhu Yin and Zhongcheng Wu and Wuzhen Shi and Guyue Hu and Weisi Lin},
  doi          = {10.1109/TMM.2025.3535326},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Video compressed sensing via wavelet residual sampling and dual-domain fusion},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DPPNet: A depth pixel-wise potential-aware network for RGB-d
salient object detection. <em>TMM</em>, 1–13. (<a
href="https://doi.org/10.1109/TMM.2025.3535386">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Depth cues are essential for visual perception tasks like Salient Object Detection (SOD). Due to varying depth reliability across scenes, some researchers propose evaluating the overall quality of the depth maps and discarding the less reliable ones to avoid contamination. However, these methods often fail to fully utilize valuable information in depth maps, leading to sub-optimal performance particularly when depth quality is unreliable. Since low-quality depth maps still contain useful information that potentially improves model performance, we propose a Depth Pixel-wise Potential-aware Network to leverage these depth cues effectively. This network includes two novel components designed: 1) A learning strategy for explicitly modeling the confidence of each depth pixel to assist the model in locating valid information in the depth map. 2) A cross-modal adaptive multiple fusion module that fuses features from both RGB and depth modalities. It aims to mitigate the contamination effect of unreliable depth maps and fully exploit the benefits of multiple fusion strategies. Experimental results show that on four publicly available datasets, our method outperforms 17 mainstream methods on various evaluation metrics. The code is available at: https://github.com/danielfaster/DPPNet.},
  archive      = {J_TMM},
  author       = {Junbin Yuan and Yiqi Wang and Zhoutao Wang and Qingzhen Xu and Bharadwaj Veeravalli and Xulei Yang},
  doi          = {10.1109/TMM.2025.3535386},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DPPNet: A depth pixel-wise potential-aware network for RGB-D salient object detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Weakly supervised LiDAR semantic segmentation via scatter
image annotation. <em>TMM</em>, 1–16. (<a
href="https://doi.org/10.1109/TMM.2025.3535350">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Weakly supervised LiDAR semantic segmentation has made significant strides with limited labeled data. However, most existing methods focus on the network training under weak supervision, while efficient annotation strategies remain largely unexplored. To tackle this gap, we implement LiDAR semantic segmentation using scatter image annotation, effectively integrating an efficient annotation strategy with network training. Specifically, we propose employing scatter images to annotate LiDAR point clouds, combining a pre-trained optical flow estimation network with a foundational image segmentation model to rapidly propagate manual annotations into dense labels for both images and point clouds. Moreover, we propose ScatterNet, a network that includes three pivotal strategies to reduce the performance gap caused by such annotations. First, it utilizes dense semantic labels as supervision for the image branch, alleviating the modality imbalance between point clouds and images. Second, an intermediate fusion branch is proposed to obtain multimodal texture and structural features. Finally, a perception consistency loss is introduced to determine which information needs to be fused and which needs to be discarded during the fusion process. Extensive experiments on the nuScenes and SemanticKITTI datasets demonstrate that our method requires less than 0.02% of the labeled points to achieve over 95% of the performance of fully-supervised methods. Notably, our labeled points are only 5% of those used in the most advanced weakly supervised methods.},
  archive      = {J_TMM},
  author       = {Yilong Chen and Zongyi Xu and Xiaoshui Huang and Shanshan Zhao and Xinqi Jiang and Xinyu Gao and Xinbo Gao},
  doi          = {10.1109/TMM.2025.3535350},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Weakly supervised LiDAR semantic segmentation via scatter image annotation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AdaMesh: Personalized facial expressions and head poses for
adaptive speech-driven 3D facial animation. <em>TMM</em>, 1–12. (<a
href="https://doi.org/10.1109/TMM.2025.3535287">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Speech-driven 3D facial animation aims at generating facial movements that are synchronized with the driving speech, which has been widely explored recently. Existing works mostly neglect the person-specific talking style in generation, including facial expression and head pose styles. Several works intend to capture the personalities by fine-tuning modules. However, limited training data leads to the lack of vividness. In this work, we propose AdaMesh, a novel adaptive speech-driven facial animation approach, which learns the personalized talking style from a reference video of about 10 seconds and generates vivid facial expressions and head poses. Specifically, we propose mixture-of-low-rank adaptation (MoLoRA) to fine-tune the expression adapter, which efficiently captures the facial expression style. For the personalized pose style, we propose a pose adapter by building a discrete pose prior and retrieving the appropriate style embedding with a semantic-aware pose style matrix without fine-tuning. Extensive experimental results show that our approach outperforms state-of-the-art methods, preserves the talking style in the reference video, and generates vivid facial animation. The supplementary video and code will be available on the project page https://github.com/thuhcsi/AdaMesh.},
  archive      = {J_TMM},
  author       = {Liyang Chen and Weihong Bao and Shun Lei and Boshi Tang and Zhiyong Wu and Shiyin Kang and Haozhi Huang and Helen Meng},
  doi          = {10.1109/TMM.2025.3535287},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {AdaMesh: Personalized facial expressions and head poses for adaptive speech-driven 3D facial animation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Replay-based incremental object detection with local
response exploration. <em>TMM</em>, 1–13. (<a
href="https://doi.org/10.1109/TMM.2025.3535403">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Incremental object detection (IOD) aims to train an object detector on non-stationary data streams without forgetting previous knowledge. Prevalent replay-based methods keep a buffer composed of carefully selected instances towards this goal. However, due to the limited storage space and uniform feature distribution, existing methods are prone to overfit on replayed instances, leading to poor generalization on diverse test data. Additionally, the imbalance in data quantity makes the detector fail to distinguish old and new classes that are visually similar, introducing bias toward new classes. To enhance the diversity of stored instances and eliminate bias, we propose a Local Response Exploration (LRE) framework, which comprises three modules. First, Region-Entropy Instance Selector (REIS) introduces a novel metric to assess instance diversity based on the entropy of local responses. Second, Confusion-Guided Instance Replay (CGIR) replaces the previous random replay approach by replaying specific old class instances based on class similarity, ensuring that parameters for similar new and old classes are updated together, thereby mitigating bias and helping mining discriminative patterns. Third, Confusion-Aware Region Segregation (CARS) adaptively differentiates biased regions from other regions based on local responses, reducing bias toward new classes while preserving relationships between new and old classes. Extensive evaluations on Pascal-VOC and MS COCO datasets demonstrate that our approach outperforms state-of-the-art methods in incremental object detection.},
  archive      = {J_TMM},
  author       = {Jian Zhong and Yifan Jiao and Bing-Kun Bao},
  doi          = {10.1109/TMM.2025.3535403},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Replay-based incremental object detection with local response exploration},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Debiased mapping for full-reference image quality
assessment. <em>TMM</em>, 1–12. (<a
href="https://doi.org/10.1109/TMM.2025.3535280">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An ideal full-reference image quality (FR-IQA) model should exhibit both high separability for images with different quality and compactness for images with the same or indistinguishable quality. However, existing learning-based FR-IQA models that directly compare images in deep-feature space, usually overly emphasize the quality separability, neglecting to maintain the compactness when images are of similar quality. In our work, we identify that the perception bias mainly stems from an inappropriate subspace where images are projected and compared. For this issue, we propose a Debiased Mapping based quality Measure (DMM), leveraging orthonormal bases formed by singular value decomposition (SVD) in the deep features domain. The SVD effectively decomposes the quality variations into singular values and mapping bases, enabling quality inference with more reliable feature difference measures. Extensive experimental results reveal that our proposed measure could mitigate the perception bias effectively and demonstrates excellent quality prediction performance on various IQA datasets. Codes are available at https://github.com/Baoliang93/DMM.},
  archive      = {J_TMM},
  author       = {Baoliang Chen and Hanwei Zhu and Lingyu Zhu and Shanshe Wang and Jingshan Pan and Shiqi Wang},
  doi          = {10.1109/TMM.2025.3535280},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Debiased mapping for full-reference image quality assessment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SimVPv2: Towards simple yet powerful spatiotemporal
predictive learning. <em>TMM</em>, 1–15. (<a
href="https://doi.org/10.1109/TMM.2025.3543051">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed remarkable advances in spatiotemporal predictive learning, with methods incorporating auxiliary inputs, complex neural architectures, and sophisticated training strategies. While SimVP has introduced a simpler, CNN-based baseline for this task, it still relies on heavy Unet-like architectures for spatial and temporal modeling, which still suffers from high complexity and computational overhead. In this paper, we propose SimVPv2, a streamlined model that eliminates the need for Unet architectures and demonstrates that plain stacks of convolutional layers, enhanced with an efficient Gated Spatiotemporal Attention mechanism, can deliver state-of-the-art performance. SimVPv2 not only simplifies the model architecture but also improves both performance and computational efficiency. On the standard Moving MNIST benchmark, SimVPv2 achieves superior performance compared to SimVP, with fewer FLOPs, about half the training time, and 60% faster inference efficiency. Extensive experiments across eight diverse datasets, including real-world tasks such as traffic forecasting and climate prediction, further demonstrate that SimVPv2 offers a powerful yet straightforward solution, achieving robust generalization across various spatiotemporal learning scenarios. We believe the proposed SimVPv2 can serve as a solid baseline to benefit the spatiotemporal predictive learning community.},
  archive      = {J_TMM},
  author       = {Cheng Tan and Zhangyang Gao and Siyuan Li and Stan Z. Li},
  doi          = {10.1109/TMM.2025.3543051},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SimVPv2: Towards simple yet powerful spatiotemporal predictive learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing event-based video reconstruction with
bidirectional temporal information. <em>TMM</em>, 1–13. (<a
href="https://doi.org/10.1109/TMM.2025.3543010">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event-based video reconstruction has emerged as an appealing research direction to break through the limitations of traditional cameras to better record dynamic scenes. Most existing methods reconstruct each frame from its corresponding event subset in chronological order. Since the temporal information contained in the whole event sequence is not fully exploited, these methods suffer inferior reconstruction quality. In this paper, we propose to enhance event-based video reconstruction by leveraging the bidirectional temporal information in event sequences. The proposed model processes event sequences in a bidirectional fashion, allowing for exploiting bidirectional information in the whole sequence. Furthermore, a transformer-based temporal information fusion module is introduced to aggregate long-range information in both temporal and spatial dimensions. Additionally, we propose a new dataset for the event-based video reconstruction task which contains a variety of objects and movement patterns. Extensive experiments demonstrate that the proposed model outperforms existing state-of-the-art event-based video reconstruction methods both quantitatively and qualitatively.},
  archive      = {J_TMM},
  author       = {Pinghai Gao and Longguang Wang and Sheng Ao and Ye Zhang and Yulan Guo},
  doi          = {10.1109/TMM.2025.3543010},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Enhancing event-based video reconstruction with bidirectional temporal information},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RDVC: Efficient deep video compression with regulable rate
and complexity optimization. <em>TMM</em>, 1–12. (<a
href="https://doi.org/10.1109/TMM.2025.3543005">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep video coding has paved a way to break through the performance bottleneck of reigning hybrid video coding. However, unlike hybrid video codecs, existing deep video codecs cannot offer both flexible rates and regulable complexities within one single codec, which limits their applications. In this paper, we propose a Regulable Deep Video Codec (RDVC) to address the above issue. First, we propose an Adaptive Feature Compression (AFC) network that generates variable rates while ensuring Rate-Distortion (RD) performance. The network introduces a two-stage coarse-to-fine rate adjustment that can be controlled by a user-specified rate level. Second, we propose a Spatio-Temporal Feature Propagation (STFP) mechanism to provide high-quality reference information for AFC process. Third, we also utilize slimmable convolutional components in our framework to adjust decoding complexity constrained by user configuration. Experimental results demonstrate that RDVC can adjust the codec structure flexibly according to different user configurations while maintaining advanced performance. On average, it reduces the bit-per-pixel (bpp) by 9.35%$/$58.12% while maintaining the same PSNR/MS-SSIM as the reference software VTM-13.2. Sourcecode is available at: https://github.com/WXJ0001/RDVC.},
  archive      = {J_TMM},
  author       = {Xiaojie Wei and Jielian Lin and Jiawei Xu and Wei Gao and Tiesong Zhao},
  doi          = {10.1109/TMM.2025.3543005},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {RDVC: Efficient deep video compression with regulable rate and complexity optimization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Contrastive learning with multiple prototypes for
unsupervised domain adaptive semantic segmentation. <em>TMM</em>, 1–16.
(<a href="https://doi.org/10.1109/TMM.2025.3543115">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unsupervised domain adaptive semantic segmentation aims to transfer knowledge from the annotated source domain to the unlabeled target domain. Recently, self-training methods have gained substantial attention, which leverage high-confidence predictions in the target domain as pseudo labels for supervision. However, limited exploration of intra-class variations across domains, including significant visual differences within each category, has led to misalignment between feature distribution across domains. In this paper, we present a unified non-parametric distance-based online clustering method to efficiently maintain multiple centroid-based prototypes within each category subspace instead of one prototype for each category subspace, which enables prototypes to possess the capacity for richer feature representation. Then, considering the variance across different dimensions of a feature representation, we then extend the prototypes from centroid-based ones to distribution-based ones. Specifically, each subspace is modeled using a Gaussian mixture model which includes several anisotropic Gaussian distributions, aimed at prioritizing discriminative dimensions and obtaining a finer measurement of the pixel-to-prototype similarity. Meanwhile, a category-aware feature space is achieved through pixel-to-prototype contrastive learning to ensure the compactness of pixel features in the same subcategory and drive the separation between pixel features of different subcategories. What&#39;s more, multi-resolution features are utilized to promote diversity and robustness among intra-class prototypes. Experiments validate the competitiveness of our two prototype-based methods against existing state-of-the-art methods, with a mIoU of $76.8\%$ on GTA $\rightarrow$ Cityscapes, $68.4\%$ on Synthia $\rightarrow$ Cityscapes, $54.5\%$ on Cityscapes $\rightarrow$ DarkZurich and $56.4\%$ on Cityscapes $\rightarrow$ ACDC. Notably, our method is able to seamlessly integrate with existing UDA methods.},
  archive      = {J_TMM},
  author       = {Jun Yu and Guochen Xie and Quansheng Liu and Zhen Kan and Lei Wang and Tianyu Liu and Qiang Ling and Wei Xu and Fang Gao},
  doi          = {10.1109/TMM.2025.3543115},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Contrastive learning with multiple prototypes for unsupervised domain adaptive semantic segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhancing semantic awareness by sentimental constraint with
automatic outlier masking for multimodal sarcasm detection.
<em>TMM</em>, 1–11. (<a
href="https://doi.org/10.1109/TMM.2025.3543074">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multimodal sarcasm detection, aiming to uncover sarcastic sentiment behind multimodal data, has gained substantial attention in multimodal communities. Recent advancements in multimodal sarcasm detection (MSD) methods have primarily focused on modality alignment with pre-trained vision-language (V-L) model. However, text-image pairs often exhibit weak or even opposite semantic correlations in MSD tasks. Consequently, directly aligning these modalities can potentially result in feature shift and inter-class confusion, ultimately hindering the model&#39;s ability. To alleviate this issue, we propose the Enhancing Semantic Awareness Model (ESAM) for multimodal sarcasm detection. Specifically, we first devise a Modality-decoupled Framework (MDF) to separate the textual and visual features from the fused multimodal representation. This decoupling enables the parallel integration of the Sentimental Congruity Constraint (SCC) within both visual and textual latent spaces, thereby enhancing the semantic awareness of different modalities. Furthermore, given that certain outlier samples with ambiguous sentiments can mislead the training and weaken the performance of SCC, we further incorporate Automatic Outlier Masking. This mechanism automatically detects and masks the outliers, guiding the model to focus on more informative samples during training. Experimental results on two public MSD datasets validate the robustness and superiority of our proposed ESAM model.},
  archive      = {J_TMM},
  author       = {Shaozu Yuan and Yiwei Wei and Hengyang Zhou and Qinfu Xu and Meng Chen and Xiaodong He},
  doi          = {10.1109/TMM.2025.3543074},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Enhancing semantic awareness by sentimental constraint with automatic outlier masking for multimodal sarcasm detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hypergraph consistency learning with relational
distillation. <em>TMM</em>, 1–12. (<a
href="https://doi.org/10.1109/TMM.2025.3543068">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper studies the problem of semi-supervised learning on graphs, which has recently aroused widespread interest in relational data mining. The focal point of exploration in this area has been the utilization of graph neural networks (GNNs), which stand out for excellent performance. Previous methods, however, typically rely on the limited labeled data while ignoring the abundant structural information in unlabeled nodes inherently on graphs, easily resulting in overfitting, especially in scenarios where only a few label nodes are available. Even worse, GNNs, despite their success, are constrained by their ability to solely capture local neighborhood information through message-passing mechanisms, thereby falling short in modeling higher-order dependencies among nodes. To circumvent the above drawbacks, we propose a simple yet effective framework called Hypergraph COnsistency LeArning (HOLA). Specifically, we employ a collaborative distillation framework consisting of a teacher network and a student network. To achieve effective interaction, we propose momentum distillation, a self-training method that enables the student network to learn from pseudo-targets generated by a momentum teacher network. Further, a novel hypergraph structure learning network is developed to model complex high-order relations among nodes with relational consistency learning, thereby transferring the knowledge to the student network. Extensive experiments conducted on a variety of benchmark datasets demonstrate the superior performance of the HOLA over various state-of-the-art methods.},
  archive      = {J_TMM},
  author       = {Siyu Yi and Zhengyang Mao and Yifan Wang and Yiyang Gu and Zhiping Xiao and Chong Chen and Xian-Sheng Hua and Ming Zhang and Wei Ju},
  doi          = {10.1109/TMM.2025.3543068},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Hypergraph consistency learning with relational distillation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An information compensation framework for zero-shot
skeleton-based action recognition. <em>TMM</em>, 1–13. (<a
href="https://doi.org/10.1109/TMM.2025.3543004">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot human skeleton-based action recognition aims to construct a model that can recognize actions outside the categories seen during training. Previous research has focused on aligning sequences&#39; visual and semantic spatial distributions. However, these methods extract semantic features simply. They ignore that proper prompt design for rich and fine-grained action cues can provide robust representation space clustering. In order to alleviate the problem of insufficient information available for skeleton sequences, we design an information compensation learning framework from an information-theoretic perspective to improve zero-shot action recognition accuracy with a multi-granularity semantic interaction mechanism. Inspired by ensemble learning, we propose a multi-level alignment (MLA) approach to compensate information for action classes. MLA aligns multi-granularity embeddings with visual embedding through a multi-head scoring mechanism to distinguish semantically similar action names and visually similar actions. Furthermore, we introduce a new loss function sampling method to obtain a tight and robust representation. Finally, these multi-granularity semantic embeddings are synthesized to form a proper decision surface for classification. Significant action recognition performance is achieved when evaluated on the challenging NTU RGB+D, NTU RGB+D 120, and PKU-MMD benchmarks and validate that multi-granularity semantic features facilitate the differentiation of action clusters with similar visual features.},
  archive      = {J_TMM},
  author       = {Haojun Xu and Yan Gao and Jie Li and Xinbo Gao},
  doi          = {10.1109/TMM.2025.3543004},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {An information compensation framework for zero-shot skeleton-based action recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Geo6D: Geometric-constraints-guided direct object 6D pose
estimation network. <em>TMM</em>, 1–14. (<a
href="https://doi.org/10.1109/TMM.2025.3543083">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Direct pose estimation networks aim to directly regress the 6D poses of target objects in the scene image using a neural network. These direct methods offer efficiency and an optimal optimization target, presenting significant potential for practical applications. However, due to the complex and implicit mappings between input features and target pose parameters, direct methods are challenging to train and prone to overfitting on mappings seen during training, resulting in limited effectiveness and generalization capability on unseen mappings. Existing methods focus primarily on improvements of the network architecture and training strategies, with less attention given to mappings. In this work, we propose a geometric constraints learning approach, which enables networks to explicitly capture and utilize the geometric mappings between inputs and optimization targets for pose estimation. Specifically, we introduce a residual pose transformation formula that preserves pose transformation constraints within both the 2D image plane and the 3D space while decoupling the absolute pose distribution, thereby addressing the pose distribution gap issue. We further design a Geo6D mechanism based on the formula, which enables the network to explicitly utilize geometric constraints for pose estimation by reconstructing the inputs and outputs. We select two different methods as our baseline and extensive experiments show that Geo6D enhances the performance and reduces the dependence on extensive training data, remaining effective even with only 10% of the typical data volume. Code is available at https://github.com/Jianqiuer/Geo6D.},
  archive      = {J_TMM},
  author       = {Jianqiu Chen and Mingshan Sun and Ye Zheng and Tianpeng Bao and Zhenyu He and Donghai Li and Guoqiang Jin and Zhao Rui and Liwei Wu and Xiaoke Jiang},
  doi          = {10.1109/TMM.2025.3543083},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Geo6D: Geometric-constraints-guided direct object 6D pose estimation network},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Self-supervised semantic soft label learning network for
deep multi-view clustering. <em>TMM</em>, 1–13. (<a
href="https://doi.org/10.1109/TMM.2025.3543075">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-view clustering, which identifies shared semantics from different perspectives and classifies data samples into distinct categories using unsupervised methods, is gaining increasing interest. This task primarily focuses on learning consistent multi-view feature representations and clustering labels. Current approaches for achieving consistent multi-view feature representations often use techniques such as cascading, weight fusion, and attention mechanism fusion. These methods reconstruct features based on original low-level features via encoder-decoder, which often contain visual private information, leading to misleading feature representations. Furthermore, in the clustering label learning process, many methods use a two-stage approach: first, they achieve consistent feature representations, and then they apply hard labeling methods like K-means or spectral clustering to obtain clustering labels. Single-stage methods typically derive consistent labels through a linear coding layer based on consistent representation learning. These methods do not fully utilize the multi-view view semantic information, and consistent representation learning may be impaired when some low-quality views are present, leading to the generation of inaccurate semantic labels. To address these issues, we propose a Self-supervised Semantic Soft Label Learning Network for Deep Multi-view Clustering. Specifically, we introduce a consensus high-level feature learning module that uses a shared MLP layer to transform low-level features into a high-level feature space. To enhance the consistency between high-level features from different views, we maximize mutual information between these features and introduce the U-Projection module, which improves the expressive power of the consensus feature via resampling the features and concatenating the fused features before and after sampling operations. Additionally, we propose a self-supervised semantic label learning module that employs a dual-branch approach to independently learn consistent view-specific semantic labels through contrastive learning, while deriving view-consensus semantic labels from shared high-level features extracted from multiple views. Finally, KL divergence is used to align the view-consensus labels with the view-specific labels. A series of extensive experiments have shown that our approach yields superior clustering results compared to existing techniques. Codes will be available at https://github.com/shayeyty/SSLNMVC.},
  archive      = {J_TMM},
  author       = {Weiqing Yan and Tingyu Yang and Chang Tang},
  doi          = {10.1109/TMM.2025.3543075},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Self-supervised semantic soft label learning network for deep multi-view clustering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AVS-mamba: Exploring temporal and multi-modal mamba for
audio-visual segmentation. <em>TMM</em>, 1–13. (<a
href="https://doi.org/10.1109/TMM.2025.3542995">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The essence of audio-visual segmentation (AVS) lies in locating and delineating sound-emitting objects within a video stream. While Transformer-based methods have shown promise, their handling of long-range dependencies struggles due to quadratic computational costs, presenting a bottleneck in complex scenarios. To overcome this limitation and facilitate complex multi-modal comprehension with linear complexity, we introduce AVS-Mamba, a selective state space model to address the AVS task. Our framework incorporates two key components for video understanding and cross-modal learning: Temporal Mamba Block for sequential video processing and Vision-to-Audio Fusion Block for advanced audio-vision integration. Building on this, we develop the Multi-scale Temporal Encoder, aimed at enhancing the learning of visual features across scales, facilitating the perception of intra- and inter-frame information. To perform multi-modal fusion, we propose the Modality Aggregation Decoder, leveraging the Vision-to-Audio Fusion Block to integrate visual features into audio features across both frame and temporal levels. Further, we adopt the Contextual Integration Pyramid to perform audio-to-vision spatial-temporal context collaboration. Through these innovative contributions, our approach achieves new state-of-the-art results on the AVSBench-object and AVSBench-semantic datasets. Our source code and model weights are available at AVS-Mamba.},
  archive      = {J_TMM},
  author       = {Sitong Gong and Yunzhi Zhuge and Lu Zhang and Yifan Wang and Pingping Zhang and Lijun Wang and Huchuan Lu},
  doi          = {10.1109/TMM.2025.3542995},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {AVS-mamba: Exploring temporal and multi-modal mamba for audio-visual segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). You only need clear images: Self-supervised single image
dehazing. <em>TMM</em>, 1–16. (<a
href="https://doi.org/10.1109/TMM.2025.3542999">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image hazing refers to adding haze to a clear image, which is important for improving the data amount and diversity of synthetic hazy images that are required to train deep image dehazing models. However, existing image hazing works generate hazy images from a given clear image with a single transmission map. This violates the fact that hazy images are diverse for a natural scene at different times. The domain shift issue between synthetic and real-world hazy images constrains the robustness of deep dehazing models when dealing with real-world hazy images. In this work, we propose an unsupervised haze generation work to synthesize multiple hazy images with diverse haze distributions from a clear image, which requires only an atmospheric scattering model without extra labeling information. Instead of estimating a transmission map from a clear image, we propose to customize the transmission maps by redefining the transmission function. In such a controllable way, hazy images with diverse haze distributions are generated, which avoids the labor-intensive collection of paired data and alleviates the common domain-shift issue of deep image dehazing. Incorporating the unsupervised hazy images generator, we also construct a generalizable self-supervised image dehazing (SSID) framework, where deep image dehazing models can be trained without any human annotations. Extensive experiments on real-world hazy images show that the proposed approach is superior to state-of-the-art unsupervised dehazing works, and achieves competitive performance with the supervised works. Moreover, the proposed SSID framework can be easily generalized to the existing deep dehazing models, greatly improving dehazing robustness on real-world hazy images. The source code is released at https://github.com/CVhnu/SSID.},
  archive      = {J_TMM},
  author       = {Jiyou Chen and Wenqi Ren and Huihuang Zhao and Qunbing Xia and Gaobo Yang},
  doi          = {10.1109/TMM.2025.3542999},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {You only need clear images: Self-supervised single image dehazing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mask-aware light field de-occlusion with gated feature
aggregation and texture-semantic attention. <em>TMM</em>, 1–16. (<a
href="https://doi.org/10.1109/TMM.2025.3543048">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {A light field image records rich information of a scene from multiple views, thereby providing complementary information for occlusion removal. However, current occlusion removal methods have several issues: (1) inefficient exploitation of spatial and angular complementary information among views; (2) indistinguishable treatment of pixels from foreground occlusion and background and (3) insufficient exploration of spatial detail supplementation. Therefore, in this paper, we propose a mask-aware de-occlusion network (MANet). Specifically, MANet is a joint training network that integrates the occlusion mask predictor (OMP) and the occlusion remover (OR). First, OMP is proposed to provide the location of occluded regions for OR, as the occlusion removal task is ill-posed without occluded region localization. In OR, we introduce gated spatial-angular feature aggregation, which uses a soft gating mechanism to focus on spatial-angular interaction features in non-occluded regions, extracting effective aggregated features specific to the de-occlusion. Then, we design a complementary strategy to fully utilize spatial-angular information among views. Finally, we propose texture-semantic attention to improve the performance of detail generation. Experimental results demonstrate the superiority of MANet, with substantial improvements in both PSNR and SSIM metrics. Moreover, MANet stands out with an efficient parameter count of 2.4M, making it a promising solution for real-world applications in public safety and security surveillance.},
  archive      = {J_TMM},
  author       = {Jieyu Chen and Ping An and Xinpeng Huang and Yilei Chen and Chao Yang and Liquan Shen},
  doi          = {10.1109/TMM.2025.3543048},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Mask-aware light field de-occlusion with gated feature aggregation and texture-semantic attention},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Facial expression recognition with heatmap neighbor
contrastive learning. <em>TMM</em>, 1–14. (<a
href="https://doi.org/10.1109/TMM.2025.3543029">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Many supervised learning-based facial expression recognition (FER) methods achieve good performance with the assistance of expression labels and a complex framework. However, there are inconsistent annotations in different expression datasets, making the above methods disadvantageous for new expression datasets or datasets with limited training data. The objective of this paper is to learn self-supervised facial expression features that enable the FER model not to rely on the annotation consistency of the different datasets. Most current self-supervised learning algorithms based on contrastive learning learn the representation by forcing different augmented views of the same image close in the embedding space, but they cannot cover all variances within a semantic class. We propose a heatmap neighbor contrastive learning (HNCL) method for FER. It treats the images corresponding to the heatmap nearest neighbors of expressions as other positives, providing more semantic variations than pre-defined augmented transformations. Therefore, our HNCL can learn better expression features covering more intraclass variances, improving the performance of the FER model based on self-supervised learning. After fine-tuning, HNCL with a simple framework achieves top-three performance on the inthe-lab datasets and even matches the performance of state-ofthe-art supervised learning methods on the in-the-wild datasets.},
  archive      = {J_TMM},
  author       = {Tong Liu and Jing Li and Jia Wu and Bo Du and Yibing Zhan and Dapeng Tao and Jun Wan},
  doi          = {10.1109/TMM.2025.3543029},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Facial expression recognition with heatmap neighbor contrastive learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cross-modal haptic compression inspired by embodied AI for
haptic communications. <em>TMM</em>, 1–12. (<a
href="https://doi.org/10.1109/TMM.2025.3542997">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Haptic data compression has gradually become a key issue for emerging real-time haptic communications in Tactile Internet (TI). However, it is challenging to achieve a tradeoff between high perceptual quality and compression ratio in haptic data compression scheme. Inspired by the perspective of embodied AI, we propose a cross-modal haptic compression scheme for haptic communications to improve the perception quality on TI devices in this paper. Since multimodal fusion is routinely employed to improve the ability of system in cognition, we assume that haptic codec is guided by visual semantics to optimize parameter settings in the coding process. We first design a multi-dimensional tactile feature fusion network (MTFFN) relying on multi-head attention mechanism. The MTFFN extracts the multi-dimensional features from the material surface and maps them to infer the coding parameters. Secondly, we provide second-order difference and linear interpolation to establish an criterion for the determination of optimal codec parameters, which are customized by the material categories so as to give high robustness. Finally, the simulation results reveal that our compression scheme can efficiently make a personalized codec procedure for different materials, obtaining more than 17% improvement in terms of compression ratio with high perceptual quality at the same time.},
  archive      = {J_TMM},
  author       = {Hang Lu and Xinmeng Tan and Mingkai Chen and Zhe Zhang and Xuguang Zhang and Jianxin Chen and Xin Wei and Tiesong Zhao},
  doi          = {10.1109/TMM.2025.3542997},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Cross-modal haptic compression inspired by embodied AI for haptic communications},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust watermarking based on multi-layer watermark feature
fusion. <em>TMM</em>, 1–14. (<a
href="https://doi.org/10.1109/TMM.2025.3543079">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The purpose of robust image watermarking is to embed a watermark into a carrier image in an invisible form and extract the watermark successfully even under noise interference conditions to achieve copyright confirmation and traceability. Although watermarking methods based on deep learning can improve the robustness by adding a noise simulation layer, few theoretical analyses of the codec structure have been conducted. Theoretical explainability is the theoretical basis for developing a network architecture, which plays a guiding role in network development. On the basis of the interpretability of convolutional networks, this paper analyzes the mathematical process of embedding and extracting watermarks in codecs and proposes a novel watermarking framework based on multi-layer watermark feature fusion. Specifically, the encoder can be a convolutional network structure of arbitrary depth, whereas the decoder needs only to adopt its corresponding deconvolution structure. To improve the quality and robustness of the generated watermarked image, the watermark is associated with an arbitrary layer feature space in the decoder. In the decoder, the network quickly converges to each original encoding feature space through the deconvolution structure, thus decoupling the watermark features. Finally, the watermark is extracted via the automatic fusion of multi-layer watermark features. The experimental results show that the proposed method is suitable for few-shot learning, and its invisibility, robustness and generalization performance on multiple datasets are significantly better than those of other advanced methods.},
  archive      = {J_TMM},
  author       = {Shaowu Wu and Wei Lu and Xiangyang Luo},
  doi          = {10.1109/TMM.2025.3543079},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Robust watermarking based on multi-layer watermark feature fusion},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Threefold encoder interaction: Hierarchical multi-grained
semantic alignment for cross-modal food retrieval. <em>TMM</em>, 1–16.
(<a href="https://doi.org/10.1109/TMM.2025.3543067">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current cross-modal food retrieval approaches focus mainly on the global visual appearance of food without explicitly considering multi-grained information. Additionally, direct calculation of the global similarity of image-recipe pairs is not particularly effective in terms of latent alignment, which suffers from mismatch during the mutual image-recipe retrieval process. This paper proposes a threefold encoder interaction (TEI) cross-modal food retrieval framework to maintain the multi-granularity of food images and the multi-levels of textual recipes to address the aforementioned challenges. The TEI framework comprises an image encoder, a recipe encoder, and a multi-grained interaction encoder. We simultaneously propose a multi-grained relation-aware attention (MRA) embedded in the multi-grained interaction encoder to capture multi-grained food visual features. The multi-grained interaction similarity scores are calculated to better establish the multi-grained correlation between recipe and image entities based on the extracted hierarchical textual and multi-grained visual features. Finally, a hierarchical multi-grained semantic alignment loss is designed to supervise the whole process of cross-modal training using the multi-grained interaction similarity scores. Extensive qualitative and quantitative experiments on the Recipe1M dataset have demonstrated that the proposed TEI framework achieves multi-grained semantic alignment between image and text modalities and is superior to other state-of-the-art methods in cross-modal food retrieval tasks.},
  archive      = {J_TMM},
  author       = {Qi Wang and Dong Wang and Weidong Min and Di Gai and Qing Han and Cheng Zha and Yuling Zhong},
  doi          = {10.1109/TMM.2025.3543067},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Threefold encoder interaction: Hierarchical multi-grained semantic alignment for cross-modal food retrieval},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UW-adapter: Adapting monocular depth estimation model in
underwater scenes. <em>TMM</em>, 1–11. (<a
href="https://doi.org/10.1109/TMM.2025.3543089">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Estimating depth maps from monocular underwater images poses one of the most challenging problems in underwater applications. Due to the lack of large-scale paired underwater color-depth datasets for effective training, existing style transfer-based and self-supervision-based approaches can improve the performance of depth estimation to some extent, but they remain unsatisfactory. Leveraging the power of massive training datasets, foundation models designed for terrestrial monocular depth estimation have demonstrated superior performance across various scenes. These models provide rich prior knowledge of 3D perception, which can be valuable for underwater depth estimation. Upon this, we introduce tunable adapters (UW-Adapter) that tailor a pre-trained foundation model specifically for underwater depth estimation, customizing it to the unique characteristics of underwater imagery. Our approach involves freezing the parameters of the pre-trained model and updating only the adapters through self-supervision. To address the complex degradation of underwater images, we propose two adapters: the transmission adapter and the high-frequency adapter. These adapters incorporate depth clues and high-frequency information as prior knowledge, thereby enhancing the performance of pre-trained model in underwater depth estimation. Experimental results demonstrate that by integrating lightweight adapters into off-the-shelf depth estimation foundation models, our method achieves superior performance across multiple datasets.},
  archive      = {J_TMM},
  author       = {Xinchen Ye and Yue Chang and Rui Xu and Haojie Li},
  doi          = {10.1109/TMM.2025.3543089},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {UW-adapter: Adapting monocular depth estimation model in underwater scenes},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Constructing balanced training samples: A new perspective on
long-tailed classification. <em>TMM</em>, 1–14. (<a
href="https://doi.org/10.1109/TMM.2025.3543084">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The most significant characteristic of long-tailed classification is that severe sample imbalance causes the model to be biased towards the head category. While the long-tailed distribution of multimedia dataset remains a constant, we can enhance the acquisition of balanced training samples and corresponding features during the learning process. This paper innovatively designs a sample provider to construct balanced training samples to enhance the acquisition of comprehensive features, and proposes a Siamese-based parameter-sharing framework to handle data with long-tailed distributions. Specifically, one branch of the Siamese network is introduced to classify samples with conventional random cropping sampling, another branch integrates the advantages of constructed balanced samples and hybrid optimization to capture the balanced features to identify more precise category boundaries. This combination not only facilitates the learning of long-tailed distribution but also strengthens the model&#39;s extraction of balanced features through the incorporation of contrastive learning. Most significantly, extensive experiments on CIFAR10-LT, CIFAR100-LT, ImageNet-LT and iNaturalist 2018 datasets demonstrate our model not only achieves superior performance but also retains the benefits of end-to-end training. Specifically, our method achieves 60.7% accuracy on ImageNet-LT with an end-to-end ResNeXt-50 backbone. The code and pre-trained weights will be available at https://github.com/WilyZhao8/CBTS},
  archive      = {J_TMM},
  author       = {Wenyi Zhao and Wei Li and Yuhan Li and Lu Yang and Zhenhao Liang and Enwen Hu and Weidong Zhang and Huihua Yang},
  doi          = {10.1109/TMM.2025.3543084},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Constructing balanced training samples: A new perspective on long-tailed classification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CR-famba: A frequency-domain assisted mamba for thin cloud
removal in optical remote sensing imagery. <em>TMM</em>, 1–10. (<a
href="https://doi.org/10.1109/TMM.2025.3542976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Optical remote sensing images are inevitably affected by cloud cover. To remove clouds from optical remote sensing images, a series of deep learning-based thin cloud removal methods have been developed. However, these methods have not explored the long-range modeling ability of state space models in optical remote sensing image thin cloud removal. In this paper, we propose a frequency-domain assisted Mamba for thin cloud removal, which is called CR-Famba. In CR-Famba, to better extract global and local features of images, we design a frequency-domain assisted state space layer (FDA-SSL). The FDA-SSL consists of two core components: residual state space block (RSSB) and frequency domain detail enhancement block (FDDEB). The RSSB utilizes the visual state space module (VSSM) to extract long-range dependencies of images from a spatial perspective while adding convolutional layers to overcome local pixel forgetting. Due to the rich detailed information of remote sensing images, we present FDDEB equipped with discrete wavelet transform (DWT) to supplement the extracted local information from the frequency domain perspective. We conduct experiments on different types of cloud-containing datasets, and the results show that our method can recover images with clearer texture details compared to other methods. https://github.com/Lab-PANbin/},
  archive      = {J_TMM},
  author       = {Jiao Liu and Bin Pan and Zhenwei Shi},
  doi          = {10.1109/TMM.2025.3542976},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-10},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CR-famba: A frequency-domain assisted mamba for thin cloud removal in optical remote sensing imagery},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Lossless LiDAR point cloud reflectance compression with a
deep hierarchical KNN context model. <em>TMM</em>, 1–13. (<a
href="https://doi.org/10.1109/TMM.2025.3542987">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recently, numerous learning-based point cloud compression methods with outstanding performance have been developed. The majority of them concentrate on point cloud geometry compression, and several works have demonstrated advances in the color attribute compression for dense point clouds. However, compression of the reflectance attribute attached to the point captured by the light detection and ranging (LiDAR) sensors remains a major challenge. In this paper, we present a lossless reflectance compression method for LiDAR point clouds (LPCs) that learns reflectance probability distributions with a deep hierarchical k-nearest-neighbors (KNN) context model, namely, the HK-PCRC. We first represent the original LPC with a series of hierarchical layers. Relying on the hierarchical structure, points in the same layer are coded in parallel by referencing the points in the previously coded layers. The approach balances the coding efficiency and time complexity while also supporting the progressive coding functionality. By introducing the KNN context, the context size is significantly reduced, which eases the computational burden while maintaining the coding performance. To enrich the context information, we further search for enhanced neighbors for each point in the context window. For each enhanced neighbor, in addition to its reflectance value, the relative distance, elevation angle, and local density are further collected. Then, a transformer-style sequential model is applied to construct an accurate deep context model. Furthermore, to efficiently fuse context features from different sources, a cross-feature fusion attention mechanism is designed for the transformer network. The comprehensive experimental results on SemanticKITTI, a large scale LiDAR benchmark, and Ford, an MPEG-specified dataset, demonstrate that our proposed framework achieves a state-of-the-art reflectance lossless compression performance, with average bit savings of 11.3% and 9.6% when compared to the state-of-the-art hand-crafted methods.},
  archive      = {J_TMM},
  author       = {Lizhi Hou and Tingyu Fan and Yiling Xu and Zhu Li},
  doi          = {10.1109/TMM.2025.3542987},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Lossless LiDAR point cloud reflectance compression with a deep hierarchical KNN context model},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust visual food recognition for enriching nutrition
knowledge bases. <em>TMM</em>, 1–12. (<a
href="https://doi.org/10.1109/TMM.2025.3542962">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Acquiring nutrition information and health-related knowledge about food is a common need among individuals. However, using conventional food names as search queries often fails to yield accurate matches to entries within food nutrition knowledge bases (FoodnKB), which frequently utilize scientific or product names. In this study, we present a method for enriching FoodnKB entries with imagery and facilitating visual access to food-related knowledge through image recognition. We start with an official food nutrition database and propose a consensus-based approach using Large Language Models to identify visually discernible and directly edible foods, expanding food synonyms and harnessing diverse web-based food images for comprehensive visual representation. To minimize manual annotation of noisy web images, we introduce a cyclic training-based area under the margin metric (cAUM) approach that effectively distinguishes appropriate images, including rare instances, from noisy ones. Additionally, we design a generic accuracy gap (AccGap) algorithm to automatically estimate the noise ratio of the web-harnessed data. Our integrated cAUM and AccGap method demonstrates superior performance in noise detection and enhancement of image recognition accuracy compared to existing noise-robust frameworks. Furthermore, we successfully apply the visually enriched FoodnKB and food recognition capabilities within a smart nutritionist mobile application.},
  archive      = {J_TMM},
  author       = {Zhaoyan Ming and Zeyu Xie and Chao Zhang and Kui Su and Changzheng Yuan and Tat-Seng Chua},
  doi          = {10.1109/TMM.2025.3542962},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Robust visual food recognition for enriching nutrition knowledge bases},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive medical topic learning for enhanced fine-grained
cross-modal alignment in medical report generation. <em>TMM</em>, 1–12.
(<a href="https://doi.org/10.1109/TMM.2025.3543101">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Medical report generation refers to the automatic creation of accurate and coherent diagnostic reports for medical images. This task can alleviate the workload of radiologists, enhance the efficiency of disease diagnosis, and therefore holds significant value and challenges. Considering the feature differences between different modalities, existing methods primarily focus on facilitating medical report generation through cross-modal alignment of images and texts. However, since medical images are very similar to each other, it is difficult to tag obvious objects, making most methods limited to coarse-grained image-text global alignment. In this paper, we propose a medical report generation model based on adaptive topic learning and fine-grained cross-modal alignment, which aligns images and texts from medical topic perspective and token perspective. From the medical topic perspective, a global-local contrastive loss is introduced to adaptively learn efficient medical topic features, and medical topics are utilized to map images and texts to the same semantic space for fine-grained alignment. From the token perspective, a token prediction module is designed to enable the model to focus on important local information by predicting the key tokens contained in the report. Experimental results on the two public datasets (i.e. IU-Xray and MIMIC-CXR) demonstrate that our proposed model outperforms state-of-the-art baselines.},
  archive      = {J_TMM},
  author       = {Xin Mei and Libin Yang and Denghong Gao and Xiaoyan Cai and Junwei Han and Tianming Liu},
  doi          = {10.1109/TMM.2025.3543101},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adaptive medical topic learning for enhanced fine-grained cross-modal alignment in medical report generation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning the scale in reference picture resampling for
versatile video coding. <em>TMM</em>, 1–12. (<a
href="https://doi.org/10.1109/TMM.2025.3543098">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Compressing high-resolution videos under low bitrate constraints is a challenging task. Resampling-based compression, which reduces the resolution before encoding and restores it after decoding, has great potential to improve the rate-distortion performance in such scenarios. In this paper, we propose a learning-based frame-level coding scale control scheme that enhances the coding performance by adjusting the coding scale for each frame. The scheme cooperates with the Reference Picture Resampling of the latest video coding standard Versatile Video Coding (VVC), which allows coding scale variations on each frame. More specifically, a dataset with 5200 videos is created by a greedy rate-distortion optimization algorithm employed to select the optimal coding scale for each frame. A neural network-based decision model is further incorporated into VVC, learning to predict the coding scale for each frame in one pass. The scheme is implemented into the Fraunhofer Versatile Video Encoder (VVenC), a fast and efficient VVC encoder, and evaluated on 4K contents. Experimental results show that the proposed scheme outperforms GOP-based coding scale adaptation methods, achieving average bitrate savings of 3.06% and 4.14% in terms of PSNR and MS-SSIM.},
  archive      = {J_TMM},
  author       = {Riyu Lu and Yingwen Zhang and Hengyu Man and Meng Wang and Shiqi Wang and Xiaopeng Fan},
  doi          = {10.1109/TMM.2025.3543098},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning the scale in reference picture resampling for versatile video coding},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Preemptive defense algorithm based on generalizable
black-box feedback regulation strategy against face-swapping deepfake
models. <em>TMM</em>, 1–16. (<a
href="https://doi.org/10.1109/TMM.2025.3543059">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the previous efforts to counteract Deepfake, detection methods were most adopted, but they could only function after-effect and could not undo the harm. Preemptive defense has recently gained attention as an alternative, but such defense works have either limited their scenario to facial-reenactment Deepfake models or only targeted specific face-swapping Deepfake model. Motivated to fill this gap, we start by establishing the Deepfake scenario modeling and finding the scenario difference among categories, then move on to the face-swapping scenario setting overlooked by previous works. Based on this scenario, we first propose a novel Black-Box Penetrating Defense Process that enables defense against face-swapping models without prior model knowledge. Then we propose a novel Double-Blind Feedback Regulation Strategy to solve the reality problem of avoiding alarming distortions after defense that had previously been ignored, which helps conduct valid preemptive defense against face-swapping Deepfake models in reality. Experimental results in comparison with state-of-the-art defense methods are conducted against popular face-swapping Deepfake models, proving our proposed method valid under practical circumstances.},
  archive      = {J_TMM},
  author       = {Zhongjie Mi and Xinghao Jiang and Tanfeng Sun and Ke Xu and Qiang Xu},
  doi          = {10.1109/TMM.2025.3543059},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Preemptive defense algorithm based on generalizable black-box feedback regulation strategy against face-swapping deepfake models},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ContextualCoder: Adaptive in-context prompting for
programmatic visual question answering. <em>TMM</em>, 1–14. (<a
href="https://doi.org/10.1109/TMM.2025.3543043">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Question Answering (VQA) presents a challenging task at the intersection of computer vision and natural language processing, aiming to bridge the semantic gap between visual perception and linguistic comprehension. Traditional VQA approaches do not distinguish between data processing and reasoning, limiting their interpretability and generalizability in complex and diverse scenarios. Conversely, Programmatic Visual Question Answering (PVQA) models leverage large language models (LLMs) to generate executable codes, providing answers with detailed and interpretable reasoning processes. However, existing PVQA models typically rely on simplistic input-output prompting, which struggles to elicit domain-specific knowledge from LLMs and often produces unclear or extraneous outputs. Furthermore, PVQA models typically rely on a basic in-context example (ICE) selection methodology that is heavily influenced by individual word similarity rather than the overall sentence context. This leads to suboptimal ICE selection and a reliance on dataset-specific ICE candidates. In this paper, we propose ContextualCoder, a novel prompting framework tailored for PVQA models. ContextualCoder leverages frozen LLMs for code generation and pre-trained visual models for code execution, eliminating the need for extensive training and enhancing model flexibility. By incorporating an innovative prompting methodology and a novel ICE selection strategy, ContextualCoder facilitates the use of diverse in-context information for code generation, thereby improving the performance of PVQA models. Our approach surpasses state-of-the-art models, as evidenced by comprehensive experiments across diverse VQA datasets, including multilingual scenarios.},
  archive      = {J_TMM},
  author       = {Ruoyue Shen and Nakamasa Inoue and Dayan Guan and Rizhao Cai and Alex C. Kot and Koichi Shinoda},
  doi          = {10.1109/TMM.2025.3543043},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {ContextualCoder: Adaptive in-context prompting for programmatic visual question answering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Modeling interactions between autonomous agents in a
multi-agent self-awareness architecture. <em>TMM</em>, 1–16. (<a
href="https://doi.org/10.1109/TMM.2025.3543110">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning from experience is a fundamental capability of intelligent agents. Autonomous systems rely on sensors that provide data about the environment and internal situations to their perception systems for learning and inference mechanisms. These systems can also learn Self-Aware and SituationAware generative modules from these data to localize themselves and interact with the environment. In this paper, we propose a self-aware cognitive architecture capable to perform tasks where the interactions between the self-state of an agent and the surrounding environment are explicitly and dynamically represented. We specifically develop a Deep Learning (DL) based Self-Aware interaction model, empowered by learning from Multi-Modal Perception (MMP) and World Models using multisensory data in a novel Multi-Agent Self-Awareness Architecture (MASAA). Two sub-modules are developed, the Situation Model (SM) and the First-Person model (FPM), that address different and interrelated aspects of the World Model (WM). The MMP model, instead, aims at learning the mapping of different sensory perceptions into Exteroceptive (EI) and Proprioceptive (PI) latent information. The WM then uses the learned MMP model as experience to predict dynamic self-behaviors and interaction patterns within the experienced environment. WM and MMP Models are learned in a data-driven way, starting from the lowerdimensional odometry data used to guide the learning of higherdimensional video data, thus generating coupled Generalized State Hierarchical Dynamic Bayesian Networks (GS-HDBNs). We test our model on KITTI, CARLA, and iCab datasets, achieving high performance and a low average localization error (RMSE) of 2.897%, when considering two interacting agents.},
  archive      = {J_TMM},
  author       = {Abrham Shiferaw Alemaw and Giulia Slavic and Pamela Zontone and Lucio Marcenaro and David Martin Gomez and Carlo Regazzoni},
  doi          = {10.1109/TMM.2025.3543110},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Modeling interactions between autonomous agents in a multi-agent self-awareness architecture},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SGG-nets: Generic rotation-invariant plugin networks for
point cloud analysis. <em>TMM</em>, 1–15. (<a
href="https://doi.org/10.1109/TMM.2025.3543001">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Rotation invariance is a crucial requirement for the analysis of 3D point clouds. However, current methods often achieve rotation invariance by employing specific network designs. These networks, though perform well on rotation-aware tasks, is inferior in general tasks such as classification and segmentation. On the other hand, many powerful point processing networks, such as PointNet++, DGCNN, etc., have general point processing abilities, but do not own the property of rotation invariance. In this paper, we propose a standalone rotation-invariant convolution operator called SGGConv (Spherical Geometric Graph-based Convolution) and two ways integrating it with common point-based networks. The networks equipped with SGGConvs are called SGG-Nets which promote the rotation-invariance ability of regular point networks without modifying their network architectures much. Our contributions are three-fold. First, we propose a rotation-invariant feature descriptor, namely Spherical Geometry Descriptor (SGD), which captures point-pair features in a Local Spherical Coordinate System (LSCS). Second, we propose the SGGConv based on SGD and LSCS with an efficient Graph-based Spherical Feature Passing (GSFP) mechanism. Thirdly, we define two modules S-SGGConvMdl and M-SGGConvMdl, which are used to integrate SGGConv into baseline point nets. We test SGG-Nets, such as SGG-PointNet++, SGG-DGCNN, SGG-RIConv++, on representative point cloud datasets. These models, equipped with our SGGConvs, not only enhance the rotation-invariance of the baseline network but also improve its performance on point cloud analysis tasks such as classification and part segmentation, without incurring too much computational overhead.},
  archive      = {J_TMM},
  author       = {Jian Zhu and Jianrong Yan and Jiebin Huang and Yongwei Nie and Bin Sheng and Tong-Yee Lee},
  doi          = {10.1109/TMM.2025.3543001},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SGG-nets: Generic rotation-invariant plugin networks for point cloud analysis},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Aggregate and discriminate: Pseudo clips-guided boundary
perception for video moment retrieval. <em>TMM</em>, 1–12. (<a
href="https://doi.org/10.1109/TMM.2025.3542894">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video moment retrieval (VMR) aims to localize a video segment in an untrimmed video that is semantically relevant to a language query. The challenge of this task lies in effectively aligning the intricate and information-dense video modality with the succinctly summarized textual modality, and further localizing the starting and ending timestamps of the target moments. Previous works have attempted to achieve multi-granularity alignment of video and query in a coarse-to-fine manner, yet these efforts still fall short in addressing the inherent disparities in representation and information density between videos and queries, leading to modal misalignments. In this paper, we propose a progressive video moment retrieval framework, initially retrieving the most relevant and irrelevant video clips to the query as semantic guidance, thereby bridging the semantic gap between video modality and language modality. Futhermore, we introduce a pseudo clips guided aggregation module to aggregate densely relevant moment clips closer together and propose a discriminative boundary-enhanced decoder with the guidance of pseudo clips to push the semantically confusing proposals away. Extensive experiments on the Charades-STA, ActivityNet Captions and TACoS datasets demonstrate that our method outperforms existing methods.},
  archive      = {J_TMM},
  author       = {Jin Liu and Zongbing Zhang and Yuting Su and Bing Yang and Xiongkuo Min and Guangtao Zhai},
  doi          = {10.1109/TMM.2025.3542894},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Aggregate and discriminate: Pseudo clips-guided boundary perception for video moment retrieval},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). All-in-focus imaging from events with occlusions.
<em>TMM</em>, 1–15. (<a
href="https://doi.org/10.1109/TMM.2025.3542978">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Event-based Synthetic Aperture Imaging (E-SAI) extends the SAI technique to observe targets behind extremely dense occlusions. Existing approaches remain confined to the de-occlusion of a specific depth plane, i.e., single depth in focus, unable to be applied to observe occluded targets with varying depths due to the decreased focus range. To achieve All-in-Focus E-SAI, i.e., recovering the occlusion-free image of all depth planes, the depth information behind the occlusions should be given to ensure accurate event refocusing. In this paper, we first prove the feasibility of predicting the depth map from captured events in the presence of dense occlusions. Then, we propose the ESAI-AF network, which consists of a Depth Estimation Module (DEM) designed to estimate the depth information from multi-view events and an Image Enhancement Module (IEM) designed to reconstruct high-quality occlusion-free images from the refocused events. We employ only multi-view occlusion-free images as supervised signals for end-to-end training of the above modules. Extensive experiments have shown that the proposed method can effectively perform All-in-Focus image reconstruction of occluded multi-depth targets and achieves superior performance to existing methods.},
  archive      = {J_TMM},
  author       = {Yichen Liu and Lixuan Wei and Yufei Guo and Lei Yu},
  doi          = {10.1109/TMM.2025.3542978},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {All-in-focus imaging from events with occlusions},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SVSRD: Spatial visual and statistical relation distillation
for class-incremental semantic segmentation. <em>TMM</em>, 1–13. (<a
href="https://doi.org/10.1109/TMM.2025.3543102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class-incremental semantic segmentation (CISS) aims to incrementally learn novel classes while retaining the ability to segment old classes, and suffers catastrophic forgetting since the old-class labels are unavailable. Most existing methods typically impose strict constraints on the consistency between the extracted features or output logits of each pixel from old and current models in an attempt to prevent forgetting through knowledge distillation (KD), which 1) results in a significant transfer of redundant knowledge while limiting the restoration of old classes (rigidity) due to potentially overlooking essential knowledge extraction, and 2) imposes strong constraints at the pixel level making it challenging for the model to learn novel classes (plasticity). To solve the above limitations, we propose a novel Spatial Visual and Statistical Relation Distillation (SVSRD) by applying multi-scale visual and statistical position relation distillation for CISS, which enjoys several merits. First, we introduce a region-based similarity matrix and impose a consistency constraint between current and old models, which preserves the essential visual knowledge to enhance the rigidity. Second, we propose a novel statistical feature calculation algorithm to investigate the distribution of the data and further preserve the rules of statistics through statistical consistency, which also promotes the model on the novel-class learning for improving the plasticity. Finally, the aforementioned constraints are jointly applied in multiple scales to alleviate old-class forgetting and enhance novel-class learning. Extensive experiments on Pascal-VOC 2012 and ADE20K demonstrate that the proposed approach performs favorably against the state-of-the-art CISS methods. Code is available at https://github.com/YyChang719/SVSRD-CISS.},
  archive      = {J_TMM},
  author       = {Yuyang Chang and Yifan Jiao and Bing-Kun Bao},
  doi          = {10.1109/TMM.2025.3543102},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SVSRD: Spatial visual and statistical relation distillation for class-incremental semantic segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Deformle cross-attention trnsformer for wekly aligned RGB–t
pedestrin detection. <em>TMM</em>, 1–12. (<a
href="https://doi.org/10.1109/TMM.2025.3543056">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Pedestrian detection plays a crucial role in autonomous driving systems. To ensure reliable and effective detection in challenging conditions, researchers have proposed RGB–T (RGB–thermal) detectors that integrate thermal images with color images for more complementary feature representations. However, existing methods face challenges in capturing the spatial and geometric correlations between different modalities, as well as in assuming perfect synchronization of the two modalities, which is unrealistic in real-world scenarios. In response to these challenges, we present a new deformableattention-based approach for weakly aligned RGB–T pedestrian detection. The proposed method uses a dual-branch crossattention mechanism to capture the inherent spatial and geometric correlations between color and thermal images. Furthermore, it incorporates positional information for each image pixel into the sampling offset generation to enhance robustness in scenarios where modalities are not precisely aligned or registered. To reduce computational complexity, we introduce a local attention mechanism that samples only a small set of keys and values within a limited region in the feature maps for each query. Extensive experiments and ablation studies conducted on multiple public datasets confirm the effectiveness of the proposed framework. Our code will be released at https://github.com/jiongger/DeformCAT.},
  archive      = {J_TMM},
  author       = {Yu Hu and Xiaobo Chen and Sheng Wang and Luyang Liu and Hengyang Shi and Lihong Fan and Jing Tian and Jun Liang},
  doi          = {10.1109/TMM.2025.3543056},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Deformle cross-attention trnsformer for wekly aligned RGB–T pedestrin detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic camera movement generation with enhanced immersion
for virtual cinematography. <em>TMM</em>, 1–14. (<a
href="https://doi.org/10.1109/TMM.2025.3542956">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {User-generated cinematic creations are gaining popularity as our daily entertainment, yet it is a challenge to master cinematography for producing immersive contents. Many existing automatic methods focus on roughly controlling predefined shot types or movement patterns, which struggle to engage viewers with the actor&#39;s circumstances. Real-world cinematographic rules show that directors can create immersion by comprehensively synchronizing the camera with the actor. Inspired by this strategy, we propose a deep camera control framework that enables actor-camera synchronization in three aspects, considering frame aesthetics, spatial action, and emotional status in the 3D virtual stage. Following rule-of-thirds, our framework first modifies the initial camera placement to position the actor aesthetically. This adjustment is facilitated by a weakly-supervised adjustor that analyzes frame composition via camera projection. We then design a GAN model that can adversarially synthesize fine-grained camera movement based on the actor&#39;s action and psychological state, using an encoder-decoder generator to map kinematics and emotional variables into camera trajectories. Moreover, we incorporate a regularizer to align the generated stylistic variances with specific emotional categories and intensities. The experimental results show that our proposed method yields immersive cinematic videos of high quality, both quantitatively and qualitatively. Live examples can be found in the supplementary video.},
  archive      = {J_TMM},
  author       = {Xinyi Wu and Haohong Wang and Aggelos K. Katsaggelos},
  doi          = {10.1109/TMM.2025.3542956},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Automatic camera movement generation with enhanced immersion for virtual cinematography},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Copy-move forgery image detection based on cross-scale
modeling and alternating refinement. <em>TMM</em>, 1–14. (<a
href="https://doi.org/10.1109/TMM.2025.3543057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The detection of image tampering, specifically copy detection, is an important problem in many domains such as military, media, and public opinion outlets. Effective means to detect such tampering is crucial in controlling the dissemination of false information. However, a major challenge in achieving high detection accuracy lies in the variability of the scale of the copied targets. To tackle this problem, we introduce an all-encompassing methodology called Cross-Scale Modeling and Alternating Refinement (CANet) to detect the genuine source and tampered region at the pixel level. CANet consists of three modules: the Cross-Scale Similar Region Detection (CS) module, the Edge-Supervised Tamper Region Detection (ET) module, and the Alternating Refinement (AR) module. The CS module extracts coarse similar region features by cross-scale correlation modeling, which can alleviate the scale gap between the source and tampered region. The obtained coarse similar region feature is refined by the AR module, in which we introduce the source and the tampered region as the auxiliary information and employ a two-stage process that sequentially models their global feature representations. The tampered region used in the AR module is obtained from the ET module using edge supervision with a salient edge selection scheme, and the source region is generated by the implicit modeling. We conducted experiments on the USC-ISI, CASIA v2.0, CoMoFoD, and MICC-F220 datasets separately. Results show that our method outperforms the state-of-the-art.},
  archive      = {J_TMM},
  author       = {Jingyu Wang and Jie Nie and Niantai Jing and Xinyue Liang and Xiaodong Wang and Chi-Hung Chi and Zhiqiang Wei},
  doi          = {10.1109/TMM.2025.3543057},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Copy-move forgery image detection based on cross-scale modeling and alternating refinement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Global spatial-temporal information-based residual ConvLSTM
for video space-time super-resolution. <em>TMM</em>, 1–13. (<a
href="https://doi.org/10.1109/TMM.2025.3542970">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {By converting low-frame-rate, low-resolution videos into high-frame-rate, high-resolution ones, space-time video super-resolution techniques can enhance visual experiences and facilitate more efficient information dissemination. We propose a convolutional neural network (CNN) for space-time video super-resolution, namely GIRNet. Our method combines long-term global information and short-term local information from the video to better extract complete and accurate spatial-temporal information. To generate highly accurate features and thus improve performance, the proposed network integrates a feature-level temporal interpolation module with deformable convolutions and a global spatial-temporal information-based residual convolutional long short-term memory (convLSTM) module. In the feature-level temporal interpolation module, we leverage deformable convolution, which adapts to deformations and scale variations of objects across different scene locations. This provides a more efficient solution than conventional convolution for extracting features from moving objects. Our network effectively uses forward and backward feature information to determine inter-frame offsets, leading to the direct generation of interpolated frame features. In the global spatial-temporal information-based residual convLSTM module, the first convLSTM is used to derive global spatial-temporal information from the input features, and the second convLSTM uses the previously computed global spatial-temporal information feature as its initial cell state. This second convLSTM adopts residual connections to preserve spatial information, thereby enhancing the output features. Experiments on the Vimeo90K dataset show that the proposed method outperforms open source state-of-the-art techniques in peak signal-to-noise-ratio (by 1.45 dB, 1.14 dB, and 0.2 dB over STARnet, TMNet, and 3DAttGAN, respectively), structural similarity index(by 0.027, 0.023, and 0.006 over STARnet, TMNet, and 3DAttGAN, respectively), and visual quality.},
  archive      = {J_TMM},
  author       = {Congrui Fu and Hui Yuan and Shiqi Jiang and Guanghui Zhang and Liquan Shen and Raouf Hamzaoui},
  doi          = {10.1109/TMM.2025.3542970},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Global spatial-temporal information-based residual ConvLSTM for video space-time super-resolution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive multi-scale language reinforcement for multimodal
named entity recognition. <em>TMM</em>, 1–12. (<a
href="https://doi.org/10.1109/TMM.2025.3543105">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Over the recent years, multimodal named entity recognition has gained increasing attentions due to its wide applications in social media. The key factor of multimodal named entity recognition is to effectively fuse information of different modalities. Existing works mainly focus on reinforcing textual representations by fusing image features via the cross-modal attention mechanism. However, these works are limited in reinforcing the text modality at the token level. As a named entity usually contains several tokens, modeling token-level inter-modal interactions is suboptimal for the multimodal named entity recognition problem. In this work, we propose a multimodal named entity recognition approach dubbed Adaptive Multi-scale Language Reinforcement (AMLR) to implement entity-level language reinforcement. To this end, our model first expands token-level textual representations into multi-scale textual representations which are composed of language units of different lengths. After that, the visual information reinforces the language modality by modeling the cross-modal attention between images and expanded multi-scale textual representations. Unlike existing token-level language reinforcement methods, the word sequences of named entities can be directly interacted with the visual features as a whole, making the modeled cross-modal correlations more reasonable. Although the underlying entity is not given, the training procedure can encourage the relevant image contents to adaptively attend to the appropriate language units, making our approach not rely on the pipeline design. Comprehensive evaluation results on two public Twitter datasets clearly demonstrate the superiority of our proposed model.},
  archive      = {J_TMM},
  author       = {Enping Li and Tianrui Li and Huaishao Luo and Jielei Chu and Lixin Duan and Fengmao Lv},
  doi          = {10.1109/TMM.2025.3543105},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Adaptive multi-scale language reinforcement for multimodal named entity recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GAN prior-enhanced novel view synthesis from monocular
degraded images. <em>TMM</em>, 1–11. (<a
href="https://doi.org/10.1109/TMM.2025.3542963">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the escalating demand for three-dimensional visual applications such as gaming, virtual reality, and autonomous driving, novel view synthesis has become a critical area of research. Current methods mainly depend on multiple views of the same subject to achieve satisfactory results, but there is often a significant lack of available data. Typically, only a single degraded image is available for reconstruction, which may be affected by occlusion, low resolution, or absence of color information. To overcome this limitation, we propose a two-stage feature matching approach designed specifically for single degraded images, leading to the synthesis of high-quality novel perspective images. This method involves the sequential use of an encoder for feature extraction followed by the fine-tuning of a generator for feature matching. Additionally, the integration of an information filtering module proposed by us during the GAN inversion process helps eliminate misleading information present in degraded images, thereby correcting the inversion direction. Extensive experimental results show that our method outperforms existing state-of-the-art single-view novel view synthesis techniques in handling challenges like occluded, grayscale, and low-resolution images. Moreover, the efficacy of our method remains unparalleled even when aforementioned method integrated with image restoration algorithms.},
  archive      = {J_TMM},
  author       = {Kehua Guo and Zheng Wu and Xianhong Wen and Shaojun Guo and Zhipeng Xi and Tianyu Chen},
  doi          = {10.1109/TMM.2025.3542963},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {GAN prior-enhanced novel view synthesis from monocular degraded images},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ArbiTrack: A novel multi-object tracking framework for a
moving UAV to detect and track arbitrarily oriented targets.
<em>TMM</em>, 1–11. (<a
href="https://doi.org/10.1109/TMM.2025.3543018">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The operation of traditional multi-object trackers on a moving unmanned aerial vehicle (UAV) faces many difficulties due to the irregular motion of UAV, the occlusion problem, and in particular arbitrarily oriented targets that are densely distributed with complex backgrounds. To solve these difficulties, this paper proposes a novel multi-object tracking framework, namely ArbiTrack, for a moving UAV to effectively detect and track arbitrarily oriented targets on the grounds. The proposed framework consists of an oriented object detection module to capture ground objects, a multi-scale context aggregation (MCA) module to improve the detection accuracy of small objects, and an adaptive motion switching (AMS) module to deal with the nonlinear complexity among UAV and ground objects. Historical information from multiple moments is used in this framework to learn the spatio-temporal characteristics so that the occlusion problem can be solved effectively. Experiments are conducted by using our OriDrone dataset and the public dataset UAVDT dataset. Results demonstrate that the proposed method achieves state-of-the-art tracking performance.},
  archive      = {J_TMM},
  author       = {Yuqing Chen and Jiayu Wang and Qianchen Zhou and Huosheng Hu},
  doi          = {10.1109/TMM.2025.3543018},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {ArbiTrack: A novel multi-object tracking framework for a moving UAV to detect and track arbitrarily oriented targets},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bi-directional deep contextual video compression.
<em>TMM</em>, 1–15. (<a
href="https://doi.org/10.1109/TMM.2025.3543061">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep video compression has made impressive process in recent years, with the majority of advancements concentrated on P-frame coding. Although efforts to enhance B-frame coding are ongoing, their compression performance is still far behind that of traditional bi-directional video codecs. In this paper, we introduce a bi-directional deep contextual video compression scheme tailored for B-frames, termed DCVC-B, to improve the compression performance of deep B-frame coding. Our scheme mainly has three key innovations. First, we develop a bi-directional motion difference context propagation method for effective motion difference coding, which significantly reduces the bit cost of bi-directional motions. Second, we propose a bi-directional contextual compression model and a corresponding bi-directional temporal entropy model, to make better use of the multi-scale temporal contexts. Third, we propose a hierarchical quality structure-based training strategy, leading to an effective bit allocation across large groups of pictures (GOP). Experimental results show that our DCVC-B achieves an average reduction of 26.6% in BD-Rate compared to the reference software for H.265/HEVC under random access conditions. Remarkably, it surpasses the performance of the H.266/VVC reference software on certain test datasets under the same configuration. We anticipate our work can provide valuable insights and bring up deep B-frame coding to the next level.},
  archive      = {J_TMM},
  author       = {Xihua Sheng and Li Li and Dong Liu and Shiqi Wang},
  doi          = {10.1109/TMM.2025.3543061},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Bi-directional deep contextual video compression},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Simultaneous detection and interaction reasoning for
object-centric action recognition. <em>TMM</em>, 1–13. (<a
href="https://doi.org/10.1109/TMM.2025.3543033">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The interactions between human and objects are important for recognizing object-centric actions. Existing methods usually adopt a two-stage pipeline, where object proposals are first detected using a pretrained detector, and then are fed to an action recognition model for extracting video features and learning the object relations for action recognition. However, since the action prior is unknown in the object detection stage, important objects could be easily overlooked, leading to inferior action recognition performance. In this paper, we propose an end-to-end object-centric action recognition framework that simultaneously performs Detection And Interaction Reasoning (dubbed DAIR) in one stage. Particularly, after extracting video features using a base network, we design three consecutive modules for simultaneously learning object detection and interaction reasoning. Firstly, we build a Patch-based Object Decoder (PatchDec) to generate object proposals from video patch tokens. Then, we design an Interactive Object Refining and Aggregation (IRA) to identify the interactive objects that are important for action recognition. The IRA module adjusts the interactiveness scores of proposals based on their relative position and appearance, and aggregates the object-level information into global video representation. Finally, we build an Object Relation Modeling (ORM) module to encode the object relations. These three modules together with the video feature extractor can be trained jointly in an end-to-end fashion, thus avoiding the heavy reliance on an off-the-shelf object detector, and reducing the multi-stage training burden. We conduct experiments on two datasets, Something-Else and Ikea-Assembly, to evaluate the performance of our proposed approach on conventional, compositional, and few-shot action recognition tasks. Through in-depth experimental analysis, we show the crucial role of interactive objects in learning for action recognition, and we can outperform state-of-the-art methods on both datasets. We hope our DAIR can provide a new perspective for object-centric action recognition. The code is available at https://github.com/lixunsong/DAIR.},
  archive      = {J_TMM},
  author       = {Xunsong Li and Pengzhan Sun and Yangcen Liu and Lixin Duan and Wen Li},
  doi          = {10.1109/TMM.2025.3543033},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Simultaneous detection and interaction reasoning for object-centric action recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A deep semantic segmentation network with semantic and
contextual refinements. <em>TMM</em>, 1–13. (<a
href="https://doi.org/10.1109/TMM.2025.3543037">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Semantic segmentation is a fundamental task in multimedia processing, which can be used for analyzing, understanding, editing contents of images and videos, among others. To accelerate the analysis of multimedia data, existing segmentation researches tend to extract semantic information by progressively reducing the spatial resolutions of feature maps. However, this approach introduces a misalignment problem when restoring the resolution of high-level feature maps. In this paper, we design a Semantic Refinement Module (SRM) to address this issue within the segmentation network. Specifically, SRM is designed to learn a transformation offset for each pixel in the upsampled feature maps, guided by high-resolution feature maps and neighboring offsets. By applying these offsets to the upsampled feature maps, SRM enhances the semantic representation of the segmentation network, particularly for pixels around object boundaries. Furthermore, a Contextual Refinement Module (CRM) is presented to capture global context information across both spatial and channel dimensions. To balance dimensions between channel and space, we aggregate the semantic maps from all four stages of the backbone to enrich channel context information. The efficacy of these proposed modules is validated on three widely used datasets-Cityscapes, Bdd100K, and ADE20K-demonstrating superior performance compared to state-of-the-art methods. Additionally, this paper extends these modules to a lightweight segmentation network, achieving an mIoU of 82.5% on the Cityscapes validation set with only 137.9 GFLOPs.},
  archive      = {J_TMM},
  author       = {Zhiyan Wang and Deyin Liu and Lin Yuanbo Wu and Song Wang and Xin Guo and Lin Qi},
  doi          = {10.1109/TMM.2025.3543037},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A deep semantic segmentation network with semantic and contextual refinements},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multiple adaptation network for multi-source and
multi-target domain adaptation. <em>TMM</em>, 1–15. (<a
href="https://doi.org/10.1109/TMM.2025.3543094">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-source domain adaptation (MSDA) has garnered significant attention due to its emphasis on transferring knowledge from multiple labeled source domains to a single unlabeled target domain. MSDA requires sufficient labeled data from multiple source domains, but in practice, massive unlabeled data exist instead of well-labeled data. Multiple target domains also provide plenty of information, which is useful for domain adaptation. However, most MSDA studies overlook the critical scenario of multi-source and multi-target domain adaptation (MMDA). To address these problems, we propose a Multiple Adaptation Network (MAN) approach for MMDA, which utilizes multiple alignment strategies for each source-target domain pair-group to align relevant specific feature spaces. MAN also aligns multiple classifiers for the relevant feature spaces to optimize the decision boundaries of multiple target domains. Moreover, to consider the task relations of multiple classifiers, we minimize the semantic differences between the target-conditioned classifiers and utilize a weight learning category to optimize this process. To fully utilize the information from multiple target domains, we transfer the style information of the target data to the source data, aiding in the training of multiple classifiers. Extensive experiments in challenge domain adaptation benchmarks, including the ImageCLEF-DA, Office-Home, DomainNet, and RGB-to-thermal datasets, demonstrate the superiority of our method over the state-of-the-art approaches.},
  archive      = {J_TMM},
  author       = {Yuwu Lu and Haoyu Huang and Xue Hu and Zhihui Lai and Xuelong Li},
  doi          = {10.1109/TMM.2025.3543094},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multiple adaptation network for multi-source and multi-target domain adaptation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CMoA: Contrastive mixture of adapters for generalized
few-shot continual learning. <em>TMM</em>, 1–14. (<a
href="https://doi.org/10.1109/TMM.2025.3543038">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The goal of Few-Shot Continual Learning (FSCL) is to incrementally learn novel tasks with limited labeled samples and preserve previous capabilities simultaneously. However, current FSCL works lack research on domain increment and domain generalization ability, which cannot cope with changes in the visual perception environment. In this paper, we set up a Generalized FSCL (GFSCL) protocol involving both class- and domain-incremental scenarios together with domain generalization assessment. Firstly, two benchmark datasets and protocols are newly arranged, and detailed baselines are provided for this unexplored configuration. Furthermore, we find that common continual learning methods have poor generalization ability on unseen domains and cannot better tackle catastrophic forgetting issue in cross-incremental tasks. Hence, we propose a rehearsal-free framework based on Vision Transformer (ViT) named Contrastive Mixture of Adapters (CMoA). It contains two non-conflicting parts: (1) By applying the fast-adaptation characteristic of adapter-embedded ViT, the mixture of Adapters (MoA) module is incorporated into ViT. For stability purpose, cosine similarity regularization and dynamic weighting are designed to make each adapter learn specific knowledge and concentrate on particular classes. (2) To further enhance domain generalization ability, we alleviate the intra-class variation by prototype-calibrated contrastive learning to improve domain-invariant representation learning. Finally, six evaluation indicators showing the overall performance and forgetting are compared by comprehensive experiments on two benchmark datasets to validate the efficacy of CMoA, and the results illustrate that CMoA can achieve comparative performance with rehearsal-based continual learning methods. The codes and protocols are available at https://github.com/yawencui/CMoA.},
  archive      = {J_TMM},
  author       = {Yawen Cui and Jian Zhao and Zitong Yu and Rizhao Cai and Xun Wang and Lei Jin and Alex C. Kot and Li Liu and Xuelong Li},
  doi          = {10.1109/TMM.2025.3543038},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CMoA: Contrastive mixture of adapters for generalized few-shot continual learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Medical transformer with mix mask generation for thorax
disease classification. <em>TMM</em>, 1–13. (<a
href="https://doi.org/10.1109/TMM.2025.3543003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Chest X-ray images have been highly involved in clinical diagnosis and treatment planning for thoracic disease. The process of medical images has attracted great attention in the machine learning community. However, the labeled medical images are limited and the regions of lesions are usually much smaller in the image. Most of the existing methods are prone to learning the spurious correlation for classification, resulting in poor generalization. In this paper, we propose a medical generation transformer network based on self-supervised learning and the adversarial strategy to capture the discriminative label-relevant regions with lesions in the images by extending the Chest X-ray images. In the proposed method, we first localize the label-relevant regions in each transformer layer. Then we keep the label-relevant regions to mask the image and construct the masked image with self-supervised learning. Thus we can generate more images to fine-tune the classification network with masked images that keep the label-relevant regions. Since the generated images are usually noisy to fine-tune the classification network, we adopt the adversarial probabilities to weight the importance of each generated image for training. Experimental results on two large-scale and popular chest X-ray datasets show that the proposed method can efficiently leverage the location of lesions to improve the performance of classification.},
  archive      = {J_TMM},
  author       = {Ziyi Liu and Zengmao Wang and Bo Du},
  doi          = {10.1109/TMM.2025.3543003},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Medical transformer with mix mask generation for thorax disease classification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking temporal context in video-QA: A comprehensive
study of single-frame static bias. <em>TMM</em>, 1–15. (<a
href="https://doi.org/10.1109/TMM.2025.3543031">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Video question answering (Video-QA) has emerged as a core task in the vision-language domain, which requires the models to understand a given video and answer textual questions related to the video. Compared to conventional image-language tasks, Video-QA is designed for improving the models&#39; capacity of memorizing and integrating multi-frame temporal cues associated with the questions. While significant performance improvements have recently been witnessed on public benchmarks, in this work, we rethink whether these improvements truly stem from better understanding of video temporal context as expected. To this end, we accomplish a strong single-frame baseline model trained with knowledge distillation. With this model, we surprisingly find that visiting only one single frame, without incorporating multi-frame and temporal information, is sufficient to achieve state-of-the-art (SOTA) performance on multiple mainstream benchmarks. This finding reveals the prevalence of single-frame bias in current benchmarks for the first time. Around the single-frame bias, we conduct an in-depth analysis on multiple popular benchmarks, which demonstrate that: (i) merely relying on one frame is able to achieve comparable performance with SOTA temporal Video-QA models; (ii) simply ensembling the prediction scores of only 3 separate frames is able to surpass temporal SOTAs. Furthermore, we observe that most of the benchmarks are biased towards central segments, and even the latest benchmarks tailored for temporal reasoning still suffer from severe single-frame bias. In case study, we find two key properties of low-bias instances: the question emphasizes temporal dependency and contextual understanding, and the associated video content presents significant variability in scenes, actions or interactions. Through further analysis on compositional reasoning datasets, we find that constructing explicit object/event interactions upon videos to fill in well-designed temporal question templates can effectively reduce the single-frame bias during annotation. We hope our analysis helps facilitate future efforts in the field towards mitigating static bias and highlighting temporal reasoning.},
  archive      = {J_TMM},
  author       = {Tianming Liang and Linhui Li and Jian-Fang Hu and Xiangyang Yu and Wei-Shi Zheng and Jianhuang Lai},
  doi          = {10.1109/TMM.2025.3543031},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Rethinking temporal context in video-QA: A comprehensive study of single-frame static bias},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rethinking depth guided reflection removal. <em>TMM</em>,
1–12. (<a href="https://doi.org/10.1109/TMM.2025.3543096">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {When photographing through glass, reflections are often observed, which negatively impact the quality of the captured images or videos. In this paper, we summarize and rethink depth guided reflection removal methods and, inspired by the human binocular vision system, investigate how to utilize depth for effective binocular video reflection removal. We propose an end-to-end learning-based reflection removal method that learns the transmission depth and designs a unified structure to achieve depth guided, cross-view, and cross-frame feature enhancement in a cascaded manner. Within the unified structure, different gating controllers are custom-designed to emphasize the direction of feature interaction. A dataset containing synthetic and real binocular mixture video dataset is built for network training and testing. Experimental results on both synthetic and real data from the proposed dataset demonstrate that the proposed method achieves superior performance in binocular video reflection removal.},
  archive      = {J_TMM},
  author       = {Lingzhi He and Yakun Chang and Runmin Cong and Hongyu Liu and Shujuan Huang and Renshuai Tao and Yao Zhao},
  doi          = {10.1109/TMM.2025.3543096},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Rethinking depth guided reflection removal},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Boundary discretization and reliable classification network
for temporal action detection. <em>TMM</em>, 1–14. (<a
href="https://doi.org/10.1109/TMM.2025.3543108">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Temporal action detection aims to recognize the action category and determine each action instance&#39;s starting and ending time in untrimmed videos. The mixed method has demonstrated notable performance by integrating both anchor-based and anchor-free approaches. However, while it leverages the strengths of each method, it also retains their respective limitations. For instance, the anchor-based approach depends on manually crafted anchors tailored to specific datasets, while the anchor-free approach predicts potential action instances at each temporal position, resulting in a significant number of false positives in category prediction. The inclusion of these limitations undermines the potential benefits of the mixed method. In this paper, we propose a novel Boundary Discretization and Reliable Classification Network (BDRC-Net) that addresses the issues above by introducing boundary discretization and reliable classification modules. Specifically, the boundary discretization module (BDM) elegantly merges anchor-based and anchor-free approaches in the form of boundary discretization, eliminating the need for the traditional handcrafted anchor design. Furthermore, the reliable classification module (RCM) predicts reliable global action categories to reduce false positives. Extensive experiments conducted on different benchmarks demonstrate that our proposed method achieves competitive detection performance. Our source code is available at https://github.com/zhenyingfang/BDRC-Net.},
  archive      = {J_TMM},
  author       = {Zhenying Fang and Jun Yu and Richang Hong},
  doi          = {10.1109/TMM.2025.3543108},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Boundary discretization and reliable classification network for temporal action detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Rain2Avoid: Learning deraining by self-supervision.
<em>TMM</em>, 1–15. (<a
href="https://doi.org/10.1109/TMM.2025.3542981">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Images captured on rainy days often contain rain streaks that can obscure important scenery and degrade the performance of high-level vision tasks, such as image segmentation in autonomous vehicles. As a result, image deraining, a low-level vision task focused on removing rain streaks from images, has gained popularity over the past decade. Recent advancements have primarily concentrated on supervised image deraining methods, which rely on paired rain-clean image datasets to train deep neural network models. However, collecting such paired real data is challenging and time-consuming. To address this, our method introduces a novel self-supervised approach that leverages the proposed locally dominant gradient prior and non-local self-similarity stochastic sampling. This approach extracts potential rain streaks and generates stochastic derained references for image deraining. Experimental results on public benchmark image-deraining datasets show that our proposed method performs favorably against state-of-the-art few-shot and self-supervised image deraining methods.},
  archive      = {J_TMM},
  author       = {Yan-Tsung Peng and Wei-Hua Li and Zihao Chen},
  doi          = {10.1109/TMM.2025.3542981},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Rain2Avoid: Learning deraining by self-supervision},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-scale retinex unfolding network for low-light image
enhancement. <em>TMM</em>, 1–13. (<a
href="https://doi.org/10.1109/TMM.2025.3543015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Retinex theory-based low-light image enhancement methods have received increasing attention and achieved tremendous advancements. However, there still exist two seldom-explored issues: 1) The above methods only formally simulate the Retinex decomposition, resulting in lacking explicit interpretability. 2) They usually are performed in single-scale space, leading to suboptimal enhancement results. In this paper, we propose an interpretable Multi-scale Retinex Unfolding Network (MRUNet) for low-light image enhancement, which can tackle both of the aforementioned issues simultaneously. Specifically, we formulate low-light image enhancement as a multi-scale Retinex optimization problem and design an iteration minimization solution to solve it. The optimization solution is further unfolded to fabricate MRUNet, which is empowered with clear physical significance and multi-scale prior knowledge in favor of image enhancement. However, it will aggravate model size and efficiency when exploiting multiple proximal mapping networks to extract multi-scale prior from multi-scale inputs. To surmount the issue, we propose a Scale-Aware Proximal mapping Module (SAPM), which efficiently collect multi-scale prior knowledge via the weight sharing strategy. In SAPM, we tailor a scale-aware transformer to model the specific scale-similarity among different scales. Extensive experiments manifest that MRUNet surpasses other Retinex-based low-light image enhancement methods on multiple benchmarks. Our code and pretrained model can be found in https://github.com/WHK-Huake/MRUNet.},
  archive      = {J_TMM},
  author       = {Huake Wang and Xingsong Hou and Jutao Li and Yadi Yan and Wenke Sun and Xin Zeng and Kaibing Zhang and Xiangyong Cao},
  doi          = {10.1109/TMM.2025.3543015},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-scale retinex unfolding network for low-light image enhancement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). WiViPose: A video-aided wi-fi framework for
environment-independent 3D human pose estimation. <em>TMM</em>, 1–15.
(<a href="https://doi.org/10.1109/TMM.2025.3543090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The inherent complexity of Wi-Fi signals makes video-aided Wi-Fi 3D pose estimation difficult. The challenges include the limited generalizability of the task across diverse environments, its significant signal heterogeneity, and its inadequate ability to analyze local and geometric information. To overcome these challenges, we introduce WiViPose, a video-aided Wi-Fi framework for 3D pose estimation, which attains enhanced cross-environment generalization through cross-layer optimization. Bilinear temporal-spectral fusion (BTSF) is initially used to fuse the time-domain and frequency-domain features derived from Wi-Fi. Video features are derived from a multiresolution convolutional pose machine and enhanced by local self-attention. Cross-modality data fusion is facilitated through an attention-based transformer, with the process further refined under a supervisory mechanism. WiViPose demonstrates effectiveness by achieving an average percentage of correct keypoints (PCK)@50 of 91.01% across three typical indoor environments.},
  archive      = {J_TMM},
  author       = {Lei Zhang and Haoran Ning and Jiaxin Tang and Zhenxiang Chen and Yaping Zhong and Yahong Han},
  doi          = {10.1109/TMM.2025.3543090},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {WiViPose: A video-aided wi-fi framework for environment-independent 3D human pose estimation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond correlation: Evaluating multimedia quality models
with the constrained concordance index. <em>TMM</em>, 1–13. (<a
href="https://doi.org/10.1109/TMM.2025.3542991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This study investigates the evaluation of multimedia quality models, focusing on the inherent uncertainties in subjective Mean Opinion Score (MOS) ratings due to factors like rater inconsistency and bias. Traditional statistical measures such as Pearson&#39;s Correlation Coefficient (PCC), Spearman&#39;s Rank Correlation Coefficient (SRCC), and Kendall&#39;s Tau (KTAU) often fail to account for these uncertainties, leading to inaccuracies in model performance assessment. We introduce the Constrained Concordance Index (CCI), a novel metric designed to overcome the limitations of existing metrics by considering the statistical significance of MOS differences and excluding comparisons where MOS confidence intervals overlap. Through comprehensive experiments across various domains including speech and image quality assessment, we demonstrate that CCI provides a more robust and accurate evaluation of instrumental quality models, especially in scenarios of low sample sizes, rater group variability, and restriction of range. Our findings suggest that incorporating rater subjectivity and focusing on statistically significant pairs can significantly enhance the evaluation framework for multimedia quality prediction models. This work not only sheds light on the overlooked aspects of subjective rating uncertainties but also proposes a methodological advancement for more reliable and accurate quality model evaluation.},
  archive      = {J_TMM},
  author       = {Alessandro Ragano and Helard Becerra Martinez and Andrew Hines},
  doi          = {10.1109/TMM.2025.3542991},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Beyond correlation: Evaluating multimedia quality models with the constrained concordance index},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GFTLS-SLT: Gloss-free transformer based lexical and semantic
awareness framework for multimodal sign language translation.
<em>TMM</em>, 1–12. (<a
href="https://doi.org/10.1109/TMM.2025.3542990">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Sign language provides communication support for deaf and severely hearing-impaired people. Sign language translation (SLT) bridges the hearing-impaired and hearing communities. Existing SLT methods use gloss as intermediate supervisory information to help the model sense gesture boundaries and understand global semantics. However, annotating gloss requires great cost, especially in multimodal SLT task. This paper proposes GFTLS-SLT: gloss-free Transformer based lexical and semantic awareness framework for SLT. The multimodal alignment and fusion module in GFTLS-SLT utilizes cross-attention to align multimodal features, and fuses them using the improved statistical and contrastive attention. To replace the role of gloss, GFTLS-SLT designs gesture lexical awareness (GLA) and global semantic awareness (GSA) modules. The GLA module utilizes the defined observation matrix to obtain the lexical meaning matrix, and makes the model sense gesture boundaries by the designed dynamic step-size lexical matching algorithm. The multimodal semantic header is used by GSA module to represent the sign language global semantic and is aligned with the spoken semantic on semantic space. In addition, the experiment results of GFTLSSLT on publicly available multimodal SLT datasets show that its performance reaches that of SLT methods with gloss supervision},
  archive      = {J_TMM},
  author       = {Jiangtao Zhang and Qingshan Wang and Qi Wang},
  doi          = {10.1109/TMM.2025.3542990},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {GFTLS-SLT: Gloss-free transformer based lexical and semantic awareness framework for multimodal sign language translation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty-aware semi-supervised learning segmentation for
remote sensing images. <em>TMM</em>, 1–15. (<a
href="https://doi.org/10.1109/TMM.2025.3543026">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Deep learning based remote sensing (RS) image segmentation significantly impacts several real application scenarios. Behind its success, massive labeled data plays an important role. However, annotating high-resolution RS images requires time-consuming and relevant expertise efforts. To address it, many works dive into semi-supervised learning which utilizes raw information embedded in unlabeled data to improve the segmentation model. Nevertheless, previous studies ignore the integrity and effectiveness of the potential context information hidden in RS data. In this work, we propose an uncertainty-aware masked consistency learning (U-MCL) framework that contains an uncertainty-aware masked denoising (U-MD) module and an uncertainty-aware masked image consistency (U-MIC) module. U-MCL initially generates a patch-wise uncertainty map for each unlabeled image during each training iteration, which is then used to derive an adaptive mask ratio for pseudo-label denoising in U-MD. Simultaneously, the uncertainty map is adopted to model a masked unlabeled image for reasoning unseen areas in U-MIC. Consequently, U-MCL is capable of enhancing model performance by engaging in accurate and stable consistency learning while preserving the integrity of the context and employing the context to infer the predictions of the masked regions safely. Extensive experiments on six RS datasets, i.e., ISPRS Vaihingen, FloodNet, MiniFrance, LoveDA, MER, and MSL, demonstrate the superiority of our U-MCL over recent most advanced methods, achieving new state-of-the-art performance under all benchmarks. Code will be available at https://github.com/xiaoqiang-lu/U-MCL.},
  archive      = {J_TMM},
  author       = {Xiaoqiang Lu and Lingling Li and Licheng Jiao and Xu Liu and Fang Liu and Wenping Ma and Shuyuan Yang},
  doi          = {10.1109/TMM.2025.3543026},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Uncertainty-aware semi-supervised learning segmentation for remote sensing images},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HGFormer: Topology-aware vision transformer with HyperGraph
learning. <em>TMM</em>, 1–12. (<a
href="https://doi.org/10.1109/TMM.2025.3542958">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The computer vision community has witnessed an extensive exploration of vision transformers in the past two years. Drawing inspiration from traditional schemes, numerous works focus on introducing vision-specific inductive biases. However, the implicit modeling of permutation invariance and fully-connected interaction with individual tokens disrupts the regional context and spatial topology, further hindering higher-order modeling. This deviates from the principle of perceptual organization that emphasizes the local groups and overall topology of visual elements. Thus, we introduce the concept of hypergraph for perceptual exploration. Specifically, we propose a topology-aware vision transformer called HyperGraph Transformer (HGFormer). Firstly, we present a Center Sampling K-Nearest Neighbors (CS-KNN) algorithm for semantic guidance during hypergraph construction. Secondly, we present a topology-aware HyperGraph Attention (HGA) mechanism that integrates hypergraph topology as perceptual indications to guide the aggregation of global and unbiased information during hypergraph messaging. Using HGFormer as visual backbone, we develop an effective and unitive representation, achieving distinct and detailed scene depictions. Empirical experiments show that the proposed HGFormer achieves competitive performance compared to the recent SoTA counterparts on various visual benchmarks. Extensive ablation and visualization studies provide comprehensive explanations of our ideas and contributions.},
  archive      = {J_TMM},
  author       = {Hao Wang and Shuo Zhang and Biao Leng},
  doi          = {10.1109/TMM.2025.3542958},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {HGFormer: Topology-aware vision transformer with HyperGraph learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DLS-HCAN: Duplex label smoothing based hierarchical
context-aware network for fine-grained 3D shape classification.
<em>TMM</em>, 1–16. (<a
href="https://doi.org/10.1109/TMM.2025.3543077">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained 3D shape classification (FGSC) has garnered significant attention recently and has made notable advancements. However, due to high inter-class similarity and intra-class diversity, it is still a challenge for existing methods to capture subtle differences between different subcategories for FGSC. On the one hand, one-hot labels in loss function are too hard to describe the above data characteristics, and on the other hand, local details are submerged in the global features extraction process and final network constraints, impacting classification results. In this paper, we propose a duplex label smoothingbased hierarchical context-aware network for fine-grained 3D shape classification, named DLS-HCAN. Specifically, DLS-HCAN firstly employs a hierarchical context-aware network (HCAN), in which the intra-view context attention mechanism (intra-ATT) and the inter-view context multilayer perceptron (inter-MLP) are designed to focus on and discern the beneficial local details. Subsequently, we propose a novel duplex label smoothing (DLS) regularization in which shape-level and view-level smooth labels are separately applied in two improved loss functions, adapting to the fine-grained data characteristics and considering the varying uniqueness of different views. Notably, our approach does not require additional annotation information. Experimental results and comparison with state-of-the-art methods demonstrate the superiority of our proposed DLS-HCAN for FGSC. In addition, our approach also achieves comparable performance for the coarse-grained dataset on ModelNet40},
  archive      = {J_TMM},
  author       = {Shaojin Bai and Liang Zheng and Jing Bai and Xiangyu Ma},
  doi          = {10.1109/TMM.2025.3543077},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DLS-HCAN: Duplex label smoothing based hierarchical context-aware network for fine-grained 3D shape classification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Spatial-temporal aware-based unsupervised network for
infrared small target detection. <em>TMM</em>, 1–15. (<a
href="https://doi.org/10.1109/TMM.2025.3543002">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the advantages of deep learning (DL) techniques, various infrared (IR) small target detection networks have been proposed. While many networks aim at single-frame detection through supervised learning, ignoring abundant spatial-temporal information and causing heavy labeling costs. In this paper, we develop a 3-D spatial-temporal knowledge aware-based unsupervised network for IR target detection (STUTD). Specifically, we transform IR sequences into 3-D spatial-temporal tensors as data foundation. Based on the designed spatial-temporal Swin Transformer block (ST-STB), we introduce a multiscale feature extraction and aggregation (MFEA) module for effective feature extraction. And a Variational Autoencoder (VAE)-style background reconstruction module with a multihead gating mechanism is designed for background reconstruction. Besides, a designed sparse cardinality selection of residuals performs element-wise filtering on the residuals between the original tensors and the reconstructed background to obtain a pure target tensor. By an unsupervised learning approach, STUTD can achieve IR small target detection. Comprehensive experiments illustrate the superiority of STUTD among state-of-the-art methods. It can be concluded that STUTD has satisfactory overall performance and real-time performance.},
  archive      = {J_TMM},
  author       = {Yuan Luo and Xiaorun Li and Shuhan Chen},
  doi          = {10.1109/TMM.2025.3543002},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Spatial-temporal aware-based unsupervised network for infrared small target detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Moiré-watermark: Robust watermarking against screen-shooting
using moiré patterns. <em>TMM</em>, 1–15. (<a
href="https://doi.org/10.1109/TMM.2025.3543008">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prevalence of digital content leakage via screen capture highlights the urgent need for robust watermarking solutions capable of withstanding cross-media transmission. Current approaches primarily focus on developing watermarking techniques resilient to screen-shooting distortions, where distinguishing the watermark signal from these distortions is paramount. In contrast, our study addresses an inverse problem by investigating the generation patterns of noise during screen-shooting and considering them as feasible representations of watermark signals. Leveraging Moiré patterns as one of the distortion signals naturally generated by the interaction between electronic screens and camera sensors, we propose Moiré-watermark, presenting watermark information encoded into meticulously crafted Moiré patterns within images. To enhance the naturalness of Moiré-watermark amidst the irregularities of screen-shooting Moiré patterns, we encode watermark signals using gratings at different angles. A corresponding angle-based decoding method facilitates effective blind extraction of watermarks. Comprehensive experimental evaluations under diverse conditions of distance, angle, lighting, and across various capturing and display devices, alongside comparisons with existing methods, validate the superior performance of Moiré-watermark.},
  archive      = {J_TMM},
  author       = {Heng Wang and Hongxia Wang and Fei Zhang and Zhenhao Shi and Xinyi Huang},
  doi          = {10.1109/TMM.2025.3543008},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Moiré-watermark: Robust watermarking against screen-shooting using moiré patterns},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A robust coverless audio steganography based on differential
privacy clustering. <em>TMM</em>, 1–16. (<a
href="https://doi.org/10.1109/TMM.2025.3543107">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Conventional audio steganography methods typically require embedding secret information into the carrier, making them vulnerable to steganalysis. To address this issue, we propose a novel coverless audio steganography method that hides information by generating carriers and establishing mapping rules rather than embedding data directly. Our approach leverages a differential privacy clustering algorithm to cluster audio data and select representative audio files, thereby enhancing the security of the steganography. Additionally, we introduce an improved audio feature extraction method that combines traditional Mel-frequency cepstral coefficients (MFCC) with global statistical information, significantly boosting the robustness of the secret information against common audio attacks, particularly time-stretching attacks. Experimental results show that our method achieves a robustness rate of up to 95% against time-stretching and maintains an average security accuracy rate exceeding 97% across various attack scenarios. The proposed method ensures that the audio carrier remains unaltered, thus effectively resisting detection by steganalysis tools. This innovative approach provides a practical and efficient solution for the secure transmission of information in the digital era.},
  archive      = {J_TMM},
  author       = {Yan Feng and Longting Xu and Xiaochen Lu and Guanglin Zhang and Wei Rao},
  doi          = {10.1109/TMM.2025.3543107},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {A robust coverless audio steganography based on differential privacy clustering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Imperceptible backdoor attacks on text-guided 3D scene
grounding. <em>TMM</em>, 1–15. (<a
href="https://doi.org/10.1109/TMM.2025.3543050">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the maturity of depth sensors, the vulnerability of 3D point cloud models has received increasing attention in various applications such as autonomous driving and robot navigation. Previous 3D adversarial attackers mainly focus on attacking naive 3D classification models by perturbing 3D objects. However, since real-world 3D applications generally rely on more complicated scene-based point cloud data, these attack methods are impractical to deploy in realistic scenarios. Therefore, in this paper, we attempt to introduce the adversarial attacks into a more practical yet challenging large-scale scene-based 3D task, i.e., text-guided 3D scene grounding. To make perturbations both effective and imperceptible in scene cases, we investigate the vulnerability of 3D grounding models to backdoor attacks, which implant backdoor triggers into 3D models via data poisoning so as to control the models&#39; predictions at test time. Specifically, we propose a novel Joint Scene-Text Backdoor Attack (JSTBA) method to embed triggers in each of the input modalities and activate the malicious behavior only when both triggers are present. We further design a visual trigger optimization strategy to place the visual trigger appropriately in the 3D scene, aiming to make it natural and imperceptible. Extensive experiments are conducted on seven classic 3D grounding models and three datasets, showing that our JSTBA attack significantly degrades the performance of 3D models on the poisoned data while gaining comparable performance with the benign models on the clean data.},
  archive      = {J_TMM},
  author       = {Daizong Liu and Wei Hu},
  doi          = {10.1109/TMM.2025.3543050},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Imperceptible backdoor attacks on text-guided 3D scene grounding},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). TBag: Three recipes for building up a lightweight hybrid
network for real-time SISR. <em>TMM</em>, 1–14. (<a
href="https://doi.org/10.1109/TMM.2025.3542966">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prevalent convolution neural network (CNN) and Transformer have revolutionized the area of single-image super-resolution (SISR). Though these models have significantly improved performance, they often struggle with real-time applications or on resource-constrained platforms due to their complexity. In this paper, we propose TBag, a lightweight hybrid network that combines the strengths of CNN and Transformer to address these challenges. Our method simplifies the Transformer block with three key optimizations: 1) No projection layer is applied to the value in the original self-attention operation; 2) The number of tokens is rescaled before the self-attention operation and then rescaled back for easing of computation; 3) The expansion factor of the original feed-forward network (FFN) is adjusted. These optimizations enable the development of an efficient hybrid network tailored for real-time SISR. Notably, the hybrid design of CNN and Transformer further enhances both local detail recovery and global feature modeling. Extensive experiments show that TBag achieves a competitive trade-off between effectiveness and efficiency compared to previous lightweight SISR methods (e.g., +0.42dB PSNR with an 86.7% reduction in latency). Moreover, TBag&#39;s real-time capabilities make it highly suitable for practical applications, with the TBag-Tiny version achieving up to 59 FPS on hardware devices. Future work will explore the potential of this hybrid approach in other image restoration tasks, such as denoising and deblurring.},
  archive      = {J_TMM},
  author       = {Ruoyi Xue and Cheng Cheng and Hang Wang and Hongbin Sun},
  doi          = {10.1109/TMM.2025.3542966},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {TBag: Three recipes for building up a lightweight hybrid network for real-time SISR},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Class incremental learning for image classification with
out-of-distribution task identification. <em>TMM</em>, 1–14. (<a
href="https://doi.org/10.1109/TMM.2025.3543088">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Class Incremental Learning (CIL) for image classification aims to address real-world scenarios by allowing a model to learn new categories while retaining the knowledge of old categories. It is more challenging than Task Incremental Learning (TIL) as task ID is not provided during testing. Therefore, transitioning from CIL to TIL is an intuitive approach to handling CIL problems for image classification. Currently, the main challenge of this approach lies in improving the accuracy of task identification. To address this issue, we propose to use a large-scale image-text pre-training model (i.e. CLIP) as the backbone, training and saving different classifiers for different tasks. Each classifier not only includes the classes of the current task, but also an Out-of-distribution (OOD) class corresponding to the classes encountered in all previous tasks. At test time, we iterate through classifiers from the last task to find the correct task ID of the test image, and perform classification in a TIL way. In addition, to tackle the issue of early-stop termination in iterative prediction due to model bias toward later tasks, we propose using CLIP zero-shot ability to assist learned OOD detection. Experiments show that our method achieves state-of-the-art performance on the traditional many-shot and the more challenging few-shot settings of CIFAR-100 and ImageNet-Subset datasets.},
  archive      = {J_TMM},
  author       = {Xusheng Cao and Haori Lu and Xialei Liu and Ming-Ming Cheng},
  doi          = {10.1109/TMM.2025.3543088},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Class incremental learning for image classification with out-of-distribution task identification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Unbiased meta reinforcement learning for interactive
recommender systems. <em>TMM</em>, 1–13. (<a
href="https://doi.org/10.1109/TMM.2025.3543045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Interactive recommender systems have garnered widespread attention due to their ability to dynamically update recommendation strategies based on user feedback, enhancing the user&#39;s interactive experience. To maximize long-term user satisfaction, existing research has incorporated reinforcement learning into interactive recommender systems and combined it with meta-learning to form a meta-reinforcement learning framework that further addresses the cold-start problem in interactive recommendation. However, on one hand, there are latent confounders affecting user feedback; on the other hand, since training samples are observed rather than experimentally obtained, selection bias and exposure bias exist in the interactive data. Most existing studies remove biases using the method of Inverse Propensity Score, which often utilizes fixed propensity scores and neglects the latent confounders affecting user feedback. In this paper, we propose an unbiased interactive recommender system (UIRS) based on a meta-reinforcement learning framework. To eliminate the impact of latent confounders in the state encoding process, we design a user preference representer consisting of three interconnected gated recurrent units. Additionally, we use the item recommendation probabilities output from the policy network as propensity scores and design the objective functions based on these scores, to eliminate biases while addressing latent confounders. Extensive experiments conducted on three benchmark datasets demonstrate that our proposed UIRS model achieves significant improvements over existing state-of-the-art baseline models.},
  archive      = {J_TMM},
  author       = {Huiting Liu and Xinlong Lv and Peng Zhao and Peipei Li and Xindong Wu},
  doi          = {10.1109/TMM.2025.3543045},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Unbiased meta reinforcement learning for interactive recommender systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real scene single image dehazing network with multi-prior
guidance and domain transfer. <em>TMM</em>, 1–16. (<a
href="https://doi.org/10.1109/TMM.2025.3543063">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image dehazing is essential to boost the visual quality of images captured in hazy conditions. Recently, many learning-based methods were proposed to achieve single image dehazing with the training of tremendous paired synthetic hazy/ real clean images. Due to the domain gap between real and synthetic scenes, these models cannot generalize well to various real hazy scenes, leading to under-dehazed results. To overcome this problem, we propose a real scene image Dehazing Network with Multi-prior Guidance and Domain Transfer (DNMGDT). Our DNMGDT is based on a parameter shared architecture trained by synthetic hazy images and real hazy images simultaneously. For real hazy images, multiple prior-based dehazed images are adopted as pseudo clean images. An Image Quality Guided Adaptive Weighting (IQGAW) scheme is proposed to form the supervision by automatically weighting different parts of these prior-based dehazed images and suppressing negative information of them. Moreover, to reduce the domain gap between real and synthetic hazy scenes, a Physical Model Guided image level Domain Transfer (PMGDT) mechanism is proposed to regularize the learning process with consistency constraint. Experiments on various datasets demonstrated the effectiveness of our proposed method especially for real hazy scenes. The official codes see https://github.com/NianWang-HJJGCDX/DNMGDT},
  archive      = {J_TMM},
  author       = {Yanzhao Su and Nian Wang and Zhigao Cui and Yanping Cai and Chuan He and Aihua Li},
  doi          = {10.1109/TMM.2025.3543063},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Real scene single image dehazing network with multi-prior guidance and domain transfer},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DCDL: Dual causal disentangled learning for zero-shot
sketch-based image retrieval. <em>TMM</em>, 1–16. (<a
href="https://doi.org/10.1109/TMM.2025.3543035">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Zero-shot sketch-based image retrieval (ZS-SBIR) is a challenging task that hinges on overcoming the cross-domain differences between sketches and images. Previous methods primarily address cross-domain differences by creating a common embedding space, improving final retrieval results. However, most previous approaches have overlooked a critical aspect: sketch-based image retrieval task actually requires only the cross-domain invariant information relevant to the retrieval. Irrelevant information (such as posture, expression, background, and specificity) may detract from retrieval accuracy. In addition, most previous methods perform well on traditional SBIR datasets but lack corresponding research on generalization and extensibility in the face of more diverse and complex data. To address these issues, we propose a Dual Causal Disentangled Learning (DCDL) for ZS-SBIR. This approach can mitigate the negative impact of irrelevant features by separating retrieval-relevant features in the latent variable space. Specifically, we constructed a causal disentanglement model using two Variational Autoencoders (VAE), each applied to the sketch and image domains, to obtain disentangled variables with exchangeable attributes. Our framework effectively integrates causal intervention with disentangled representation learning, enabling a clearer separation of cross-domain retrieval-relevant and intra-class irrelevant features, which can be recombined into new reconstructed samples. Concurrently, we designed a Dual Alignment Module (DAM), leveraging the accurate and comprehensive semantic features provided by a text encoder pre-trained on large-scale datasets to supplement semantic associations and align disentangled retrieval-relevant features. The Dual Alignment Module enhances the model&#39;s ability to generalize across diverse datasets by effectively aligning retrieval-relevant information from different domains. Extensive experiments demonstrate that our method achieves state-of-the-art (SOTA) performance on the Sketchy and TU-Berlin datasets. Additionally, more experiments on larger scale dataset QuickDraw, fine-grained datasets, Shoe-V2 and Chair-V2, as well as an inter-dataset further validate the generalization and extensibility of DCDL.},
  archive      = {J_TMM},
  author       = {Qiang Li and Shihao Wang and Wei Zhang and Shaojin Bai and Weizhi Nie and Anan Liu},
  doi          = {10.1109/TMM.2025.3543035},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DCDL: Dual causal disentangled learning for zero-shot sketch-based image retrieval},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Language knowledge-assisted representation learning for
skeleton-based action recognition. <em>TMM</em>, 1–16. (<a
href="https://doi.org/10.1109/TMM.2025.3543034">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {How humans understand and recognize the actions of others is a complex neuroscientific problem that involves a combination of cognitive mechanisms and neural networks. Research has shown that humans have brain areas that recognize actions that process top-down attentional information, such as the temporoparietal association area. Also, humans have brain regions dedicated to understanding the minds of others and analyzing their intentions, such as the medial prefrontal cortex of the temporal lobe. Skeleton-based action recognition creates mappings for the complex connections between the human skeleton movement patterns and behaviors. Although existing studies encoded meaningful node relationships and synthesized action representations for classification with good results, few of them considered incorporating a priori knowledge to aid potential representation learning for better performance. LA-GCN proposes a graph convolution network using large-scale language models (LLM) knowledge assistance. First, the LLM knowledge is mapped into a priori global relationship (GPR) topology and a priori category relationship (CPR) topology between nodes. The GPR guides the generation of new “bone” representations, aiming to emphasize essential node information from the data level. The CPR mapping simulates category prior knowledge in human brain regions, encoded by the PC-AC module and used to add additional supervision-forcing the model to learn class-distinguishable features. In addition, to improve information transfer efficiency in topology modeling, we propose multi-hop attention graph convolution. It aggregates each node&#39;s k-order neighbor simultaneously to speed up model convergence. LA-GCN reaches state-of-the-art on NTU RGB+D, NTU RGB+D 120, and NW-UCLA datasets.},
  archive      = {J_TMM},
  author       = {Haojun Xu and Yan Gao and Zheng Hui and Jie Li and Xinbo Gao},
  doi          = {10.1109/TMM.2025.3543034},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Language knowledge-assisted representation learning for skeleton-based action recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Efficient hierarchical feature collaboration transformer for
image inpainting. <em>TMM</em>, 1–13. (<a
href="https://doi.org/10.1109/TMM.2025.3543039">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Existing image inpainting methods face limitations in detail restoration. Although transformer-based models have made certain progress recently, the lack of hierarchical feature interaction and insufficient consideration of the importance of features at different network levels lead to semantic ambiguity in image reconstruction. To enhance the visual quality and accuracy of image inpainting, we adopt a multi-level feature fusion approach and propose a novel, efficient hierarchical feature collaboration transformer (HFCT). Our approach comprises two modules: dual stream gated feature fusion (DSGF) and region-separated attention module (RSAM), effectively capturing features at different levels of the network and enhancing inter-level information exchange. The DSGF module uses soft gating to fuse primary and advanced features, strengthening the connection from local to global consistency and reducing artifacts. The RSAM module resolves attention isolation issues in feature fusion through region-separated attention, strengthening the understanding of feature relationships, capturing more image semantics, and improving restoration accuracy. Extensive experiments on the Paris StreetView, CelebA-HQ, and Places2 benchmark datasets demonstrate that our proposed method achieves superior image inpainting quality compared to several state-of-the-art inpainting algorithms. Please refer to the project page: https://github.com/csfunuo/HFCT.},
  archive      = {J_TMM},
  author       = {Dengyong Zhang and Nuo Fu and Xin Liao and Jiaxin Chen and Hengfu Yang and Gaobo Yang},
  doi          = {10.1109/TMM.2025.3543039},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Efficient hierarchical feature collaboration transformer for image inpainting},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Viewport prediction with unsupervised multiscale causal
representation learning for virtual reality video streaming.
<em>TMM</em>, 1–13. (<a
href="https://doi.org/10.1109/TMM.2025.3543087">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rise of the metaverse has driven the rapid development of various applications, such as Virtual Reality (VR) and Augmented Reality (AR). As a form of multimedia in the metaverse, VR video streaming (a.k.a., VR spherical video streaming and $360^{\circ }$ video streaming) can provide users with a $360^{\circ }$ immersive experience. Generally, transmitting VR video requires far more bandwidth than regular videos, which greatly strains existing network transmission. Predicting and selectively streaming VR video in the users&#39; viewports in advance can reduce bandwidth consumption and system latency. However, existing methods either consider only historical viewport-based prediction methods or predict viewports by correlations between visual features of video frames, making it hard to adapt to the dynamics of users and video content. In the meantime, spurious correlations between visual features lead to inaccurate and unreliable prediction results. Hence, we propose an unsupervised multiscale causal representation learning (UMCRL)-based method to predict viewports in VR video streaming, including user preference-based and video content-based viewport prediction models. The former is designed by a position predictor to predict the future users&#39; viewports based on their historical viewports in multiple video frames to adapt to users&#39; dynamic preferences. The latter achieves unsupervised multiscale causal representation learning through an asymmetric causal regressor, used to infer the causalities between local and global-local visual features in video frames, thereby helping the model understand the contextual information in the videos. We embed the causalities in the transformer decoder via causal self-attention for predicting the users&#39; viewports, adapting to the dynamic changes of video content. Finally, combining the results of the two aforementioned models yields the final prediction of the users&#39; viewports. In addition, the QoE of users is satisfied by assigning different bitrates to the tiles in the viewport through a pyramid-based bitrate allocation. The experimental results verify the effectiveness of the method.},
  archive      = {J_TMM},
  author       = {Yingjie Liu and Dan Wang and Bin Song},
  doi          = {10.1109/TMM.2025.3543087},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Viewport prediction with unsupervised multiscale causal representation learning for virtual reality video streaming},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced generative framework with LLMs for multimodal
emotion-cause pair extraction in conversations. <em>TMM</em>, 1–12. (<a
href="https://doi.org/10.1109/TMM.2025.3543080">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Emotion-Cause Pair Extraction (ECPE) in conversations aims to identify the emotional utterances (even their categories) along with their corresponding causal utterances, which is crucial in understanding the cause-effect relationship in dialogues. While prior studies of ECPE have predominantly focused on purely textual dialogues and neglected the exploration on the natural scenario of the dialogues with multimodal features, i.e., Multimodal Emotion-Cause Pair Extraction (MECPE) in conversations. To attempt this scenario, we propose a Generative approach for Multimodal Emotion-Cause pair extraction (GMEC) with a single stage, thus effectively reducing errors associated with the propagation and accumulation for MECPE. This approach can not only uniformly handle the information of diverse modalities, but also address all emotion and cause analysis tasks uniformly. Additionally, instead of utilizing the fixed commonsense knowledge base as previously, we resort to the Large Language Models (LLMs), which possess a powerful ability to emerge new knowledge, thereby acting as implicit knowledge engines for MECPE. We refer to this approach as enhanced GMEC. Extensive experimental results and detailed analysis demonstrate a notable improvement in the generative approach. Moreover, the integration of external knowledge from LLMs optimizes the efficiency of data utilization, particularly in few-shot scenarios. The integration of the generative model with LLMs has resulted in a cumulative enhancement of 4.94%, 10.90% on MECPE and MECPE-C (with emotion Category).},
  archive      = {J_TMM},
  author       = {Xincheng Ju and Dong Zhang and Junhui Li and Shoushan Li and Guodong Zhou},
  doi          = {10.1109/TMM.2025.3543080},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Enhanced generative framework with LLMs for multimodal emotion-cause pair extraction in conversations},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SyNet: A synergistic network for 3D object detection through
geometric-semantic-based multi-interaction fusion. <em>TMM</em>, 1–11.
(<a href="https://doi.org/10.1109/TMM.2025.3542993">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by rising demands in autonomous driving, robotics, etc., 3D object detection has recently achieved great advancement by fusing optical images and LiDAR point data. On the other hand, most existing optical-LiDAR fusion methods straightly overlay RGB images and point clouds without adequately exploiting the synergy between them, leading to suboptimal fusion and 3D detection performance. Additionally, they often suffer from limited localization accuracy without proper balancing of global and local object information. To address this issue, we design a synergistic network (SyNet) that fuses geometric information, semantic information, as well as global and local information of objects for robust and accurate 3D detection. The SyNet captures synergies between optical images and LiDAR point clouds from three perspectives. The first is geometric, which derives high-quality depth by projecting point clouds onto multi-view images, enriching optical RGB images with 3D spatial information for a more accurate interpretation of image semantics. The second is semantic, which voxelizes point clouds and establishes correspondences between the derived voxels and image pixels, enriching 3D point clouds with semantic information for more accurate 3D detection. The third is balancing local and global object information, which introduces deformable self-attention and cross-attention to process the two types of complementary information in parallel for more accurate object localization. Extensive experiments show that SyNet achieves 70.7% mAP and 73.5% NDS on the nuScenes test set, demonstrating its effectiveness and superiority as compared with the state-of-the-art.},
  archive      = {J_TMM},
  author       = {Xiaoqin Zhang and Kenan Bi and Sixian Chan and Shijian Lu and Xiaolong Zhou},
  doi          = {10.1109/TMM.2025.3542993},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SyNet: A synergistic network for 3D object detection through geometric-semantic-based multi-interaction fusion},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Frefusion: Frequency domain transformer for infrared and
visible image fusion. <em>TMM</em>, 1–9. (<a
href="https://doi.org/10.1109/TMM.2025.3543019">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible and infrared image fusion(VIF) provides more comprehensive understanding of a scene and can facilitate subsequent processing. Although frequency domain contains valuable global information in low frequency and rapid pixel intensity variation data in high frequency of images, existing fusion methods mainly focus on spatial domain. To close this gap, a novel VIF method in frequency domain is proposed. First, a frequency-domain feature extraction module is developed for source images. Then, a frequency-domain transformer fusion method is designed to merge the extracted features. Finally, a residual reconstruction module is introduced to obtain final fused images. To the best of our knowledge, it is the first time that image fusion study is conducted from frequency domain perspective. Comprehensive experiments on three datasets, i.e., MSRS, TNO, and Roadscene, demonstrate that the proposed approach obtains superior fusion performance over several state-of-the-art fusion methods, indicating its great potential as a generic backbone for VIF tasks.},
  archive      = {J_TMM},
  author       = {Junjie Shi and Puhong Duan and Xiaoguang Ma and Jianning Chi and Yong Dai},
  doi          = {10.1109/TMM.2025.3543019},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-9},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Frefusion: Frequency domain transformer for infrared and visible image fusion},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SeaCap: Multi-sight embedding and alignment for one-stage
image captioner. <em>TMM</em>, 1–15. (<a
href="https://doi.org/10.1109/TMM.2025.3535303">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent mainstream image captioning methods usually adopt two-stage captioners, i.e., calculating the object features of the given image by a pre-trained detector and then feeding them into a language model to generate the descriptive sentences. However, such a two-stage procedure will lead to a task-based information gap that decreases the performance of the captioners, because the object features learned from the detection task are suboptimal representations and cannot provide all the necessary information for subsequent sentence generation. Besides, the object features are usually represented by the last pooling features of the detector that lose the local details of images. In this paper, we propose a novel One-Stage Image Captioner using dynamic multi-sight embedding and alignment, called SeaCap, which directly transforms input images into descriptive sentences in one stage to eliminate the information gap. Specifically, to obtain rich features, we use the Swin Transformer to capture multi-level features, followed by a sights alignment module to alleviate the vision confusion, and then feed them into a novel dynamic multi-sight embedding module to exploit both the global structure and local texture of input images. To enhance the global modeling capacity of the visual encoder, we propose a new dual-dimensional refining module to non-locally model the interaction of the embedded features. As a result, SeaCap can obtain rich and useful information to improve the performance of the captioner. Extensive comparisons on the benchmark MS-COCO, Flickr8K and Flickr30 K datasets verified the superior performance of our method.},
  archive      = {J_TMM},
  author       = {Bo Wang and Zhao Zhang and Mingbo Zhao and Xiaojie Jin and Mingliang Xu and Meng Wang},
  doi          = {10.1109/TMM.2025.3535303},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {SeaCap: Multi-sight embedding and alignment for one-stage image captioner},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Neural volumetric video coding with hierarchical coded
representation of dynamic volume. <em>TMM</em>, 1–16. (<a
href="https://doi.org/10.1109/TMM.2025.3544415">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes a novel multi-view (MV) video coding technique that leverages a four-dimensional (4D) voxel-grid representation to enhance coding efficiency, particularly in novel view synthesis. Although the voxel grid approximation provides a continuous representation for dynamic scenes, its volumetric nature requires substantial storage. The compression of MV videos can be interpreted as the compression of dense features. However, the substantial size of these features poses a significant problem relative to the generation of dynamic scenes at arbitrary viewpoints. To address this challenge, this study introduces a hierarchical coded representation of dynamic volumes based on low-rank tensor decomposition of volumetric features and develops effective coding techniques based on this representation. The proposed method employs a two-level coding strategy to capture the temporal characteristics of the decomposed features. At a higher level, spatial features are encoded, representing 3D structural information, with time-invariant components over short intervals of an MV video sequence. At a lower level, temporal features are encoded to capture the dynamics of current scenes. The spatial features are shared in a group, and temporal features are encoded at each time step. The experimental results demonstrate that the proposed technique outperforms existing MV video coding standards and current state-of-the-art methods, providing superior rate-distortion performance in the novel view synthesis of MV video compression.},
  archive      = {J_TMM},
  author       = {Ju-Yeon Shin and Jung-Kyung Lee and Gun Bang and Jun-Sik Kim and Je-Won Kang},
  doi          = {10.1109/TMM.2025.3544415},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Neural volumetric video coding with hierarchical coded representation of dynamic volume},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging content and context cues for low-light image
enhancement. <em>TMM</em>, 1–16. (<a
href="https://doi.org/10.1109/TMM.2025.3543047">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low-light conditions have an adverse impact on machine cognition, limiting the performance of computer vision systems in real life. Since low-light data is limited and difficult to annotate, we focus on image processing to enhance low-light images and improve the performance of any downstream task model, instead of fine-tuning each of the models which can be prohibitively expensive. We propose to improve the existing zero-reference low-light enhancement by leveraging the CLIP model to capture image prior and for semantic guidance. Specifically, we propose a data augmentation strategy to learn an image prior via prompt learning, based on image sampling, to learn the image prior without any need for paired or unpaired normal-light data. Next, we propose a semantic guidance strategy that maximally takes advantage of existing low-light annotation by introducing both content and context cues about the image training patches. We experimentally show, in a qualitative study, that the proposed prior and semantic guidance help to improve the overall image contrast and hue, as well as improve background-foreground discrimination, resulting in reduced over-saturation and noise over-amplification, common in related zero-reference methods. As we target machine cognition, rather than rely on assuming the correlation between human perception and downstream task performance, we conduct and present an ablation study and comparison with related zero-reference methods in terms of task-based performance across many low-light datasets, including image classification, object and face detection, showing the effectiveness of our proposed method.},
  archive      = {J_TMM},
  author       = {Igor Morawski and Kai He and Shusil Dangi and Winston H. Hsu},
  doi          = {10.1109/TMM.2025.3543047},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Leveraging content and context cues for low-light image enhancement},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DSAF: Dual space alignment framework for visible-infrared
person re-identification. <em>TMM</em>, 1–13. (<a
href="https://doi.org/10.1109/TMM.2025.3542988">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visible-infrared person re-identification (VI-ReID) is a cross-modality retrieval task that aims to match visible and infrared pedestrian images across non-overlapped cameras. However, we observe that three crucial challenges remain inadequately addressed by existing methods: (i) limited discriminative capacity for modality-shared representation, (ii) modality misalignment, and (iii) neglect of identity consistency knowledge. To solve the above issues, we propose a novel dual space alignment framework (DSAF) to constrain the modality in two specific spaces. Specifically, for (i), we design a lightweight and plug-and-play modality invariant enhancement (MIE) module to capture fine-grained semantic information and render identity discriminative. This facilitates the establishment of correlations between visible and infrared modalities, enabling the model to learn robust modality-shared features. To tackle (ii), a dual space alignment (DSA) is introduced to conduct the pixel-level alignment in both Euclidean space and Hilbert space. DSA establishes an elastic relationship between these two spaces, remaining invariant knowledge across two spaces. To solve (iii), we propose an adaptive identity-consistent learning (AIL) to discover identity-consistent knowledge between visible and infrared modalities in a dynamic manner. Extensive experiments on mainstream VI-ReID benchmarks show the superiority and flexibility of our proposed method, achieving competitive performance on mainstream datasets.},
  archive      = {J_TMM},
  author       = {Yan Jiang and Xu Cheng and Hao Yu and Xingyu Liu and Haoyu Chen and Guoying Zhao},
  doi          = {10.1109/TMM.2025.3542988},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {DSAF: Dual space alignment framework for visible-infrared person re-identification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CACP: Covariance-aware cross-domain prototypes for domain
adaptive semantic segmentation. <em>TMM</em>, 1–12. (<a
href="https://doi.org/10.1109/TMM.2025.3543016">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Domain adaptive semantic segmentation aims to reduce domain shifts / discrepancies between source and target domains, improving the source domain model&#39;s generalization ability to the target domain. Recently, prototypical methods, which primarily use single-source or single-target domain prototypes as category centers to aggregate features from both domains, have achieved competitive performance in this task. However, due to large domain shifts, single-source domain prototypes have finite generalization ability and not all source domain knowledge is conducive to model generalization. Single-target domain prototypes are noisy because they are prematurely initialized with all features filtered by pseudo labels, which causes error accumulation in the prototypes. To address these issues, we proposes a covariance-aware cross-domain prototypes method (CACP) to achieve robust domain adaptation. We propose to use both domain prototypes to dynamically rectify pseudo labels in the target domain, effectively reducing the recognition difficulty of hard target domain samples and narrowing the gap between features of the same category in both domains. In addition, to further generalize the model to the target domain, we propose two modules based on covariance correlation, FSPC (Features Selection by Prototypes Covariances) and WSPC (Weighting Source by Prototypes Coefficients), to learn discriminative characteristics. FSPC selects highly correlated features to update target domain prototypes online, denoising and enhancing discriminativeness between categories. WSPC utilizes the correlation coefficients between target domain prototypes and source domain features to weight each point in the source domain, eliminating the information interference from the source domain. In particular, CACP achieves excellent performance on the GTA5 $\to$ Cityscapes and SYNTHIA $\to$ Cityscapes tasks with minimal computational resources and time.},
  archive      = {J_TMM},
  author       = {Yanbing Xue and Xinyu Tian and Feifei Zhang and Xianbin Wen and Zan Gao and Shengyong Chen},
  doi          = {10.1109/TMM.2025.3543016},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {CACP: Covariance-aware cross-domain prototypes for domain adaptive semantic segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Diverse visible-to-thermal image translation via
controllable temperature encoding. <em>TMM</em>, 1–11. (<a
href="https://doi.org/10.1109/TMM.2025.3543053">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Translating readily available visible (VIS) images into thermal infrared (TIR) images effectively alleviates the shortage of TIR data. While current methods have yielded commendable results, they fall short in generating diverse and realistic thermal infrared images, primarily due to insufficient consideration of temperature variations. In this paper, we propose a Thermally Controlled GAN (TC-GAN) that leverages VIS images to generate diverse TIR images, with the ability to control the relative temperatures of multiple objects, particularly those with temperature variations. Firstly, we introduce the physical coding module, which employs a conditional variational autoencoder GAN to learn the distributions of relative temperature information for the objects and environmental state information. Then, the physical information can be obtained by sampling the distribution. When this information is fused with the visible image, it facilitates the generation of diverse TIR images. To ensure authenticity and strengthen the physical constraints across different regions of the image, we introduce a self-attention mechanism in the generator that prioritizes the relative temperature relationships within the image. Additionally, we utilize a local discriminator that focuses on objects with actively changing temperatures and their interactions with the surrounding environment, thereby reducing the discontinuity between the target and the background. Experiments on the Drone Vehicle and AVIID datasets show that our approach outperforms mainstream diversity generation methods in terms of authenticity and diversity.},
  archive      = {J_TMM},
  author       = {Lei Zhao and Mengwei Li and Bo Li and Xingxing Wei},
  doi          = {10.1109/TMM.2025.3543053},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Diverse visible-to-thermal image translation via controllable temperature encoding},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visual class incremental learning with textual priors
guidance based on an adapted vision-language model. <em>TMM</em>, 1–13.
(<a href="https://doi.org/10.1109/TMM.2025.3543109">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An ideal artificial intelligence (AI) system should have the capability to continually learn like humans. However, when learning new knowledge, AI systems often suffer from catastrophic forgetting of old knowledge. Although many continual learning methods have been proposed, they often ignore the issue of misclassifying similar classes and make insufficient use of textual priors of visual classes to improve continual learning performance. In this study, we propose a continual learning framework based on a pre-trained vision-language model (VLM) that does not require storing old class data. This framework utilizes parameter-efficient fine-tuning of the VLM&#39;s text encoder for constructing a shared and consistent semantic textual space throughout the continual learning process. The textual priors of visual classes are encoded by the adapted VLM&#39;s text encoder to generate discriminative semantic representations, which are then used to guide the learning of visual classes. Additionally, fake out-of-distribution (OOD) images constructed from each training image further assist in the learning of visual classes. Extensive empirical evaluations on three natural datasets and one medical dataset demonstrate the superiority of the proposed framework. The source code is available at https://openi.pcl.ac.cn/OpenMedIA/CIL_Adapterd_VLM.},
  archive      = {J_TMM},
  author       = {Wentao Zhang and Tong Yu and Ruixuan Wang and Jianhui Xie and Emanuele Trucco and Wei-Shi Zheng and Xiaobo Yang},
  doi          = {10.1109/TMM.2025.3543109},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Visual class incremental learning with textual priors guidance based on an adapted vision-language model},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Completed interaction networks for pedestrian trajectory
prediction. <em>TMM</em>, 1–11. (<a
href="https://doi.org/10.1109/TMM.2025.3542967">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The social and environmental interactions, as well as the pedestrian goal are crucial for pedestrian trajectory prediction. This is because they could learn both complex interactions in the scenes and the intentions of the pedestrians. However, most existing methods either learn the one-moment social interactions, or supervise the pedestrian trajectories using long-term goal, resulting in suboptimal prediction performances. In this paper, we propose a novel network named Completed Interaction Network (CINet) to simultaneously consider the social interactions in all moments, the environmental interactions and the short-term goal of pedestrians in a unified framework for pedestrian trajectory prediction. Specifically, we propose the Spatio-Temporal Transformer Layer (STTL) to fully mine the spatio-temporal information among historical trajectories of all pedestrians in order to obtain the social interactions in all moments. Additionally, we present the Gradual Goal Module (GGM) to capture the environmental interactions under the supervision of the short-term goal, which is beneficial to understanding the intentions of the pedestrian. Afterwards, we employ the cross-attention to effectively integrate the all-moment social and environmental interactions. The experimental results on three standard pedestrian datasets, i.e., ETH/UCY, SDD and inD demonstrate that our method achieves a new state-of-the-art performance. Furthermore, the visualization results indicate that our method could predict trajectories more reasonably in complex scenarios such as sharp turns, infeasible areas and so on.},
  archive      = {J_TMM},
  author       = {Zhong Zhang and Jianglin Zhou and Shuang Liu and Baihua Xiao},
  doi          = {10.1109/TMM.2025.3542967},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Completed interaction networks for pedestrian trajectory prediction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-modal reference learning for fine-grained
text-to-image retrieval. <em>TMM</em>, 1–14. (<a
href="https://doi.org/10.1109/TMM.2025.3543066">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Fine-grained text-to-image retrieval aims to retrieve a fine-grained target image with a given text query. Existing methods typically assume that each training image is accurately depicted by its textual descriptions. However, textual descriptions can be ambiguous and fail to depict discriminative visual details in images, leading to inaccurate representation learning. To alleviate the effects of text ambiguity, we propose a Multi-Modal Reference learning framework to learn robust representations. We first propose a multi-modal reference construction module to aggregate all visual and textual details of the same object into a comprehensive multi-modal reference. The multi-modal reference hence facilitates the subsequent representation learning and retrieval similarity computation. Specifically, a reference-guided representation learning module is proposed to use multi-modal references to learn more accurate visual and textual representations. Additionally, we introduce a reference-based refinement method that employs the object references to compute a reference-based similarity that refines the initial retrieval results. Extensive experiments are conducted on five fine-grained text-to-image retrieval datasets for different text-to-image retrieval tasks. The proposed method has achieved superior performance over state-of-the-art methods. For instance, on the text-to-person image retrieval dataset RSTPReid, our method achieves the Rank1 accuracy of 56.2%, surpassing the recent CFine by 5.6%.},
  archive      = {J_TMM},
  author       = {Zehong Ma and Hao Chen and Wei Zeng and Limin Su and Shiliang Zhang},
  doi          = {10.1109/TMM.2025.3543066},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Multi-modal reference learning for fine-grained text-to-image retrieval},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Graph contrastive learning for fusion of graph structure and
attribute information. <em>TMM</em>, 1–12. (<a
href="https://doi.org/10.1109/TMM.2025.3542984">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Graph Contrastive Learning (GCL) plays a crucial role in multimedia applications due to its effectiveness in analyzing graph-structured data. Existing GCL methods focus on maximizing the agreement of node representations across different augmentations, which leads to the neglect of unique and complementary information in each augmentation. In this paper, we propose a fusion-based GCL model (FB-GCL) that learns fused representations to effectively capture complementary information from both the graph structure and node attributes. Our model consists of two modules: a graph fusion encoder and a graph contrastive module. The graph fusion encoder adaptively fuses the representations learned from the topology graph and the attribute graph. The graph contrastive module extracts supervision signals from the raw graph by leveraging both the pairwise relationships within the graph structure and the multi-label information from the attributes. Extensive experiments on seven benchmark datasets demonstrate that FB-GCL enhances performance in node classification and link prediction tasks. This improvement is especially valuable for multimedia data analysis, as integrating graph structure and attribute information is crucial for effectively understanding and processing complex datasets.},
  archive      = {J_TMM},
  author       = {Zhuomin Liang and Liang Bai and Xian Yang and Jiye Liang},
  doi          = {10.1109/TMM.2025.3542984},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Graph contrastive learning for fusion of graph structure and attribute information},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AFANet: Adaptive frequency-aware network for
weakly-supervised few-shot semantic segmentation. <em>TMM</em>, 1–11.
(<a href="https://doi.org/10.1109/TMM.2025.3535348">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Few-shot learning aims to recognize novel concepts by leveraging prior knowledge learned from a few samples. However, for visually intensive tasks such as few-shot semantic segmentation, pixel-level annotations are time-consuming and costly. Therefore, in this paper, we utilize the more challenging image-level annotations and propose an adaptive frequency-aware network (AFANet) for weakly-supervised few-shot semantic segmentation (WFSS). Specifically, we first propose a cross-granularity frequency-aware module (CFM) that decouples RGB images into high-frequency and low-frequency distributions and further optimizes semantic structural information by realigning them. Unlike most existing WFSS methods using the textual information from the multi-modal language-vision model, e.g., CLIP, in an offline learning manner, we further propose a CLIP-guided spatial-adapter module (CSM), which performs spatial domain adaptive transformation on textual information through online learning, thus providing enriched cross-modal semantic information for CFM. Extensive experiments on the Pascal-5i and COCO-20i datasets demonstrate that AFANet has achieved state-of-the-art performance.},
  archive      = {J_TMM},
  author       = {Jiaqi Ma and Guo-Sen Xie and Fang Zhao and Zechao Li},
  doi          = {10.1109/TMM.2025.3535348},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {AFANet: Adaptive frequency-aware network for weakly-supervised few-shot semantic segmentation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Learning with noisy low-cost MOS for image quality
assessment via dual-bias calibration. <em>TMM</em>, 1–14. (<a
href="https://doi.org/10.1109/TMM.2025.3543014">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Learning-based Image Quality Assessment (IQA) models have obtained impressive performance with the help of reliable subjective quality labels, where Mean Opinion Score (MOS) is the most popular choice. However, in view of the subjective bias of individual annotators, the Labor-Abundant MOS (LA-MOS) typically requires large collections of opinion scores from multiple annotators for each image, which significantly increases the learning cost. In this paper, we aim to learn robust IQA models from Low-Cost MOS (LC-MOS), which only requires very few opinion scores or even a single opinion score for each image. More specifically, we consider the LC-MOS as the noisy observation of LA-MOS and enforce the IQA model learned from LC-MOS to approach the unbiased estimation of LA-MOS. Thus, we represent the subjective bias between LC-MOS and LA-MOS, and the model bias between IQA predictions learned from LC-MOS and LA-MOS (i.e., dual-bias) as two latent variables with unknown parameters. By means of the expectation-maximization-based alternating optimization, we can jointly estimate the parameters of the dual-bias, which suppresses the misleading of LC-MOS via a gated dual-bias calibration (GDBC) module. To the best of our knowledge, this is the first exploration of robust IQA model learning from noisy low-cost labels. Theoretical analysis and extensive experiments on four popular IQA datasets show that the proposed method is robust toward different bias rates and annotation numbers and significantly outperforms the other Learning-based IQA models when only LC-MOS is available. Furthermore, we also achieve comparable performance with respect to the other models learned with LA-MOS.},
  archive      = {J_TMM},
  author       = {Lei Wang and Qingbo Wu and Desen Yuan and King Ngi Ngan and Hongliang Li and Fanman Meng and Linfeng Xu},
  doi          = {10.1109/TMM.2025.3543014},
  journal      = {IEEE Transactions on Multimedia},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Multimed.},
  title        = {Learning with noisy low-cost MOS for image quality assessment via dual-bias calibration},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
