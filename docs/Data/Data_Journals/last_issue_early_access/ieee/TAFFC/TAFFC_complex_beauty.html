<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TAFFC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="taffc---14">TAFFC - 14</h2>
<ul>
<li><details>
<summary>
(2025). RVISA: Reasoning and verification for implicit sentiment
analysis. <em>IEEE Transactions on Affective Computing</em>, 1–12. (<a
href="https://doi.org/10.1109/TAFFC.2025.3537799">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Under the context of the increasing social demand for fine-grained sentiment analysis (SA), implicit sentiment analysis (ISA) poses a significant challenge owing to the absence of salient cue words in expressions. Thus, reliable reasoning is required to understand how sentiment is evoked, enabling the identification of implicit sentiments. In the era of large language models (LLMs), encoder-decoder (ED) LLMs have emerged as popular backbone models for SA applications, given their impressive text comprehension and reasoning capabilities across diverse tasks. In comparison, decoder-only (DO) LLMs exhibit superior natural language generation and in-context learning capabilities. However, their responses may contain misleading or inaccurate information. To accurately identify implicit sentiments with reliable reasoning, this study introduces a two-stage reasoning framework named Reasoning and Verification for Implicit Sentiment Analysis (RVISA), which leverages the generation ability of DO LLMs and reasoning ability of ED LLMs to train an enhanced reasoner. The framework involves three-hop reasoning prompting to explicitly furnish sentiment elements as cues. The generated rationales are then used to fine-tune an ED LLM into a skilled reasoner. Additionally, we develop a straightforward yet effective answer-based verification mechanism to ensure the reliability of reasoning learning. Evaluation of the proposed method on two benchmark datasets demonstrates that it achieves state-of-the-art performance in ISA.},
  archive  = {J},
  author   = {Wenna Lai and Haoran Xie and Guandong Xu and Qing Li},
  doi      = {10.1109/TAFFC.2025.3537799},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {2},
  pages    = {1-12},
  title    = {RVISA: Reasoning and verification for implicit sentiment analysis},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Decoupled multi-perspective fusion for speech depression
detection. <em>IEEE Transactions on Affective Computing</em>, 1–15. (<a
href="https://doi.org/10.1109/TAFFC.2025.3538519">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Speech Depression Detection (SDD) has garnered attention from researchers due to its low cost and convenience. However, current algorithms lack methods for extracting interpretable acoustic features based on clinical manifestations. In addition, effectively fusing these features to overcome individual heterogeneity remains a challenge. This study proposes a decoupled multi-perspective fusion (DMPF) model. The model extracts five key features of voiceprint, emotion, pause, energy, and tremor based on the multi-perspective clinical manifestations. These features are then decoupled into common and private features, which fused through graph attention network to obtain the comprehensive depression representation. Notably, this study has collected a depression speech dataset, which includes standardized and comprehensive tasks along with diagnostic labels provided by psychologists. Extensive subject-independent experiments were conducted on the DAIC-WOZ, MODMA and MPSC datasets. The voiceprint features can automatically cluster the depressed and non-depressed populations. Furthermore, DMPF can effectively fuse common and private features from different perspectives, achieving AUC of 84.20%, 85.34%, 86.13% on three datasets. The results illustrate the interpretability of multi-perspective features and demonstrate that the combination of speech manifestations can enhance the detection ability, which can provide a multi-perspective observational tool for physicians and clinical practice. Code is available at https://github.com/zmh56/SDD-for-DMPF-MPSC.},
  archive  = {J},
  author   = {Minghui Zhao and Hongxiang Gao and Lulu Zhao and Zhongyu Wang and Fei Wang and Wenming Zheng and Jianqing Li and Chengyu Liu},
  doi      = {10.1109/TAFFC.2025.3538519},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {2},
  pages    = {1-15},
  title    = {Decoupled multi-perspective fusion for speech depression detection},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Assessment of distraction and the impact on technology
acceptance of robot monitoring behaviour in older adults care. <em>IEEE
Transactions on Affective Computing</em>, 1–14. (<a
href="https://doi.org/10.1109/TAFFC.2025.3539015">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {People&#39;s successful coexistence with robots strictly depends on people&#39;s acceptance of robots&#39; presence in their daily activities. This is particularly relevant when the robot&#39;s actions may interfere with or intrude on people&#39;s activities, creating discomfort and possible rejection. We believe that people&#39;s acceptance of a robot may vary depending on the activities they are involved in. In this study, we investigate the impact of a robot&#39;s actions on people&#39;s engagement in an activity while the robot has the task of monitoring them. We observed the behaviours of 18 older adults with respect to the robot while they were carrying out tasks that require different cognitive workloads (e.g., working at the PC, talking on the phone). We used subjective and objective metrics, such as social cues, to evaluate people&#39;s engagement in the robot and their disengagement in their own tasks. We observed that people were distracted by the robot&#39;s behaviours based on the cognitive loads required by their activity. Our results show that variation in people&#39;s engagement in the robot and the task is affected by their perception of the usefulness of and trust in the robot, and by individuals&#39; personality traits and acceptance of the robot. People with higher trust in the robot, and a higher degree of conscientiousness and emotional stability, tend to continue with their task, paying less attention to the robot. We observed, in contrast, that a robot perceived as a social entity caught more easily their attention when people have a higher extroverted personality. Our findings also showed that variations in the affective and emotional demeanour of the participants are a predictor of their distraction to an external observer.},
  archive  = {J},
  author   = {Gianpaolo Maggi and Luca Raggioli and Alessandra Rossi and Silvia Rossi},
  doi      = {10.1109/TAFFC.2025.3539015},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {2},
  pages    = {1-14},
  title    = {Assessment of distraction and the impact on technology acceptance of robot monitoring behaviour in older adults care},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SDRS: Sentiment-aware disentangled representation shifting
for multimodal sentiment analysis. <em>IEEE Transactions on Affective
Computing</em>, 1–13. (<a
href="https://doi.org/10.1109/TAFFC.2025.3539225">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multimodal sentiment analysis (MSA) aims to leverage the complementary information from multiple modalities for affective understanding of user-generated videos. Existing methods mainly focused on designing sophisticated feature fusion strategies to integrate the separately extracted multimodal representations, ignoring the interference of the information irrelevant to sentiment. In this paper, we propose to disentangle the unimodal representations into sentiment-specific and sentiment-independent features, the former of which are fused for the MSA task. Specifically, we design a novel Sentiment-aware Disentangled Representation Shifting framework, termed SDRS, with two components. Interactive sentiment-aware representation disentanglement aims to extract sentiment-specific feature representations for each nonverbal modality by considering the contextual influence of other modalities with the newly developed cross-attention autoencoder. Attentive cross-modal representation shifting tries to shift the textual representation in a latent token space using the nonverbal sentiment-specific representations after projection. The shifted representation is finally employed to fine-tune a pre-trained language model for multimodal sentiment analysis. Extensive experiments are conducted on three public benchmark datasets, i.e., CMU-MOSI, CMU-MOSEI, and CH-SIMS. The results demonstrate that the proposed SDRS framework not only obtains state-of-the-art results based solely on multimodal labels but also outperforms the methods that additionally require the labels of each modality.},
  archive  = {J},
  author   = {Sicheng Zhao and Zhenhua Yang and Henglin Shi and Xiaocheng Feng and Lingpengkun Meng and Bing Qin and Chenggang Yan and Jianhua Tao and Guiguang Ding},
  doi      = {10.1109/TAFFC.2025.3539225},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {2},
  pages    = {1-13},
  title    = {SDRS: Sentiment-aware disentangled representation shifting for multimodal sentiment analysis},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Analyzing the visual road scene for driver stress
estimation. <em>IEEE Transactions on Affective Computing</em>, 1–16. (<a
href="https://doi.org/10.1109/TAFFC.2025.3539003">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {This paper studies the contribution of the visual road scene to estimate the driver-reported stress levels. Our research leverages on previous work showing that environmental factors, such as traffic congestion, weather conditions, and driving context, impact driver&#39;s stress. Each of the models we evaluated is trained and tested with the publicly available AffectiveROAD dataset to estimate three categories of driver-reported stress level. We test three types of modelling approaches: (i) single-frame baselines (Random Forest, SVM, and Convolutional Neural Networks); (ii) Temporal Segment Networks (TSN) and two variants of it, which use learned weights (TSN-w) and LSTM (TSN-LSTM) as consensus functions; and (iii) video classification Transformers. Our experiments reveal that the TSN-w, TSN-LSTM, and Transformer models achieve statistically equivalent performances, all significantly outperforming the other models. Particularly noteworthy is TSN-w, which attains the highest performance observed with an average accuracy of 0.77. We further provide an explainability analysis using Class Activation Mapping and image semantic segmentation to identify the elements of the road scene that contribute the most to high levels of stress. Our results demonstrate that the visible road scene offers significant contextual information for estimating driver-reported stress levels, with potential implications for the design of safer urban road environments.},
  archive  = {J},
  author   = {Cristina Bustos and Albert Sole-Ribalta and Neska Elhaouij and Javier Borge-Holthoefer and Agata Lapedriza and Rosalind Picard},
  doi      = {10.1109/TAFFC.2025.3539003},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {2},
  pages    = {1-16},
  title    = {Analyzing the visual road scene for driver stress estimation},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Text-guided reconstruction network for sentiment analysis
with uncertain missing modalities. <em>IEEE Transactions on Affective
Computing</em>, 1–15. (<a
href="https://doi.org/10.1109/TAFFC.2025.3541743">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Multimodal Sentiment Analysis (MSA) is an attractive research that aims to integrate sentiment expressed in textual, visual, and acoustic signals. There are two main problems in the existing methods: 1) the dominant role of the text is underutilization in unaligned multimodal data, and 2) the modality under uncertain missing feature is not sufficiently explored. This paper proposes a Text-guided Reconstruction Network (TgRN) for MSA with uncertain missing modalities in non-aligned sequences. The TgRN network includes three primary modules: Text-guided Extraction Module (TEM), Reconstruction Module (RM) and Text-guided Fusion Module (TFM). First, the TEM consists of the text-guided cross attention units and self-attention units to capture inter-modal features and intra-modal features, respectively. Second, leveraging enhanced attention units and a three-way squeeze-and-excitation block, the RM is designed to learn semantic information from incomplete data and reconstruct missing modality features. Third, the TFM utilizes a progressive modality-mixing adaptation gate to explore the dynamic correlations between nonverbal and verbal modalities, effectively addressing the modality gap issue. Finally, under the supervision of sentiment prediction loss and reconstruction loss, the TgRN effectively processes both uncertain missing-modality conditions and ideal complete modality conditions. Extensive experiments on CMU-MOSI and CH-SIMS demonstrate that our proposed method outperforms state-of-the-art approaches.},
  archive  = {J},
  author   = {Piao Shi and Min Hu and Satoshi Nakagawa and Xiangming Zheng and Xuefeng Shi and Fuji Ren},
  doi      = {10.1109/TAFFC.2025.3541743},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {2},
  pages    = {1-15},
  title    = {Text-guided reconstruction network for sentiment analysis with uncertain missing modalities},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SigWavNet: Learning multiresolution signal wavelet network
for speech emotion recognition. <em>IEEE Transactions on Affective
Computing</em>, 1–16. (<a
href="https://doi.org/10.1109/TAFFC.2025.3537991">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {In the field of human-computer interaction and psychological assessment, speech emotion recognition (SER) plays an important role in deciphering emotional states from speech signals. Despite advancements, challenges persist due to system complexity, feature distinctiveness issues, and noise interference. This paper introduces a new end-to-end (E2E) deep learning multi-resolution framework for SER, addressing these limitations by extracting meaningful representations directly from raw waveform speech signals. By leveraging the properties of the fast discrete wavelet transform (FDWT), including the cascade algorithm, conjugate quadrature filter, and coefficient denoising, our approach introduces a learnable model for both wavelet bases and denoising through deep learning techniques. The framework incorporates an activation function for learnable asymmetric hard thresholding of wavelet coefficients. Our approach exploits the capabilities of wavelets for effective localization in both time and frequency domains. We then combine one-dimensional dilated convolutional neural networks (1D dilated CNN) with a spatial attention layer and bidirectional gated recurrent units (Bi-GRU) with a temporal attention layer to efficiently capture the nuanced spatial and temporal characteristics of emotional features. By handling variable-length speech without segmentation and eliminating the need for pre or post-processing, the proposed model outperformed state-of-the-art methods on IEMOCAP and EMO-DB datasets. The source code of this paper is shared on the Github repository: https://github.com/alaaNfissi/SigWavNet-Learning-Multiresolution-Signal-Wavelet-Network-for-Speech-Emotion-Recognition.},
  archive  = {J},
  author   = {Alaa Nfissi and Wassim Bouachir and Nizar Bouguila and Brian Mishara},
  doi      = {10.1109/TAFFC.2025.3537991},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {2},
  pages    = {1-16},
  title    = {SigWavNet: Learning multiresolution signal wavelet network for speech emotion recognition},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Conformal depression prediction. <em>IEEE Transactions on
Affective Computing</em>, 1–11. (<a
href="https://doi.org/10.1109/TAFFC.2025.3542023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {While existing depression prediction methods based on deep learning show promise, their practical application is hindered by the lack of trustworthiness, as these deep models are often deployed as black box models, leaving us uncertain on the confidence of their predictions. For high-risk clinical applications like depression prediction, uncertainty quantification is essential in decision-making. In this paper, we introduce conformal depression prediction (CDP), a depression prediction method with uncertainty quantification based on conformal prediction (CP), giving valid confidence intervals with theoretical coverage guarantees for the model predictions. CDP is a plug-and-play module that requires neither model retraining nor an assumption about the depression data distribution. As CDP provides only an average coverage guarantee across all inputs rather than per-input performance guarantee, we further propose CDP-ACC, an improved conformal prediction with approximate conditional coverage. CDP-ACC firstly estimates the prediction distribution through neighborhood relaxation, and then introduces a conformal score function by constructing nested sequences, so as to provide a tighter prediction interval adaptive to specific input. We empirically demonstrate the application of CDP in uncertainty-aware facial depression prediction, as well as the effectiveness and superiority of CDP-ACC on the AVEC 2013 and AVEC 2014 datasets. Our code is publicly available at https://github.com/PushineLee/CDP.},
  archive  = {J},
  author   = {Yonghong Li and Shan Qu and Xiuzhuang Zhou},
  doi      = {10.1109/TAFFC.2025.3542023},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {2},
  pages    = {1-11},
  title    = {Conformal depression prediction},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Automatic depression recognition with an ensemble of
multimodal spatio-temporal routing features. <em>IEEE Transactions on
Affective Computing</em>, 1–18. (<a
href="https://doi.org/10.1109/TAFFC.2025.3543226">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Depression, driven by growing societal pressures, significantly disrupts individuals&#39; physical and mental health. Automatic Depression Recognition (ADR) via facial videos has gained attention to enhance diagnostic accuracy and efficiency. However, extant methods often segment videos, losing long-term behavioral cues and introducing noise, while also exhibiting performance drops across diverse cultural and racial datasets. This study proposes a multimodal ADR approach encompassing three key components: (1) Long-term Depression Behavior Module (LDBM) employing a Transformer to capture extended depression cues, (2) Noisy Information Elimination (NIE) strategy leveraging LDBM attention scores to reduce noise and boost diagnostic precision, and (3) Multimodal Spatio-temporal Routing Feature Ensemble (MSRE) that fuses texture, Facial Action Primitives (FAPs), and Remote Photoplethysmography (rPPG) data for improved cross-dataset generalizability. Experiments on AVEC 2013, AVEC 2014, and a newly constructed CMDep dataset of 123 clinically diagnosed participants validate our method, achieving MAE/RMSE scores of 5.38/6.74, 5.09/6.83, and 5.59/8.03, respectively. The CMDep dataset includes facial expression and voice signals, with labels derived from BDI-II scores. Additionally, our method has been integrated into a user-friendly mobile application, providing a tool for real-time self-assessment of depression. This integration broadens the scope of depression detection, making it accessible to diverse populations worldwide. The code is publicly available at https://github.com/Amcky/Multimodal-Depression.},
  archive  = {J},
  author   = {Yaowei Wang and Zulong Lin and Chengrong Yang and Yujue Zhou and Yun Yang},
  doi      = {10.1109/TAFFC.2025.3543226},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {2},
  pages    = {1-18},
  title    = {Automatic depression recognition with an ensemble of multimodal spatio-temporal routing features},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Empathy level alignment via reinforcement learning for
empathetic response generation. <em>IEEE Transactions on Affective
Computing</em>, 1–12. (<a
href="https://doi.org/10.1109/TAFFC.2025.3544594">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Empathetic response generation, aiming to understand the user&#39;s situation and feelings and respond empathically, is crucial in building human- like dialogue systems. Traditional approaches typically employ maximum likelihood estimation as the optimization objective during training, yet fail to align the empathy levels between generated and target responses. To this end, we propose an empathetic response generation framework using reinforcement learning (EmpRL). The framework develops an effective empathy reward function and generates empathetic responses by maximizing the expected reward through reinforcement learning. EmpRL utilizes the pre-trained T5 model as the generator and further fine-tunes it to initialize the policy. To align the empathy levels between generated and target responses within a given context, an empathy reward function containing three empathy communication mechanisms—emotional reaction, interpretation, and exploration—is constructed using pre-designed and pre-trained empathy identifiers. During reinforcement learning training, the proximal policy optimization algorithm is used to fine-tune the policy, enabling the generation of empathetic responses. Both automatic and human evaluations demonstrate that the proposed EmpRL framework significantly improves the quality of generated responses, enhances the similarity in empathy levels between generated and target responses, and produces empathetic responses covering both affective and cognitive aspects.},
  archive  = {J},
  author   = {Hui Ma and Bo Zhang and Bo Xu and Jian Wang and Hongfei Lin and Xiao Sun},
  doi      = {10.1109/TAFFC.2025.3544594},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {2},
  pages    = {1-12},
  title    = {Empathy level alignment via reinforcement learning for empathetic response generation},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Semantic and emotional dual channel for emotion recognition
in conversation. <em>IEEE Transactions on Affective Computing</em>,
1–18. (<a href="https://doi.org/10.1109/TAFFC.2025.3544608">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Emotion recognition in conversation (ERC) aims at accurately identifying emotional states expressed in conversational content. Existing ERC methods, although relying on semantic understanding, often encounter challenges when confronted with incomplete or misleading semantic information. In addition, when dealing with the interaction between emotional and semantic information, existing methods are often difficult to effectively distinguish the complex relationship between the two, which affects the accuracy of emotion recognition. To address the problems of semantic misdirection and emotional cross-talk encountered by traditional models when confronted with complex conversational data, we propose a semantic and emotional dual channel (SEDC) strategy for emotion recognition in conversations to process emotional and semantic information independently. Under this strategy, emotion information provides an auxiliary recognition function when the semantics are unclear or lacking, enhancing the accuracy of the model. Our model consists of two modules: the emotion processing module accurately captures the emotional features of each utterance through contrastive learning, and then constructs a dialogue emotion propagation map to simulate the emotional information conveyed in the dialogue; the semantic processing module combines an external knowledge base to enhance the semantic expression of the dialogue through knowledge enhancement strategies. This divide-and-conquer approach allows us to more deeply analyze the emotional and semantic dimensions of complex dialogues. Experimental results on the IEMOCAP, EmoryNLP, MELD, and DailyDialog datasets show that our approach significantly outperforms existing techniques and effectively improves the accuracy of dialogue emotion recognition. Our code is available at https://anonymous.4open.science/r/SEDC-FCF1.},
  archive  = {J},
  author   = {Zhenyu Yang and Zhibo Zhang and Yuhu Cheng and Tong Zhang and Xuesong Wang},
  doi      = {10.1109/TAFFC.2025.3544608},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {2},
  pages    = {1-18},
  title    = {Semantic and emotional dual channel for emotion recognition in conversation},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Progressive masking oriented self-taught learning for
occluded facial expression recognition. <em>IEEE Transactions on
Affective Computing</em>, 1–14. (<a
href="https://doi.org/10.1109/TAFFC.2025.3544677">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Self-taught learning (STL) is a promising solution that reduces the performance gap between weakly supervised and fully supervised learning for easily accessible, label-free images. The success of traditional STL solutions relies on the assumption that the target appearance is completely visible and well-defined. In real-world facial expression recognition scenarios, however, saliency regions are often partially occluded, which significantly hampers the generalization capability of STL methods. Nevertheless, few studies have investigated the impact of occlusion on STL. In this paper, we propose an interweaved autoencoder network for weakly supervised facial expression recognition in occlusion scenarios. The key innovation of our network lies in the Residual Connection Union (RCU) blocks that can integrate the Convolutional Neural Network (CNN) and Transformer layers into a multi-scale structure. The RCU enables a progressive masking strategy to accurately identify and focus on contributive yet often overlooked image patches by analyzing the relationships among region-level target representations. In addition, we introduce a self-knowledge distillation module for the effective training of the proposed autoencoder network. Extensive experiments are conducted on four public datasets to demonstrate the superiority of our method over related works.},
  archive  = {J},
  author   = {Bin Kang and Shuangshuang Wang and Zongyu Wang and Xin Li and Haie Dou and Lei Wang and Zhijie Xia},
  doi      = {10.1109/TAFFC.2025.3544677},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {2},
  pages    = {1-14},
  title    = {Progressive masking oriented self-taught learning for occluded facial expression recognition},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring the relationship between stress-physiology and
pain in the daily life of patients with chronic widespread pain.
<em>IEEE Transactions on Affective Computing</em>, 1–14. (<a
href="https://doi.org/10.1109/TAFFC.2025.3545477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Chronic widespread pain remains a complex and incompletely understood condition. To complement existing pain assessment strategies, this study explored the ecological validity of unobtrusively captured daily life physiological signals as indicators of pain. Therefore, we collected physiological data using a wearable wristband from 46 patients with chronic widespread pain for seven days. Linear mixed-effect models revealed several significant associations between physiological signals, such as mean heart rate and momentary pain intensity. However, making individual pain predictions with multivariate machine learning models did not add value. While this study underscores the potential of ambulatory physiology for pain assessment, future research should validate and expand upon these initial findings to further enhance pain management strategies.},
  archive  = {J},
  author   = {Emilie Pattyn and Nattapong Thammasan and Hannah Davidoff and Walter De Raedt and Gudrun Vera Eisele and Ruud Van Stiphout and Maarten De Vos and Olivia J. Kirtley and Peter Van Wambeke and Bart Morlion and Elfi Vergaelen and Chris Van Hoof},
  doi      = {10.1109/TAFFC.2025.3545477},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {2},
  pages    = {1-14},
  title    = {Exploring the relationship between stress-physiology and pain in the daily life of patients with chronic widespread pain},
  year     = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Beyond overfitting: Doubly adaptive dropout for
generalizable AU detection. <em>IEEE Transactions on Affective
Computing</em>, 1–14. (<a
href="https://doi.org/10.1109/TAFFC.2025.3545915">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract = {Facial Action Units (AUs) are essential for conveying psychological states and emotional expressions. While automatic AU detection systems leveraging deep learning have progressed, they often overfit to specific datasets and individual features, limiting their cross-domain applicability. To overcome these limitations, we propose a doubly adaptive dropout approach for cross-domain AU detection, which enhances the robustness of convolutional feature maps and spatial tokens against domain shifts. This approach includes a Channel Drop Unit (CD-Unit) and a Token Drop Unit (TD-Unit), which work together to reduce domain-specific noise at both the channel and token levels. The CD-Unit preserves domain-agnostic local patterns in feature maps, while the TD-Unit helps the model identify AU relationships generalizable across domains. An auxiliary domain classifier, integrated at each layer, guides the selective omission of domain-sensitive features. To prevent excessive feature dropout, a progressive training strategy is used, allowing for selective exclusion of sensitive features at any model layer. Our method consistently outperforms existing techniques in cross-domain AU detection, as demonstrated by extensive experimental evaluations. Visualizations of attention maps also highlight clear and meaningful patterns related to both individual and combined AUs, further validating the approach&#39;s effectiveness.},
  archive  = {J},
  author   = {Yong Li and Yi Ren and Xuesong Niu and Yi Ding and Xiu-Shen Wei and Cuntai Guan},
  doi      = {10.1109/TAFFC.2025.3545915},
  journal  = {IEEE Transactions on Affective Computing},
  month    = {2},
  pages    = {1-14},
  title    = {Beyond overfitting: Doubly adaptive dropout for generalizable AU detection},
  year     = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
