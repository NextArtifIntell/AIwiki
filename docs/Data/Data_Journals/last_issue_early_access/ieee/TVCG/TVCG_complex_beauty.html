<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TVCG_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tvcg---36">TVCG - 36</h2>
<ul>
<li><details>
<summary>
(2025). DashSpace: A live collaborative platform for immersive and
ubiquitous analytics. <em>TVCG</em>, 1–13. (<a
href="https://doi.org/10.1109/TVCG.2025.3537679">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We introduce DashSpace, a live collaborative immersive and ubiquitous analytics (IA/UA) platform designed for handheld and head-mounted Augmented/Extended Reality (AR/XR) implemented using WebXR and open standards. To bridge the gap between existing web-based visualizations and the immersive analytics setting, DashSpace supports visualizing both legacy D3 and Vega-Lite visualizations on 2D planes, and extruding Vega-Lite specifications into 2.5D. It also supports fully 3D visual representations using the Optomancy grammar. To facilitate authoring new visualizations in immersive XR, the platform provides a visual authoring mechanism where the user groups specification snippets to construct visualizations dynamically. The approach is fully persistent and collaborative, allowing multiple participants—whose presence is shown using 3D avatars and webcam feeds—to interact with the shared space synchronously, both co-located and remotely. We present three examples of DashSpace in action: immersive data analysis in 3D space, synchronous collaboration, and immersive data presentations.},
  archive      = {J_TVCG},
  author       = {Marcel Borowski and Peter W. S. Butcher and Janus Bager Kristensen and Jonas Oxenbøll Petersen and Panagiotis D. Ritsos and Clemens N. Klokmose and Niklas Elmqvist},
  doi          = {10.1109/TVCG.2025.3537679},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {DashSpace: A live collaborative platform for immersive and ubiquitous analytics},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Fast and readable layered network visualizations using large
neighborhood search. <em>TVCG</em>, 1–14. (<a
href="https://doi.org/10.1109/TVCG.2025.3537898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Layered network visualizations assign each node to one of several parallel axes. They can convey sequence or flow data, hierarchies, or multiple data classes, but edge crossings and long edges often impair readability. Layout algorithms can reduce edge crossings and shorten edges using quick heuristics or optimal methods that prioritize human readability over computation speed. This work uses an optimization metaheuristic to provide the best of both worlds: high-quality layouts within a predetermined execution time. Our adaptation of the large neighborhood search (LNS) metaheuristic repeatedly selects fixed-sized subgraphs to lay out optimally. We conducted a computational evaluation using 450 synthetic networks to compare five ways of selecting candidate nodes, four ways of selecting their neighboring subgraph, and three criteria for determining subgraph size. LNS generally halved the number of crossings vs. the barycentric heuristic while maintaining a reasonable runtime. Our best approach randomly selected candidate nodes, used degree centrality to pick cluster-like neighborhoods, and chose smaller neighborhoods that could be optimally laid out in 0.6 or 1.2 seconds (vs. 6 seconds). In a case study visualizing 13 control flow graphs, most with over 1000 nodes, we show that our method can be employed to create visualizations with fewer crossings than Tabu Search, another metaheuristic, and vastly outperforms an ILP solver when runtime is bounded. A free copy of this paper and all supplemental materials are available at https://osf.io/w3fev/.},
  archive      = {J_TVCG},
  author       = {Connor Wilson and Tarik Crnovrsanin and Eduardo Puerta and Cody Dunne},
  doi          = {10.1109/TVCG.2025.3537898},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Fast and readable layered network visualizations using large neighborhood search},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Complex surface fabrication via developable surface
approximation: A survey. <em>TVCG</em>, 1–20. (<a
href="https://doi.org/10.1109/TVCG.2025.3538782">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Complex surfaces are commonly observed in various applications and have significant value in enhancing comfort, aesthetics, and functionality. However, their fabrication often involves complex and costly processes. To simplify the fabrication difficulty, significant research has focused on using 3D developable surfaces to approximate target 3D surfaces. This process involves converting target 3D surfaces into developable surfaces and then flattening them into 2D patterns. Since the geometric and topological diversity of target surfaces, this task is both comprehensive and intricate, encompassing multiple aspects from design to fabrication. In this paper, we review relevant technologies and methods in fabrication processes, classify them, and summarize a pipeline from design to fabrication. This provides a comprehensive introduction to the field for researchers and practitioners. Through the analysis of relevant literature, we also discuss some of the research challenges and future research opportunities.},
  archive      = {J_TVCG},
  author       = {Chao Yuan and Nan Cao and Yang Shi},
  doi          = {10.1109/TVCG.2025.3538782},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Complex surface fabrication via developable surface approximation: A survey},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Can people’s brains synchronize during remote AR
collaboration? <em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3538509">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent studies have demonstrated that brain synchrony can indicate the quality of social interaction in real-world communication. However, there is a lack of research on measurement of brain synchrony during social interactions in remote AR. In this study, we investigated the brain synchrony of remote augmented reality (AR; Study 1) and face-to-face (FTF; Study 2) interactions. Functional near-infrared spectroscopy was used to measure the brain synchrony during the tangram puzzle task. In a collaboration condition, participants worked together to solve the puzzle. In an individual condition, participants solved the puzzle independently. We recruited 46 participants in Study 1 and 48 participants in Study 2. Study 1 showed there was a significant difference in brain synchrony between the individual and collaboration conditions, and a positive correlation was observed between brain synchrony and the task performance in the collaboration condition. A comparison between Study 1 and 2 suggested that the difference between the collaboration and individual conditions was maintained, and some differences were observed in the brain synchrony between the AR and FTF interactions. These results suggest that measurement of brain synchrony is beneficial for social interaction in remote AR collaborations. The implications of these results on future remote interactions are discussed.},
  archive      = {J_TVCG},
  author       = {Jaehwan You and Myeongul Jung and Kwanguk Kim},
  doi          = {10.1109/TVCG.2025.3538509},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Can people&#39;s brains synchronize during remote AR collaboration?},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring spatial hybrid user interface for visual
sensemaking. <em>TVCG</em>, 1–16. (<a
href="https://doi.org/10.1109/TVCG.2025.3538771">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We built a spatial hybrid system that combines a personal computer (PC) and virtual reality (VR) for visual sensemaking, addressing limitations in both environments. Although VR offers immense potential for interactive data visualization (e.g., large display space and spatial navigation), it can also present challenges such as imprecise interactions and user fatigue. At the same time, a PC offers precise and familiar interactions but has limited display space and interaction modality. Therefore, we iteratively designed a spatial hybrid system (PC+VR) to complement these two environments by enabling seamless switching between PC and VR environments. To evaluate the system&#39;s effectiveness and user experience, we compared it to using a single computing environment (i.e., PC-only and VR-only). Our study results (N=18) showed that spatial PC+VR could combine the benefits of both devices to outperform user preference for VR-only without a negative impact on performance from device switching overhead. Finally, we discussed future design implications.},
  archive      = {J_TVCG},
  author       = {Wai Tong and Haobo Li and Meng Xia and Wong Kam-Kwai and Ting-Chuen Pong and Huamin Qu and Yalong Yang},
  doi          = {10.1109/TVCG.2025.3538771},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Exploring spatial hybrid user interface for visual sensemaking},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). From requirement to solution: Unveiling problem-driven
design patterns in visual analytics. <em>TVCG</em>, 1–18. (<a
href="https://doi.org/10.1109/TVCG.2025.3538768">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Visual Analytics (VA) researchers frequently collaborate closely with domain experts to derive requirements and select appropriate solutions to fulfill these requirements. Despite strides made in exploring requirement and solution spaces, challenges persist due to the absence of guidance in the initial consideration space and the lack of shared problem-solving knowledge, often resulting in suboptimal solutions. To address these issues, we conducted an empirical study of VA research, with a focus on mapping the relations between requirement and solution spaces. Analyzing 220 VA papers, we formulate refined topologies for data, requirements, and solutions. We propose conceptualizing the connections between requirements, data, and solutions through knowledge graphs and utilizing solution paths to encapsulate fundamental problem-solving knowledge in visual analytics research. Through the integration of solution paths into a graph and analyzing their interconnections, we identified a subset of problem-driven design patterns that demonstrated the efficacy of our approach. By externalizing problem-solving knowledge and formulating problem-driven design patterns, our aim is to streamline the exploration of consideration space, facilitating the inclusion of “good” solutions, and establish a benchmark for shared design decisions among researchers and readers.},
  archive      = {J_TVCG},
  author       = {Yuchen Wu and Shenghan Gao and Shizhen Zhang and Xiaofeng Dou and Xingbo Wang and Quan Li},
  doi          = {10.1109/TVCG.2025.3538768},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {From requirement to solution: Unveiling problem-driven design patterns in visual analytics},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Floor plan restoration: A multimodal method under one
second. <em>TVCG</em>, 1–13. (<a
href="https://doi.org/10.1109/TVCG.2025.3539497">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Floor plan restoration aims to recover vector and semantic information from raster floor plan images, which is significant for advanced applications including interior design, interative walkthroughs, and layout planning. Existing methods generally adopt a two-stage paradigm: a parsing stage to extract semantic elements such as rooms, walls, doors, and windows from raster images; and then a vectorization stage to convert them into vector graphics. However, these methods are deficient in both accuracy and efficiency due to the neglect of the unique cues of floor plans compared to natural images. To address the above issues, we propose MMParseNet that yields accurate parsing results by incorporating multimodal cues unique to floor plans, such as room names, furniture icons, and room boundaries. Moreover, we implement an efficiency-optimized vectorization method based on PCA that avoids unnecessary iterative solutions. We conduct both quantitative and qualitative experiments on three public and one self-built dataset. The results exhibit consistent improvements in accuracy and sub-second overall restoration time across various datasets.},
  archive      = {J_TVCG},
  author       = {Tao Wen and You-Ming Fu and Chun-Xia Xiao and Hai-Ming Xiang and Chao Liang},
  doi          = {10.1109/TVCG.2025.3539497},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Floor plan restoration: A multimodal method under one second},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visual acuity consistent foveated rendering towards retinal
resolution. <em>TVCG</em>, 1–14. (<a
href="https://doi.org/10.1109/TVCG.2025.3539851">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Prior foveated rendering methods often suffer from a limitation where the shading load escalates with increasing display resolution, leading to decreased efficiency, particularly when dealing with retinal-level resolutions. To tackle this challenge, we begin with the essence of the human visual system (HVS) perception and present visual acuity-consistent foveated rendering (VaFR), aiming to achieve exceptional rendering performance at retinal-level resolutions. Specifically, we propose a method with a novel log-polar mapping function derived from the human visual acuity model, which accommodates the natural bandwidth of the visual system. This mapping function and its associated shading rate guarantee a consistent output of rendering information, regardless of variations in the display resolution of the VR HMD. Consequently, our VaFR outperforms alternative methods, improving rendering speed while preserving perceptual visual quality, particularly when operating at retinal resolutions. We validate our approach using both the rasterization and ray-casting rendering pipelines. We also validate our approach using different binocular rendering strategies for HMD devices. In diverse testing scenarios, our approach delivers better perceptual visual quality than prior foveated rendering while achieving an impressive speedup of 6.5×-9.29× for deferred rendering of 3D scenarios and an even more powerful speedup of 10.4×-16.4× for ray-casting at retinal resolution. Additionally, our approach significantly enhances the rendering performance of binocular 8K path tracing, achieving smooth frame rates.},
  archive      = {J_TVCG},
  author       = {Zhi Zhang and Meng Gai and Sheng Li},
  doi          = {10.1109/TVCG.2025.3539851},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visual acuity consistent foveated rendering towards retinal resolution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MAICO: A visualization design study on AI-assisted music
composition. <em>TVCG</em>, 1–16. (<a
href="https://doi.org/10.1109/TVCG.2025.3539779">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We contribute a design study on using visual analytics for AI-assisted music composition. The main result is the interface MAICO (Music AI Co-creativity), which allows composers and other music creators to interactively generate, explore, select, edit, and compare samples from generative music models. MAICO is based on the idea of visual parameter space analysis and supports the simultaneous analysis of hundreds of short samples of symbolic music from multiple models, displaying them in different metric- and similarity-based layouts. We developed and evaluated MAICO together with a professional composer who actively used it for five months to create, among other things, a composition for the Biennale Arte 2024 in Venice, which was recorded by the Munich Symphonic Orchestra. We discuss our design choices and lessons learned from this endeavor to support Human-AI co-creativity with visual analytics.},
  archive      = {J_TVCG},
  author       = {Simeon Rau and Frank Heyen and Benedikt Brachtel and Michael Sedlmair},
  doi          = {10.1109/TVCG.2025.3539779},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MAICO: A visualization design study on AI-assisted music composition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Bridging network science and vision science: Mapping
perceptual mechanisms to network visualization tasks. <em>TVCG</em>,
1–16. (<a href="https://doi.org/10.1109/TVCG.2025.3541571">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Network visualizations are understudied in graphical perception. As a result, most network visualization designs still largely rely on designer intuition and algorithm optimizations rather than being guided by knowledge of human perception. The lack of perceptual understanding of network visualizations also limits the generalizability of past empirical evaluations, given their focus on performance over causal interpretation. To bridge this gap between perception and network visualization, we introduce a framework highlighting five key perceptual mechanisms used in node-link diagrams and adjacency matrices: attention, visual search, perceptual organization, ensemble coding, and object recognition. Our framework describes the role these perceptual mechanisms play in common network analytical tasks. We use the framework to revisit four past empirical investigations and outline future design experiments that can help produce more perceptually effective network visualizations. We anticipate this connection will afford translational understanding to guide more effective network visualization design and offer hypotheses for perception-aware network visualizations.},
  archive      = {J_TVCG},
  author       = {S. Sandra Bae and Kyle Cave and Carsten Görg and Paul Rosen and Danielle Albers Szafir and Cindy Xiong Bearfield},
  doi          = {10.1109/TVCG.2025.3541571},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Bridging network science and vision science: Mapping perceptual mechanisms to network visualization tasks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). SPORT: From zero-shot prompts to real-time motion
generation. <em>TVCG</em>, 1–13. (<a
href="https://doi.org/10.1109/TVCG.2025.3542631">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Real-time motion generation has garnered significant attention within the fields of computer animation and gaming. Existing methods typically realize motion control via isolated style or content labels, resulting in short, simply motion clips. In this paper, we propose a motion generation framework, called SPORT (“from zero-Shot Prompt tO Real-Time motion generation”), for generating real-time and ever-changing motions using zero-shot prompts. SPORT consists of three primary components: (1) a body-part phase autoencoder that ensures smooth transitions between diverse motions; (2) a body-part content encoder that mitigates semantic gap between texts and motions; (3) a diffusion-based decoder that accelerates the denoising process while enhancing the diversity and realism of motions. Moreover, we develop a prototype for real-time application in Unity, demonstrating that our approach effectively considering the semantic gap caused by abstract style texts and rapidly changing terrains. Through qualitative and quantitative comparisons, we show that SPORT outperforms other approaches in terms of motion quality, style diversity and inference speed.},
  archive      = {J_TVCG},
  author       = {Bin Ji and Ye Pan and Zhimeng Liu and Shuai Tan and Xiaokang Yang},
  doi          = {10.1109/TVCG.2025.3542631},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {SPORT: From zero-shot prompts to real-time motion generation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Hierarchical point saliency for 3D keypoint detection.
<em>TVCG</em>, 1–18. (<a
href="https://doi.org/10.1109/TVCG.2025.3542465">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Keypoint detection plays a fundamental role in many applications, such as 3D reconstruction, object registration, and shape retrieval, and has attracted significant interest from researchers in computer vision and graphics. However, due to the ambiguity of the keypoint and the complexity of 3D objects, it is still tricky for existing 3D keypoint detection methods to generate stable keypoints with good coverage, especially for unsupervised detection methods. This paper proposes a 3D keypoint detection method based on hierarchical point saliency. This method can effectively and accurately locate the keypoints of a 3D point cloud, and it does not require complex training processes. First, we propose a simple and effective point descriptor called the local geometric structure feature, which can effectively characterize the geometric structure changes of 3D point clouds and has a strong feature identification ability. Second, we define two saliency measures used to characterize the saliency of points in the point cloud, which are low-level and high-level saliency. Third, we hierarchically characterize the saliency of points by combining the low-level and high-level saliency, thus measuring the probability that a point belongs to a keypoint. Finally, we extensively test our method on three benchmark 3D point cloud datasets, and the experimental results demonstrate that our method achieves state-of-the-art performance in keypoint detection tasks, significantly superior to the prior hand-crafted and deep-learning-based 3D keypoint detection methods.},
  archive      = {J_TVCG},
  author       = {Chengzhuan Yang and Yinhuang Chen and Qian Yu and Hui Wei and Fei Wu and Zhonglong Zheng},
  doi          = {10.1109/TVCG.2025.3542465},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Hierarchical point saliency for 3D keypoint detection},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). One shot learning for edge detection on point clouds.
<em>TVCG</em>, 1–12. (<a
href="https://doi.org/10.1109/TVCG.2025.3542475">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Each scanner possesses its unique characteristics and exhibits its distinct sampling error distribution. Training a network on a dataset that includes data collected from different scanners is less effective than training it on data specific to a single scanner. Therefore, we present a novel one-shot learning method allowing for edge extraction on point clouds, by learning the specific data distribution of the target point cloud, and thus achieve superior results compared to networks that were trained on general data distributions. More specifically, we present how to train a lightweight network named OSFENet (One-Shot Feature Extraction Network), by designing a filtered-KNN-based surface patch representation that supports a one-shot learning framework. Additionally, we introduce an RBF_DoS module, which integrates Radial Basis Function-based Descriptor of the Surface patch, highly beneficial for the edge extraction on point clouds. The advantage of the proposed OSFENet is demonstrated through comparative analyses against 7 baselines on the ABC dataset, and its practical utility is validated by results across diverse real-scanned datasets, including indoor scenes like S3DIS dataset, and outdoor scenes such as the Semantic3D dataset and UrbanBIS dataset.},
  archive      = {J_TVCG},
  author       = {Zhikun Tu and Yuhe Zhang and Yiou Jia and Kang Li and Daniel Cohen-Or},
  doi          = {10.1109/TVCG.2025.3542475},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {One shot learning for edge detection on point clouds},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Leveraging foundation models for crafting narrative
visualization: A survey. <em>TVCG</em>, 1–20. (<a
href="https://doi.org/10.1109/TVCG.2025.3542504">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Narrative visualization transforms data into engaging stories, making complex information accessible to a broad audience. Foundation models, with their advanced capabilities such as natural language processing, content generation, and multimodal integration, hold substantial potential for enriching narrative visualization. Recently, a collection of techniques have been introduced for crafting narrative visualizations based on foundation models from different aspects. We build our survey upon 66 papers to study how foundation models can progressively engage in this process and then propose a reference model categorizing the reviewed literature into four essential phases: Analysis, Narration, Visualization, and Interaction. Furthermore, we identify eight specific tasks (e.g. Insight Extraction and Authoring) where foundation models are applied across these stages to facilitate the creation of visual narratives. Detailed descriptions, related literature, and reflections are presented for each task. To make it a more impactful and informative experience for diverse readers, we discuss key research problems and provide the strengths and weaknesses in each task to guide people in identifying and seizing opportunities while navigating challenges in this field.},
  archive      = {J_TVCG},
  author       = {Yi He and Ke Xu and Shixiong Cao and Yang Shi and Qing Chen and Nan Cao},
  doi          = {10.1109/TVCG.2025.3542504},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Leveraging foundation models for crafting narrative visualization: A survey},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Drillboards: Adaptive visualization dashboards for dynamic
personalization of visualization experiences. <em>TVCG</em>, 1–14. (<a
href="https://doi.org/10.1109/TVCG.2025.3542606">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We present drillboards, a technique for adaptive visualization dashboards consisting of a hierarchy of coordinated charts that the user can drill down to reach a desired level of detail depending on their expertise, interest, and desired effort. This functionality allows different users to personalize the same dashboard to their specific needs and expertise. The technique is based on a formal vocabulary of chart representations and rules for merging multiple charts of different types and data into single composite representations. The drillboard hierarchy is created by iteratively applying these rules starting from a baseline dashboard, with each consecutive operation yielding a new dashboard with fewer charts and progressively more abstract and simplified views. We also present an authoring tool for building drillboards and show how it can be applied to an agricultural dataset with hundreds of expert users. Our evaluation asked three domain experts to author drillboards for their own datasets, which we then showed to casual end-users with favorable outcomes.},
  archive      = {J_TVCG},
  author       = {Sungbok Shin and Inyoup Na and Niklas Elmqvist},
  doi          = {10.1109/TVCG.2025.3542606},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Drillboards: Adaptive visualization dashboards for dynamic personalization of visualization experiences},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GNNFairViz: Visual analysis for graph neural network
fairness. <em>TVCG</em>, 1–17. (<a
href="https://doi.org/10.1109/TVCG.2025.3542419">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in Graph Neural Networks (GNNs) show promise for various applications like social networks and financial networks. However, they exhibit fairness issues, particularly in human-related decision contexts, risking unfair treatment of groups historically subject to discrimination. While several visual analytics studies have explored fairness in machine learning (ML), few have tackled the particular challenges posed by GNNs. We propose a visual analytics framework for GNN fairness analysis, offering insights into how attribute and structural biases may introduce model bias. Our framework is model-agnostic and tailored for real-world scenarios with multiple and multinary sensitive attributes, utilizing an extended suite of fairness metrics. To operationalize the framework, we develop GNNFairViz, a visual analysis tool that integrates seamlessly into the GNN development workflow, offering interactive visualizations. Our tool enables GNN model developers, the target users, to analyze model bias comprehensively, facilitating node selection, fairness inspection, and diagnostics. We evaluate our approach through two usage scenarios and expert interviews, confirming its effectiveness and usability in GNN fairness analysis. Furthermore, we summarize two general insights into GNN fairness based on our observations on the usage of GNNFairViz, highlighting the prevalence of the “Overwhelming Effect” in highly unbalanced datasets and the importance of suitable GNN architecture selection for bias mitigation.},
  archive      = {J_TVCG},
  author       = {Xinwu Ye and Jielin Feng and Erasmo Purificato and Ludovico Boratto and Michael Kamp and Zengfeng Huang and Siming Chen},
  doi          = {10.1109/TVCG.2025.3542419},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {GNNFairViz: Visual analysis for graph neural network fairness},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Uncertainty-aware spectral visualization. <em>TVCG</em>,
1–15. (<a href="https://doi.org/10.1109/TVCG.2025.3542898">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {One common task in time series analysis is the visual investigation of spectra such as Fourier spectra or wavelet spectra to identify dominating frequencies. In this paper, we present the propagation of data uncertainty to the spectra and its visualization. We consider the Fourier and continuous wavelet transformations, which are two common spectral analysis methods. Deriving the propagation for time series that can be modeled as a Gaussian process leads to a combination of weighted non-central chi-squared distributions in the spectrum. Percentile-based visualizations explicitly encode the non-normal uncertainty in the 1D Fourier and 2D wavelet spectrum. We enrich the visualization by including correlations, sensitivity, and signal-to-noise analysis. For visual exploration, we combine the different visualizations into an interactive approach that allows for investigating the uncertain time series in the temporal and spectral domains. Finally, we show the usefulness of our approach by applying it to several real-world data sets and by a qualitative interview study with visualization experts.},
  archive      = {J_TVCG},
  author       = {Marina Evers and Daniel Weiskopf},
  doi          = {10.1109/TVCG.2025.3542898},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Uncertainty-aware spectral visualization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Computer-aided colorization state-of-the-science: A survey.
<em>TVCG</em>, 1–19. (<a
href="https://doi.org/10.1109/TVCG.2025.3543527">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper reviews published research in the field of computer-aided colorization technology. We argue that within this context, the colorization task can be considered to originate from computer graphics, advance by introducing computer vision, and progress towards the fusion of vision and graphics. Hence, we propose a specific taxonomy and organize the research work chronologically. We extend the existing reconstruction-based colorization evaluation techniques on the basis that aesthetic assessment should be introduced to ensure the computer- colored images closely satisfy human visual-related requirements. We then perform an aesthetic assessment using the proposed metric and existing evaluations, comparing the colorization performance of seven representative unconditional colorization models. Finally, we identify unresolved issues and propose fruitful areas for future research and development. Details of the project associated with this survey can be obtained at https://github.com/DanielCho-HK/Colorization.},
  archive      = {J_TVCG},
  author       = {Yu Cao and Xin Duan and Xiangqiao Meng and P. Y. Mok and Ping Li and Tong-Yee Lee},
  doi          = {10.1109/TVCG.2025.3543527},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-19},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Computer-aided colorization state-of-the-science: A survey},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continuous scatterplot and image moments for time-varying
bivariate field analysis of electronic structure evolution.
<em>TVCG</em>, 1–14. (<a
href="https://doi.org/10.1109/TVCG.2025.3543619">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Photoinduced electronic transitions are complex quantum-mechanical processes where electrons move between energy levels due to the absorption of light. This induces dynamics i.e., coupled changes in the electronic structure and nuclear geometry, that drive physical and chemical processes of importance in diverse fields ranging from photobiology and materials design to medicine. The evolving electronic structure can be characterized by two electron density fields: hole and particle natural transition orbitals (NTOs). A study of the two density fields helps understand the movement of electronic charge from one part of the molecule to another, specifically the donor and acceptor regions. Previous works in this area rely on side-by-side visual comparisons of isosurfaces, statistical approaches, or visual analysis of bivariate fields restricted to limited time instances. We propose a new method to analyze time-varying bivariate fields with a large number of instances, as pertinent to understand electronic structure changes during light-induced dynamics. Since the NTO fields depend on the nuclear geometry, the nuclear motion leads to a large number of bivariate field instances. Structures like tracking graphs have been used to analyze time-varying univariate fields. This paper presents a structured and practical approach to feature-directed visual exploration of time-varying bivariate fields using continuous scatterplots (CSPs) and image moment-based descriptors, tailored for studying the evolving electronic structure following photoexcitation. The CSP of the bivariate field at every time step is represented using an image moment vector of length 4. The collection of all image moment vector descriptors is considered as a point cloud in $\mathbb {R}^{4}$ and visualized using principal component analysis. Choosing an appropriate pair of principal components results in a representation of the point cloud as a curve on the plane. This representation supports tasks such as identifying interesting time steps, identifying patterns within the bivariate field, and tracking their evolution over time. We present two case studies on excited-state dynamics in molecular systems that demonstrate how the time-varying bivariate field analysis helps provide application-specific insights.},
  archive      = {J_TVCG},
  author       = {Mohit Sharma and Talha Bin Masood and Nanna Holmgaard List and Ingrid Hotz and Vijay Natarajan},
  doi          = {10.1109/TVCG.2025.3543619},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Continuous scatterplot and image moments for time-varying bivariate field analysis of electronic structure evolution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Survey on 3D reconstruction techniques: Large-scale urban
city reconstruction and requirements. <em>TVCG</em>, 1–20. (<a
href="https://doi.org/10.1109/TVCG.2025.3540669">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {3D representations of large-scale and urban scenes are crucial across various industries, including autonomous driving, urban planning, natural resource supervision and many more. Large-scale industrial reconstructions are inherently complex and multifaceted. Many existing surveys primarily focus on academic progressions and often neglect the intricate and diverse needs of industry. This survey aims to bridge this gap by providing a comprehensive analysis of 3D reconstruction methods, with a focus on industrial requirements such as scalability and integration of human interaction. Our approach involves utilizing Affinity Diagramming to systematically analyze qualitative data gathered from industrial partners. This methodology enables us to gain deep insights into how recent literature addresses these specific industrial needs. The survey encompasses various aspects, including input and reconstruction modalities, architectural models, datasets, evaluation metrics, and the incorporation of prior knowledge. We further discuss practical implications derived from our analysis, highlighting key considerations for future advancements in 3D reconstruction methods tailored for large-scale applications.},
  archive      = {J_TVCG},
  author       = {Andreas Christodoulides and Gary K.L. Tam and James Clarke and Richard Smith and Jon Horgan and Nicholas Micallef and Jeremy Morley and Nelly Villamizar and Sean Walton},
  doi          = {10.1109/TVCG.2025.3540669},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-20},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Survey on 3D reconstruction techniques: Large-scale urban city reconstruction and requirements},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Introducing agent personality in crowd simulation improves
social presence and experienced realism in immersive VR. <em>TVCG</em>,
1–15. (<a href="https://doi.org/10.1109/TVCG.2025.3543740">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Convincing crowd behavior simulation is becoming essential in many application domains, including video games, cinematography, urban planning, safety simulations, and training. In this paper, we propose a novel and lightweight mesoscopic system for personality-based crowd simulation in immersive virtual reality (iVR). We use the Big Five personality framework, also known as OCEAN, to model a synthetic personality for each autonomous agent. Agents can autonomously aggregate in formations using machine learning-based clustering techniques operating on OCEAN. Moreover, agents can also externalize their personality traits by performing peculiar behavioral animations. To choose which animations to perform, we adopt a probabilistic approach that considers each OCEAN dimension as a continuous spectrum with two extremes linked to pairs of animations. Our system is designed to be flexible and suitable for different applications. Flexibility is achieved by using graphs to store agent and map topology data that control how the agents move and behave at runtime. In a within-subjects study with 40 users, we compare our personality-based system against a basic system that does not use personality. Results show that introducing personality into iVR crowd simulation enhances users&#39; social presence and experienced realism. Introducing personality also increases the perceived match between the agents and the virtual environment where the simulation takes place.},
  archive      = {J_TVCG},
  author       = {Massimiliano Pascoli and Fabio Buttussi and Konstantin Schekotihin and Luca Chittaro},
  doi          = {10.1109/TVCG.2025.3543740},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Introducing agent personality in crowd simulation improves social presence and experienced realism in immersive VR},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Informal skill-sharing in collaborative immersive analytics.
<em>TVCG</em>, 1–14. (<a
href="https://doi.org/10.1109/TVCG.2025.3542675">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Newcomers to immersive analytics systems would benefit from informally learning data analysis skills (e.g. reading a vis, using a system) when collaborating with more expert participants. We want to design tools that facilitate this informal learning by encouraging the informal sharing of data literacy skills during collaborative immersive analysis. A first step toward this goal is to understand how informal skill sharing takes place in order to identify how it could be improved. We experimentally studied informal skill-sharing in pairs of participants analyzing scatterplots in a shared virtual reality environment. We used an original mixed-method to analyze video and log recordings as well as subjective experiential data, based on common ground theory and the grounding process. We uncovered 101 episodes of skill-sharing, organized in 14 recurring types, and identified associated problems from which we could propose six implications for designing systems that favor informal skill-sharing, hence skill learning. The method can be used to study informal skill sharing in other systems enabling embodied face-to-face collaboration, but would need to be simplified for large-scale use.},
  archive      = {J_TVCG},
  author       = {Pierre Vaslin and Yannick Prié},
  doi          = {10.1109/TVCG.2025.3542675},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Informal skill-sharing in collaborative immersive analytics},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PolyGraph: A graph-based method for floorplan reconstruction
from 3D scans. <em>TVCG</em>, 1–13. (<a
href="https://doi.org/10.1109/TVCG.2025.3544769">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The task of reconstructing indoor floorplans has become an increasingly popular subject, offering substantial benefits across various applications such as interior design, virtual reality, and robotics. Despite the growing interest, existing approaches frequently encounter challenges due to high computational costs and sensitivity to errors in primitive detection. In this paper, we introduce PolyGraph, a new computational framework that combines a deep-learning based primitive detection network with an optimization-based reconstruction algorithm to facilitate high-quality reconstruction results. Specifically, we develop a novel guided wall point primitive estimation network capable of generating dense samples along wall boundaries. This network not only retains structural detail but also shows improved robustness in the detection phase. Then, PolyGraph utilizes wall points to establish a graph-based representation, formulating indoor floorplan reconstruction as a subgraph optimization problem. This approach significantly reduces the search space comparing to existing pixel-level optimization approaches. By utilizing “structural weight”, we seamlessly integrate the structural information of walls and rooms into graph representations, ensuring high-quality reconstruction results. Experimental results demonstrate PolyGraph&#39;s effectiveness and its advantages compared to other optimization-based approaches, showcasing its computational efficiency, and its ability to preserve structural integrity and capture fine details, as quantified by the structure metrics. The source code is publicly available at https://github.com/Fern327/PolyGraph},
  archive      = {J_TVCG},
  author       = {Qian Sun and Chenrong Fang and Shuang Liu and Yidan Sun and Yu Shang and Ying He},
  doi          = {10.1109/TVCG.2025.3544769},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PolyGraph: A graph-based method for floorplan reconstruction from 3D scans},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Comment analyzer: A tool for analyzing comment sets and
thread structures of news articles. <em>TVCG</em>, 1–13. (<a
href="https://doi.org/10.1109/TVCG.2025.3544733">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The lack of visually guided data exploration tools limits the scope of research questions communication scientists are able to study. The Comment Analyzer steps in where traditional statistical tools fail when it comes to researching the commenting behavior of news article readers. The basis of such an analysis are comment-thread corpora in which comments are tagged with various deliberative quality indicators as well as political stance. Our analysis tool provides a visual querying system for the exploration and analysis of such corpora and allows social scientists to gain insights into the distributions and relations between comment attributes, the homogeneity of thread sets, frequent thread structures and changes in comment qualities over the course of a single but in particular of multiple threads at once. We developed the tool in close collaboration with communication scientists in a user-centered approach. The system has proven its utility in thorough reviews with the communication scientists, by corroborating existing findings in the literature but particularly by provoking and answering new research questions. Final reviews with five independent experts confirmed these observations and revealed the potential of the Comment Analyzer for other datasets currently being created and analyzed in the communication sciences},
  archive      = {J_TVCG},
  author       = {Dora Kiesel and Patrick Riehmann and Ines Engelmann and Hanna Ramezani and Bernd Froehlich},
  doi          = {10.1109/TVCG.2025.3544733},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Comment analyzer: A tool for analyzing comment sets and thread structures of news articles},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Visualizing causality in mixed reality for manual task
learning: A study. <em>TVCG</em>, 1–16. (<a
href="https://doi.org/10.1109/TVCG.2025.3542949">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mixed Reality (MR) is gaining prominence in manual task skill learning due to its in-situ, embodied, and immersive experience. To teach manual tasks, current methodologies break the task into hierarchies (tasks into subtasks) and visualize not only the current subtasks but also the future ones that are causally related. We investigate the impact of visualizing causality within an MR framework on manual task skill learning. We conducted a user study with 48 participants, experimenting with how presenting tasks in hierarchical causality levels (no causality, event-level, interaction-level, and gesture-level causality) affects user comprehension and performance in a complex assembly task. The research finds that displaying all causality levels enhances user understanding and task execution, with a compromise of learning time. Based on the results, we further provide design recommendations and in-depth discussions for future manual task learning systems.},
  archive      = {J_TVCG},
  author       = {Rahul Jain and Jingyu Shi and Andrew Benton and Moiz Rasheed and Hyungjun Doh and Subramanian Chidambaram and Karthik Ramani},
  doi          = {10.1109/TVCG.2025.3542949},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Visualizing causality in mixed reality for manual task learning: A study},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Every angle is worth a second glance: Mining kinematic
skeletal structures from multi-view joint cloud. <em>TVCG</em>, 1–14.
(<a href="https://doi.org/10.1109/TVCG.2025.3542442">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-person motion capture over sparse angular observations is a challenging problem under interference from both self- and mutual-occlusions. Existing works produce accurate 2D joint detection, however, when these are triangulated and lifted into 3D, available solutions all struggle in selecting the most accurate candidates and associating them to the correct joint type and target identity. As such, in order to fully utilize all accurate 2D joint location information, we propose to independently triangulate between all same-typed 2D joints from all camera views regardless of their target ID, forming the Joint Cloud. Joint Cloud consist of both valid joints lifted from the same joint type and target ID, as well as falsely constructed ones that are from different 2D sources. These redundant and inaccurate candidates are processed over the proposed Joint Cloud Selection and Aggregation Transformer (JCSAT) involving three cascaded encoders which deeply explore the trajectile, skeletal structural, and view-dependent correlations among all 3D point candidates in the cross-embedding space. An Optimal Token Attention Path (OTAP) module is proposed which subsequently selects and aggregates informative features from these redundant observations for the final prediction of human motion. To demonstrate the effectiveness of JCSAT, we build and publish a new multi-person motion capture dataset BUMocap-X with complex interactions and severe occlusions. Comprehensive experiments over the newly presented as well as benchmark datasets validate the effectiveness of the proposed framework, which outperforms all existing state-of-the-art methods, especially under challenging occlusion scenarios.},
  archive      = {J_TVCG},
  author       = {Junkun Jiang and Jie Chen and Ho Yin Au and Mingyuan Chen and Wei Xue and Yike Guo},
  doi          = {10.1109/TVCG.2025.3542442},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Every angle is worth a second glance: Mining kinematic skeletal structures from multi-view joint cloud},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Interweaving mathematics and art: Drawing graphs as celtic
knots and links with CelticGraph. <em>TVCG</em>, 1–12. (<a
href="https://doi.org/10.1109/TVCG.2025.3545481">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Celtic knots, an ancient art form often linked to Celtic heritage, have been used historically in the decoration of monuments and manuscripts, often symbolizing the notions of eternity and interconnectedness. This paper introduces the framework CelticGraph designed for illustrating graphs in the style of Celtic knots and links. The process of creating these drawings raises interesting combinatorial concepts in the theory of circuits in planar graphs. Further, CelticGraph uses a novel algorithm to represent edges as Bézier curves, aiming to show each link as a smooth curve with limited curvature. We also show that with our production mechanisms we can compute any 4-regular plane graph and thereby any celtic knot or link. The CelticGraph framework for drawing graphs as celtic knots and links is implemented as an add-on of Vanted, a network visualization and analysis tool.},
  archive      = {J_TVCG},
  author       = {Niklas Gröne and Peter Eades and Karsten Klein and Patrick Eades and Leo Schreiber and Ulf Hailer and Hugo A. D. do Nascimento and Falk Schreiber},
  doi          = {10.1109/TVCG.2025.3545481},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Interweaving mathematics and art: Drawing graphs as celtic knots and links with CelticGraph},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PDPilot: Exploring partial dependence plots through ranking,
filtering, and clustering. <em>TVCG</em>, 1–14. (<a
href="https://doi.org/10.1109/TVCG.2025.3545025">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Partial dependence plots (PDPs) and individual conditional expectation (ICE) plots are visualizations used for explaining the behavior of machine learning (ML) models trained on tabular datasets. They show how the values of a feature or pair of features impact a model&#39;s predictions. However, in models with a large number of features, it is impractical for an ML practitioner to analyze all possible plots. To address this, we present new techniques for ranking and filtering PDP and ICE plots and build upon existing strategies for clustering the lines in ICE plots. Together, these techniques aim to help ML practitioners efficiently explore PDP and ICE plots and identify interesting model behavior. We integrate these techniques into PDPilot, a visual analytics tool that runs in Jupyter notebooks. We use PDPilot to study how 7 ML practitioners utilize the ranking, filtering, and clustering techniques to analyze an ML model.},
  archive      = {J_TVCG},
  author       = {Daniel Kerrigan and Brian Barr and Enrico Bertini},
  doi          = {10.1109/TVCG.2025.3545025},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {PDPilot: Exploring partial dependence plots through ranking, filtering, and clustering},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MC-NeRF: Multi-camera neural radiance fields for
multi-camera image acquisition systems. <em>TVCG</em>, 1–18. (<a
href="https://doi.org/10.1109/TVCG.2025.3546290">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Neural Radiance Fields (NeRF) use multi-view images for 3D scene representation, demonstrating remarkable performance. As one of the primary sources of multi-view images, multi-camera systems encounter challenges such as varying intrinsic parameters and frequent pose changes. Most previous NeRF-based methods assume a unique camera and rarely consider multi-camera scenarios. Besides, some NeRF methods that can optimize intrinsic and extrinsic parameters still remain susceptible to suboptimal solutions when these parameters are poor initialized. In this paper, we propose MC-NeRF, a method for joint optimization of both intrinsic and extrinsic parameters alongside NeRF, allowing individual camera parameters for each image. First, we analyze the coupling issue that arises from the joint optimization between intrinsics and extrinsics, and propose a decoupling constraint utilizing auxiliary images. To further address the degenerate cases in the decoupling process, we introduce an efficient auxiliary image acquisition scheme to mitigate these effects. Furthermore, recognizing that most existing datasets are designed for a unique camera, we provided a new dataset that includes both simulated data and real-world data. Experiments demonstrate the effectiveness of our method in scenarios where each image corresponds to different camera parameters. Specifically, our approach outperforms the baselines favorably in terms of intrinsics estimation, extrinsics estimation, scale estimation, and rendering quality. The Code and supplementary materials are available at https://in2-viaun.github.io/MC-NeRF.},
  archive      = {J_TVCG},
  author       = {Yu Gao and Lutong Su and Hao Liang and Yufeng Yue and Yi Yang and Mengyin Fu},
  doi          = {10.1109/TVCG.2025.3546290},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {MC-NeRF: Multi-camera neural radiance fields for multi-camera image acquisition systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). XROps: A visual workflow management system for dynamic
immersive analytics. <em>TVCG</em>, 1–13. (<a
href="https://doi.org/10.1109/TVCG.2025.3546467">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Immersive analytics is gaining attention across multiple domains due to its capability to facilitate intuitive data analysis in expansive environments through user interaction with data. However, creating immersive analytics systems for specific tasks is challenging due to the need for programming expertise and significant development effort. Despite the introduction of various immersive visualization authoring toolkits, domain experts still face hurdles in adopting immersive analytics into their workflow, particularly when faced with dynamically changing tasks and data in real time. To lower such technical barriers, we introduce XROps, a web-based authoring system that allows users to create immersive analytics applications through interactive visual programming, without the need for low-level scripting or coding. XROps enables dynamic immersive analytics authoring by allowing users to modify each step of the data visualization process with immediate feedback, enabling them to build visualizations on-the-fly and adapt to changing environments. It also supports the integration and visualization of real-time sensor data from XR devices—a key feature of immersive analytics—facilitating the creation of various analysis scenarios. We evaluated the usability of XROps through a user study and demonstrate its efficacy and usefulness in several example scenarios. We have released a web platform (https://vience.io/xrops) to demonstrate various examples to supplement our findings.},
  archive      = {J_TVCG},
  author       = {Suemin Jeon and JunYoung Choi and Haejin Jeong and Won-Ki Jeong},
  doi          = {10.1109/TVCG.2025.3546467},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {XROps: A visual workflow management system for dynamic immersive analytics},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RGAvatar: Relightable 4D gaussian avatar from monocular
videos. <em>TVCG</em>, 1–15. (<a
href="https://doi.org/10.1109/TVCG.2025.3543603">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Relightable 4D avatar reconstruction which enables high fidelity and real-time rendering continues to be a crucial but challenging problem, especially from monocular videos. Previous NeRF-based 4D avatars enable photo-realistic relighting but are too slow for rendering, while point-based or mesh-based 4D avatars are efficient but have limited rendering quality. The recent success of 3D Gaussian Splatting, i.e., 3DGS, has inspired a series of impressive 4D Gaussian avatars, however, most of which only focus on faithful appearance reconstruction but are not relightable. To address such issues, this paper proposes a new Relightable 4D Gaussian Avatar, i.e., RGAvatar, tailored for high fidelity relightable rendering from monocular videos. Our key idea is to introduce a new relightable 4D Gaussian representation, based on which we can directly perform high fidelity Physically Based Rendering, and an effective joint learning mechanism for compact 4D Gaussian reconstruction with SDF regulation and accurate materials and lighting decomposition. By comparing with previous state-of-the-art approaches, RGAvatar can significantly outperform previous approaches in relightable rendering quality and speed. To our best knowledge, RGAvatar contributes a new state-of-the-art 4D Gaussian avatar from monocular videos, which enables high fidelity relightable rendering in a quite efficient manner.},
  archive      = {J_TVCG},
  author       = {Zhe Fan and Shi-Sheng Huang and Yichi Zhang and Dachao Shang and Juyong Zhang and Yudong Guo and Hua Huang},
  doi          = {10.1109/TVCG.2025.3543603},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {RGAvatar: Relightable 4D gaussian avatar from monocular videos},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A serial perspective on photometric stereo of filtering and
serializing spatial information. <em>TVCG</em>, 1–15. (<a
href="https://doi.org/10.1109/TVCG.2025.3546657">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we introduce a novel method of Filtering and Serializing Spatial Information to tackle uncalibrated photometric stereo tasks, termed FSSI-PS. Photometric stereo aims to recover surface normals from images with varying lighting and is crucial for tasks like 3D reconstruction and defect detection. Current methods in complex surface reconstruction are costly and inaccurate due to redundant feature representations from GCN or Transformer modules, caused by the weak global information extraction capability of GCNs or the large computational cost of Transformers. Furthermore, the trainset&#39;s lack of richness in texture complexity makes reconstruction more difficult. We address these issues by optimizing feature maps and dataset richness through serializing and filtering. Firstly, we use Mamba-RNN to optimize feature representation by directly fusing feature maps, which reduces redundancy and uses minimal computational resources. Specifically, we treat input spatial information as a sequence and serialize it by sorting. Furthermore, we introduce the Mean Angular Variation metric to assess reconstruction difficulty by measuring texture complexity. It classifies PS-Sculpture and PS-Blobby into three categories: Difficult, Normal, and Simple. We use this to construct DNS-S+B, a photometric stereo training set with rich complexity levels. Our method is compared with state-of-the-art methods on the DiLiGenT and LUCES benchmarks to highlight effectiveness.},
  archive      = {J_TVCG},
  author       = {Minzhe Xu and Xin Ding and You Yang and Yinqiang Zheng and Qiong Liu},
  doi          = {10.1109/TVCG.2025.3546657},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {A serial perspective on photometric stereo of filtering and serializing spatial information},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). AttributionScanner: A visual analytics system for model
validation with metadata-free slice finding. <em>TVCG</em>, 1–12. (<a
href="https://doi.org/10.1109/TVCG.2025.3546644">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Data slice finding is an emerging technique for validating machine learning (ML) models by identifying and analyzing subgroups in a dataset that exhibit poor performance, often characterized by distinct feature sets or descriptive metadata. However, in the context of validating vision models involving unstructured image data, this approach faces significant challenges, including the laborious and costly requirement for additional metadata and the complex task of interpreting the root causes of underperformance. To address these challenges, we introduce AttributionScanner, an innovative human-in-the-loop Visual Analytics (VA) system, designed for metadata-free data slice finding. Our system identifies interpretable data slices that involve common model behaviors and visualizes these patterns through an Attribution Mosaic design. Our interactive interface provides straightforward guidance for users to detect, interpret, and annotate predominant model issues, such as spurious correlations (model biases) and mislabeled data, with minimal effort. Additionally, it employs a cutting-edge model regularization technique to mitigate the detected issues and enhance the model&#39;s performance. The efficacy of AttributionScanner is demonstrated through use cases involving two benchmark datasets, with qualitative and quantitative evaluations showcasing its substantial effectiveness in vision model validation, ultimately leading to more reliable and accurate models.},
  archive      = {J_TVCG},
  author       = {Xiwei Xuan and Jorge Piazentin Ono and Liang Gou and Kwan-Liu Ma and Liu Ren},
  doi          = {10.1109/TVCG.2025.3546644},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {AttributionScanner: A visual analytics system for model validation with metadata-free slice finding},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Controllable human video generation from sparse sketches.
<em>TVCG</em>, 1–11. (<a
href="https://doi.org/10.1109/TVCG.2025.3543687">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent advancements in human fashion video generation have transformed the field, producing various promising effects. Existing methods mainly focus on pose control but lack the ability to achieve sketch-based control, largely due to the absence of appearance-consistent and shape-varying knowledge in existing datasets. Moreover, the necessity of sequential structure inputs to control video generation hinders real-world applications. To address these limitations, we introduce Sketch2HumanVideo, an approach that, for the first time, achieves sketch-controllable human video generation with three conditions: temporally sparse sketches, a spatially sparse pose sequence, and a reference appearance image. Our key contribution is a sparse sketch encoder, which takes the first two conditions as input, enabling precise and multi-view control of shape motion. To provide the above knowledge, we leverage the expertise of two pretrained models to synthesize a dataset comprising shape-varying yet appearance-consistent examples for model training. Furthermore, we introduce an enlarging-and-resampling scheme to enhance high-frequency details of local regions in resource-constrained scenarios, thereby promoting the generation of realistic videos. Through qualitative and quantitative experiments, our method showcases superior performance to state-of-the-art approaches and flexible control. We will release the code upon the paper&#39;s acceptance.},
  archive      = {J_TVCG},
  author       = {Linzi Qu and Jiaxiang Shang and Miu-Ling Lam and Hongbo Fu},
  doi          = {10.1109/TVCG.2025.3543687},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Controllable human video generation from sparse sketches},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EverywhereAR: A visual authoring system for creating
adaptive AR game scenes. <em>TVCG</em>, 1–14. (<a
href="https://doi.org/10.1109/TVCG.2025.3544021">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a pivotal application of Augmented Reality (AR) technology, AR games empower players to bridge reality with virtuality, offering a distinct and immersive experience set apart from traditional games. However, when creating AR games, one of the most formidable challenges faced by designers pertains to the unpredictability of intricate real-world environments, which hinders crafting naturally integrated scenes where virtual objects harmoniously blend with the players&#39; surroundings. In this paper, we introduce EverywhereAR, a system that is capable of flexibly realizing the designer&#39;s idea in various real-world scenes. It provides a designer-friendly Game Scene Template development interface, for designers to quickly graphify their inspirations. To achieve the best AR game scene, this work proposes a highly customizable integration method. According to the integrated AR scene graph, the system will arrange each virtual object in a reasonable position to make the generated game scene look natural. We conducted an experiment to evaluate our system&#39;s performance across various game scene templates and real-world environments. Results from the experiment indicated that our system was able to generate AR game scenes matching the quality of scenes manually created by professional designers. In addition, we conducted another experiment to assess the effectiveness and usability of the proposed interface. The experiment results showed that the interface was intuitive and efficient, allowing users to create a simple game scene within one minute.},
  archive      = {J_TVCG},
  author       = {Jia Liu and Renjie Zhang and Isidro Butaslac and Taishi Sawabe and Yuichiro Fujimoto and Masayuki Kanbara and Hirokazu Kato},
  doi          = {10.1109/TVCG.2025.3544021},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {EverywhereAR: A visual authoring system for creating adaptive AR game scenes},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time, free-viewpoint holographic patient rendering for
telerehabilitation via a single camera: A data-driven approach with 3D
gaussian splatting for real-world adaptation. <em>TVCG</em>, 1–14. (<a
href="https://doi.org/10.1109/TVCG.2025.3544297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Telerehabilitation is a cost-effective alternative to in-clinic rehabilitation. Although convenient, it lacks immersive and free-viewpoint patient visualization. Current research explores two solutions to this issue. Mesh-based methods use 3D models and motion capture for AR visualization. However, they are labor-intensive and less photorealistic than 2D images. Microsoft&#39;s Holoportation generates photorealistic 3D models with eight RGBD cameras in real time. However, it requires complex setups, high GPU power, and high-speed communication infrastructure, making deployment challenging. This paper presents a Real-Time Free-Viewpoint Holographic Patient Rendering (RT-FVHP) system for telerehabilitation. Unlike traditional methods that require manually crafted assets such as 3D meshes, texture maps, and skeletal rigging, our data-driven approach eliminates the need for explicit asset definitions. Inspired by the HumanNeRF framework, we retarget dynamic human poses to a canonical pose and leverage 3D Gaussian Splatting to train a neural network in canonical space for patient representation. The trained model generates 2D RGB$\sigma$ outputs via Gaussian Splatting rasterization, guided by camera parameters and human pose inputs. Compatible with HoloLens 2 and web-based platforms, RT-FVHP operates effectively under real-world conditions, including handling occlusions caused by treadmills. Occlusion handling is accomplished using our Shape-Enforced Gaussian Density Control (SGDC), which initializes and densifies 3D Gaussians in occluded regions using estimated SMPL human body priors. This approach minimizes manual intervention while ensuring complete body reconstruction. With efficient Gaussian rasterization, the model delivers real-time performance of up to 400 FPS at 1080p resolution on a dedicated RTX6000 GPU.},
  archive      = {J_TVCG},
  author       = {Shengting Cao and Jiamiao Zhao and Fei Hu and Yu Gan},
  doi          = {10.1109/TVCG.2025.3544297},
  journal      = {IEEE Transactions on Visualization and Computer Graphics},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Vis. Comput. Graph.},
  title        = {Real-time, free-viewpoint holographic patient rendering for telerehabilitation via a single camera: A data-driven approach with 3D gaussian splatting for real-world adaptation},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
