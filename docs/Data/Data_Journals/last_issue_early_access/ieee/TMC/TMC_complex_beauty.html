<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TMC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tmc---80">TMC - 80</h2>
<ul>
<li><details>
<summary>
(2025). Robust device-free mmwave sensing with specular reflection
interference mitigation. <em>TMC</em>, 1–16. (<a
href="https://doi.org/10.1109/TMC.2025.3538112">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Device-Free mmWave Sensing (DFWS) could sense target state by analyzing how target activities influence the surrounding mmWave signals. It has emerged as a promising sensing technology. However, when employing DFWS indoors, specular reflection interference arises due to the specular reflectors. This interference often induces ghost targets, impacting the accurate estimation of the number and position of targets, resulting in degradation in sensing performance. To tackle this issue, we delve into the generation mechanism of specular reflection interference and analyze its multi-domain characteristics. Through exploration, we discern its temporal sparsity, spatial symmetry or collinearity, and frequency correlation characteristics, and propose four metrics to measure them, accordingly. Specifically, we propose a temporal characteristic quantitative evaluation metric based on identity matching, spatial symmetry and collinearity quantitative evaluation metrics based on geometric analysis, and a frequency correlation quantitative evaluation metric based on Doppler velocity correction, respectively. Based on these metrics, we design a novel Specular Reflection Interference Mitigation (SRIM) method and develop a robust SRIM-DFWS prototype system based on a 60 GHz mmWave radar to validate our proposed method. Experimental results demonstrate that our proposed method could achieve accurate and effective mitigation of specular reflection interference in device-free target tracking.},
  archive      = {J_TMC},
  author       = {Yulin Liu and Jie Wang and Qinghua Gao and Miao Pan and Yuguang Fang},
  doi          = {10.1109/TMC.2025.3538112},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Robust device-free mmwave sensing with specular reflection interference mitigation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Securing autonomous UAV cluster with blockchain-based
threshold key management system utilizing crypto-asset and
multisignature. <em>TMC</em>, 1–14. (<a
href="https://doi.org/10.1109/TMC.2025.3538462">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Unmanned aerial vehicles deployed in remote locations rely on self-governed key management for their protection. However, conventional key management depends on a centralized ground-based station or single vehicle. Such a system is vulnerable to compromised certificate authority problems and single-points-of-failure. This paper proposed to resolve these vulnerabilities using a blockchain-based threshold key management system. The proposed system utilized blockchain&#39;s concepts of crypto-asset and multisignature. Keys are defined as crypto-assets to improve their management in the blockchain network. Multisignature facilitates collaboration during key management based on a threshold value. The threshold value is also configurable to meet systems&#39; security and performance requirements. The proposed system secured the process of re-enforcement, sub-clustering, re-merging, and inter-cluster migration. Security analysis revealed that the proposed system complied with most key management security guidelines. The custom signature module used to authenticate intra-cluster communication was also verified as safe. Threats to the cluster were identified, assessed for risk, and mitigated accordingly. Performance analysis found that both AODV and DSDV routing protocols offer consistent performance but DSDV prevailed during the worst-case network scenario. The paper finally identified research gaps, including the requirement for an optimized mechanism for collecting consent signatures.},
  archive      = {J_TMC},
  author       = {Mebanjop Kharjana and Subhas Chandra Sahana and Goutam Saha},
  doi          = {10.1109/TMC.2025.3538462},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Securing autonomous UAV cluster with blockchain-based threshold key management system utilizing crypto-asset and multisignature},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). How to prevent social media platforms from knowing the
images you share with friends. <em>TMC</em>, 1–17. (<a
href="https://doi.org/10.1109/TMC.2025.3538885">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The surge in image sharing on social media platforms escalates private information extraction for commercial use, increasing user demand for privacy protection. However, the dynamics of group communication within online social networks and the image compression imposed by platforms present significant challenges to secure key exchange and reliable image sharing in existing solutions. In this paper, we propose PrivSocial to prevent social media platforms from extracting private information in images shared within group communications. Specifically, we propose two frameworks, a server-based framework and a subscription-based framework, making PrivSocial applicable to different social media platforms and providing users with optional security levels, enhancing the flexibility and efficiency. To achieve intra-group key agreement and ensure image privacy protection, both frameworks integrate optimized continuous group key agreement and a novel image encryption scheme resisting compression. We implement an Android-based Priv-raster application and deploy a prototype on Twitter. Furthermore, we evaluate the proposed encryption scheme, and experimental results show that it has efficient encryption and decryption performance while being resistant to jigsaw puzzle solver attacks. The multi-user simulation experiments also demonstrate that the processing time of a single user is mere milliseconds, and the scheme can efficiently support tens of thousands of groups.},
  archive      = {J_TMC},
  author       = {Dawei Li and Yuxiao Guo and Di Liu and Qifan Liu and Song Bian and Zhenyu Guan},
  doi          = {10.1109/TMC.2025.3538885},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {How to prevent social media platforms from knowing the images you share with friends},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Burst-sensitive traffic forecast via multi-property
personalized fusion in federated learning. <em>TMC</em>, 1–17. (<a
href="https://doi.org/10.1109/TMC.2025.3538871">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For distributed network traffic prediction with data localization and privacy protection, Federated Learning (FL) enables collaborative training without raw data exchange across Base Stations (BSs). Nevertheless, traffic across BSs exhibit inherently heterogeneous trend burst and smooth fluctuation properties, but existing FL methods model single-scale series from only one view, which cannot simultaneously capture diverse trend and fluctuation properties, especially distinct burst distributions. In this paper, we propose Personalized Federated Forecasting with Multi-property Self-fusion (P2FMS), which can represent multi-scale traffic properties from different views. With precise multi-property representations, a fusion-level prediction decision is learned for each client in a personalized manner to promptly sense traffic bursts and improve forecasting performance in non-IID settings. Specifically, P2FMS decomposes the traffic series into distinct time scales, based on which, we effectively extract closeness, period, and trend properties from different views. The closeness and period are embedded through global-view representations with spatial correlations, while non-stationary trends are individually fitted from the client-side view. Furthermore, a personalized combiner is designed to accurately quantify the proportion of general fluctuation raws (i.e., closeness and period) and specific trend property in predictions, which enables multi-property self-fusion for each client to accommodate heterogeneous traffic patterns and enhance prediction accuracy. Besides, an alternant training mechanism is introduced to optimize property representation and fusion modules with the convergence guarantee. Extensive experiments on real-world datasets show that P2FMS outperforms status quo methods in both prediction performance and convergence time.},
  archive      = {J_TMC},
  author       = {Jingjing Xue and Sheng Sun and Min Liu and Yuwei Wang and Xuying Meng and Jingyuan Wang and JunBo Zhang and Ke Xu},
  doi          = {10.1109/TMC.2025.3538871},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Burst-sensitive traffic forecast via multi-property personalized fusion in federated learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-channel analog beamforming transceiver for mmWave
communications. <em>TMC</em>, 1–13. (<a
href="https://doi.org/10.1109/TMC.2025.3539169">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper introduces an analog multi-channel millimeter-wave transceiver architecture that offers advantages in terms of low hardware complexity and computational efficiency compared to digital beamforming and hybrid beamforming techniques. Also, it is known that analog beamforming with a single phase-shifter network faces limitations in maintaining consistent accuracy across a wideband spectrum. To this end, the proposed architecture leverages the inherent bandwidth-splitting property of the multi-channel transceiver. Thus, each sub-band signal is processed by its corresponding channel in the transceiver with an independent analog beamformer per channel. This approach can significantly improve the beamforming accuracy in a wideband communication system such as 5G and future 6G cellular networks. The simulation results demonstrate that increasing the channels in the multi-channel transceiver enables multi-channel analog beamforming to achieve a comparable bit-error-rate (BER) performance to digital beamforming when interference is not considered. Moreover, when interference is present, the proposed multi-channel analog beamforming exhibits enhanced resilience to high power interference compared with digital beamforming with limited analog-to-digital conversion resolution.},
  archive      = {J_TMC},
  author       = {Haotian Zhao and Kamran Entesari and Sebastian Hoyos},
  doi          = {10.1109/TMC.2025.3539169},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-channel analog beamforming transceiver for mmWave communications},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Topology-aware microservice architecture in edge networks:
Deployment optimization and implementation. <em>TMC</em>, 1–15. (<a
href="https://doi.org/10.1109/TMC.2025.3539312">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a ubiquitous deployment paradigm, integrating microservice architecture (MSA) into edge networks promises to enhance the flexibility and scalability of services. However, it also presents significant challenges stemming from dispersed node locations and intricate network topologies. In this paper, we have proposed a topology-aware MSA characterized by a three-tier network traffic model encompassing the service, microservices, and edge node layers. This model meticulously characterizes the complex dependencies between edge network topologies and microservices, mapping microservice deployment onto link traffic to accurately estimate communication delay. Building upon this model, we have formulated a weighted sum communication delay optimization problem considering different types of services. Then, a novel topology-aware and individual-adaptive microservices deployment (TAIA-MD) scheme is proposed to solve the problem efficiently, which accurately senses the network topology and incorporates an individual-adaptive mechanism in a genetic algorithm to accelerate the convergence and avoid local optima. Extensive simulations show that, compared to the existing deployment schemes, TAIA-MD improves the communication delay performance by approximately 30% to 60% and effectively enhances the overall network performance. Furthermore, we implement the TAIA-MD scheme on a practical microservice physical platform. The experimental results demonstrate that TAIA-MD achieves superior robustness in withstanding link failures and network fluctuations.},
  archive      = {J_TMC},
  author       = {Yuang Chen and Chang Wu and Fangyu Zhang and Chengdi Lu and Yongsheng Huang and Hancheng Lu},
  doi          = {10.1109/TMC.2025.3539312},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Topology-aware microservice architecture in edge networks: Deployment optimization and implementation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time distributed charging station recommendation for
electric vehicles: A federated meta-RL approach. <em>TMC</em>, 1–14. (<a
href="https://doi.org/10.1109/TMC.2025.3539496">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The growth of Electric Vehicles (EVs) places an increasingly heavy burden on the limited charging infrastructure, necessitating an effective charging station recommendation strategy that assists EVs in finding the most suitable charging stations. Deep reinforcement learning is a promising technology that has been applied to optimize EVs&#39; charging recommendations. However, existing schemes have low scalability and high communication costs as they usually require collecting real-time information on both charging requests and charger availability at various stations during policy training or execution. To address this challenge, we develop a real-time distributed charging station recommendation approach, named ReDirect, to minimize the charging duration experienced by EVs, considering dynamic charging requests of EVs and fluctuating availability at charging stations. ReDirect employs federated meta-reinforcement learning (RL) to empower distributed stations to collaboratively learn effective recommendation strategies and make decisions without sharing their local information, yielding improved scalability, reduced communication overhead, and enhanced data privacy. Furthermore, we conduct a rigorous theoretical analysis of the convergence performance of ReDirect. Extensive experimental results on real-world datasets demonstrate that ReDirect performs closely to the centralized recommendation algorithm and outperforms several state-of-the-art distributed algorithms in EV charging duration while realizing a balanced distribution of charging requests across multiple stations.},
  archive      = {J_TMC},
  author       = {Yongchao Zhang and Jia Hu and Geyong Min and Jie Gao and Nektarios Georgalas},
  doi          = {10.1109/TMC.2025.3539496},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Real-time distributed charging station recommendation for electric vehicles: A federated meta-RL approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint class-balanced client selection and bandwidth
allocation for cost-efficient federated learning in mobile edge
computing networks. <em>TMC</em>, 1–17. (<a
href="https://doi.org/10.1109/TMC.2025.3539284">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated Learning (FL) has significant potential to protect data privacy and mitigate network burden in mobile edge computing (MEC) networks. However, due to the system and data heterogeneity of mobile clients (MCs), client selection and bandwidth allocation is key for achieving cost-efficient FL in MEC networks with limited bandwidth. To address these challenges, we investigate the issue of joint client selection and bandwidth allocation for reducing the cost (i.e., latency and energy consumption) of FL training. We formulate the problem and decompose it into a holistic subproblem to reduce the number of rounds and a partial subproblem to reduce the costs of FL each round. We propose a joint class-balanced client selection and bandwidth allocation (CBCSBA) framework to address the whole problem. Specifically, for the holistic subproblem, CBCSBA combines MCs into groups, each having data distribution as close as possible to class-balanced distribution; For the partial subproblem, CBCSBA reduces costs by exploratively selecting a group and sequentially optimizing the latency and energy consumption of MCs within the group. Experimental results show that CBCSBA outperforms the baseline frameworks in reducing latency by 28.2% and energy consumption by 25.3% on average in the considered four datasets. Our code is available here.},
  archive      = {J_TMC},
  author       = {Jian Tang and Xiuhua Li and Hui Li and Penghua Li and Xiaofei Wang and Victor C. M. Leung},
  doi          = {10.1109/TMC.2025.3539284},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint class-balanced client selection and bandwidth allocation for cost-efficient federated learning in mobile edge computing networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint frame drop and object detection task offloading for
mobile devices via RL with lyapunov optimization. <em>TMC</em>, 1–15.
(<a href="https://doi.org/10.1109/TMC.2025.3539356">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Object detection has become an increasingly important application for mobile devices. However, state-of-the-art object detection relies heavily on deep neural network, which is often burdensome to compute on mobile devices. To this end, we develop a layering framework for joint video frame drop and object detection task offloading. In the lower layer, by invoking Lyapunov optimization, we devise an algorithm for partitioning and offloading the computation tasks of deep neural networks. This algorithm also specifies the flow control for admitting the application traffic into the network. In the upper layer, we use the flow control as a form of guidance in the action space in order to develop a reinforcement learning (RL) algorithm that selectively drops video frames with object detection performance in consideration. By the nature of design, this Lyapunov-guided RL guarantees the network stability. We show through simulations that our Lyapunov-guided RL drops video frames with reasonable object detection performance and reduced latency while keeping the network stable. We also implemented our algorithm on the remote-controlled (RC) car equipped with microprocessor and GPU, and demonstrate the applicability of our algorithm to real-time object detection tasks from the video stream generated as the RC car moves.},
  archive      = {J_TMC},
  author       = {Vaughn Sohn and Suhwan Kim and Hyang-Won Lee},
  doi          = {10.1109/TMC.2025.3539356},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint frame drop and object detection task offloading for mobile devices via RL with lyapunov optimization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dual fine-grained authentication without trusted authority
for data collection in TDT systems. <em>TMC</em>, 1–13. (<a
href="https://doi.org/10.1109/TMC.2025.3539281">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In transportation 5.0, digital twin (DT) is considered a promising paradigm to integrate physical entities into cyber physical systems by collecting massive data. However, the open collection process and key exposure issues bring critical security challenges. Furthermore, applying the existing authentication schemes to data collection in transportation DT (TDT) systems encounters three deficiencies: 1) forward security for collected data can only be achieved at a coarse-grained level; 2) one or more additional trusted authorities are introduced, causing the robustness of TDT systems to be downgraded; 3) dynamic attribute updating and revocability of physical entities are rarely considered. Therefore, we propose a dual fine-grained authentication scheme (DFAS) in this paper. Our DFAS can not only ensure data integrity and authenticity but also enable fine-grained access control, namely, only registered physical entities with authorized attributes can generate valid signatures. Meanwhile, DFAS provides the key puncturing for physical entities to guarantee fine-grained forward security without relying on any trusted authority. In addition, a non-interactive attribute updating and revocation of malicious entities are realized in DFAS. Finally, the security analysis indicates that DFAS can deal with various security challenges for data collection in TDT systems. The performance evaluation demonstrates that DFAS is efficient and practical.},
  archive      = {J_TMC},
  author       = {Chenhao Wang and Yang Ming and Hang Liu and Yutong Deng},
  doi          = {10.1109/TMC.2025.3539281},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Dual fine-grained authentication without trusted authority for data collection in TDT systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust task offloading and resource allocation under
imperfect computing capacity information in edge intelligence systems.
<em>TMC</em>, 1–14. (<a
href="https://doi.org/10.1109/TMC.2025.3539296">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In edge intelligence systems, task offloading and resource allocation policies critically depend on the required computing capacity of the task, which can only be accurately measured after execution, presenting significant design challenges. In this paper, we address the problem of robust task offloading and resource allocation under imperfect computing capacity information, where the exact value as well as distribution knowledge of the required computing capacity cannot be obtained in advance. Specifically, we formulate the energy-time cost (ETC) minimization problem using min-max robust optimization. To tackle this challenging issue, we propose a decoupling method. This method first assumes the offloading policy is predetermined and derives two independent subproblems: local ETC and edge ETC. Then, we provide a closed-form optimal solution for the local ETC problem. The edge ETC problem is equivalently transformed into a geometric programming (GP) problem, and we introduce an effective iterative algorithm to obtain a stationary point, utilizing successive convex approximation (SCA). Finally, we design a coordinate descent (CD)-based algorithm to optimize the offloading policy effectively. Extensive simulations demonstrate that the proposed policy significantly outperforms other benchmark methods, achieving near-optimal performance even in the presence of high estimation errors in computing capacity.},
  archive      = {J_TMC},
  author       = {Zhaojun Nan and Yunchu Han and Jintao Yan and Sheng Zhou and Zhisheng Niu},
  doi          = {10.1109/TMC.2025.3539296},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Robust task offloading and resource allocation under imperfect computing capacity information in edge intelligence systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Power optimization for low transmission delay in software
defined data center networks. <em>TMC</em>, 1–14. (<a
href="https://doi.org/10.1109/TMC.2025.3538791">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Software-Defined Data Center Networks (SDDCNs) utilizes Software Defined Networking (SDN) as a network architecture to achieve highly flexible, programmable, and automated management of Data Center Networks (DCNs). The high energy consumption of DCNs remains a persistent and significant challenge. Thus, the energy saving is crucial and imperative for DCNs. Current energy-efficient solutions primarily rely on flow consolidation and dynamic device sleeping techniques to reduce energy consumption. However, these approaches often yield long Flow Completion Time (FCT), potentially resulting in violations of service-level agreements, particularly for delay-sensitive applications. In this paper, we formulate the problem of minimizing the power consumption in SDDCNs as key objective while ensuring timely FCT for delay-sensitive applications. To solve this problem, we first introduce the Active Network Generation (ANG) approach, which generates a minimal active subnet with the least number of active devices while meeting the current traffic demand. Subsequently, we propose two algorithms based on the type of applications: the Delay-Tolerant Flow Route (DTFR) algorithm for delay-tolerant applications and the Delay-Sensitive Flow Route (DSFR) algorithm for delay-sensitive applications. Simulation results demonstrate that our propose solution achieves an energy-saving rate of up to 67.77% and significantly reduces FCT compared to benchmark solutions.},
  archive      = {J_TMC},
  author       = {Yong Zhao and Jiannong Cao and Xingwei Wang and Fuliang Li and Qiang He and Xiaojie Liu},
  doi          = {10.1109/TMC.2025.3538791},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Power optimization for low transmission delay in software defined data center networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-agent deep reinforcement learning with trajectory
prediction for task migration-assisted computation offloading.
<em>TMC</em>, 1–18. (<a
href="https://doi.org/10.1109/TMC.2025.3539945">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Multi-access edge computing has become an effective paradigm to provide offloading services for computation-intensive and delay-sensitive tasks on vehicles. However, high mobility of vehicles usually incurs spatio-temporal load-imbalances among edge servers. Therefore, task migration is employed to maintain dynamic workload balancing by transmitting excessive tasks from overloaded to underloaded servers. Recent studies adopt deep reinforcement learning approaches to generate offloading and migration decisions based on current observations of systems. However, we argue that the migration direction is highly dependent on vehicular movements, and task migration towards the wrong direction could lead to additional delays. Therefore, we emphasize the importance of guiding task migration via exploring prospective trajectories of vehicles. We propose a Mobility-Aware Cooperative Multi-Agent (MCMA) deep reinforcement learning approach to make vehicle-by-vehicle decisions in multi-edge computation offloading scenarios. A two-stage decision framework is designed to solve the joint optimization problem of computation offloading and resource allocation. Additionally, an Informer-based multi-step vehicular trajectory prediction module is incorporated to enhance the capability of forecasting vehicular movements. Extensive experiments and analysis are conducted on synthetic and realistic scenarios, showing that our approach consistently outperforms both heuristic and DRL-based methods. The simulation scenarios and source codes are publicly available here.},
  archive      = {J_TMC},
  author       = {Xinyi Zhang and Chunyang Wang and Yanmin Zhu and Jian Cao and Tong Liu},
  doi          = {10.1109/TMC.2025.3539945},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-agent deep reinforcement learning with trajectory prediction for task migration-assisted computation offloading},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Aeacus: QUIC-powered low-latency and strong-consistency name
resolution in 5 g. <em>TMC</em>, 1–15. (<a
href="https://doi.org/10.1109/TMC.2025.3539590">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Domain Name System (DNS) serves as a foundational networking service, yet its inherent time-to-live (TTL)-based cache mechanism presents a conundrum—striving for both low query latency and robust cache consistency proves challenging. To address this, we introduce Aeacus: a middleware seamlessly integrated into the 5 G core, engineered to optimize name resolution for QUIC. Aeacus adeptly fortifies DNS with substantial cache consistency by capitalizing on QUIC handshake states to detect cache inconsistency, without compromising query delay. Furthermore, Aeacus orchestrates the amalgamation of DNS queries and QUIC handshake messages, effectively truncating one round-trip of message exchange and reviving expired DNS cache to bolster cache hit rates. Our dual-pronged deployment, encompassing both commercial and test 5 G networks, demonstrates Aeacus&#39; prowess. In direct comparison with DNS, Aeacus successfully truncates connection setup delays by a remarkable 8.9% to 71.8%, all while introducing a mere 5.9% overhead attributed to supplementary packet processing and forwarding expenses. Importantly, existing DNS-based systems reap the benefits of Aeacus without necessitating modifications. We demonstrate Aeacus&#39; seamless enhancement of DNS-based load balancers, extending QUIC&#39;s 0-RTT handshake to include 0-RTT connection setup and service migration.},
  archive      = {J_TMC},
  author       = {Xuebing Li and Byungjin Cho and Saimanoj Katta and Jose Costa Requena and Yu Xiao},
  doi          = {10.1109/TMC.2025.3539590},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Aeacus: QUIC-powered low-latency and strong-consistency name resolution in 5 g},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A tractable analysis model of information freshness for
mobile edge computing assisted IoT system with layer-coded HARQ.
<em>TMC</em>, 1–14. (<a
href="https://doi.org/10.1109/TMC.2025.3539662">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This work focuses on an internet of things (IoT) status update system with the assistance of mobile edge computing (MEC) under short packet communications. The edge server includes a successive interference canceller (SIC) and a buffer pool. To simultaneously improve both information timeliness and throughput, a layer-coded hybrid automatic repeat request (L-HARQ) protocol is proposed especially considering backtrack decoding (BD) delay. In the MEC-assisted L-HARQ IoT status update system, the source successively transmits mixing updates to an access point (AP). The mixing update consists of the new generation and the part of the previously failed one. All the undecoded mixing packets are delivered to the edge server. Once a successful feedforward decoding (FD) occurs, the edge server attempts to recover undecoded mixing packets until a BD failure occurs or all buffers empty. The successful BD results are also delivered to the IR and the obtained prior information is fed back to the edge server for the next BD. By employing the stochastic hybrid system (SHS) model, the average age of informations (AoIs) are derived under circle-shift preemption (CS-P) and fixed preemption (F-P) policies. Considering the fact that the number of buffers is limited and the successful FD and BD do not always cause the reduction of system AoI, this work proposes a non-binary age evolution model as well as the state simplification mothed in SHS. The presented numerical results show that the CS-P and F-P policies obtain the enhanced information freshness compared with the non-preemptive blocking policy, and the CS-P policy outperforms the F-P one in terms of information freshness. The average AoI greatly depends on BD depth. When it is small, the average AoI is small. However, the two policies have the same average throughput that mainly depends on the size of buffer pool and the success probabilities of both FD and BD.},
  archive      = {J_TMC},
  author       = {Yue Li and Xiangdong Jia and Xiaoping Ma and Yuxin Guo and Hailong Tian},
  doi          = {10.1109/TMC.2025.3539662},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A tractable analysis model of information freshness for mobile edge computing assisted IoT system with layer-coded HARQ},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Environment independent gait recognition based on wi-fi
signals. <em>TMC</em>, 1–16. (<a
href="https://doi.org/10.1109/TMC.2025.3540011">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Gait recognition plays a pivotal role in the area of mobile computing. While various research approaches leverage images, radar, RF signals, pressure sensors, wearables, and other methods, utilizing Wi-Fi signals for gait recognition offers distinct advantages such as a wide sensing range, simple deployment, and passive sensing capabilities. However, traditional gait recognition systems relying on Wi-Fi signals often suffer from performance degradation due to variations in walking directions and environmental conditions. To address this issue, in this paper we propose EIGait, a gait recognition system based on Wi-Fi signal time-frequency spectrograms. EIGait enhances the robustness and generalizability of extracted features through spectrogram augmentation, self-contrastive learning, and domain-adversarial training. Particularly, improvements to ResNet in EIGait yield a Spectrogram ResNet, which is better suited for time-frequency spectrograms. In addition, using merely a single pair of Wi-Fi transmitter and receiver, and by minimal signal denoising, we achieve the state-of-the-art performance. To evaluate the performance of EIGait, we conduct extensive experiments. In a typical indoor environment, EIGait achieves F1 scores ranging from 98.11% to 98.31% for four to eight individuals. In cross-direction gait recognition, we obtain F1 scores of 96.64% to 94.45% for four to eight individuals. Moreover, under the more challenging conditions of cross-room gait recognition, EIGait attains F1 scores of 92.09% to 89.61% for four to eight individuals. Additionally, we conduct experiments on the public dataset 3.0, and the results also demonstrate significant superiority.},
  archive      = {J_TMC},
  author       = {Wei Yang and Zhixiang Li and Sheng Chen},
  doi          = {10.1109/TMC.2025.3540011},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Environment independent gait recognition based on wi-fi signals},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MmRotation: Unlocking versatility of a single mmWave radar
via horizontal mobility and azimuthal rotation. <em>TMC</em>, 1–16. (<a
href="https://doi.org/10.1109/TMC.2025.3539985">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Indoor mmWave-based sensing technologies have garnered substantial interest from both the industrial and academic. Yet, the intrinsic challenge posed by the limited Field-of-View (FOV) of mmWave radars significantly restricts their coverage. This limitation necessitates careful selection of installation positions and orientations to optimize performance, thereby severely curtailing the versatility and widespread adoption of these systems. Traditionally, expanding coverage involved increasing the number of radar units. This paper introduces a novel approach to enhance the FOV by incorporating mobility, achieved by affixing the radar onto a pan-tilt unit capable of rotating along both the horizontal and azimuthal. Nevertheless, the disparity between the pan-tilt and the radar presents significant challenges for accurately rotating the radar&#39;s orientation. To mitigate this, we propose an automated calibration algorithm for radar and pan-tilt, ensuring precise calibration. Additionally, we have devised a radar orientation adjustment algorithm intended to automatically align the radar&#39;s FOV with the positions of detected objects to facilitate various applications. Through three case studies, we have demonstrated that mmRotation can greatly expand the sensing range, enabling support for multiple applications on a single radar, such as vital signs monitoring and fall detection. Comprehensive experimental results underscore that our system surpasses the current state-of-the-art (SOTA).},
  archive      = {J_TMC},
  author       = {Duo Zhang and Xusheng Zhang and Zhehui Yin and Yaxiong Xie and Hewen Wei and Zhaoxin Chang and Wenwei Li and Daqing Zhang},
  doi          = {10.1109/TMC.2025.3539985},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MmRotation: Unlocking versatility of a single mmWave radar via horizontal mobility and azimuthal rotation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Information freshness in multi-hop satellite IoT systems.
<em>TMC</em>, 1–16. (<a
href="https://doi.org/10.1109/TMC.2025.3540259">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Space-air-ground integration has become paramount in the next generation of wireless communication systems in the era marked by the seamless integration of terrestrial and celestial domains. On the other hand, the age of information (AoI) has recently emerged as a vital metric for evaluating the timeliness and freshness of data in these multi-hop communication systems. This paper focuses on investigating the information freshness of multi-hop satellite IoT systems while considering several automatic repeat request (ARQ) and hybrid ARQ (HARQ) schemes over different hops to promise the reliability of data transmissions. Specifically, a group of remote sensors transmits their data to a terrestrial base station (B) via the code-division multiple access (CDMA) strategy to exploit CDMA&#39;s natural merits, e.g., anti-jamming and simultaneous transmissions. Then, B sends these received data to a data destination (D) via a satellite (R) under the transparent forwarding strategy. We derive the closed form of the outage probability for the CDMA protocol considering multi-user interference (MUI) and the closed form of the end-to-end outage probability for the three kinds of ARQ and HARQ schemes on the dual-hop B-R-D transmission, and then finally derive the expression of the corresponding AoI. Finally, numerical results show that simulations match well with the theoretical results, proving the analysis&#39;s correctness and the pros and cons of the different ARQ/HARQ schemes.},
  archive      = {J_TMC},
  author       = {Ying Ke and Zihan Ni and Di Zhang and Xiaqing Miao and Chee Yen Leow and Shuai Wang and Gaofeng Pan and Jianping An},
  doi          = {10.1109/TMC.2025.3540259},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Information freshness in multi-hop satellite IoT systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Profit maximization of delay-sensitive, differential
accuracy inference services in mobile edge computing. <em>TMC</em>,
1–15. (<a href="https://doi.org/10.1109/TMC.2025.3540017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The integration of Artificial Itelligence (AI) and edge computing has sparked significant interest in edge inference services. In this paper, we consider delay-sensitive, differential accuracy inference services in a Mobile Edge Computing (MEC) network while meeting user stringent delay and accuracy requirements. We formulate two novel profit maximization problems under static and dynamic settings of service request arrivals, with the aim of maximizing the accumulative profit of admitted requests. We assign differential accuracy service requests to the corresponding resolution instances of their requested service models, assuming that each resolution instance can serve up to $L\geq 1$ the same type of service requests. Since the profit maximization problem is NP-hard, we first formulate an Integer Linear Program (ILP) solution if the problem size is small or medium; otherwise, we devise a constant randomized algorithm with high probability. Then, we consider dynamic service request admissions without the knowledge of future request arrivals for a given finite time horizon, for which we develop a simple yet effective prediction mechanism to accurately predict the number of different resolution instances of each model needed, and pre-deploy the predicted number of resolution instances into cloudlets to reduce instantiating delays. We then devise an online algorithm with a provable competitive ratio for the dynamic profit maximization problem by leveraging the primal-dual dynamic updating technique. Finally, we evaluate the performance of the proposed algorithms by simulations. The simulation results demonstrate that the proposed algorithms are promising.},
  archive      = {J_TMC},
  author       = {Yuncan Zhang and Weifa Liang and Zichuan Xu and Xiaohua Jia and Yuanyuan Yang},
  doi          = {10.1109/TMC.2025.3540017},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Profit maximization of delay-sensitive, differential accuracy inference services in mobile edge computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A wearable PPG-based monitoring system for personalized free
weight training. <em>TMC</em>, 1–15. (<a
href="https://doi.org/10.1109/TMC.2025.3540165">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Free weight training (FWT) is of utmost importance for physical well-being. The success of FWT depends largely on choosing the suitable workload, as improper selections can lead to suboptimal outcomes or injury. Current workload estimation approaches rely on manual recording and specialized equipment with limited feedback. Therefore, we introduce PPGSpotter, a wearable PPG-based FWT monitoring system in a convenient, low-cost, and fine-grained manner. By characterizing the arterial geometry compressions caused by the deformation of distinct muscle groups, PPGSpotter can infer essential FWT factors such as current workload, repetitions, and exercise type and provide recommendations for workload adjustment. To remove pulse-related interference, we develop an arterial interference elimination approach based on adaptive filtering, effectively extracting the pure motion-derived signal (MDS). Furthermore, we explore 2D representations of MDS within the phase space to extract spatiotemporal information, enabling PPGSpotter to address the challenge of resisting sensor shifts. Finally, we leverage a multi-task CNN-based network and workload adjustment guidance to achieve personalized FWT monitoring. Extensive experiments with 15 participants confirm that PPGSpotter can achieve promising workload estimation (0.59  kg RMSE), repetitions estimation (0.96  reps RMSE), and exercise type recognition (91.57% F1-score) while providing valid workload adjustment recommendations (0.22  kg RMSE).},
  archive      = {J_TMC},
  author       = {Xiaochen Liu and Fan Li and Yetong Cao and Shengchun Zhai and Binghui Shi and Song Yang and Yu Wang},
  doi          = {10.1109/TMC.2025.3540165},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A wearable PPG-based monitoring system for personalized free weight training},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UCMM: Unsupervised convolutional networks for accurate and
efficient map matching with mobile cellular data. <em>TMC</em>, 1–14.
(<a href="https://doi.org/10.1109/TMC.2025.3540300">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The map matching of cellular data reconstructs real trajectories of users by exploiting the sequential connections between mobile devices and cell towers. The difficulty in obtaining paired cellular-GPS data and the cellular variation compromise the accuracy and reliability of existing map matching approaches. In this paper, we propose a novel unsupervised convolutional network for cellular map matching (UCMM) to address these challenges. UCMM employs a dual encoder-decoder network to capture a shared representation from both the cellular and GPS domains in an unsupervised manner. It leverages a dedicated convolutional architecture to tackle the varying lengths of output sequential data. An attention mechanism is specially introduced to deal with the cellular variation. The effectiveness of UCMM is demonstrated through comprehensive evaluations, which show that UCMM achieves a substantial improvement in matching accuracy and deduction of training time compared with the best-known prior works. These improvements make UCMM a significant advancement in the field of map matching.},
  archive      = {J_TMC},
  author       = {Mingxin Cai and Chen Ma and Yuchen Li and Zhonghao Lyu and Yutong Liu and Linghe Kong and Guihai Chen},
  doi          = {10.1109/TMC.2025.3540300},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {UCMM: Unsupervised convolutional networks for accurate and efficient map matching with mobile cellular data},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Coordinating communication and computing for wireless VR in
open radio access networks. <em>TMC</em>, 1–15. (<a
href="https://doi.org/10.1109/TMC.2025.3540111">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by diverse applications, radio access networks (RAN) are expected to embrace built-in computing and intelligence, forming a versatile wireless computing platform that closely integrates communication and computing. To fully unleash the potential of such a synergistic system, it is essential to coordinate communication and computing with intelligence unlocked by the radio intelligent controllers (RICs) in O-RAN. Building on the groundwork established by existing theoretical studies and simulations, we develop a platform that can emulate the events in the real-world system in more detail, bringing theoretical works closer to practical implementation. In this paper, we first introduce ns-GP-O-RAN, a software simulation platform developed over ns-3, enabling communication, computation task processing, large-scale data collection, and testing of system-level orchestration policies through user-level control. Taking virtual reality (VR) as an example, we formulate the computation offloading problem and develop a prediction-based computation offloading xAPP, which contains a prediction phase to predict users&#39; end-to-end (E2E) performance with the deep neural network and a system-level decision-making phase for global orchestration with the differential evolution algorithm. We evaluate the system capacity and E2E latency over the developed ns-GP-O-RAN, which is more effective than existing approaches.},
  archive      = {J_TMC},
  author       = {Hongtao Li and Ziqi Chen and Fengxian Guo and Nan Li and Yaohua Sun and Mugen Peng and Yuanwei Liu},
  doi          = {10.1109/TMC.2025.3540111},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Coordinating communication and computing for wireless VR in open radio access networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi-dimensional training optimization for efficient
federated synergy learning. <em>TMC</em>, 1–16. (<a
href="https://doi.org/10.1109/TMC.2025.3540566">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge learning (EL) is an end-to-edge collaborative learning paradigm enabling devices to participate in model training and data analysis, opening countless opportunities for edge intelligence. As a promising EL framework, federated synergy learning (FSyL) mitigates the computation and communication overhead on resource-constrained devices by offloading partial model layers to the edge server for synergistic training. Nevertheless, due to the system and statistical heterogeneity, naively using existing FSyL methods is significantly time-consuming and causes accuracy degradation. Motivated by this issue, this paper introduces a novel FSyL framework that integrates multi-dimensional training optimization and formulates the edge learning cost minimization (ELCM) problem. To tackle the ELCM efficiently, we design OL-MG, an OnLine Model Splitting and Resource Provisioning Game. Specifically, we first reformulate and decompose the original ELCM based on data quality evaluation. Then, given a model splitting decision, we determine the optimal resource provisioning in Sub-problem1, based on which optimal model splitting in Sub-problem2 is modeled as a potential game. Subsequently, we introduce a decentralized algorithm to find a Nash equilibrium (NE) solution. Furthermore, we further extend OL-MG to support a budget-aware multi-edge scenario. Extensive experiments demonstrate that the proposed mechanism significantly outperforms state-of-the-art methods in cost-saving and accuracy improvement.},
  archive      = {J_TMC},
  author       = {Shucun Fu and Fang Dong and Runze Chen and Dian Shen and Jinghui Zhang and Qiang He},
  doi          = {10.1109/TMC.2025.3540566},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi-dimensional training optimization for efficient federated synergy learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Delay-sensitive task offloading with edge caching through
martingale-based deep reinforcement learning. <em>TMC</em>, 1–18. (<a
href="https://doi.org/10.1109/TMC.2025.3540413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the forthcoming era of 6 G networks, delay-sensitive applications for Internet of Things (IoT) are poised to become the prevailing services with ultra-reliable and low-latency (URLLC) requirements. Unlike traditional video caching, IoT-based edge caching faces unique challenges due to diverse data types, update frequencies, and computational needs, requiring integrated storage and computational resource management. To support the more stringent requirements for these innovative applications, mobile edge computing (MEC) is introduced to enhance the service reliability of delay-sensitive applications in the 6 G era. However, task offloading, as an indispensable procedure in MEC, would encounter many challenges, such as network jitter and resource insufficiency, possibly leading to unpredictable queuing delays and other negative issues. To ensure reliable services in a dynamical MEC environment, the caching-enabled MEC network has emerged as a novel architecture, placing computing and storage resources in the edge network. In this paper, we investigate the caching-enabled MEC to support reliable task offloading for delay-sensitive applications, with a focus on IoT scenarios. In our system model, we formulate the task process as a two-hop tandem queuing system with limited capacity, including task transmission and computation queues. The Martingale theory is leveraged to analyze the delay violation probability in this system, demonstrating how the offloading and caching decisions affect the end-to-end (E2E) delay. Besides, task offloading and resource allocation policies are integrated to reduce high system costs, including energy consumption and cache resource rental costs. Based on the delay analysis of martingale theory, we propose an advanced deep reinforcement learning (DRL) algorithm called Dynamic Request Aware Soft Actor-Critic (DRA-SAC) algorithm to achieve minimal system costs by obtaining the optimal task offloading and resource allocation policies, including caching and computation resources. We conduct some illustrative studies to evaluate the proposed scheme. The algorithm we have put forward outperforms benchmark algorithms regarding both cache hit ratio and system cost.},
  archive      = {J_TMC},
  author       = {Chongwu Dong and Weidong Li and Zhi Zhou and Xu Chen and ZhiHong Tian and Wushao Wen},
  doi          = {10.1109/TMC.2025.3540413},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Delay-sensitive task offloading with edge caching through martingale-based deep reinforcement learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EarLock: Personal authentication system for hearables using
sound leakage signals. <em>TMC</em>, 1–15. (<a
href="https://doi.org/10.1109/TMC.2025.3540584">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Earphone-type wearable devices, also known as “hearables,” will have many functions in the future. Some of those functions will require authentication of the wearer for access to the user&#39;s privacy information or settlement of payments. In this study, we propose a new personal authentication system for hearables called EarLock. EarLock authenticates the wearer by acquiring and analyzing ear canal and auricle shape information using sound leakage from the device. The system can be implemented using a speaker and external microphone that are highly compatible with hearables. We implemented three prototype devices and investigated EarLock&#39;s authentication performance under various practical scenarios, including walking conditions, noisy environments, and situations with object interference. Experimental results showed that the in-ear, open-ear, and bone-conduction devices achieved balanced accuracy (BAC) scores of 87.2–93.7%, 83.4–94.7%, and 85.9–90.0%.},
  archive      = {J_TMC},
  author       = {Takashi Amesaka and Hiroki Watanabe and Yuta Sugiura and Masanori Sugimoto and Buntarou Shizuki},
  doi          = {10.1109/TMC.2025.3540584},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {EarLock: Personal authentication system for hearables using sound leakage signals},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). An efficient two-stage networking topology design for
mega-constellation of low earth orbit satellites. <em>TMC</em>, 1–15.
(<a href="https://doi.org/10.1109/TMC.2025.3540671">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low Earth Orbit (LEO) satellites play a crucial role in providing high-speed internet to remote areas and ensuring network resilience during outages. The design of efficient satellite constellations requires optimizing network topology, which is a complex task due to the large solution space and the need for fault tolerance. This paper presents the AlphaSat algorithm, a two-phase approach to improve latency and network robustness in LEO constellations. In the initialization phase, Monte Carlo Tree Search (MCTS) is used to generate an initial topology by selecting links from a vast search space. In the refinement phase, an edge-switching method is applied to enhance network resilience and performance. AlphaSat is evaluated on OneWeb, Starlink, and Telesat mega-constellations, demonstrating superior performance over existing algorithms. The results show significant reductions in latency ranging from 4.7% to 44.5% and improvements in network robustness, increasing by 3.3% to 28.3%. Furthermore, AlphaSat effectively balances network load and optimizes power consumption, offering a promising solution for efficient and resilient LEO satellite network design.},
  archive      = {J_TMC},
  author       = {Han Hu and Yifeng Lyu and Kaifeng Song and Rongfei Fan and Cheng Zhan and Jian Yang},
  doi          = {10.1109/TMC.2025.3540671},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {An efficient two-stage networking topology design for mega-constellation of low earth orbit satellites},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Mobility-aware seamless service migration and resource
allocation in multi-edge IoV systems. <em>TMC</em>, 1–17. (<a
href="https://doi.org/10.1109/TMC.2025.3540407">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Edge Computing (MEC) offers low-latency and high-bandwidth support for Internet-of-Vehicles (IoV) applications. However, due to high vehicle mobility and finite communication coverage of base stations, it is hard to maintain uninterrupted and high-quality services without proper service migration among MEC servers. Existing solutions commonly rely on prior knowledge and rarely consider efficient resource allocation during the service migration process, making it hard to reach optimal performance in dynamic IoV environments. To address these important challenges, we propose SR-CL, a novel mobility-aware seamless Service migration and Resource allocation framework via Convex-optimization-enabled deep reinforcement Learning in multi-edge IoV systems. First, we decouple the Mixed Integer Nonlinear Programming (MINLP) problem of service migration and resource allocation into two sub-problems. Next, we design a new actor-critic-based asynchronous-update deep reinforcement learning method to handle service migration, where the delayed-update actor makes migration decisions and the one-step-update critic evaluates the decisions to guide the policy update. Notably, we theoretically derive the optimal resource allocation with convex optimization for each MEC server, thereby further improving system performance. Using the real-world datasets of vehicle trajectories and testbed, extensive experiments are conducted to verify the effectiveness of the proposed SR-CL. Compared to benchmark methods, the SR-CL achieves superior convergence and delay performance under various scenarios.},
  archive      = {J_TMC},
  author       = {Zheyi Chen and Sijin Huang and Geyong Min and Zhaolong Ning and Jie Li and Yan Zhang},
  doi          = {10.1109/TMC.2025.3540407},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Mobility-aware seamless service migration and resource allocation in multi-edge IoV systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Pushing the limits of WiFi-based gait recognition towards
non-gait human behaviors. <em>TMC</em>, 1–17. (<a
href="https://doi.org/10.1109/TMC.2025.3540863">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {WiFi-based gait recognition technologies have seen significant advancements in recent years. However, most existing approaches rely on a critical assumption: users must walk continuously and maintain a consistent body posture. This poses a substantial challenge when users engage in non-periodic or discontinuous behaviors (e.g., stopping, starting, or turning mid-walk), which can disrupt the extraction of gait-related features and degrade recognition performance. To address this issue, we propose freeGait, a novel approach designed to mitigate the impact of non-gait behaviors in WiFi-based gait recognition systems. Our solution models this problem as domain adaptation, where we learn domain-independent representations to isolate gait features from behavior-dependent noise. We treat human behaviors with labeled user data as source domains and behaviors without user labels as target domains. However, applying domain adaptation directly is challenging due to the ambiguous classification boundaries in the target domains for WiFi signals. To overcome this, we align the posterior distributions between the source and target domains and constrain the conditional distribution within the target domains to enhance gait classification accuracy. Additionally, we implement a data augmentation module to generate data resembling the labeled data, while supervised learning ensures distinctiveness between users. Our experiments, conducted with 20 participants across 3 different scenarios, demonstrate that freeGait can accurately predict data across 15 domains by labeling only a small subset from 6 source domains, achieving up to a 45% improvement in user classification accuracy compared to existing methods.},
  archive      = {J_TMC},
  author       = {Dawei Yan and Panlong Yang and Fei Shang and Feiyu Han and Yubo Yan and Xiang-Yang Li},
  doi          = {10.1109/TMC.2025.3540863},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Pushing the limits of WiFi-based gait recognition towards non-gait human behaviors},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Environment-tolerant trust opportunity routing based on
reinforcement learning for internet of underwater things. <em>TMC</em>,
1–13. (<a href="https://doi.org/10.1109/TMC.2025.3540774">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The Internet of Underwater Things (IoUT) has garnered significant interest due to its potential applications in monitoring underwater environments. However, the unique characteristics of acoustic communication, such as long propagation delays and high attenuation, present considerable obstacles for achieving efficient and dependable data transmission. Opportunistic routing is a crucial technique for enhancing packet delivery ratios by selecting a set of forwarding nodes and utilizing their cooperative forwarding to boost network throughput. Nevertheless, choosing an excessive number of forwarding nodes can lead to wasteful energy usage and extended communication delays. Moreover, the overlooked trustworthiness of forwarded nodes in most research works can undermine the effectiveness of opportunistic routing. Therefore, this study presents a novel trust opportunistic routing scheme that employs reinforcement learning to achieve resilience in constantly changing underwater settings. The combination of reinforcement learning and trust management enables the proposed opportunistic routing scheme to adapt to the unstable underwater environment and unknown malicious attacks. Initially, a method is introduced for measuring environmental fitness by considering multiple trust factors, including communication success rate, data reliability, and location dynamics. The proposed scheme then uses reinforcement learning to develop a reliable opportunistic routing method based on quantified state information. This component employs the obtained state to formulate action strategies and obtains reward values from environmental inputs. The reward update equation integrates these qualities to optimize the deployment of superior action strategies, finally achieving trust opportunistic routing for underwater data collection. Fundamental experimental results demonstrate that the proposed protocol performs exceptionally well in demanding underwater conditions, outperforming existing methods in packet transmission rate, energy efficiency, and end-to-end delay.},
  archive      = {J_TMC},
  author       = {Yu He and Guangjie Han and Yun Hou and Chuan Lin},
  doi          = {10.1109/TMC.2025.3540774},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Environment-tolerant trust opportunity routing based on reinforcement learning for internet of underwater things},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust and effort-efficient image-based indoor localization
with generative features. <em>TMC</em>, 1–18. (<a
href="https://doi.org/10.1109/TMC.2025.3541045">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Image-based indoor localization using smartphones has gained wide popularity in daily life. Existing solutions usually utilize visual landmarks and extract fingerprint features for localization, so the fingerprint density directly affects the localization accuracy. However, the collection of dense fingerprints with high resolution is extremely labor-intensive during site surveys and faces heavy computation/storage overhead during matching fingerprint features. Meanwhile, the efficient extraction of features restricts users to shoot with specific poses, otherwise the localization accuracy will be seriously decreased. To tackle these challenges, we propose an Automated Real-time Generative Image Localization System, named ARGILS. Our core idea is to replace the dense sampling with the cross-sparse sampling and generate fingerprint features for missing locations, and finally obtain locations quickly and efficiently through feature orthogonal decomposition. Specifically, cross-sparse sampling refers to fingerprint sampling to achieve full scene coverage and facilitate the generation of missing fingerprint features. We design a distance-constrained generative adversarial network to generate features for missing locations to extend the database, ensuring high localization accuracy with sparse sampling. Additionally, we develop an orthogonal feature extraction method that decomposes image features into horizontal and vertical directions in 2D space. Then two-stage retrieval is used in both directions to obtain the precise location. To address the impact of obstacles, we introduce a scanning localization that improves localization robustness through keyframe filtering and clustering. We have implemented ARGILS and performed extensive real-world evaluations. Experiment results show that when reducing 75% site survey effort, the average location error of ARGILS is around 2.5m in a shopping mall, 48% lower than state-of-the-art methods. ARGILS can also efficiently speed up the localization process, with the time consumption ranging from 0.1 to 0.3 seconds on smartphones of various configurations.},
  archive      = {J_TMC},
  author       = {Zhenhan Zhu and Yanchao Zhao and Maoxing Tang and Yanling Bu and Hao Han},
  doi          = {10.1109/TMC.2025.3541045},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Robust and effort-efficient image-based indoor localization with generative features},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PicaCAN: Reverse engineering physical semantics of signals
in CAN messages using physically-induced causalities. <em>TMC</em>,
1–17. (<a href="https://doi.org/10.1109/TMC.2025.3541102">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {With the rapid development of Connected and Autonomous Vehicles, In-Vehicle Network attacks have garnered heightened research scrutiny due to vehicles&#39; increasing connectivities to the external environment. The common characteristic among these attacks is to tamper with targeted powertrain-related signals in the Powertrain Controller Area Network (PT-CAN) and further physically threaten vehicles&#39; safety. These powertrain-related signals are encoded within CAN messages grounded by the syntax specification, which is proprietary to Original Equipment Manufacturers and publicly unavailable. Thus, to undertake comprehensive security analysis and strategies, reverse engineering PT-CAN to the semantic level is urgently needed. However, the existing methods rely on interactions (injecting challenge signals/actions) with the targeted vehicle, and certain manual efforts are required. To fill this gap, we propose PicaCAN, a novel framework to extract signals from CAN messages and reverse engineer their physical semantics based on physically induced causality. Once access to the CAN traffic, PicaCANoffers the researcher an eye on the vehicle&#39;s powertrain system, decoding binaries flows into powertrain-related signals automatically. We experimentally evaluate PicaCANon PT-CAN of three automobiles containing two power types. The experimental results show that PicaCANcould successfully extract physical signals representing all targeted semantics (pedals, engine speed, etc.) from two Internal Combustion Engine Vehicles and one Hybrid Electric Vehicle under EV mode.},
  archive      = {J_TMC},
  author       = {Yucheng Ruan and Chengcheng Zhao and Zeyu Yang and Yuanchao Shu and Peng Cheng and Jiming Chen},
  doi          = {10.1109/TMC.2025.3541102},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {PicaCAN: Reverse engineering physical semantics of signals in CAN messages using physically-induced causalities},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing federated semantic learning in distributed
AIGC-enabled human digital twins: A multi-criteria and multi-shard user
selection framework. <em>TMC</em>, 1–18. (<a
href="https://doi.org/10.1109/TMC.2025.3541191">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Artificial intelligence-generated content (AIGC) has been proposed as a solution to meet the requirements of ultra-reliable, secure, and privacy-preserving connectivity in human digital twin (HDT) networks. In such an AIGC-enhanced HDT, contents representing the true statuses of physical twins are generated in the virtual environment for the immediate update and evolution of the corresponding virtual twins (VTs). However, adopting a distributed AIGC in HDT presents several challenges, including the need for personalized VTs, data privacy concerns, and insufficient contextual understanding. This paper introduces a multi-layer federated semantic learning framework to address these challenges, incorporating batch learning to meet the training requirements for semantic-channel encoders and decoders. Furthermore, we introduce a novel user association framework to maximize the overall system performance under shard formation constraints. We then formulate a long-term joint optimization problem for user selection over finite learning periods. A novel Lyapunov-based online optimization strategy was proposed to mitigate the impact of time-varying and unpredictable training conditions. Additionally, we introduce a multi-arm bandit-based method and a context-centric user selection approach to solve the optimization problem. The results demonstrate that the proposed user association framework addresses the limitations of existing approaches, thereby improving the overall performance of the multi-shard AIGC-enhanced HDT.},
  archive      = {J_TMC},
  author       = {Samuel D. Okegbile and Haoran Gao and Oluwasegun Talabi and Jun Cai and Dusit Niyato and Xuemin Shen},
  doi          = {10.1109/TMC.2025.3541191},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Optimizing federated semantic learning in distributed AIGC-enabled human digital twins: A multi-criteria and multi-shard user selection framework},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CAMTUNER: Adaptive video analytics pipelines via real-time
automated camera parameter tuning. <em>TMC</em>, 1–16. (<a
href="https://doi.org/10.1109/TMC.2025.3540667">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Video Analytics Pipelines (VAP), Analytics Units (AUs) such as object detection and face recognition operating on remote servers rely heavily on surveillance cameras to capture high-quality video streams to achieve high accuracy. Modern network cameras offer an array of parameters that directly influence video quality. While a few of such parameters, e.g., exposure, focus and white balance, are automatically adjusted by the camera internally, the others are not. We denote such camera parameters as non-automated (NAUTO) parameters. In this work, we first show that in a typical surveillance camera deployment, environmental condition changes can have significant adverse effect on the accuracy of insights from the AUs, but such adverse impact can potentially be mitigated by dynamically adjusting NAUTO camera parameters in response to changes in environmental conditions. Second, since most end-users lack the skill or understanding to appropriately configure these parameters and typically use a fixed parameter setting, we present CAMTUNER, to our knowledge, the first framework that dynamically adapts NAUTO camera parameters to optimize the accuracy of AUs in a VAP in response to adverse changes in environmental conditions. CAMTUNER is based on SARSA reinforcement learning and it incorporates two novel components: a light-weight analytics quality estimator and a virtual camera that drastically speed up offline RL training. Our controlled experiments and real-world VAP deployment show that compared to a VAP using the default camera setting, CAMTUNER enhances VAP accuracy by detecting 15.9% additional persons and 2.6%-4.2% additional cars (without any false positives) in a large enterprise parking lot. CAMTUNER opens up new avenues for elevating video analytics accuracy, transcending mere incremental enhancements achieved through refining deep-learning models.},
  archive      = {J_TMC},
  author       = {Sibendu Paul and Kunal Rao and Giuseppe Coviello and Murugan Sankaradas and Y. Charlie Hu and Srimat T. Chakradhar},
  doi          = {10.1109/TMC.2025.3540667},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CAMTUNER: Adaptive video analytics pipelines via real-time automated camera parameter tuning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). 3D cooperative positioning via RIS and sidelink
communications with zero access points. <em>TMC</em>, 1–18. (<a
href="https://doi.org/10.1109/TMC.2025.3541575">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconfigurable intelligent surfaces (RISs) are expected to be a main component of future 6 G networks due to their capability to create a controllable wireless environment, achieve extended coverage, and improve localization accuracy. In this paper, we present a novel cooperative positioning use case of the RIS in mmWave frequencies and show that in the presence of RIS, together with sidelink communications, localization with zero access points (APs) is possible. We show that multiple (at least three) half-duplex single-antenna user equipments (UEs) can cooperatively estimate their positions through device-to-device communications with a single RIS as an anchor without the need for any APs. We start by formulating a three-dimensional positioning problem with Cramér-Rao lower bound (CRLB) derived for performance analysis. After that, we discuss the RIS profile design and the power allocation strategy between the UEs. Then, we propose low-complexity estimators for estimating the channel parameters and UEs&#39; positions. Finally, we evaluate the performance of the proposed estimators and RIS profiles in the considered scenario via extensive simulations and show that sub-meter level positioning accuracy can be achieved under multi-path propagation.},
  archive      = {J_TMC},
  author       = {Mustafa Ammous and Hui Chen and Henk Wymeersch and Shahrokh Valaee},
  doi          = {10.1109/TMC.2025.3541575},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {3D cooperative positioning via RIS and sidelink communications with zero access points},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Enhanced VR experience with edge computing: The impact of
decoding latency. <em>TMC</em>, 1–18. (<a
href="https://doi.org/10.1109/TMC.2025.3541741">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Virtual reality (VR) applications have revolutionized digital interaction by providing immersive experiences. $360^{\circ }$ VR video streaming has experienced significant growth and popularity as a pivotal VR application. However, the combination of limited network bandwidth and the demand for high-quality videos frequently hinders the achievement of a satisfactory quality of experience (QoE). Although prior methods have enhanced QoE, the effects of decoding latency have been poorly studied. It is technically challenging to design a quality adaptation algorithm that can balance the pursuit of high-quality videos and the limitation of limited bandwidth resources. To address this challenge, we propose an edge-end architecture for $360^{\circ }$ VR video streaming and aim to enhance overall QoE by solving a performance optimization problem. Specifically, our experiments on commercial mobile devices in real-world situations reveal that decoding latency significantly influences QoE. First, decoding latency plays a major role in contributing to end-to-end latency, which exceeds the transmission latency. Second, decoding latency can differ considerably between devices with varying computational capabilities. Building on this insight, we propose a novel latency-aware quality adaptation (LAQA) algorithm. LAQA lies in developing a solution that can allocate video quality in real-time and enhance overall QoE. LAQA involves not only the quality of the received content, the transmission latency and the quality variance, but also the decoding latency and the fairness of the user quality. Subsequently, we formulate a combinatorial optimization problem to maximize overall QoE. Through extensive validation with experimental data from real-world situations, LAQA offers a promising approach to enhance QoE and ensure fairness performance in different devices. In particular, LAQA achieves 16.77% and 10.66% enhancement over the state-of-the-art combinatorial optimization and reinforcement learning algorithm, respectively, in terms of QoE at 4K resolution. Furthermore, LAQA ensures excellent scalability by simulating the number of users ranging from 15 to 60, making it a robust solution for diverse and growing user scales. We have open-sourced our implementations on https://github.com/Gemwise/LAQA.},
  archive      = {J_TMC},
  author       = {Liang Huang and Yuqi Li and Hongyuan Liang and Kaikai Chi and Yuan Wu},
  doi          = {10.1109/TMC.2025.3541741},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Enhanced VR experience with edge computing: The impact of decoding latency},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Adaptive 360-degree streaming: Optimizing with multi-window
and stochastic viewport prediction. <em>TMC</em>, 1–14. (<a
href="https://doi.org/10.1109/TMC.2025.3541748">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The tile-based approach is widely adopted in adaptive 360-degree video streaming systems, due to its efficiency in managing limited bandwidth resources. Recently, significant research efforts have been devoted to viewport-prediction-enabled bitrate adaptation for tile-based 360-degree Adaptive Bit-Rate (ABR) streaming, towards improving the average video quality while reducing rebuffering. However, the inherent uncertainty of users&#39; viewports has posed limitations on users&#39; Quality of Experience (QoE) for tile-based 360-degree ABR streaming. In this paper, we introduce a multi-window and stochastic viewport prediction approach to address the viewport uncertainty. In particular, considering our goal of maximizing the expectation of future QoE, we investigate a viewport distribution prediction model, to cope with the inherent randomness. Additionally, to accommodate the varying gap between the playback and the download process, we explore the multiple-window viewport prediction models to capture different prediction gaps. Even with the utilization of distributional prediction and multi-window models, predicting viewports far into the future is still inherently challenging. Accordingly, we propose a patience pattern temporarily suspending the download process, allowing for the accumulation of additional head movement trajectory data. Finally, we employ a model predictive control (MPC) approach for sequential decision-making, formulating the MPC problem as a mixed-integer non-linear programming (MINLP) task. To mitigate the computational burden associated with solving MINLP, we introduce a mixed-integer linear programming transformation to achieve efficient decision-making. Extensive experiments, utilizing real-world traces and user head movement trajectories, demonstrate that the proposed method outperforms state-of-the-art methods, improving overall QoE performance by 16.75% - 18.91%.},
  archive      = {J_TMC},
  author       = {Weichao Feng and Shuoyao Wang and Yu Dai},
  doi          = {10.1109/TMC.2025.3541748},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Adaptive 360-degree streaming: Optimizing with multi-window and stochastic viewport prediction},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Query-aware semantic encoder-based resource allocation in
task-oriented communications. <em>TMC</em>, 1–17. (<a
href="https://doi.org/10.1109/TMC.2025.3541636">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Task-oriented communications with semantic encoders are promising to enhance the communication efficiency, by selecting and transmitting valuable data according to task requirements/queries. However, existing semantic encoders lack the capability to track the changing in queries, leading to biased data selection. This paper proposes a query-aware semantic encoder, i.e., Query-Data Cross (QDC) encoder for task-oriented communications. By consistently focusing on data features that are most relevant to the current query at the transmitter, QDC can adapt to changing queries. Based on the dynamic semantic relevance obtained by QDC, a relevance-based data selection and bandwidth allocation optimization (RDSBA) problem is formulated, considering a multi-device task-oriented communication system, where devices should transmit valuable data with high relevance to the queries broadcasted by the base station (BS). RDSBA aims to maximize the data profit of all devices, which is defined as the difference between the relevance of data selected for the BS and the cost of obtaining the data. Then, a DRL-based data selection and bandwidth allocation (DRL-DB) algorithm is proposed to solve the NP-hard optimization problem. Simulation results demonstrate that QDC can smartly track the changing in queries and achieve an accuracy of at least 85% in relevance evaluation, more than 8% higher than existing schemes. Based on the relevance provided by QDC, the proposed RDSBA scheme with DRL-DB can increase the data profit by at least 18%, comparing to existing schemes.},
  archive      = {J_TMC},
  author       = {Qing Cai and Yiqing Zhou and Ling Liu and Hanxiao Yu and Yihao Wu and Ningzhe Shi and Jinglin Shi},
  doi          = {10.1109/TMC.2025.3541636},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Query-aware semantic encoder-based resource allocation in task-oriented communications},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Robust motion-guided frame sampler with interpretive
evaluation for video action recognition. <em>TMC</em>, 1–12. (<a
href="https://doi.org/10.1109/TMC.2025.3541580">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Due to the presence of redundancy and interference, frame sampling is a promising but challenging solution to mitigate the expensive computation of video action recognition. Although the motion prior has shown great potential for frame selection, existing motion-based strategies suffer from limitations in terms of robustness and interpretive evaluation. In this paper, we devise a robust frame sampling strategy called positive motion guided sampler (PMGSampler). It consists of two procedures, local motion capture and global motion statistics. At the local level, we propose two concepts about inter-frame motion amplitude and motion continuity, which helps to perceive the movement of subjects and identify abnormal events that may generate negative pseudo-motion information. Then, through a global analysis of the obtained local motions, the sampler becomes more sensitive to informative frames and robust to outliers. The proposed sampler can be applied to most existing models for improving recognition accuracy. We conduct extensive experiments on four widely-used benchmarks to demonstrate the superiority of our PMGSampler over other methods of the same type. In addition, to analyse how sampled frames influence action recognition, we present a visual interpretation method for video models, termed as spatio-temporal class activation map (STCAM). By introducing spatial and temporal branches, our STCAM is able to visualise the salience of spatio-temporal features. With the help of STCAM, we can further intuitively evaluate the performance of different sampling strategies. Code is available at https://github.com/zyx-cv/PMGSampler.},
  archive      = {J_TMC},
  author       = {Jing Bai and Yuxiang Zhang and Yiran Wang and Zhu Xiao and Yong Xiong and Licheng Jiao},
  doi          = {10.1109/TMC.2025.3541580},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Robust motion-guided frame sampler with interpretive evaluation for video action recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Distributed physical layer authentication framework
exploiting array pattern feature for mmWave MIMO systems. <em>TMC</em>,
1–14. (<a href="https://doi.org/10.1109/TMC.2025.3541725">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Authentication in millimeter-Wave (mmWave) Multiple-Input Multiple-Output (MIMO) systems is a critical issue due to the unique characteristics of mmWave communication, such as highly directional beamforming and the ability to support massive device connectivity. To address this challenge, this paper proposes a novel low-complexity decision-level-based Distributed Physical Layer Authentication (DPLA) framework to combat identity-based impersonation attacks in mmWave MIMO systems. The DPLA framework leverages Beam Pattern (BP) deviation, which arises from hardware-specific gain errors, as a key authentication feature. A fusion center is introduced to make the final authentication decision by aggregating local decisions from multiple collaborative nodes, enabling multi-directional perception. Specifically, a low-complexity hybrid combining fusion rule is carefully designed to accommodate the fully connected structure of mmWave MIMO systems, balancing computational efficiency and authentication performance. A rigorous performance analysis is conducted by deriving closed-form analytical expressions for the probabilities of correct detection and false alarm. Furthermore, the asymptotic detection and discrimination performance are systematically analyzed in the large-scale antenna regime. To further enhance authentication accuracy, digital signaling matrices are designed using the deflection coefficient maximization principle. The feasibility of the proposed framework is validated through a comprehensive evaluation, demonstrating its superior robustness and efficiency compared to benchmark methods.},
  archive      = {J_TMC},
  author       = {Pinchang Zhang and Keshuang Han and Yuanyu Zhang and Yulong Shen and Fu Xiao and Xiaohong Jiang},
  doi          = {10.1109/TMC.2025.3541725},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Distributed physical layer authentication framework exploiting array pattern feature for mmWave MIMO systems},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). S2A-P2FS: Secure storage auditing with privacy-preserving
flexible data sharing in cloud-assisted industrial IoT. <em>TMC</em>,
1–17. (<a href="https://doi.org/10.1109/TMC.2025.3538057">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of the Industrial Internet of Things (IIoT) has led to an explosion of industrial data. Due to computing and storage capacity limitations, IIoT devices often outsource the collected data to remote cloud servers. Unfortunately, cloud storage and cloud sharing services are not as reliable as they claim to be. Existing schemes aim to check data integrity in the cloud through cloud auditing. However, they suffer from a number of security and privacy vulnerabilities. The challenge of designing a secure storage auditing framework for industrial IoT comes from two aspects: 1) lack of physical protection of data owner IIoT devices; 2) privacy issues due to auditing of sensitive shared data. Inspired by the aforementioned challenges, we design the secure storage audit framework to support flexible cloud data sharing in IIoT: S2A-P2FS. The first contribution in our work is the Polynomial Prefix Message Authentication Code(P2MAC) design. We design an innovative P2MAC data structure as a label, which can simultaneously achieve efficient data verification in cloud data storage and privacy protection in flexible cloud data sharing for cloud auditing. The second contribution is the design of a unique Physical Unclonable Function(PUF) for IIoT. Harsh industrial conditions hinder the stable operation of PUFs. To protect the trustness of IIoT data owners, we propose a robust PUF-based physical protection mechanism for IIoT devices. The key point is that the required key is not stored in the memory of IIoT but hidden within its physical structure. A security analysis was conducted to demonstrate the robustness of S2A-P2FS against known vulnerabilities. A prototype was implemented in a real-world IIoT scenario. Experimental results indicate that, compared to state-of-the-art schemes, S2A-P2FS achieves over a 3x speedup in computational time and requires only 67.5% of the communication cost.},
  archive      = {J_TMC},
  author       = {Xiaohu Shan and Haiyang Yu and Yurun Chen and Yuwen Chen and Zhen Yang},
  doi          = {10.1109/TMC.2025.3538057},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {S2A-P2FS: Secure storage auditing with privacy-preserving flexible data sharing in cloud-assisted industrial IoT},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Feature-based machine unlearning for vertical federated
learning in IoT networks. <em>TMC</em>, 1–14. (<a
href="https://doi.org/10.1109/TMC.2025.3530529">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In the era of the Internet of Things (IoT), managing the deluge of data generated by distributed devices presents unique challenges, particularly concerning privacy and the efficient use of computational resources. Vertical Federated Learning (VFL) offers a promising avenue for collaborative machine learning without centralizing data, thereby addressing privacy concerns inherent in traditional approaches. However, as data privacy laws and personal data deletion requests become more prevalent, the necessity for effective machine unlearning strategies within VFL frameworks grows increasingly important. To this end, this paper introduces a novel approach to feature-based machine unlearning tailored specifically for VFL systems in IoT networks. Our methodology enables the selective removal of data influence from trained models without the need for full retraining, thus preserving model utility while ensuring compliance with privacy requirements. By integrating a combination of feature relevance measuring techniques and efficient communication protocols, our solution minimizes the data footprint on network nodes, reduces bandwidth consumption, and maintains the integrity and performance of the learning models. To the best of our knowledge, our proposed framework represents the first practical approach to enable machine unlearning within vertical federated learning environments. We demonstrate the effectiveness of our approach through rigorous evaluation using several IoT datasets, highlighting significant improvements in unlearning efficiency and model robustness compared to existing techniques. Our work not only furthers the development of sustainable and compliant machine learning models in IoT but also sets a foundational framework for future research in secure and efficient data management within federated environments.},
  archive      = {J_TMC},
  author       = {Zijie Pan and Zuobin Ying and Yajie Wang and Chuan Zhang and Weiting Zhang and Wanlei Zhou and Liehuang Zhu},
  doi          = {10.1109/TMC.2025.3530529},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Feature-based machine unlearning for vertical federated learning in IoT networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Symbiotic resource pricing in the computing continuum era.
<em>TMC</em>, 1–14. (<a
href="https://doi.org/10.1109/TMC.2025.3542017">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Though extensive research efforts have been devoted to the problem of computing resource pricing, they mainly focus on single computing paradigms. In this paper, we provide a holistic approach to this problem, by treating the whole computing continuum, consisting of cloud, edge, and fog computing providers, simultaneously offering their resources to the users. Within such a complex setting, we establish the concept of symbiotic computing resource pricing and sharing, where the computing providers and the users coexist within a mutually beneficial ecosystem, sharing services and resources as a means of ensuring their business survival and service satisfaction. Under this prism, we introduce two key pricing families, namely the non-cooperative one which involves competition and is treated through game theoretic approaches, and the cooperative resource pricing (full or partial), which addresses complex scenarios through optimization and coalition. A thorough performance assessment is provided, through modeling and simulation, in order to highlight and quantify the key characteristics and tradeoffs of the various resource pricing approaches introduced.},
  archive      = {J_TMC},
  author       = {Aisha B Rahman and Panagiotis Charatsaris and Eirini Eleni Tsiropoulou and Symeon Papavassiliou},
  doi          = {10.1109/TMC.2025.3542017},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Symbiotic resource pricing in the computing continuum era},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Truthful online combinatorial auction-based mechanisms for
task offloading in mobile edge computing. <em>TMC</em>, 1–14. (<a
href="https://doi.org/10.1109/TMC.2025.3542135">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile edge computation (MEC) is envisioned as a prospective approach for processing the computation-intensive and delay-sensitive tasks of smart mobile devices (SMDs) through offloading them to base stations (BSs) nearby. In fact, efficient task offloading mechanisms are crucial to accomplish an MEC system. The key challenge is to make on-spot decisions upon the arrival of each task and at the same time achieve truthfulness of each SMD. The challenge further escalates, when the unique characteristics of an MEC system, such as locality constraint, delay constraint, etc., are explicitly considered. To solve the challenge, we present a truthful online combinatorial auction-based mechanism (TOCA) for task offloading in an MEC system. Specifically, we first devise the candidate offloading scheme determination algorithm, aiming to determine the candidate offloading schemes of an SMD upon the arrival of its task. Next, we devise the winning offloading scheme selection and pricing algorithm based on the online primal-dual optimization framework, to decide the winning scheme among the SMD&#39;s candidate offloading schemes and calculate its payment. By solid theoretical analysis, we verify that TOCA achieves truthfulness, individual rationality and computational efficiency and a smaller competitive ratio. Trace-driven simulation studies validate the effectiveness and efficacy of TOCA.},
  archive      = {J_TMC},
  author       = {Xueyi Wang and Xingwei Wang and Chen Wang and Rongfei Zeng and Lianbo Ma and Qiang He and Min Huang},
  doi          = {10.1109/TMC.2025.3542135},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Truthful online combinatorial auction-based mechanisms for task offloading in mobile edge computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimization-inspired graph neural network for cellular
network optimization. <em>TMC</em>, 1–14. (<a
href="https://doi.org/10.1109/TMC.2025.3542434">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of wireless communications have driven the need for careful optimization of network parameters to improve network performance and reduce operational cost. Traditional methods, however, struggle with the vast number of tunable parameters and lack scalability in diverse network scenarios. To address these challenges, this paper introduces an optimization-inspired bipartite graph neural network (Bi-GNN) approach for scalable network optimization. Our approach leverages the bipartite structure of network topologies, and incorporates a message-passing mechanism by unfolding the Zeroth-Order Block Coordinate Projected Gradient Descent (ZO-BCPGD) algorithm, which ensures not only high-performance optimization but also manageable computational demand. We demonstrate the permutation and dimensionality equivariance property of the Bi-GNN, which significantly enhances the model&#39;s generalizability across various network structures and sizes. Furthermore, we theoretically analyze the expressive power and generalization ability of the Bi-GNN, demonstrating its adeptness at complex network optimization tasks. The training process, parallel execution, and practical implementation techniques are also discussed to ensure the model&#39;s applicability in real-world scenarios. Numerical results verify that the Bi-GNN outperforms existing methods in both coverage ratios and computational cost. Furthermore, our approach exhibits robust scalability across various network scenarios, making it a versatile tool for optimizing a wide range of wireless networks.},
  archive      = {J_TMC},
  author       = {Pengcheng He and Yijia Tang and Fan Xu and Qingjiang Shi},
  doi          = {10.1109/TMC.2025.3542434},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Optimization-inspired graph neural network for cellular network optimization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint mobile energy replenishment and data gathering in
wireless sensor networks via federated deep reinforcement learning.
<em>TMC</em>, 1–13. (<a
href="https://doi.org/10.1109/TMC.2025.3543009">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Recent years have witnessed the proliferation of wireless energy transfer for Wireless Sensor Networks (WSNs), which are mainly used for data gathering in real-world applications. A number of studies have investigated mobile vehicle scheduling to charge sensor nodes via wireless Mobile Chargers (MCs). Unfortunately, most of them cannot parallelly charge all nodes in an intelligent manner with the global network attributes. Furthermore, the time-variable charging ignores the optimal data gathering, resulting in poor Joint Energy Replenishment and Data Gathering (JERDG). To fill this gap, this paper proposes a Federated Deep Reinforcement Learning (FDRL)-based JERDG (FERG) solution for WSNs. To this end, FERG firstly partitions the networks into a set of clusters to distribute the workload evenly among multiple MCs, and then designs an FDRL-based framework that incorporates various time-variant network attributes to determine the optimal schedule for charging and data gathering via multiple MCs and a base station (BS). The BS as the cloud server is responsible for global training of JERDG models, while multiple MCs will parallelly train local models to jointly charge energy-exhausted nodes and gather the data from all nodes in clusters. To reserve more personalized characteristics of each cluster, a density-based partial aggregation strategy is designed to train the global model. Furthermore, a reward-weighted update and selection solution is proposed to generate and exploit reference samples with high rewards. Simulation results obtained from various scenarios demonstrate that FERG significantly outperforms the state-of-the-art approaches in terms of network lifetime, energy efficiency and data collection latency.},
  archive      = {J_TMC},
  author       = {Haojun Huang and Junhao Zhang and Bang Wang and Wang Miao and Geyong Min},
  doi          = {10.1109/TMC.2025.3543009},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint mobile energy replenishment and data gathering in wireless sensor networks via federated deep reinforcement learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Optimizing monitoring utility of unmanned aerial vehicles
considering adverse effects. <em>TMC</em>, 1–17. (<a
href="https://doi.org/10.1109/TMC.2025.3543399">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {For Unmanned Aerial Vehicles (UAVs) monitoring tasks, capturing high quality images of target objects is important for subsequent recognition. Concerning the problem, many prior works study placement/trajectory planning for UAVs to maximize the quality of captured images. However, all of them overlook a fact that UAV monitoring may cause a huge risk/annoyance on living objects. In this paper, we investigate the novel problem of oPtimizing unmanned aErial vehicles plAcement by Considering both monitoring utility and adverse Effects (PEACE). We propose an approach to solve PEACE, which is proved to be NP-hard. Overall, our approach achieves a $1- \frac{1}{e}-\varepsilon$ approximation ratio. First, we approximate the original problem of PEACE as a classical problem of Monotone Submodular function Maximization under a Uniform Matroid constraint (MSMUM) with a controlled gap. Then, for MSMUM, we propose a combination of algorithms achieving a $1-\frac{1}{e}$ approximation and $O(n\log n)$ time complexity considering the correlation among the UAV monitoring strategies. The proposed algorithms outperform existing algorithms for MSMUM through theoretical analysis and experimental results. Extensive simulations and field experiments demonstrate the effectiveness of our approach, achieving performance gains of 9.0% to 1434.5% compared to existing methods.},
  archive      = {J_TMC},
  author       = {Haihan Zhang and Haipeng Dai and Yu Qiu and Enze Yu and Ruiben Zhou and Weijun Wang and Jingwu Wang and Guihai Chen},
  doi          = {10.1109/TMC.2025.3543399},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Optimizing monitoring utility of unmanned aerial vehicles considering adverse effects},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Dynamic distributed model compression for efficient
decentralized federated learning and incentive provisioning in edge
computing networks. <em>TMC</em>, 1–18. (<a
href="https://doi.org/10.1109/TMC.2025.3543295">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We study decentralized federated learning (DFL) in edge computing networks where edge nodes (ENs) collaboratively train their artificial intelligence (AI) models in a serverless manner without sharing local data. We consider the following critical DFL challenges: i) scarce bandwidth resources of ENs; ii) dynamic, heterogeneous edge environment; iii) incentive provisioning and complex tradeoffs between the DFL performance and training costs. To resolve these challenges, we develop a new model compression method where ENs utilize dynamic, non-identical compression rates to improve the communication efficiency of DFL under time-varying, heterogeneous resource constraints. We show that our method can be formulated as a graphical Markov potential game where ENs act as players deciding on their compression factors and the number of data samples used for model updates. Each EN is incentivized to participate in DFL through rewards based on the EN&#39;s contribution to training. We prove that our game has a dominant pure-strategy Nash equilibrium (NE) maximizing its potential function and propose a dynamic distributed compression algorithm in which each EN can find its dominant strategy independently. We show that this algorithm converges to the Pareto-optimal NE, representing the most efficient solution of our game enhancing the DFL performance with minimal costs.},
  archive      = {J_TMC},
  author       = {Alia Asheralieva and Dusit Niyato and Xuetao Wei},
  doi          = {10.1109/TMC.2025.3543295},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Dynamic distributed model compression for efficient decentralized federated learning and incentive provisioning in edge computing networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Average reliability-optimal offloading for mobile edge
computing in LowLatency industrial IoT networks. <em>TMC</em>, 1–14. (<a
href="https://doi.org/10.1109/TMC.2025.3541661">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we consider a multi-access mobile edge computing (MEC) network with multiple sensors and one MEC server in industrial Internet of Things networks, where the MEC server provides a joint computation service (in the computation phase) for a set of sub-tasks offloaded by different sensors (in the communication phase). Due to the requirements of low latency and ultra reliability, we utilize finite blocklength information theory to characterize the reliability of the communication phase and exploit extreme value theory to investigate the delay violation probability in the computation phase. Following these characterizations, we derive the average end-to-end error probability of the entire service and provide two average endto-end reliability-optimal design frameworks considering fixed frames structure and dynamic frames structure, in both of which the goal is to minimize the average end-to-end error probability by optimally allocating the total time length to each frame, as well as allocating each frame length to the communication phase and the computation phase. For the fixed frames structure, the original problem is decomposed, and the joint convexity of the decomposed sub-problems is rigorously proved, and the optimal solutions are obtained by the proposed optimal time allocation algorithm. Moreover, for the dynamic frames structure, we reformulate the optimization problem by introducing an average time constraint. By exploiting Lagrange multipliers, we transform the reformulated optimization problem into a dual problem with strong duality, the solutions of which can be obtained by the proposed time allocation algorithm. Via simulations, we validate the proven convexity and the approximation in our analytical model and evaluate the performance for both fixed frames length structure and dynamic frames length structure.},
  archive      = {J_TMC},
  author       = {Jie Wang and Yao Zhu and Yulin Hu and M. Cenk Gursoy and Anke Schmeink},
  doi          = {10.1109/TMC.2025.3541661},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Average reliability-optimal offloading for mobile edge computing in LowLatency industrial IoT networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint view selection, multigroup multicast beamforming, and
DIBR for RIS-aided multi-view videos. <em>TMC</em>, 1–18. (<a
href="https://doi.org/10.1109/TMC.2025.3543297">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The rapid development of multi-view videos (MVV) transmission is an irresistible trend. Concurrently, reconfigurable intelligent surface (RIS)-assisted wireless communication has drawn significant attention. We observe that the view selection based on the base station and the view synthesis based on depth-image-based rendering (DIBR) can effectively reduce power consumption. Therefore, this paper studies the view selection and synthesis for RIS-aided MVV in multigroup multicast beamforming. To deal with this complicated scenario, we investigate a problem, named the joint View selection, Multicast beamforming, and DIBR (JVMD), to minimize the total multicast beamforming power, the view transmission operation power, and view synthesis, subject to quality-of-service (QoS), RIS phase shifts, view selection, and DIBR constraints. Unfortunately, the mathematical model is a complicated mixed discrete-continuous optimization problem. To tackle this challenging problem, we designed an algorithm, named View selection, Beamforming, RIS phase, and DIBR (VBRD) algorithm. First, we deal with the discrete optimization problem of selecting the view. VBRD uses the dual-based approximation methodology to round back a primal&#39;s integer solution. Then, in the continuous optimization problem, we apply the alternating optimization (AO) method to determine beamforming, RIS phase, and DIBR. Finally, simulation results show the performance of exploiting view synthesis for RIS-assisted wireless communication.},
  archive      = {J_TMC},
  author       = {Chi-Han Lee and De-Nian Yang and Guang-Siang Lee and Chih-Hang Wang and Wanjiun Liao},
  doi          = {10.1109/TMC.2025.3543297},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint view selection, multigroup multicast beamforming, and DIBR for RIS-aided multi-view videos},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). CAAF: An NDN-based cache-aware adaptive forwarding strategy
for reliable content delivery in VANETs. <em>TMC</em>, 1–15. (<a
href="https://doi.org/10.1109/TMC.2025.3543458">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The high mobility in Vehicular Ad-hoc Networks (VANETs) significantly affects the reliability of data transmission. To solve this problem, Named Data Networking (NDN)-based VANETs are proposed, utilizing in-network caching and named-based forwarding to overcome the dual challenges of mobility and connectivity. Although in-network caching enhances content availability, a strategy that accurately locates and efficiently utilizes the cached content in VANETs with highly dynamic environments is still lacking. In this paper, we propose a novel NDN-based cache-aware adaptive forwarding (CAAF) strategy for VANETs. CAAF proactively predicts content locations and ensures reliable content retrieval by adaptively selecting forwarding nodes that prioritize fast delivery and stable transmission. Specifically, we design a content information table for each vehicle to record information about the Interest packets it receives. Furthermore, these tables are updated periodically across all vehicles and a prediction model is used to predict real-time in-network caching during the update interval. Subsequently, we execute a filter mechanism to sieve candidate forwarding vehicles that satisfy both the accessibility and stability requirements. These candidates are then evaluated using a multi-attribute decision-making method across diverse parameters to determine the optimal forwarding node. Our extensive simulation results demonstrate that the proposed CAAF outperforms the state-of-the-art forwarding strategy regarding content retrieval delay and Interest satisfaction ratio across diverse scenarios.},
  archive      = {J_TMC},
  author       = {Haodong Wang and Jiangping Han and Kaiping Xue and Jiayu Yang and Jian Li and Qibin Sun and Jun Lu},
  doi          = {10.1109/TMC.2025.3543458},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {CAAF: An NDN-based cache-aware adaptive forwarding strategy for reliable content delivery in VANETs},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MIDDLE: A mobility-driven device-edge-cloud federated
learning framework. <em>TMC</em>, 1–18. (<a
href="https://doi.org/10.1109/TMC.2025.3543723">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) can be implemented in large-scale wireless networks in a hierarchical way, introducing edge servers as relays between the cloud server and devices. These devices are dispersed within multiple clusters coordinated by edges. However, the devices are typically mobile users with unpredictable trajectories, and the impact of their mobility on the model training process is not well-studied. In this work, we propose a new MobIlity-Driven feDerated LEarning framework, namely MIDDLE. MIDDLE addresses unbalanced model updates by capitalizing on model aggregation opportunities on mobile devices due to their mobility across edges. It consists of two components: on-device model aggregation, which aggregates models from different edges carried by mobile devices as they move across edges, and in-edge device selection, adjusting the current edge optimization direction through careful device selection. Theoretical analysis emphasizes that on-device model aggregation can reduce bias in model updating on edges and the cloud, thereby accelerating the FL model convergence. Building on this analysis, we introduce on-device global control averaging, modifying the training process on mobile devices and extending MIDDLE into $\text{MIDDLE}^{+}$. Extensive experimental results validate that MIDDLE and $\text{MIDDLE}^{+}$ can reduce the time steps to reach the target accuracy by 19.44% and 20.37% at least, respectively.},
  archive      = {J_TMC},
  author       = {Songli Zhang and Zhenzhe Zheng and Fan Wu and Bingshuai Li and Yunfeng Shao and Guihai Chen},
  doi          = {10.1109/TMC.2025.3543723},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MIDDLE: A mobility-driven device-edge-cloud federated learning framework},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Build yourself before collaboration: Vertical federated
learning with limited aligned samples. <em>TMC</em>, 1–14. (<a
href="https://doi.org/10.1109/TMC.2025.3543923">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Vertical Federated Learning (VFL) has emerged as a crucial privacy-preserving learning paradigm that involves training models using distributed features from shared samples. However, the performance of VFL can be hindered when the number of shared or aligned samples is limited, a common issue in mobile environments where user data are diverse and unaligned across multiple devices. Existing approaches use feature generation and pseudo-label estimation for unaligned samples to address this issue, unavoidably introducing noise during the generation process. In this work, we propose Local Enhanced Effective Vertical Federated Learning (LEEF-VFL), which fully utilizes unaligned samples in the local learning before collaboration. Unlike previous methods that overlook private labels owned by each client, we leverage these private labels to learn from all local samples, constructing robust local models to serve as solid foundations for collaborative learning. Additionally, we reveal that the limited number of aligned samples introduces distribution bias from global data distribution. In this case, we propose to minimize the distribution discrepancies between the aligned samples and the global data distribution to enhance collaboration. Extensive experiments demonstrate the effectiveness of LEEF-VFL in addressing the challenges of limited aligned samples, making it suitable for VFL in mobile computing environments. Codes are available at https://github.com/shentt67/LEEF-VFL.},
  archive      = {J_TMC},
  author       = {Wei Shen and Mang Ye and Wei Yu and Pong C. Yuen},
  doi          = {10.1109/TMC.2025.3543923},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Build yourself before collaboration: Vertical federated learning with limited aligned samples},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Exploring MEC server strategy in blockchain networks: Mining
for mobile users or for self. <em>TMC</em>, 1–14. (<a
href="https://doi.org/10.1109/TMC.2025.3544311">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Blockchain-based decentralized applications (DApps) offer enhanced security and decentralization features; nevertheless, their maintenance demands substantial computational resources and poses challenges for deployment in mobile networks. To address this, a number of studies have explored offloading blockchain mining tasks from mobile users to mobile edge computing (MEC) servers. However, the existing literature overlooks the fact that MEC servers can not only mine for mobile users but also mine for themselves, potentially explaining why MEC mining offloading has not gained broad acceptance within the industry. In this work, we exploit a more practical case and rethink the question of whether MEC servers lease computing power to mobile users by taking into account that MEC servers can mine for themselves. We establish a game model and apply backward induction to analytically characterize Nash equilibria for mining strategies adopted by MEC and mobile users. Our findings suggest that, if MEC can mine for self, MEC would mine for mobile users only under specific conditions where mobile users possess superior information gathering capability (at least better than the MEC server) or the whole blockchain system exhibits significant network value. We further provide a series of simulations to verify our conclusion and illustrate the impact of network parameters on the strategies of both sides.},
  archive      = {J_TMC},
  author       = {Xintong Ling and Rui Jiang and Weihang Cao and Mingkai Chen and Jiaheng Wang and Zhi Ding and Xiqi Gao},
  doi          = {10.1109/TMC.2025.3544311},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Exploring MEC server strategy in blockchain networks: Mining for mobile users or for self},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). FOOL: Addressing the downlink bottleneck in satellite
computing with neural feature compression. <em>TMC</em>, 1–18. (<a
href="https://doi.org/10.1109/TMC.2025.3544516">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Nanosatellite constellations equipped with sensors capturing large geographic regions provide unprecedented opportunities for Earth observation. As constellation sizes increase, network contention poses a downlink bottleneck. Orbital Edge Computing (OEC) leverages limited onboard compute resources to reduce transfer costs by processing the raw captures at the source. However, current solutions have limited practicability due to reliance on crude filtering methods or over-prioritizing particular downstream tasks. This work presents an OEC-native and task-agnostic feature compression method that preserves prediction performance and partitions high-resolution satellite imagery to maximize throughput. Further, it embeds context and leverages inter-tile dependencies to lower transfer costs with negligible overhead. While the encoding prioritizes features for downstream tasks, we can reliably recover images with competitive scores on quality measures at lower bitrates. We extensively evaluate transfer cost reduction by including the peculiarity of intermittently available network connections in low earth orbit. Lastly, we test the feasibility of our system for standardized nanosatellite form factors. We demonstrate that the proposed approach permits downlinking over 100× the data volume without relying on prior information on the downstream tasks.},
  archive      = {J_TMC},
  author       = {Alireza Furutanpey and Qiyang Zhang and Philipp Raith and Tobias Pfandzelter and Shangguang Wang and Schahram Dustdar},
  doi          = {10.1109/TMC.2025.3544516},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {FOOL: Addressing the downlink bottleneck in satellite computing with neural feature compression},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Incremental wavelet-capsules: A cross-environment solution
for WiFi identification. <em>TMC</em>, 1–17. (<a
href="https://doi.org/10.1109/TMC.2025.3544836">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {WiFi-based identity recognition differs from traditional identification technologies as it is not limited by lighting conditions and does not require dense, specialized sensors or wearable devices. This makes it valuable in modern human–machine interactions. However, the diversity of real-world environmental conditions substantially limits the application of existing WiFi-based identity recognition algorithms, particularly when applied across different environments. As a solution, we introduce the incremental wavelet capsule (IWC) model, which combines a newly designed wavelet convolution layer with a capsule network to accelerate precise feature extraction. We adopt a hybrid incremental learning strategy, solving the catastrophic forgetting problem in cross-environment tasks and enabling the model to adapt to new environments in the data stream without forgetting the original environment. Furthermore, we developed a customized data augmentation method for WiFi signals, enhancing the model&#39;s adaptability and stability across various environments. Experimental results show that the IWC model achieves an average recognition accuracy of 97.36% across five different environments and maintains an accuracy of 91.5% even when only 5% of the training data from a new environment is used. These findings demonstrate the model&#39;s robust performance and practicality in cross-environment scenarios.},
  archive      = {J_TMC},
  author       = {Zhiyi Zhou and Xinxin Lu and Lei Wang and Yu Tian and Yunbo Chen and Bingxian Lu},
  doi          = {10.1109/TMC.2025.3544836},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Incremental wavelet-capsules: A cross-environment solution for WiFi identification},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ActivityMamba: A CNN-mamba hybrid neural network for
efficient human activity recognition. <em>TMC</em>, 1–15. (<a
href="https://doi.org/10.1109/TMC.2025.3544573">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Current research in human activity recognition primarily emphasizes enhancing accuracy, with limited exploration into computational efficiency and hardware compatibility. Recently, Mamba has sparked substantial interest within the realm of deep learning. Mamba is a hardware-aware algorithm enabling very efficient training and inference. Researchers are applying Mamba to various tasks, demonstrating significant promise in both language and vision tasks. It is worthwhile to investigate the use of Mamba for efficient human activity recognition. In this paper, we proposed a hybrid neural network that integrates CNN and visual Mamba, called ActivityMamba. The SE-Mamba block in ActivityMamba utilizes both CNN&#39;s local and Mamba&#39;s global context modeling while keeping computation and memory efficiency. We evaluated the ActivityMamba on five public benchmark datasets collected by using three different sensing techniques. ActivityMamba achieved higher performance than vision transformers, vision Mamba, and CNNs with fewer FLOPs and parameters. It sets a new SOTA on all five datasets, which are 91.78% OA and 89.13% F1 on the USC-HAD dataset, 99.19% OA and 98.64% F1 on the UT-HAR dataset, 99.82% OA and F1 on the DIAT dataset, 98.59% OA and 98.65% F1 on the UCI-HAR dataset, and 95.41% OA and 93.14% F1 on the UniMib dataset. Our work is the first to investigate the CNN-Mamba hybrid network for efficient human activity recognition.},
  archive      = {J_TMC},
  author       = {Fei Luo and Anna Li and Bin Jiang and Salabat Khan and Kaishun Wu and Lu Wang},
  doi          = {10.1109/TMC.2025.3544573},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ActivityMamba: A CNN-mamba hybrid neural network for efficient human activity recognition},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). DT-RSSI: Digital twin-replica of sensing statistics for IRA
in intelligent NG-HetNetIs. <em>TMC</em>, 1–11. (<a
href="https://doi.org/10.1109/TMC.2025.3544918">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Intelligent resource allocation maintains a better quality of service among devices in next-generation heterogeneous network infrastructures (NG-HetNetIs). NG-HetNetIs include industry 5.0 enabled infrastructures like Internet of Things (IoT), cognitive radio (CR) enabled B5G and 6G networks, unmanned aerial vehicles (UAVs), wireless sensor networks (WSNs) and autonomous vehicles (AVs). Digital twin (DT) joins hand with cognitive radio and resource aggregation technologies to provide the integrated framework for intelligent resource allocation in NG-HetNetIs. In NG-HetNetIs, the obtained statistics of measured radio activity as prior information play an instrumental role in enabling optimized resource allocation using context awareness. Unfortunately, the already available static approaches are inefficient to replicate (DT) the radio activity in a heterogeneous radio environment. To address the issue, static implementation framework is extended as dynamic radio activity characterization framework (DRAC) to have context awareness in NG-HetNetIs. The proposed DRAC replicates (DT) the wide sense stationarity of time and carrier aggregated radio activity due to its exploitation of more localized temporal and spectral information in NG-HetNets. The obtained localized statistics using DRAC can be exploited as appropriate prior knowledge and test statistics during the spectrum sensing phase of NG-HetNetIs for intelligent resource allocation instead of a single statistic obtained by the static approach.},
  archive      = {J_TMC},
  author       = {Muhammad Khurram Ehsan and Neelma Naz and Ali Hassan Sodhro and Shahid Mumtaz and Asad Mahmood},
  doi          = {10.1109/TMC.2025.3544918},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-11},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {DT-RSSI: Digital twin-replica of sensing statistics for IRA in intelligent NG-HetNetIs},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Forward legal anonymous group pairing-onion routing for
mobile opportunistic networks. <em>TMC</em>, 1–18. (<a
href="https://doi.org/10.1109/TMC.2025.3544674">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile Opportunistic Networks (MONs) often experience frequent interruptions in end-to-end connections, which increases the likelihood of message loss during delivery and makes users more susceptible to various cyber attacks. However, most currently proposed anonymous routing protocols are primarily designed for networks with stable connections, making it challenging to protect user identities in MONs. To address these challenges, we propose FLAG-POR (Forward Legal Anonymous Group Pairing-Onion Routing), a novel anonymous routing protocol specifically tailored to enhance message delivery anonymity and security in MONs. Specifically, we abstract the mobile opportunistic network as a contact graph. By introducing the concept of “groups&quot; into the pairing-onion routing protocol, which encrypts messages and relay nodes layer by layer, we develop a novel group-based pairing-onion routing protocol. This protocol ensures message confidentiality and relay node anonymity, while also improving message forwarding rates, as any node within a group can potentially act as a relay. To ensure message authenticity, we employ the efficient SM2 signing algorithm to generate signatures for the message source. Furthermore, by incorporating parameters such as the public key validity period and master key validity period into the group pairing-onion routing protocol, we achieve forward security in message delivery. We conduct a thorough theoretical analysis of the protocol&#39;s security and performance. The experimental results demonstrate that our FLAG-POR protocol outperforms baseline anonymous protocols in terms of delivery success rate, traceability rate, path anonymity, and node anonymity. Additionally, the FLAG-POR scheme effectively resists three potential threats to the routing system: collusion attack threat, node identification threat, and path identification threat, in any situation},
  archive      = {J_TMC},
  author       = {Xiuzhen Zhu and Limei Lin and Yanze Huang and Xiaoding Wang and Sun-Yuan Hsieh and Jie Wu},
  doi          = {10.1109/TMC.2025.3544674},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Forward legal anonymous group pairing-onion routing for mobile opportunistic networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Topology-compressed data delivery in large-scale
heterogeneous satellite networks: An age-driven spatial-temporal graph
neural network approach. <em>TMC</em>, 1–15. (<a
href="https://doi.org/10.1109/TMC.2025.3544574">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In Large-Scale Heterogeneous Satellite Networks (LSHSNs) integrating Low Earth Orbit (LEO) and Medium Earth Orbit (MEO) satellites, high-timeliness data delivery confronts dynamical connectivity and obvious latency, which heavily challenges existing graph-dependable transmission strategies requiring to obtain global topological information with huge computational cost and signaling overhead. To address this issue, in this paper, we propose an Age-predicting Local Information Dependable Transmission (ALIDT) mechanism for the LSHSN by considering the impact of time-varying topology on the timeliness of data, in which a novel metric of data freshness called Forwarding-aware Age of Information (FAoI) is well-designed to evaluate the timeliness in data forwarding at node. In particular, we develop a satellite Coverage-based Local Information Sharing (CLIS)-assisted Spatial-Temporal Graph Neural Network (STGNN) to extract the topological features in both temporal and spatial dimensions and a Graph Matching Network (GMN)-based topology compression algorithm to improve computation efficiency. The simulation results indicate that the proposed mechanism performs better in improving the storage overhead, throughput and average FAoI compared with the conventional Open Shortest Path First (OSPF) routing algorithm with Time-Varying Graph (TVG) model, GNN-based Multipath Routing (GMR) algorithm, and Gated Recurrent Units (GRU) based metric prediction algorithm in hybrid satellite networks, respectively.},
  archive      = {J_TMC},
  author       = {Ronghao Gao and Bo Zhang and Qinyu Zhang and Zhihua Yang},
  doi          = {10.1109/TMC.2025.3544574},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Topology-compressed data delivery in large-scale heterogeneous satellite networks: An age-driven spatial-temporal graph neural network approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A fast UAV trajectory planning framework in RIS-assisted
communication systems with accelerated learning via multithreading and
federating. <em>TMC</em>, 1–16. (<a
href="https://doi.org/10.1109/TMC.2025.3544903">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Reconfigurable Intelligent Surface (RIS)-assisted Unmanned Aerial Vehicle (UAV) communications have been realized as essential to space-air-group system integration in the 6G technology landscape. Trajectory planning plays a crucial role in RIS-assisted UAV communications to face the challenges of UAV&#39;s limited power capacities and dynamic wireless channels. Existing solutions assume complete channel state information, focus on single-rotor UAVs, and rely heavily on time-consuming training processes for machine learning; thus, they lack applicability to deal with highly dynamic real-world scenarios. To fill these research gaps, we aim to characterize RISassisted UAV communications and design responsive and accurate UAV trajectory planning algorithms in this paper. We first develop a communication model with incomplete information and an energy consumption model for quadrotor UAVs. We then formulate UAV trajectory planning as an optimization problem to minimize UAV&#39;s energy consumption while maintaining communication throughput. To solve this problem, we design an acceleration framework, FedX, for reinforcement learning (RL) solvers and present two fast trajectory planning algorithms, FedSAC and FedPPO, as instantiations of the FedX framework. Our evaluation results indicate that the proposed framework is effective and efficient–more than 3 times faster with 5 agents and 7 times faster with 10 agents than standard RL algorithms, making it suitable for using RL solvers within wireless networks and mobile computing environments. We also discuss and identify the pros and cons of our proposed framework.},
  archive      = {J_TMC},
  author       = {Jun Huang and Beining Wu and Qiang Duan and Liang Dong and Shui Yu},
  doi          = {10.1109/TMC.2025.3544903},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A fast UAV trajectory planning framework in RIS-assisted communication systems with accelerated learning via multithreading and federating},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). LCEFL: A lightweight contribution evaluation approach for
federated learning. <em>TMC</em>, 1–15. (<a
href="https://doi.org/10.1109/TMC.2025.3545140">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The prerequisite for implementing incentive mechanisms and reliable participant selection schemes in federated learning is to obtain the contribution of each participant. Available evaluation methods for participant contributions require the server to possess a test dataset, often impractical. Additionally, the excessively high complexity of these works is unacceptable when training complex models in large-scale federated learning system. To address these issues, we propose a lightweight contribution evaluation method for federated learning participants, named LCEFL, based on model projection theory, which does not require the server to provide a test dataset. In addition, a model compression method is designed to be used in LCEFL to reduce the computational complexity. Furthermore, a trusted aggregation method based on LCEFL is proposed, where the weight of each participant&#39;s local model is determined by its trust level, which can be calculated using its contribution evaluation result. Experimental results show that LCEFL can achieve nearly the same accuracy as schemes based on Shapley Value, while significantly reducing computational overhead by more than 50%. Compared to available aggregation methods, the proposed trusted aggregation scheme is able to accelerate the convergence speed of the global model and improve its accuracy by 2% to 45%.},
  archive      = {J_TMC},
  author       = {Jingjing Guo and Jiaxing Li and Zhiquan Liu and Yupeng Xiong and Yong Ma and Athanasios V. Vasilakos and Xinghua Li and Jianfeng Ma},
  doi          = {10.1109/TMC.2025.3545140},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {LCEFL: A lightweight contribution evaluation approach for federated learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Individualized data generation in personalized federated
learning. <em>TMC</em>, 1–16. (<a
href="https://doi.org/10.1109/TMC.2025.3545244">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Most Personalized Federated Learning (PFL) algorithms merge the model parameters of each client with other (similar or generic) model parameters to optimize the personalized model (PM). However, the merged model parameters in these algorithms may fit low relevance data, thereby limiting the performance of PM. In this paper, we generate similar data for each client through the collaboration of a generic model (GM) on the server, rather than merging model parameters. To train a generator capable of generating data for all classes on the server without real data, we employ the GM as the discriminator in adversarial training with the generator. Additionally, we introduce a similarity assessment metric, which allows for the assessment of the similarity between local data and data from other classes. Nevertheless, the presence of non-IID data among clients can weaken the performance of the GM, consequently impacting the training of the generator and similarity assessment. To address this issue, we design a directive mechanism so that GM can be optimized during adversarial training without the need for additional training. The experimental results validate the superiority of our algorithm over state-of-the-art algorithms in terms of accuracy, loss, and convergence speed.},
  archive      = {J_TMC},
  author       = {Yunyun Cai and Wei Xi and Yuhao Shen and Cerui Sun and Shuai Wang and Wei Gong and Jizhong Zhao},
  doi          = {10.1109/TMC.2025.3545244},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Individualized data generation in personalized federated learning},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ReflexGest: Recognizing hand gestures under VLC-capable
lamps. <em>TMC</em>, 1–13. (<a
href="https://doi.org/10.1109/TMC.2025.3545340">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {As a main approach towards touch-free human-computer interaction, hand gesture recognition (HGR) has long been a research focus for both academia and industry. Meanwhile, visible light communication (VLC) has become increasingly popular with VLC-ready commercial products (e.g., Philips lamps) available on the market. These facts provoke us to ask: can we leverage a VLC-ready lamp to realize integrated sensing and communication (ISAC) by conducting both HGR and VLC simultaneously? To this end, we propose ReflexGest as our answer to this question. ReflexGest is implemented upon a table lamp for the sake of practicality; this VLC-ready lamp is equipped with a ring-shaped light-emitting diode (LED) array and a photodiode (PD, for light intensity sensing) originally aiming for up/down-link VLCs. Demanding hand gestures to be performed between the lamp and a table surface, ReflexGest exploits the variation of the reflection and their unique correlation with the corresponding hand gestures to achieve HGR. In particular, ReflexGest first handles the limited sensing ability of the PD by enhancing the LED lamp and thus diversifying the light emission patterns. Moreover, ReflexGest combats the reflection interference from varying table surfaces via an adversarial learning technique to distill only the features relevant to hand gestures. Our extensive evaluations demonstrate that ReflexGest is able to deliver accurate HGR under realistic VLC traffic.},
  archive      = {J_TMC},
  author       = {Ziwei Liu and Jifei Zhu and Jiaqi Yang and Yimao Sun and Yanbing Yang and Jun Luo},
  doi          = {10.1109/TMC.2025.3545340},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ReflexGest: Recognizing hand gestures under VLC-capable lamps},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). ALANINE: A novel decentralized personalized federated
learning for heterogeneous LEO satellite constellation. <em>TMC</em>,
1–16. (<a href="https://doi.org/10.1109/TMC.2025.3545429">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Low Earth Orbit (LEO) satellite constellations have seen significant growth and functional enhancement in recent years, which integrates various capabilities like communication, navigation, and remote sensing. However, the heterogeneity of data collected by different satellites and the problems of efficient inter-satellite collaborative computation pose significant obstacles to realizing the potential of these constellations. Existing approaches struggle with data heterogeneity, varing image resolutions, and the need for efficient on-orbit model training. To address these challenges, we propose a novel decentralized PFL framework, namely, A Novel DecentraLized PersonAlized Federated Learning for HeterogeNeous LEO SatellIte CoNstEllation (ALANINE). ALANINE incorporates decentralized FL (DFL) for satellite image Super Resolution (SR), which enhances input data quality. Then it utilizes PFL to implement a personalized approach that accounts for unique characteristics of satellite data. In addition, the framework employs advanced model pruning to optimize model complexity and transmission efficiency. The framework enables efficient data acquisition and processing while improving the accuracy of PFL image processing models. Simulation results demonstrate that ALANINE exhibits superior performance in on-orbit training of SR and PFL image processing models compared to traditional centralized approaches. This novel method shows significant improvements in data acquisition efficiency, process accuracy, and model adaptability to local satellite conditions.},
  archive      = {J_TMC},
  author       = {Liang Zhao and Shenglin Geng and Xiongyan Tang and Ammar Hawbani and Yunhe Sun and Lexi Xu and Daniele Tarchi},
  doi          = {10.1109/TMC.2025.3545429},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {ALANINE: A novel decentralized personalized federated learning for heterogeneous LEO satellite constellation},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Vehicle-assisted service caching for task offloading in
vehicular edge computing. <em>TMC</em>, 1–12. (<a
href="https://doi.org/10.1109/TMC.2025.3545444">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of artificial intelligence (AI) enables vehicular edge computing (VEC) servers to be able to provide more intelligent services. However, the limited storage resources of VEC servers constrain the deployment of intelligent service contents, which greatly restricts the intelligence level of the VEC network. To resolve this problem, we first design a novel vehicle-assisted VEC network architecture and further propose VaCo, a Vehicle-assisted Collaborative caching system. VaCo allows VEC servers to download the cached service content from any vehicle in the VEC network to support task offloading. VaCo mainly considers the real-time scheduling problem of vehicle storage resources under the dynamic VEC network and the benefit problem caused by invoking vehicle resources under the highly dynamic load environment. VaCo models the vehicle storage resources as an independent resource pool and deploys a cross-VEC server content retrieval mechanism to achieve unified and efficient management of the storage resources of the vehicle cluster and the VEC server cluster. Then, we propose a multi-swarm collaborative optimization scheme to jointly optimize the service failure rate and cost, and further propose a Pareto-based optimization scheme to ensuring that VaCo can correctly evaluate the benefits of invoking vehicle resources in a dynamic VEC network. Finally, we implement VaCo and conduct extensive evaluations on real-world dataset. The experimental results on the real trajectory dataset show that VaCo can effectively utilize vehicle resources and ensure the benefits of both vehicles and VEC servers simultaneously.},
  archive      = {J_TMC},
  author       = {Hongbo Jiang and Jianghao Cai and Zhu Xiao and Kehua Yang and Hongyang Chen and Jiangchuan Liu},
  doi          = {10.1109/TMC.2025.3545444},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Vehicle-assisted service caching for task offloading in vehicular edge computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MoCo: Urban user mobile contact detection based on cellular
signaling trace. <em>TMC</em>, 1–16. (<a
href="https://doi.org/10.1109/TMC.2025.3545437">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile contact exhibits user co-traveling events within the same transportation tool, which is crucial for resident profiling, face-to-face interaction detection, etc. In this paper, we investigate urban user mobile contact detection with cellular signaling traces, which is cost-efficient to enable large-scale detection. Specifically, we develop a data collection platform to collect substantial user signaling traces, covering different types of road scenarios within a city. With the collected traces, we perform systematic data analysis to reveal several technical challenges, which are sparsity of signaling trajectory, remote base station noise, and fuzzy matching difficulties. To address challenges, we propose a mobile contact detection method named MoCo. In MoCo framework, we first conduct data denoising to remove the noise from remote base stations. Then, we devise a spatio-temporal filter to eliminate unlikely mobile contact traces in both spatial and temporal domains, reducing the computational overhead. Finally, we design a detection network that integrates the submodules of data alignment, feature encoder, spatio-temporal representation learner, and user mobile contact detector. Extensive evaluation results demonstrate the superiority of MoCo in comparison with state-of-the-art baselines. Robust experiments show that MoCo can work efficiently in different transportation modes and urban densities.},
  archive      = {J_TMC},
  author       = {Sijing Duan and Feng Lyu and Jing Zhang and Huali Lu and Peng Yang and Huaqing Wu and Yaoxue Zhang and Xuemin Shen},
  doi          = {10.1109/TMC.2025.3545437},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MoCo: Urban user mobile contact detection based on cellular signaling trace},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint design of power allocation and beamforming for
IRS-assisted millimeter-wave communication system with imperfect CSI.
<em>TMC</em>, 1–15. (<a
href="https://doi.org/10.1109/TMC.2025.3545413">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, the joint power allocation (PA), passive beamforming (BF) and hybrid BF (HBF) including digital and analogue BFs are designed for an intelligent reflecting surface (IRS)-assisted millimeter-Wave (mmWave) communication system with imperfect channel state information (CSI) and multiple mobile users to optimize the weighted sum rate (WSR) and energy efficiency (EE). The achievable WSR and EE of the IRS-mmWave system are firstly derived based on imperfect cascaded CSI for performance optimization. Then, the non-convex constrained problem is formulated to maximize the WSR, where the PA, HBF, phase and amplitude of IRS elements are jointly optimized. Given PA and passive BF (PBF), closed-form suboptimal HBF is obtained for each iteration. Also, given HBF and PBF, using the block coordinate descent (BCD) methods, closed-form PA is derived. Moreover, the phase and amplitude of IRS elements are derived for PBF design during each iteration. With the obtained HBF, the digital and analogue BFs are also derived. Based on this, joint schemes of PA, HBF and PBF are developed. Besides, an efficient iterative algorithm based upon the alternating optimization (AO), weighted minimum meansquare error (WMMSE) and Dinkelbach methods are presented for EE maximization and the suboptimal solution is obtained. Correspondingly, the energy-efficient design for joint PA, HBF and PBF is provided. Simulation results verify the proposed solutions},
  archive      = {J_TMC},
  author       = {Xiangbin Yu and Chenghong Yang and Jiawei Bai and Kezhi Wang and Yun Rui and Xiaoyu Dang},
  doi          = {10.1109/TMC.2025.3545413},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint design of power allocation and beamforming for IRS-assisted millimeter-wave communication system with imperfect CSI},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Toward optimal broadcast mode in offline finding network.
<em>TMC</em>, 1–16. (<a
href="https://doi.org/10.1109/TMC.2025.3545561">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This paper proposes ElastiCast, a novel Bluetooth Low Energy (BLE) broadcast mode that reduces the neighbor discovery latency in offline finding networks (OFNs). ElastiCast adapts the broadcast mode of the lost devices to the scan modes of the finder devices, considering their diversity. We start with an overview of OFNs, followed by a detailed analysis of the issues and challenges of existing solutions, which motivates the design of ElastiCast. Then we provide Blender, a simulator that models the neighbor discovery behavior of different broadcasters and scanners. By adopting Blender, ElastiCast can be implemented with three components: Local Optima Estimation, Common Interest Extraction, and Interval Multiplexing, in which we capture the key features of BLE neighbor discovery and globally optimize the broadcast mode interacting with diverse scan modes. Experimental evaluation results and commercial product deployment experience demonstrate that ElastiCast is effective in achieving stable and bounded neighbor discovery latency within the power budget.},
  archive      = {J_TMC},
  author       = {Tong Li and Yukuan Ding and Jiaxin Liang and Kai Zheng and Xu Zhang and Tian Pan and Dan Wang and Ke Xu},
  doi          = {10.1109/TMC.2025.3545561},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Toward optimal broadcast mode in offline finding network},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A traffic-aware graph neural network for user association in
cellular networks. <em>TMC</em>, 1–12. (<a
href="https://doi.org/10.1109/TMC.2025.3545464">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In this paper, we utilize a graph neural network to perform joint user association and access point activation/deactivation to optimize network blockage. Instead of using theoretical models to characterize the distribution of network traffic, we use a real-world network traffic dataset. In order to train the graph neural network, our method leverages reinforcement learning, specifically employing the deep deterministic policy gradient (DDPG) algorithm. This approach allows us to benefit from the advantages of both value-based and policy-based reinforcement learning methods. A simple but flexible reward function is defined to capture the trade-off between the fraction of active access points and network blockage. The graph neural network&#39;s awareness of the network topology gives the proposed method a clear performance advantage over the commonly used greedy heuristic optimization method, reducing blockage by up to more than 50% on real-world network traffic data while also reducing computational complexity from O(N3) to O(N). The proposed user association and access point activation method is not limited by the network architecture or technology, and can be applied to different generations of cellular networks},
  archive      = {J_TMC},
  author       = {Saeed Jamshidiha and Vahid Pourahmadi and Abbas Mohammadi},
  doi          = {10.1109/TMC.2025.3545464},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {A traffic-aware graph neural network for user association in cellular networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Partial offloading strategy based on deep reinforcement
learning in the internet of vehicles. <em>TMC</em>, 1–15. (<a
href="https://doi.org/10.1109/TMC.2025.3543976">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Driven by the increasing demands of vehicular tasks, edge offloading has emerged as a promising paradigm to enhance quality of experience (QoE) in Internet of Vehicles (IoV) networks. This approach enables vehicles to offload computation-intensive tasks to edge servers, resulting in reduced computation delays and lower energy consumption. However, traditional binary offloading limits the efficiency of edge offloading. To address this gap, we propose a partial offloading strategy that jointly optimizes the offloading ratio, computation, and communication resources in IoV. Recognizing the varying priorities of vehicular tasks regarding task delay and energy consumption, we formulate two distinct scenarios: one focused on minimizing delay and the other on minimizing energy consumption. Furthermore, we employ a reinforcement learning approach to establish a multi-dimensional joint optimization function by setting different objectives for each scenario. Based on this framework, we introduce a multi-state iteration deep deterministic policy gradient algorithm (SIDDPG), which effectively determines task partitioning and resource allocation. Simulation results demonstrate that the proposed algorithm outperforms benchmark schemes in terms of task delay and energy consumption.},
  archive      = {J_TMC},
  author       = {Shujuan Tian and Xinjie Zhu and Bochao Feng and Zhirun Zheng and Haolin Liu and Zhetao Li},
  doi          = {10.1109/TMC.2025.3543976},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Partial offloading strategy based on deep reinforcement learning in the internet of vehicles},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Realistic facial expression reconstruction using millimeter
wave. <em>TMC</em>, 1–17. (<a
href="https://doi.org/10.1109/TMC.2025.3540877">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The technology of facial expression reconstruction has paved the way for various face-centric applications such as virtual reality (VR) modeling, human-computer interaction, and affective computing. Existing vision-based solutions present challenges in privacy leakage and poor lighting conditions. In this paper, we introduce a nonintrusive facial expression reconstruction system, mm3DFace, which uses a millimeter wave (mmWave) radar to reconstruct facial expressions in a privacy-preserving and passive manner. mm3DFace first captures and pre-processes mmWave signals reflected by a human face, and extracts intricate facial geometric features using a ConvNeXt model integrated with triple loss embedding. Subsequently, mm3DFace derives pose-invariant facial representations utilizing region-divided affine transformation, and further generates individual facial shapes with 68 facial landmarks. Then, dynamic facial expressions with 3D facial avatars are reconstructed to exhibit realistic facial expressions. Finally, mm3DFace enables micro-expression recognition with mmWave signals, which ensures the capability of describing tiny facial changes. Through extensive real-world experiments involving 15 participants, mm3DFace achieves a normalized mean error of 3.94%, a mean absolute error of 2.30mm, and a 3D-mean absolute error of 4.10mm in tracking 68 facial landmarks, which demonstrates the efficacy and practicality of mm3DFace in real-world 3D facial reconstruction scenarios.},
  archive      = {J_TMC},
  author       = {Hao Kong and Jiahong Xie and Jiadi Yu and Yingying Chen and Linghe Kong and Yanmin Zhu and Feilong Tang},
  doi          = {10.1109/TMC.2025.3540877},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Realistic facial expression reconstruction using millimeter wave},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Real-time abnormal address detection for mobile devices in
location-based services. <em>TMC</em>, 1–17. (<a
href="https://doi.org/10.1109/TMC.2025.3545875">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {An address, a textual description of a geographical location, plays an important role in location-based services such as instant delivery. However, abnormal addresses (i.e., an address without detailed or accurate information) have led to significant costs. In real-world settings, abnormal address detection is not trivial because it needs to be completed in real-time to support massive online queries from mobile devices. In this study, we design FastAddr, a fast abnormal address detection framework, which detects abnormal addresses in real time. FastAddr consists of a novel contrastive address augmentation module and a lightweight multi-head attention model. We further design FastAddr+ to enhance FastAddr by utilizing large-scale spatial entities. A comprehensive three-phase evaluation is conducted. (i) We evaluate FastAddr on a real-world dataset and it yields the average F1 of $85.7\%$ in 0.058 milliseconds, which outperforms the state-of-the-art models by $47.4\%$ with a similar detection time. (ii) An offline A/B test shows that FastAddr outperforms the previous model significantly. (iii) We also conduct an online A/B test to compare FastAddr with the deployed model, which shows an improvement of F1 by more than $20\%$. Moreover, we conduct two case studies on real industry data, demonstrating both the efficiency and effectiveness of FastAddr.},
  archive      = {J_TMC},
  author       = {Zhiqing Hong and Heng Yang and Haotian Wang and Wenjun Lyu and Yu Yang and Guang Wang and Yunhuai Liu and Yang Wang and Desheng Zhang},
  doi          = {10.1109/TMC.2025.3545875},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Real-time abnormal address detection for mobile devices in location-based services},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Compromising rechargeable sensor networks in marine
environment. <em>TMC</em>, 1–17. (<a
href="https://doi.org/10.1109/TMC.2025.3546276">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Marine Wireless Rechargeable Sensor Networks (MWRSNs), enhanced by recent Wireless Power Transfer (WPT) technology, present a significant advancement in extending network life. Traditional methods improve network performance through algorithm optimization, but neglect charging security, exposing networks to potential attacks. This paper addresses this problem from an adversarial view and develops a novel attack for MWRSN through Denying of Charge (DoC) to maximize network destructiveness. We start by establishing a generalized on-demand charging model, essential for developing DoC tactics. Subsequently, we unveil the Collaborative DoC (CoDoC) algorithm, capable of manipulating and falsifying charging requests. Central to CoDoC is the Request Prediction Method (RPM), which forecasts the initiation of charging requests and facilitates rapid request surges to enhance the attack&#39;s efficacy. CoDoC is able to disguise the presence of the attack, which is able to escape from being detected by the base station. Theoretical analyses are provided to explore the features of the proposed scheme. To demonstrate the outperformed features of the proposed schemes, extensive simulations and test-bed experiments are conducted. Our analysis and extensive simulations demonstrate that CoDoC increases sensor node failures by 20% to 142% compared to traditional methods, highlighting its effectiveness in marine environments.},
  archive      = {J_TMC},
  author       = {Qiwei Wang and Chi Lin and Haipeng Dai and Mohammad S. Obaidat and Kuei-Fang Hsiao and Xin Fan},
  doi          = {10.1109/TMC.2025.3546276},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Compromising rechargeable sensor networks in marine environment},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Wireless charging for uncertain location nodes.
<em>TMC</em>, 1–18. (<a
href="https://doi.org/10.1109/TMC.2025.3546316">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Benefiting from Wireless Power Transfer (WPT) technology, Wireless Rechargeable Sensor Networks (WRSNs) effectively address the lifetime bottleneck of sensor nodes, enabling them to work perpetually. Most state-of-the-art studies assume that all WRSNs&#39; information is known or precise in advance. However, sensor nodes may be deployed randomly in a large-scale area, and some critical information (such as node location) may be unavailable or difficult to obtain precisely. In this work, we eliminate the effect of uncertain or imprecise node location and formalize the Maximizing Charging Energy utility for uncertain location nodes problem (i.e., MCE problem). With magnetic resonance coupling and beamforming technologies, we propose a novel node localization method to determine precise node location information. In addition, we present a reinforcement learning framework and a charging path scheduling method to maximize charging energy. To validate the effectiveness of our proposed scheme in real-world scenarios, we conduct test-bed experiments. The results demonstrate that our approach significantly improves charging efficiency by an average of 20.9% in a large-scale network, even when the locations of sensors are entirely unknown.},
  archive      = {J_TMC},
  author       = {Chi Lin and Qiwei Wang and Hengyi Li and Shibo Hao and Yi Wang and Lei Wang and Xin Fan and Guowei Wu},
  doi          = {10.1109/TMC.2025.3546316},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Wireless charging for uncertain location nodes},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint optimization of offloading and caching in
full-duplex-enabled edge computing networks. <em>TMC</em>, 1–16. (<a
href="https://doi.org/10.1109/TMC.2025.3546263">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Edge computing (EC) reduces task processing and content download delay by providing computation and caching resources directly to task offloading (TO) users and content request (CR) users. However, existing studies often focus exclusively on either TO users or CR users within EC networks, neglecting the interaction between these two groups. To address this gap, we investigate the offloading and caching decision-making in scenarios where TO and CR users coexist. Furthermore, we employ full-duplex (FD) technology to enhance spectral utilization for edge-end transmissions. Specifically, we jointly optimize offloading and caching in FD-enabled EC networks. To accomplish this, we decompose the formulated optimization problem into three sub-problems using the alternating optimization (AO) method. We then propose a three-subproblem alternating iterative delay minimization algorithm to effectively tackle the challenges of offloading and caching. Additionally, we analyze the convergence and complexity of our proposed algorithm. Finally, we conduct extensive simulations to evaluate the effectiveness of our approach. The simulation results demonstrate that the delay reduction achieved by our algorithm is between 24.78% and 89.23% greater than that of comparative algorithms.},
  archive      = {J_TMC},
  author       = {Xingxia Dai and Shujuan Tian and Haolin Liu and Zhetao Li and Hongbo Jiang and Qingyong Deng},
  doi          = {10.1109/TMC.2025.3546263},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Joint optimization of offloading and caching in full-duplex-enabled edge computing networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Multi–task model personalization for federated supervised
SVM in heterogeneous networks. <em>TMC</em>, 1–17. (<a
href="https://doi.org/10.1109/TMC.2025.3546550">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated systems enable collaborative training on highly heterogeneous, non-i.i.d. data through model personalization, which can be facilitated by employing multi-task learning. However, multi-task learning algorithms are often implemented using methods like stochastic gradient descent, which may suffer from slow convergence in a multi-task federated setting. To accelerate the training procedure, we design an efficient iterative distributed method based on the alternating direction method of multipliers (ADMM) for support vector machines (SVMs), which tackles federated classification and regression. The proposed method utilizes efficient computations and model exchange in a network of heterogeneous nodes and allows personalization of the learning model in the presence of non-i.i.d. data. To ensure data privacy, we introduce a randomization algorithm that helps avoid data inversion. Finally, we analyze the impact of the proposed privacy mechanisms and participant hardware and data heterogeneity on the system performance. Our experiments confirm the advantages of the proposed ADMM-based personalized federated multi-task learning.},
  archive      = {J_TMC},
  author       = {Aleksei Ponomarenko-Timofeev and Olga Galinina and Ravikumar Balakrishnan and Nageen Himayat and Sergey Andreev and Yevgeni Koucheryavy},
  doi          = {10.1109/TMC.2025.3546550},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Multi–Task model personalization for federated supervised SVM in heterogeneous networks},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). EdgeMoE: Empowering sparse large language models on mobile
devices. <em>TMC</em>, 1–16. (<a
href="https://doi.org/10.1109/TMC.2025.3546466">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Large language models (LLMs) such as GPTs and Mixtral-8x7B have revolutionized machine intelligence due to their exceptional abilities in generic ML tasks. Transiting LLMs from datacenters to edge devices brings benefits like better privacy and availability, but is challenged by their massive parameter size and thus unbearable runtime costs. To this end, we present EdgeMoE, an on-device inference engine for mixture-of-expert (MoE) LLMs – a popular form of sparse LLM that scales its parameter size with almost constant computing complexity. EdgeMoE achieves both memory- and compute-efficiency by partitioning the model into the storage hierarchy: non-expert weights are held in device memory; while expert weights are held on external storage and fetched to memory only when activated. This design is motivated by a key observation that expert weights are bulky but infrequently used due to sparse activation. To further reduce the expert I/O swapping overhead, EdgeMoE incorporates two novel techniques: (1) expert-wise bitwidth adaptation that reduces the expert sizes with tolerable accuracy loss; (2) expert preloading that predicts the activated experts ahead of time and preloads it with the compute-I/O pipeline. On popular MoE LLMs and edge devices, EdgeMoE showcase significant memory savings and speedup over competitive baselines. The code is available at https://github.com/UbiquitousLearning/mllm.},
  archive      = {J_TMC},
  author       = {Rongjie Yi and Liwei Guo and Shiyun Wei and Ao Zhou and Shangguang Wang and Mengwei Xu},
  doi          = {10.1109/TMC.2025.3546466},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-16},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {EdgeMoE: Empowering sparse large language models on mobile devices},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Continual reinforcement learning for digital twin
synchronization optimization. <em>TMC</em>, 1–15. (<a
href="https://doi.org/10.1109/TMC.2025.3546507">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {This article investigates the adaptive resource allocation scheme for digital twin (DT) synchronization optimization over dynamic wireless networks. In our considered model, a base station (BS) continuously collects factory physical object state data from wireless devices to build a real-time virtual DT system for factory event analysis. Due to continuous data transmission, maintaining DT synchronization must use extensive wireless resources. To address this issue, a subset of devices is selected to transmit their sensing data, and resource block (RB) allocation is optimized. This problem is formulated as a constrained Markov process (CMDP) problem that minimizes the long-term mismatch between the physical and virtual systems. To solve this CMDP, we first transform the problem into a dual problem that refines RB constraint impacts on device scheduling strategies. We then propose a continual reinforcement learning (CRL) algorithm to solve the dual problem. The CRL algorithm learns a stable policy across historical experiences for quick adaptation to dynamics in physical states and network capacity. Simulation results show that the CRL can adapt quickly to network capacity changes and reduce normalized root mean square error (NRMSE) between physical and virtual states by up to 55.2%, using the same RB number as traditional methods.},
  archive      = {J_TMC},
  author       = {Haonan Tong and Mingzhe Chen and Jun Zhao and Ye Hu and Zhaohui Yang and Yuchen Liu and Changchuan Yin},
  doi          = {10.1109/TMC.2025.3546507},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {Continual reinforcement learning for digital twin synchronization optimization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). MIPair: Exploiting magnetic induction for laptop-phone
pairing. <em>TMC</em>, 1–15. (<a
href="https://doi.org/10.1109/TMC.2025.3546758">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Transferring important files, photos, and other sensitive data between laptops and smartphones has become a routine necessity in daily life. Device pairing acts as the most fundamental need to secure the communication channel between two unconnected devices. Traditional pairing methods leveraging PINs or QR codes require tedious human efforts in the pairing procedures to establish a shared communication key. Nevertheless, these designs are vulnerable to shoulder-surfing attacks in which attackers might record and replay the pairing credentials. It is preferred to have more intuitive pairing designs that minimize users&#39; overhead in the pairing process while providing secure keys for communication purposes. In this paper, we propose MIPair for laptop-phone pairing by leveraging magnetic induction (MI) signals. MIPair is based on a key observation that changes in the CPU workload of a device cause variations in internal current, thereby inducing changes in surrounding magnetic fields. Moreover, the trends of MI signal variations are highly correlated with CPU workload trends. Thus, users simply need to place a smartphone on the keyboard of a laptop. By randomly altering the workload of the laptop through a stimulation program, the smartphone can capture MI signals with similar changing trends, thereby converting them into similar bit sequences that form the basis of a symmetric key. We propose essential techniques to overcome challenges such as time asynchronization between two devices, unfixed state transition time in MI signal, and shared key distribution from the two similar bit sequences. Our real-world experiments demonstrate reliable pairing as well as robustness against common attacks and high randomness of the generated keys. When generating a 128-bit key, MIPair achieves a successful pairing rate as high as 98% and a usable pairing time of 8.35 seconds.},
  archive      = {J_TMC},
  author       = {Yu Liu and Chenyu Huang and Kaiyi Wang and Zheng Qin and Wenqiang Jin},
  doi          = {10.1109/TMC.2025.3546758},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-15},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {MIPair: Exploiting magnetic induction for laptop-phone pairing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). UMIMO: Universal unsupervised learning for mmWave radar
sensing with MIMO array synthesis. <em>TMC</em>, 1–17. (<a
href="https://doi.org/10.1109/TMC.2025.3546757">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Millimeter-wave (mmWave) radar sensing powered by deep learning is now emerging in numerous applications, which are predominantly trained in a supervised manner. However, due to the non-interpretable nature of mmWave signals, labeling the radar data has always been a difficult task. While there have been investigations on unsupervised pre-training for mmWave radar sensing, these methods are tailored to specific signal representations. In this paper, we propose UMIMO, an unsupervised learning framework combining the hardware nature of MIMO radar and deep learning techniques to resolve the challenge raised by the insufficient labeled data. UMIMO leverages the antenna arrays synthesized from multiple transmitting and receiving antennas in mmWave radar to construct positive samples for contrastive learning. To achieve this, we propose the constraints on angular resolution and grating lobes to generate effective signal representations with different synthetic arrays. We conduct experiments using UMIMO on three tasks: contactless ECG monitoring, 3D human pose estimation, and human silhouette generation. All experimental results demonstrate that UMIMO can effectively improve the performance of learning-based mmWave radar sensing in an unsupervised manner.},
  archive      = {J_TMC},
  author       = {Haoyu Zhang and Dongheng Zhang and Ruiyuan Song and Zhi Wu and Jinbo Chen and Liang Fang and Zhi Lu and Yang Hu and Hui Lin and Yan Chen},
  doi          = {10.1109/TMC.2025.3546757},
  journal      = {IEEE Transactions on Mobile Computing},
  month        = {2},
  pages        = {1-17},
  shortjournal = {IEEE Trans. Mobile Comput.},
  title        = {UMIMO: Universal unsupervised learning for mmWave radar sensing with MIMO array synthesis},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
