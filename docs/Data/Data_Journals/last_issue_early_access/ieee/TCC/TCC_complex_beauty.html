<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <title>TCC_complex_beauty</title>
  <style>
    html {font-size: 22px;}
    body {margin: 0 auto; max-width: 76em;}
    #copyID {font-size: 18px;}
  </style>
  <script>
    function copy(element) {
      if (element.type == "button"){
      element.type="text";
      }
      element.style.color="black";
      element.style.backgroundColor="#C7EDCC";
      element.select();
      element.setSelectionRange(0, 99999);
      navigator.clipboard.writeText(element.value);
      window.getSelection().removeAllRanges();
      element.type="button";
    }
  </script>
</head>
<body>

<h2 id="tcc---9">TCC - 9</h2>
<ul>
<li><details>
<summary>
(2025). Developments on the “machine learning as a service for high
energy physics” framework and related cloud native solution.
<em>TCC</em>, 1–12. (<a
href="https://doi.org/10.1109/TCC.2025.3535793">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Machine Learning (ML) techniques have been successfully used in many areas of High Energy Physics (HEP) and will play a significant role in the success of upcoming High-Luminosity Large Hadron Collider (HL-LHC) program at CERN. An unprecedented amount of data at the exascale will be collected by LHC experiments in the next decade, and this effort will require novel approaches to train and use ML models. The work presented in this paper is focused on the developments of a ML as a Service (MLaaS) solution for HEP, aiming to provide a cloud service that allows HEP users to run ML pipelines via HTTPs calls. These pipelines are executed by using MLaaS4HEP framework, which allows reading data, processing data, and training ML models directly using ROOT files of arbitrary size from local or distributed data sources. In particular, new features implemented on the framework will be presented as well as updates on the architecture of an existing prototype of the MLaaS4HEP cloud service will be provided. This solution includes two OAuth2 proxy servers as authentication/authorization layer, a MLaaS4HEP server, an XRootD proxy server for enabling access to remote ROOT data, and the TensorFlow as a Service (TFaaS) service in charge of the inference phase.},
  archive      = {J_TCC},
  author       = {Luca Giommi and Daniele Spiga and Mattia Paladino and Valentin Kuznetsov and Daniele Bonacorsi},
  doi          = {10.1109/TCC.2025.3535793},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Developments on the “Machine learning as a service for high energy physics” framework and related cloud native solution},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Joint computation offloading and resource allocation in
mobile-edge cloud computing: A two-layer game approach. <em>TCC</em>,
1–18. (<a href="https://doi.org/10.1109/TCC.2025.3538090">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Mobile-Edge Cloud Computing (MECC) plays a crucial role in balancing low-latency services at the edge with the computational capabilities of cloud data centers (DCs). However, many existing studies focus on single-provider settings or limit their analysis to interactions between mobile devices (MDs) and edge servers (ESs), often overlooking the competition that occurs among ESs from different providers. This paper introduces an innovative two-layer game framework that captures independent self-interested competition among MDs and ESs, providing a more accurate reflection of multi-vendor environments. Additionally, the framework explores the influence of cloud-edge collaboration on ES competition, offering new insights into these dynamics. The proposed model extends previous research by developing algorithms that optimize task offloading and resource allocation strategies for both MDs and ESs, ensuring the convergence to Nash equilibrium in both layers. Simulation results demonstrate the potential of the framework to improve resource efficiency and system responsiveness in multi-provider MECC environments.},
  archive      = {J_TCC},
  author       = {Zhenli He and Ying Guo and Xiaolong Zhai and Mingxiong Zhao and Wei Zhou and Keqin Li},
  doi          = {10.1109/TCC.2025.3538090},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Joint computation offloading and resource allocation in mobile-edge cloud computing: A two-layer game approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). Cache allocation in multi-tenant edge computing: An online
model-based reinforcement learning approach. <em>TCC</em>, 1–14. (<a
href="https://doi.org/10.1109/TCC.2025.3538158">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {We consider a Network Operator (NO) that owns Edge Computing (EC) resources, virtualizes them and lets third party Service Providers (SPs) run their services, using the allocated slice of resources. We focus on one specific resource, i.e., cache space, and on the problem of how to allocate it among several SPs in order to minimize the backhaul traffic. Due to confidentiality guarantees, the NO cannot observe the nature of the traffic of SPs, which is encrypted. Allocation decisions are thus challenging, since they must be taken solely based on observed monitoring information. Another challenge is that not all the traffic is cacheable. We propose a data-driven cache allocation strategy, based on Reinforcement Learning (RL). Unlike most RL applications, in which the decision policy is learned offline on a simulator, we assume no previous knowledge is available to build such a simulator. We thus apply RL in an online fashion, i.e., the model and the policy are learned by directly perturbing and monitoring the actual system. Since perturbations generate spurious traffic, we thus need to limit perturbations. This requires learning to be extremely efficient. To this aim, we devise a strategy that learns an approximation of the cost function, while interacting with the system. We then use such an approximation in a Model-Based RL (MB-RL) to speed up convergence. We prove analytically that our strategy brings cache allocation boundedly close to the optimum and stably remains in such an allocation. We show in simulations that such convergence is obtained within few minutes. We also study its fairness, its sensitivity to several scenario characteristics and compare it with a method from the state-of-the-art.},
  archive      = {J_TCC},
  author       = {Ayoub Ben-Ameur and Andrea Araldo and Tijani Chahed and György Dán},
  doi          = {10.1109/TCC.2025.3538158},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {Cache allocation in multi-tenant edge computing: An online model-based reinforcement learning approach},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A cost-aware operator migration approach for distributed
stream processing system. <em>TCC</em>, 1–14. (<a
href="https://doi.org/10.1109/TCC.2025.3538512">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Stream processing is integral to edge computing due to its low-latency attributes. Nevertheless, variability in user group sizes and disparate computing capabilities of edge devices necessitate frequent operator migrations within the stream. Moreover, intricate dependencies among stream operators often obscure the detection of potential bottleneck operators until an identified bottleneck is migrated in the stream. To address this, we propose a Cost-Aware Operator Migration (CAOM) scheme. The CAOM scheme incorporates a bottleneck operator detection mechanism that directly identifies all bottleneck operators based on task running metrics. This approach avoids multiple consecutive operator migrations in complex tasks, reducing the number of task interruptions caused by operator migration. Moreover, CAOM takes into account the temporal variance in operator migration costs. By factoring in the fluctuating data generation rate from data sources at different time intervals, CAOM selects the optimal start time for operator migration to minimize the amount of accumulated data during task interruptions. Finally, we implemented CAOM on Apache Flink and evaluated its performance using the WordCount and Nexmark applications. Our experiments show that CAOM effectively reduces the number of necessary operator migrations in tasks with complex topologies and decreases the latency overhead associated with operator migration compared to state-of-the-art schemes.},
  archive      = {J_TCC},
  author       = {Jiawei Tan and Zhuo Tang and Wentong Cai and Wen Jun Tan and Xiong Xiao and Jiapeng Zhang and Yi Gao and Kenli Li},
  doi          = {10.1109/TCC.2025.3538512},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {2},
  pages        = {1-14},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A cost-aware operator migration approach for distributed stream processing system},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). GHPFL: Advancing personalized edge-based learning through
optimized bandwidth utilization. <em>TCC</em>, 1–12. (<a
href="https://doi.org/10.1109/TCC.2025.3540023">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Federated learning (FL) is increasingly adopted to combine knowledge from clients in training without revealing their private data. In order to improve the performance of different participants, personalized FL has recently been proposed. However, considering the non-independent and identically distributed (non-IID) data and limited bandwidth at clients, the model performance could be compromised. In reality, clients near each other often tend to have similar data distributions. In this work, we train the personalized edge-based model in the client-edge-server FL. While considering the differences in data distribution, we fully utilize the limited bandwidth resources. To make training efficient and accurate at the same time, An intuitive idea is to learn as much useful knowledge as possible from other edges and reduce the accuracy loss incurred by non-IID data. Therefore, we devise Grouping Hierarchical Personalized Federated Learning (GHPFL). In this framework, each edge establishes physical connections with multiple clients, while the server physically connects with edges. It clusters edges into groups and establishes client-edge logical connections for synchronization. This is based on data similarities that the nodes actively identify, as well as the underlying physical topology. We perform a large-scale evaluation to demonstrate GHPFL&#39;s benefits over other schemes.},
  archive      = {J_TCC},
  author       = {Kaiwei Mo and Wei Lin and Jiaxun Lu and Chun Jason Xue and Yunfeng Shao and Hong Xu},
  doi          = {10.1109/TCC.2025.3540023},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {GHPFL: Advancing personalized edge-based learning through optimized bandwidth utilization},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). PPEC: A privacy-preserving, cost-effective incremental
density peak clustering analysis on encrypted outsourced data.
<em>TCC</em>, 1–12. (<a
href="https://doi.org/10.1109/TCC.2025.3541749">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Call detail records (CDRs) provide valuable insights into user behavior, which are instrumental for telecom companies in optimizing network coverage and service quality. However, while cloud computing facilitates clustering analysis on a vast scale of CDR data, it introduces privacy risks. The challenge lies in striking a balance between efficiency, security, and cost-effectiveness in privacy-preserving algorithms. To tackle this issue, we propose a privacy-preserving and cost-effective incremental density peak clustering scheme. Our approach leverages homomorphic encryption and order-preserving encryption to enable direct computations and clustering on encrypted data. Moreover, it employs reaching definition analysis to optimize the execution flow of static tasks, pinpointing the optimal junctures for transitioning between the two types of encryption to reduce communication overhead. Furthermore, our scheme utilizes a game theory-based verification strategy to ascertain the accuracy of the results. This methodology can be effectively deployed on the Ethereum blockchain via smart contracts. A comprehensive security analysis confirms that our scheme upholds both privacy and data integrity. Experimental evaluations substantiate the clustering accuracy, communication load, and computational efficiency of our scheme, thereby validating its viability in real-world applications.},
  archive      = {J_TCC},
  author       = {Haomiao Yang and ZiKang Ding and Ruiheng Lu and Kunlan Xiang and Hongwei Li and Dakui Wu},
  doi          = {10.1109/TCC.2025.3541749},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {2},
  pages        = {1-12},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {PPEC: A privacy-preserving, cost-effective incremental density peak clustering analysis on encrypted outsourced data},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). HyperDrive: Direct network telemetry storage via
programmable switches. <em>TCC</em>, 1–13. (<a
href="https://doi.org/10.1109/TCC.2025.3543477">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {In cloud datacenter operations, telemetry and logs are indispensable, enabling essential services such as network diagnostics, auditing, and knowledge discovery. The escalating scale of data centers, coupled with increased bandwidth and finer-grained telemetry, results in an overwhelming volume of data. This proliferation poses significant storage challenges for telemetry systems. In this paper, we introduce HyperDrive, an innovative system designed to efficiently store large volumes of telemetry and logs in data centers using programmable switches. This in-network approach effectively mitigates bandwidth bottlenecks commonly associated with traditional endpoint-based methods. To our knowledge, we are the first to use a programmable switch to directly control storage, bypassing the CPU to achieve the best performance. With merely 21% of a switch&#39;s resources, our HyperDrive implementation showcases remarkable scalability and efficiency. Through rigorous evaluation, it has demonstrated linear scaling capabilities, efficiently managing 12 SSDs on a single server with minimal host overhead. In an eight-server testbed, HyperDrive achieved an impressive throughput of approximately 730Gbps, underscoring its potential to transform data center telemetry and logging practices.},
  archive      = {J_TCC},
  author       = {Ziyuan Liu and Zhixiong Niu and Ran Shu and Wenxue Cheng and Lihua Yuan and Jacob Nelson and Dan R. K. Ports and Peng Cheng and Yongqiang Xiong},
  doi          = {10.1109/TCC.2025.3543477},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {HyperDrive: Direct network telemetry storage via programmable switches},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). A dynamic and secure join query protocol for multi-user
environment in cloud computing. <em>TCC</em>, 1–13. (<a
href="https://doi.org/10.1109/TCC.2025.3544628">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {The development of cloud computing needs to continuously improve and perfect the privacy-preserving techniques for the user&#39;s confidential data. Multi-user join query, as an important method of data sharing, allows multiple legitimate data users to perform join query over the data owner&#39;s encrypted database. However, some existing join query protocols may face some challenges in the practical application, such as practicality, security, and efficiency. In this paper, we put forward a dynamic and secure join query protocol in the multi-user environment. Compared with some existing protocols, the proposed protocol has the following advantages. On the one hand, we utilize the dynamic oblivious cross tags structure to realize an efficient join query with forward and backward security. On the other hand, we combine the randomizable distributed key-homomorphic pseudo-random functions with join query to support multiple data users, which can provide resilience against the single user&#39;s key leakage and resist collusion attacks between the cloud server and a subset of data users. We formally define and prove the security of proposed protocol. In addition, we give a detailed analysis of computation and communication overheads to demonstrate the efficiency of proposed protocol. Finally, we carry out some experimental evaluations to further demonstrate the superiority of functionality and efficiency.},
  archive      = {J_TCC},
  author       = {Hongjun Li and Debiao He and Qi Feng and Xiaolin Yang and Qingcai Luo},
  doi          = {10.1109/TCC.2025.3544628},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {2},
  pages        = {1-13},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {A dynamic and secure join query protocol for multi-user environment in cloud computing},
  year         = {2025},
}
</textarea>
</details></li>
<li><details>
<summary>
(2025). RLDR: Reinforcement learning-based fast data recovery in
cloud-of-clouds storage systems. <em>TCC</em>, 1–18. (<a
href="https://doi.org/10.1109/TCC.2025.3546528">www</a>)
</summary>
<textarea id="copyID" onclick="copy(this)" rows="16" cols="145">
@article{ ,
  abstract     = {Cloud-of-clouds storage systems are widely used in online applications, where user data are encrypted, encoded, and stored in multiple clouds. When some cloud nodes fail, the storage systems can reconstruct the lost data and store it in the substitute nodes. It is a challenge to reduce the latency of data recovery to ensure data reliability. In this paper, we adopt a Reinforcement Learning-based Data Recovery (RLDR) approach to reduce the regeneration time. By employing the Monte-Carlo method, our approach can construct the tree-topology-based regeneration process, a.k.a. regeneration tree, to effectively reduce the regeneration time. Through rigorous analysis, we apply the information flow graph to optimize the inter-cloud traffic for a given regeneration tree. To verify the merit of RLDR, We conduct extensive experiments on real-world traces. Experiments demonstrate that RLDR can significantly accelerate the regeneration process. Specifically, RLDR can reduce the regeneration time by up to 92% and increase the throughput by up to twelve-fold, compared to the prior art.},
  archive      = {J_TCC},
  author       = {Jiajie Shen and Bochun Wu and Maoyi Wang and Sai Zou and Laizhong Cui and Wei Ni},
  doi          = {10.1109/TCC.2025.3546528},
  journal      = {IEEE Transactions on Cloud Computing},
  month        = {2},
  pages        = {1-18},
  shortjournal = {IEEE Trans. Cloud Comput.},
  title        = {RLDR: Reinforcement learning-based fast data recovery in cloud-of-clouds storage systems},
  year         = {2025},
}
</textarea>
</details></li>
</ul>

</body>
</html>
